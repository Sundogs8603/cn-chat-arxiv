<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;</title><link>http://arxiv.org/abs/2310.13012</link><description>&lt;p&gt;
H2O&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
H2O Open Ecosystem for State-of-the-art Large Language Models. (arXiv:2310.13012v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13012
&lt;/p&gt;
&lt;p&gt;
H2O&#25512;&#20986;&#20102;&#24320;&#25918;&#29983;&#24577;&#31995;&#32479;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#27979;&#35797;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;h2oGPT&#21644;H2O LLM Studio&#12290;&#36825;&#19968;&#24320;&#28304;&#39033;&#30446;&#25552;&#20379;&#20102;&#20840;&#38754;&#24320;&#25918;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#33021;&#22815;&#24110;&#21161;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#20351;&#20854;&#26356;&#21152;&#21487;&#20449;&#36182;&#21644;&#21487;&#35775;&#38382;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#39033;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#39118;&#38505;&#65292;&#20363;&#22914;&#23384;&#22312;&#20559;&#35265;&#12289;&#31169;&#26377;&#12289;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#26377;&#23475;&#30340;&#25991;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#24320;&#25918;&#12289;&#36879;&#26126;&#21644;&#23433;&#20840;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#24320;&#28304;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#24320;&#21457;&#21644;&#27979;&#35797;LLMs&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#25512;&#21160;&#23545;&#23553;&#38381;&#28304;&#26041;&#27861;&#30340;&#24320;&#25918;&#24335;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;h2oGPT&#65292;&#21363;&#20174;70&#20159;&#21040;700&#20159;&#21442;&#25968;&#30340;&#19968;&#31995;&#21015;&#31934;&#32454;&#35843;&#25972;&#30340;LLMs&#12290;&#25105;&#20204;&#36824;&#25512;&#20986;&#20102;H2O LLM Studio&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#21644;&#26080;&#20195;&#30721;GUI&#65292;&#19987;&#20026;&#20351;&#29992;&#26368;&#26032;&#30340;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;LLMs&#30340;&#39640;&#25928;&#31934;&#32454;&#35843;&#25972;&#12289;&#35780;&#20272;&#21644;&#37096;&#32626;&#32780;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#35777;&#19979;&#25480;&#26435;&#20351;&#29992;&#12290;&#25105;&#20204;&#30456;&#20449;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#24182;&#20351;&#20854;&#26356;&#21487;&#35775;&#38382;&#21644;&#21487;&#20449;&#36182;&#12290;&#28436;&#31034;&#32593;&#22336;&#20026;&#65306;https://gpt.h2o.ai/
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) represent a revolution in AI. However, they also pose many significant risks, such as the presence of biased, private, copyrighted or harmful text. For this reason we need open, transparent and safe solutions. We introduce a complete open-source ecosystem for developing and testing LLMs. The goal of this project is to boost open alternatives to closed-source approaches. We release h2oGPT, a family of fine-tuned LLMs from 7 to 70 Billion parameters. We also introduce H2O LLM Studio, a framework and no-code GUI designed for efficient fine-tuning, evaluation, and deployment of LLMs using the most recent state-of-the-art techniques. Our code and models are licensed under fully permissive Apache 2.0 licenses. We believe open-source language models help to boost AI development and make it more accessible and trustworthy. The demo is available at: https://gpt.h2o.ai/
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;39&#20010;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#30446;&#26631;&#12289;&#20998;&#35789;&#26041;&#27861;&#12289;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#20998;&#35789;&#21644;&#27169;&#22411;&#30446;&#26631;&#31561;&#22240;&#32032;&#23545;MLMs&#23398;&#20064;&#30340;&#31038;&#20250;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.12936</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#19982;&#20219;&#21153;&#34920;&#29616;&#30340;&#39044;&#27979;&#22240;&#32032;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models. (arXiv:2310.12936v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;39&#20010;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#30446;&#26631;&#12289;&#20998;&#35789;&#26041;&#27861;&#12289;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#20998;&#35789;&#21644;&#27169;&#22411;&#30446;&#26631;&#31561;&#22240;&#32032;&#23545;MLMs&#23398;&#20064;&#30340;&#31038;&#20250;&#20559;&#35265;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#25253;&#21578;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#39044;&#35757;&#32451;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;(MLMs)&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;MLM&#19982;&#35768;&#22810;&#28508;&#22312;&#22240;&#32032;&#30456;&#20851;&#65292;&#22914;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#12289;&#35757;&#32451;&#30446;&#26631;&#12289;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#39046;&#22495;&#12289;&#20998;&#35789;&#21644;&#39044;&#35757;&#32451;&#35821;&#26009;&#20013;&#21253;&#21547;&#30340;&#35821;&#35328;&#31561;&#12290;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#22240;&#32032;&#20013;&#21738;&#20123;&#24433;&#21709;&#20102;MLMs&#23398;&#20064;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#20026;&#20102;&#30740;&#31350;&#27169;&#22411;&#22240;&#32032;&#19982;MLMs&#23398;&#20064;&#30340;&#31038;&#20250;&#20559;&#35265;&#20197;&#21450;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#23545;39&#20010;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#12289;&#35757;&#32451;&#30446;&#26631;&#12289;&#20998;&#35789;&#26041;&#27861;&#12289;&#35757;&#32451;&#25968;&#25454;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#39044;&#35757;&#32451;MLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22312;&#20808;&#21069;&#30340;&#25991;&#29486;&#20013;&#32463;&#24120;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#22914;&#20998;&#35789;&#25110;&#27169;&#22411;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various types of social biases have been reported with pretrained Masked Language Models (MLMs) in prior work. However, multiple underlying factors are associated with an MLM such as its model size, size of the training data, training objectives, the domain from which pretraining data is sampled, tokenization, and languages present in the pretrained corpora, to name a few. It remains unclear as to which of those factors influence social biases that are learned by MLMs. To study the relationship between model factors and the social biases learned by an MLM, as well as the downstream task performance of the model, we conduct a comprehensive study over 39 pretrained MLMs covering different model sizes, training objectives, tokenization methods, training data domains and languages. Our results shed light on important factors often neglected in prior literature, such as tokenization or model objectives.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#25925;&#20107;&#32423;&#31867;&#27604;&#35821;&#26009;&#24211;StoryAnalogy&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;&#31867;&#27604;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;StoryAnalogy&#20013;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.12874</link><description>&lt;p&gt;
StoryAnalogy: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34893;&#29983;&#20986;&#25925;&#20107;&#32423;&#31867;&#27604;&#20197;&#35299;&#24320;&#31867;&#27604;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
StoryAnalogy: Deriving Story-level Analogies from Large Language Models to Unlock Analogical Understanding. (arXiv:2310.12874v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#25925;&#20107;&#32423;&#31867;&#27604;&#35821;&#26009;&#24211;StoryAnalogy&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;&#31867;&#27604;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#36825;&#20123;&#20219;&#21153;&#23545;&#20110;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#21644;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#21457;&#29616;&#36890;&#36807;&#20351;&#29992;StoryAnalogy&#20013;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#20107;&#20043;&#38388;&#30340;&#31867;&#27604;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#26368;&#20851;&#38190;&#30340;&#33021;&#21147;&#20043;&#19968;&#12290;&#26412;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#35268;&#27169;&#24040;&#22823;&#30340;&#25925;&#20107;&#32423;&#31867;&#27604;&#35821;&#26009;&#24211;StoryAnalogy&#26469;&#35780;&#20272;&#35782;&#21035;&#21644;&#29983;&#25104;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;24K&#20010;&#25925;&#20107;&#23545;&#65292;&#24182;&#23545;&#26469;&#33258;&#25193;&#23637;&#32467;&#26500;&#26144;&#23556;&#29702;&#35770;&#30340;&#20004;&#20010;&#30456;&#20284;&#24615;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#22312;StoryAnalogy&#19978;&#30340;&#27979;&#35797;&#65292;&#39318;&#27425;&#35780;&#20272;&#20102;&#25925;&#20107;&#32423;&#31867;&#27604;&#30340;&#35782;&#21035;&#21644;&#29983;&#25104;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#31867;&#27604;&#35782;&#21035;&#20219;&#21153;&#23545;&#20110;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#20197;&#21450;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;LLaMa&#65289;&#26469;&#35828;&#37117;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20854;&#20013;ChatGPT&#22312;&#22810;&#36873;&#39064;&#20013;&#21482;&#33021;&#36798;&#21040;&#32422;30%&#30340;&#20934;&#30830;&#29575;&#65288;&#23545;&#20110;&#20154;&#31867;&#32780;&#35328;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;85%&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;StoryAnalogy&#20013;&#30340;&#25968;&#25454;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#29983;&#25104;&#36136;&#37327;&#65292;&#20854;&#20013;&#32463;&#36807;&#24494;&#35843;&#30340;FlanT5-xxl&#27169;&#22411;&#33719;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogy-making between narratives is one of the most critical abilities in natural language understanding. In this paper, we evaluate the ability to identify and generate analogy by building a first-of-its-kind large-scale story-level analogy corpus, StoryAnalogy, which contains 24K story pairs from diverse domains with human annotations on two similarities from the extended Structure-Mapping Theory. We design a set of tests on StoryAnalogy, presenting the first evaluation of story-level analogy identification and generation. Interestingly, we find that the analogy identification tasks are extremely challenging not only for the sentence embedding models but also for the recent large language models (LLMs) such as ChatGPT and LLaMa, where ChatGPT only achieved around 30% accuracy in multiple-choice questions (&gt; 85% accuracy for humans). Finally, we find that data in StoryAnalogy can improve LLMs analogy generation quality, where a fine-tuned FlanT5-xxl model yields comparable performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2310.12823</link><description>&lt;p&gt;
AgentTuning: &#20026;LLMs&#23454;&#29616;&#36890;&#29992;&#20195;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
AgentTuning: Enabling Generalized Agent Abilities for LLMs. (arXiv:2310.12823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#36890;&#36807;&#26500;&#24314;AgentInstruct&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#19968;&#31181;&#28151;&#21512;&#35757;&#32451;&#26041;&#27861;&#65292;&#20316;&#32773;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25552;&#39640;LLMs&#20195;&#29702;&#33021;&#21147;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;LLMs&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#23427;&#20204;&#20316;&#20026;&#20195;&#29702;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#23545;&#22797;&#26434;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#21830;&#19994;&#27169;&#22411;&#12290;&#36825;&#20123;&#20195;&#29702;&#20219;&#21153;&#23558;LLMs&#20316;&#20026;&#36127;&#36131;&#35268;&#21010;&#12289;&#35760;&#24518;&#21644;&#24037;&#20855;&#21033;&#29992;&#30340;&#20013;&#22830;&#25511;&#21046;&#22120;&#65292;&#38656;&#35201;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26041;&#27861;&#21644;&#24378;&#22823;&#30340;LLMs&#25165;&#33021;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25552;&#31034;&#26041;&#27861;&#26469;&#23436;&#25104;&#29305;&#23450;&#30340;&#20195;&#29702;&#20219;&#21153;&#65292;&#20294;&#32570;&#20047;&#30740;&#31350;&#19987;&#27880;&#20110;&#25552;&#39640;LLMs&#33258;&#36523;&#30340;&#20195;&#29702;&#33021;&#21147;&#32780;&#19981;&#25439;&#23475;&#20854;&#36890;&#29992;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AgentTuning&#65292;&#19968;&#31181;&#31616;&#21333;&#32780;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#36890;&#29992;&#30340;LLM&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;AgentInstruct&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#39640;&#36136;&#37327;&#30340;&#20132;&#20114;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open large language models (LLMs) with great performance in various tasks have significantly advanced the development of LLMs. However, they are far inferior to commercial models such as ChatGPT and GPT-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ LLMs as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust LLMs to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of LLMs themselves without compromising their general abilities. In this work, we present AgentTuning, a simple and general method to enhance the agent abilities of LLMs while maintaining their general LLM capabilities. We construct AgentInstruct, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#28151;&#21512;&#35821;&#38899;&#27969;&#24335;&#32763;&#35793;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#27969;&#24335;&#35774;&#32622;&#21644;&#32763;&#35793;&#21040;&#31532;&#19977;&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.12648</link><description>&lt;p&gt;
&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;&#28151;&#21512;&#35821;&#38899;&#27969;&#24335;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Real-World Streaming Speech Translation for Code-Switched Speech. (arXiv:2310.12648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38754;&#21521;&#30495;&#23454;&#19990;&#30028;&#30340;&#28151;&#21512;&#35821;&#38899;&#27969;&#24335;&#32763;&#35793;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20027;&#35201;&#20851;&#27880;&#20102;&#27969;&#24335;&#35774;&#32622;&#21644;&#32763;&#35793;&#21040;&#31532;&#19977;&#31181;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#32447;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#35821;&#35328;&#65288;CS&#65289;&#21363;&#22312;&#19968;&#21477;&#35805;&#20013;&#28151;&#21512;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#65292;&#26159;&#36890;&#20449;&#20013;&#24120;&#35265;&#30340;&#29616;&#35937;&#65292;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#29615;&#22659;&#19979;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20808;&#21069;&#20851;&#20110;CS&#35821;&#38899;&#30340;&#30740;&#31350;&#22312;&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#20165;&#38480;&#20110;&#31163;&#32447;&#22330;&#26223;&#65292;&#24182;&#19988;&#20165;&#33021;&#32763;&#35793;&#25104;&#28304;&#35821;&#20013;&#30340;&#19968;&#31181;&#35821;&#35328;&#65288;&#21333;&#35821;&#36716;&#24405;&#65289;&#12290;&#26412;&#25991;&#30528;&#37325;&#30740;&#31350;&#20102;&#20004;&#20010;&#23578;&#26410;&#25506;&#32034;&#30340;&#38754;&#21521;&#29616;&#23454;&#19990;&#30028;&#30340;CS&#35821;&#38899;&#32763;&#35793;&#39046;&#22495;&#65306;&#27969;&#24335;&#35774;&#32622;&#21644;&#32763;&#35793;&#21040;&#31532;&#19977;&#31181;&#35821;&#35328;&#65288;&#21363;&#28304;&#35821;&#20013;&#26410;&#21253;&#21547;&#30340;&#35821;&#35328;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;Fisher&#21644;Miami&#27979;&#35797;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;&#35199;&#29677;&#29273;&#35821;&#21644;&#24503;&#35821;&#20316;&#20026;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;&#31163;&#32447;&#21644;&#27969;&#24335;ST&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#24314;&#31435;&#20102;&#20043;&#21069;&#25552;&#21040;&#30340;&#20004;&#20010;&#35774;&#32622;&#30340;&#22522;&#32447;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching (CS), i.e. mixing different languages in a single sentence, is a common phenomenon in communication and can be challenging in many Natural Language Processing (NLP) settings. Previous studies on CS speech have shown promising results for end-to-end speech translation (ST), but have been limited to offline scenarios and to translation to one of the languages present in the source (\textit{monolingual transcription}).  In this paper, we focus on two essential yet unexplored areas for real-world CS speech translation: streaming settings, and translation to a third language (i.e., a language not included in the source). To this end, we extend the Fisher and Miami test and validation datasets to include new targets in Spanish and German. Using this data, we train a model for both offline and streaming ST and we establish baseline results for the two settings mentioned earlier.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.12580</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pretraining Language Models with Text-Attributed Heterogeneous Graphs. (arXiv:2310.12580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26126;&#30830;&#32771;&#34385;&#21040;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#25299;&#25169;&#21644;&#24322;&#26500;&#20449;&#24687;&#12290;&#36890;&#36807;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#21644;&#36741;&#21161;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#39044;&#27979;&#20102;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#20013;&#30340;&#33410;&#28857;&#12290;&#21516;&#26102;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#25991;&#26412;&#20016;&#23500;&#24615;&#21152;&#26435;&#30340;&#33410;&#28857;&#25277;&#26679;&#31574;&#30053;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65288;&#22914;&#23398;&#26415;&#32593;&#32476;&#12289;&#31038;&#20132;&#24179;&#21488;&#65289;&#65292;&#19981;&#21516;&#31867;&#22411;&#30340;&#23454;&#20307;&#19981;&#20165;&#19982;&#25991;&#26412;&#30456;&#20851;&#65292;&#36824;&#36890;&#36807;&#21508;&#31181;&#20851;&#31995;&#30456;&#36830;&#65292;&#36825;&#21487;&#20197;&#34987;&#25277;&#35937;&#20026;&#25991;&#26412;&#23646;&#24615;&#24322;&#26500;&#22270;&#65288;Text-Attributed Heterogeneous Graphs&#65292;TAHGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios (e.g., academic networks, social platforms), different types of entities are not only associated with texts but also connected by various relationships, which can be abstracted as Text-Attributed Heterogeneous Graphs (TAHGs). Current pretraining tasks for Language Models (LMs) primarily focus on separately learning the textual information of each entity and overlook the crucial aspect of capturing topological connections among entities in TAHGs. In this paper, we present a new pretraining framework for LMs that explicitly considers the topological and heterogeneous information in TAHGs. Firstly, we define a context graph as neighborhoods of a target node within specific orders and propose a topology-aware pretraining task to predict nodes involved in the context graph by jointly optimizing an LM and an auxiliary heterogeneous graph neural network. Secondly, based on the observation that some nodes are text-rich while others have little text, we devise a tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IncidentAI&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23433;&#20840;&#39044;&#38450;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#30001;&#39640;&#21387;&#27668;&#20307;&#20445;&#25252;&#31649;&#29702;&#39046;&#22495;&#30340;&#19987;&#23478;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20107;&#25925;&#25253;&#21578;&#20197;&#39044;&#38450;&#26410;&#26469;&#30340;&#25925;&#38556;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;NLP&#21644;&#20107;&#25925;&#31649;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2310.12074</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#23433;&#20840;&#30340;&#25805;&#20316;&#65306;&#39044;&#38450;&#26410;&#26469;&#25925;&#38556;&#30340;&#39640;&#21387;&#27668;&#20307;&#20107;&#25925;&#19987;&#23478;&#21442;&#19982;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards Safer Operations: An Expert-involved Dataset of High-Pressure Gas Incidents for Preventing Future Failures. (arXiv:2310.12074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IncidentAI&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23433;&#20840;&#39044;&#38450;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#30001;&#39640;&#21387;&#27668;&#20307;&#20445;&#25252;&#31649;&#29702;&#39046;&#22495;&#30340;&#19987;&#23478;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20107;&#25925;&#25253;&#21578;&#20197;&#39044;&#38450;&#26410;&#26469;&#30340;&#25925;&#38556;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20419;&#36827;NLP&#21644;&#20107;&#25925;&#31649;&#29702;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23433;&#20840;&#39044;&#38450;&#30340;&#26032;&#30340;IncidentAI&#25968;&#25454;&#38598;&#12290;&#19982;&#36890;&#24120;&#21482;&#21253;&#21547;&#19968;&#20010;&#20219;&#21153;&#30340;&#20808;&#21069;&#35821;&#26009;&#24211;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;&#19977;&#20010;&#20219;&#21153;&#65306;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#21644;&#20449;&#24687;&#26816;&#32034;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#33267;&#23569;&#26377;&#20845;&#24180;&#39640;&#21387;&#27668;&#20307;&#20445;&#25252;&#31649;&#29702;&#23454;&#36341;&#32463;&#39564;&#30340;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25968;&#25454;&#38598;&#22312;&#23433;&#20840;&#39044;&#38450;&#22330;&#26223;&#20013;&#30340;&#36129;&#29486;&#12290;&#23545;&#19977;&#20010;&#20219;&#21153;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26377;&#21161;&#20110;&#20998;&#26512;&#20107;&#25925;&#25253;&#21578;&#20197;&#36991;&#20813;&#26410;&#26469;&#30340;&#25925;&#38556;&#12290;&#35813;&#25968;&#25454;&#38598;&#20415;&#20110;NLP&#21644;&#20107;&#25925;&#31649;&#29702;&#31038;&#21306;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#20063;&#21487;&#25552;&#20379;&#35775;&#38382;&#65288;IncidentAI&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/Cinnamon/incident-ai-dataset &#33719;&#21462;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new IncidentAI dataset for safety prevention. Different from prior corpora that usually contain a single task, our dataset comprises three tasks: named entity recognition, cause-effect extraction, and information retrieval. The dataset is annotated by domain experts who have at least six years of practical experience as high-pressure gas conservation managers. We validate the contribution of the dataset in the scenario of safety prevention. Preliminary results on the three tasks show that NLP techniques are beneficial for analyzing incident reports to prevent future failures. The dataset facilitates future research in NLP and incident management communities. The access to the dataset is also provided (the IncidentAI dataset is available at: https://github.com/Cinnamon/incident-ai-dataset).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12059</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education. (arXiv:2310.12059v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36234;&#21335;&#26222;&#36890;&#25945;&#32946;&#20013;&#23545;&#22810;&#39033;&#36873;&#25321;&#39064;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#31526;&#21495;&#32465;&#23450;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#24615;&#21644;&#23569;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25191;&#34892;&#22810;&#39033;&#36873;&#25321;&#31526;&#21495;&#32465;&#23450;&#65288;MCSB&#65289;&#29992;&#20110;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#65288;MCQA&#65289;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#37325;&#28857;&#25918;&#22312;&#36234;&#21335;&#35821;&#19978;&#65292;&#22240;&#20026;&#36234;&#21335;&#35821;&#20013;&#30340;&#25361;&#25112;&#24615;MCQA&#25968;&#25454;&#38598;&#36739;&#33521;&#35821;&#23569;&#12290;&#29616;&#26377;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;ViMMRC 1.0&#21644;ViMMRC 2.0&#65292;&#19987;&#27880;&#20110;&#25991;&#23398;&#38382;&#39064;&#12290;&#36234;&#21335;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;ChatGPT&#22312;2019&#24180;&#33267;2023&#24180;&#30340;&#36234;&#21335;&#22269;&#23478;&#39640;&#20013;&#27605;&#19994;&#32771;&#35797;&#65288;VNHSGE&#65289;&#20013;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;ChatGPT&#22914;&#20309;&#36880;&#27493;&#35299;&#20915;VNHSGE&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20026;&#25968;&#23398;&#12289;&#29289;&#29702;&#12289;&#21270;&#23398;&#21644;&#29983;&#29289;&#30340;LaTeX&#20844;&#24335;&#36755;&#20837;&#25552;&#20379;&#32467;&#26500;&#21270;&#25351;&#21335;&#65292;&#21019;&#24314;&#19968;&#20010;&#26032;&#39062;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#35780;&#20272;LLMs&#21644;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;MCSB&#33021;&#21147;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#35201;&#27714;&#20351;&#29992;&#20005;&#26684;&#30340;LaTeX&#26679;&#24335;&#36827;&#34892;&#36755;&#20837;&#12290;&#25105;&#20204;&#37325;&#28857;&#39044;&#27979;&#23383;&#31526;&#65288;A&#12289;B&#12289;C&#25110;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or 
&lt;/p&gt;</description></item><item><title>LoHoRavens&#26159;&#19968;&#20010;&#38024;&#23545;&#26426;&#22120;&#20154;&#26700;&#38754;&#25805;&#20316;&#30340;&#38271;&#26102;&#31243;&#35821;&#35328;&#26465;&#20214;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#39068;&#33394;&#12289;&#22823;&#23567;&#12289;&#31354;&#38388;&#12289;&#31639;&#26415;&#21644;&#24341;&#29992;&#31561;&#21508;&#31181;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#22914;&#20309;&#23558;&#35266;&#27979;&#21453;&#39304;&#32435;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38381;&#29615;&#35268;&#21010;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.12020</link><description>&lt;p&gt;
LoHoRavens: &#19968;&#39033;&#38024;&#23545;&#26426;&#22120;&#20154;&#26700;&#38754;&#25805;&#20316;&#30340;&#38271;&#26102;&#31243;&#35821;&#35328;&#26465;&#20214;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation. (arXiv:2310.12020v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12020
&lt;/p&gt;
&lt;p&gt;
LoHoRavens&#26159;&#19968;&#20010;&#38024;&#23545;&#26426;&#22120;&#20154;&#26700;&#38754;&#25805;&#20316;&#30340;&#38271;&#26102;&#31243;&#35821;&#35328;&#26465;&#20214;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#39068;&#33394;&#12289;&#22823;&#23567;&#12289;&#31354;&#38388;&#12289;&#31639;&#26415;&#21644;&#24341;&#29992;&#31561;&#21508;&#31181;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#36824;&#25506;&#32034;&#20102;&#22312;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#22914;&#20309;&#23558;&#35266;&#27979;&#21453;&#39304;&#32435;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38381;&#29615;&#35268;&#21010;&#20013;&#30340;&#20004;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#39564;&#24335;&#20195;&#29702;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34701;&#21512;&#20026;&#20307;&#39564;&#24335;&#25351;&#23548;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#27880;&#37322;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#38271;&#26102;&#31243;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#32570;&#20047;&#29992;&#20110;&#27979;&#35797;&#35821;&#35328;&#26465;&#20214;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#25512;&#29702;&#38271;&#26102;&#31243;&#33021;&#21147;&#30340;&#20844;&#20849;&#22522;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#26700;&#38754;&#25805;&#20316;&#20219;&#21153;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;LoHoRavens&#8221;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#39068;&#33394;&#12289;&#22823;&#23567;&#12289;&#31354;&#38388;&#12289;&#31639;&#26415;&#21644;&#24341;&#29992;&#31561;&#21508;&#31181;&#38271;&#26102;&#31243;&#25512;&#29702;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38271;&#26102;&#31243;&#25805;&#20316;&#30340;&#20219;&#21153;&#65292;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#30340;&#27169;&#24577;&#36807;&#28193;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#26426;&#22120;&#20154;&#25191;&#34892;&#36807;&#31243;&#20013;&#23558;&#35266;&#27979;&#21453;&#39304;&#32435;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38381;&#29615;&#35268;&#21010;&#20013;&#65292;&#28982;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#23545;&#27492;&#36827;&#34892;&#30340;&#25506;&#32034;&#36739;&#23569;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#35299;&#20915;&#27169;&#24577;&#36807;&#28193;&#38382;&#39064;&#30340;&#26041;&#27861;&#65306;&#26631;&#39064;&#29983;&#25104;&#21644;&#24555;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
The convergence of embodied agents and large language models (LLMs) has brought significant advancements to embodied instruction following. Particularly, the strong reasoning capabilities of LLMs make it possible for robots to perform long-horizon tasks without expensive annotated demonstrations. However, public benchmarks for testing the long-horizon reasoning capabilities of language-conditioned robots in various scenarios are still missing. To fill this gap, this work focuses on the tabletop manipulation task and releases a simulation benchmark, \textit{LoHoRavens}, which covers various long-horizon reasoning aspects spanning color, size, space, arithmetics and reference. Furthermore, there is a key modality bridging problem for long-horizon manipulation tasks with LLMs: how to incorporate the observation feedback during robot execution for the LLM's closed-loop planning, which is however less studied by prior work. We investigate two methods of bridging the modality gap: caption ge
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2310.11960</link><description>&lt;p&gt;
&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65306;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#30340;&#20998;&#27835;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fast Multipole Attention: A Divide-and-Conquer Attention Mechanism for Long Sequences. (arXiv:2310.11960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#23558;&#27880;&#24847;&#21147;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#33258;&#27880;&#24847;&#21147;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38480;&#21046;&#20102;Transformer&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#19978;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#20351;&#29992;&#20998;&#27835;&#31574;&#30053;&#26469;&#20943;&#23569;&#27880;&#24847;&#21147;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#30340;&#26032;&#22411;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23558;&#38271;&#24230;&#20026;n&#30340;&#24207;&#21015;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20174;O(n^2)&#38477;&#20302;&#21040;O(n log n)&#25110;O(n)&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20840;&#23616;&#24863;&#30693;&#33539;&#22260;&#12290;&#36825;&#31181;&#20998;&#23618;&#26041;&#27861;&#23558;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20998;&#20026;O(log n)&#32423;&#30340;&#20998;&#36776;&#29575;&#65292;&#36739;&#36828;&#36317;&#31163;&#30340;&#32452;&#32676;&#36234;&#26469;&#36234;&#22823;&#65292;&#24182;&#23398;&#20064;&#35745;&#31639;&#32452;&#32676;&#25968;&#37327;&#30340;&#26435;&#37325;&#12290;&#22240;&#27492;&#65292;&#20197;&#39640;&#25928;&#20998;&#23618;&#30340;&#26041;&#24335;&#22312;&#36739;&#20302;&#30340;&#20998;&#36776;&#29575;&#20013;&#32771;&#34385;&#36828;&#31163;&#24444;&#27492;&#30340;&#26631;&#35760;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#24555;&#36895;&#22810;&#26497;&#21270;&#27880;&#24847;&#21147;&#30340;&#24635;&#20307;&#22797;&#26434;&#24230;&#20026;O(n)&#25110;O(n log n)&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved state-of-the-art performance in many areas. However, the quadratic complexity of self-attention with respect to the input length hinders the applicability of Transformer-based models to long sequences. To address this, we present Fast Multipole Attention, a new attention mechanism that uses a divide-and-conquer strategy to reduce the time and memory complexity of attention for sequences of length $n$ from $\mathcal{O}(n^2)$ to $\mathcal{O}(n \log n)$ or $O(n)$, while retaining a global receptive field. The hierarchical approach groups queries, keys, and values into $\mathcal{O}( \log n)$ levels of resolution, where groups at greater distances are increasingly larger in size and the weights to compute group quantities are learned. As such, the interaction between tokens far from each other is considered in lower resolution in an efficient hierarchical manner. The overall complexity of Fast Multipole Attention is $\mathcal{O}(n)$ or $\mathcal{O}(n \
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.11878</link><description>&lt;p&gt;
&#20174;&#19981;&#19968;&#33268;&#21040;&#27934;&#23519;&#65306;&#23545;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#30340;&#29702;&#30001;&#25968;&#25454;&#38598;&#26500;&#24314;&#36827;&#34892;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#21487;&#20449;&#36182;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;COC&#30740;&#31350;&#20165;&#38480;&#20110;&#30001;&#21333;&#20010;&#19987;&#23478;&#36827;&#34892;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24459;&#24072;&#22312;&#23545;&#26696;&#20214;&#20107;&#23454;&#36827;&#34892;&#35780;&#20272;&#26102;&#21487;&#33021;&#23384;&#22312;&#20998;&#27495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RAVE&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#29702;&#30001;&#21464;&#24322;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#22269;&#38469;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#20004;&#20301;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#24369;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20182;&#20204;&#30340;&#20998;&#27495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;COC&#29305;&#23450;&#30340;&#23376;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#39318;&#27425;&#20851;&#27880;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#65292;&#36825;&#22312;COC&#20803;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#32454;&#31890;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;SOTA COC&#27169;&#22411;&#22312;RAVE&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#31995;&#26041;&#38754;&#26377;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.11772</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#26469;&#25913;&#36827;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Long Document Topic Segmentation Models With Enhanced Coherence Modeling. (arXiv:2310.11772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#22686;&#24378;&#19968;&#33268;&#24615;&#24314;&#27169;&#30340;&#26041;&#24335;&#25913;&#36827;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#36890;&#36807;&#24341;&#20837;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#22312;&#25429;&#25417;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#30340;&#28145;&#23618;&#20851;&#31995;&#26041;&#38754;&#26377;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#21010;&#20998;&#23545;&#20110;&#33719;&#21462;&#32467;&#26500;&#21270;&#30340;&#38271;&#25991;&#26723;&#21644;&#25913;&#21892;&#20449;&#24687;&#26816;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#20854;&#33021;&#22815;&#33258;&#21160;&#20174;&#22823;&#37327;&#26631;&#27880;&#25968;&#25454;&#20013;&#25506;&#32034;&#20027;&#39064;&#36716;&#21464;&#30340;&#32447;&#32034;&#65292;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#31070;&#32463;&#27169;&#22411;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#38271;&#25991;&#26723;&#20027;&#39064;&#21010;&#20998;&#30340;&#21457;&#23637;&#65292;&#20294;&#23545;&#35821;&#20041;&#19968;&#33268;&#24615;&#21644;&#20027;&#39064;&#21010;&#20998;&#20043;&#38388;&#26356;&#28145;&#23618;&#27425;&#30340;&#20851;&#31995;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22686;&#24378;&#20102;&#26377;&#30417;&#30563;&#27169;&#22411;&#20174;&#32467;&#26500;&#21644;&#30456;&#20284;&#24615;&#20004;&#20010;&#26041;&#38754;&#25429;&#25417;&#19968;&#33268;&#24615;&#30340;&#33021;&#21147;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#20027;&#39064;&#21010;&#20998;&#24615;&#33021;&#65292;&#21253;&#25324;&#20027;&#39064;&#24863;&#30693;&#30340;&#21477;&#23376;&#32467;&#26500;&#39044;&#27979;&#65288;TSSP&#65289;&#21644;&#23545;&#27604;&#35821;&#20041;&#30456;&#20284;&#24615;&#23398;&#20064;&#65288;CSSL&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25552;&#20986;&#20102;TSSP&#20219;&#21153;&#65292;&#36890;&#36807;&#23398;&#20064;&#26080;&#24207;&#25991;&#26723;&#20013;&#30456;&#37051;&#21477;&#23376;&#30340;&#21407;&#22987;&#20851;&#31995;&#65292;&#24378;&#21046;&#27169;&#22411;&#29702;&#35299;&#32467;&#26500;&#20449;&#24687;&#65292;&#35813;&#26080;&#24207;&#25991;&#26723;&#30001;&#21516;&#26102;&#30772;&#22351;&#20027;&#39064;&#21644;
&lt;/p&gt;
&lt;p&gt;
Topic segmentation is critical for obtaining structured long documents and improving downstream tasks like information retrieval. Due to its ability of automatically exploring clues of topic shift from a large amount of labeled data, recent supervised neural models have greatly promoted the development of long document topic segmentation, but leaving the deeper relationship of semantic coherence and topic segmentation underexplored. Therefore, this paper enhances the supervised model's ability to capture coherence from both structure and similarity perspectives to further improve the topic segmentation performance, including the Topic-aware Sentence Structure Prediction (TSSP) and Contrastive Semantic Similarity Learning (CSSL). Specifically, the TSSP task is proposed to force the model to comprehend structural information by learning the original relations of adjacent sentences in a disarrayed document, which is constructed by jointly disrupting the original document at the topic and 
&lt;/p&gt;</description></item><item><title>VECHR&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#35782;&#21035;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#29702;&#30001;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#27169;&#22411;&#19982;&#19987;&#23478;&#30340;&#19968;&#33268;&#24615;&#26377;&#38480;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#26102;&#40065;&#26834;&#24615;&#20063;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11368</link><description>&lt;p&gt;
VECHR&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11368
&lt;/p&gt;
&lt;p&gt;
VECHR&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#35782;&#21035;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#29702;&#30001;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#27169;&#22411;&#19982;&#19987;&#23478;&#30340;&#19968;&#33268;&#24615;&#26377;&#38480;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#26102;&#40065;&#26834;&#24615;&#20063;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#33030;&#24369;&#24615;&#23545;&#20110;&#20102;&#35299;&#21644;&#23454;&#26045;&#26377;&#38024;&#23545;&#24615;&#30340;&#25903;&#25345;&#20197;&#22686;&#24378;&#26377;&#38656;&#35201;&#30340;&#20010;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#19968;&#28857;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#23588;&#20026;&#37325;&#35201;&#65292;&#27861;&#38498;&#23558;&#20844;&#32422;&#26631;&#20934;&#35843;&#25972;&#20026;&#28385;&#36275;&#23454;&#38469;&#20010;&#20307;&#38656;&#27714;&#65292;&#20174;&#32780;&#30830;&#20445;&#26377;&#25928;&#30340;&#20154;&#26435;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#33030;&#24369;&#24615;&#27010;&#24565;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#65292;&#20043;&#21069;&#27809;&#26377;NLP&#30740;&#31350;&#28041;&#21450;&#21040;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VECHR&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33030;&#24369;&#24615;&#31867;&#22411;&#20998;&#31867;&#21644;&#35299;&#37322;&#29702;&#30001;&#12290;&#25105;&#20204;&#20174;&#39044;&#27979;&#21644;&#35299;&#37322;&#24615;&#30340;&#35282;&#24230;&#23545;VECHR&#19978;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29305;&#28857;&#65292;&#39044;&#27979;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#19988;&#27169;&#22411;&#21644;&#19987;&#23478;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#24635;&#20307;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited per
&lt;/p&gt;</description></item><item><title>VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.11069</link><description>&lt;p&gt;
VoxArabica&#65306;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
VoxArabica: A Robust Dialect-Aware Arabic Speech Recognition System. (arXiv:2310.11069v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11069
&lt;/p&gt;
&lt;p&gt;
VoxArabica&#26159;&#19968;&#20010;&#31283;&#20581;&#30340;&#26041;&#35328;&#24863;&#30693;&#38463;&#25289;&#20271;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#65292;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35821;&#26041;&#35328;&#35782;&#21035;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#12290;&#35813;&#31995;&#32479;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#29992;&#20110;&#19981;&#21516;&#26041;&#35328;&#30340;&#35782;&#21035;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21151;&#33021;&#30340;&#32593;&#32476;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#25289;&#20271;&#35821;&#26159;&#19968;&#31181;&#22797;&#26434;&#30340;&#35821;&#35328;&#65292;&#20840;&#29699;&#26377;&#36229;&#36807;4.5&#20159;&#20154;&#21475;&#20351;&#29992;&#35768;&#22810;&#19981;&#21516;&#30340;&#26041;&#35328;&#21644;&#21475;&#38899;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#22810;&#26679;&#24615;&#21644;&#21464;&#21270;&#65292;&#20026;&#38463;&#25289;&#20271;&#35821;&#26500;&#24314;&#19968;&#20010;&#31283;&#20581;&#19988;&#36890;&#29992;&#30340;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#21644;&#28436;&#31034;&#19968;&#20010;&#21517;&#20026;VoxArabica&#30340;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#29992;&#20110;&#26041;&#35328;&#35782;&#21035;(DID)&#21644;&#38463;&#25289;&#20271;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#12290;&#25105;&#20204;&#22312;&#30417;&#30563;&#29615;&#22659;&#19979;&#35757;&#32451;&#20102;&#21508;&#31181;&#27169;&#22411;&#65292;&#20363;&#22914;HuBERT(DID)&#12289;Whisper&#21644;XLS-R(ASR)&#65292;&#29992;&#20110;&#38463;&#25289;&#20271;&#35821;&#30340;DID&#21644;ASR&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;DID&#27169;&#22411;&#34987;&#35757;&#32451;&#29992;&#20110;&#35782;&#21035;&#38500;&#20102;&#26631;&#20934;&#38463;&#25289;&#20271;&#20043;&#22806;&#30340;17&#31181;&#19981;&#21516;&#30340;&#26041;&#35328;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;(MSA)&#12289;&#22467;&#21450;&#35821;&#12289;&#25705;&#27931;&#21733;&#35821;&#21644;&#28151;&#21512;&#25968;&#25454;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;ASR&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;ASR&#20013;&#30340;&#20854;&#20182;&#26041;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;Whisper&#21644;MMS&#31561;&#19981;&#21516;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#38598;&#25104;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#32593;&#32476;&#30028;&#38754;&#20013;&#65292;&#20855;&#26377;&#22810;&#26679;&#30340;&#21151;&#33021;&#65292;&#22914;&#38899;&#39057;&#24405;&#21046;&#12289;&#19978;&#20256;&#25991;&#20214;&#12289;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#20986;&#38382;&#39064;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arabic is a complex language with many varieties and dialects spoken by over 450 millions all around the world. Due to the linguistic diversity and variations, it is challenging to build a robust and generalized ASR system for Arabic. In this work, we address this gap by developing and demoing a system, dubbed VoxArabica, for dialect identification (DID) as well as automatic speech recognition (ASR) of Arabic. We train a wide range of models such as HuBERT (DID), Whisper, and XLS-R (ASR) in a supervised setting for Arabic DID and ASR tasks. Our DID models are trained to identify 17 different dialects in addition to MSA. We finetune our ASR models on MSA, Egyptian, Moroccan, and mixed data. Additionally, for the remaining dialects in ASR, we provide the option to choose various models such as Whisper and MMS in a zero-shot setting. We integrate these models into a single web interface with diverse features such as audio recording, file upload, model selection, and the option to raise fl
&lt;/p&gt;</description></item><item><title>ChatGPT&#22312;&#31532;&#20108;&#35821;&#35328;&#20889;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#35821;&#35328;&#23398;&#20064;&#32773;&#21512;&#20316;&#65292;&#22312;&#20445;&#25345;&#20010;&#20154;&#34920;&#36798;&#22768;&#38899;&#30340;&#21516;&#26102;&#25552;&#21319;&#20889;&#20316;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;ChatGPT&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.10903</link><description>&lt;p&gt;
&#26032;&#20852;&#30340;AI&#36741;&#21161;&#35805;&#35821;&#65306;ChatGPT&#19982;&#31532;&#20108;&#35821;&#35328;&#20889;&#20316;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Emergent AI-Assisted Discourse: Case Study of a Second Language Writer Authoring with ChatGPT. (arXiv:2310.10903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10903
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22312;&#31532;&#20108;&#35821;&#35328;&#20889;&#20316;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#19982;&#35821;&#35328;&#23398;&#20064;&#32773;&#21512;&#20316;&#65292;&#22312;&#20445;&#25345;&#20010;&#20154;&#34920;&#36798;&#22768;&#38899;&#30340;&#21516;&#26102;&#25552;&#21319;&#20889;&#20316;&#33021;&#21147;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;ChatGPT&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24555;&#36895;&#26222;&#21450;&#24341;&#21457;&#20102;&#20851;&#20110;&#20854;&#23545;&#20154;&#31867;&#20889;&#20316;&#30340;&#24433;&#21709;&#30340;&#20105;&#35758;&#12290;&#22312;&#23545;&#20889;&#20316;&#26631;&#20934;&#19979;&#38477;&#30340;&#25285;&#24551;&#20013;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#22312;&#23398;&#26415;&#20889;&#20316;&#20013;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#35821;&#35328;&#23398;&#20064;&#32773;&#20013;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#26696;&#20363;&#30740;&#31350;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#21338;&#22763;&#29983;&#20975;&#28789;&#65288;Kailing&#65289;&#22312;&#25972;&#20010;&#23398;&#26415;&#20889;&#20316;&#36807;&#31243;&#20013;&#22914;&#20309;&#20351;&#29992;ChatGPT&#30340;&#32463;&#39564;&#12290;&#30740;&#31350;&#37319;&#29992;&#27963;&#21160;&#29702;&#35770;&#20316;&#20026;&#29702;&#35770;&#26694;&#26550;&#26469;&#29702;&#35299;&#21033;&#29992;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#36827;&#34892;&#20889;&#20316;&#30340;&#36807;&#31243;&#65292;&#30740;&#31350;&#25968;&#25454;&#21253;&#25324;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12289;&#20889;&#20316;&#26679;&#26412;&#21644;GPT&#26085;&#24535;&#30340;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20975;&#28789;&#22312;&#21508;&#20010;&#20889;&#20316;&#38454;&#27573;&#26377;&#25928;&#22320;&#19982;ChatGPT&#21512;&#20316;&#65292;&#21516;&#26102;&#20445;&#30041;&#33258;&#24049;&#29420;&#29305;&#30340;&#20316;&#32773;&#22768;&#38899;&#21644;&#20027;&#20307;&#24615;&#12290;&#36825;&#31361;&#26174;&#20102;ChatGPT&#31561;AI&#24037;&#20855;&#22312;&#22686;&#24378;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#23398;&#26415;&#20889;&#20316;&#33021;&#21147;&#26102;&#19981;&#20250;&#25513;&#30422;&#20010;&#20154;&#30340;&#30495;&#23454;&#24615;&#12290;&#26412;&#26696;&#20363;&#30740;&#31350;&#23545;ChatGPT&#22312;&#23398;&#26415;&#30028;&#20013;&#30340;&#20351;&#29992;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of ChatGPT has incited debates regarding its impact on human writing. Amid concerns about declining writing standards, this study investigates the role of ChatGPT in facilitating academic writing, especially among language learners. Using a case study approach, this study examines the experiences of Kailing, a doctoral student, who integrates ChatGPT throughout their academic writing process. The study employs activity theory as a lens for understanding writing with generative AI tools and data analyzed includes semi-structured interviews, writing samples, and GPT logs. Results indicate that Kailing effectively collaborates with ChatGPT across various writing stages while preserving her distinct authorial voice and agency. This underscores the potential of AI tools such as ChatGPT to enhance academic writing for language learners without overshadowing individual authenticity. This case study offers a critical exploration of how ChatGPT is utilized in the academi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10765</link><description>&lt;p&gt;
BiomedJourney: &#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#20013;&#30340;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys. (arXiv:2310.10765v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#20351;&#29992;GPT-4&#22788;&#29702;&#22270;&#20687;&#25253;&#21578;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#35757;&#32451;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22270;&#20687;&#32534;&#36753;&#30340;&#25351;&#23548;&#23398;&#20064;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#22914;InstructPix2Pix&#65292;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21487;&#20197;&#23558;&#36825;&#20123;&#26041;&#27861;&#24212;&#29992;&#20110;&#21453;&#20107;&#23454;&#22270;&#20687;&#29983;&#25104;&#65292;&#20174;&#32780;&#24110;&#21161;&#21306;&#20998;&#22240;&#26524;&#32467;&#26500;&#21644;&#20266;&#30456;&#20851;&#65292;&#24182;&#20419;&#36827;&#30142;&#30149;&#36827;&#23637;&#24314;&#27169;&#30340;&#31283;&#20581;&#22270;&#20687;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#36890;&#29992;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#24182;&#19981;&#36866;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#30340;&#30740;&#31350;&#36824;&#36828;&#26410;&#28145;&#20837;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;BiomedJourney&#65292;&#36890;&#36807;&#25351;&#23548;&#23398;&#20064;&#22810;&#27169;&#24577;&#24739;&#32773;&#26053;&#31243;&#65292;&#36827;&#34892;&#21453;&#20107;&#23454;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#29983;&#25104;&#12290;&#32473;&#23450;&#19968;&#20010;&#25293;&#25668;&#20110;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#20004;&#20010;&#29983;&#29289;&#21307;&#23398;&#22270;&#20687;&#30340;&#24739;&#32773;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#22788;&#29702;&#30456;&#24212;&#30340;&#22270;&#20687;&#25253;&#21578;&#65292;&#24182;&#29983;&#25104;&#30142;&#30149;&#36827;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#19977;&#20803;&#32452;&#65288;&#20808;&#21069;&#22270;&#20687;&#12289;&#36827;&#23637;&#25551;&#36848;&#12289;&#26032;&#22270;&#20687;&#65289;&#26469;&#35757;&#32451;&#19968;&#20010;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapid progress has been made in instruction-learning for image editing with natural-language instruction, as exemplified by InstructPix2Pix. In biomedicine, such methods can be applied to counterfactual image generation, which helps differentiate causal structure from spurious correlation and facilitate robust image interpretation for disease progression modeling. However, generic image-editing models are ill-suited for the biomedical domain, and counterfactual biomedical image generation is largely underexplored. In this paper, we present BiomedJourney, a novel method for counterfactual biomedical image generation by instruction-learning from multimodal patient journeys. Given a patient with two biomedical images taken at different time points, we use GPT-4 to process the corresponding imaging reports and generate a natural language description of disease progression. The resulting triples (prior image, progression description, new image) are then used to train a latent diffusion mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10701</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind for Multi-Agent Collaboration via Large Language Models. (arXiv:2310.10701v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#28216;&#25103;&#20013;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#65292;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#34920;&#29616;&#20986;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21644;&#35268;&#21010;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20294;&#23427;&#22312;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#35268;&#21010;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#25991;&#26412;&#28216;&#25103;&#20013;&#35780;&#20272;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#29702;&#35770;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#20986;&#29616;&#20102;&#21327;&#20316;&#34892;&#20026;&#21644;&#39640;&#32423;&#29702;&#35770;&#25512;&#29702;&#33021;&#21147;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#33021;&#20307;&#22312;&#38271;&#26399;&#35268;&#21010;&#19978;&#23384;&#22312;&#20248;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#29366;&#24577;&#30340;&#38169;&#35823;&#35748;&#30693;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#26126;&#30830;&#30340;&#20449;&#24565;&#29366;&#24577;&#34920;&#31034;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#23427;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#29702;&#35770;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated impressive accomplishments in both reasoning and planning, their abilities in multi-agent collaborations remains largely unexplored. This study evaluates LLM-based agents in a multi-agent cooperative text game with Theory of Mind (ToM) inference tasks, comparing their performance with Multi-Agent Reinforcement Learning (MARL) and planning-based baselines. We observed evidence of emergent collaborative behaviors and high-order Theory of Mind capabilities among LLM-based agents. Our results reveal limitations in LLM-based agents' planning optimization due to systematic failures in managing long-horizon contexts and hallucination about the task state. We explore the use of explicit belief state representations to mitigate these issues, finding that it enhances task performance and the accuracy of ToM inferences for LLM-based agents.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35821;&#20041;&#24605;&#32500;&#38142;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20197;&#27492;&#26469;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.10698</link><description>&lt;p&gt;
&#23558;&#20195;&#30721;&#35821;&#20041;&#19982;LLMs&#30456;&#32467;&#21512;&#65306;&#20195;&#30721;&#29983;&#25104;&#30340;&#35821;&#20041;&#24605;&#32500;&#38142;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Bridging Code Semantic and LLMs: Semantic Chain-of-Thought Prompting for Code Generation. (arXiv:2310.10698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35821;&#20041;&#24605;&#32500;&#38142;&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#24341;&#20837;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#20197;&#27492;&#26469;&#23454;&#29616;&#26356;&#31934;&#32454;&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#21270;&#20195;&#30721;&#29983;&#25104;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#23545;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#21644;&#20195;&#30721;&#20043;&#38388;&#36827;&#34892;&#39640;&#32423;&#35821;&#20041;&#26144;&#23556;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#22522;&#20110;LLMs&#30340;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#20381;&#36182;&#20110;&#20165;&#20351;&#29992;&#35299;&#30721;&#22120;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#24120;&#23558;&#20195;&#30721;&#20165;&#35270;&#20026;&#32431;&#25991;&#26412;&#26631;&#35760;&#65292;&#21363;&#23558;&#38656;&#27714;&#20316;&#20026;&#25552;&#31034;&#36755;&#20837;&#65292;&#24182;&#23558;&#20195;&#30721;&#20316;&#20026;&#19968;&#31995;&#21015;&#24179;&#38754;&#26631;&#35760;&#36755;&#20986;&#65292;&#21487;&#33021;&#20250;&#24573;&#30053;&#28304;&#20195;&#30721;&#20013;&#22266;&#26377;&#30340;&#20016;&#23500;&#35821;&#20041;&#29305;&#24449;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#35821;&#20041;&#24605;&#32500;&#38142;&#8221;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#31216;&#20026;SeCoT&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#28304;&#20195;&#30721;&#30340;&#35821;&#20041;&#20449;&#24687;&#65288;&#20363;&#22914;&#25968;&#25454;&#27969;&#21644;&#25511;&#21046;&#27969;&#65289;&#26356;&#31934;&#30830;&#22320;&#25551;&#36848;&#20102;&#31243;&#24207;&#25191;&#34892;&#34892;&#20026;&#12289;&#24847;&#22270;&#21644;&#21151;&#33021;&#12290;&#36890;&#36807;&#24341;&#23548;LLMs&#32771;&#34385;&#21644;&#25972;&#21512;&#35821;&#20041;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#23545;&#20195;&#30721;&#26356;&#31934;&#32454;&#30340;&#29702;&#35299;&#21644;&#34920;&#31034;&#65292;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have showcased remarkable prowess in code generation. However, automated code generation is still challenging since it requires a high-level semantic mapping between natural language requirements and codes. Most existing LLMs-based approaches for code generation rely on decoder-only causal language models often treate codes merely as plain text tokens \ie feeding the requirements as a prompt input, and outputing code as flat sequence of tokens, potentially missing the rich semantic features inherent in source code. To bridge this gap, this paper proposes the "Semantic Chain-of-Thought" approach to intruduce semantic information of code, named SeCoT. Our motivation is that the semantic information of the source code (\eg data flow and control flow) describes more precise program execution behavior, intention and function. By guiding LLM consider and integrate semantic information, we can achieve a more granular understanding and representation of code, enhan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.10638</link><description>&lt;p&gt;
&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
In-Context Pretraining: Language Modeling Beyond Document Boundaries. (arXiv:2310.10638v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#25991;&#26723;&#36793;&#30028;&#30340;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#36328;&#25991;&#26723;&#30340;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#36890;&#36807;&#39044;&#27979;&#32473;&#23450;&#25991;&#26723;&#21069;&#32512;&#30340;&#26631;&#35760;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#33021;&#22815;&#30452;&#25509;&#36827;&#34892;&#38271;&#31687;&#29983;&#25104;&#21644;&#25552;&#31034;&#24335;&#20219;&#21153;&#65292;&#36825;&#21487;&#20197;&#31616;&#21270;&#20026;&#25991;&#26723;&#23436;&#25104;&#12290;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#36890;&#36807;&#36830;&#25509;&#38543;&#26426;&#32452;&#21512;&#30340;&#30701;&#25991;&#26723;&#26469;&#35757;&#32451;LMs&#65292;&#20197;&#21019;&#24314;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#20294;&#21069;&#19968;&#20010;&#25991;&#26723;&#23545;&#20110;&#39044;&#27979;&#19979;&#19968;&#20010;&#25991;&#26723;&#27809;&#26377;&#25552;&#20379;&#20219;&#20309;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#65292;&#21363;&#22312;&#30456;&#20851;&#25991;&#26723;&#24207;&#21015;&#19978;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#26126;&#30830;&#40723;&#21169;&#23427;&#20204;&#36328;&#36234;&#25991;&#26723;&#36793;&#30028;&#36827;&#34892;&#38405;&#35835;&#21644;&#25512;&#29702;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#25913;&#21464;&#25991;&#26723;&#39034;&#24207;&#65292;&#20351;&#27599;&#20010;&#19978;&#19979;&#25991;&#21253;&#21547;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#24182;&#30452;&#25509;&#24212;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#31649;&#36947;&#26469;&#36827;&#34892;&#19978;&#19979;&#25991;&#39044;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25991;&#26723;&#25490;&#24207;&#38382;&#39064;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26377;&#25968;&#21313;&#20159;&#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#24076;&#26395;&#22312;&#27599;&#20010;&#25991;&#26723;&#20013;&#26368;&#22823;&#21270;&#19978;&#19979;&#25991;&#30456;&#20284;&#24615;&#32780;&#19981;&#37325;&#22797;&#20219;&#20309;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) are currently trained to predict tokens given document prefixes, enabling them to directly perform long-form generation and prompting-style tasks which can be reduced to document completion. Existing pretraining pipelines train LMs by concatenating random sets of short documents to create input contexts but the prior documents provide no signal for predicting the next document. We instead present In-Context Pretraining, a new approach where language models are pretrained on a sequence of related documents, thereby explicitly encouraging them to read and reason across document boundaries. We can do In-Context Pretraining by simply changing the document ordering so that each context contains related documents, and directly applying existing pretraining pipelines. However, this document sorting problem is challenging. There are billions of documents and we would like the sort to maximize contextual similarity for every document without repeating any data. To do
&lt;/p&gt;</description></item><item><title>RegaVAE&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#25991;&#26412;&#32534;&#30721;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#33719;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;</title><link>http://arxiv.org/abs/2310.10567</link><description>&lt;p&gt;
RegaVAE&#65306;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
RegaVAE: A Retrieval-Augmented Gaussian Mixture Variational Auto-Encoder for Language Modeling. (arXiv:2310.10567v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10567
&lt;/p&gt;
&lt;p&gt;
RegaVAE&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#26816;&#32034;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#25991;&#26412;&#32534;&#30721;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#25429;&#33719;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#26377;&#25928;&#30340;&#20449;&#24687;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36807;&#26102;&#20449;&#24687;&#21644;&#34394;&#26500;&#29616;&#35937;&#31561;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#65306;1&#65289;&#30830;&#23450;&#35201;&#26816;&#32034;&#30340;&#20449;&#24687;&#65292;2&#65289;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#32467;&#21512;&#26816;&#32034;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26377;&#20215;&#20540;&#30340;&#26816;&#32034;&#20449;&#24687;&#19981;&#20165;&#24212;&#19982;&#24403;&#21069;&#30340;&#28304;&#25991;&#26412;&#30456;&#20851;&#65292;&#36824;&#24212;&#32771;&#34385;&#21040;&#26410;&#26469;&#30340;&#30446;&#26631;&#25991;&#26412;&#65292;&#22240;&#20026;&#35821;&#35328;&#27169;&#22411;&#26159;&#23545;&#26410;&#26469;&#20196;&#29260;&#36827;&#34892;&#24314;&#27169;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#65292;&#20351;&#29992;&#20174;&#32039;&#20945;&#28508;&#22312;&#31354;&#38388;&#20013;&#27966;&#29983;&#30340;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#32858;&#21512;&#27604;&#20351;&#29992;&#21463;&#19978;&#19979;&#25991;&#38271;&#24230;&#38480;&#21046;&#21644;&#23481;&#26131;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#26174;&#24335;&#21407;&#22987;&#25991;&#26412;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;RegaVAE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26500;&#24314;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#23427;&#23558;&#25991;&#26412;&#35821;&#26009;&#24211;&#32534;&#30721;&#20026;&#28508;&#22312;&#31354;&#38388;&#65292;&#20174;&#28304;&#25991;&#26412;&#21644;&#30446;&#26631;&#25991;&#26412;&#20013;&#25429;&#33719;&#24403;&#21069;&#21644;&#26410;&#26469;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;VAE&#21021;&#22987;&#21270;&#27169;&#22411;&#21442;&#25968;&#20197;&#25552;&#39640;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval-augmented language models show promise in addressing issues like outdated information and hallucinations in language models (LMs). However, current research faces two main problems: 1) determining what information to retrieve, and 2) effectively combining retrieved information during generation. We argue that valuable retrieved information should not only be related to the current source text but also consider the future target text, given the nature of LMs that model future tokens. Moreover, we propose that aggregation using latent variables derived from a compact latent space is more efficient than utilizing explicit raw text, which is limited by context length and susceptible to noise. Therefore, we introduce RegaVAE, a retrieval-augmented language model built upon the variational auto-encoder (VAE). It encodes the text corpus into a latent space, capturing current and future information from both source and target text. Additionally, we leverage the VAE to initialize the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.10195</link><description>&lt;p&gt;
AdaLomo: &#20302;&#20869;&#23384;&#20248;&#21270;&#19982;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;
&lt;/p&gt;
&lt;p&gt;
AdaLomo: Low-memory Optimization with Adaptive Learning Rate. (arXiv:2310.10195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10195
&lt;/p&gt;
&lt;p&gt;
AdaLomo&#26159;&#19968;&#31181;&#20302;&#20869;&#23384;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#35268;&#27169;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#20869;&#23384;&#65292;&#20174;&#32780;&#35774;&#32622;&#20102;&#24456;&#39640;&#30340;&#38376;&#27099;&#12290;&#23613;&#31649;&#26368;&#36817;&#25552;&#20986;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;LOMO&#65289;&#20943;&#23569;&#20102;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#20854;&#20248;&#21270;&#25216;&#26415;&#31867;&#20284;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;&#23545;&#36229;&#21442;&#25968;&#25935;&#24863;&#24182;&#23637;&#29616;&#20986;&#27425;&#20248;&#30340;&#25910;&#25947;&#24615;&#65292;&#26080;&#27861;&#19982;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#21270;&#22120;AdamW&#30340;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#36890;&#36807;&#23545;Adam&#20248;&#21270;&#22120;&#36827;&#34892;&#32463;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#21160;&#37327;&#26469;&#35828;&#65292;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#23545;&#20110;&#24357;&#21512;&#24046;&#36317;&#26356;&#20026;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#30340;&#20302;&#20869;&#23384;&#20248;&#21270;&#65288;AdaLomo&#65289;&#65292;&#20026;&#27599;&#20010;&#21442;&#25968;&#25552;&#20379;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#12290;&#20026;&#20102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#20248;&#21270;&#22120;&#29366;&#24577;&#20013;&#37319;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26469;&#20272;&#35745;&#20108;&#38454;&#30697;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20998;&#32452;&#26356;&#26032;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update norma
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2310.09881</link><description>&lt;p&gt;
&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning with Iterative Demonstration Selection. (arXiv:2310.09881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#26469;&#36873;&#25321;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#35268;&#27169;&#30340;&#36827;&#23637;&#30340;&#25512;&#21160;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;ICL&#30340;&#24615;&#33021;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#31034;&#33539;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#31034;&#33539;&#20316;&#20026;&#19978;&#19979;&#25991;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#25361;&#25112;&#21644;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#25991;&#29486;&#24050;&#32463;&#24378;&#35843;&#20102;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#31034;&#33539;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#24573;&#35270;&#20102;&#26368;&#20248;&#31034;&#33539;&#36873;&#25321;&#32500;&#24230;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#20107;&#23454;&#12290;&#20511;&#37492;&#20004;&#20010;&#32500;&#24230;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36845;&#20195;&#31034;&#33539;&#36873;&#25321;(IDS)&#12290;&#20351;&#29992;&#38646;&#26679;&#26412;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;(Zero-shot-CoT)&#65292;IDS&#36845;&#20195;&#22320;&#36873;&#25321;&#37027;&#20123;&#19982;&#27979;&#35797;&#26679;&#26412;&#19981;&#21516;&#20294;&#20173;&#19982;&#20043;&#24378;&#30456;&#20851;&#30340;&#31034;&#33539;&#20316;&#20026;ICL&#30340;&#31034;&#33539;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IDS&#22312;&#31034;&#33539;&#36873;&#25321;&#20043;&#21069;&#23558;Zero-shot-CoT&#24212;&#29992;&#20110;&#27979;&#35797;&#26679;&#26412;&#12290;&#36755;&#20986;&#30340;&#25512;&#29702;&#36335;&#24452;&#26159;...
&lt;/p&gt;
&lt;p&gt;
Spurred by advancements in scale, large language models (LLMs) have demonstrated strong few-shot learning ability via in-context learning (ICL). However, the performance of ICL has been shown to be highly sensitive to the selection of few-shot demonstrations. Selecting the most suitable examples as context remains an ongoing challenge and an open problem. Existing literature has highlighted the importance of selecting examples that are diverse or semantically similar to the test sample while ignoring the fact that the optimal selection dimension, i.e., diversity or similarity, is task-specific. Leveraging the merits of both dimensions, we propose Iterative Demonstration Selection (IDS). Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), IDS iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations. Specifically, IDS applies Zero-shot-CoT to the test sample before demonstration selection. The output reasoning path is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21512;&#24182;&#19987;&#23478;&#8221;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#19987;&#23478;&#30340;&#27700;&#24179;&#26469;&#25913;&#36827;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09832</link><description>&lt;p&gt;
&#21512;&#24182;&#19987;&#23478;&#65306;&#25913;&#36827;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Merging Experts into One: Improving Computational Efficiency of Mixture of Experts. (arXiv:2310.09832v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21512;&#24182;&#19987;&#23478;&#8221;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#19987;&#23478;&#30340;&#27700;&#24179;&#26469;&#25913;&#36827;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#25193;&#22823;&#36890;&#24120;&#20250;&#24102;&#26469;NLP&#20219;&#21153;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#20250;&#20276;&#38543;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#31232;&#30095;&#30340;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21487;&#20197;&#36890;&#36807;&#28608;&#27963;&#27599;&#20010;&#36755;&#20837;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#21442;&#25968;&#65288;&#20363;&#22914;&#19968;&#20010;&#19987;&#23478;&#65289;&#26469;&#20943;&#23569;&#25104;&#26412;&#65292;&#20294;&#22914;&#26524;&#22686;&#21152;&#28608;&#27963;&#30340;&#19987;&#23478;&#25968;&#37327;&#65292;&#20854;&#35745;&#31639;&#23558;&#26174;&#33879;&#22686;&#21152;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36873;&#25321;&#22810;&#20010;&#19987;&#23478;&#30340;&#20248;&#36234;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21512;&#24182;&#19987;&#23478;&#8221;&#65288;MEO&#65289;&#65292;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#19987;&#23478;&#30340;&#27700;&#24179;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MEO&#26174;&#30528;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20363;&#22914;&#65292;FLOPS&#20174;&#26222;&#36890;MoE&#30340;72.0G&#38477;&#20302;&#21040;28.6G&#65288;MEO&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \textbf{\texttt{Merging Experts into One}} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the ef
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2310.09343</link><description>&lt;p&gt;
&#23545;&#24120;&#35782;&#24863;&#30693;&#23545;&#35805;&#20195;&#29702;&#30340;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dialogue Chain-of-Thought Distillation for Commonsense-aware Conversational Agents. (arXiv:2310.09343v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#35805;&#24605;&#36335;&#25552;&#28860;&#30340;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#25945;&#24072;&#65292;&#24182;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20154;&#31867;&#21270;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#20351;&#29992;&#24120;&#35782;&#25512;&#29702;&#26469;&#26377;&#25928;&#22320;&#29702;&#35299;&#21644;&#22238;&#24212;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#26679;&#30340;&#36830;&#36143;&#24615;&#21644;&#20449;&#24687;&#21547;&#37327;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22312;&#19968;&#20010;&#21333;&#36339;&#20869;&#35782;&#21035;&#21644;&#32858;&#21512;&#20851;&#38190;&#35777;&#25454;&#30340;&#20219;&#21153;&#20063;&#26159;&#20855;&#26377;&#30456;&#24403;&#22823;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#20010;&#22797;&#26434;&#24615;&#30340;&#21407;&#22240;&#26159;&#36825;&#26679;&#30340;&#35777;&#25454;&#20998;&#25955;&#22312;&#23545;&#35805;&#30340;&#22810;&#20010;&#36718;&#27425;&#20013;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#22810;&#20010;&#36339;&#20013;&#36827;&#34892;&#25972;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#28966;&#28857;&#26159;&#20419;&#36827;&#23545;&#35805;&#19978;&#30340;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23545;&#35805;&#24605;&#36335;&#65288;CoT&#65289;&#25512;&#29702;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#25552;&#28860;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#40784;&#36807;&#28388;&#22120;&#36873;&#25321;&#24615;&#22320;&#25552;&#28860;&#19968;&#33268;&#21644;&#26377;&#29992;&#30340;&#29702;&#30001;&#65292;&#21033;&#29992;LLMs&#20316;&#20026;&#19981;&#21487;&#38752;&#30340;&#25945;&#24072;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;DOCTOR&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#30340;CoT&#29702;&#30001;&#20197;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#30340;&#23545;&#35805;&#24605;&#36335;&#25512;&#29702;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-like chatbots necessitate the use of commonsense reasoning in order to effectively comprehend and respond to implicit information present within conversations. Achieving such coherence and informativeness in responses, however, is a non-trivial task. Even for large language models (LLMs), the task of identifying and aggregating key evidence within a single hop presents a substantial challenge. This complexity arises because such evidence is scattered across multiple turns in a conversation, thus necessitating integration over multiple hops. Hence, our focus is to facilitate such multi-hop reasoning over a dialogue context, namely dialogue chain-of-thought (CoT) reasoning. To this end, we propose a knowledge distillation framework that leverages LLMs as unreliable teachers and selectively distills consistent and helpful rationales via alignment filters. We further present DOCTOR, a DialOgue Chain-of-ThOught Reasoner that provides reliable CoT rationales for response generation. We
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#36394;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#23398;&#20064;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#65292;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08740</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#26426;&#25511;&#21046;&#30340;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#21450;&#20854;&#32467;&#26500;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
A Zero-Shot Language Agent for Computer Control with Structured Reflection. (arXiv:2310.08740v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08740
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#35821;&#35328;&#20195;&#29702;&#26426;&#21046;&#65292;&#23427;&#19981;&#38656;&#35201;&#19987;&#23478;&#31034;&#36394;&#65292;&#24182;&#19988;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#23398;&#20064;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#65292;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35745;&#21010;&#21644;&#25191;&#34892;&#39640;&#32423;&#30446;&#26631;&#26041;&#38754;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#22914;&#22312;&#27963;&#21160;&#30340;&#35745;&#31639;&#26426;&#29615;&#22659;&#65288;&#20363;&#22914;MiniWoB ++&#65289;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#24120;&#35201;&#27714;&#27169;&#22411;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#25110;&#23569;/&#22810;&#26679;&#26412;&#25552;&#31034;&#20174;&#20219;&#21153;&#30340;&#36319;&#36394;&#31034;&#20363;&#20013;&#23398;&#20064;&#12290;&#22312;&#27809;&#26377;&#36825;&#20123;&#36319;&#36394;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20195;&#29702;&#26426;&#21046;&#22914;&#20309;&#33021;&#22815;&#33258;&#20027;&#23398;&#20064;&#24182;&#25913;&#21892;&#22312;&#35745;&#31639;&#26426;&#19978;&#30340;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#19968;&#20010;&#20195;&#29702;&#26426;&#26500;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#20195;&#29702;&#26426;&#21046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23427;&#19981;&#38656;&#35201;&#32473;&#23450;&#30340;&#19987;&#23478;&#31034;&#36394;&#12290;&#25105;&#20204;&#30340;&#20195;&#29702;&#26426;&#21046;&#23545;&#20110;&#37096;&#20998;&#35266;&#23519;&#29615;&#22659;&#19978;&#30340;&#21487;&#25191;&#34892;&#34892;&#21160;&#36827;&#34892;&#35268;&#21010;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#21453;&#24605;&#21644;&#32467;&#26500;&#21270;&#24605;&#32771;&#31649;&#29702;&#26469;&#35782;&#21035;&#21644;&#23398;&#20064;&#38169;&#35823;&#65292;&#20174;&#32780;&#36880;&#27493;&#25512;&#36827;&#20219;&#21153;&#12290;&#22312;MiniWoB ++&#30340;&#31616;&#21333;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#20195;&#29702;&#26426;&#21046;&#24448;&#24448;&#32988;&#36807;&#26368;&#36817;&#30340;SoTA&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23545;&#20110;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#21453;&#24605;&#20195;&#29702;&#26426;&#21046;&#19982;&#20808;&#21069;&#30340;&#20195;&#29702;&#26426;&#21046;&#25345;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing capacity at planning and executing a high-level goal in a live computer environment (e.g. MiniWoB++). To perform a task, recent works often require a model to learn from trace examples of the task via either supervised learning or few/many-shot prompting. Without these trace examples, it remains a challenge how an agent can autonomously learn and improve its control on a computer, which limits the ability of an agent to perform a new task. We approach this problem with a zero-shot agent that requires no given expert traces. Our agent plans for executable actions on a partially observed environment, and iteratively progresses a task by identifying and learning from its mistakes via self-reflection and structured thought management. On the easy tasks of MiniWoB++, we show that our zero-shot agent often outperforms recent SoTAs, with more efficient reasoning. For tasks with more complexity, our reflective agent performs on par with prior 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.08659</link><description>&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models. (arXiv:2310.08659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LoftQ&#65306;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;LoRA&#31934;&#35843;&#24863;&#30693;&#37327;&#21270;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#20026;LoRA&#31934;&#35843;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#65292;&#20197;&#32531;&#35299;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#26381;&#21153;&#30340;&#19981;&#21487;&#25110;&#32570;&#30340;&#25216;&#26415;&#65292;&#24182;&#26368;&#36817;&#34987;&#24212;&#29992;&#20110;LoRA&#31934;&#35843;&#20013;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#19978;&#21516;&#26102;&#24212;&#29992;&#37327;&#21270;&#21644;LoRA&#31934;&#35843;&#30340;&#22330;&#26223;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24120;&#24120;&#35266;&#23519;&#21040;&#23436;&#25972;&#31934;&#35843;&#21644;&#37327;&#21270;&#21152;LoRA&#31934;&#35843;&#26041;&#27861;&#20043;&#38388;&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#19978;&#23384;&#22312;&#19968;&#33268;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LoftQ&#65288;LoRA-Fine-Tuning-aware Quantization&#65289;&#8212;&#8212;&#19968;&#31181;&#26032;&#30340;&#37327;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#21516;&#26102;&#23545;LLM&#36827;&#34892;&#37327;&#21270;&#65292;&#24182;&#25214;&#21040;&#36866;&#24403;&#30340;&#20302;&#31209;&#21021;&#22987;&#21270;&#26469;&#36827;&#34892;LoRA&#31934;&#35843;&#12290;&#36825;&#31181;&#21021;&#22987;&#21270;&#20943;&#36731;&#20102;&#37327;&#21270;&#27169;&#22411;&#21644;&#20840;&#31934;&#24230;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12289;&#38382;&#31572;&#12289;&#25688;&#35201;&#21644;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#65292;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning. In this work we focus on the scenario where quantization and LoRA fine-tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrepancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural language understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and outperforms exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.08395</link><description>&lt;p&gt;
&#20351;&#29992;&#24605;&#36335;&#38142;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models with Chain-of-Thought for Few-Shot Knowledge Base Question Generation. (arXiv:2310.08395v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#30693;&#35782;&#24211;&#38382;&#39064;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#38382;&#39064;&#29983;&#25104;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#24615;&#36923;&#36753;&#24418;&#24335;&#21644;&#32534;&#20889;&#25552;&#31034;&#26469;&#23454;&#29616;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#29983;&#25104;&#65288;KBQG&#65289;&#30340;&#20219;&#21153;&#26159;&#23558;&#36923;&#36753;&#24418;&#24335;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#30001;&#20110;&#22823;&#35268;&#27169;&#38382;&#39064;&#27880;&#37322;&#30340;&#26114;&#36149;&#25104;&#26412;&#65292;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#24613;&#38656;&#24320;&#21457;KBQG&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#20013;&#36807;&#20110;&#20381;&#36182;&#27880;&#37322;&#25968;&#25454;&#30340;&#24494;&#35843;&#65292;&#36825;&#23545;&#20110;&#23569;&#26679;&#26412;&#38382;&#39064;&#29983;&#25104;&#24182;&#19981;&#21512;&#36866;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#30340;&#21360;&#35937;&#21147;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#24605;&#36335;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25512;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#31574;&#30053;&#65292;&#25105;&#20204;&#23558;KBQG&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#25512;&#29702;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20010;&#23436;&#25972;&#38382;&#39064;&#30340;&#29983;&#25104;&#34987;&#20998;&#20026;&#19968;&#31995;&#21015;&#30340;&#23376;&#38382;&#39064;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26041;&#27861;KQG-CoT&#39318;&#20808;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#27744;&#20013;&#26816;&#32034;&#25903;&#25345;&#24615;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#32771;&#34385;&#36923;&#36753;&#24418;&#24335;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32534;&#20889;&#19968;&#20010;&#25552;&#31034;&#26469;&#26126;&#30830;&#25512;&#29702;&#38142;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of Question Generation over Knowledge Bases (KBQG) aims to convert a logical form into a natural language question. For the sake of expensive cost of large-scale question annotation, the methods of KBQG under low-resource scenarios urgently need to be developed. However, current methods heavily rely on annotated data for fine-tuning, which is not well-suited for few-shot question generation. The emergence of Large Language Models (LLMs) has shown their impressive generalization ability in few-shot tasks. Inspired by Chain-of-Thought (CoT) prompting, which is an in-context learning strategy for reasoning, we formulate KBQG task as a reasoning problem, where the generation of a complete question is splitted into a series of sub-question generation. Our proposed prompting method KQG-CoT first retrieves supportive logical forms from the unlabeled data pool taking account of the characteristics of the logical form. Then, we write a prompt to explicit the reasoning chain of generati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.08130</link><description>&lt;p&gt;
&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#23454;&#29616;&#32454;&#31890;&#24230;&#23545;&#35805;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Conversational Decoding via Isotropic and Proximal Search. (arXiv:2310.08130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#24182;&#22312;&#23545;&#35805;&#39046;&#22495;&#30340;&#35780;&#20272;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#37319;&#29992;&#36890;&#29992;&#30340;&#25991;&#26412;&#35299;&#30721;&#26041;&#27861;&#26469;&#36827;&#34892;&#23545;&#35805;&#22238;&#24212;&#29983;&#25104;&#12290;&#34429;&#28982;&#37319;&#29992;&#20102;&#23545;&#35805;&#29305;&#23450;&#30340;&#32534;&#30721;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#29983;&#25104;&#30340;&#22238;&#24212;&#36136;&#37327;&#65292;&#20294;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#21040;wu2023learning&#30340;&#21551;&#21457;&#65292;&#35748;&#20026;&#22909;&#30340;&#23545;&#35805;&#29305;&#24449;&#31354;&#38388;&#24212;&#36981;&#24490;&#23616;&#37096;&#24615;&#21644;&#21508;&#21521;&#21516;&#24615;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#23545;&#35805;&#35299;&#30721;&#26041;&#27861;&#65292;&#31216;&#20026;&#21508;&#21521;&#21516;&#24615;&#21644;&#36817;&#31471;&#25628;&#32034;&#65288;IPS&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#29983;&#25104;&#20449;&#24687;&#38598;&#20013;&#30340;&#35821;&#20041;&#22238;&#24212;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#37327;&#21644;&#21306;&#20998;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23545;&#35805;&#39046;&#22495;&#20013;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose text decoding approaches are usually adopted for dialogue response generation. Although the quality of the generated responses can be improved with dialogue-specific encoding methods, conversational decoding methods are still under-explored. Inspired by \citet{wu2023learning} that a good dialogue feature space should follow the rules of locality and isotropy, we present a fine-grained conversational decoding method, termed \textit{isotropic and proximal search (IPS)}. Our method is designed to generate the semantic-concentrated response, while still maintaining informativeness and discrimination against the context. Experiments show that our approach outperforms existing decoding strategies in the dialogue field across both automatic and human evaluation metrics. More in-depth analyses further confirm the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#33258;&#25105;&#25913;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#19968;&#20010;&#26032;&#30340;&#25490;&#21517;&#25351;&#26631;&#65288;PeRFICS&#65289;&#26469;&#35299;&#20915;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#24320;&#28304;LLMs&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.07611</link><description>&lt;p&gt;
LLM&#30340;&#27665;&#20027;&#21270;&#65306;&#33258;&#25105;&#25913;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#24615;&#33021;&#19982;&#25104;&#26412;&#30340;&#26435;&#34913;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Democratizing LLMs: An Exploration of Cost-Performance Trade-offs in Self-Refined Open-Source Models. (arXiv:2310.07611v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20986;&#33258;&#25105;&#25913;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#21644;&#19968;&#20010;&#26032;&#30340;&#25490;&#21517;&#25351;&#26631;&#65288;PeRFICS&#65289;&#26469;&#35299;&#20915;&#24615;&#33021;&#21644;&#25104;&#26412;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#22823;&#23567;&#30340;&#24320;&#28304;&#27169;&#22411;&#20013;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#24320;&#28304;LLMs&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31169;&#26377;LLMs&#30340;&#20027;&#23548;&#22320;&#20301;&#23548;&#33268;&#20102;&#21463;&#38480;&#30340;&#35775;&#38382;&#21644;&#25552;&#39640;&#30340;&#20449;&#24687;&#38544;&#31169;&#38382;&#39064;&#12290;&#23545;&#20110;&#20449;&#24687;&#25935;&#24863;&#21644;&#39640;&#23481;&#37327;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#65292;&#39640;&#24615;&#33021;&#30340;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#22312;&#24615;&#33021;&#26041;&#38754;&#33853;&#21518;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;(1)&#19968;&#31181;&#26080;&#22806;&#37096;&#24433;&#21709;&#30340;&#38750;&#38024;&#23545;&#24615;&#36845;&#20195;&#33258;&#25105;&#25209;&#35780;&#21644;&#33258;&#25105;&#25913;&#36827;&#30340;&#21464;&#20307;&#12290;(2)&#19968;&#31181;&#26032;&#39062;&#30340;&#25490;&#21517;&#25351;&#26631; - &#24615;&#33021;&#12289;&#25913;&#36827;&#21644;&#25512;&#29702;&#25104;&#26412;&#24471;&#20998;(PeRFICS)&#65292;&#20197;&#32771;&#34385;&#25913;&#36827;&#21518;&#30340;&#24615;&#33021;&#21644;&#25104;&#26412;&#26469;&#25214;&#21040;&#32473;&#23450;&#20219;&#21153;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#26368;&#20808;&#36827;&#24320;&#28304;&#27169;&#22411;&#20174;7B&#21040;65B&#65292;&#24179;&#22343;&#25913;&#21892;&#20102;8.2%&#30340;&#22522;&#20934;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#21363;&#20351;&#26159;&#20869;&#23384;&#21344;&#29992;&#26497;&#23567;&#30340;&#27169;&#22411;&#65292;&#27604;&#22914;Vicuna-7B&#65292;&#22312;&#25972;&#20307;&#19978;&#20063;&#26377;11.74%&#30340;&#25913;&#21892;&#65292;&#24182;&#22312;Vicuna&#22522;&#20934;&#27979;&#35797;&#30340;&#39640;&#21019;&#36896;&#21147;&#21644;&#24320;&#25918;&#24615;&#20219;&#21153;&#20013;&#25552;&#39640;&#20102;25.39%&#12290;Vicuna-13B&#26356;&#36827;&#19968;&#27493;&#65292;&#22312;&#25913;&#36827;&#21518;&#36229;&#36234;&#20102;ChatGPT&#12290;&#36825;&#39033;&#24037;&#20316;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#65292;&#20026;&#24320;&#28304;LLMs&#30340;&#21457;&#23637;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#20248;&#21270;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74% improvement overall and up to a 25.39% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#27604;&#36739;#BlackLivesMatter&#21644;#BlueLivesMatter&#36816;&#21160;&#30456;&#20851;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#22270;&#65292;&#24182;&#26681;&#25454;&#20316;&#32773;&#30340;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#21270;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27169;&#25311;&#36816;&#21160;&#20013;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.07155</link><description>&lt;p&gt;
"&#20004;&#20010;&#36816;&#21160;&#30340;&#25925;&#20107;": &#20351;&#29992;&#24369;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#32467;&#26500;&#21270;&#39044;&#27979;&#26041;&#27861;&#37492;&#21035;&#21644;&#27604;&#36739;#BlackLivesMatter&#21644;#BlueLivesMatter&#36816;&#21160;&#30456;&#20851;&#30340;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
"A Tale of Two Movements": Identifying and Comparing Perspectives in #BlackLivesMatter and #BlueLivesMatter Movements-related Tweets using Weakly Supervised Graph-based Structured Prediction. (arXiv:2310.07155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#27604;&#36739;#BlackLivesMatter&#21644;#BlueLivesMatter&#36816;&#21160;&#30456;&#20851;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;&#12290;&#36890;&#36807;&#23558;&#25991;&#26412;&#36716;&#25442;&#20026;&#22270;&#65292;&#24182;&#26681;&#25454;&#20316;&#32773;&#30340;&#31038;&#20132;&#32593;&#32476;&#36827;&#34892;&#32467;&#26500;&#21270;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27169;&#25311;&#36816;&#21160;&#20013;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#25512;&#21160;&#31038;&#20250;&#21464;&#38761;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#20419;&#36827;&#22312;&#32447;&#31038;&#20250;&#36816;&#21160;&#30340;&#24418;&#25104;&#12290;&#33258;&#21160;&#29702;&#35299;&#25512;&#21160;&#36816;&#21160;&#21644;&#21453;&#23545;&#36816;&#21160;&#22768;&#38899;&#30340;&#35266;&#28857;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#24456;&#38590;&#33719;&#24471;&#24102;&#26377;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#65292;&#26174;&#24335;&#22320;&#23545;#BlakcLivesMatter&#30456;&#20851;&#25512;&#25991;&#20013;&#30340;&#35266;&#28857;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#31038;&#20250;-&#35821;&#35328;&#23398;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25991;&#26412;&#20998;&#35299;&#20026;&#32467;&#26500;&#21270;&#20803;&#32032;&#24182;&#19982;&#20316;&#32773;&#30340;&#31038;&#20132;&#32593;&#32476;&#30456;&#36830;&#25509;&#23558;&#20854;&#36716;&#25442;&#20026;&#22270;&#24418;&#65292;&#28982;&#21518;&#20351;&#29992;&#32467;&#26500;&#21270;&#39044;&#27979;&#26469;&#35782;&#21035;&#35266;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#19968;&#23567;&#32452;&#26631;&#35760;&#31034;&#20363;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20154;&#24037;&#35757;&#32451;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#20854;&#19982;&#25163;&#21160;&#27880;&#37322;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;&#23427;&#20204;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30340;&#27979;&#35797;&#38598;&#36827;&#34892;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;...
&lt;/p&gt;
&lt;p&gt;
Social media has become a major driver of social change, by facilitating the formation of online social movements. Automatically understanding the perspectives driving the movement and the voices opposing it, is a challenging task as annotated data is difficult to obtain. We propose a weakly supervised graph-based approach that explicitly models perspectives in #BackLivesMatter-related tweets. Our proposed approach utilizes a social-linguistic representation of the data. We convert the text to a graph by breaking it into structured elements and connect it with the social network of authors, then structured prediction is done over the elements for identifying perspectives. Our approach uses a small seed set of labeled examples. We experiment with large language models for generating artificial training examples, compare them to manual annotation, and find that it achieves comparable performance. We perform quantitative and qualitative analyses using a human-annotated test set. Our model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#21644;&#25439;&#22833;&#21152;&#26435;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23545;&#20110;&#24815;&#29992;&#34920;&#36798;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.07081</link><description>&lt;p&gt;
&#36328;&#36234;&#38376;&#27099;&#65306;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#25439;&#22833;&#21152;&#26435;&#23454;&#29616;&#24815;&#29992;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting. (arXiv:2310.07081v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#26816;&#32034;&#22686;&#24378;&#21644;&#25439;&#22833;&#21152;&#26435;&#20004;&#31181;&#26041;&#27861;&#65292;&#25104;&#21151;&#25913;&#36827;&#20102;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#23545;&#20110;&#24815;&#29992;&#34920;&#36798;&#30340;&#32763;&#35793;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24815;&#29992;&#35821;&#22312;&#26085;&#24120;&#35821;&#35328;&#20013;&#24456;&#24120;&#35265;&#65292;&#20294;&#23545;&#20110;&#32763;&#35793;&#20154;&#21592;&#26469;&#35828;&#24120;&#24120;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#24847;&#20041;&#19981;&#26159;&#30001;&#23427;&#20204;&#30340;&#37096;&#20998;&#30340;&#24847;&#20041;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20173;&#28982;&#38590;&#20197;&#32763;&#35793;&#24815;&#29992;&#34920;&#36798;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#24815;&#29992;&#32763;&#35793;&#21644;&#30456;&#20851;&#38382;&#39064;&#30340;&#31616;&#21333;&#25551;&#36848;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36827;&#34892;&#19968;&#20010;&#32508;&#21512;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;transformer&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#27491;&#30830;&#22320;&#40664;&#35748;&#37319;&#29992;&#24815;&#29992;&#32763;&#35793;&#26102;&#30340;&#20020;&#30028;&#28857;&#12290;&#20026;&#20102;&#25193;&#22823;&#22810;&#35821;&#36164;&#28304;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#27861;&#35821;&#12289;&#33452;&#20848;&#35821;&#21644;&#26085;&#35821;&#20013;&#21253;&#21547;&#24815;&#29992;&#34920;&#36798;&#30340;&#22823;&#32422;4k&#20010;&#33258;&#28982;&#21477;&#23376;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#25913;&#36827;&#23545;&#33258;&#28982;&#24815;&#29992;&#35821;&#30340;&#32763;&#35793;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#25216;&#26415;&#65306;&#22312;&#21487;&#33021;&#20855;&#26377;&#24815;&#29992;&#24615;&#30340;&#21477;&#23376;&#19978;&#31574;&#30053;&#24615;&#22320;&#21152;&#26435;&#35757;&#32451;&#25439;&#22833;&#65292;&#24182;&#20351;&#29992;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#12290;&#36825;&#19981;&#20165;&#20351;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#24815;&#29992;&#21477;&#23376;&#19978;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;13%&#65292;&#36824;&#25913;&#36827;&#20102;&#32763;&#35793;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06918</link><description>&lt;p&gt;
&#29992;&#32858;&#28966;-&#20449;&#24687;&#29109;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings with Focal-InfoNCE. (arXiv:2310.06918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#65292;&#21033;&#29992;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#25913;&#36827;&#20102;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SimCSE&#30340;&#26368;&#26032;&#25104;&#21151;&#26497;&#22823;&#22320;&#25512;&#36827;&#20102;&#21477;&#23376;&#34920;&#31034;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;SimCSE&#30340;&#21407;&#22987;&#34920;&#36798;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20013;&#30828;&#36127;&#26679;&#26412;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;SimCSE&#19982;&#30828;&#36127;&#26679;&#26412;&#25366;&#25496;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#32858;&#28966;-&#20449;&#24687;&#29109;&#20989;&#25968;&#22312;&#23545;&#27604;&#30446;&#26631;&#20013;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#35843;&#33410;&#39033;&#65292;&#38477;&#20302;&#19982;&#26131;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25439;&#22833;&#65292;&#24182;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#20110;&#22256;&#38590;&#36127;&#26679;&#26412;&#12290;&#22312;&#21508;&#31181;STS&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#12289;&#34920;&#31034;&#23545;&#40784;&#24615;&#21644;&#19968;&#33268;&#24615;&#26041;&#38754;&#25913;&#36827;&#20102;&#21477;&#23376;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent success of SimCSE has greatly advanced state-of-the-art sentence representations. However, the original formulation of SimCSE does not fully exploit the potential of hard negative samples in contrastive learning. This study introduces an unsupervised contrastive learning framework that combines SimCSE with hard negative mining, aiming to enhance the quality of sentence embeddings. The proposed focal-InfoNCE function introduces self-paced modulation terms in the contrastive objective, downweighting the loss associated with easy negatives and encouraging the model focusing on hard negatives. Experimentation on various STS benchmarks shows that our method improves sentence embeddings in terms of Spearman's correlation and representation alignment and uniformity.
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#25991;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#34429;&#28982;&#22312;&#25991;&#26412;&#29255;&#27573;&#31532;&#19968;&#27425;&#21576;&#29616;&#26102;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#24403;&#35760;&#24518;&#65288;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#24320;&#22987;&#21457;&#25381;&#20316;&#29992;&#26102;&#65292;&#34920;&#29616;&#24555;&#36895;&#20998;&#27495;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20998;&#27495;&#28304;&#20110;&#20013;&#38388;&#23618;&#30340;&#29305;&#23450;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#36890;&#36807;&#21152;&#20837;&#24130;&#24459;&#36817;&#26399;&#20559;&#22909;&#20351;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2310.06408</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#25991;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;
&lt;/p&gt;
&lt;p&gt;
Humans and language models diverge when predicting repeating text. (arXiv:2310.06408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06408
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#37325;&#22797;&#25991;&#26412;&#26102;&#23384;&#22312;&#20998;&#27495;&#65292;&#34429;&#28982;&#22312;&#25991;&#26412;&#29255;&#27573;&#31532;&#19968;&#27425;&#21576;&#29616;&#26102;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#24403;&#35760;&#24518;&#65288;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#24320;&#22987;&#21457;&#25381;&#20316;&#29992;&#26102;&#65292;&#34920;&#29616;&#24555;&#36895;&#20998;&#27495;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#31181;&#20998;&#27495;&#28304;&#20110;&#20013;&#38388;&#23618;&#30340;&#29305;&#23450;&#27880;&#24847;&#21147;&#22836;&#65292;&#24182;&#36890;&#36807;&#21152;&#20837;&#24130;&#24459;&#36817;&#26399;&#20559;&#22909;&#20351;&#27169;&#22411;&#26356;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#20110;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#33021;&#20934;&#30830;&#22320;&#27169;&#25311;&#20154;&#31867;&#30340;&#21333;&#35789;&#39044;&#27979;&#21644;&#38405;&#35835;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#19982;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#23384;&#22312;&#20998;&#27495;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20154;&#31867;&#23545;&#20116;&#20010;&#30001;&#37325;&#22797;&#25991;&#26412;&#29255;&#27573;&#32452;&#25104;&#30340;&#21050;&#28608;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#39044;&#27979;&#30340;&#25968;&#25454;&#38598;&#12290;&#20154;&#31867;&#21644;GPT-2&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29255;&#27573;&#30340;&#31532;&#19968;&#27425;&#21576;&#29616;&#20013;&#39044;&#27979;&#32467;&#26524;&#39640;&#24230;&#19968;&#33268;&#65292;&#20294;&#24403;&#35760;&#24518;&#65288;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#24320;&#22987;&#21457;&#25381;&#20316;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#36805;&#36895;&#20998;&#27495;&#12290;&#25105;&#20204;&#36861;&#36394;&#20102;&#36825;&#31181;&#20998;&#27495;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#23427;&#28304;&#20110;&#20013;&#38388;&#23618;&#30340;&#29305;&#23450;&#27880;&#24847;&#21147;&#22836;&#12290;&#21521;&#36825;&#20123;&#27880;&#24847;&#21147;&#22836;&#21152;&#20837;&#24130;&#24459;&#36817;&#26399;&#20559;&#22909;&#33021;&#22815;&#20351;&#27169;&#22411;&#30340;&#34920;&#29616;&#26356;&#25509;&#36817;&#20154;&#31867;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#24773;&#26223;&#33021;&#22815;&#20419;&#36827;&#23558;&#35821;&#35328;&#27169;&#22411;&#26356;&#21152;&#25509;&#36817;&#20154;&#31867;&#34892;&#20026;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models that are trained on the next-word prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06404</link><description>&lt;p&gt;
Hexa: &#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#25105;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Hexa: Self-Improving for Knowledge-Grounded Dialogue System. (arXiv:2310.06404v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#25439;&#22833;&#20989;&#25968;&#30340;&#33258;&#20030;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#39537;&#21160;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#20351;&#29992;&#27169;&#22359;&#21270;&#30340;&#26041;&#27861;&#26126;&#30830;&#22320;&#21033;&#29992;&#20013;&#38388;&#27493;&#39588;&#65288;&#22914;&#32593;&#32476;&#25628;&#32034;&#12289;&#35760;&#24518;&#26816;&#32034;&#65289;&#12290;&#28982;&#32780;&#65292;&#19982;&#23545;&#35805;&#21709;&#24212;&#30456;&#27604;&#65292;&#36825;&#20123;&#27493;&#39588;&#30340;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#22240;&#20026;&#22312;&#26222;&#36890;&#23545;&#35805;&#20013;&#26080;&#27861;&#35266;&#23519;&#21040;&#23427;&#20204;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#25968;&#25454;&#30340;&#32570;&#22833;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#25105;&#25552;&#21319;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#20013;&#38388;&#27493;&#39588;&#30340;&#29983;&#25104;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#22320;&#38754;&#30495;&#23454;&#25968;&#25454;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24341;&#23548;&#25552;&#31034;&#21644;&#20462;&#25913;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#24341;&#23548;&#33258;&#21160;&#29983;&#25104;&#22238;&#31572;&#22810;&#26679;&#24615;&#30340;&#33258;&#20030;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#32463;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#33258;&#25105;&#25552;&#21319;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#20013;&#38388;&#21644;&#26368;&#32456;&#22238;&#31572;&#26041;&#38754;&#25913;&#21892;&#20102;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common practice in knowledge-grounded dialogue generation is to explicitly utilize intermediate steps (e.g., web-search, memory retrieval) with modular approaches. However, data for such steps are often inaccessible compared to those of dialogue responses as they are unobservable in an ordinary dialogue. To fill in the absence of these data, we develop a self-improving method to improve the generative performances of intermediate steps without the ground truth data. In particular, we propose a novel bootstrapping scheme with a guided prompt and a modified loss function to enhance the diversity of appropriate self-generated responses. Through experiments on various benchmark datasets, we empirically demonstrate that our method successfully leverages a self-improving mechanism in generating intermediate and final responses and improves the performances on the task of knowledge-grounded dialogue generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#24182;&#21457;&#29616;&#20256;&#32479;&#30340;&#27169;&#22411;&#36873;&#25321;&#26234;&#24935;&#32570;&#20047;&#28145;&#24230;&#65292;&#24182;&#19988;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#36138;&#23146;&#25628;&#32034;&#30340;&#21484;&#22238;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.06374</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#39044;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models. (arXiv:2310.06374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#20998;&#26512;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#20013;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#24433;&#21709;&#12290;&#24182;&#21457;&#29616;&#20256;&#32479;&#30340;&#27169;&#22411;&#36873;&#25321;&#26234;&#24935;&#32570;&#20047;&#28145;&#24230;&#65292;&#24182;&#19988;&#22312;&#20851;&#38190;&#35789;&#29983;&#25104;&#20013;&#36138;&#23146;&#25628;&#32034;&#30340;&#21484;&#22238;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20026;&#20851;&#38190;&#35789;&#29983;&#25104;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#35774;&#35745;&#20915;&#31574;&#20173;&#26410;&#34987;&#25506;&#32034;&#65292;&#24182;&#19988;&#24120;&#24120;&#26159;&#38543;&#24847;&#20915;&#31574;&#30340;&#12290;&#26412;&#25991;&#23545;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#35789;&#29983;&#25104;&#20219;&#21153;&#30340;&#27169;&#22411;&#36873;&#25321;&#21644;&#35299;&#30721;&#31574;&#30053;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#19968;&#20010;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20551;&#35774;&#38416;&#26126;&#20102;&#20026;&#20160;&#20040;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36866;&#29992;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#12290;&#28982;&#21518;&#25105;&#20204;&#21457;&#29616;&#20256;&#32479;&#30340;&#36873;&#25321;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26234;&#24935;&#32570;&#20047;&#28145;&#24230;&#65306;&#65288;1&#65289;&#20165;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#25110;&#36827;&#34892;&#20219;&#21153;&#29305;&#23450;&#36866;&#24212;&#24615;&#35843;&#25972;&#24182;&#19981;&#26159;&#21442;&#25968;&#39640;&#25928;&#30340;&#65307;&#65288;2&#65289;&#23613;&#31649;&#23558;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#19982;&#20219;&#21153;&#36866;&#24212;&#32467;&#21512;&#26377;&#21033;&#20110;&#20851;&#38190;&#35789;&#29983;&#25104;&#65292;&#20294;&#23427;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#38459;&#30861;&#27867;&#21270;&#33021;&#21147;&#12290;&#20851;&#20110;&#35299;&#30721;&#65292;&#25105;&#20204;&#35777;&#26126;&#36138;&#23146;&#25628;&#32034;&#34429;&#28982;&#22312; F1 &#24471;&#20998;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21484;&#22238;&#29575;&#26041;&#38754;&#33853;&#21518;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase Generation (KPG) is a longstanding task in NLP with widespread applications. The advent of sequence-to-sequence (seq2seq) pre-trained language models (PLMs) has ushered in a transformative era for KPG, yielding promising performance improvements. However, many design decisions remain unexplored and are often made arbitrarily. This paper undertakes a systematic analysis of the influence of model selection and decoding strategies on PLM-based KPG. We begin by elucidating why seq2seq PLMs are apt for KPG, anchored by an attention-driven hypothesis. We then establish that conventional wisdom for selecting seq2seq PLMs lacks depth: (1) merely increasing model size or performing task-specific adaptation is not parameter-efficient; (2) although combining in-domain pre-training with task adaptation benefits KPG, it does partially hinder generalization. Regarding decoding, we demonstrate that while greedy search delivers strong F1 scores, it lags in recall compared with sampling-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;</title><link>http://arxiv.org/abs/2310.05703</link><description>&lt;p&gt;
Siamese&#32534;&#30721;&#22120;&#30340;&#24402;&#22240;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Attribution Method for Siamese Encoders. (arXiv:2310.05703v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#35299;&#37322;&#21477;&#23376;&#36716;&#25442;&#22120;&#27169;&#22411;&#20013;&#37325;&#35201;&#30340;&#39044;&#27979;&#20196;&#29260;&#23545;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#21477;&#23376;&#36716;&#25442;&#22120;&#31561;Siamese&#32534;&#30721;&#22120;&#27169;&#22411;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#20851;&#27880;&#30340;&#36755;&#20837;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#20010;&#38556;&#30861;&#26159;&#23427;&#20204;&#30340;&#39044;&#27979;&#19981;&#33021;&#24402;&#22240;&#20110;&#20010;&#21035;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#20204;&#27604;&#36739;&#30340;&#26159;&#20004;&#20010;&#36755;&#20837;&#32780;&#19981;&#26159;&#19968;&#20010;&#36755;&#20837;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#38598;&#25104;&#26799;&#24230;&#21407;&#29702;&#25512;&#24191;&#21040;&#20855;&#26377;&#22810;&#20010;&#36755;&#20837;&#30340;&#27169;&#22411;&#65292;&#25512;&#23548;&#20986;&#19968;&#31181;&#36866;&#29992;&#20110;Siamese&#32534;&#30721;&#22120;&#30340;&#23616;&#37096;&#24402;&#22240;&#26041;&#27861;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#29305;&#24449;&#23545;&#24402;&#22240;&#30340;&#24418;&#24335;&#65292;&#24182;&#21487;&#23558;&#20854;&#31616;&#21270;&#20026;&#21477;&#23376;&#36716;&#25442;&#22120;&#30340;&#20196;&#29260;-&#20196;&#29260;&#30697;&#38453;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24341;&#20837;&#38598;&#25104;&#38597;&#21487;&#27604;&#30697;&#38453;&#65292;&#24182;&#32487;&#25215;&#20102;&#38598;&#25104;&#26799;&#24230;&#30340;&#20248;&#21183;&#24418;&#24335;&#29305;&#24615;&#65306;&#23427;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#23436;&#25972;&#35745;&#31639;&#22270;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#23454;&#38469;&#39044;&#27979;&#32467;&#26524;&#12290;&#19968;&#39033;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#21477;&#23376;&#36716;&#25442;&#22120;&#20013;&#65292;&#24456;&#23569;&#30340;&#20196;&#29260;&#23545;&#24448;&#24448;&#21487;&#20197;&#35299;&#37322;&#22823;&#37096;&#20998;&#30340;&#39044;&#27979;&#65292;&#24182;&#19988;&#23427;&#20204;&#20027;&#35201;&#38598;&#20013;&#22312;&#21517;&#35789;&#21644;&#21160;&#35789;&#19978;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#33719;&#24471;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#23427;&#38656;&#35201;&#20851;&#27880;&#22823;&#22810;&#25968;&#30340;&#20196;&#29260;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Siamese encoder models such as sentence transformers (ST), little is known about the aspects of inputs they pay attention to. A barrier is that their predictions cannot be attributed to individual features, as they compare two inputs rather than processing a single one. This paper derives a local attribution method for Siamese encoders by generalizing the principle of integrated gradients to models with multiple inputs. The solution takes the form of feature-pair attributions, and can be reduced to a token-token matrix for STs. Our method involves the introduction of integrated Jacobians and inherits the advantageous formal properties of integrated gradients: it accounts for the model's full computation graph and is guaranteed to converge to the actual prediction. A pilot study shows that in an ST few token-pairs can often explain large fractions of predictions, and it focuses on nouns and verbs. For accurate predictions, it however needs to attend to the majorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#21518;&#25509;&#36817;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2310.05597</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#65311;&#30740;&#31350;&#35757;&#32451;&#30446;&#26631;&#21644;&#19982;&#20154;&#31867;&#34920;&#29616;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can language models learn analogical reasoning? Investigating training objectives and comparisons to human performance. (arXiv:2310.05597v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#24182;&#27979;&#35797;&#20102;&#20960;&#31181;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#23569;&#37327;&#25968;&#25454;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#65292;&#24182;&#22312;&#19982;&#20154;&#31867;&#22522;&#20934;&#36827;&#34892;&#27604;&#36739;&#21518;&#25509;&#36817;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#31867;&#27604;&#26159;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#35789;&#23884;&#20837;&#30340;&#24120;&#35265;&#26041;&#24335;&#65292;&#20294;&#30740;&#31350;&#31867;&#27604;&#25512;&#29702;&#26159;&#21542;&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#30340;&#20219;&#21153;&#20063;&#24456;&#26377;&#24847;&#20041;&#12290;&#26412;&#25991;&#27979;&#35797;&#20102;&#20960;&#31181;&#23398;&#20064;&#22522;&#26412;&#31867;&#27604;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#30340;&#26159;&#37027;&#20123;&#26356;&#31526;&#21512;&#20154;&#31867;&#31867;&#27604;&#25512;&#29702;&#35780;&#20272;&#26631;&#20934;&#30340;&#31867;&#27604;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21457;&#29616;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#23569;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#31867;&#27604;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20855;&#26377;&#20154;&#31867;&#22522;&#20934;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#22312;&#35757;&#32451;&#21518;&#65292;&#27169;&#22411;&#25509;&#36817;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;InterroLang&#30340;&#23545;&#35805;&#24335;&#35299;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#35805;&#30028;&#38754;&#24110;&#21161;&#29992;&#25143;&#20197;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#25506;&#32034;&#20855;&#26377;&#35299;&#37322;&#30340;NLP&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#28548;&#28165;&#21644;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#36827;&#34892;&#20132;&#20114;&#12290;&#30740;&#31350;&#20013;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;NLP&#25805;&#20316;&#65292;&#24182;&#22312;&#19977;&#20010;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#24037;&#20855;&#30340;&#27491;&#30830;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#21487;&#27169;&#25311;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05592</link><description>&lt;p&gt;
InterroLang: &#36890;&#36807;&#23545;&#35805;&#24335;&#35299;&#37322;&#25506;&#32034;NLP&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InterroLang: Exploring NLP Models and Datasets through Dialogue-based Explanations. (arXiv:2310.05592v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;InterroLang&#30340;&#23545;&#35805;&#24335;&#35299;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#23545;&#35805;&#30028;&#38754;&#24110;&#21161;&#29992;&#25143;&#20197;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#25506;&#32034;&#20855;&#26377;&#35299;&#37322;&#30340;NLP&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#28548;&#28165;&#21644;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#36827;&#34892;&#20132;&#20114;&#12290;&#30740;&#31350;&#20013;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;NLP&#25805;&#20316;&#65292;&#24182;&#22312;&#19977;&#20010;NLP&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#24037;&#20855;&#30340;&#27491;&#30830;&#24615;&#12289;&#26377;&#29992;&#24615;&#21644;&#21487;&#27169;&#25311;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21457;&#23637;&#30340;NLP&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#20197;&#21508;&#31181;&#26041;&#24335;&#25171;&#24320;&#20102;&#40657;&#31665;&#65292;&#20294;&#26159;&#22312;&#36825;&#19968;&#21162;&#21147;&#20013;&#32570;&#23569;&#30340;&#26159;&#25552;&#20379;&#23545;&#35805;&#30028;&#38754;&#30340;&#20132;&#20114;&#24037;&#20855;&#12290;&#36825;&#26679;&#30340;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#24110;&#21161;&#29992;&#25143;&#20197;&#24773;&#22659;&#21270;&#30340;&#26041;&#24335;&#36890;&#36807;&#28548;&#28165;&#25110;&#21518;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#25506;&#32034;&#20855;&#26377;&#35299;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#24335;&#35299;&#37322;&#26694;&#26550;TalkToModel&#65288;Slack&#31561;&#65292;2022&#65289;&#25913;&#32534;&#20026;NLP&#39046;&#22495;&#65292;&#22686;&#21152;&#20102;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#31561;&#26032;&#30340;NLP&#29305;&#23450;&#25805;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#19977;&#20010;NLP&#20219;&#21153;&#65288;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#65292;&#38382;&#31572;&#65292;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65289;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35782;&#21035;&#29992;&#25143;&#30340;&#35299;&#37322;&#26597;&#35810;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#31934;&#35843;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#27169;&#22411;&#65292;&#24182;&#23454;&#26045;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#29992;&#25143;&#30740;&#31350;&#65306;&#65288;1&#65289;&#35780;&#20272;&#23545;&#35805;&#30340;&#27491;&#30830;&#24615;&#21644;&#26377;&#29992;&#24615;&#65292;&#65288;2&#65289;&#21487;&#27169;&#25311;&#24615;&#65292;&#21363;&#22914;&#20309;&#23545;&#35937;&#21270;&#29992;&#25143;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recently developed NLP explainability methods let us open the black box in various ways (Madsen et al., 2022), a missing ingredient in this endeavor is an interactive tool offering a conversational interface. Such a dialogue system can help users explore datasets and models with explanations in a contextualized manner, e.g. via clarification or follow-up questions, and through a natural language interface. We adapt the conversational explanation framework TalkToModel (Slack et al., 2022) to the NLP domain, add new NLP-specific operations such as free-text rationalization, and illustrate its generalizability on three NLP tasks (dialogue act classification, question answering, hate speech detection). To recognize user queries for explanations, we evaluate fine-tuned and few-shot prompting models and implement a novel Adapter-based approach. We then conduct two user studies on (1) the perceived correctness and helpfulness of the dialogues, and (2) the simulatability, i.e. how object
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#35780;&#20272;&#65292;&#20197;&#24212;&#23545;&#36890;&#29992;&#30446;&#26631;&#26080;&#20851;&#26041;&#27861;&#24341;&#21457;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#20851;&#27880;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24314;&#31435;&#21487;&#20449;&#24230;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.05442</link><description>&lt;p&gt;
&#24314;&#31435;&#21487;&#20449;&#24230;&#65306;&#37325;&#26032;&#24605;&#32771;&#20219;&#21153;&#21644;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Establishing Trustworthiness: Rethinking Tasks and Model Evaluation. (arXiv:2310.05442v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05442
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20219;&#21153;&#21644;&#27169;&#22411;&#35780;&#20272;&#65292;&#20197;&#24212;&#23545;&#36890;&#29992;&#30446;&#26631;&#26080;&#20851;&#26041;&#27861;&#24341;&#21457;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#20851;&#27880;&#22312;&#19981;&#26029;&#22686;&#21152;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24314;&#31435;&#21487;&#20449;&#24230;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29702;&#35299;&#26159;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#35748;&#30693;&#33021;&#21147;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#19968;&#30452;&#22312;&#21162;&#21147;&#23558;&#20854;&#35745;&#31639;&#24314;&#27169;&#20960;&#21313;&#24180;&#12290;&#20256;&#32479;&#19978;&#65292;&#35821;&#35328;&#26234;&#33021;&#30340;&#22810;&#20010;&#26041;&#38754;&#34987;&#20998;&#38548;&#25104;&#20855;&#26377;&#19987;&#38376;&#27169;&#22411;&#26550;&#26500;&#21644;&#30456;&#24212;&#35780;&#20272;&#21327;&#35758;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#31038;&#21306;&#30446;&#30585;&#20102;&#19968;&#31181;&#36890;&#36807;&#29983;&#25104;&#27169;&#22411;&#39537;&#21160;&#30340;&#36890;&#29992;&#30446;&#26631;&#26080;&#20851;&#26041;&#27861;&#30340;&#25103;&#21095;&#24615;&#36716;&#21464;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#20998;&#38548;&#24335;&#35821;&#35328;&#20219;&#21153;&#27010;&#24565;&#27491;&#22312;&#34987;&#25171;&#30772;&#65292;&#38543;&#20043;&#32780;&#26469;&#30340;&#26159;&#23545;&#35780;&#20272;&#21644;&#20998;&#26512;&#30340;&#36234;&#26469;&#36234;&#22823;&#30340;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#27491;&#22312;&#26356;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#36827;&#34892;&#37096;&#32626;&#65292;&#21253;&#25324;&#20197;&#21069;&#26410;&#39044;&#26009;&#21040;&#30340;&#38646;-shot&#35774;&#23450;&#65292;&#22686;&#21152;&#20102;&#23545;&#21487;&#20449;&#21644;&#21487;&#38752;&#31995;&#32479;&#30340;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#26159;&#26102;&#20505;&#37325;&#26032;&#24605;&#32771;NLP&#20013;&#20219;&#21153;&#21644;&#27169;&#22411;&#35780;&#20272;&#30340;&#23450;&#20041;&#65292;&#24182;&#36861;&#27714;&#23545;&#35821;&#35328;&#30340;&#26356;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#23558;&#21487;&#20449;&#24230;&#25918;&#22312;&#39318;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language understanding is a multi-faceted cognitive capability, which the Natural Language Processing (NLP) community has striven to model computationally for decades. Traditionally, facets of linguistic intelligence have been compartmentalized into tasks with specialized model architectures and corresponding evaluation protocols. With the advent of large language models (LLMs) the community has witnessed a dramatic shift towards general purpose, task-agnostic approaches powered by generative models. As a consequence, the traditional compartmentalized notion of language tasks is breaking down, followed by an increasing challenge for evaluation and analysis. At the same time, LLMs are being deployed in more real-world scenarios, including previously unforeseen zero-shot setups, increasing the need for trustworthy and reliable systems. Therefore, we argue that it is time to rethink what constitutes tasks and model evaluation in NLP, and pursue a more holistic view on language, placing tr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.05317</link><description>&lt;p&gt;
&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#36890;&#36807;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26469;&#22686;&#24378;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing Long-form Text Generation in Mental Health with Task-adaptive Tokenization. (arXiv:2310.05317v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05317
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#20998;&#35789;&#36807;&#31243;&#26469;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20943;&#23569;&#26631;&#35760;&#25968;&#37327;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#24615;&#33021;&#65292;&#24182;&#19988;&#21487;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#20316;&#20026;&#19968;&#31181;&#26041;&#24335;&#65292;&#23558;&#29983;&#25104;&#27969;&#27700;&#32447;&#36866;&#24212;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#35201;&#27714;&#65292;&#24182;&#22686;&#24378;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#38271;&#25991;&#26412;&#29983;&#25104;&#12290;&#21463;&#35748;&#30693;&#31185;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#22120;&#20174;&#22810;&#20010;&#32467;&#26524;&#20013;&#37319;&#26679;&#21487;&#21464;&#30340;&#20998;&#27573;&#65292;&#37319;&#26679;&#27010;&#29575;&#22522;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26500;&#24314;&#19987;&#29992;&#35789;&#27719;&#30340;&#31574;&#30053;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#35789;&#27719;&#21512;&#24182;&#21327;&#35758;&#65292;&#21487;&#20197;&#23558;&#20219;&#21153;&#29305;&#23450;&#30340;&#26631;&#35760;&#25972;&#21512;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20998;&#35789;&#27493;&#39588;&#20013;&#12290;&#36890;&#36807;&#23545;&#20013;&#33521;&#25991;&#24515;&#29702;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#20219;&#21153;&#33258;&#36866;&#24212;&#20998;&#35789;&#26041;&#27861;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#29983;&#25104;&#24615;&#33021;&#25552;&#21319;&#65292;&#26368;&#39640;&#21487;&#36798;60%&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#20998;&#35789;&#26041;&#27861;&#19982;&#38750;&#24120;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#33021;&#22815;&#24471;&#21040;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose task-adaptive tokenization as a way to adapt the generation pipeline to the specifics of a downstream task and enhance long-form generation in mental health. Inspired by insights from cognitive science, our task-adaptive tokenizer samples variable segmentations from multiple outcomes, with sampling probabilities optimized based on task-specific data. We introduce a strategy for building a specialized vocabulary and introduce a vocabulary merging protocol that allows for the integration of task-specific tokens into the pre-trained model's tokenization step. Through extensive experiments on psychological question-answering tasks in both Chinese and English, we find that our task-adaptive tokenization approach brings a significant improvement in generation performance while using up to 60% fewer tokens. Preliminary experiments point to promising results when using our tokenization approach with very large language models.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.05280</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#38543;&#26426;&#40550;&#40521;&#26356;&#21361;&#38505;&#21527;&#65311;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Are Personalized Stochastic Parrots More Dangerous? Evaluating Persona Biases in Dialogue Systems. (arXiv:2310.05280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05280
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20154;&#26684;&#20559;&#35265;&#23545;&#31038;&#20132;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#26469;&#34913;&#37327;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#19979;&#30340;&#20559;&#35265;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25353;&#29031;&#33258;&#30001;&#24418;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#65292;&#21253;&#25324;&#22312;&#23545;&#35805;&#20013;&#27169;&#20223;&#36890;&#29992;&#25110;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#30340;&#20154;&#26684;&#12290;&#36890;&#29992;&#20154;&#26684;&#25351;&#30340;&#26159;&#26469;&#33258;&#26576;&#19968;&#20154;&#21475;&#32676;&#20307;&#30340;&#20010;&#20307;&#65288;&#20363;&#22914;&#20122;&#27954;&#20154;&#65289;&#65292;&#32780;&#29305;&#23450;&#20154;&#26684;&#21487;&#20197;&#26159;&#21382;&#21490;&#20154;&#29289;&#30340;&#23454;&#38469;&#22995;&#21517;&#12290;&#34429;&#28982;&#37319;&#29992;&#20154;&#26684;&#20351;&#23545;&#35805;&#31995;&#32479;&#26356;&#20855;&#21560;&#24341;&#21147;&#21644;&#20146;&#21644;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#65292;&#21487;&#33021;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#32780;&#21152;&#21095;&#31038;&#20250;&#20559;&#35265;&#65292;&#36827;&#19968;&#27493;&#36896;&#25104;&#31038;&#20250;&#20260;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#8220;&#20154;&#26684;&#20559;&#35265;&#8221;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#20041;&#20026;&#26377;&#23475;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#23545;&#19981;&#21516;&#20154;&#26684;&#37319;&#29992;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#23558;&#20154;&#26684;&#20559;&#35265;&#20998;&#20026;&#26377;&#23475;&#34920;&#36798;&#21644;&#26377;&#23475;&#35748;&#21516;&#20004;&#31867;&#65292;&#21516;&#26102;&#24314;&#31435;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#34913;&#37327;&#20116;&#20010;&#26041;&#38754;&#30340;&#20154;&#26684;&#20559;&#35265;&#65306;&#20882;&#29359;&#24615;&#12289;&#26377;&#27602;&#24310;&#32493;&#12289;&#20851;&#24576;&#12289;&#21051;&#26495;&#21360;&#35937;&#30340;&#35748;&#21516;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models empower them to follow freeform instructions, including imitating generic or specific demographic personas in conversations. Generic personas refer to an individual from a demographic group (e.g. an Asian person), whereas specific personas can be actual names of historical figures. While the adoption of personas allows dialogue systems to be more engaging and approachable to users, it also carries the potential risk of exacerbating social biases in model responses, further causing societal harms through interactions with users. In this paper, we systematically study "persona biases", which we define to be the sensitivity of harmful dialogue model behaviors to different persona adoptions. We categorize persona biases into biases in harmful expression and harmful agreement, as well as establish a comprehensive evaluation framework to measure persona biases in five aspects: Offensiveness, Toxic Continuation, Regard, Stereotype Agreement, and To
&lt;/p&gt;</description></item><item><title>DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2310.05074</link><description>&lt;p&gt;
DialCoT&#36935;&#21040;&#20102;PPO&#65306;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
DialCoT Meets PPO: Decomposing and Exploring Reasoning Paths in Smaller Language Models. (arXiv:2310.05074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05074
&lt;/p&gt;
&lt;p&gt;
DialCoT&#26159;&#19968;&#31181;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#20998;&#35299;&#21644;&#25506;&#32034;&#25512;&#29702;&#36335;&#24452;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#23427;&#38477;&#20302;&#20102;&#20219;&#21153;&#38590;&#24230;&#65292;&#24182;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#65288;CoT&#65289;&#25552;&#31034;&#24050;&#32463;&#34987;&#35777;&#26126;&#26377;&#21161;&#20110;&#22686;&#24378;&#33267;&#23569;&#20855;&#26377;1000&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#20855;&#26377;&#19981;&#21040;100&#20159;&#21442;&#25968;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#30340;&#25512;&#29702;&#20219;&#21153;&#26102;&#65292;&#23427;&#26159;&#26080;&#25928;&#29978;&#33267;&#26377;&#23475;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#35805;&#24341;&#23548;&#30340;&#38142;&#24335;&#24605;&#32500;&#65288;DialCoT&#65289;&#65292;&#23427;&#37319;&#29992;&#23545;&#35805;&#26684;&#24335;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#65292;&#24341;&#23548;&#27169;&#22411;&#26397;&#30528;&#26368;&#32456;&#31572;&#26696;&#21069;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;PPO&#31639;&#27861;&#20248;&#21270;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#35299;&#20915;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#36807;&#31243;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#20351;&#20854;&#26356;&#36866;&#21512;&#20110;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#36873;&#25321;&#65292;&#20351;&#20854;&#26356;&#20934;&#30830;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) prompting has proven to be effective in enhancing the reasoning capabilities of Large Language Models (LLMs) with at least 100 billion parameters. However, it is ineffective or even detrimental when applied to reasoning tasks in Smaller Language Models (SLMs) with less than 10 billion parameters. To address this limitation, we introduce Dialogue-guided Chain-of-Thought (DialCoT) which employs a dialogue format to generate intermediate reasoning steps, guiding the model toward the final answer. Additionally, we optimize the model's reasoning path selection using the Proximal Policy Optimization (PPO) algorithm, further enhancing its reasoning capabilities. Our method offers several advantages compared to previous approaches. Firstly, we transform the process of solving complex reasoning questions by breaking them down into a series of simpler sub-questions, significantly reducing the task difficulty and making it more suitable for SLMs. Secondly, we optimize the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#24605;&#24615;&#22320;&#23398;&#20064;&#21644;&#36981;&#24490;&#25351;&#21335;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05066</link><description>&lt;p&gt;
&#25351;&#21335;&#23398;&#20064;&#29992;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Guideline Learning for In-context Information Extraction. (arXiv:2310.05066v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;&#30340;&#25351;&#21335;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21453;&#24605;&#24615;&#22320;&#23398;&#20064;&#21644;&#36981;&#24490;&#25351;&#21335;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#36890;&#36807;&#20165;&#20381;&#36182;&#20219;&#21153;&#25351;&#20196;&#21644;&#23569;&#37327;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26469;&#25191;&#34892;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20248;&#21270;&#20219;&#20309;&#21442;&#25968;&#12290;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#12290;&#26368;&#36817;&#65292;&#19978;&#19979;&#25991;&#20449;&#24687;&#25277;&#21462;(IE)&#22312;&#30740;&#31350;&#30028;&#24341;&#36215;&#20102;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19978;&#19979;&#25991;IE&#30340;&#24615;&#33021;&#36890;&#24120;&#33853;&#21518;&#20110;&#26368;&#20808;&#36827;&#30340;&#26377;&#30417;&#30563;&#19987;&#23478;&#27169;&#22411;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#20010;&#20027;&#35201;&#21407;&#22240;: &#20219;&#21153;&#25551;&#36848;&#19981;&#26126;&#30830;&#12290;&#26377;&#38480;&#38271;&#24230;&#30340;&#19978;&#19979;&#25991;&#38590;&#20197;&#20805;&#20998;&#34920;&#36798;&#22797;&#26434;&#30340;IE&#20219;&#21153;&#25351;&#20196;&#21644;&#21508;&#31181;&#36793;&#30028;&#24773;&#20917;&#65292;&#23548;&#33268;&#20219;&#21153;&#29702;&#35299;&#19982;&#20154;&#31867;&#20986;&#29616;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;IE&#30340;&#25351;&#21335;&#23398;&#20064;(GL)&#26694;&#26550;&#65292;&#20854;&#21453;&#24605;&#24615;&#22320;&#23398;&#20064;&#24182;&#36981;&#24490;&#25351;&#21335;&#12290;&#22312;&#23398;&#20064;&#38454;&#27573;&#65292;GL&#22522;&#20110;&#23569;&#37327;&#38169;&#35823;&#26696;&#20363;&#33258;&#21160;&#21512;&#25104;&#19968;&#32452;&#25351;&#21335;&#65292;&#22312;&#25512;&#29702;&#38454;&#27573;&#65292;GL&#26816;&#32034;&#26377;&#29992;&#30340;&#25351;&#21335;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;ICL&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;s
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can perform a new task by merely conditioning on task instructions and a few input-output examples, without optimizing any parameters. This is called In-Context Learning (ICL). In-context Information Extraction (IE) has recently garnered attention in the research community. However, the performance of In-context IE generally lags behind the state-of-the-art supervised expert models. We highlight a key reason for this shortfall: underspecified task description. The limited-length context struggles to thoroughly express the intricate IE task instructions and various edge cases, leading to misalignment in task comprehension with humans. In this paper, we propose a Guideline Learning (GL) framework for In-context IE which reflectively learns and follows guidelines. During the learning phrase, GL automatically synthesizes a set of guidelines based on a few error cases, and during inference, GL retrieves helpful guidelines for better ICL. Moreover, we propose a s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#21382;&#21490;&#12289;&#25991;&#21270;&#21644;&#35821;&#35328;&#36827;&#34892;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21360;&#23612;&#30340;&#20013;&#23567;&#23398;&#27700;&#24179;&#65292;&#23545;&#21360;&#23612;&#22320;&#26041;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#20102;&#35299;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2310.04928</link><description>&lt;p&gt;
&#20165;&#36890;&#36807;&#21360;&#23612;&#20013;&#23567;&#23398;&#32771;&#35797;&#65306;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;IndoMMLU&#30340;&#20840;&#38754;&#27979;&#35797;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (arXiv:2310.04928v2 [cs.CL]&#24050;&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Large Language Models Only Pass Primary School Exams in Indonesia: A Comprehensive Test on IndoMMLU. (arXiv:2310.04928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04928
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#21382;&#21490;&#12289;&#25991;&#21270;&#21644;&#35821;&#35328;&#36827;&#34892;&#30340;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21360;&#23612;&#30340;&#20013;&#23567;&#23398;&#27700;&#24179;&#65292;&#23545;&#21360;&#23612;&#22320;&#26041;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#20102;&#35299;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#32463;&#24120;&#22312;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#29616;&#23454;&#19990;&#30028;&#30693;&#35782;&#20027;&#35201;&#26159;&#22522;&#20110;&#33521;&#25991;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#35780;&#20272;LLM&#22312;&#33521;&#25991;&#20197;&#22806;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#21512;&#36866;&#30340;&#25968;&#25454;&#38598;&#32780;&#21463;&#38459;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IndoMMLU&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#21360;&#23612;&#25991;&#21270;&#21644;&#35821;&#35328;&#30340;&#22810;&#20219;&#21153;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#21360;&#24230;&#23612;&#35199;&#20122;&#20013;&#23567;&#23398;&#21040;&#39640;&#31561;&#25945;&#32946;&#20837;&#23398;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#32856;&#35831;&#19987;&#19994;&#25945;&#24072;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;14,981&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;64&#20010;&#20219;&#21153;&#21644;&#25945;&#32946;&#27700;&#24179;&#65292;&#24182;&#26377;46%&#30340;&#38382;&#39064;&#19987;&#27880;&#20110;&#35780;&#20272;&#23545;&#21360;&#23612;&#35821;&#30340;&#29087;&#32451;&#31243;&#24230;&#20197;&#21450;&#23545;&#21360;&#24230;&#23612;&#35199;&#20122;&#20061;&#20010;&#22320;&#26041;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#20102;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;GPT-3.5&#20165;&#33021;&#36890;&#36807;&#21360;&#23612;&#30340;&#20013;&#23567;&#23398;&#27700;&#24179;&#65292;&#23545;&#21360;&#23612;&#22320;&#26041;&#35821;&#35328;&#21644;&#25991;&#21270;&#30340;&#30693;&#35782;&#26377;&#38480;&#12290;&#20854;&#20182;&#36739;&#23567;&#30340;&#27169;&#22411;&#65292;&#22914;BLOOMZ&#21644;Falcon&#65292;&#20063;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) are often pre-trained on large-scale multilingual texts, their reasoning abilities and real-world knowledge are mainly evaluated based on English datasets. Assessing LLM capabilities beyond English is increasingly vital but hindered due to the lack of suitable datasets. In this work, we introduce IndoMMLU, the first multi-task language understanding benchmark for Indonesian culture and languages, which consists of questions from primary school to university entrance exams in Indonesia. By employing professional teachers, we obtain 14,981 questions across 64 tasks and education levels, with 46% of the questions focusing on assessing proficiency in the Indonesian language and knowledge of nine local languages and cultures in Indonesia. Our empirical evaluations show that GPT-3.5 only manages to pass the Indonesian primary school level, with limited knowledge of local Indonesian languages and culture. Other smaller models such as BLOOMZ and Falcon per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04445</link><description>&lt;p&gt;
LoFT: &#29992;&#20110;&#25913;&#36827;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#30340;&#26412;&#22320;&#20195;&#29702;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model. (arXiv:2310.04445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LoFT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#26469;&#25913;&#21892;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23545;&#40784;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#29305;&#21046;&#30340;&#25915;&#20987;&#21518;&#32512;&#21644;&#26377;&#23475;&#26597;&#35810;&#26469;&#35268;&#36991;&#65292;&#20197;&#24341;&#21457;&#26377;&#23475;&#30340;&#21709;&#24212;&#12290;&#20026;&#20102;&#23545;&#26410;&#30693;&#29305;&#24449;&#30340;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#65292;&#21487;&#20197;&#20351;&#29992;&#20844;&#20849;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#26469;&#26500;&#24314;&#25915;&#20987;&#65292;&#24182;&#23558;&#25104;&#21151;&#30340;&#25915;&#20987;&#20174;&#20844;&#20849;&#20195;&#29702;&#20256;&#36882;&#21040;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#12290;&#25915;&#20987;&#30340;&#25104;&#21151;&#29575;&#21462;&#20915;&#20110;&#20195;&#29702;&#27169;&#22411;&#33021;&#22815;&#22810;&#22823;&#31243;&#24230;&#19978;&#36924;&#36817;&#31169;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#23545;&#20110;&#25915;&#20987;&#21487;&#20256;&#36882;&#24615;&#26469;&#35828;&#65292;&#21482;&#35201;&#20195;&#29702;&#33021;&#22815;&#22312;&#26377;&#23475;&#26597;&#35810;&#30340;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#20869;&#36924;&#36817;&#30446;&#26631;&#27169;&#22411;&#21363;&#21487;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#26412;&#22320;&#24494;&#35843;&#65288;LoFT&#65289;&#8221;&#65292;&#21363;&#22312;&#19982;&#26377;&#23475;&#26597;&#35810;&#22788;&#20110;&#35789;&#27719;-&#35821;&#20041;&#37051;&#22495;&#30340;&#30456;&#20284;&#26597;&#35810;&#19978;&#36827;&#34892;&#20195;&#29702;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#20197;&#20943;&#23567;&#20195;&#29702;&#21644;&#30446;&#26631;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#19977;&#31181;&#20419;&#20351;&#31169;&#26377;&#30446;&#26631;&#27169;&#22411;&#21464;&#24471;&#26131;&#21463;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \emph{Local Fine-Tuning (LoFT)}, \textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.03368</link><description>&lt;p&gt;
&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;HalluQA&#65288;&#20013;&#25991;&#24187;&#35273;&#38382;&#31572;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;HalluQA&#21253;&#21547;450&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#65292;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#32771;&#34385;&#20102;&#20013;&#22269;&#21382;&#21490;&#25991;&#21270;&#12289;&#39118;&#20439;&#21644;&#31038;&#20250;&#29616;&#35937;&#12290;&#22312;&#26500;&#24314;HalluQA&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24187;&#35273;&#31867;&#22411;&#65306;&#27169;&#20223;&#24615;&#34394;&#20551;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#22522;&#20110;GLM-130B&#21644;ChatGPT&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#26469;&#21028;&#26029;&#27169;&#22411;&#36755;&#20986;&#26159;&#21542;&#26159;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;24&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;ERNIE-Bot&#12289;Baichuan2&#12289;ChatGLM&#12289;Qwen&#12289;SparkDesk&#31561;&#12290;&#22312;&#36825;24&#20010;&#27169;&#22411;&#20013;&#65292;&#26377;18&#20010;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;HalluQA&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#20027;&#35201;&#30340;&#24187;&#35273;&#31867;&#22411;&#21450;&#20854;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Add
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20195;&#29702;&#36171;&#20104;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#22810;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.02374</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65306;&#20010;&#24615;&#21270;&#30340;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Conversational Health Agents: A Personalized LLM-Powered Agent Framework. (arXiv:2310.02374v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02374
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#20195;&#29702;&#36171;&#20104;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#23454;&#29616;&#20010;&#24615;&#21270;&#30340;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#26080;&#32541;&#38598;&#25104;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#22810;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20581;&#24247;&#20195;&#29702;&#65288;CHAs&#65289;&#26159;&#19968;&#31181;&#20114;&#21160;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#36827;&#34892;&#20849;&#24773;&#23545;&#35805;&#21644;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#22686;&#24378;&#20010;&#20154;&#20581;&#24247;&#25252;&#29702;&#26381;&#21153;&#12290;&#24403;&#21069;&#30340;CHAs&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#31995;&#32479;&#65292;&#20027;&#35201;&#20851;&#27880;&#23545;&#35805;&#65292;&#20294;&#24448;&#24448;&#32570;&#20047;&#20840;&#38754;&#30340;&#20195;&#29702;&#33021;&#21147;&#12290;&#36825;&#21253;&#25324;&#20174;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#20840;&#22825;&#20505;&#25968;&#25454;&#25910;&#38598;&#28304;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#33719;&#21462;&#20010;&#20154;&#29992;&#25143;&#30340;&#20581;&#24247;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#25972;&#21512;&#26368;&#26032;&#21457;&#24067;&#30340;&#20581;&#24247;&#35265;&#35299;&#65292;&#24182;&#19982;&#24050;&#24314;&#31435;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36830;&#25509;&#12290;&#25105;&#20204;&#27491;&#22312;&#24320;&#21457;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#36171;&#20104;CHAs&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;&#30693;&#35782;&#33719;&#21462;&#21644;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26469;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;CHA&#24179;&#21488;&#30001;LLMs&#39537;&#21160;&#65292;&#26080;&#32541;&#38598;&#25104;&#20102;&#21307;&#30103;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#23545;&#35805;&#65292;&#24182;&#19982;&#21508;&#31181;&#29992;&#25143;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#36827;&#34892;&#25509;&#21475;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#22788;&#29702;&#22797;&#26434;&#21307;&#30103;&#20219;&#21153;&#26041;&#38754;&#30340;&#29087;&#32451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational Health Agents (CHAs) are interactive systems designed to enhance personal healthcare services by engaging in empathetic conversations and processing multimodal data. While current CHAs, especially those utilizing Large Language Models (LLMs), primarily focus on conversation, they often lack comprehensive agent capabilities. This includes the ability to access personal user health data from wearables, 24/7 data collection sources, and electronic health records, as well as integrating the latest published health insights and connecting with established multimodal data analysis tools. We are developing a framework to empower CHAs by equipping them with critical thinking, knowledge acquisition, and problem-solving abilities. Our CHA platform, powered by LLMs, seamlessly integrates healthcare tools, enables multilingual and multimodal conversations, and interfaces with a variety of user data analysis tools. We illustrate its proficiency in handling complex healthcare tasks, s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#35748;&#20026;&#65292;&#23545;&#35805;&#31649;&#29702;&#22120;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#26041;&#38754;&#30340;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#26159;&#23548;&#33268;&#23545;&#35805;&#31649;&#29702;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.01339</link><description>&lt;p&gt;
&#25913;&#36827;&#23545;&#35805;&#31649;&#29702;&#65306;&#36136;&#37327;&#25968;&#25454;&#38598; vs &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Dialogue Management: Quality Datasets vs Models. (arXiv:2310.01339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01339
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35748;&#20026;&#65292;&#23545;&#35805;&#31649;&#29702;&#22120;&#22312;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#26041;&#38754;&#30340;&#20027;&#35201;&#38382;&#39064;&#22312;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#30340;&#27169;&#22411;&#12290;&#30740;&#31350;&#21457;&#29616;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#38169;&#35823;&#26159;&#23548;&#33268;&#23545;&#35805;&#31649;&#29702;&#22833;&#36133;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#31995;&#32479;(TODS)&#24050;&#32463;&#25104;&#20026;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#26426;&#22120;&#21644;&#35745;&#31639;&#26426;&#20132;&#20114;&#30340;&#20851;&#38190;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#23545;&#35805;&#31649;&#29702;&#22120;&#65292;&#36890;&#36807;&#25552;&#20379;&#26368;&#20339;&#21709;&#24212;&#23558;&#23545;&#35805;&#24341;&#23548;&#21040;&#29992;&#25143;&#30340;&#30446;&#26631;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#31995;&#32479;(RBS)&#12289;&#24378;&#21270;&#23398;&#20064;(RL)&#21644;&#30417;&#30563;&#23398;&#20064;(SL)&#20316;&#20026;&#27491;&#30830;&#23545;&#35805;&#31649;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#26681;&#25454;&#29992;&#25143;&#30340;&#36755;&#20837;&#36873;&#25321;&#26368;&#20339;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#35748;&#20026;&#65292;&#23545;&#35805;&#31649;&#29702;&#22120;&#26410;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#30340;&#20027;&#35201;&#21407;&#22240;&#22312;&#20110;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#36804;&#20170;&#20026;&#27490;&#37319;&#29992;&#30340;&#27169;&#22411;&#65307;&#36825;&#24847;&#21619;&#30528;&#25968;&#25454;&#38598;&#38169;&#35823;&#65292;&#22914;&#38169;&#35823;&#26631;&#35760;&#65292;&#23548;&#33268;&#23545;&#35805;&#31649;&#29702;&#30340;&#22823;&#37096;&#20998;&#22833;&#36133;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;Multiwoz 2.1&#21644;SGD&#20013;&#30340;&#20027;&#35201;&#38169;&#35823;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21512;&#25104;&#23545;&#35805;&#29983;&#25104;&#22120;&#20197;&#23436;&#20840;&#25511;&#21046;&#23545;&#35805;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialogue systems (TODS) have become crucial for users to interact with machines and computers using natural language. One of its key components is the dialogue manager, which guides the conversation towards a good goal for the user by providing the best possible response. Previous works have proposed rule-based systems (RBS), reinforcement learning (RL), and supervised learning (SL) as solutions for the correct dialogue management; in other words, select the best response given input by the user. However, this work argues that the leading cause of DMs not achieving maximum performance resides in the quality of the datasets rather than the models employed thus far; this means that dataset errors, like mislabeling, originate a large percentage of failures in dialogue management. We studied the main errors in the most widely used datasets, Multiwoz 2.1 and SGD, to demonstrate this hypothesis. To do this, we have designed a synthetic dialogue generator to fully control the am
&lt;/p&gt;</description></item><item><title>GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.16583</link><description>&lt;p&gt;
GPT-Fathom&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#35299;&#26512;GPT-4&#21450;&#20854;&#21518;&#32493;&#29256;&#26412;&#30340;&#28436;&#21270;&#36335;&#24452;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom: Benchmarking Large Language Models to Decipher the Evolutionary Path towards GPT-4 and Beyond. (arXiv:2309.16583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16583
&lt;/p&gt;
&lt;p&gt;
GPT-Fathom&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#28304;&#22871;&#20214;&#65292;&#23427;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20174;GPT-3&#21040;GPT-4&#28436;&#21270;&#36335;&#24452;&#30340;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#36827;&#23637;&#65292;&#20154;&#20204;&#36843;&#20999;&#38656;&#35201;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#22871;&#20214;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;&#29616;&#26377;&#30340;LLM&#25490;&#34892;&#27036;&#36890;&#24120;&#21442;&#32771;&#20854;&#20182;&#35770;&#25991;&#20013;&#25253;&#21578;&#30340;&#24471;&#20998;&#65292;&#35774;&#32622;&#21644;&#25552;&#31034;&#19981;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#26080;&#24847;&#38388;&#40723;&#21169;&#36873;&#25321;&#26377;&#21033;&#30340;&#35774;&#32622;&#21644;&#25552;&#31034;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GPT-Fathom&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;OpenAI Evals&#26500;&#24314;&#30340;&#24320;&#28304;&#21644;&#21487;&#37325;&#22797;&#30340;LLM&#35780;&#20272;&#22871;&#20214;&#12290;&#25105;&#20204;&#22312;&#23545;&#40784;&#30340;&#29615;&#22659;&#35774;&#32622;&#19979;&#31995;&#32479;&#35780;&#20272;&#20102;10&#22810;&#20010;&#20027;&#35201;&#30340;LLMs&#20197;&#21450;OpenAI&#30340;&#20256;&#32479;&#27169;&#22411;&#22312;20&#22810;&#20010;&#31934;&#36873;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#34920;&#29616;&#65292;&#28085;&#30422;&#20102;7&#20010;&#33021;&#21147;&#31867;&#21035;&#12290;&#25105;&#20204;&#23545;OpenAI&#26089;&#26399;&#27169;&#22411;&#30340;&#22238;&#39038;&#24615;&#30740;&#31350;&#20026;&#25105;&#20204;&#25581;&#31034;&#20102;&#20174;GPT-3&#21040;GPT-4&#30340;&#28436;&#21270;&#36335;&#24452;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#30446;&#21069;&#65292;&#31038;&#21306;&#28212;&#26395;&#20102;&#35299;GPT-3&#22914;&#20309;&#36880;&#27493;&#25913;&#36827;&#21040;GPT-4&#65292;&#21253;&#25324;&#20687;&#28155;&#21152;&#20195;&#30721;&#25968;&#25454;&#26159;&#21542;&#25552;&#39640;&#20102;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;LLM&#33021;&#21147;&#30340;&#21738;&#20123;&#26041;&#38754;&#31561;&#25216;&#26415;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid advancement of large language models (LLMs), there is a pressing need for a comprehensive evaluation suite to assess their capabilities and limitations. Existing LLM leaderboards often reference scores reported in other papers without consistent settings and prompts, which may inadvertently encourage cherry-picking favored settings and prompts for better results. In this work, we introduce GPT-Fathom, an open-source and reproducible LLM evaluation suite built on top of OpenAI Evals. We systematically evaluate 10+ leading LLMs as well as OpenAI's legacy models on 20+ curated benchmarks across 7 capability categories, all under aligned settings. Our retrospective study on OpenAI's earlier models offers valuable insights into the evolutionary path from GPT-3 to GPT-4. Currently, the community is eager to know how GPT-3 progressively improves to GPT-4, including technical details like whether adding code data improves LLM's reasoning capability, which aspects of LLM capabili
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12075</link><description>&lt;p&gt;
&#20351;&#29992;Prompt&#35843;&#20248;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#20027;&#39064;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#20316;&#20026;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#27491;&#22312;&#25104;&#20026;&#32454;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#23545;Prompt Tuning&#21644;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#20844;&#21496;&#20998;&#31867;&#20026;&#25237;&#36164;&#20844;&#21496;&#19987;&#26377;&#30340;&#34892;&#19994;&#20998;&#31867;&#27861;&#65292;&#20197;&#25903;&#25345;&#20854;&#20027;&#39064;&#25237;&#36164;&#31574;&#30053;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;PLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#20998;&#31867;&#32463;&#24120;&#34987;&#25253;&#21578;&#20026;&#20248;&#20110;&#20351;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#27599;&#20010;&#26631;&#31614;&#30001;&#22810;&#20010;&#20196;&#29260;&#32452;&#25104;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;a&#65289;&#29983;&#25104;&#30340;&#26631;&#31614;&#21487;&#33021;&#19981;&#21305;&#37197;&#34892;&#19994;&#20998;&#31867;&#27861;&#20013;&#30340;&#20219;&#20309;&#26631;&#31614;&#65307;&#65288;b&#65289;&#22312;&#32454;&#35843;&#38454;&#27573;&#65292;&#24517;&#39035;&#20197;&#20219;&#24847;&#39034;&#24207;&#25552;&#20379;&#22810;&#20010;&#26631;&#31614;&#65307;&#65288;c&#65289;&#27169;&#22411;&#20026;&#27599;&#20010;&#26631;&#31614;&#25552;&#20379;&#20108;&#36827;&#21046;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#38480;&#21046;&#65288;a&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32763;&#35793;&#27495;&#20041;&#21477;&#23376;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#27495;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;&#25552;&#20986;&#20102;&#25913;&#36827;&#22788;&#29702;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#19978;&#26377;&#30528;&#19982;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#36229;&#36234;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#30740;&#31350;&#20026;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#28040;&#27495;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.11668</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#32763;&#35793;&#28040;&#27495;&#25928;&#26524;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Effective Disambiguation for Machine Translation with Large Language Models. (arXiv:2309.11668v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#32763;&#35793;&#27495;&#20041;&#21477;&#23376;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#27495;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;&#25552;&#20986;&#20102;&#25913;&#36827;&#22788;&#29702;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#35328;&#26041;&#21521;&#19978;&#26377;&#30528;&#19982;&#26368;&#20808;&#36827;&#31995;&#32479;&#30456;&#24403;&#29978;&#33267;&#36229;&#36234;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#30740;&#31350;&#20026;&#26426;&#22120;&#32763;&#35793;&#30340;&#26377;&#25928;&#28040;&#27495;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#65292;&#35299;&#20915;&#35821;&#20041;&#27495;&#20041;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#12290;&#26368;&#36817;&#22312;&#27495;&#20041;&#21477;&#23376;&#30340;&#32763;&#35793;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20256;&#32479;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#26292;&#38706;&#20986;&#26469;&#65292;&#26080;&#27861;&#25429;&#25417;&#21040;&#20854;&#20013;&#35768;&#22810;&#24773;&#20917;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#34920;&#29616;&#20986;&#19982;&#20256;&#32479;NMT&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#20102;&#25511;&#21046;&#30446;&#26631;&#36755;&#20986;&#30340;&#26032;&#33539;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#22312;&#32763;&#35793;&#21253;&#21547;&#22810;&#20041;&#35789;&#21644;&#31232;&#26377;&#35789;&#20041;&#30340;&#27495;&#20041;&#21477;&#23376;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#31934;&#24515;&#31574;&#21010;&#30340;&#27495;&#20041;&#25968;&#25454;&#38598;&#24494;&#35843;&#26469;&#25913;&#36827;&#22788;&#29702;&#27492;&#31867;&#27495;&#20041;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20116;&#20010;&#35821;&#35328;&#26041;&#21521;&#20013;&#26377;&#22235;&#20010;&#26041;&#21521;&#33021;&#22815;&#19982;DeepL&#21644;NLLB&#31561;&#26368;&#20808;&#36827;&#31995;&#32479;&#21305;&#25932;&#29978;&#33267;&#36229;&#36234;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026; effective disambiguation for machine translation &#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resolving semantic ambiguity has long been recognised as a central challenge in the field of machine translation. Recent work on benchmarking translation performance on ambiguous sentences has exposed the limitations of conventional Neural Machine Translation (NMT) systems, which fail to capture many of these cases. Large language models (LLMs) have emerged as a promising alternative, demonstrating comparable performance to traditional NMT models while introducing new paradigms for controlling the target outputs. In this paper, we study the capabilities of LLMs to translate ambiguous sentences containing polysemous words and rare word senses. We also propose two ways to improve the handling of such ambiguity through in-context learning and fine-tuning on carefully curated ambiguous datasets. Experiments show that our methods can match or outperform state-of-the-art systems such as DeepL and NLLB in four out of five language directions. Our research provides valuable insights into effec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.07430</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#24037;&#20316;&#20013;&#65292;&#27983;&#35272;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#24182;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#26102;&#38388;&#20998;&#37197;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20005;&#26684;&#30340;&#26816;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#20102;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#20219;&#21153;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#12289;&#24739;&#32773;&#38382;&#39064;&#12289;&#30149;&#21382;&#35760;&#24405;&#21644;&#21307;&#24739;&#23545;&#35805;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#27169;&#22411;&#21644;&#36866;&#24212;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#19981;&#20250;&#24102;&#26469;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#20845;&#21517;&#21307;&#29983;&#36827;&#34892;&#30340;&#20020;&#24202;&#38405;&#35835;&#32773;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;LLM&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#23450;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#20154;&#31867;&#22312;&#38754;&#23545;&#30340;&#20849;&#21516;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06553</link><description>&lt;p&gt;
&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#19979;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Offline Prompt Evaluation and Optimization with Inverse Reinforcement Learning. (arXiv:2309.06553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#38598;&#21644;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65292;&#39044;&#27979;&#25552;&#31034;&#24615;&#33021;&#12289;&#25552;&#39640;&#25104;&#26412;&#25928;&#30410;&#12289;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#25581;&#31034;LLMs&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#38656;&#35201;&#22312;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#24191;&#38420;&#25628;&#32034;&#31354;&#38388;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#34429;&#28982;&#25552;&#31034;&#24037;&#31243;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#35797;&#38169;&#23581;&#35797;&#20013;&#25152;&#38656;&#30340;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#21644;&#30456;&#20851;&#25104;&#26412;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20851;&#38190;&#26159;&#65292;&#25552;&#31034;&#20248;&#21270;&#30340;&#25928;&#29575;&#21462;&#20915;&#20110;&#26114;&#36149;&#30340;&#25552;&#31034;&#35780;&#20272;&#36807;&#31243;&#12290;&#26412;&#24037;&#20316;&#20171;&#32461;&#20102;Prompt-OIRL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#24357;&#21512;&#26377;&#25928;&#25552;&#31034;&#35780;&#20272;&#21644;&#21487;&#36127;&#25285;&#24615;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#19987;&#23478;&#35780;&#20272;&#30340;&#31163;&#32447;&#25968;&#25454;&#38598;&#65292;&#36816;&#29992;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#31163;&#32447;&#12289;&#26597;&#35810;&#20381;&#36182;&#22411;&#25552;&#31034;&#35780;&#20272;&#30340;&#22870;&#21169;&#27169;&#22411;&#12290;Prompt-OIRL&#30340;&#20248;&#28857;&#26159;&#22810;&#26041;&#38754;&#30340;&#65306;&#23427;&#39044;&#27979;&#25552;&#31034;&#30340;&#24615;&#33021;&#65292;&#25104;&#26412;&#39640;&#25928;&#65292;&#29983;&#25104;&#26131;&#35835;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advances in the development of Large Language Models (LLMs) like ChatGPT have achieved remarkable performance by leveraging human expertise. Yet, fully eliciting LLMs' potential for complex tasks requires navigating the vast search space of natural language prompts. While prompt engineering has shown promise, the requisite human-crafted prompts in trial-and-error attempts and the associated costs pose significant challenges. Crucially, the efficiency of prompt optimization hinges on the costly procedure of prompt evaluation. This work introduces Prompt-OIRL, an approach rooted in offline inverse reinforcement learning that seeks to bridge the gap between effective prompt evaluation and affordability. Our method draws on offline datasets from expert evaluations, employing Inverse-RL to derive a reward model for offline, query-dependent prompt evaluations. The advantages of Prompt-OIRL are manifold: it predicts prompt performance, is cost-efficient, produces human-readable res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;Transformer&#26550;&#26500;&#20013;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#21344;&#25454;&#20102;&#27169;&#22411;&#24456;&#22823;&#19968;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#20294;&#23427;&#26159;&#20887;&#20313;&#30340;&#12290;&#36890;&#36807;&#31227;&#38500;&#35299;&#30721;&#22120;&#23618;&#30340;FFN&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20849;&#20139;&#21333;&#20010;FFN&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#24182;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2309.01826</link><description>&lt;p&gt;
&#21482;&#38656;&#35201;&#19968;&#20010;&#23485;&#24230;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
One Wide Feedforward is All You Need. (arXiv:2309.01826v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;Transformer&#26550;&#26500;&#20013;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#21344;&#25454;&#20102;&#27169;&#22411;&#24456;&#22823;&#19968;&#37096;&#20998;&#30340;&#21442;&#25968;&#65292;&#20294;&#23427;&#26159;&#20887;&#20313;&#30340;&#12290;&#36890;&#36807;&#31227;&#38500;&#35299;&#30721;&#22120;&#23618;&#30340;FFN&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20849;&#20139;&#21333;&#20010;FFN&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#24182;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26377;&#20004;&#20010;&#20027;&#35201;&#30340;&#38750;&#23884;&#20837;&#32452;&#20214;&#65306;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#12290;&#27880;&#24847;&#21147;&#25429;&#25417;&#21040;&#19981;&#32771;&#34385;&#20301;&#32622;&#30340;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#32780;FFN&#29420;&#31435;&#22320;&#23545;&#27599;&#20010;&#36755;&#20837;&#26631;&#35760;&#36827;&#34892;&#38750;&#32447;&#24615;&#36716;&#25442;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;FFN&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#23613;&#31649;&#23427;&#21344;&#25454;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#30456;&#24403;&#22823;&#27604;&#20363;&#65292;&#20294;&#23427;&#26159;&#39640;&#24230;&#20887;&#20313;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#23618;&#31227;&#38500;FFN&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20849;&#20139;&#19968;&#20010;&#21333;&#19968;&#30340;FFN&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21482;&#26377;&#36731;&#24494;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#22686;&#21152;&#20849;&#20139;FFN&#30340;&#38544;&#34255;&#32500;&#24230;&#65292;&#25105;&#20204;&#23558;&#27492;&#26550;&#26500;&#32553;&#23567;&#22238;&#21407;&#22987;&#22823;&#23567;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#19982;&#21407;&#22987;Transformer Big&#30456;&#27604;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture has two main non-embedding components: Attention and the Feed Forward Network (FFN). Attention captures interdependencies between words regardless of their position, while the FFN non-linearly transforms each input token independently. In this work we explore the role of the FFN, and find that despite taking up a significant fraction of the model's parameters, it is highly redundant. Concretely, we are able to substantially reduce the number of parameters with only a modest drop in accuracy by removing the FFN on the decoder layers and sharing a single FFN across the encoder. Finally we scale this architecture back to its original size by increasing the hidden dimension of the shared FFN, achieving substantial gains in both accuracy and latency with respect to the original Transformer Big.
&lt;/p&gt;</description></item><item><title>OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.13137</link><description>&lt;p&gt;
OmniQuant&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
OmniQuant: Omnidirectionally Calibrated Quantization for Large Language Models. (arXiv:2308.13137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13137
&lt;/p&gt;
&lt;p&gt;
OmniQuant&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#20445;&#25345;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#20102;&#20854;&#24222;&#22823;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#38480;&#21046;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#20943;&#23569;&#20869;&#23384;&#21344;&#29992;&#21644;&#25552;&#39640;LLM&#30340;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#25163;&#24037;&#21046;&#23450;&#37327;&#21270;&#21442;&#25968;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#20302;&#24182;&#19988;&#19981;&#33021;&#22788;&#29702;&#26497;&#20302;&#20301;&#37327;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#20840;&#21521;&#26657;&#20934;&#37327;&#21270;&#65288;OmniQuant&#65289;&#25216;&#26415;&#65292;&#29992;&#20110;LLMs&#65292;&#23427;&#22312;&#22810;&#31181;&#37327;&#21270;&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#20248;&#21270;&#21508;&#31181;&#37327;&#21270;&#21442;&#25968;&#26469;&#20445;&#25345;PTQ&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;OmniQuant&#21253;&#21547;&#20004;&#20010;&#21019;&#26032;&#32452;&#20214;&#65292;&#21253;&#25324;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#21098;&#35009;&#65288;LWC&#65289;&#21644;&#21487;&#23398;&#20064;&#30340;&#31561;&#25928;&#21464;&#25442;&#65288;LET&#65289;&#12290;LWC&#36890;&#36807;&#20248;&#21270;&#21098;&#35009;&#38408;&#20540;&#26469;&#35843;&#33410;&#26435;&#37325;&#30340;&#26497;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LET&#22788;&#29702;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing tasks. However, their practical deployment is hindered by their immense memory and computation requirements. Although recent post-training quantization (PTQ) methods are effective in reducing memory footprint and improving the computational efficiency of LLM, they hand-craft quantization parameters, which leads to low performance and fails to deal with extremely low-bit quantization. To tackle this issue, we introduce an Omnidirectionally calibrated Quantization (OmniQuant) technique for LLMs, which achieves good performance in diverse quantization settings while maintaining the computational efficiency of PTQ by efficiently optimizing various quantization parameters. OmniQuant comprises two innovative components including Learnable Weight Clipping (LWC) and Learnable Equivalent Transformation (LET). LWC modulates the extreme values of weights by optimizing the clipping threshold. Meanwhile, LET tackles activa
&lt;/p&gt;</description></item><item><title>AgentVerse&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21327;&#21516;&#21644;&#21160;&#24577;&#35843;&#25972;&#21512;&#20316;&#22242;&#38431;&#30340;&#32452;&#25104;&#65292;&#23454;&#29616;&#36229;&#36234;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24341;&#21457;&#20986;&#32676;&#20307;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20250;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#30340;&#21327;&#20316;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10848</link><description>&lt;p&gt;
AgentVerse: &#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#21644;&#25506;&#32034; emergent behaviors&#12290;
&lt;/p&gt;
&lt;p&gt;
AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors. (arXiv:2308.10848v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10848
&lt;/p&gt;
&lt;p&gt;
AgentVerse&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#21327;&#21516;&#21644;&#21160;&#24577;&#35843;&#25972;&#21512;&#20316;&#22242;&#38431;&#30340;&#32452;&#25104;&#65292;&#23454;&#29616;&#36229;&#36234;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24341;&#21457;&#20986;&#32676;&#20307;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#31038;&#20250;&#34892;&#20026;&#65292;&#20174;&#32780;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#22242;&#38431;&#30340;&#21327;&#20316;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36171;&#33021;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#20869;&#36827;&#34892;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#65292;&#20010;&#20307;&#20043;&#38388;&#30340;&#21512;&#20316;&#32463;&#24120;&#38656;&#35201;&#20197;&#22686;&#24378;&#20219;&#21153;&#23436;&#25104;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#21463;&#21040;&#20154;&#31867;&#32676;&#20307;&#21160;&#21147;&#23398;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;&#22823;&#20110;&#20854;&#21508;&#20010;&#37096;&#20998;&#20043;&#21644;&#30340;&#31995;&#32479;&#36827;&#34892;&#21327;&#21516;&#21644;&#21160;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20010;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#37096;&#32626;&#36229;&#36807;&#21333;&#20010;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#32452;&#65292;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#21512;&#20316;&#20219;&#21153;&#23436;&#25104;&#36807;&#31243;&#20013;&#65292;&#32676;&#20307;&#20869;&#20010;&#20307;&#26234;&#33021;&#20307;&#20043;&#38388;&#20986;&#29616;&#30340;&#31038;&#20250;&#34892;&#20026;&#30340;&#28044;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#34892;&#20026;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#21487;&#33021;&#30340;&#31574;&#30053;&#65292;&#20197;&#21033;&#29992;&#31215;&#26497;&#30340;&#34892;&#20026;&#65292;&#24182;&#20943;&#36731;&#28040;&#26497;&#30340;&#34892;&#20026;&#65292;&#20197;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#32452;&#30340;&#21327;&#20316;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340; \framework &#26694;&#26550;&#30340;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents empowered by Large Language Models (LLMs) have undergone significant improvements, enabling them to generalize across a broad spectrum of tasks. However, in real-world scenarios, cooperation among individuals is often required to enhance the efficiency and effectiveness of task accomplishment. Hence, inspired by human group dynamics, we propose a multi-agent framework \framework that can collaboratively and dynamically adjust its composition as a greater-than-the-sum-of-its-parts system. Our experiments demonstrate that \framework framework can effectively deploy multi-agent groups that outperform a single agent. Furthermore, we delve into the emergence of social behaviors among individual agents within a group during collaborative task accomplishment. In view of these behaviors, we discuss some possible strategies to leverage positive ones and mitigate negative ones for improving the collaborative potential of multi-agent groups. Our codes for \framework will soon be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2308.03415</link><description>&lt;p&gt;
&#20302;&#24310;&#36831;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#31471;&#21040;&#31471;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation. (arXiv:2308.03415v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#30340;&#25361;&#25112;&#24341;&#36215;&#20102;&#30740;&#31350;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#35768;&#22810;&#20986;&#29256;&#29289;&#21644;&#20849;&#20139;&#20219;&#21153;&#20063;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;&#22240;&#27492;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#35780;&#20272;&#36825;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#31995;&#32479;&#30340;&#29305;&#23450;&#26041;&#38754;&#34987;&#35780;&#20272;&#65292;&#24182;&#19988;&#24448;&#24448;&#26080;&#27861;&#27604;&#36739;&#19981;&#21516;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#25191;&#34892;&#21644;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#21508;&#20010;&#26041;&#38754;&#30340;&#26694;&#26550;&#12290;&#35780;&#20272;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36827;&#34892;&#30340;&#65292;&#21253;&#25324;&#38899;&#39057;&#30340;&#20998;&#27573;&#20197;&#21450;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#30340;&#36816;&#34892;&#26102;&#38388;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#26694;&#26550;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20855;&#26377;&#20462;&#35746;&#36755;&#20986;&#36873;&#39033;&#30340;&#27169;&#22411;&#20197;&#21450;&#20855;&#26377;&#22266;&#23450;&#36755;&#20986;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30452;&#25509;&#27604;&#36739;&#20102;&#26368;&#20808;&#36827;&#30340;&#32423;&#32852;&#31995;&#32479;&#21644;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#26368;&#21518;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#19968;&#20010;&#32479;&#19968;&#30340;&#24230;&#37327;&#26469;&#35780;&#20272;&#20302;&#24310;&#36831;&#35821;&#38899;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches.  In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components.  Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework all
&lt;/p&gt;</description></item><item><title>"Baby's CoThought" &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#32452;&#25968;&#25454;&#35757;&#32451;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;10&#20010;&#35821;&#35328;&#23398;&#12289;NLU&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;BabyLM&#30340;&#34920;&#29616;&#36229;&#36807;RoBERTa-base&#36229;&#36807;3&#20010;&#28857;&#65292;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.01684</link><description>&lt;p&gt;
Baby's CoThought: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#39640;&#32039;&#20945;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Baby's CoThought: Leveraging Large Language Models for Enhanced Reasoning in Compact Models. (arXiv:2308.01684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01684
&lt;/p&gt;
&lt;p&gt;
"Baby's CoThought" &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#32452;&#25968;&#25454;&#35757;&#32451;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#32463;&#36807;&#35780;&#20272;&#21457;&#29616;&#65292;&#22312;10&#20010;&#35821;&#35328;&#23398;&#12289;NLU&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;BabyLM&#30340;&#34920;&#29616;&#36229;&#36807;RoBERTa-base&#36229;&#36807;3&#20010;&#28857;&#65292;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#25552;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;(NLU)&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;"CoThought"&#27969;&#27700;&#32447;&#21033;&#29992;LLMs&#30340;CoT&#25552;&#31034;&#65292;&#39640;&#25928;&#22320;&#35757;&#32451;&#36739;&#23567;&#30340;"baby"&#35821;&#35328;&#27169;&#22411;(BabyLMs)&#12290;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#23545;&#23569;&#20110;100M&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#37325;&#32452;&#65292;&#23558;&#20854;&#36716;&#21270;&#20026;&#38754;&#21521;&#20219;&#21153;&#30340;&#12289;&#21487;&#35835;&#24615;&#24378;&#30340;&#25991;&#26412;&#65292;&#31867;&#20284;&#20110;&#35821;&#35328;&#23398;&#20064;&#32773;&#30340;&#23398;&#26657;&#25945;&#26448;&#12290;&#28982;&#21518;&#65292;&#22312;&#36825;&#20010;&#37325;&#32452;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#20197;RoBERTa(Liu&#31561;&#20154;&#65292;2019)&#30340;&#26041;&#24335;&#23545;BabyLM&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#22312;4&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;BabyLM&#22312;10&#20010;&#35821;&#35328;&#23398;&#12289;NLU&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;RoBERTa-base&#36229;&#36807;3&#20010;&#28857;&#65292;&#23637;&#29616;&#20986;&#26356;&#22909;&#30340;&#25552;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23567;&#22411;&#30340;&#12289;&#30001;LLM&#37325;&#32452;&#30340;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#32039;&#20945;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#24182;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate remarkable performance on a variety of Natural Language Understanding (NLU) tasks, primarily due to their in-context learning ability. This ability is utilized in our proposed "CoThought" pipeline, which efficiently trains smaller "baby" language models (BabyLMs) by leveraging the Chain of Thought (CoT) prompting of LLMs. Our pipeline restructures a dataset of less than 100M in size using GPT-3.5-turbo, transforming it into task-oriented, human-readable texts that are comparable to the school texts for language learners. The BabyLM is then pretrained on this restructured dataset in a RoBERTa (Liu et al., 2019) fashion. In evaluations across 4 benchmarks, our BabyLM outperforms the RoBERTa-base in 10 linguistic, NLU, and question answering tasks by more than 3 points, showing superior ability to extract contextual information. These results suggest that compact LMs pretrained on small, LLM-restructured data can better understand tasks and achieve
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Trie&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#30701;&#21069;&#32512;&#21644;&#26410;&#35265;&#21069;&#32512;&#30340;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26597;&#35810;&#30340;&#27969;&#34892;&#24230;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.15455</link><description>&lt;p&gt;
&#22522;&#20110;Trie&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31639;&#27861;&#65292;&#20197;&#25552;&#21319;&#23545;&#30701;&#21069;&#32512;&#21644;&#26410;&#35265;&#21069;&#32512;&#30340;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Trie-NLG: Trie Context Augmentation to Improve Personalized Query Auto-Completion for Short and Unseen Prefixes. (arXiv:2307.15455v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15455
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Trie&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;&#31639;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#30701;&#21069;&#32512;&#21644;&#26410;&#35265;&#21069;&#32512;&#30340;&#38382;&#39064;&#65292;&#24182;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26597;&#35810;&#30340;&#27969;&#34892;&#24230;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#33258;&#21160;&#34917;&#20840;(QAC)&#26088;&#22312;&#20026;&#32473;&#23450;&#30340;&#26597;&#35810;&#21069;&#32512;&#25552;&#20379;&#21512;&#29702;&#30340;&#34917;&#20840;&#24314;&#35758;&#12290;&#20256;&#32479;&#30340;QAC&#31995;&#32479;&#21033;&#29992;&#21382;&#21490;&#26597;&#35810;&#26085;&#24535;&#20013;&#30340;Trie&#25968;&#25454;&#32467;&#26500;&#26469;&#25552;&#20379;&#26368;&#21463;&#27426;&#36814;&#30340;&#34917;&#20840;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20219;&#20309;QAC&#31995;&#32479;&#26469;&#35828;&#65292;&#26377;&#20004;&#31181;&#29305;&#23450;&#30340;&#22330;&#26223;&#24456;&#38590;&#22788;&#29702;&#65306;&#30701;&#21069;&#32512;(&#26412;&#36136;&#19978;&#23384;&#22312;&#27495;&#20041;)&#21644;&#26410;&#35265;&#21069;&#32512;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#20010;&#24615;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;(NLG)&#27169;&#22411;&#65292;&#21033;&#29992;&#21069;&#19968;&#20010;&#20250;&#35805;&#30340;&#26597;&#35810;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;NLG&#27169;&#22411;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(1)&#21069;&#36848;&#20250;&#35805;&#26597;&#35810;&#21487;&#33021;&#19982;&#24403;&#21069;&#21069;&#32512;&#30340;&#29992;&#25143;&#24847;&#22270;&#26080;&#20851;&#19988;&#21253;&#21547;&#22122;&#22768;&#65307;(2)NLG&#27169;&#22411;&#26080;&#27861;&#30452;&#25509;&#34701;&#21512;&#21382;&#21490;&#26597;&#35810;&#30340;&#27969;&#34892;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;QAC&#31639;&#27861;Trie-NLG&#65292;&#35813;&#31639;&#27861;&#21516;&#26102;&#21033;&#29992;Trie&#20013;&#30340;&#27969;&#34892;&#24230;&#20449;&#24687;&#21644;&#21069;&#19968;&#20010;&#20250;&#35805;&#26597;&#35810;&#20013;&#30340;&#20010;&#24615;&#21270;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query auto-completion (QAC) aims at suggesting plausible completions for a given query prefix. Traditionally, QAC systems have leveraged tries curated from historical query logs to suggest most popular completions. In this context, there are two specific scenarios that are difficult to handle for any QAC system: short prefixes (which are inherently ambiguous) and unseen prefixes. Recently, personalized Natural Language Generation (NLG) models have been proposed to leverage previous session queries as context for addressing these two challenges. However, such NLG models suffer from two drawbacks: (1) some of the previous session queries could be noisy and irrelevant to the user intent for the current prefix, and (2) NLG models cannot directly incorporate historical query popularity. This motivates us to propose a novel NLG model for QAC, Trie-NLG, which jointly leverages popularity signals from trie and personalization signals from previous session queries. We train the Trie-NLG model b
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.10700</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22609;&#36896;&#24182;&#21463;&#21040;&#31038;&#20250;&#30340;&#24433;&#21709;&#65306;arXiv&#20986;&#29256;&#27169;&#24335;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large language models shape and are shaped by society: A survey of arXiv publication patterns. (arXiv:2307.10700v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10700
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#24613;&#21095;&#22686;&#21152;&#65292;&#30740;&#31350;&#37325;&#28857;&#36880;&#28176;&#36716;&#21521;&#31038;&#20250;&#24433;&#21709;&#12290;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#21576;&#29616;&#25345;&#32493;&#22686;&#38271;&#30340;&#36235;&#21183;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#27880;&#37325;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35770;&#25991;&#25968;&#37327;&#36817;&#24180;&#26469;&#21576;&#24613;&#21095;&#22686;&#21152;&#65292;&#36825;&#31181;&#21464;&#21270;&#23545;&#31185;&#23398;&#39046;&#22495;&#20135;&#29983;&#20102;&#25103;&#21095;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#30446;&#21069;&#36824;&#27809;&#26377;&#36827;&#34892;&#35814;&#32454;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;CS&#21644;Stat arXiv&#19978;&#21457;&#24067;&#30340;388K&#31687;&#35770;&#25991;&#65292;&#24182;&#37325;&#28857;&#20851;&#27880;2023&#24180;&#19982;2018-2022&#24180;&#20043;&#38388;&#21457;&#34920;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#35770;&#25991;&#30340;&#27604;&#20363;&#22686;&#21152;&#24773;&#20917;&#65292;&#24471;&#21040;&#20102;&#26368;&#22810;&#20851;&#27880;&#30340;&#19982;LLM&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#25776;&#20889;LLM&#35770;&#25991;&#30340;&#20316;&#32773;&#65292;&#20316;&#32773;&#30340;&#30740;&#31350;&#20027;&#39064;&#19982;&#32972;&#26223;&#30340;&#30456;&#20851;&#24615;&#65292;&#21306;&#20998;&#39640;&#34987;&#24341;&#29992;LLM&#35770;&#25991;&#30340;&#22240;&#32032;&#65292;&#20197;&#21450;&#22269;&#38469;&#21512;&#20316;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#30740;&#31350;&#36234;&#26469;&#36234;&#20851;&#27880;&#31038;&#20250;&#24433;&#21709;&#65306;&#22312;&#35745;&#31639;&#26426;&#19982;&#31038;&#20250;&#23376;arXiv&#19978;&#65292;&#19982;LLM&#30456;&#20851;&#30340;&#35770;&#25991;&#27604;&#20363;&#22686;&#21152;&#20102;18&#20493;&#65292;&#26032;&#21457;&#34920;&#20851;&#20110;LLM&#30340;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20851;&#27880;&#24212;&#29992;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;LLM&#30740;&#31350;&#20063;&#21463;&#21040;&#31038;&#20250;&#21160;&#24577;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a steep recent increase in the number of large language model (LLM) papers, producing a dramatic shift in the scientific landscape which remains largely undocumented through bibliometric analysis. Here, we analyze 388K papers posted on the CS and Stat arXivs, focusing on changes in publication patterns in 2023 vs. 2018-2022. We analyze how the proportion of LLM papers is increasing; the LLM-related topics receiving the most attention; the authors writing LLM papers; how authors' research topics correlate with their backgrounds; the factors distinguishing highly cited LLM papers; and the patterns of international collaboration. We show that LLM research increasingly focuses on societal impacts: there has been an 18x increase in the proportion of LLM-related papers on the Computers and Society sub-arXiv, and authors newly publishing on LLMs are more likely to focus on applications and societal impacts than more experienced authors. LLM research is also shaped by social dyn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.09384</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Query Reformulation for Conversational Search. (arXiv:2307.09384v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09384
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#12289;&#35299;&#37322;&#24615;&#19981;&#36275;&#21644;&#27495;&#20041;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#38899;&#21161;&#25163;&#30340;&#26222;&#21450;&#65292;&#23545;&#35805;&#25628;&#32034;&#22312;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#24341;&#36215;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23545;&#35805;&#25628;&#32034;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#20005;&#37325;&#38459;&#30861;&#20102;&#30417;&#30563;&#24335;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#26356;&#21152;&#20851;&#27880;&#38646;&#26679;&#26412;&#23545;&#35805;&#25628;&#32034;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;&#23384;&#22312;&#19977;&#20010;&#20027;&#35201;&#38480;&#21046;&#65306;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#30340;&#26816;&#32034;&#22120;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#32570;&#20047;&#36275;&#22815;&#30340;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#20182;&#20204;&#26080;&#27861;&#35299;&#20915;&#22240;&#30465;&#30053;&#32780;&#23548;&#33268;&#30340;&#24120;&#35265;&#23545;&#35805;&#27495;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#26597;&#35810;&#37325;&#26500;&#65288;ZeQR&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26681;&#25454;&#20808;&#21069;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#37325;&#26500;&#26597;&#35810;&#65292;&#32780;&#26080;&#38656;&#23545;&#35805;&#25628;&#32034;&#25968;&#25454;&#30340;&#30417;&#30563;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#20102;&#35774;&#35745;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#26126;&#30830;&#35299;&#20915;&#20004;&#20010;&#24120;&#35265;&#30340;&#27495;&#20041;&#65306;&#21327;&#35843;&#21644;&#30465;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the popularity of voice assistants continues to surge, conversational search has gained increased attention in Information Retrieval. However, data sparsity issues in conversational search significantly hinder the progress of supervised conversational search methods. Consequently, researchers are focusing more on zero-shot conversational search approaches. Nevertheless, existing zero-shot methods face three primary limitations: they are not universally applicable to all retrievers, their effectiveness lacks sufficient explainability, and they struggle to resolve common conversational ambiguities caused by omission. To address these limitations, we introduce a novel Zero-shot Query Reformulation (ZeQR) framework that reformulates queries based on previous dialogue contexts without requiring supervision from conversational search data. Specifically, our framework utilizes language models designed for machine reading comprehension tasks to explicitly resolve two common ambiguities: cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.17020</link><description>&lt;p&gt;
&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21028;&#20915;&#25991;&#20214;&#23545;&#29359;&#32618;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classifying Crime Types using Judgment Documents from Social Media. (arXiv:2306.17020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26469;&#36171;&#20104;&#27169;&#22411;&#23545;&#23567;&#25968;&#25454;&#38598;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29359;&#32618;&#34892;&#20026;&#20107;&#23454;&#26469;&#30830;&#23450;&#29359;&#32618;&#31867;&#22411;&#30340;&#20219;&#21153;&#22312;&#31038;&#20250;&#31185;&#23398;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#21644;&#26377;&#24847;&#20041;&#12290;&#20294;&#35813;&#39046;&#22495;&#38754;&#20020;&#30340;&#38382;&#39064;&#26159;&#65292;&#30001;&#20110;&#29359;&#32618;&#26412;&#36523;&#30340;&#24615;&#36136;&#65292;&#25968;&#25454;&#26679;&#26412;&#26412;&#36523;&#20998;&#24067;&#19981;&#22343;&#21248;&#12290;&#21516;&#26102;&#65292;&#21496;&#27861;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#23569;&#26377;&#20844;&#24320;&#21487;&#29992;&#65292;&#26080;&#27861;&#20135;&#29983;&#29992;&#20110;&#30452;&#25509;&#35757;&#32451;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;NLP&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26032;&#30340;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#29359;&#32618;&#20107;&#23454;&#25968;&#25454;&#39044;&#22788;&#29702;&#27169;&#22359;(CFDPM)&#65292;&#36890;&#36807;&#29983;&#25104;&#26032;&#26679;&#26412;&#26469;&#24179;&#34913;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#32570;&#38519;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;(CAIL-big)&#20316;&#20026;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#25105;&#20204;&#33258;&#24049;&#25910;&#38598;&#30340;&#19968;&#20010;&#23567;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#36171;&#20104;&#27169;&#22411;&#23545;&#19981;&#29087;&#24713;&#30340;&#23567;&#25968;&#25454;&#38598;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20351;&#29992;&#25913;&#36827;&#30340;Bert&#27169;&#22411;&#21644;&#21160;&#24577;&#36974;&#34109;&#26469;&#25913;&#36827;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
The task of determining crime types based on criminal behavior facts has become a very important and meaningful task in social science. But the problem facing the field now is that the data samples themselves are unevenly distributed, due to the nature of the crime itself. At the same time, data sets in the judicial field are less publicly available, and it is not practical to produce large data sets for direct training. This article proposes a new training model to solve this problem through NLP processing methods. We first propose a Crime Fact Data Preprocessing Module (CFDPM), which can balance the defects of uneven data set distribution by generating new samples. Then we use a large open source dataset (CAIL-big) as our pretraining dataset and a small dataset collected by ourselves for Fine-tuning, giving it good generalization ability to unfamiliar small datasets. At the same time, we use the improved Bert model with dynamic masking to improve the model. Experiments show that the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;FUTEX&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;&#65307;&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.14003</link><description>&lt;p&gt;
&#20840;&#25991;&#31185;&#25216;&#35770;&#25991;&#30340;&#24369;&#30417;&#30563;&#22810;&#26631;&#31614;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Multi-Label Classification of Full-Text Scientific Papers. (arXiv:2306.14003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#22810;&#26631;&#31614;&#20998;&#31867;&#26694;&#26550;FUTEX&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#35299;&#20915;&#20102;&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;&#65307;&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#30340;&#31185;&#25216;&#35770;&#25991;&#20998;&#31867;&#20381;&#36182;&#20110;&#20998;&#31867;&#25551;&#36848;&#32780;&#38750;&#20154;&#24037;&#26631;&#27880;&#26679;&#26412;&#24314;&#31435;&#20998;&#31867;&#22120;&#12290;&#24050;&#26377;&#30340;&#24369;&#30417;&#30563;&#20998;&#31867;&#30740;&#31350;&#36739;&#23569;&#32771;&#34385;&#21040;&#20004;&#20010;&#25361;&#25112;&#65306;(1)&#22312;&#32454;&#31890;&#24230;&#26631;&#31614;&#31354;&#38388;&#20013;&#23558;&#35770;&#25991;&#20998;&#31867;&#20026;&#30740;&#31350;&#20027;&#39064;&#21644;&#20027;&#39064;&#65292;&#21487;&#33021;&#26377;&#22810;&#20010;; (2)&#24212;&#21033;&#29992;&#20840;&#25991;&#26469;&#34917;&#20805;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#24212;&#21033;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#35770;&#25991;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#31561;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FUTEX&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#36328;&#35770;&#25991;&#32593;&#32476;&#32467;&#26500;&#21644;&#21508;&#25237;&#31295;&#20869;&#37096;&#20998;&#31456;&#33410;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of relying on human-annotated training samples to build a classifier, weakly supervised scientific paper classification aims to classify papers only using category descriptions (e.g., category names, category-indicative keywords). Existing studies on weakly supervised paper classification are less concerned with two challenges: (1) Papers should be classified into not only coarse-grained research topics but also fine-grained themes, and potentially into multiple themes, given a large and fine-grained label space; and (2) full text should be utilized to complement the paper title and abstract for classification. Moreover, instead of viewing the entire paper as a long linear sequence, one should exploit the structural information such as citation links across papers and the hierarchy of sections and paragraphs in each paper. To tackle these challenges, in this study, we propose FUTEX, a framework that uses the cross-paper network structure and the in-paper hierarchy structure to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.13089</link><description>&lt;p&gt;
GIMLET&#65306;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#25351;&#20196;&#20998;&#23376;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning. (arXiv:2306.13089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GIMLET&#30340;&#32479;&#19968;&#22270;&#25991;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#19981;&#36275;&#21644;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20351;&#29992;GIMLET&#33021;&#22815;&#22686;&#24378;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30001;&#20110;&#26114;&#36149;&#30340;&#23454;&#39564;&#36896;&#25104;&#30340;&#26631;&#31614;&#19981;&#36275;&#38382;&#39064;&#23558;&#26159;&#20854;&#20027;&#35201;&#29942;&#39048;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25991;&#26412;&#30693;&#35782;&#36827;&#34892;&#20219;&#21153;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#20998;&#23376;&#30456;&#20851;&#20219;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#20998;&#23376;-&#25991;&#26412;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#21407;&#22240;&#26159;&#22788;&#29702;&#25351;&#20196;&#19981;&#36275;&#20197;&#21450;&#22270;&#24418;&#23481;&#37327;&#26377;&#38480;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIMLET&#65292;&#23427;&#32479;&#19968;&#20102;&#22270;&#24418;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#37319;&#29992;&#24191;&#20041;&#20301;&#32622;&#23884;&#20837;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34987;&#25193;&#23637;&#20197;&#32534;&#30721;&#22270;&#24418;&#32467;&#26500;&#21644;&#25351;&#20196;&#25991;&#26412;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#22270;&#24418;&#32534;&#30721;&#27169;&#22359;&#12290;GIMLET&#36824;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#35299;&#32806;&#20102;&#22270;&#24418;&#30340;&#32534;&#30721;&#21644;&#20219;&#21153;&#25351;&#20196;&#65292;&#22686;&#24378;&#20102;&#36328;&#26032;&#20219;&#21153;&#30340;&#22270;&#24418;&#29305;&#24449;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs. To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.06815</link><description>&lt;p&gt;
TrojPrompt&#65306;&#22522;&#20110;&#40657;&#30418;&#26041;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26408;&#39532;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TrojPrompt: A Black-box Trojan Attack on Pre-trained Language Models. (arXiv:2306.06815v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21019;&#24615;&#22320;&#30740;&#31350;&#20102;&#22522;&#20110; prompt &#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; API &#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#40657;&#30418;&#26694;&#26550;&#8212;&#8212;TrojPrompt&#65292;&#29992;&#20110;&#29983;&#25104;&#36890;&#29992;&#21644;&#38544;&#34109;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#23398;&#20064;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36866;&#24212;&#24615;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;&#24182;&#22312;&#19987;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#37327;&#36523;&#23450;&#21046;&#30340;&#24212;&#29992;&#31243;&#24207;&#21644;API&#20013;&#23637;&#29616;&#20102;&#26480;&#20986;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#65292;&#23613;&#31649;prompt&#23398;&#20064;&#30340;API&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23427;&#20204;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#22312;prompt&#23398;&#20064;&#30340;PLM API&#30340;&#29305;&#27931;&#20234;&#26131;&#24863;&#24615;&#26041;&#38754;&#36827;&#34892;&#20102;&#24320;&#21019;&#24615;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#31163;&#25955;&#25552;&#31034;&#65292;&#23569;&#26679;&#26412;&#21644;&#40657;&#30418;&#35774;&#32622;&#26159;&#20960;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#21518;&#38376;&#25915;&#20987;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrojPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#21160;&#30340;&#40657;&#30418;&#26694;&#26550;&#65292;&#21487;&#26377;&#25928;&#29983;&#25104;&#36890;&#29992;&#30340;&#21644;&#38544;&#31192;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;&#30828;&#25552;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;API&#39537;&#21160;&#30340;&#36890;&#29992;&#35302;&#21457;&#22120;&#21457;&#29616;&#31639;&#27861;&#65292;&#36890;&#36807;&#26597;&#35810;&#21463;&#23475;&#32773;PLM API&#65292;&#20026;&#21508;&#31181;&#36755;&#20837;&#29983;&#25104;&#36890;&#29992;&#35302;&#21457;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning has been proven to be highly effective in improving pre-trained language model (PLM) adaptability, surpassing conventional fine-tuning paradigms, and showing exceptional promise in an ever-growing landscape of applications and APIs tailored for few-shot learning scenarios. Despite the growing prominence of prompt learning-based APIs, their security concerns remain underexplored. In this paper, we undertake a pioneering study on the Trojan susceptibility of prompt-learning PLM APIs. We identified several key challenges, including discrete-prompt, few-shot, and black-box settings, which limit the applicability of existing backdoor attacks. To address these challenges, we propose TrojPrompt, an automatic and black-box framework to effectively generate universal and stealthy triggers and insert Trojans into hard prompts. Specifically, we propose a universal API-driven trigger discovery algorithm for generating universal triggers for various inputs by querying victim PLM API
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102; CMExam &#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#12289;&#26469;&#33258;&#20110;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#23458;&#35266;&#30340;&#26041;&#27861;&#12290;&#22312; CMExam &#19978;&#65292;GPT-4 &#34920;&#29616;&#26368;&#22909;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.03030</link><description>&lt;p&gt;
&#22522;&#20110; CMExam &#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#27979;&#8212;&#8212;&#19968;&#20221;&#32508;&#21512;&#30340;&#20013;&#22269;&#21307;&#23398;&#32771;&#35797;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese Medical Exam Dataset. (arXiv:2306.03030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03030
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102; CMExam &#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#12289;&#26469;&#33258;&#20110;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#26631;&#20934;&#21270;&#19988;&#23458;&#35266;&#30340;&#26041;&#27861;&#12290;&#22312; CMExam &#19978;&#65292;GPT-4 &#34920;&#29616;&#26368;&#22909;&#65292;&#36825;&#34920;&#26126;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#30340;&#36827;&#27493;&#24050;&#32463;&#25913;&#21464;&#20102;&#38382;&#31572;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#20840;&#38754;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#23545; LLM &#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; CMExam&#65292;&#23427;&#26469;&#33258;&#20013;&#22269;&#22269;&#23478;&#21307;&#30103;&#25191;&#19994;&#32771;&#35797;&#65292;&#30001;60,000&#22810;&#20010;&#36873;&#25321;&#39064;&#21644;&#27169;&#22411;&#25512;&#29702;&#35299;&#37322;&#30340;&#31572;&#26696;&#35299;&#26512;&#26500;&#25104;&#65292;&#21487;&#36827;&#34892;&#26631;&#20934;&#21270;&#21644;&#23458;&#35266;&#21270;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#28145;&#20837;&#20998;&#26512; LLM&#65292;&#25105;&#20204;&#36992;&#35831;&#21307;&#23398;&#19987;&#19994;&#20154;&#22763;&#23545;&#20116;&#20010;&#39069;&#22806;&#30340;&#38382;&#39064;&#36880;&#20010;&#36827;&#34892;&#26631;&#27880;&#65292;&#21253;&#25324;&#30142;&#30149;&#32452;&#12289;&#20020;&#24202;&#31185;&#23460;&#12289;&#21307;&#23398;&#23398;&#31185;&#12289;&#33021;&#21147;&#39046;&#22495;&#21644;&#38590;&#24230;&#32423;&#21035;&#12290;&#38500;&#20102;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312; CMExam &#19978;&#23545;&#20195;&#34920;&#24615;&#30340; LLM &#21644; QA &#31639;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4 &#30340;&#20934;&#30830;&#24230;&#26368;&#39640;&#65292;&#20026;61.6&#65285;&#65292;&#21152;&#26435; F1 &#20998;&#25968;&#20026;0.617&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#24040;&#22823;&#28508;&#21147;&#20197;&#21450;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#23545;&#35780;&#20272;&#20854;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.18396</link><description>&lt;p&gt;
LLM&#21487;&#20197;&#29702;&#35299;&#21152;&#23494;&#25552;&#31034;&#65306;&#38754;&#21521;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;Transformers
&lt;/p&gt;
&lt;p&gt;
LLMs Can Understand Encrypted Prompt: Towards Privacy-Computing Friendly Transformers. (arXiv:2305.18396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20351;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26041;&#27861;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#65292;&#23454;&#29616;&#20102;&#22823;&#24133;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#35745;&#31639;&#21152;&#36895;&#21644;&#36890;&#20449;&#24320;&#38144;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#22312;&#26381;&#21153;&#22120;&#23458;&#25143;&#31471;&#29615;&#22659;&#20013;&#20026;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26500;&#24314;&#31169;&#26377;&#25512;&#26029;&#26694;&#26550;&#65292;&#20854;&#20013;&#26381;&#21153;&#22120;&#25345;&#26377;&#27169;&#22411;&#21442;&#25968;&#65292;&#23458;&#25143;&#31471;&#36755;&#20837;&#31169;&#26377;&#25968;&#25454;&#36827;&#34892;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#24403;&#31169;&#26377;&#36755;&#20837;&#36890;&#36807;&#21407;&#22987;LLMs&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#26102;&#65292;&#36825;&#20123;&#26694;&#26550;&#20250;&#20135;&#29983;&#26174;&#30528;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#36817;&#20284;&#26367;&#25442;transformer&#26550;&#26500;&#20013;&#35745;&#31639;&#21644;&#36890;&#20449;&#23494;&#38598;&#30340;&#36816;&#31639;&#31526;&#21487;&#20197;&#22823;&#22823;&#38477;&#20302;&#31169;&#26377;&#25512;&#26029;&#25104;&#26412;&#65292;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#24494;&#20046;&#20854;&#24494;&#12290;&#19982;&#26368;&#26032;&#30340;Iron&#65288;NeurIPS 2022&#65289;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#38544;&#31169;&#35745;&#31639;&#21451;&#22909;&#30340;&#27169;&#22411;&#25512;&#26029;&#31649;&#36947;&#22312;&#35745;&#31639;&#19978;&#23454;&#29616;&#20102;$5 \times$&#30340;&#21152;&#36895;&#65292;&#22312;&#36890;&#20449;&#24320;&#38144;&#19978;&#23454;&#29616;&#20102;80\%&#30340;&#38477;&#20302;&#65292;&#21516;&#26102;&#20960;&#20046;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior works have attempted to build private inference frameworks for transformer-based large language models (LLMs) in a server-client setting, where the server holds the model parameters and the client inputs the private data for inference. However, these frameworks impose significant overhead when the private inputs are forward propagated through the original LLMs. In this paper, we show that substituting the computation- and communication-heavy operators in the transformer architecture with privacy-computing friendly approximations can greatly reduce the private inference costs with minor impact on model performance. Compared to the state-of-the-art Iron (NeurIPS 2022), our privacy-computing friendly model inference pipeline achieves a $5\times$ acceleration in computation and an 80\% reduction in communication overhead, while retaining nearly identical accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#38598;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#37327;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#26469;&#32553;&#23567;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#19982;&#26377;&#26631;&#31614;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20174;&#32780;&#22312;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#21644;&#37197;&#23545;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18287</link><description>&lt;p&gt;
LaFTer: &#20351;&#29992;&#35821;&#35328;&#21644;&#26080;&#26631;&#31614;&#22270;&#20687;&#38598;&#36827;&#34892;&#26080;&#26631;&#31614;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
LaFTer: Label-Free Tuning of Zero-shot Classifier using Language and Unlabeled Image Collections. (arXiv:2305.18287v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#22270;&#20687;&#38598;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#22823;&#37327;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#26469;&#32553;&#23567;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#19982;&#26377;&#26631;&#31614;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#20174;&#32780;&#22312;&#26080;&#38656;&#20219;&#20309;&#26631;&#31614;&#21644;&#37197;&#23545;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#35270;&#35273;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#65288;SOTA&#65289;&#30340;&#25104;&#26524;&#65292;&#23454;&#29616;&#20102;&#20351;&#29992;&#31616;&#21333;&#35821;&#35328;&#25552;&#31034;&#23450;&#20041;&#30340;&#28508;&#22312;&#26080;&#38480;&#31867;&#21035;&#30340;&#24320;&#25918;&#35789;&#27719;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#36825;&#20123;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#20173;&#28982;&#19981;&#21450;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#35757;&#32451;&#30340;&#19987;&#29992;&#65288;&#23553;&#38381;&#31867;&#21035;&#38598;&#65289;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#21644;&#20219;&#20309;&#37197;&#23545;&#30340;VL&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#32553;&#23567;&#36825;&#20010;&#24046;&#36317;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#26631;&#31614;&#22270;&#20687;&#38598;&#21644;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#19968;&#32452;&#25551;&#36848;&#24863;&#20852;&#36259;&#31867;&#21035;&#30340;&#25991;&#26412;&#65292;&#26377;&#25928;&#22320;&#26367;&#20195;&#37027;&#20123;&#31867;&#21035;&#30340;&#26631;&#35760;&#35270;&#35273;&#23454;&#20363;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26080;&#26631;&#31614;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#22522;&#26412;VL&#27169;&#22411;&#21644;&#20854;&#20182;&#24403;&#20195;&#26041;&#27861;&#21644;&#22522;&#20934;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale pre-trained Vision and Language (VL) models have set a new state-of-the-art (SOTA) in zero-shot visual classification enabling open-vocabulary recognition of potentially unlimited set of categories defined as simple language prompts. However, despite these great advances, the performance of these zeroshot classifiers still falls short of the results of dedicated (closed category set) classifiers trained with supervised fine tuning. In this paper we show, for the first time, how to reduce this gap without any labels and without any paired VL data, using an unlabeled image collection and a set of texts auto-generated using a Large Language Model (LLM) describing the categories of interest and effectively substituting labeled visual instances of those categories. Using our label-free approach, we are able to attain significant performance improvements over the zero-shot performance of the base VL model and other contemporary methods and baselines on a wide variety of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26631;&#37327;&#21103;&#35789;&#65292;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#8220;&#32477;&#23545;&#8221;&#19982;&#8220;&#30456;&#23545;&#8221;&#35789;&#30340;&#34920;&#29616;&#65292;&#22312;&#28041;&#21450;&#36923;&#36753;&#25512;&#29702;&#30340;NLP&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.16426</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#32477;&#23545;&#8221;&#19982;&#8220;&#30456;&#23545;&#8221;&#21103;&#35789;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Not wacky vs. definitely wacky: A study of scalar adverbs in pretrained language models. (arXiv:2305.16426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26631;&#37327;&#21103;&#35789;&#65292;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#8220;&#32477;&#23545;&#8221;&#19982;&#8220;&#30456;&#23545;&#8221;&#35789;&#30340;&#34920;&#29616;&#65292;&#22312;&#28041;&#21450;&#36923;&#36753;&#25512;&#29702;&#30340;NLP&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20041;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#20551;&#35774;&#20986;&#29616;&#22312;&#30456;&#20284;&#35821;&#22659;&#20013;&#30340;&#35789;&#20855;&#26377;&#30456;&#20284;&#30340;&#21547;&#20041;&#12290;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#31867;&#20284;&#20110;&#20027;&#39064;&#20851;&#32852;&#20294;&#22312;&#36923;&#36753;&#21147;&#24230;&#19978;&#19981;&#21516;&#30340;&#35789;&#24448;&#24448;&#34987;&#35270;&#20026;&#35821;&#20041;&#19978;&#30456;&#20284;&#65292;&#36825;&#23545;&#28041;&#21450;&#36923;&#36753;&#25512;&#29702;&#30340;NLP&#24212;&#29992;&#36896;&#25104;&#20102;&#26222;&#36941;&#25361;&#25112;&#12290;&#20851;&#20110;&#29616;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#12289;RoBERTa&#21644;GPT-3&#65289;&#22312;&#36923;&#36753;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#25253;&#21578;&#23384;&#22312;&#28151;&#26434;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#30740;&#31350;&#26631;&#37327;&#21103;&#35789;&#65292;&#36825;&#26159;&#19968;&#31867;&#20855;&#26377;&#24378;&#28872;&#36923;&#36753;&#21147;&#24230;&#30340;&#35789;&#27719;&#65292;&#25512;&#36827;&#20102;&#36825;&#19968;&#35752;&#35770;&#12290;&#36890;&#36807;&#20351;&#29992;&#19977;&#39033;&#19981;&#21516;&#30340;&#20219;&#21153;&#65288;&#21253;&#25324;&#33258;&#28982;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26500;&#36896;&#30340;&#31034;&#20363;&#65289;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;BERT&#12289;RoBERTa&#12289;GPT-2&#21644;GPT-3&#22312;&#36825;&#20123;&#24120;&#35265;&#35789;&#35821;&#26041;&#38754;&#26159;&#21542;&#23637;&#29616;&#20986;&#19968;&#33324;&#30340;&#12289;&#31867;&#20154;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#38382;&#65306;1&#65289;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#21306;&#20998;&#36825;&#19977;&#20010;&#35821;&#20041;&#31867;&#22411;&#20013;&#30340;&#24046;&#24322;&#65311;
&lt;/p&gt;
&lt;p&gt;
Vector space models of word meaning all share the assumption that words occurring in similar contexts have similar meanings. In such models, words that are similar in their topical associations but differ in their logical force tend to emerge as semantically close, creating well-known challenges for NLP applications that involve logical reasoning. Modern pretrained language models, such as BERT, RoBERTa and GPT-3 hold the promise of performing better on logical tasks than classic static word embeddings. However, reports are mixed about their success. In the current paper, we advance this discussion through a systematic study of scalar adverbs, an under-explored class of words with strong logical force. Using three different tasks, involving both naturalistic social media data and constructed examples, we investigate the extent to which BERT, RoBERTa, GPT-2 and GPT-3 exhibit general, human-like, knowledge of these common words. We ask: 1) Do the models distinguish amongst the three sema
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16340</link><description>&lt;p&gt;
&#20998;&#27573;&#24490;&#29615;Transformer:&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segmented Recurrent Transformer: An Efficient Sequence-to-Sequence Model. (arXiv:2305.16340v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#26469;&#20943;&#23569;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#20351;&#29992;RAF&#23618;&#22788;&#29702;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#35270;&#35273;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24212;&#29992;&#20013;&#20351;&#29992;&#25104;&#20026;&#19981;&#21487;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#23558;&#25972;&#20010;&#24207;&#21015;&#21010;&#20998;&#25104;&#33509;&#24178;&#27573;&#12290;&#28982;&#21518;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#32467;&#26500;&#30340;&#31070;&#32463;&#20803;&#26469;&#32858;&#21512;&#36328;&#27573;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20855;&#26377;&#36739;&#20302;&#35745;&#31639;/&#20869;&#23384;&#25104;&#26412;&#30340;&#24207;&#21015;&#22788;&#29702;&#33021;&#21147;&#27169;&#22411;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;Attention&#26426;&#21046;&#23545;&#21333;&#20010;&#27573;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#27573;&#24490;&#29615;Transformer&#65288;SRformer&#65289;&#65292;&#23427;&#23558;&#20998;&#27573;Attention&#21644;&#24490;&#29615;Attention&#30456;&#32467;&#21512;&#12290;&#23427;&#20351;&#29992;&#24490;&#29615;accumulate and fire&#65288;RAF&#65289;&#23618;&#22312;&#30456;&#37051;&#27573;&#20043;&#38388;&#22788;&#29702;&#20449;&#24687;&#12290;&#36890;&#36807;&#26356;&#26032;key&#30340;&#20135;&#21697;&#26469;&#34917;&#20607;&#20943;&#23569;Attention&#31383;&#21475;&#38271;&#24230;&#20135;&#29983;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have shown dominant performance across a range of domains including language and vision. However, their computational cost grows quadratically with the sequence length, making their usage prohibitive for resource-constrained applications. To counter this, our approach is to divide the whole sequence into segments. The information across segments can then be aggregated using neurons with recurrence leveraging their inherent memory. Such an approach leads to models with sequential processing capability at a lower computation/memory cost. To investigate this idea, first, we examine the effects of using local attention mechanism on the individual segments. Then we propose a segmented recurrent transformer (SRformer) that combines segmented attention with recurrent attention. It uses recurrent accumulate and fire (RAF) layers to process information between consecutive segments. The loss caused by reducing the attention window length is compensated by updating the product of key
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21327;&#21516;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#25913;&#21892;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15294</link><description>&lt;p&gt;
&#29992;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#21327;&#21516;&#22686;&#24378;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy. (arXiv:2305.15294v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#21327;&#21516;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#25913;&#21892;&#20102;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25991;&#26412;&#22788;&#29702;&#22120;&#21644;&#25512;&#29702;&#22120;&#65292;&#20294;&#20173;&#28982;&#21463;&#21040;&#35832;&#22914;&#36807;&#26102;&#30693;&#35782;&#21644;&#24187;&#35273;&#31561;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;&#23558;&#23427;&#20204;&#19982;&#29616;&#23454;&#19990;&#30028;&#36830;&#25509;&#36215;&#26469;&#12290;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#20197;&#22312;&#22806;&#37096;&#30693;&#35782;&#19978;&#25171;&#19979;&#27169;&#22411;&#29983;&#25104;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#26816;&#32034;&#22120;&#24448;&#24448;&#38590;&#20197;&#25429;&#25417;&#21040;&#22797;&#26434;&#20449;&#24687;&#38656;&#27714;&#30340;&#30456;&#20851;&#24615;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#25552;&#20986;&#36890;&#36807;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31215;&#26497;&#21442;&#19982;&#26816;&#32034;&#26469;&#25913;&#21892;&#30456;&#20851;&#24615;&#24314;&#27169;&#65292;&#21363;&#36890;&#36807;&#29983;&#25104;&#26469;&#25913;&#21892;&#26816;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#36845;&#20195;&#26816;&#32034;-&#29983;&#25104;&#65288;Iter-RetGen&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#26041;&#24335;&#21327;&#21516;&#26816;&#32034;&#21644;&#29983;&#25104;&#21487;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#27169;&#22411;&#36755;&#20986;&#23637;&#31034;&#20102;&#23436;&#25104;&#20219;&#21153;&#25152;&#38656;&#30340;&#20869;&#23481;&#65292;&#22240;&#27492;&#20026;&#26816;&#32034;&#26356;&#30456;&#20851;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#20449;&#24687;&#19978;&#19979;&#25991;&#65292;&#36827;&#32780;&#26377;&#21161;&#20110;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#29983;&#25104;&#26356;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are powerful text processors and reasoners, but are still subject to limitations including outdated knowledge and hallucinations, which necessitates connecting them to the world. Retrieval-augmented large language models have raised extensive attention for grounding model generation on external knowledge. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to improve retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner. A model output shows what might be needed to finish a task, and thus provides an informative context for retrieving more relevant knowledge which in turn helps generate a better output in the next iteration. Compared with recent work which 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20998;&#37197;&#35821;&#35328;&#27169;&#22411;&#25439;&#22833;&#30340;&#26435;&#37325;&#65292;&#23454;&#29616;&#22312;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#24310;&#38271;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#26102;&#38388;&#65292;&#24182;&#25552;&#21319;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15076</link><description>&lt;p&gt;
Meta-Learning Online Adaptation of Language Models. &#65288;arXiv:2305.15076v2 [cs.CL] &#26356;&#26032;&#65289;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning Online Adaptation of Language Models. (arXiv:2305.15076v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20998;&#37197;&#35821;&#35328;&#27169;&#22411;&#25439;&#22833;&#30340;&#26435;&#37325;&#65292;&#23454;&#29616;&#22312;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#24310;&#38271;&#35821;&#35328;&#27169;&#22411;&#30340;&#26377;&#25928;&#26102;&#38388;&#65292;&#24182;&#25552;&#21319;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20854;&#21442;&#25968;&#20013;&#32534;&#30721;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24191;&#27867;&#19990;&#30028;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#38745;&#24577;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24456;&#24555;&#36807;&#26102;&#65292;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#26377;&#25928; "&#36135;&#26550;&#23551;&#21629;"&#12290;&#34429;&#28982;&#22312;&#32447;&#24494;&#35843;&#21487;&#20197;&#20943;&#36731;&#36825;&#31181;&#36864;&#21270;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#22320;&#22312;&#19968;&#31995;&#21015;&#25991;&#26723;&#19978;&#36827;&#34892;&#24494;&#35843;&#20250;&#23548;&#33268;&#20449;&#24687;&#21560;&#25910;&#27700;&#24179;&#36739;&#20302;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#32447;&#24494;&#35843;&#27809;&#26377;&#20805;&#20998;&#20851;&#27880;&#37325;&#35201;&#20449;&#24687;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#29992;&#20110;&#34920;&#31034;&#20107;&#23454;&#20449;&#24687;&#30340;&#37325;&#35201;&#26631;&#35760;&#30340;&#26799;&#24230;&#20449;&#21495;&#34987;&#20174;&#26412;&#36136;&#19978;&#22024;&#26434;&#30340;&#26631;&#35760;&#30340;&#26799;&#24230;&#28153;&#27809;&#65292;&#36825;&#34920;&#26126;&#21160;&#24577;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#23398;&#20064;&#29575;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#22914;&#20309;&#22686;&#21152;&#26435;&#37325;&#30340;&#26631;&#35760;&#12290;&#25105;&#20204;&#20803;&#35757;&#32451;&#19968;&#20010;&#23567;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#22312;&#22312;&#32447;&#24494;&#35843;&#36807;&#31243;&#20013;&#37325;&#26032;&#35843;&#25972;&#27599;&#20010;&#26631;&#35760;&#30340;&#35821;&#35328;&#27169;&#22411;&#25439;&#22833;&#65292;&#20854;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#36807;&#26102;&#30340;&#22522;&#30784;&#38382;&#31572;&#27169;&#22411;&#22312;&#21333;&#20010;&#21152;&#26435;&#26799;&#24230;&#20043;&#21518;&#23545;&#25991;&#26723;&#30340;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective "shelf life." While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15074</link><description>&lt;p&gt;
LLM&#20204;&#36827;&#27493;&#21040;&#20102;&#20160;&#20040;&#31243;&#24230;&#65311;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#22522;&#20934;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Have LLMs Advanced Enough? A Challenging Problem Solving Benchmark For Large Language Models. (arXiv:2305.15074v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15074
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#36890;&#36807;&#35780;&#20272;&#21508;&#31181;&#27169;&#22411;&#65292;&#32467;&#26524;&#26174;&#31034;&#30446;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#38382;&#39064;&#26102;&#23384;&#22312;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#25277;&#35937;&#27010;&#24565;&#36716;&#21270;&#19981;&#20934;&#30830;&#21644;&#38590;&#20197;&#26816;&#32034;&#30456;&#20851;&#27010;&#24565;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29616;&#26377;&#30340;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;JEEBench&#65292;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#39640;&#31454;&#20105;&#30340;&#21360;&#24230;&#29702;&#24037;&#23398;&#38498;&#65288;IIT&#65289;JEE-Advanced&#32771;&#35797;&#20013;&#31934;&#36873;&#20986;&#20102;515&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39044;&#24037;&#31243;&#25968;&#23398;&#12289;&#29289;&#29702;&#21644;&#21270;&#23398;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#22522;&#20934;&#20013;&#65292;&#38271;&#26399;&#25512;&#29702;&#21644;&#28145;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#36816;&#29992;&#23545;&#38382;&#39064;&#30340;&#35299;&#20915;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23545;&#21508;&#31181;&#24320;&#28304;&#21644;&#19987;&#26377;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;&#20351;&#29992;&#20102;&#33258;&#19968;&#33268;&#24615;&#12289;&#33258;&#25105;&#23436;&#21892;&#21644;&#24605;&#32500;&#38142;&#25552;&#31034;&#31561;&#25216;&#26415;&#65292;&#26368;&#39640;&#24615;&#33021;&#20063;&#19981;&#21040;40\%&#12290;&#26368;&#22909;&#30340;&#27169;&#22411;GPT-4&#30340;&#20856;&#22411;&#22833;&#36133;&#27169;&#24335;&#21253;&#25324;&#20195;&#25968;&#25805;&#20316;&#38169;&#35823;&#12289;&#23558;&#25277;&#35937;&#27010;&#24565;&#20934;&#30830;&#22320;&#36716;&#21270;&#20026;&#25968;&#23398;&#26041;&#31243;&#20197;&#21450;&#26080;&#27861;&#26816;&#32034;&#30456;&#20851;&#30340;&#39046;&#22495;&#29305;&#23450;&#27010;&#24565;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#20165;&#20165;&#36890;&#36807;&#36755;&#20837;&#25552;&#31034;&#19981;&#33021;&#35753;&#27169;&#22411;&#25104;&#21151;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of large language models (LLMs) on existing reasoning benchmarks has significantly improved over the past years. In response, we present JEEBench, a considerably more challenging benchmark dataset for evaluating the problem solving abilities of LLMs. We curate 515 challenging pre-engineering mathematics, physics and chemistry problems from the highly competitive IIT JEE-Advanced exam. Long-horizon reasoning on top of deep in-domain knowledge is essential for solving problems in this benchmark. Our evaluation on various open-source and proprietary models reveals that the highest performance, even after using techniques like self-consistency, self-refinement and chain-of-thought prompting, is less than 40\%. The typical failure modes of GPT-4, the best model, are errors in algebraic manipulation, difficulty in grounding abstract concepts into mathematical equations accurately and failure in retrieving relevant domain-specific concepts. We also observe that by mere prompti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPlan&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#25351;&#23548;&#20195;&#29702;&#23436;&#25104;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;LLM&#25552;&#31034;&#19982;&#20219;&#21153;&#35299;&#20915;&#35745;&#21010;&#30456;&#32467;&#21512;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;AutoPlan&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ALFWorld&#19978;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#28436;&#31034;&#22522;&#32447;&#30456;&#24403;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;HotpotQA&#19978;&#36229;&#36807;&#20102;8%&#12290;</title><link>http://arxiv.org/abs/2305.15064</link><description>&lt;p&gt;
&#33258;&#21160;&#35268;&#21010;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#30340;&#33258;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
AutoPlan: Automatic Planning of Interactive Decision-Making Tasks With Large Language Models. (arXiv:2305.15064v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15064
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPlan&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#25351;&#23548;&#20195;&#29702;&#23436;&#25104;&#22797;&#26434;&#30340;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;&#36890;&#36807;&#23558;LLM&#25552;&#31034;&#19982;&#20219;&#21153;&#35299;&#20915;&#35745;&#21010;&#30456;&#32467;&#21512;&#24182;&#36827;&#34892;&#20248;&#21270;&#65292;AutoPlan&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;ALFWorld&#19978;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#28436;&#31034;&#22522;&#32447;&#30456;&#24403;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#22312;HotpotQA&#19978;&#36229;&#36807;&#20102;8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23545;&#20110;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLM&#20013;&#39044;&#35757;&#32451;&#30693;&#35782;&#19982;&#23454;&#38469;&#29615;&#22659;&#35268;&#21017;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#65292;LLM&#22312;&#22797;&#26434;&#30340;&#20915;&#31574;&#20219;&#21153;&#20013;&#32463;&#24120;&#22833;&#36133;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#26799;&#24230;&#35745;&#31639;&#65292;&#35201;&#20040;&#38656;&#35201;&#32791;&#26102;&#30340;&#19978;&#19979;&#25991;&#28436;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AutoPlan&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23436;&#25104;&#20132;&#20114;&#24335;&#20915;&#31574;&#20219;&#21153;&#12290;AutoPlan&#36890;&#36807;&#36845;&#20195;&#30340;&#32463;&#39564;&#25910;&#38598;&#21644;&#21453;&#24605;&#65292;&#23558;LLM&#25552;&#31034;&#19982;&#20219;&#21153;&#35299;&#20915;&#35745;&#21010;&#30456;&#32467;&#21512;&#65292;&#24182;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#34429;&#28982;&#26410;&#20351;&#29992;&#19978;&#19979;&#25991;&#28436;&#31034;&#65292;&#20294;AutoPlan&#22312;ALFWorld&#19978;&#23454;&#29616;&#20102;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#28436;&#31034;&#22522;&#32447;&#30456;&#24403;&#30340;&#25104;&#21151;&#29575;&#65292;&#29978;&#33267;&#22312;HotpotQA&#19978;&#36229;&#36807;&#20102;8%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/owaski/AutoPlan&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) are promising for making decisions in grounded environments. However, LLMs frequently fail in complex decision-making tasks due to the misalignment between the pre-trained knowledge in LLMs and the actual rules in the environment. Existing methods require either costly gradient computation or lengthy in-context demonstrations. In this paper, we propose AutoPlan, an approach to guide LLM-based agents to accomplish interactive decision-making tasks. AutoPlan augments the LLM prompt with a task-solving plan and optimizes it through iterative experience collection and reflection. Our experiments show that AutoPlan, though using no in-context demonstrations, achieves success rates on par with the baselines using human-written demonstrations on ALFWorld and even outperforms them by 8% on HotpotQA. The code is available at https://github.com/owaski/AutoPlan.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#26159;&#21542;&#26159;&#19968;&#20010;&#20248;&#31168;&#30340;&#25968;&#25454;&#20998;&#26512;&#24072;&#65292;&#24182;&#36890;&#36807;&#38754;&#23545;&#38754;&#27604;&#36739;&#30740;&#31350;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15038</link><description>&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#20248;&#31168;&#30340;&#25968;&#25454;&#20998;&#26512;&#24072;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is GPT-4 a Good Data Analyst?. (arXiv:2305.15038v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#26159;&#21542;&#26159;&#19968;&#20010;&#20248;&#31168;&#30340;&#25968;&#25454;&#20998;&#26512;&#24072;&#65292;&#24182;&#36890;&#36807;&#38754;&#23545;&#38754;&#27604;&#36739;&#30740;&#31350;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#22914;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#35821;&#35328;&#29983;&#25104;&#12289;&#25968;&#25454;&#21465;&#36848;&#31561;&#65292;&#35768;&#22810;&#25968;&#25454;&#20998;&#26512;&#24072;&#21487;&#33021;&#25285;&#24515;&#20182;&#20204;&#30340;&#24037;&#20316;&#26159;&#21542;&#20250;&#34987;&#20154;&#24037;&#26234;&#33021;&#21462;&#20195;&#12290;&#36825;&#20010;&#26377;&#20105;&#35758;&#30340;&#35805;&#39064;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20173;&#22788;&#20110;&#24847;&#35265;&#20998;&#27495;&#30340;&#38454;&#27573;&#65292;&#27809;&#26377;&#20219;&#20309;&#26126;&#30830;&#30340;&#32467;&#35770;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#8220;GPT-4&#26159;&#19968;&#20010;&#22909;&#30340;&#25968;&#25454;&#20998;&#26512;&#24072;&#21527;&#65311;&#8221;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36827;&#34892;&#38754;&#23545;&#38754;&#30340;&#27604;&#36739;&#30740;&#31350;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;GPT-4&#35270;&#20026;&#19968;&#20010;&#25968;&#25454;&#20998;&#26512;&#24072;&#65292;&#20351;&#29992;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#25968;&#25454;&#24211;&#36827;&#34892;&#31471;&#21040;&#31471;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;GPT-4&#30340;&#25552;&#31034;&#35821;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#31995;&#32479;&#22320;&#27604;&#36739;&#20960;&#20010;&#19987;&#19994;&#20154;&#21592;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) have demonstrated their powerful capabilities in plenty of domains and tasks, including context understanding, code generation, language generation, data storytelling, etc., many data analysts may raise concerns if their jobs will be replaced by artificial intelligence (AI). This controversial topic has drawn great attention in public. However, we are still at a stage of divergent opinions without any definitive conclusion. Motivated by this, we raise the research question of "is GPT-4 a good data analyst?" in this work and aim to answer it by conducting head-to-head comparative studies. In detail, we regard GPT-4 as a data analyst to perform end-to-end data analysis with databases from a wide range of domains. We propose a framework to tackle the problems by carefully designing the prompts for GPT-4 to conduct experiments. We also design several task-specific evaluation metrics to systematically compare the performance between several professional human
&lt;/p&gt;</description></item><item><title>Self-ICL&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#29983;&#25104;&#31034;&#33539;&#30340;&#26041;&#24335;&#23454;&#29616;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#22312;&#30340;&#33021;&#21147;&#26469;&#29983;&#25104;&#20266;&#36755;&#20837;&#24182;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20266;&#36755;&#20837;&#21644;&#26631;&#31614;&#20316;&#20026;&#31034;&#33539;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;23&#20010;&#38590;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#65292;Self-ICL&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#22836;&#23545;&#22836;&#27604;&#36739;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.15035</link><description>&lt;p&gt;
&#33258;&#25105;&#29983;&#25104;&#31034;&#33539;&#30340;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;(Self-ICL)
&lt;/p&gt;
&lt;p&gt;
Self-ICL: Zero-Shot In-Context Learning with Self-Generated Demonstrations. (arXiv:2305.15035v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15035
&lt;/p&gt;
&lt;p&gt;
Self-ICL&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#33258;&#25105;&#29983;&#25104;&#31034;&#33539;&#30340;&#26041;&#24335;&#23454;&#29616;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#22312;&#30340;&#33021;&#21147;&#26469;&#29983;&#25104;&#20266;&#36755;&#20837;&#24182;&#39044;&#27979;&#20266;&#26631;&#31614;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20266;&#36755;&#20837;&#21644;&#26631;&#31614;&#20316;&#20026;&#31034;&#33539;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#22312;23&#20010;&#38590;&#20219;&#21153;&#19978;&#30340;&#35780;&#20272;&#65292;Self-ICL&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#22836;&#23545;&#22836;&#27604;&#36739;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#38646;&#26679;&#26412;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#23569;&#37327;&#36755;&#20837;&#36755;&#20986;&#31034;&#33539;&#19979;&#36866;&#24212;&#30446;&#26631;&#20219;&#21153;&#30340;&#24778;&#20154;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#33021;&#21147;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#36827;&#34892;ICL&#65292;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#20174;&#29616;&#26377;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#36873;&#25321;&#20195;&#34920;&#24615;&#30340;&#31034;&#33539;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35774;&#32622;&#19982;&#29616;&#23454;&#19990;&#30028;&#30340;&#23454;&#36341;&#19981;&#19968;&#33268;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#36890;&#24120;&#22312;&#27809;&#26377;&#35775;&#38382;&#31034;&#33539;&#27744;&#30340;&#24773;&#20917;&#19979;&#26597;&#35810;LLMs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#25105;&#29983;&#25104;&#31034;&#33539;&#30340;&#38646;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;(Self-ICL) - &#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#23548;LLMs&#30340;&#20869;&#22312;&#33021;&#21147;&#26469;&#25191;&#34892;&#38646;&#26679;&#26412;ICL&#12290;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#36755;&#20837;&#65292;Self-ICL&#39318;&#20808;&#25552;&#31034;&#27169;&#22411;&#29983;&#25104;&#20266;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#36890;&#36807;&#38646;&#26679;&#26412;&#25552;&#31034;&#20026;&#20266;&#36755;&#20837;&#39044;&#27979;&#20266;&#26631;&#31614;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#36755;&#20837;-&#26631;&#31614;&#23545;&#20316;&#20026;&#31034;&#33539;&#26469;&#36827;&#34892;&#27979;&#35797;&#36755;&#20837;&#30340;ICL&#12290;&#22312;23&#20010;BIG-Bench&#38590;&#20219;&#21153;&#30340;&#35780;&#20272;&#20013;&#65292;Self-ICL&#22312;&#24179;&#22343;&#20934;&#30830;&#29575;&#21644;&#22836;&#23545;&#22836;&#27604;&#36739;&#26041;&#38754;&#20248;&#20110;&#38646;&#26679;&#26412;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#36335;&#65292;Self-ICL&#23454;&#29616;&#20102;&#36830;&#32493;ICL&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have exhibited striking in-context learning (ICL) ability to adapt to target tasks with a few input-output demonstrations. For better ICL, different methods are proposed to select representative demonstrations from existing training corpora. However, such settings are not aligned with real-world practices, as end-users usually query LMs without access to demonstration pools. In this work, we introduce Self-ICL -- a simple framework which bootstraps LMs' intrinsic capabilities to perform zero-shot ICL. Given a test input, Self-ICL first prompts the model to generate pseudo-inputs. Next, the model predicts pseudo-labels for the pseudo-inputs via zero-shot prompting. Finally, we perform ICL for the test input with the pseudo-input-label pairs as demonstrations. Evaluation on 23 BIG-Bench Hard tasks shows Self-ICL outperforms zero-shot baselines on both average accuracy and head-to-head comparison. Moreover, with zero-shot chain-of-thought, Self-ICL achieves re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ImageNetVC&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;1000&#20010;ImageNet&#31867;&#21035;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;&#12290;&#36890;&#36807;&#35813;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#22686;&#24378;&#27169;&#22411;&#23545;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#25484;&#25569;&#31243;&#24230;&#65292;&#20026;&#20016;&#23500;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.15028</link><description>&lt;p&gt;
ImageNetVC&#65306;&#22312;1000&#20010;ImageNet&#31867;&#21035;&#19978;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ImageNetVC: Zero- and Few-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories. (arXiv:2305.15028v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ImageNetVC&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;1000&#20010;ImageNet&#31867;&#21035;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;&#12290;&#36890;&#36807;&#35813;&#35780;&#20272;&#65292;&#20998;&#26512;&#20102;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#22686;&#24378;&#27169;&#22411;&#23545;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#25484;&#25569;&#31243;&#24230;&#65292;&#20026;&#20016;&#23500;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#65292;&#23545;&#20840;&#38754;&#30340;&#35270;&#35273;&#30693;&#35782;&#25552;&#20986;&#20102;&#37325;&#35201;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;LLMs&#21450;&#20854;&#35270;&#35273;&#22686;&#24378;&#22411;&#27169;&#22411;&#65288;VaLMs&#65289;&#22312;&#25484;&#25569;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#30340;&#27700;&#24179;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ImageNetVC&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#29992;&#20110;&#23545;1000&#20010;ImageNet&#31867;&#21035;&#36827;&#34892;&#38646;&#27425;&#21644;&#23569;&#27425;&#36828;&#36317;&#31163;&#35270;&#35273;&#24120;&#35782;&#35780;&#20272;&#12290;&#21033;&#29992;ImageNetVC&#65292;&#25105;&#20204;&#23545;&#21333;&#27169;&#24577;LLMs&#21644;VaLMs&#30340;&#22522;&#26412;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24433;&#21709;&#22823;&#35268;&#27169;&#27169;&#22411;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#22240;&#32032;&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#20102;&#35270;&#35273;&#24120;&#35782;&#30693;&#35782;&#30340;&#35821;&#35328;&#27169;&#22411;&#21457;&#23637;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/hemingkx/ImageNetVC&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Large Language Models (LLMs) have been serving as general-purpose interfaces, posing a significant demand for comprehensive visual knowledge. However, it remains unclear how well current LLMs and their visually augmented counterparts (VaLMs) can master visual commonsense knowledge. To investigate this, we propose ImageNetVC, a human-annotated dataset specifically designed for zero- and few-shot visual commonsense evaluation across 1,000 ImageNet categories. Utilizing ImageNetVC, we benchmark the fundamental visual commonsense knowledge of both unimodal LLMs and VaLMs. Furthermore, we analyze the factors affecting the visual commonsense knowledge of large-scale models, providing insights into the development of language models enriched with visual commonsense knowledge. Our code and dataset are available at https://github.com/hemingkx/ImageNetVC.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Calc-X&#21644;Calcformers&#65292;&#23427;&#20204;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15017</link><description>&lt;p&gt;
Calc-X&#21644;Calcformers&#65306;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#22686;&#24378;&#31639;&#26415;&#25512;&#29702;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Calc-X and Calcformers: Empowering Arithmetical Chain-of-Thought through Interaction with Symbolic Systems. (arXiv:2305.15017v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15017
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;Calc-X&#21644;Calcformers&#65292;&#23427;&#20204;&#36890;&#36807;&#19982;&#31526;&#21495;&#31995;&#32479;&#30340;&#20132;&#20114;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20934;&#30830;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#36827;&#34892;&#31639;&#26415;&#35745;&#31639;&#30340;&#20219;&#21153;&#20013;&#24448;&#24448;&#20250;&#20135;&#29983;&#20107;&#23454;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Calc-X&#65292;&#36825;&#26159;&#19968;&#20010;&#28436;&#31034;&#22914;&#20309;&#22312;&#25512;&#29702;&#38142;&#20013;&#27491;&#30830;&#20351;&#29992;&#35745;&#31639;&#22120;&#30340;&#25968;&#25454;&#38598;&#21512;&#12290;Calc-X&#36866;&#29992;&#20110;&#25945;&#23548;&#35821;&#35328;&#27169;&#22411;&#23558;&#35745;&#31639;&#20219;&#21153;&#36716;&#31227;&#21040;&#31526;&#21495;&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#35843;&#26597;&#24182;&#32479;&#19968;&#20102;&#20960;&#20010;&#24050;&#26377;&#30340;&#25512;&#29702;&#38142;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26631;&#20934;&#26684;&#24335;&#65292;&#32467;&#26524;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30&#19975;&#20010;&#38656;&#35201;&#36827;&#34892;&#31639;&#26415;&#25512;&#29702;&#30340;&#26679;&#26412;&#30340;&#26631;&#20934;&#25968;&#25454;&#38598;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26032;&#30340;Calc-X&#38598;&#21512;&#26469;&#35757;&#32451;&#25105;&#20204;&#31216;&#20043;&#20026;Calcformers&#30340;&#24320;&#28304;&#35745;&#31639;&#22120;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#26222;&#36890;&#35821;&#35328;&#27169;&#22411;&#22522;&#32447;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#36817;&#20046;&#32763;&#20493;&#12290;&#25105;&#20204;&#20844;&#24320;&#25552;&#20379;&#25152;&#26377;&#30340;Calc-X&#25968;&#25454;&#38598;&#12289;&#28304;&#20195;&#30721;&#21644;Calcformers&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite outstanding performance in many tasks, language models are notoriously inclined to make factual errors in tasks requiring arithmetic computation. We address this deficiency by creating Calc-X, a collection of datasets that demonstrates the appropriate use of a calculator in reasoning chains. Calc-X is suitable for teaching language models to offload computations to a symbolic system. We survey and unify several existing chain-of-thought datasets into a proposed format, resulting in a standard collection of over 300,000 samples requiring arithmetic reasoning. Finally, we use the new Calc-X collection to train open-source calculator-using models we call Calcformers and show that these models approximately double the accuracy of generating correct results compared to vanilla language model baselines. We make all Calc-X datasets, source code and Calcformers models publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;RAP&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#30340;&#19990;&#30028;&#27169;&#22411;&#24182;&#27169;&#25311;&#38271;&#26399;&#34892;&#21160;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20687;&#20154;&#31867;&#22823;&#33041;&#19968;&#26679;&#30340;&#26377;&#24847;&#35782;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2305.14992</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#23601;&#26159;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Reasoning with Language Model is Planning with World Model. (arXiv:2305.14992v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#26694;&#26550;RAP&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#37096;&#30340;&#19990;&#30028;&#27169;&#22411;&#24182;&#27169;&#25311;&#38271;&#26399;&#34892;&#21160;&#32467;&#26524;&#65292;&#20174;&#32780;&#20351;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#20687;&#20154;&#31867;&#22823;&#33041;&#19968;&#26679;&#30340;&#26377;&#24847;&#35782;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#25552;&#31034;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26102;&#65288;&#20363;&#22914;&#24605;&#32500;&#38142;&#65289;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#19968;&#20123;&#23545;&#20154;&#31867;&#26469;&#35828;&#23481;&#26131;&#30340;&#38382;&#39064;&#19978;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20363;&#22914;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#29983;&#25104;&#25191;&#34892;&#20219;&#21153;&#30340;&#34892;&#21160;&#35745;&#21010;&#65292;&#25110;&#36827;&#34892;&#22797;&#26434;&#30340;&#25968;&#23398;&#12289;&#36923;&#36753;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;&#36825;&#31181;&#19981;&#36275;&#28304;&#20110;LLMs&#32570;&#20047;&#19968;&#20010;&#20869;&#37096;&#30340;&#8220;&#19990;&#30028;&#27169;&#22411;&#8221;&#65292;&#29992;&#20110;&#39044;&#27979;&#19990;&#30028;&#30340;&#29366;&#24577;&#65288;&#20363;&#22914;&#29615;&#22659;&#29366;&#20917;&#12289;&#20013;&#38388;&#21464;&#37327;&#20540;&#65289;&#24182;&#27169;&#25311;&#34892;&#21160;&#30340;&#38271;&#26399;&#32467;&#26524;&#12290;&#36825;&#20351;&#24471;LLMs&#26080;&#27861;&#20687;&#20154;&#31867;&#22823;&#33041;&#37027;&#26679;&#36827;&#34892;&#26377;&#24847;&#35782;&#30340;&#35268;&#21010;&#65292;&#20854;&#20013;&#21253;&#25324;&#25506;&#32034;&#26367;&#20195;&#30340;&#25512;&#29702;&#36335;&#24452;&#12289;&#39044;&#27979;&#26410;&#26469;&#30340;&#29366;&#24577;&#21644;&#22238;&#25253;&#65292;&#24182;&#23545;&#29616;&#26377;&#30340;&#25512;&#29702;&#27493;&#39588;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#25512;&#29702;&#26694;&#26550;&#65292;&#21363;RAP&#65288;&#36890;&#36807;&#35268;&#21010;&#36827;&#34892;&#25512;&#29702;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown remarkable reasoning capabilities, especially when prompted to generate intermediate reasoning steps (e.g., Chain-of-Thought, CoT). However, LLMs can still struggle with problems that are easy for humans, such as generating action plans for executing tasks in a given environment, or performing complex math, logical, and commonsense reasoning. The deficiency stems from the key fact that LLMs lack an internal $\textit{world model}$ to predict the world $\textit{state}$ (e.g., environment status, intermediate variable values) and simulate long-term outcomes of actions. This prevents LLMs from performing deliberate planning akin to human brains, which involves exploring alternative reasoning paths, anticipating future states and rewards, and iteratively refining existing reasoning steps. To overcome the limitations, we propose a new LLM reasoning framework, $\underline{R}$easoning vi$\underline{a}$ $\underline{P}$lanning $\textbf{(RAP)}$. RAP repurpo
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23613;&#31649;&#22312;&#33521;&#25991;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38463;&#25289;&#20271;&#35821;&#19978;&#30340;&#24615;&#33021;&#19981;&#22914;&#32463;&#36807;&#38463;&#25289;&#20271;&#35821;&#24494;&#35843;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.14976</link><description>&lt;p&gt;
GPTAraEval: &#23545;Arabic NLP&#19978;&#30340;ChatGPT&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
GPTAraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP. (arXiv:2305.14976v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14976
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23613;&#31649;&#22312;&#33521;&#25991;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#38463;&#25289;&#20271;&#35821;&#19978;&#30340;&#24615;&#33021;&#19981;&#22914;&#32463;&#36807;&#38463;&#25289;&#20271;&#35821;&#24494;&#35843;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20986;&#29616;&#26631;&#24535;&#30528;NLP&#39046;&#22495;&#30340;&#19968;&#27425;&#21464;&#38761;&#65292;&#23588;&#20854;&#22312;&#35768;&#22810;&#33521;&#25991;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#35813;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#26159;&#26410;&#30693;&#39046;&#22495;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#30693;&#35782;&#31354;&#30333;&#65292;&#37325;&#28857;&#35780;&#20272;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#26041;&#35328;&#19978;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22823;&#35268;&#27169;&#30340;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;44&#20010;&#19981;&#21516;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28041;&#21450;60&#22810;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#23545;ChatGPT&#22312;Arabic NLP&#20013;&#36827;&#34892;&#30340;&#39318;&#27425;&#20840;&#38754;&#24615;&#24615;&#33021;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#33521;&#35821;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;ChatGPT&#22312;&#38463;&#25289;&#20271;&#35821;&#19978;&#30340;&#24615;&#33021;&#22987;&#32456;&#19981;&#22914;&#32463;&#36807;&#38463;&#25289;&#20271;&#35821;&#24494;&#35843;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#27604;&#20102;ChatGPT&#21644;GPT-4&#22312;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#21644;&#26041;&#35328;&#38463;&#25289;&#20271;&#35821;&#65288;DA&#65289;&#19978;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
ChatGPT's emergence heralds a transformative phase in NLP, particularly demonstrated through its excellent performance on many English benchmarks. However, the model's efficacy across diverse linguistic contexts remains largely uncharted territory. This work aims to bridge this knowledge gap, with a primary focus on assessing ChatGPT's capabilities on Arabic languages and dialectal varieties. Our comprehensive study conducts a large-scale automated and human evaluation of ChatGPT, encompassing 44 distinct language understanding and generation tasks on over 60 different datasets. To our knowledge, this marks the first extensive performance analysis of ChatGPT's deployment in Arabic NLP. Our findings indicate that, despite its remarkable performance in English, ChatGPT is consistently surpassed by smaller models that have undergone finetuning on Arabic. We further undertake a meticulous comparison of ChatGPT and GPT-4's Modern Standard Arabic (MSA) and Dialectal Arabic (DA), unveiling th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#19987;&#38376;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#36739;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14926</link><description>&lt;p&gt;
&#36890;&#29992;&#30340;&#33258;&#36866;&#24212;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Universal Self-Adaptive Prompting. (arXiv:2305.14926v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20171;&#32461;&#19968;&#31181;&#19987;&#38376;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38646;&#26679;&#26412;&#24615;&#33021;&#36739;&#24369;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#25512;&#29702;&#27169;&#22411;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26631;&#24535;&#26159;&#23427;&#20204;&#20986;&#33394;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#33021;&#21147;&#65292;&#36890;&#24120;&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#39640;&#24230;&#20196;&#20154;&#22402;&#28046;&#24182;&#19988;&#26368;&#20026;&#36890;&#29992;&#65292;LLMs&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#36890;&#24120;&#36739;&#24369;&#65292;&#22240;&#20026;&#32570;&#20047;&#24341;&#23548;&#24182;&#19988;&#38590;&#20197;&#22312;&#22522;&#20110;&#26222;&#36890;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#29616;&#26377;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#65292;&#24403;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#19981;&#21487;&#29992;&#26102;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#36890;&#29992;&#33258;&#36866;&#24212;&#25552;&#31034;(USP)&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#38024;&#23545;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;(&#21516;&#26102;&#20860;&#23481;&#23569;&#26679;&#26412;&#23398;&#20064;)&#12290;USP&#21482;&#38656;&#35201;&#23569;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#19968;&#20010;&#20165;&#36827;&#34892;&#25512;&#29702;&#30340;LLM&#65292;&#38750;&#24120;&#28789;&#27963;&#65306;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#25552;&#31034;&#65292;USP&#23558;&#21487;&#33021;&#30340;NLP&#20219;&#21153;&#24402;&#31867;&#20026;&#19977;&#31181;&#21487;&#33021;&#30340;&#20219;&#21153;&#31867;&#22411;&#20043;&#19968;&#65292;&#28982;&#21518;&#20351;&#29992;&#30456;&#24212;&#30340;&#36873;&#25321;&#22120;&#26469;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#26597;&#35810;&#21644;&#38646;&#26679;&#26412;&#27169;&#22411;&#29983;&#25104;&#30340;&#21709;&#24212;&#20316;&#20026;&#20266;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
A hallmark of modern large language models (LLMs) is their impressive general zero-shot and few-shot abilities, often elicited through in-context learning (ICL) via prompting. However, while highly coveted and being the most general, zero-shot performances in LLMs are still typically weaker due to the lack of guidance and the difficulty of applying existing automatic prompt design methods in general tasks when ground-truth labels are unavailable. In this study, we address this by presenting Universal Self-Adaptive Prompting (USP), an automatic prompt design approach specifically tailored for zero-shot learning (while compatible with few-shot). Requiring only a small amount of unlabeled data and an inference-only LLM, USP is highly versatile: to achieve universal prompting, USP categorizes a possible NLP task into one of the three possible task types and then uses a corresponding selector to select the most suitable queries and zero-shot model-generated responses as pseudo-demonstration
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27979;&#37327;&#29702;&#35770;&#30340;&#26694;&#26550;MetricEval&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35299;&#20915;&#20154;&#24037;&#35780;&#20272;&#30340;&#25928;&#24230;&#32467;&#26500;&#28151;&#28102;&#21644;&#22522;&#20110;LLM&#30340;&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14889</link><description>&lt;p&gt;
&#35780;&#20272;&#35780;&#20272;&#25351;&#26631;&#65306;&#20351;&#29992;&#27979;&#37327;&#29702;&#35770;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#25351;&#26631;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Evaluating Evaluation Metrics: A Framework for Analyzing NLG Evaluation Metrics using Measurement Theory. (arXiv:2305.14889v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#27979;&#37327;&#29702;&#35770;&#30340;&#26694;&#26550;MetricEval&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#36890;&#36807;&#35813;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35299;&#20915;&#20154;&#24037;&#35780;&#20272;&#30340;&#25928;&#24230;&#32467;&#26500;&#28151;&#28102;&#21644;&#22522;&#20110;LLM&#30340;&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;--&#35780;&#20272;&#25351;&#26631;&#30340;&#35774;&#35745;&#21644;&#35780;&#20272;&#12290;&#25105;&#20204;&#24847;&#35782;&#21040;&#29616;&#26377;&#33258;&#21160;&#25351;&#26631;&#30340;&#23616;&#38480;&#24615;&#20197;&#21450;&#24403;&#21069;&#20154;&#24037;&#35780;&#20272;&#23384;&#22312;&#30340;&#35823;&#24046;&#65292;&#25552;&#20986;&#20102;MetricEval&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#27979;&#37327;&#29702;&#35770;&#65292;&#25945;&#32946;&#27979;&#35797;&#35774;&#35745;&#30340;&#22522;&#30784;&#65292;&#29992;&#20110;&#27010;&#24565;&#21270;&#21644;&#35780;&#20272;NLG&#35780;&#20272;&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#21644;&#25928;&#24230;&#12290;&#35813;&#26694;&#26550;&#35268;&#33539;&#20102;&#27979;&#37327;&#35823;&#24046;&#30340;&#26469;&#28304;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#23454;&#35777;&#25968;&#25454;&#35780;&#20272;&#35780;&#20272;&#25351;&#26631;&#30340;&#32479;&#35745;&#24037;&#20855;&#12290;&#20511;&#21161;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#37327;&#21270;&#25351;&#26631;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#35299;&#37322;&#32467;&#26524;&#12290;&#20026;&#20102;&#31034;&#33539;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#23454;&#36341;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#32452;&#29992;&#20110;&#25688;&#35201;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#30830;&#23450;&#20102;&#20154;&#24037;&#35780;&#20272;&#20013;&#30340;&#25928;&#24230;&#32467;&#26500;&#28151;&#28102;&#21644;&#22522;&#20110;LLM&#30340;&#25351;&#26631;&#20013;&#30340;&#21487;&#38752;&#24615;&#30456;&#20851;&#38382;&#39064;&#12290;&#36890;&#36807;MetricEval&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#35780;&#20272;&#25351;&#26631;&#30340;&#35774;&#35745;&#12289;&#35780;&#20272;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address a fundamental challenge in Natural Language Generation (NLG) model evaluation -- the design and evaluation of evaluation metrics. Recognizing the limitations of existing automatic metrics and noises from how current human evaluation was conducted, we propose MetricEval, a framework informed by measurement theory, the foundation of educational test design, for conceptualizing and evaluating the reliability and validity of NLG evaluation metrics. The framework formalizes the source of measurement error and offers statistical tools for evaluating evaluation metrics based on empirical data. With our framework, one can quantify the uncertainty of the metrics to better interpret the result. To exemplify the use of our framework in practice, we analyzed a set of evaluation metrics for summarization and identified issues related to conflated validity structure in human-eval and reliability in LLM-based metrics. Through MetricEval, we aim to promote the design, evaluation, and interp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.14794</link><description>&lt;p&gt;
&#30465;&#24515;&#23398;&#20064;&#21464;&#24471;&#39046;&#20808;&#65306;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#31616;&#21333;&#31181;&#23376;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing Made State-of-the-art: Revisiting the Simple Seed-based Weak Supervision for Text Classification. (arXiv:2305.14794v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#20266;&#26631;&#31614;&#29983;&#25104;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#21333;&#35789;&#21024;&#38500;&#26469;&#32531;&#35299;&#22240;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#35265;&#32780;&#24102;&#26469;&#30340;&#24433;&#21709;&#65292;&#25552;&#39640;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20854;&#24615;&#33021;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#35774;&#35745;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#23558;&#39640;&#23618;&#27425;&#30340;&#20154;&#31867;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#31181;&#23376;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#29983;&#25104;&#20266;&#26631;&#31614;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#31181;&#23376;&#21305;&#37197;&#30340;&#26377;&#38480;&#24615;&#33021;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#31181;&#23376;&#21305;&#37197;&#35268;&#21017;&#27880;&#20837;&#30340;&#26631;&#31614;&#20559;&#24046;&#65292;&#36825;&#20250;&#38459;&#27490;&#20998;&#31867;&#22120;&#23398;&#20064;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#26469;&#36873;&#25321;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#31616;&#21333;&#22320;&#21024;&#38500;&#21305;&#37197;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#31181;&#23376;&#35789;&#21487;&#20197;&#32531;&#35299;&#26631;&#31614;&#20559;&#24046;&#24182;&#24110;&#21161;&#23398;&#20064;&#26356;&#22909;&#30340;&#32622;&#20449;&#24230;&#12290;&#38543;&#21518;&#65292;&#31181;&#23376;&#21305;&#37197;&#30340;&#24615;&#33021;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#65292;&#20351;&#23427;&#36798;&#21040;&#25110;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22788;&#29702;&#31181;&#23376;&#35789;&#19981;&#20026;&#20154;&#30693;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#31616;&#21333;&#22320;&#21024;&#38500;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#21333;&#35789;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in weakly supervised text classification mostly focus on designing sophisticated methods to turn high-level human heuristics into quality pseudo-labels. In this paper, we revisit the seed matching-based method, which is arguably the simplest way to generate pseudo-labels, and show that its power was greatly underestimated. We show that the limited performance of seed matching is largely due to the label bias injected by the simple seed-match rule, which prevents the classifier from learning reliable confidence for selecting high-quality pseudo-labels. Interestingly, simply deleting the seed words present in the matched input texts can mitigate the label bias and help learn better confidence. Subsequently, the performance achieved by seed matching can be improved significantly, making it on par with or even better than the state-of-the-art. Furthermore, to handle the case when the seed words are not made known, we propose to simply delete the word tokens in the input tex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#31574;&#30053;Bi-Drop&#26469;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#24494;&#35843;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#32463;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#65292;Bi-Drop&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#23545;&#20110;&#22810;&#20219;&#21153;&#12289;&#22810;&#39046;&#22495;&#36716;&#31227;&#12289;&#25968;&#25454;&#19981;&#22343;&#34913;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14760</link><description>&lt;p&gt;
Bi-Drop: &#33258;&#36866;&#24212;&#23376;&#32593;&#32476;&#20248;&#21270;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Bi-Drop: Generalizable Fine-tuning for Pre-trained Language Models via Adaptive Subnetwork Optimization. (arXiv:2305.14760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#31574;&#30053;Bi-Drop&#26469;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#24494;&#35843;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#32463;&#36807;GLUE&#22522;&#20934;&#27979;&#35797;&#65292;Bi-Drop&#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#23545;&#20110;&#22810;&#20219;&#21153;&#12289;&#22810;&#39046;&#22495;&#36716;&#31227;&#12289;&#25968;&#25454;&#19981;&#22343;&#34913;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20063;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#24050;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#35757;&#32451;&#38598;&#26377;&#38480;&#65292;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#24494;&#35843;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#24494;&#35843;&#31574;&#30053;Bi-Drop&#26469;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32570;&#28857;&#12290;&#23427;&#21033;&#29992;dropout&#29983;&#25104;&#30340;&#21508;&#31181;&#23376;&#27169;&#22411;&#30340;&#26799;&#24230;&#20449;&#24687;&#26377;&#36873;&#25321;&#24615;&#22320;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#12290;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Bi-Drop&#30340;&#24615;&#33021;&#20248;&#20110;&#20043;&#21069;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#22312;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#23545;&#27604;&#32431;&#24494;&#35843;&#26356;&#24378;&#30340;&#31283;&#20581;&#24615;&#19982;&#36890;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20219;&#21153;&#12289;&#22810;&#39046;&#22495;&#36716;&#31227;&#12289;&#25968;&#25454;&#19981;&#22343;&#34913;&#21644;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;Bi-Drop&#30340;&#34920;&#29616;&#24046;&#24322;&#26174;&#33879;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models have achieved remarkable success in a variety of natural language understanding tasks. Nevertheless, finetuning large pretrained models on downstream tasks is susceptible to overfitting if the training set is limited, which will lead to diminished performance. In this work, we propose a dynamic fine-tuning strategy for pretrained language models called Bi-Drop. It utilizes the gradient information of various sub-models generated by dropout to update the model parameters selectively. Experiments on the GLUE benchmark show that Bi-Drop outperforms previous fine-tuning methods by a considerable margin, and exhibits consistent superiority over vanilla fine-tuning across various pretrained models. Furthermore, empirical results indicate that Bi-Drop yields substantial improvements in the multiple task or domain transfer, data imbalance, and low-resource scenarios, demonstrating superb generalization ability and robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.14755</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#27169;&#22411;&#21450;&#35780;&#20272;&#22312;&#25991;&#20307;&#25913;&#20889;&#20013;&#30340;&#24517;&#35201;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting. (arXiv:2305.14755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#36890;&#36807;few-shot prompting&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#19981;&#19968;&#23450;&#33021;&#21453;&#26144;&#20986;&#20154;&#31867;&#30340;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25991;&#20307;&#25913;&#20889;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#25805;&#20316;&#65292;&#20294;&#26159;&#24573;&#35270;&#25991;&#26412;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#21487;&#20197;&#23548;&#33268;&#25913;&#20889;&#32467;&#26524;&#26159;&#19968;&#33324;&#21270;&#12289;&#27495;&#20041;&#21644;&#19981;&#36830;&#36143;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#21040;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#65292;&#37325;&#28857;&#20851;&#27880;&#24418;&#24335;&#12289;&#27602;&#24615;&#21644;&#24773;&#24863;&#36716;&#31227;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545; GPT-3.5 &#21644; GPT NeoX &#30340; few-shot &#25552;&#38382;&#27604;&#36739;&#37325;&#20889;&#30340;&#26041;&#27861;&#65292;&#24182;&#27604;&#36739;&#38750;&#19978;&#19979;&#25991;&#25913;&#20889;&#21644;&#19978;&#19979;&#25991;&#25913;&#20889;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20154;&#20204;&#36890;&#24120;&#26356;&#21916;&#27426;&#19978;&#19979;&#25991;&#25913;&#20889;&#65292;&#20294;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#65288;&#22914; BLEU&#65292;sBERT&#65289;&#19981;&#26159;&#36825;&#26679;&#30340;&#12290;&#20026;&#24357;&#21512;&#36825;&#31181;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#29992;&#33258;&#21160;&#24230;&#37327;&#25351;&#26631;&#30340;&#19978;&#19979;&#25991;&#34701;&#21512;&#29256;&#26412;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#26356;&#33021;&#21453;&#26144;&#20154;&#31867;&#20559;&#22909;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#25991;&#24378;&#35843;&#22312;&#25991;&#20307;&#25913;&#20889;&#30340;&#37325;&#20889;&#21644;&#35780;&#20272;&#38454;&#27573;&#25972;&#21512;&#25991;&#26412;&#19978;&#19979;&#25991;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing stylistic text rewriting methods operate on a sentence level, but ignoring the broader context of the text can lead to generic, ambiguous, and incoherent rewrites. In this paper, we propose the integration of preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting, focusing on formality, toxicity, and sentiment transfer tasks. We conduct a comparative evaluation of rewriting through few-shot prompting of GPT-3.5 and GPT NeoX, comparing non-contextual rewrites to contextual rewrites. Our experiments show that humans often prefer contextual rewrites over non-contextual ones, but automatic metrics (e.g., BLEU, sBERT) do not. To bridge this gap, we propose context-infused versions of common automatic metrics, and show that these better reflect human preferences. Overall, our paper highlights the importance of integrating preceding textual context into both the rewriting and evaluation stages of stylistic text rewriting.
&lt;/p&gt;</description></item><item><title>ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14740</link><description>&lt;p&gt;
ECHo: &#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
ECHo: Event Causality Inference via Human-centric Reasoning. (arXiv:2305.14740v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14740
&lt;/p&gt;
&lt;p&gt;
ECHo&#26159;&#19968;&#20010;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#25512;&#29702;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;CoT&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; ECHo&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#31038;&#20132;&#24773;&#22659;&#30340;&#20107;&#20214;&#22240;&#26524;&#25512;&#26029;&#35786;&#26029;&#25968;&#25454;&#38598;&#12290; ECHo&#21033;&#29992;&#20174;&#29359;&#32618;&#21095;&#20013;&#25910;&#38598;&#30340;&#30495;&#23454;&#20154;&#31867;&#20013;&#24515;&#28436;&#32462;&#20449;&#24687;&#65292;&#36890;&#36807;&#28608;&#21457;&#20013;&#38388;&#24515;&#28789;&#29702;&#35770;&#65288;ToM&#65289;&#26469;&#24357;&#21512;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#40511;&#27807;&#65292;&#20174;&#32780;&#25552;&#39640;&#31038;&#20132;&#26234;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;Chain-of-Thought&#65288;CoT&#65289;&#33539;&#24335;&#23545;&#40784;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;&#24403;&#21069;AI&#31995;&#32479;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20010;ToM&#22686;&#24378;&#30340;CoT&#31649;&#36947;&#21487;&#20197;&#22312; &#38646;-shot&#35270;&#35273;&#21644;&#35821;&#35328;&#29702;&#35299;&#20013;&#21253;&#23481;&#21644;&#25972;&#21512;&#21508;&#31181;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#20114;&#34917;&#30340;&#22522;&#20110;&#20154;&#31867;&#20013;&#24515;&#30340;ECHo&#20219;&#21153;&#26469;&#23457;&#26597;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;ECHo&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#26292;&#38706;&#25512;&#29702;&#20013;&#30340;&#19981;&#23436;&#21892;&#21644;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce ECHo, a diagnostic dataset of event causality inference grounded in visual-and-linguistic social scenarios. ECHo employs real-world human-centric deductive information collected from crime drama, bridging the gap in multimodal reasoning towards higher social intelligence through the elicitation of intermediate Theory-of-Mind (ToM). We propose a unified framework aligned with the Chain-of-Thought (CoT) paradigm to assess the reasoning capability of current AI systems. This ToM-enhanced CoT pipeline can accommodate and integrate various large foundation models in zero-shot visual-and-linguistic understanding. With this framework, we scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks. Further analysis demonstrates ECHo as a challenging dataset to expose imperfections and inconsistencies in reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.14735</link><description>&lt;p&gt;
&#36793;&#32536;&#32858;&#28966;&#65306;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#25439;&#20154;&#32676;&#30340;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Centering the Margins: Outlier-Based Identification of Harmed Populations in Toxicity Detection. (arXiv:2305.14735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#24120;&#20540;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#22312;&#27602;&#24615;&#26816;&#27979;&#20013;&#21463;&#21040;&#20260;&#23475;&#30340;&#20154;&#32676;&#65292;&#21457;&#29616;&#23545;&#20110;&#36825;&#20123;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#65292;&#20182;&#20204;&#38754;&#20020;&#30340;&#27602;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#23545;&#36793;&#32536;&#31038;&#21306;&#24433;&#21709;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#30830;&#23450;&#29305;&#23450;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#36825;&#20123;&#26041;&#27861;&#26088;&#22312;&#35299;&#20915;&#38024;&#23545;&#24369;&#21183;&#32676;&#20307;&#30340;&#20260;&#23475;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#20250;&#25513;&#30422;&#30001;&#20132;&#21449;&#23376;&#32676;&#25110;&#36328;&#20154;&#21475;&#32676;&#20307;&#20849;&#20139;&#30340;&#20260;&#23475;&#27169;&#24335;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23558;&#8220;&#36793;&#32536;&#8221;&#23450;&#20041;&#20026;&#20855;&#26377;&#36828;&#31163;&#8220;&#24120;&#24577;&#8221; &#30340;&#20154;&#21475;&#23646;&#24615;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#24230;&#37327;&#38024;&#23545;&#36825;&#20123;&#24322;&#24120;&#20540;&#30340;&#20260;&#23475;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#25351;&#25968;&#65288;GPDI&#65289;&#65292;&#20197;&#34913;&#37327;&#25968;&#25454;&#38598;&#32454;&#20998;&#20026;&#23376;&#32452;&#23545;&#38754;&#20020;&#22686;&#21152;&#30340;&#20260;&#23475;&#30340;&#35782;&#21035;&#31243;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#26816;&#27979;&#27602;&#24615;&#26816;&#27979;&#20013;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;&#38024;&#23545;&#24322;&#24120;&#20540;&#30340;&#25991;&#26412;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;&#27602;&#24615;&#26816;&#39564;&#20013;&#27602;&#24615;&#26356;&#39640;&#65292;&#39640;&#36798;28&#65285;&#33267;86&#65285;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#20110;&#20154;&#21475;&#23398;&#24322;&#24120;&#20540;&#65292;&#27169;&#22411;&#24615;&#33021;&#22987;&#32456;&#36739;&#24046;&#65292;&#24322;&#24120;&#20540;&#21644;&#38750;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#38169;&#35823;&#24046;&#36317;&#39640;&#36798;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
A standard method for measuring the impacts of AI on marginalized communities is to determine performance discrepancies between specified demographic groups. These approaches aim to address harms toward vulnerable groups, but they obscure harm patterns faced by intersectional subgroups or shared across demographic groups. We instead operationalize "the margins" as data points that are statistical outliers due to having demographic attributes distant from the "norm" and measure harms toward these outliers. We propose a Group-Based Performance Disparity Index (GPDI) that measures the extent to which a subdivision of a dataset into subgroups identifies those facing increased harms. We apply our approach to detecting disparities in toxicity detection and find that text targeting outliers is 28% to 86% more toxic for all types of toxicity examined. We also discover that model performance is consistently worse for demographic outliers, with disparities in error between outliers and non-outli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14711</link><description>&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#22522;&#20110;&#22270;&#20687;&#23383;&#24149;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gender Biases in Automatic Evaluation Metrics: A Case Study on Image Captioning. (arXiv:2305.14711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#26696;&#20197;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#22312;&#22270;&#20687;&#23383;&#24149;&#31561;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#24050;&#32463;&#35777;&#26126;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#24182;&#19982;&#20154;&#31867;&#21028;&#26029;&#39640;&#24230;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#23578;&#26410;&#34987;&#28145;&#20837;&#25506;&#35752;&#8212;&#8212;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#32534;&#30721;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35780;&#20272;&#30446;&#30340;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#34920;&#29616;&#24182;&#28508;&#22312;&#22320;&#25918;&#22823;&#20559;&#35265;&#12290;&#26412;&#25991;&#38024;&#23545;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#65292;&#23545;&#27169;&#22411;&#35780;&#20272;&#24230;&#37327;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#21644;&#37327;&#21270;&#20102;&#19981;&#21516;&#35780;&#20272;&#24230;&#37327;&#20013;&#20851;&#20110;&#32844;&#19994;&#12289;&#27963;&#21160;&#21644;&#29289;&#20307;&#27010;&#24565;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#36825;&#20123;&#26377;&#20559;&#35265;&#30340;&#24230;&#37327;&#24102;&#26469;&#30340;&#36127;&#38754;&#21518;&#26524;&#65292;&#27604;&#22914;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#20559;&#21521;&#26377;&#20559;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21521;&#29983;&#25104;&#27169;&#22411;&#20256;&#25773;&#20559;&#35265;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained model-based evaluation metrics have demonstrated strong performance with high correlations with human judgments in various natural language generation tasks such as image captioning. Despite the impressive results, their impact on fairness is under-explored -- it is widely acknowledged that pretrained models can encode societal biases, and utilizing them for evaluation purposes may inadvertently manifest and potentially amplify biases. In this paper, we conduct a systematic study in gender biases of model-based evaluation metrics with a focus on image captioning tasks. Specifically, we first identify and quantify gender biases in different evaluation metrics regarding profession, activity, and object concepts. Then, we demonstrate the negative consequences of using these biased metrics, such as favoring biased generation models in deployment and propagating the biases to generation models through reinforcement learning. We also present a simple but effective alternative to r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#27880;&#37322;&#32773;&#21644;&#27880;&#37322;&#30340;&#30697;&#38453;&#34920;&#31034;&#20197;&#25429;&#25417;&#20854;&#29305;&#28857;&#65292;&#24182;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#26469;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24110;&#21161;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14663</link><description>&lt;p&gt;
&#20320;&#23601;&#26159;&#20320;&#30340;&#27880;&#37322;&#65306;&#36890;&#36807;&#27880;&#37322;&#32773;&#34920;&#31034;&#23454;&#29616;&#26356;&#22909;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
You Are What You Annotate: Towards Better Models through Annotator Representations. (arXiv:2305.14663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14663
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#24314;&#27880;&#37322;&#32773;&#21644;&#27880;&#37322;&#30340;&#30697;&#38453;&#34920;&#31034;&#20197;&#25429;&#25417;&#20854;&#29305;&#28857;&#65292;&#24182;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#20351;&#29992;&#23427;&#20204;&#26469;&#26174;&#33879;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#24110;&#21161;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#65292;&#27880;&#37322;&#32773;&#30340;&#19981;&#19968;&#33268;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#23384;&#22312;&#22810;&#31181;&#21407;&#22240;&#23548;&#33268;&#27880;&#37322;&#32773;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#21253;&#25324;&#20219;&#21153;&#30340;&#20027;&#35266;&#24615;&#12289;&#22256;&#38590;&#26696;&#20363;&#12289;&#19981;&#26126;&#30830;&#30340;&#25351;&#21335;&#31561;&#31561;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#26159;&#20165;&#20165;&#27719;&#24635;&#27880;&#37322;&#30340;&#26631;&#31614;&#26469;&#33719;&#24471;&#25968;&#25454;&#27880;&#37322;&#65292;&#32780;&#26159;&#25552;&#35758;&#35201;&#26126;&#30830;&#32771;&#34385;&#27880;&#37322;&#32773;&#29305;&#24449;&#65292;&#24182;&#22312;&#24314;&#27169;&#36807;&#31243;&#20013;&#21033;&#29992;&#23427;&#20204;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#30697;&#38453;&#20998;&#21035;&#20026;&#27880;&#37322;&#32773;&#65288;&#27880;&#37322;&#32773;&#23884;&#20837;&#65289;&#21644;&#20854;&#27880;&#37322;&#65288;&#27880;&#37322;&#23884;&#20837;&#65289;&#21019;&#24314;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#23569;&#20110;1&#65285;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#22312;&#21508;&#31181;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#36890;&#36807;&#25429;&#25417;&#21333;&#20010;&#27880;&#37322;&#32773;&#30340;&#29420;&#29305;&#20542;&#21521;&#21644;&#20027;&#35266;&#24615;&#65292;&#25105;&#20204;&#30340;&#23884;&#20837;&#26377;&#21161;&#20110;&#27665;&#20027;&#21270;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21253;&#21547;&#22810;&#20803;&#21270;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotator disagreement is ubiquitous in natural language processing (NLP) tasks. There are multiple reasons for such disagreements, including the subjectivity of the task, difficult cases, unclear guidelines, and so on. Rather than simply aggregating labels to obtain data annotations, we instead propose to explicitly account for the annotator idiosyncrasies and leverage them in the modeling process. We create representations for the annotators (annotator embeddings) and their annotations (annotation embeddings) with learnable matrices associated with each. Our approach significantly improves model performance on various NLP benchmarks by adding fewer than 1% model parameters. By capturing the unique tendencies and subjectivity of individual annotators, our embeddings help democratize AI and ensure that AI models are inclusive of diverse viewpoints.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.14617</link><description>&lt;p&gt;
COMET-M: &#22312;&#22797;&#26434;&#21477;&#23376;&#20013;&#25512;&#29702;&#22810;&#20010;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
COMET-M: Reasoning about Multiple Events in Complex Sentences. (arXiv:2305.14617v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14617
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;COMET-M&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22797;&#26434;&#21477;&#23376;&#20013;&#22810;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#65292;&#24182;&#22312;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30456;&#23545;&#20110;&#20043;&#21069;&#30340;COMET&#27169;&#22411;&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#36890;&#24120;&#28041;&#21450;&#32472;&#21046;&#24120;&#35782;&#25512;&#26029;&#65292;&#20197;&#25512;&#29702;&#26410;&#26126;&#30830;&#38472;&#36848;&#30340;&#20869;&#23481;&#12290;&#22312;&#22810;&#20107;&#20214;&#21477;&#23376;&#20013;&#65292;&#38656;&#35201;&#22522;&#20110;&#19978;&#19979;&#25991;&#30693;&#35782;&#29702;&#35299;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;COMET-M&#65288;Multi-Event&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#24120;&#35782;&#27169;&#22411;&#65292;&#33021;&#22815;&#38024;&#23545;&#22797;&#26434;&#21477;&#23376;&#20869;&#30340;&#30446;&#26631;&#20107;&#20214;&#29983;&#25104;&#24120;&#35782;&#25512;&#26029;&#12290;COMET-M&#26159;&#22522;&#20110;COMET&#65288;Bosselut et al.&#65292;2019&#65289;&#21457;&#23637;&#32780;&#26469;&#30340;&#65292;&#21518;&#32773;&#25797;&#38271;&#20026;&#31616;&#21333;&#21477;&#23376;&#29983;&#25104;&#20197;&#20107;&#20214;&#20026;&#20013;&#24515;&#30340;&#25512;&#26029;&#65292;&#20294;&#22312;&#33258;&#28982;&#25991;&#26412;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#22810;&#20107;&#20214;&#21477;&#23376;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19968;&#20010;&#21253;&#21547;35K&#20010;&#20154;&#31867;&#32534;&#20889;&#25512;&#26029;&#30340;&#22810;&#20107;&#20214;&#25512;&#26029;&#25968;&#25454;&#38598;&#12290; &#25105;&#20204;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#25512;&#26029;&#19978;&#35757;&#32451;&#20102;COMET-M&#65292;&#24182;&#21019;&#24314;&#20102;&#20351;&#29992;&#33258;&#21160;&#26631;&#35760;&#31034;&#20363;&#30340;&#22522;&#32447;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COMET-M&#22312;&#29983;&#25104;&#22810;&#20107;&#20214;&#25512;&#26029;&#26041;&#38754;&#30456;&#23545;&#20110;COMET&#20855;&#26377;&#26174;&#30528;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;COMET-M&#25104;&#21151;&#39044;&#27979;&#20102;&#27979;&#35797;&#38598;&#20013;60&#65285;&#30340;&#22797;&#26434;&#21477;&#23376;&#30446;&#26631;&#20107;&#20214;&#30340;&#24120;&#35782;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the speaker's intended meaning often involves drawing commonsense inferences to reason about what is not stated explicitly. In multi-event sentences, it requires understanding the relationships between events based on contextual knowledge. We propose COMET-M (Multi-Event), an event-centric commonsense model capable of generating commonsense inferences for a target event within a complex sentence. COMET-M builds upon COMET (Bosselut et al., 2019), which excels at generating event-centric inferences for simple sentences, but struggles with the complexity of multi-event sentences prevalent in natural text. To overcome this limitation, we curate a multi-event inference dataset of 35K human-written inferences. We trained COMET-M on the human-written inferences and also created baselines using automatically labeled examples. Experimental results demonstrate the significant performance improvement of COMET-M over COMET in generating multi-event inferences. Moreover, COMET-M succ
&lt;/p&gt;</description></item><item><title>MathDial&#26159;&#19968;&#20010;&#30001;&#23454;&#38469;&#25945;&#24072;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21322;&#21512;&#25104;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33258;&#21160;&#23545;&#35805;&#36741;&#23548;&#24037;&#20855;&#32570;&#23569;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#20851;&#27880;&#36890;&#36807;&#24341;&#23548;&#23398;&#29983;&#20351;&#29992;&#38382;&#39064;&#26469;&#25506;&#32034;&#25968;&#23398;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14536</link><description>&lt;p&gt;
MathDial: &#19968;&#20010;&#20197;&#25968;&#23398;&#25512;&#29702;&#38382;&#39064;&#20026;&#22522;&#30784;&#30340;&#23500;&#21547;&#25945;&#32946;&#24615;&#23646;&#24615;&#30340;&#23545;&#35805;&#36741;&#23548;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
MathDial: A Dialogue Tutoring Dataset with Rich Pedagogical Properties Grounded in Math Reasoning Problems. (arXiv:2305.14536v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14536
&lt;/p&gt;
&lt;p&gt;
MathDial&#26159;&#19968;&#20010;&#30001;&#23454;&#38469;&#25945;&#24072;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21322;&#21512;&#25104;&#29983;&#25104;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33258;&#21160;&#23545;&#35805;&#36741;&#23548;&#24037;&#20855;&#32570;&#23569;&#39640;&#36136;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#20851;&#27880;&#36890;&#36807;&#24341;&#23548;&#23398;&#29983;&#20351;&#29992;&#38382;&#39064;&#26469;&#25506;&#32034;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#21160;&#23545;&#35805;&#36741;&#23548;&#24037;&#20855;&#22312;&#20010;&#24615;&#21270;&#21644;&#25552;&#39640;&#25945;&#32946;&#21487;&#21450;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#31867;&#31995;&#32479;&#30340;&#30740;&#31350;&#19968;&#30452;&#21463;&#21040;&#38459;&#30861;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#27492;&#31867;&#25968;&#25454;&#38598;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#24405;&#21046;&#36741;&#23548;&#20250;&#24341;&#21457;&#38544;&#31169;&#38382;&#39064;&#65292;&#32780;&#20247;&#21253;&#21017;&#20250;&#23548;&#33268;&#25968;&#25454;&#36136;&#37327;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#30495;&#27491;&#30340;&#32769;&#24072;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#37197;&#23545;&#65292;&#26500;&#24314;&#24120;&#35265;&#23398;&#29983;&#38169;&#35823;&#30340;&#25903;&#26550;&#65292;&#26469;&#21322;&#21512;&#25104;&#22320;&#29983;&#25104;&#36825;&#20123;&#23545;&#35805;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#26469;&#25910;&#38598;MathDial&#30340;&#26368;&#26032;&#24037;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#24403;&#21069;&#25317;&#26377;&#32422;1.5k&#20010;&#22522;&#20110;&#22810;&#27493;&#25968;&#23398;&#35789;&#38382;&#39064;&#30340;&#36741;&#23548;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23637;&#29616;&#20986;&#20016;&#23500;&#30340;&#25945;&#32946;&#24615;&#23646;&#24615;&#65292;&#20027;&#35201;&#20851;&#27880;&#36890;&#36807;&#24341;&#23548;&#23398;&#29983;&#20351;&#29992;&#38382;&#39064;&#26469;&#25506;&#32034;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;MathDial&#21450;&#20854;&#22320;&#38754;&#27880;&#37322;&#21487;&#29992;&#20110;&#24494;&#35843;...
&lt;/p&gt;
&lt;p&gt;
Although automatic dialogue tutors hold great potential in making education personalized and more accessible, research on such systems has been hampered by a lack of sufficiently large and high-quality datasets. However, collecting such datasets remains challenging, as recording tutoring sessions raises privacy concerns and crowdsourcing leads to insufficient data quality. To address this problem, we propose a framework to semi-synthetically generate such dialogues by pairing real teachers with a large language model (LLM) scaffolded to represent common student errors. In this paper, we describe our ongoing efforts to use this framework to collect MathDial, a dataset of currently ca. 1.5k tutoring dialogues grounded in multi-step math word problems. We show that our dataset exhibits rich pedagogical properties, focusing on guiding students using sense-making questions to let them explore problems. Moreover, we outline that MathDial and its grounding annotations can be used to finetune 
&lt;/p&gt;</description></item><item><title>NAIL&#26159;&#19968;&#31181;&#24102;&#26377;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;&#27169;&#22411;&#65292;&#21487;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#21487;&#20197;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#21305;&#37197;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14499</link><description>&lt;p&gt;
NAIL: &#24102;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
NAIL: Lexical Retrieval Indices with Efficient Non-Autoregressive Decoders. (arXiv:2305.14499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14499
&lt;/p&gt;
&lt;p&gt;
NAIL&#26159;&#19968;&#31181;&#24102;&#26377;&#39640;&#25928;&#38750;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#30340;&#35789;&#27719;&#26816;&#32034;&#25351;&#25968;&#27169;&#22411;&#65292;&#21487;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20860;&#23481;&#65292;&#24182;&#19988;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#23427;&#21487;&#20197;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#21305;&#37197;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25991;&#26723;&#37325;&#26032;&#25490;&#21517;&#22120;&#22312;&#31934;&#24230;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#38656;&#35201;&#19987;&#29992;&#30828;&#20214;&#36827;&#34892;&#26381;&#21153;&#65292;&#36825;&#26159;&#26114;&#36149;&#24182;&#19988;&#36890;&#24120;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#26381;&#21153;&#26102;&#38388;&#35201;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#25429;&#25417;Transformer&#20132;&#21449;&#20851;&#27880;&#27169;&#22411;&#25910;&#30410;&#39640;&#36798;86&#65285;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#21482;&#38656;&#35201;&#27599;&#20010;&#25991;&#26723;&#36716;&#25442;&#22120;FLOP&#30340;10-6&#65285;&#30340;&#35789;&#27719;&#24471;&#20998;&#21151;&#33021;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#21830;&#21697;CPU&#25552;&#20379;&#26381;&#21153;&#12290;&#24403;&#19982;BM25&#26816;&#32034;&#22120;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#21305;&#37197;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#30340;&#36136;&#37327;&#65292;&#35813;&#26816;&#32034;&#22120;&#20173;&#38656;&#35201;&#21152;&#36895;&#22120;&#36827;&#34892;&#26597;&#35810;&#32534;&#30721;&#12290;&#25105;&#20204;&#23558;NAIL&#65288;&#24102;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#33258;&#22238;&#24402;&#32034;&#24341;&#65289;&#24341;&#20837;&#20026;&#19982;&#26368;&#36817;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;T5&#12289;GPT-3&#21644;PaLM&#65289;&#20860;&#23481;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#35813;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21487;&#20197;&#21033;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26816;&#26597;&#28857;&#65292;&#24182;&#21487;&#20197;&#24494;&#35843;&#20197;&#26377;&#25928;&#22320;&#26500;&#24314;&#19981;&#38656;&#35201;n
&lt;/p&gt;
&lt;p&gt;
Neural document rerankers are extremely effective in terms of accuracy. However, the best models require dedicated hardware for serving, which is costly and often not feasible. To avoid this serving-time requirement, we present a method of capturing up to 86% of the gains of a Transformer cross-attention model with a lexicalized scoring function that only requires 10-6% of the Transformer's FLOPs per document and can be served using commodity CPUs. When combined with a BM25 retriever, this approach matches the quality of a state-of-the art dual encoder retriever, that still requires an accelerator for query encoding. We introduce NAIL (Non-Autoregressive Indexing with Language models) as a model architecture that is compatible with recent encoder-decoder and decoder-only large language models, such as T5, GPT-3 and PaLM. This model architecture can leverage existing pre-trained checkpoints and can be fine-tuned for efficiently constructing document representations that do not require n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#27604;&#36739;&#20013;&#32654;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#65292;&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21462;&#31038;&#20250;&#35268;&#33539;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#20855;&#26377;&#30740;&#31350;&#31038;&#20250;&#25991;&#21270;&#24322;&#21516;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14492</link><description>&lt;p&gt;
&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#21644;&#21487;&#35299;&#37322;&#25991;&#26412;&#34164;&#21547;&#26469;&#27604;&#36739;&#31038;&#20250;&#25991;&#21270;&#35268;&#33539;&#30340;&#24322;&#21516;&#65306;&#20197;&#20013;&#22269;&#21644;&#32654;&#22269;&#25991;&#21270;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Sociocultural Norm Similarities and Differences via Situational Alignment and Explainable Textual Entailment. (arXiv:2305.14492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#27604;&#36739;&#20013;&#32654;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#65292;&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25552;&#21462;&#31038;&#20250;&#35268;&#33539;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27979;&#35797;&#21457;&#29616;&#35813;&#26041;&#27861;&#20855;&#26377;&#30740;&#31350;&#31038;&#20250;&#25991;&#21270;&#24322;&#21516;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#25991;&#21270;&#25512;&#29702;&#38656;&#35201;&#24314;&#31435;&#22312;&#30456;&#24212;&#25991;&#21270;&#32972;&#26223;&#19979;&#30340;&#20934;&#30830;&#35268;&#33539;&#22522;&#30784;&#20043;&#19978;&#12290; &#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31038;&#20250;&#35268;&#33539;&#35745;&#31639;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32654;&#22269;&#31038;&#20250;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#21457;&#29616;&#21644;&#27604;&#36739;&#20013;&#22269;&#21644;&#32654;&#22269;&#25991;&#21270;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290; &#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20013;&#25991;&#38382;&#31572;&#24179;&#21488; - &#30693;&#20046; - &#21644;&#29616;&#26377;&#30340;SocialChemistry&#25968;&#25454;&#38598;&#20316;&#20026;&#19981;&#21516;&#25991;&#21270;&#32500;&#24230;&#30340;&#20195;&#29702;&#65292;&#36890;&#36807;&#24773;&#22659;&#23545;&#40784;&#36328;&#25991;&#21270;&#27604;&#36739;&#31038;&#20250;&#24773;&#22659;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;Chain-of-Thought&#25552;&#31034;&#23884;&#20837;&#21040;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#26694;&#26550;&#20013;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;3,069&#20010;&#36328;&#20013;&#22269;&#21644;&#32654;&#22269;&#25991;&#21270;&#23545;&#40784;&#30340;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#30456;&#24212;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#27979;&#35797;&#27169;&#22411;&#22312;&#36328;&#25991;&#21270;&#31038;&#20250;&#35268;&#33539;&#26041;&#38754;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31038;&#20250;&#35268;&#33539;&#30340;&#21487;&#35299;&#37322;&#25991;&#26412;&#34164;&#21547;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#39044;&#27979;&#19968;&#20010;&#25991;&#21270;&#20013;&#30340;&#31038;&#20250;&#35268;&#33539;&#30340;&#35299;&#37322;&#26159;&#21542;&#26263;&#31034;&#21478;&#19968;&#20010;&#25991;&#21270;&#20013;&#30340;&#31867;&#20284;&#35268;&#33539;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20219;&#21153;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#30740;&#31350;&#31038;&#20250;&#25991;&#21270;&#30340;&#24322;&#21516;&#30340;&#20016;&#23500;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing systems that can reason across cultures requires that they are grounded in the norms of the contexts in which they operate. However, current research on developing computational models of social norms has primarily focused on American society. Here, we propose a novel approach to discover and compare descriptive social norms across Chinese and American cultures. We demonstrate our approach by leveraging discussions on a Chinese Q&amp;A platform-Zhihu-and the existing SocialChemistry dataset as proxies for contrasting cultural axes, align social situations cross-culturally, and extract social norms from texts using in-context learning. Embedding Chain-of-Thought prompting in a human-AI collaborative framework, we build a high-quality dataset of 3,069 social norms aligned with social situations across Chinese and American cultures alongside corresponding free-text explanations. To test the ability of models to reason about social norms across cultures, we introduce the task of expl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SALSA&#26694;&#26550;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#65292;&#36890;&#36807;21&#31181;&#19981;&#21516;&#32534;&#36753;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#21644;&#20154;&#31867;&#25991;&#26412;&#31616;&#21270;&#30340;&#20559;&#22909;&#21644;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;LENS-SALSA&#25351;&#26631;&#29992;&#20110;&#33258;&#21160;&#31616;&#21270;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.14458</link><description>&lt;p&gt;
&#22312;&#25104;&#21151;&#21644;&#22833;&#36133;&#20043;&#38388;&#36339;&#33310;&#65306;&#20351;&#29992;SALSA&#36827;&#34892;&#32534;&#36753;&#32423;&#21035;&#30340;&#31616;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dancing Between Success and Failure: Edit-level Simplification Evaluation using SALSA. (arXiv:2305.14458v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;SALSA&#26694;&#26550;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#65292;&#36890;&#36807;21&#31181;&#19981;&#21516;&#32534;&#36753;&#31867;&#22411;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#27169;&#22411;&#21644;&#20154;&#31867;&#25991;&#26412;&#31616;&#21270;&#30340;&#20559;&#22909;&#21644;&#34920;&#29616;&#65292;&#24182;&#24320;&#21457;&#20102;LENS-SALSA&#25351;&#26631;&#29992;&#20110;&#33258;&#21160;&#31616;&#21270;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3.5&#65289;&#21487;&#20197;&#20135;&#29983;&#39640;&#24230;&#35780;&#32423;&#30340;&#31616;&#21270;&#25991;&#26412;&#65292;&#20294;&#24403;&#21069;&#30340;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#26410;&#33021;&#25552;&#20379;&#23545;&#31995;&#32479;&#29305;&#23450;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#28165;&#26224;&#20102;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SALSA&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32534;&#36753;&#30340;&#20154;&#31867;&#27880;&#37322;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#25972;&#20307;&#21644;&#31934;&#32454;&#30340;&#25991;&#26412;&#31616;&#21270;&#35780;&#20272;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;21&#31181;&#22522;&#20110;&#35821;&#35328;&#23398;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#28085;&#30422;&#20102;&#27010;&#24565;&#12289;&#21477;&#27861;&#21644;&#35789;&#27719;&#31616;&#21333;&#24615;&#30340;&#25152;&#26377;&#25104;&#21151;&#21644;&#22833;&#36133;&#32500;&#24230;&#12290;&#20351;&#29992;SALSA&#65292;&#25105;&#20204;&#22312;700&#20010;&#31616;&#21270;&#26696;&#20363;&#19978;&#25910;&#38598;&#20102;12K&#20010;&#32534;&#36753;&#27880;&#37322;&#65292;&#25581;&#31034;&#20102;&#24494;&#35843;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;LLM&#21644;&#20154;&#31867;&#20043;&#38388;&#36716;&#21270;&#26041;&#27861;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#24182;&#21457;&#29616;GPT-3.5&#25191;&#34892;&#30340;&#39640;&#36136;&#37327;&#32534;&#36753;&#27604;&#20154;&#31867;&#26356;&#22810;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#39057;&#32321;&#30340;&#38169;&#35823;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#31934;&#32454;&#27880;&#37322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LENS-SALSA&#65292;&#19968;&#31181;&#26080;&#21442;&#32771;&#33258;&#21160;&#31616;&#21270;&#24230;&#37327;&#65292;&#35757;&#32451;&#20197;&#30452;&#25509;&#20174;&#36755;&#20837;&#25991;&#26412;&#21644;&#25552;&#35758;&#30340;&#31616;&#21270;&#20013;&#39044;&#27979;&#21477;&#23376;&#21644;&#32534;&#36753;&#32423;&#21035;&#36136;&#37327;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (e.g., GPT-3.5) are uniquely capable of producing highly rated text simplification, yet current human evaluation methods fail to provide a clear understanding of systems' specific strengths and weaknesses. To address this limitation, we introduce SALSA, an edit-based human annotation framework that enables holistic and fine-grained text simplification evaluation. We develop twenty one linguistically grounded edit types, covering the full spectrum of success and failure across dimensions of conceptual, syntactic and lexical simplicity. Using SALSA, we collect 12K edit annotations on 700 simplifications, revealing discrepancies in the distribution of transformation approaches performed by fine-tuned models, few-shot LLMs and humans, and finding GPT-3.5 performs more quality edits than humans, but still exhibits frequent errors. Using our fine-grained annotations, we develop LENS-SALSA, a reference-free automatic simplification metric, trained to predict sentence- an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#36873;&#25321;&#24102;&#26377;&#25512;&#29702;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14333</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#36873;&#25321;&#24102;&#26377;&#25512;&#29702;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Automatic Model Selection with Large Language Models for Reasoning. (arXiv:2305.14333v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#26469;&#33258;&#21160;&#36873;&#25321;&#24102;&#26377;&#25512;&#29702;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought&#65288;CoT&#65289;&#21644;Program-Aided Language Models&#65288;PAL&#65289;&#20195;&#34920;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#21508;&#33258;&#20855;&#26377;&#33258;&#24049;&#30340;&#20248;&#21183;&#12290;CoT&#37319;&#29992;&#33258;&#28982;&#35821;&#35328;&#65292;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;PAL&#21033;&#29992;&#32534;&#31243;&#35821;&#35328;&#65292;&#20135;&#29983;&#26356;&#32467;&#26500;&#21270;&#21644;&#20005;&#23494;&#30340;&#36923;&#36753;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21160;&#24577;&#36873;&#25321;&#23427;&#20204;&#20043;&#38388;&#30340;&#26368;&#20339;&#26041;&#27861;&#26469;&#32467;&#21512;&#20004;&#32773;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#24378;&#35843;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#32463;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20843;&#20010;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#19982;Codex&#12289;ChatGPT&#21644;GPT-4&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#33258;&#19968;&#33268;&#24615;&#30456;&#36741;&#30456;&#25104;&#65307;&#24403;&#25972;&#21512;&#22312;&#19968;&#36215;&#26102;&#65292;&#23427;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;GSM8K&#21644;SVAMP&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#20998;&#21035;&#36798;&#21040;96.8%&#21644;93.7%&#30340;&#20934;&#30830;&#29575;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Chain-of-Thought (CoT) and Program-Aided Language Models (PAL) represent two distinct reasoning methods, each with its own strengths. CoT employs natural language, offering flexibility and interpretability, while PAL utilizes programming language, yielding more structured and rigorous logic. We introduce a model selection method to combine the best of both worlds by employing a large language model (LLM) to dynamically select between them. Our theoretical analysis underscores the feasibility of this method, which is further corroborated by empirical results. Our proposed method demonstrates significant performance improvements across eight reasoning datasets with Codex, ChatGPT, and GPT-4. Additionally, our method is complementary to self-consistency; when integrated, it can further enhance performance while significantly reducing computation costs. Moreover, we achieve new state-of-the-art results on GSM8K and SVAMP, with respective accuracies of 96.8% and 93.7%. Our code, data and pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#25991;&#29486;&#65292;&#25506;&#32034;&#20102;&#36171;&#20104;&#21147;&#37327;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;TalkUp&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#25429;&#25417;&#36171;&#20104;&#21147;&#37327;&#21644;&#21066;&#24369;&#21147;&#37327;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20026;&#25506;&#32034;&#21547;&#20041;&#30340;&#21547;&#33988;&#24615;&#12289;&#39044;&#35774;&#20197;&#21450;&#31038;&#20250;&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#30340;&#24847;&#20041;&#25552;&#20379;&#20102;&#19968;&#20010;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2305.14326</link><description>&lt;p&gt;
TalkUp&#65306;&#20026;&#29702;&#35299;&#36171;&#20104;&#21147;&#37327;&#30340;&#35821;&#35328;&#38138;&#24179;&#36947;&#36335;
&lt;/p&gt;
&lt;p&gt;
TalkUp: Paving the Way for Understanding Empowering Language. (arXiv:2305.14326v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20511;&#37492;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#25991;&#29486;&#65292;&#25506;&#32034;&#20102;&#36171;&#20104;&#21147;&#37327;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;TalkUp&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#25429;&#25417;&#36171;&#20104;&#21147;&#37327;&#21644;&#21066;&#24369;&#21147;&#37327;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20026;&#25506;&#32034;&#21547;&#20041;&#30340;&#21547;&#33988;&#24615;&#12289;&#39044;&#35774;&#20197;&#21450;&#31038;&#20250;&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#30340;&#24847;&#20041;&#25552;&#20379;&#20102;&#19968;&#20010;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36171;&#20104;&#21147;&#37327;&#30340;&#35821;&#35328;&#22312;&#25945;&#32946;&#12289;&#24037;&#20316;&#29615;&#22659;&#21644;&#21307;&#30103;&#31561;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#22659;&#20013;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#23613;&#31649;&#35821;&#35328;&#25216;&#26415;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#20294;&#36171;&#20104;&#21147;&#37327;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24456;&#23569;&#34987;&#30740;&#31350;&#65292;&#32780;&#19988;&#30001;&#20110;&#20854;&#38544;&#24615;&#30340;&#29305;&#36136;&#65292;&#36825;&#19968;&#27010;&#24565;&#30340;&#23454;&#36341;&#21270;&#26412;&#36136;&#19978;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#30740;&#31350;&#20511;&#37492;&#20102;&#35821;&#35328;&#23398;&#21644;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#25991;&#29486;&#65292;&#25506;&#32034;&#20102;&#36171;&#20104;&#21147;&#37327;&#30340;&#35821;&#35328;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20247;&#21253;&#30340;&#26041;&#24335;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340; Reddit &#24086;&#23376;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#24086;&#23376;&#36827;&#34892;&#20102;&#36171;&#20104;&#21147;&#37327;&#30340;&#26631;&#27880;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#24086;&#23376;&#23545;&#35835;&#32773;&#30340;&#36171;&#20104;&#21147;&#37327;&#30340;&#21407;&#22240;&#20197;&#21450;&#21457;&#24086;&#32773;&#21644;&#35835;&#32773;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;TalkUp&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#33021;&#22815;&#25429;&#25417;&#36171;&#20104;&#21147;&#37327;&#21644;&#21066;&#24369;&#21147;&#37327;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26356;&#24191;&#27867;&#22320;&#35828;&#65292;TalkUp&#20026;&#25506;&#32034;&#21547;&#20041;&#30340;&#21547;&#33988;&#24615;&#12289;&#39044;&#35774;&#20197;&#21450;&#31038;&#20250;&#32972;&#26223;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#30340;&#24847;&#20041;&#25552;&#20379;&#20102;&#19968;&#20010;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empowering language is important in many real-world contexts, from education to workplace dynamics to healthcare. Though language technologies are growing more prevalent in these contexts, empowerment has seldom been studied in NLP, and moreover, it is inherently challenging to operationalize because of its implicit nature. This work builds from linguistic and social psychology literature to explore what characterizes empowering language. We then crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons why these posts are empowering to readers, and the social relationships between posters and readers. Our preliminary analyses show that this dataset, which we call TalkUp, can be used to train language models that capture empowering and disempowering language. More broadly, TalkUp provides an avenue to explore implication, presuppositions, and how social context influences the meaning of language.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#25552;&#21319;&#36328;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#23637;&#31034;&#20102;&#21033;&#29992;LLMs&#29983;&#25104;&#25968;&#25454;&#30340;&#24635;&#20307;&#20248;&#21183;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#31034;&#20363;&#20855;&#26377;&#36739;&#39640;&#30340;&#33258;&#28982;&#24615;&#21644;&#36923;&#36753;&#36830;&#36143;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14288</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#25968;&#25454;&#22686;&#24378;&#25552;&#21319;&#36328;&#35821;&#35328;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
LLM-powered Data Augmentation for Enhanced Cross-lingual Performance. (arXiv:2305.14288v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#20197;&#25552;&#21319;&#36328;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#23637;&#31034;&#20102;&#21033;&#29992;LLMs&#29983;&#25104;&#25968;&#25454;&#30340;&#24635;&#20307;&#20248;&#21183;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#31034;&#20363;&#20855;&#26377;&#36739;&#39640;&#30340;&#33258;&#28982;&#24615;&#21644;&#36923;&#36753;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#36328;&#35821;&#35328;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20960;&#20010;LLMs&#65292;&#21253;&#25324;Dolly-v2&#65292;StableVicuna&#65292;ChatGPT&#21644;GPT-4&#65292;&#26469;&#22686;&#24378;XCOPA&#65292;XWinograd&#21644;XStoryCloze&#19977;&#20010;&#25968;&#25454;&#38598;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35780;&#20272;&#20102;&#23567;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;mBERT&#21644;XLMR&#30340;&#24494;&#35843;&#25928;&#26524;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20197;&#33521;&#35821;&#21644;&#30446;&#26631;&#35821;&#35328;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24615;&#33021;&#65292;&#20197;&#21450;&#32763;&#35793;&#25104;&#33521;&#35821;&#30340;&#29983;&#25104;&#25968;&#25454;&#65292;&#25581;&#31034;&#20102;&#21033;&#29992;LLMs&#29983;&#25104;&#25968;&#25454;&#30340;&#24635;&#20307;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#26368;&#20339;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;13.4&#20010;&#20934;&#30830;&#29575;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20154;&#24037;&#35780;&#20272;&#65292;&#35810;&#38382;&#27597;&#35821;&#32773;&#35780;&#20272;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#29983;&#25104;&#30340;&#31034;&#20363;&#30340;&#33258;&#28982;&#24615;&#21644;&#36923;&#36753;&#36830;&#36143;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;ChatG&#36825;&#26679;&#30340;LLMs&#21487;&#20197;&#20135;&#29983;&#36739;&#33258;&#28982;&#21644;&#36923;&#36753;&#36830;&#36143;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the potential of leveraging Large Language Models (LLMs) for data augmentation in multilingual commonsense reasoning datasets where the available training data is extremely limited. To achieve this, we utilise several LLMs, namely Dolly-v2, StableVicuna, ChatGPT, and GPT-4, to augment three datasets: XCOPA, XWinograd, and XStoryCloze. Subsequently, we evaluate the effectiveness of fine-tuning smaller multilingual models, mBERT and XLMR, using the synthesised data. We compare the performance of training with data generated in English and target languages, as well as translated English-generated data, revealing the overall advantages of incorporating data generated by LLMs, e.g. a notable 13.4 accuracy score improvement for the best case. Furthermore, we conduct a human evaluation by asking native speakers to assess the naturalness and logical coherence of the generated examples across different languages. The results of the evaluation indicate that LLMs such as ChatG
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26597;&#35810;&#37325;&#20889;-&#26816;&#32034;-&#38405;&#35835;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#20851;&#27880;&#25628;&#32034;&#26597;&#35810;&#26412;&#36523;&#30340;&#36866;&#24212;&#24615;&#65292;&#37319;&#29992;&#21487;&#35757;&#32451;&#30340;&#37325;&#20889;&#22120;&#26469;&#26356;&#22909;&#22320;&#23545;&#40784;&#26597;&#35810;&#19982;&#20923;&#32467;&#27169;&#22359;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14283</link><description>&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26597;&#35810;&#37325;&#20889;
&lt;/p&gt;
&lt;p&gt;
Query Rewriting for Retrieval-Augmented Large Language Models. (arXiv:2305.14283v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14283
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#26597;&#35810;&#37325;&#20889;-&#26816;&#32034;-&#38405;&#35835;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#20851;&#27880;&#25628;&#32034;&#26597;&#35810;&#26412;&#36523;&#30340;&#36866;&#24212;&#24615;&#65292;&#37319;&#29992;&#21487;&#35757;&#32451;&#30340;&#37325;&#20889;&#22120;&#26469;&#26356;&#22909;&#22320;&#23545;&#40784;&#26597;&#35810;&#19982;&#20923;&#32467;&#27169;&#22359;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#21518;&#38405;&#35835;&#27969;&#31243;&#20013;&#20805;&#24403;&#24378;&#22823;&#30340;&#40657;&#30418;&#35835;&#32773;&#65292;&#22312;&#30693;&#35782;&#23494;&#38598;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#20174;&#26597;&#35810;&#37325;&#20889;&#30340;&#35282;&#24230;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#37325;&#26032;&#20889;&#20837;-&#26816;&#32034;-&#38405;&#35835;&#65292;&#32780;&#19981;&#26159;&#20043;&#21069;&#30340;&#26816;&#32034;&#21518;&#38405;&#35835;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;LLMs&#12290;&#19982;&#20197;&#24448;&#20391;&#37325;&#20110;&#35843;&#25972;&#26816;&#32034;&#22120;&#25110;&#38405;&#35835;&#22120;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20851;&#27880;&#30340;&#26159;&#25628;&#32034;&#26597;&#35810;&#26412;&#36523;&#30340;&#36866;&#24212;&#24615;&#65292;&#22240;&#20026;&#36755;&#20837;&#25991;&#26412;&#19982;&#26816;&#32034;&#25152;&#38656;&#30693;&#35782;&#20043;&#38388;&#19981;&#21487;&#36991;&#20813;&#23384;&#22312;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#31034;LLM&#29983;&#25104;&#26597;&#35810;&#65292;&#28982;&#21518;&#20351;&#29992;&#32593;&#32476;&#25628;&#32034;&#24341;&#25806;&#26469;&#26816;&#32034;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#20351;&#26597;&#35810;&#19982;&#20923;&#32467;&#27169;&#22359;&#23545;&#40784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#26041;&#26696;&#12290;&#37319;&#29992;&#19968;&#20010;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21487;&#35757;&#32451;&#30340;&#37325;&#20889;&#22120;&#65292;&#20197;&#36866;&#24212;&#40657;&#30418;LLM&#38405;&#35835;&#22120;&#12290;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#65292;&#20351;&#29992;LLM&#38405;&#35835;&#22120;&#30340;&#21453;&#39304;&#35757;&#32451;&#37325;&#20889;&#22120;&#12290;&#22312;Do&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) play powerful, black-box readers in the retrieve-then-read pipeline, making remarkable progress in knowledge-intensive tasks. This work introduces a new framework, Rewrite-Retrieve-Read instead of the previous retrieve-then-read for the retrieval-augmented LLMs from the perspective of the query rewriting. Unlike prior studies focusing on adapting either the retriever or the reader, our approach pays attention to the adaptation of the search query itself, for there is inevitably a gap between the input text and the needed knowledge in retrieval. We first prompt an LLM to generate the query, then use a web search engine to retrieve contexts. Furthermore, to better align the query to the frozen modules, we propose a trainable scheme for our pipeline. A small language model is adopted as a trainable rewriter to cater to the black-box LLM reader. The rewriter is trained using the feedback of the LLM reader by reinforcement learning. Evaluation is conducted on do
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14257</link><description>&lt;p&gt;
&#20998;&#23618;&#25552;&#31034;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#35266;&#23519;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24635;&#26159;&#25226;\emph{&#23436;&#25972;}&#35266;&#23519;&#65288;&#20363;&#22914;&#32593;&#39029;&#65289;&#25918;&#21040;&#25552;&#31034;&#20013;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;\emph{&#21387;&#32553;}&#21644;\emph{&#30456;&#20851;}&#30340;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;\summ&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;\actor&#25552;&#31034;&#26681;&#25454;&#24635;&#32467;&#30340;&#35266;&#23519;&#39044;&#27979;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#23588;&#20854;&#23637;&#31034;&#20102;&#23427;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#23548;&#33322;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#35266;&#23519;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#19978;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;6.2\%&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#20195;&#30721;&#20999;&#25442;&#30340;&#35821;&#22659;&#20013;&#65292;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.14235</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26080;&#27861;&#36827;&#34892;&#20195;&#30721;&#20999;&#25442;
&lt;/p&gt;
&lt;p&gt;
Multilingual Large Language Models Are Not (Yet) Code-Switchers. (arXiv:2305.14235v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14235
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#22312;&#20195;&#30721;&#20999;&#25442;&#30340;&#35821;&#22659;&#20013;&#65292;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26368;&#36817;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#38646;-shot&#25110;&#23569;&#37327;-shot&#30340;&#25552;&#31034;&#26041;&#27861;&#23637;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#22823;&#37327;&#30740;&#31350;&#20851;&#20110;&#23427;&#20204;&#22312;&#21333;&#35821;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#22312;&#20195;&#30721;&#20999;&#25442; (CSW) &#30340;&#35821;&#22659;&#20013;&#65292;&#21363;&#22312;&#19968;&#20010;&#35805;&#35821;&#20013;&#20132;&#26367;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#65292;&#23427;&#20204;&#30340;&#28508;&#21147;&#30340;&#30740;&#31350;&#36824;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22810;&#20010;&#22810;&#35821;&#35328;LLM&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32463;&#39564;&#35777;&#23454;&#20998;&#26512;&#65292;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#21333;&#35789;&#32423;&#35821;&#35328;&#35782;&#21035;&#31561;&#22235;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#22810;&#35821;&#35328;LLMs&#22312;&#26576;&#20123;&#20219;&#21153;&#20013;&#36890;&#36807;&#38646;-shot&#25110;&#23569;&#37327;-shot&#30340;&#25552;&#31034;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#65292;&#20294;&#19982;&#35268;&#27169;&#23567;&#24471;&#22810;&#30340;&#31934;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#23427;&#20204;&#20173;&#28982;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30446;&#21069;LLMs&#20013;&#30340;"&#22810;&#35821;&#35328;&#33021;&#21147;"&#24182;&#19981;&#24847;&#21619;&#30528;&#20855;&#22791;&#20195;&#30721;&#20999;&#25442;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current "multilingualism" in LLMs does not inherently imply proficiency with code-switching
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;</title><link>http://arxiv.org/abs/2305.14232</link><description>&lt;p&gt;
&#20026;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#39044;&#35757;&#32451;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding. (arXiv:2305.14232v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SciMult&#30340;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#20849;&#20139;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#22240;&#20854;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#30340;&#28508;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#36328;&#22810;&#20010;&#24322;&#26500;&#20219;&#21153;&#20849;&#21516;&#21033;&#29992;&#39044;&#35757;&#32451;&#25968;&#25454;&#65288;&#20363;&#22914;&#65292;&#26497;&#38480;&#22810;&#26631;&#31614;&#35770;&#25991;&#20998;&#31867;&#12289;&#24341;&#25991;&#39044;&#27979;&#21644;&#25991;&#29486;&#25628;&#32034;&#65289;&#20173;&#28982;&#22522;&#26412;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;SciMult&#65292;&#37325;&#28857;&#26159;&#20419;&#36827;&#19981;&#21516;&#31185;&#23398;&#25991;&#29486;&#29702;&#35299;&#20219;&#21153;&#20043;&#38388;&#30340;&#20849;&#20139;&#36890;&#29992;&#30693;&#35782;&#65292;&#21516;&#26102;&#38450;&#27490;&#20219;&#21153;&#29305;&#23450;&#25216;&#33021;&#30456;&#20114;&#24178;&#25200;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25216;&#26415;-&#20219;&#21153;&#24863;&#30693;&#30340;&#29305;&#21270;&#21644;&#25351;&#20196;&#35843;&#25972;&#12290;&#21069;&#32773;&#37319;&#29992;&#20102;&#20855;&#26377;&#20219;&#21153;&#24863;&#30693;&#23376;&#23618;&#30340;&#22810;&#19987;&#23478;&#21464;&#21387;&#22120;&#26550;&#26500;&#65307;&#21518;&#32773;&#22312;&#36755;&#20837;&#25991;&#26412;&#20043;&#21069;&#28155;&#21152;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#25351;&#20196;&#20197;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific literature understanding tasks have gained significant attention due to their potential to accelerate scientific discovery. Pre-trained language models (LMs) have shown effectiveness in these tasks, especially when tuned via contrastive learning. However, jointly utilizing pre-training data across multiple heterogeneous tasks (e.g., extreme multi-label paper classification, citation prediction, and literature search) remains largely unexplored. To bridge this gap, we propose a multi-task contrastive learning framework, SciMult, with a focus on facilitating common knowledge sharing across different scientific literature understanding tasks while preventing task-specific skills from interfering with each other. To be specific, we explore two techniques -task-aware specialization and instruction tuning. The former adopts a Mixture-of-Experts Transformer architecture with task-aware sub-layers; the latter prepends task-specific instructions to the input text so as to produce t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;56&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20998;&#35789;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20998;&#35789;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36890;&#36807;&#23376;&#35789;&#20998;&#35789;&#26041;&#24335;&#36827;&#34892;&#26631;&#35760;&#30340;&#21333;&#35789;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#27169;&#22411;&#26469;&#25913;&#36827;&#22797;&#21512;&#20998;&#35789;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14214</link><description>&lt;p&gt;
CompoundPiece&#65306;&#35780;&#20272;&#21644;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#22797;&#21512;&#20998;&#35789;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
CompoundPiece: Evaluating and Improving Decompounding Performance of Language Models. (arXiv:2305.14214v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#36328;56&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20998;&#35789;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#21512;&#20998;&#35789;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36890;&#36807;&#23376;&#35789;&#20998;&#35789;&#26041;&#24335;&#36827;&#34892;&#26631;&#35760;&#30340;&#21333;&#35789;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19987;&#38376;&#30340;&#27169;&#22411;&#26469;&#25913;&#36827;&#22797;&#21512;&#20998;&#35789;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35768;&#22810;&#35821;&#35328;&#37117;&#20855;&#26377;&#23558;&#20004;&#20010;&#25110;&#22810;&#20010;&#21333;&#35789;&#32467;&#21512;&#25104;&#22797;&#21512;&#35789;&#30340;&#36807;&#31243;&#65292;&#20294;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20165;&#38480;&#20110;&#20855;&#26377;&#36807;&#24230;&#22797;&#21512;&#24418;&#25104;&#33021;&#21147;&#30340;&#35821;&#35328;&#65288;&#22914;&#24503;&#35821;&#12289;&#33655;&#20848;&#35821;&#65289;&#65292;&#24182;&#19988;&#27809;&#26377;&#21253;&#21547;&#22823;&#37327;&#35821;&#35328;&#20013;&#22797;&#21512;&#21644;&#38750;&#22797;&#21512;&#35789;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24191;&#27867;&#30340;&#22797;&#21512;&#20998;&#35789;&#20219;&#21153;&#65292;&#21363;&#23558;&#22797;&#21512;&#35789;&#25286;&#20998;&#20026;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20171;&#32461;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Wiktionary&#30340;255k&#20010;&#22797;&#21512;&#21644;&#38750;&#22797;&#21512;&#35789;&#30340;&#36328;56&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#25968;&#25454;&#32570;&#21475;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19968;&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22797;&#21512;&#20998;&#35789;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#24615;&#33021;&#36739;&#24046;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36890;&#36807;&#23376;&#35789;&#20998;&#35789;&#26041;&#24335;&#36827;&#34892;&#26631;&#35760;&#30340;&#21333;&#35789;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35757;&#32451;&#19987;&#38376;&#29992;&#20110;&#22797;&#21512;&#20998;&#35789;&#30340;&#27169;&#22411;&#12290;&#25152;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#20381;&#36182;&#20110;&#23436;&#20840;&#33258;&#25105;&#30417;&#30563;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
While many languages possess processes of joining two or more words to create compound words, previous studies have been typically limited only to languages with excessively productive compound formation (e.g., German, Dutch) and there is no public dataset containing compound and non-compound words across a large number of languages. In this work, we systematically study decompounding, the task of splitting compound words into their constituents, at a wide scale. We first address the data gap by introducing a dataset of 255k compound and non-compound words across 56 diverse languages obtained from Wiktionary. We then use this dataset to evaluate an array of Large Language Models (LLMs) on the decompounding task. We find that LLMs perform poorly, especially on words which are tokenized unfavorably by subword tokenization. We thus introduce a novel methodology to train dedicated models for decompounding. The proposed two-stage procedure relies on a fully self-supervised objective in the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;&#65292;CTQ Scorer&#33021;&#22815;&#36873;&#25321;&#26368;&#20248;&#31034;&#20363;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#36229;&#36807;&#38543;&#26426;&#36873;&#25321;&#21644;&#21333;&#22240;&#32032;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.14105</link><description>&lt;p&gt;
CTQScorer: &#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#36827;&#34892;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#20197;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CTQScorer: Combining Multiple Features for In-context Example Selection for Machine Translation. (arXiv:2305.14105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#21512;&#22810;&#31181;&#29305;&#24449;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;&#65292;CTQ Scorer&#33021;&#22815;&#36873;&#25321;&#26368;&#20248;&#31034;&#20363;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#35328;&#23545;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#26174;&#33879;&#36229;&#36807;&#38543;&#26426;&#36873;&#25321;&#21644;&#21333;&#22240;&#32032;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#36755;&#20837;&#25552;&#31034;&#20960;&#20010;&#31034;&#20363;&#26102;&#65288;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#33021;&#21147;&#12290;&#32763;&#35793;&#36136;&#37327;&#21462;&#20915;&#20110;&#25152;&#36873;&#31034;&#20363;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#22914;&#20854;&#36136;&#37327;&#21644;&#30456;&#20851;&#24615;&#65292;&#20294;&#20197;&#24448;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#20010;&#29420;&#31435;&#30340;&#29305;&#24449;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#32467;&#21512;&#24433;&#21709;&#31034;&#20363;&#36873;&#25321;&#30340;&#19981;&#21516;&#29305;&#24449;&#12290;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#22238;&#24402;&#27169;&#22411;CTQ Scorer&#65288;&#19978;&#19979;&#25991;&#32763;&#35793;&#36136;&#37327;&#65289;&#65292;&#23427;&#22522;&#20110;&#22810;&#31181;&#29305;&#24449;&#36873;&#25321;&#31034;&#20363;&#20197;&#26368;&#22823;&#21270;&#32763;&#35793;&#36136;&#37327;&#12290;&#22312;&#22810;&#31181;&#35821;&#35328;&#23545;&#21644;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;CTQ Scorer&#26174;&#33879;&#36229;&#36807;&#20102;&#38543;&#26426;&#36873;&#25321;&#20197;&#21450;&#25991;&#29486;&#20013;&#25253;&#21578;&#30340;&#24378;&#21333;&#22240;&#32032;&#22522;&#32447;&#12290;&#30456;&#23545;&#20110;&#24378;BM25&#22522;&#32447;&#65292;&#25105;&#20204;&#36824;&#30475;&#21040;&#24179;&#22343;&#25913;&#21892;&#20102;2.5&#20010;COMET&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated the capability to perform on machine translation when the input is prompted with a few examples (in-context learning). Translation quality depends on various features of the selected examples, such as their quality and relevance, but previous work has predominantly focused on individual features in isolation. In this paper, we propose a general framework for combining different features influencing example selection. We learn a regression model, CTQ Scorer (Contextual Translation Quality), that selects examples based on multiple features in order to maximize the translation quality. On multiple language pairs and language models, we show that CTQ Scorer helps significantly outperform random selection as well as strong single-factor baselines reported in the literature. We also see an improvement of over 2.5 COMET points on average with respect to a strong BM25 retrieval-based baseline.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13999</link><description>&lt;p&gt;
&#36808;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13999
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#19988;&#31232;&#30095;&#30340;&#21069;&#39304;&#23618;&#65288;S-FFN&#65289;&#65292;&#22914;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25193;&#22823;Transformer&#27169;&#22411;&#35268;&#27169;&#20197;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#20165;&#28608;&#27963;&#37096;&#20998;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;FFN&#21442;&#25968;&#65292;S-FFN&#22312;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65288;&#20197;FLOPs&#35745;&#31639;&#65289;&#19981;&#21464;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#31232;&#30095;&#31070;&#32463;&#35760;&#24518;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#19979;&#65292;&#20998;&#26512;&#20102;S-FFN&#30340;&#20004;&#20010;&#20027;&#35201;&#35774;&#35745;&#36873;&#25321;&#65306;&#20869;&#23384;&#22359;&#65288;&#21363;&#19987;&#23478;&#65289;&#22823;&#23567;&#21644;&#20869;&#23384;&#22359;&#36873;&#25321;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;S-FFN&#26550;&#26500;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#36873;&#25321;&#26041;&#27861; - Avg-K&#65292;&#36890;&#36807;&#22343;&#20540;&#32858;&#21512;&#30340;&#38544;&#34255;&#29366;&#24577;&#26469;&#36873;&#25321;&#22359;&#65292;&#30456;&#27604;&#21253;&#25324;Switch Transformer&#65288;Fedus&#31561;&#65292;2021&#65289;&#21644;HashLaye&#22312;&#20869;&#30340;&#29616;&#26377;MoE&#26550;&#26500;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#26469;&#21387;&#32553;&#22810;&#35821;&#35328;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20302;&#31209;&#30697;&#38453;&#26469;&#26500;&#24314;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;Fuse Distillation&#25216;&#26415;&#23558;&#22810;&#20010;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#20013;&#30340;&#30693;&#35782;&#21387;&#32553;&#21040;&#19968;&#20010;&#20849;&#20139;&#27169;&#22359;&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#27169;&#22411;&#24207;&#21015;&#21270;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.13993</link><description>&lt;p&gt;
&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#21387;&#32553;&#22810;&#35821;&#35328;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Condensing Multilingual Knowledge with Lightweight Language-Specific Modules. (arXiv:2305.13993v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#26469;&#21387;&#32553;&#22810;&#35821;&#35328;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#20302;&#31209;&#30697;&#38453;&#26469;&#26500;&#24314;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#65292;&#24182;&#20351;&#29992;Fuse Distillation&#25216;&#26415;&#23558;&#22810;&#20010;&#35821;&#35328;&#29305;&#23450;&#27169;&#22359;&#20013;&#30340;&#30693;&#35782;&#21387;&#32553;&#21040;&#19968;&#20010;&#20849;&#20139;&#27169;&#22359;&#20013;&#65292;&#25552;&#39640;&#20102;&#25512;&#29702;&#21644;&#27169;&#22411;&#24207;&#21015;&#21270;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#29305;&#23450;&#65288;LS&#65289;&#27169;&#22359;&#32435;&#20837;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#19968;&#31181;&#25552;&#21319;&#24615;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#19982;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#27861;&#30456;&#20284;&#65292;&#22240;&#20026;&#23427;&#19981;&#20250;&#22686;&#21152;FLOPs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20840;&#36830;&#25509;&#23618;&#20013;&#24341;&#20837;&#30340;&#20840;&#31209;&#30697;&#38453;&#25152;&#24102;&#26469;&#30340;&#21442;&#25968;&#25968;&#37327;&#22826;&#22810;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#25193;&#23637;&#21040;&#25968;&#30334;&#31181;&#35821;&#35328;&#65288;&#19987;&#23478;&#65289;&#26102;&#24448;&#24448;&#38590;&#20197;&#31649;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#35821;&#35328;&#29305;&#23450;&#30697;&#38453;&#21512;&#25104;&#65288;LMS&#65289;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#20004;&#20010;&#26174;&#33879;&#36739;&#23567;&#30340;&#30697;&#38453;&#29983;&#25104;&#20302;&#31209;&#30697;&#38453;&#26469;&#26500;&#24314;LS&#27169;&#22359;&#20197;&#36817;&#20284;&#20840;&#31209;&#30697;&#38453;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;Fuse Distillation&#65288;FD&#65289;&#25216;&#26415;&#23558;&#22810;&#20010;LS&#27169;&#22359;&#20013;&#30340;&#22810;&#35821;&#35328;&#30693;&#35782;&#21387;&#32553;&#21040;&#19968;&#20010;&#20849;&#20139;&#27169;&#22359;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#21644;&#27169;&#22411;&#24207;&#21015;&#21270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;LMS&#26041;&#27861;&#22312;&#19982;&#21516;&#26679;&#25968;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#24773;&#20917;&#19979;&#26174;&#33879;&#20248;&#20110;&#20043;&#21069;&#30340;LS&#26041;&#27861;&#21644;MoE&#26041;&#27861;&#65292;&#20363;&#22914;1.73 BLE
&lt;/p&gt;
&lt;p&gt;
Incorporating language-specific (LS) modules is a proven method to boost performance in multilingual machine translation. This approach bears similarity to Mixture-of-Experts (MoE) because it does not inflate FLOPs. However, the scalability of this approach to hundreds of languages (experts) tends to be unmanageable due to the prohibitive number of parameters introduced by full-rank matrices in fully-connected layers. In this work, we introduce the Language-Specific Matrix Synthesis (LMS) method. This approach constructs LS modules by generating low-rank matrices from two significantly smaller matrices to approximate the full-rank matrix. Furthermore, we condense multilingual knowledge from multiple LS modules into a single shared module with the Fuse Distillation (FD) technique to improve the efficiency of inference and model serialization. We show that our LMS method significantly outperforms previous LS methods and MoE methods with the same amount of extra parameters, e.g., 1.73 BLE
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13971</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#32422;&#26463;&#30340;&#35821;&#35328;&#27169;&#22411;&#28789;&#27963;&#35299;&#30721;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Flexible Grammar-Based Constrained Decoding for Language Models. (arXiv:2305.13971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#20016;&#23500;&#35299;&#30721;&#27493;&#39588;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35821;&#27861;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#38598;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#23569;&#37327;&#26679;&#26412;&#34920;&#29616;&#65292;&#20294;&#22312;&#29983;&#25104;&#20449;&#24687;&#25552;&#21462;&#25152;&#38656;&#30340;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26102;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20010;&#38480;&#21046;&#28304;&#20110;LLM&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#20542;&#21521;&#20110;&#29983;&#25104;&#33258;&#30001;&#25991;&#26412;&#32780;&#19981;&#26159;&#36981;&#24490;&#29305;&#23450;&#35821;&#27861;&#30340;&#31934;&#30830;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35299;&#30721;&#27493;&#39588;&#20013;&#20351;&#29992;&#24418;&#24335;&#35821;&#27861;&#32422;&#26463;&#26469;&#20016;&#23500;&#27169;&#22411;&#12290;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#65292;&#21482;&#26377;&#31526;&#21512;&#35821;&#27861;&#20135;&#29983;&#35268;&#21017;&#30340;&#26377;&#25928;&#20196;&#29260;&#33021;&#34987;&#32771;&#34385;&#21040;&#12290;&#36825;&#26679;&#23601;&#24378;&#21046;&#21482;&#20135;&#29983;&#26377;&#25928;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#38750;&#24120;&#36890;&#29992;&#21644;&#28789;&#27963;&#65292;&#20801;&#35768;&#20219;&#20309;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;(CFG)&#38598;&#25104;&#21040;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#32422;&#26463;beam&#25628;&#32034;&#23454;&#29616;&#20013;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;NLP&#20219;&#21153;&#30340;&#36755;&#20986;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#24418;&#24335;&#35821;&#35328;&#65292;&#20351;&#23427;&#20204;&#36866;&#21512;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30452;&#25509;&#20351;&#29992;&#12290;&#23545;&#20110;&#36755;&#20986;&#31354;&#38388;&#21462;&#20915;&#20110;&#36755;&#20837;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36755;&#20837;&#30340;CFG&#65292;&#26681;&#25454;&#29305;&#23450;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#26356;&#26032;&#20135;&#29983;&#35268;&#21017;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#22797;&#26434;&#36755;&#20986;&#32467;&#26500;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22235;&#20010;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs have shown impressive few-shot performance across many tasks. However, they still struggle when it comes to generating complex output structures, such as those required for Information Extraction. This limitation stems from the fact that LLMs, without finetuning, tend to generate free text rather than precise structures that follow a specific grammar. In this work, we propose to enrich the decoding step with formal grammar constraints. During beam search, only valid token continuations compliant with the grammar production rules are considered. This enforces the generation of valid sequences exclusively. Our framework is highly general and flexible, allowing any Context-Free Grammar (CFG) to be integrated into our custom constrained beam search implementation. We demonstrate that the outputs of many NLP tasks can be represented as formal languages, making them suitable for direct use in our framework. For task where the output space is dependent on the input, we propose input-depe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13735</link><description>&lt;p&gt;
&#36890;&#36807;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Aligning Large Language Models through Synthetic Feedback. (arXiv:2305.13735v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13735
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21512;&#25104;&#21453;&#39304;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#21147;&#25104;&#26412;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#20854;&#20013;&#65292;&#36890;&#36807;&#23545;&#23610;&#23544;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890; LLMS&#30340;&#21709;&#24212;&#36827;&#34892;&#22870;&#21169;&#24314;&#27169;&#65292;&#26469;&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#25552;&#20379;&#22797;&#26434;&#30340;LLMs&#25511;&#21046;&#65292;&#20363;&#22914;&#20351;&#23427;&#20204;&#25353;&#29031;&#29305;&#23450;&#30340;&#25351;&#20196;&#25805;&#20316;&#32780;&#19981;&#20250;&#20135;&#29983;&#26377;&#23475;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#31867;&#31034;&#33539;&#21644;&#21453;&#39304;&#12290;&#26368;&#36817;&#65292;&#24320;&#28304;&#27169;&#22411;&#35797;&#22270;&#36890;&#36807;&#25552;&#28860;&#26469;&#33258;&#24050;&#23545;&#40784;&#30340;LLMs&#65288;&#22914;InstructGPT&#25110;ChatGPT&#65289;&#30340;&#25968;&#25454;&#26469;&#22797;&#21046;&#23545;&#40784;&#23398;&#20064;&#36807;&#31243;&#12290;&#34429;&#28982;&#36825;&#20010;&#36807;&#31243;&#20943;&#23569;&#20102;&#20154;&#21147;&#25104;&#26412;&#65292;&#20294;&#26159;&#26500;&#24314;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#25945;&#24072;&#27169;&#22411;&#30340;&#20381;&#36182;&#24615;&#24456;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#23545;&#40784;&#23398;&#20064;&#26694;&#26550;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#20154;&#31867;&#21171;&#21160;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#39044;&#20808;&#23545;&#40784;&#30340;LLMs&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#23567;&#21644;&#25552;&#31034;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#26222;&#36890;LLMs&#30340;&#21709;&#24212;&#36827;&#34892;&#21512;&#25104;&#21453;&#39304;&#30340;&#22870;&#21169;&#24314;&#27169;(RM)&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;RM&#27169;&#25311;&#39640;&#36136;&#37327;&#30340;&#31034;&#33539;&#26469;&#35757;&#32451;&#30417;&#30563;&#31574;&#30053;&#65292;&#24182;&#36827;&#19968;&#27493;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) to human values has become increasingly important as it enables sophisticated steering of LLMs, e.g., making them follow given instructions while keeping them less toxic. However, it requires a significant amount of human demonstrations and feedback. Recently, open-sourced models have attempted to replicate the alignment learning process by distilling data from already aligned LLMs like InstructGPT or ChatGPT. While this process reduces human efforts, constructing these datasets has a heavy dependency on the teacher models. In this work, we propose a novel framework for alignment learning with almost no human labor and no dependency on pre-aligned LLMs. First, we perform reward modeling (RM) with synthetic feedback by contrasting responses from vanilla LLMs with various sizes and prompts. Then, we use the RM for simulating high-quality demonstrations to train a supervised policy and for further optimizing the model with reinforcement learning. Our 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.13631</link><description>&lt;p&gt;
&#22522;&#20110;&#23454;&#20307;&#30340;&#22810;&#27169;&#24577;&#32593;&#32476;&#20869;&#23481;&#22270;&#20687;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
EDIS: Entity-Driven Image Search over Multimodal Web Content. (arXiv:2305.13631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13631
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;EDIS&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;100&#19975;&#20010;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#37197;&#23545;&#65292;&#26088;&#22312;&#40723;&#21169;&#24320;&#21457;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#21644;&#21305;&#37197;&#30340;&#26816;&#32034;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#23454;&#38469;&#25628;&#32034;&#24212;&#29992;&#20013;&#23454;&#29616;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#38656;&#35201;&#22312;&#25968;&#25454;&#38598;&#35268;&#27169;&#12289;&#23454;&#20307;&#29702;&#35299;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#34701;&#21512;&#26041;&#38754;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Making image retrieval methods practical for real-world search applications requires significant progress in dataset scales, entity comprehension, and multimodal information fusion. In this work, we introduce \textbf{E}ntity-\textbf{D}riven \textbf{I}mage \textbf{S}earch (EDIS), a challenging dataset for cross-modal image search in the news domain. EDIS consists of 1 million web images from actual search engine results and curated datasets, with each image paired with a textual description. Unlike datasets that assume a small set of single-modality candidates, EDIS reflects real-world web image search scenarios by including a million multimodal image-text pairs as candidates. EDIS encourages the development of retrieval models that simultaneously address cross-modal information fusion and matching. To achieve accurate ranking results, a model must: 1) understand named entities and events from text queries, 2) ground entities onto images or text descriptions, and 3) effectively fuse tex
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;</title><link>http://arxiv.org/abs/2305.13547</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup&#65306;&#22686;&#24378;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13547
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24448;&#24448;&#36935;&#21040;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20351;&#29992; Mixup &#36827;&#34892;&#25968;&#25454;&#25193;&#20805;&#24050;&#32463;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968; Mixup &#26041;&#27861;&#24182;&#19981;&#32771;&#34385;&#35757;&#32451;&#19981;&#21516;&#38454;&#27573;&#30340;&#23398;&#20064;&#38590;&#24230;&#24046;&#24322;&#24182;&#20135;&#29983;&#24102;&#26377; one hot &#26631;&#31614;&#30340;&#26032;&#26679;&#26412;&#65292;&#23548;&#33268;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#65288;SE&#65289;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#12290;SE &#20851;&#27880;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#21464;&#21270;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32447;&#24615;&#25554;&#20540;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#21407;&#22987;&#26679;&#26412;&#30340; one-hot &#26631;&#31614;&#65292;&#20197;&#29983;&#25104;&#26032;&#30340;&#36719;&#26631;&#31614;&#29992;&#20110;&#28151;&#21512;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification tasks often encounter few shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup has shown to be effective on various text classification tasks. However, most of the mixup methods do not consider the varying degree of learning difficulty in different stages of training and generate new samples with one hot labels, resulting in the model over confidence. In this paper, we propose a self evolution learning (SE) based mixup approach for data augmentation in text classification, which can generate more adaptive and model friendly pesudo samples for the model training. SE focuses on the variation of the model's learning ability. To alleviate the model confidence, we introduce a novel instance specific label smoothing approach, which linearly interpolates the model's output and one hot labels of the original samples to generate new soft for label mixing up. Through experimental analysis, in addition to improving cla
&lt;/p&gt;</description></item><item><title>Look-back&#26159;&#19968;&#31181;&#33258;&#30001;&#29983;&#25104;&#25991;&#26412;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#39044;&#27979;&#21487;&#33021;&#30340;&#37325;&#22797;&#30701;&#35821;&#21644;&#20027;&#39064;&#20559;&#31227;&#65292;&#24182;&#21024;&#38500;&#21487;&#33021;&#23548;&#33268;&#22833;&#36133;&#30340;&#20196;&#29260;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#27969;&#30021;&#21644;&#36830;&#36143;&#12290;</title><link>http://arxiv.org/abs/2305.13477</link><description>&lt;p&gt;
&#33258;&#30001;&#29983;&#25104;&#25991;&#26412;&#30340;&#22238;&#28335;&#35299;&#30721;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Look-back Decoding for Open-Ended Text Generation. (arXiv:2305.13477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13477
&lt;/p&gt;
&lt;p&gt;
Look-back&#26159;&#19968;&#31181;&#33258;&#30001;&#29983;&#25104;&#25991;&#26412;&#30340;&#25913;&#36827;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#39044;&#27979;&#21487;&#33021;&#30340;&#37325;&#22797;&#30701;&#35821;&#21644;&#20027;&#39064;&#20559;&#31227;&#65292;&#24182;&#21024;&#38500;&#21487;&#33021;&#23548;&#33268;&#22833;&#36133;&#30340;&#20196;&#29260;&#65292;&#29983;&#25104;&#30340;&#25991;&#26412;&#26356;&#27969;&#30021;&#21644;&#36830;&#36143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#21069;&#32512;&#65288;&#19978;&#19979;&#25991;&#65289;&#26102;&#65292;&#33258;&#30001;&#29983;&#25104;&#23601;&#24847;&#21619;&#30528;&#35299;&#30721;&#20986;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#19981;&#20250;&#31361;&#28982;&#20559;&#31163;&#21069;&#38754;&#30340;&#20027;&#39064;&#21644;&#20449;&#24687;&#37327;&#22810;&#65292;&#19981;&#20250;&#20986;&#29616;&#19981;&#24517;&#35201;&#30340;&#37325;&#22797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Look-back&#31639;&#27861;&#65292;&#19968;&#31181;&#25913;&#36827;&#30340;&#35299;&#30721;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;Kullback-Leibler&#25955;&#24230;&#26469;&#36319;&#36394;&#24403;&#21069;&#21644;&#21382;&#21490;&#35299;&#30721;&#27493;&#39588;&#20043;&#38388;&#30340;&#20998;&#24067;&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;Look-back&#21487;&#20197;&#33258;&#21160;&#39044;&#27979;&#21487;&#33021;&#30340;&#37325;&#22797;&#30701;&#35821;&#21644;&#20027;&#39064;&#20559;&#31227;&#65292;&#24182;&#21024;&#38500;&#21487;&#33021;&#23548;&#33268;&#22833;&#36133;&#30340;&#20196;&#29260;&#65292;&#23558;&#19979;&#19968;&#20010;&#20196;&#29260;&#27010;&#29575;&#20998;&#24067;&#38480;&#21046;&#22312;&#21487;&#33021;&#30340;&#36317;&#31163;&#33539;&#22260;&#20869;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#31526;&#21512;&#21382;&#21490;&#12290;&#25105;&#20204;&#22312;&#25991;&#26723;&#24310;&#32493;&#21644;&#25925;&#20107;&#29983;&#25104;&#19978;&#25191;&#34892;&#35299;&#30721;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;Look-back&#33021;&#22815;&#29983;&#25104;&#26356;&#27969;&#30021;&#21644;&#36830;&#36143;&#30340;&#25991;&#26412;&#65292;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#24378;&#30340;&#35299;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a prefix (context), open-ended generation aims to decode texts that are coherent, which don't abruptly drift from previous topics, and informative, which don't suffer from undesired repetitions. In this paper, we propose Look-back, an improved decoding algorithm that leverages the Kullback-Leibler divergence to track the distribution distance between current and historical decoding steps. Thus Look-back can automatically predict potential repetitive phrase and topic drift, and remove tokens that may cause the failure modes, restricting the next token probability distribution within a plausible distance to the history. We perform decoding experiments on document continuation and story generation, and demonstrate that Look-back is able to generate more fluent and coherent text, outperforming other strong decoding methods significantly in both automatic and human evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13469</link><description>&lt;p&gt;
MAILEX: &#37038;&#20214;&#20107;&#20214;&#19982;&#21442;&#25968;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
MAILEX: Email Event and Argument Extraction. (arXiv:2305.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#37038;&#20214;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;\dataset&#65292;&#27604;&#36739;&#20102;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#23384;&#22312;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#38590;&#28857;&#65292;&#26410;&#26469;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598; \dataset&#65292;&#29992;&#20110;&#20174;&#37038;&#20214;&#20018;&#20013;&#25191;&#34892;&#20107;&#20214;&#25277;&#21462;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#20102;&#37038;&#20214;&#39046;&#22495;&#20013;&#30340; 10 &#31181;&#20107;&#20214;&#31867;&#22411;&#21644; 76 &#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#25968;&#25454;&#38598;&#21253;&#25324;&#32422; 4K &#23553;&#26631;&#35760;&#26377;&#32422; 9K &#20010;&#20107;&#20214;&#23454;&#20363;&#30340;&#37038;&#20214;&#12290;&#20026;&#20102;&#20102;&#35299;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;&#65292;&#21363;&#24207;&#21015;&#26631;&#35760;&#21644;&#29983;&#25104;&#24335;&#31471;&#21040;&#31471;&#25277;&#21462;&#65288;&#21253;&#25324;&#20960;&#29575; GPT-3.5&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#37038;&#20214;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#36828;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#22240;&#20026;&#23384;&#22312;&#35832;&#22810;&#38590;&#28857;&#65292;&#20363;&#22914;&#25552;&#21462;&#38750;&#36830;&#32493;&#20849;&#20139;&#35302;&#21457;&#22120;&#36328;&#24230;&#12289;&#25552;&#21462;&#38750;&#21629;&#21517;&#23454;&#20307;&#21442;&#25968;&#21644;&#24314;&#27169;&#37038;&#20214;&#20250;&#35805;&#21382;&#21490;&#31561;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#29305;&#23450;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#20013;&#38656;&#35201;&#36827;&#34892;&#26356;&#22810;&#30740;&#31350;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present the first dataset, \dataset, for performing event extraction from conversational email threads. To this end, we first proposed a new taxonomy covering 10 event types and 76 arguments in the email domain. Our final dataset includes $\sim$4K emails annotated with $\sim$9K event instances. To understand the task challenges, we conducted a series of experiments comparing two commonly-seen lines of approaches for event extraction, i.e., sequence labeling and generative end-to-end extraction (including few-shot GPT-3.5). Our results showed that the task of email event extraction is far from being addressed, due to challenges lying in, e.g., extracting non-continuous, shared trigger spans, extracting non-named entity arguments, and modeling the email conversational history. Our work thus suggests more investigations in this domain-specific event extraction task in the future.\footnote{The source code and dataset can be obtained from \url{https://github.com/salokr/Emai
&lt;/p&gt;</description></item><item><title>DADA&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#20010;&#26041;&#35328;&#65292;&#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#21160;&#24577;&#32858;&#21512;&#36866;&#37197;&#22120;&#65292;&#21487;&#20026;SAE&#35757;&#32451;&#30340;&#27169;&#22411;&#36171;&#20104;&#22810;&#26041;&#35328;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#38024;&#23545;&#29305;&#23450;&#26041;&#35328;&#21464;&#20307;&#36827;&#34892;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.13406</link><description>&lt;p&gt;
DADA: &#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#21160;&#24577;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
DADA: Dialect Adaptation via Dynamic Aggregation of Linguistic Rules. (arXiv:2305.13406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13406
&lt;/p&gt;
&lt;p&gt;
DADA&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#20010;&#26041;&#35328;&#65292;&#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#21160;&#24577;&#32858;&#21512;&#36866;&#37197;&#22120;&#65292;&#21487;&#20026;SAE&#35757;&#32451;&#30340;&#27169;&#22411;&#36171;&#20104;&#22810;&#26041;&#35328;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#38024;&#23545;&#29305;&#23450;&#26041;&#35328;&#21464;&#20307;&#36827;&#34892;&#36866;&#24212;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20027;&#35201;&#38598;&#20013;&#20110;&#26631;&#20934;&#32654;&#24335;&#33521;&#35821;&#65288;SAE&#65289;&#65292;&#22312;&#24212;&#29992;&#20110;&#20854;&#20182;&#33521;&#35821;&#26041;&#35328;&#26102;&#34920;&#29616;&#24448;&#24448;&#36739;&#24046;&#12290;&#32780;&#29616;&#26377;&#30340;&#32531;&#35299;&#26041;&#27861;&#38024;&#23545;&#21333;&#20010;&#30446;&#26631;&#26041;&#35328;&#30340;&#20559;&#24046;&#65292;&#20294;&#20551;&#35774;&#20102;&#21487;&#20197;&#35775;&#38382;&#39640;&#31934;&#24230;&#30340;&#26041;&#35328;&#35782;&#21035;&#31995;&#32479;&#12290;&#26041;&#35328;&#20043;&#38388;&#30340;&#30028;&#38480;&#22266;&#26377;&#24377;&#24615;&#65292;&#20351;&#24471;&#23558;&#35821;&#35328;&#21010;&#20998;&#20026;&#31163;&#25955;&#39044;&#23450;&#20041;&#30340;&#33539;&#30068;&#26356;&#21152;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DADA&#65288;&#22522;&#20110;&#35821;&#35328;&#35268;&#21017;&#30340;&#26041;&#35328;&#36866;&#24212;&#24615;&#21160;&#24577;&#32858;&#21512;&#65289;&#65292;&#19968;&#31181;&#36890;&#36807;&#32452;&#21512;&#22788;&#29702;&#29305;&#23450;&#35821;&#35328;&#29305;&#24449;&#30340;&#36866;&#37197;&#22120;&#65292;&#20026;SAE&#35757;&#32451;&#30340;&#27169;&#22411;&#36171;&#20104;&#22810;&#26041;&#35328;&#30340;&#40065;&#26834;&#24615;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#12290;DADA&#30340;&#32452;&#21512;&#26550;&#26500;&#20801;&#35768;&#26377;&#38024;&#23545;&#24615;&#22320;&#36866;&#24212;&#29305;&#23450;&#26041;&#35328;&#21464;&#20307;&#65292;&#21516;&#26102;&#36866;&#24212;&#21508;&#31181;&#26041;&#35328;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DADA&#23545;&#20110;&#21333;&#20219;&#21153;&#21644;&#25351;&#20196;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#37117;&#26159;&#26377;&#25928;&#30340;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#21644;&#21487;&#35299;&#37322;&#30340;&#26694;&#26550;&#26469;&#36866;&#24212;&#21508;&#31181;&#26041;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models (LLMs) that mainly focus on Standard American English (SAE) often lead to significantly worse performance when being applied to other English dialects. While existing mitigations tackle discrepancies for individual target dialects, they assume access to high-accuracy dialect identification systems. The boundaries between dialects are inherently flexible, making it difficult to categorize language into discrete predefined categories. In this paper, we propose DADA (Dialect Adaptation via Dynamic Aggregation), a modular approach to imbue SAE-trained models with multi-dialectal robustness by composing adapters which handle specific linguistic features. The compositional architecture of DADA allows for both targeted adaptation to specific dialect variants and simultaneous adaptation to various dialects. We show that DADA is effective for both single task and instruction finetuned language models, offering an extensible and interpretable framework for adapting
&lt;/p&gt;</description></item><item><title>&#20803;&#35821;&#35328;&#25552;&#31034;&#19982;&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#30456;&#27604;&#65292;&#23545;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#26469;&#35828;&#65292;&#20803;&#35821;&#35328;&#21028;&#26029;&#25928;&#26524;&#36739;&#24046;&#65292;&#24182;&#19988;&#38543;&#30528;&#25552;&#31034;&#26597;&#35810;&#20559;&#31163;&#30452;&#25509;&#27979;&#37327;&#30340;&#27010;&#29575;&#65292;&#19968;&#33268;&#24615;&#21464;&#24046;&#12290;&#25552;&#31034;&#30340;&#36127;&#38754;&#32467;&#26524;&#19981;&#33021;&#20316;&#20026;&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#27010;&#25324;&#30340;&#30830;&#20991;&#35777;&#25454;&#12290;&#20174;&#38381;&#28304;API&#36801;&#31227;&#20013;&#65292;&#25105;&#20204;&#20063;&#20250;&#22833;&#21435;&#19968;&#23450;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.13264</link><description>&lt;p&gt;
&#25552;&#31034;&#19981;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27010;&#29575;&#27979;&#37327;&#30340;&#26367;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompting is not a substitute for probability measurements in large language models. (arXiv:2305.13264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13264
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35821;&#35328;&#25552;&#31034;&#19982;&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#30456;&#27604;&#65292;&#23545;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#26469;&#35828;&#65292;&#20803;&#35821;&#35328;&#21028;&#26029;&#25928;&#26524;&#36739;&#24046;&#65292;&#24182;&#19988;&#38543;&#30528;&#25552;&#31034;&#26597;&#35810;&#20559;&#31163;&#30452;&#25509;&#27979;&#37327;&#30340;&#27010;&#29575;&#65292;&#19968;&#33268;&#24615;&#21464;&#24046;&#12290;&#25552;&#31034;&#30340;&#36127;&#38754;&#32467;&#26524;&#19981;&#33021;&#20316;&#20026;&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#27010;&#25324;&#30340;&#30830;&#20991;&#35777;&#25454;&#12290;&#20174;&#38381;&#28304;API&#36801;&#31227;&#20013;&#65292;&#25105;&#20204;&#20063;&#20250;&#22833;&#21435;&#19968;&#23450;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#29616;&#22312;&#26159;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35821;&#35328;&#30693;&#35782;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#32780;&#20854;&#20182;&#26041;&#27861;&#30452;&#25509;&#35835;&#21462;&#27169;&#22411;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#25552;&#31034;&#38656;&#35201;&#27169;&#22411;&#36890;&#36807;&#22788;&#29702;&#35821;&#35328;&#36755;&#20837;&#26469;&#35775;&#38382;&#36825;&#20123;&#20869;&#37096;&#20449;&#24687;&#65292;&#20174;&#32780;&#38544;&#21547;&#22320;&#27979;&#35797;&#19968;&#31181;&#26032;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#65306;&#20803;&#35821;&#35328;&#21028;&#26029;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20803;&#35821;&#35328;&#25552;&#31034;&#21644;&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#20316;&#20026;&#34913;&#37327;&#27169;&#22411;&#35821;&#35328;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;LLM&#30340;&#20803;&#35821;&#35328;&#21028;&#26029;&#19981;&#22914;&#30452;&#25509;&#20174;&#34920;&#31034;&#20013;&#27966;&#29983;&#30340;&#25968;&#37327;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#25552;&#31034;&#26597;&#35810;&#20559;&#31163;&#30452;&#25509;&#27979;&#37327;&#19979;&#19968;&#20010;&#21333;&#35789;&#27010;&#29575;&#65292;&#19968;&#33268;&#24615;&#21464;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20381;&#36182;&#20110;&#20803;&#35821;&#35328;&#25552;&#31034;&#30340;&#36127;&#38754;&#32467;&#26524;&#19981;&#33021;&#20316;&#20026;LLM&#32570;&#20047;&#29305;&#23450;&#35821;&#35328;&#27010;&#25324;&#30340;&#30830;&#20991;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#31361;&#26174;&#20102;&#20174;&#38381;&#28304;API&#36801;&#31227;&#20013;&#25152;&#22833;&#21435;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs 
&lt;/p&gt;</description></item><item><title>SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.13186</link><description>&lt;p&gt;
SCITAB: &#19968;&#20010;&#23545;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
SCITAB: A Challenging Benchmark for Compositional Reasoning and Claim Verification on Scientific Tables. (arXiv:2305.13186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13186
&lt;/p&gt;
&lt;p&gt;
SCITAB&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#21644;&#30456;&#20851;&#30340;&#31185;&#23398;&#34920;&#26684;&#65292;&#35201;&#27714;&#36827;&#34892;&#32452;&#21512;&#25512;&#29702;&#21644;&#20107;&#23454;&#39564;&#35777;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;SCITAB&#25552;&#20986;&#20102;&#35768;&#22810;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#27495;&#20041;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25152;&#26377;&#27169;&#22411;&#20013;&#65292;&#38500;&#20102;GPT-4&#20043;&#22806;&#65292;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#25552;&#31034;&#25216;&#26415;&#22914;&#24605;&#32500;&#38142;&#23545;&#20110;&#22312;SCITAB&#19978;&#25552;&#21319;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#31185;&#23398;&#20107;&#23454;&#26680;&#26597;&#22522;&#20934;&#27979;&#35797;&#23384;&#22312;&#19968;&#20123;&#19981;&#36275;&#65292;&#20363;&#22914;&#26469;&#33258;&#20247;&#21253;&#23457;&#26597;&#30340;&#20559;&#35265;&#21644;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#35777;&#25454;&#30340;&#36807;&#24230;&#20381;&#36182;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SCITAB&#65292;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1.2K&#20010;&#32463;&#39564;&#35777;&#30340;&#31185;&#23398;&#20107;&#23454;&#65292;&#36825;&#20123;&#20107;&#23454;1&#65289;&#26469;&#28304;&#20110;&#30495;&#23454;&#30340;&#31185;&#23398;&#20986;&#29256;&#29289;&#65292;2&#65289;&#38656;&#35201;&#32452;&#21512;&#25512;&#29702;&#36827;&#34892;&#39564;&#35777;&#12290;&#36825;&#20123;&#20107;&#23454;&#19982;&#21253;&#21547;&#35777;&#25454;&#30340;&#31185;&#23398;&#34920;&#26684;&#36827;&#34892;&#37197;&#23545;&#65292;&#24182;&#36827;&#34892;&#20102;&#26631;&#35760;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;SCITAB&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#34920;&#26684;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#38500;&#20102;GPT-4&#22806;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#20165;&#30053;&#39640;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#27969;&#34892;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#23545;SCITAB&#30340;&#24615;&#33021;&#25552;&#21319;&#19981;&#22823;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SCITAB&#25552;&#20986;&#30340;&#20960;&#20010;&#29420;&#29305;&#25361;&#25112;&#65292;&#21253;&#25324;&#34920;&#26684;&#23450;&#20301;&#12289;&#20107;&#23454;&#30340;&#27495;&#20041;&#24615;&#21644;&#32452;&#21512;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current scientific fact-checking benchmarks exhibit several shortcomings, such as biases arising from crowd-sourced claims and an over-reliance on text-based evidence. We present SCITAB, a challenging evaluation dataset consisting of 1.2K expert-verified scientific claims that 1) originate from authentic scientific publications and 2) require compositional reasoning for verification. The claims are paired with evidence-containing scientific tables annotated with labels. Through extensive evaluations, we demonstrate that SCITAB poses a significant challenge to state-of-the-art models, including table-based pretraining models and large language models. All models except GPT-4 achieved performance barely above random guessing. Popular prompting techniques, such as Chain-of-Thought, do not achieve much performance gains on SCITAB. Our analysis uncovers several unique challenges posed by SCITAB, including table grounding, claim ambiguity, and compositional reasoning. Our codes and data are 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DecoMT&#65292;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#25552;&#31034;&#27861;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#35821;&#35328;&#30340;&#21333;&#35843;&#23545;&#40784;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#31995;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13085</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#30340;&#20998;&#35299;&#25552;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decomposed Prompting for Machine Translation Between Related Languages using Large Language Models. (arXiv:2305.13085v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13085
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DecoMT&#65292;&#19968;&#31181;&#22522;&#20110;&#20998;&#35299;&#25552;&#31034;&#27861;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#35821;&#35328;&#30340;&#21333;&#35843;&#23545;&#40784;&#29305;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#35821;&#31995;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32763;&#35793;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#30456;&#20851;&#35821;&#35328;&#20043;&#38388;&#30340;&#26426;&#22120;&#32763;&#35793;&#65292;&#21363;&#22312;&#21516;&#19968;&#35821;&#31995;&#20013;&#20855;&#26377;&#35789;&#24207;&#21644;&#35789;&#27719;&#30456;&#20284;&#24615;&#31561;&#35821;&#35328;&#29305;&#24449;&#30340;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#30340;&#26426;&#22120;&#32763;&#35793;&#21033;&#29992;&#19968;&#23567;&#32452;&#32763;&#35793;&#23545;&#31034;&#20363;&#26469;&#29983;&#25104;&#27979;&#35797;&#21477;&#23376;&#30340;&#32763;&#35793;&#12290;&#36825;&#20010;&#36807;&#31243;&#35201;&#27714;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#29983;&#25104;&#32763;&#35793;&#65292;&#21516;&#26102;&#30830;&#20445;&#20196;&#29260;&#39034;&#24207;&#20445;&#25345;&#19968;&#33268;&#65292;&#20197;&#20135;&#29983;&#27969;&#30021;&#20934;&#30830;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#22312;&#30456;&#20851;&#35821;&#35328;&#20013;&#65292;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#35821;&#35328;&#30340;&#21333;&#35843;&#23545;&#40784;&#29305;&#24615;&#26469;&#31616;&#21270;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;DecoMT&#65292;&#19968;&#31181;&#23558;&#32763;&#35793;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#35789;&#22359;&#32763;&#35793;&#30340;&#26032;&#39062;&#25552;&#31034;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#30456;&#20851;&#35821;&#35328;&#23545;&#30340;&#19981;&#21516;&#35821;&#31995;&#19978;&#36827;&#34892;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#35299;&#25552;&#31034;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates machine translation between related languages i.e., languages within the same family that share linguistic characteristics such as word order and lexical similarity. Machine translation through few-shot prompting leverages a small set of translation pair examples to generate translations for test sentences. This procedure requires the model to learn how to generate translations while simultaneously ensuring that token ordering is maintained to produce a fluent and accurate translation. We propose that for related languages, the task of machine translation can be simplified by leveraging the monotonic alignment characteristic of such languages. We introduce DecoMT, a novel approach of few-shot prompting that decomposes the translation process into a sequence of word chunk translations. Through automatic and human evaluation conducted on multiple related language pairs across various language families, we demonstrate that our proposed approach of decomposed prompt
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#25903;&#25345;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#19987;&#23478;&#23545;&#26631;&#31614;&#21644;&#35299;&#37322;&#30340;&#27880;&#37322;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.12710</link><description>&lt;p&gt;
&#36229;&#36234;&#26631;&#31614;&#65306;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#26550;&#26500;&#36171;&#20104;&#20154;&#31867;&#26631;&#27880;&#32773;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Labels: Empowering Human Annotators with Natural Language Explanations through a Novel Active-Learning Architecture. (arXiv:2305.12710v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20027;&#21160;&#23398;&#20064;&#26550;&#26500;&#65292;&#36890;&#36807;&#25972;&#21512;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#21644;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#37319;&#29992;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;&#37319;&#26679;&#31574;&#30053;&#65292;&#25903;&#25345;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#19987;&#23478;&#23545;&#26631;&#31614;&#21644;&#35299;&#37322;&#30340;&#27880;&#37322;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#39046;&#22495;&#19987;&#23478;&#65288;&#22914;&#21307;&#29983;&#65289;&#24456;&#23569;&#20165;&#27880;&#37322;&#20915;&#31574;&#26631;&#31614;&#32780;&#19981;&#25552;&#20379;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20302;&#36164;&#28304;&#23398;&#20064;&#25216;&#26415;&#65292;&#22914;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#65292;&#26088;&#22312;&#25903;&#25345;&#20154;&#31867;&#26631;&#27880;&#32773;&#65292;&#21364;&#22823;&#22810;&#21482;&#20851;&#27880;&#26631;&#31614;&#32780;&#24573;&#35270;&#25968;&#25454;&#28857;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;AL&#26550;&#26500;&#65292;&#20197;&#25903;&#25345;&#19987;&#23478;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#23545;&#26631;&#31614;&#21644;&#35299;&#37322;&#27880;&#37322;&#30340;&#23454;&#38469;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;AL&#26550;&#26500;&#21033;&#29992;&#19968;&#20010;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#65292;&#26681;&#25454;&#20154;&#31867;&#35299;&#37322;&#29983;&#25104;&#35299;&#37322;&#65292;&#19968;&#20010;&#21033;&#29992;&#29983;&#25104;&#35299;&#37322;&#36827;&#34892;&#39044;&#27979;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#25968;&#25454;&#22810;&#26679;&#24615;&#30340;AL&#37319;&#26679;&#31574;&#30053;&#65292;&#20174;&#35299;&#37322;&#27880;&#37322;&#20013;&#21463;&#30410;&#12290;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#35777;&#26126;&#20102;&#23558;&#35299;&#37322;&#32435;&#20837;AL&#37319;&#26679;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#30340;AL&#26550;&#26500;&#25913;&#21892;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world domain experts (e.g., doctors) rarely annotate only a decision label in their day-to-day workflow without providing explanations. Yet, existing low-resource learning techniques, such as Active Learning (AL), that aim to support human annotators mostly focus on the label while neglecting the natural language explanation of a data point. This work proposes a novel AL architecture to support experts' real-world need for label and explanation annotations in low-resource scenarios. Our AL architecture leverages an explanation-generation model to produce explanations guided by human explanations, a prediction model that utilizes generated explanations toward prediction faithfully, and a novel data diversity-based AL sampling strategy that benefits from the explanation annotations. Automated and human evaluations demonstrate the effectiveness of incorporating explanations into AL sampling and the improved human annotation efficiency and trustworthiness with our AL architecture. Add
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33258;&#21160;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;AuT-Few&#28040;&#38500;&#20102;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.12576</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automated Few-shot Classification with Instruction-Finetuned Language Models. (arXiv:2305.12576v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12576
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#35843;&#20248;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;AuT-Few&#28040;&#38500;&#20102;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#30340;&#38656;&#27714;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#19968;&#31867;&#25104;&#21151;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#25552;&#31034;&#32467;&#21512;&#36215;&#26469;&#65292;&#36825;&#20123;&#25552;&#31034;&#26159;&#25163;&#24037;&#21046;&#20316;&#30340;&#20219;&#21153;&#25551;&#36848;&#65292;&#20197;&#34917;&#20805;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#25163;&#24037;&#35774;&#35745;&#25552;&#31034;&#36890;&#24120;&#38656;&#35201;&#39046;&#22495;&#30693;&#35782;&#21644;&#22823;&#37327;&#29468;&#27979;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25552;&#31034;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AuT-Few&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#23545;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#30340;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#65288;i&#65289;&#19968;&#20010;&#20174;&#35843;&#20248;&#25351;&#20196;&#30693;&#35782;&#24211;&#20013;&#36873;&#25321;&#21512;&#36866;&#20219;&#21153;&#25351;&#20196;&#30340;&#25351;&#20196;&#26816;&#32034;&#27169;&#22359;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#29983;&#25104;&#20004;&#20010;&#19981;&#21516;&#30340;&#12289;&#35821;&#20041;&#26377;&#24847;&#20041;&#30340;&#31867;&#21035;&#25551;&#36848;&#21644;&#36873;&#25321;&#26426;&#21046;&#12290;&#22312;&#21253;&#25324;8&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;12&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AuT-Few&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;AuT-Few&#22312;&#25968;&#25454;&#38598;&#19978;&#26159;&#25490;&#21517;&#26368;&#39640;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
A particularly successful class of approaches for few-shot learning combines language models with prompts -- hand-crafted task descriptions that complement data samples. However, designing prompts by hand for each task commonly requires domain knowledge and substantial guesswork. We observe, in the context of classification tasks, that instruction finetuned language models exhibit remarkable prompt robustness, and we subsequently propose a simple method to eliminate the need for handcrafted prompts, named AuT-Few. This approach consists of (i) a prompt retrieval module that selects suitable task instructions from the instruction-tuning knowledge base, and (ii) the generation of two distinct, semantically meaningful, class descriptions and a selection mechanism via cross-validation. Over $12$ datasets, spanning $8$ classification tasks, we show that AuT-Few outperforms current state-of-the-art few-shot learning methods. Moreover, AuT-Few is the best ranking method across datasets on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12517</link><description>&lt;p&gt;
&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Retrieving Texts based on Abstract Descriptions. (arXiv:2305.12517v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#35821;&#20041;&#26816;&#32034;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25688;&#35201;&#25551;&#36848;&#30340;&#25991;&#26412;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#24403;&#21069;&#30340;&#25991;&#26412;&#23884;&#20837;&#26041;&#27861;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#38024;&#23545;&#25991;&#26412;&#30340;&#20449;&#24687;&#25552;&#21462;&#65292;&#25351;&#20196;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#25991;&#26723;&#38598;&#21512;&#20013;&#23450;&#20301;&#31526;&#21512;&#32473;&#23450;&#25551;&#36848;&#30340;&#25991;&#26412;&#65288;&#35821;&#20041;&#26816;&#32034;&#65289;&#24182;&#19981;&#36866;&#29992;&#12290;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#30340;&#30456;&#20284;&#24230;&#25628;&#32034;&#21487;&#20197;&#36890;&#36807;&#26597;&#35810;&#25191;&#34892;&#26816;&#32034;&#65292;&#20294;&#23884;&#20837;&#20013;&#30340;&#30456;&#20284;&#24230;&#23450;&#20041;&#19981;&#26126;&#30830;&#19988;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#23545;&#20110;&#35768;&#22810;&#29992;&#20363;&#26469;&#35828;&#37117;&#26159;&#27425;&#20248;&#30340;&#12290;&#37027;&#20040;&#65292;&#20160;&#20040;&#26159;&#26377;&#25928;&#26816;&#32034;&#30340;&#22909;&#30340;&#26597;&#35810;&#34920;&#31034;&#65311;&#25105;&#20204;&#30830;&#23450;&#20102;&#26681;&#25454;&#20869;&#23481;&#30340;&#25688;&#35201;&#25551;&#36848;&#26816;&#32034;&#21477;&#23376;&#30340;&#26126;&#30830;&#23450;&#20041;&#19988;&#19968;&#33268;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#25991;&#26412;&#23884;&#20837;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#27169;&#22411;&#65292;&#22312;&#26631;&#20934;&#26368;&#36817;&#37051;&#25628;&#32034;&#20013;&#30340;&#34920;&#29616;&#26174;&#33879;&#25552;&#21319;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#36890;&#36807;&#25552;&#31034;LLM&#33719;&#24471;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#34429;&#28982;&#24456;&#23481;&#26131;&#20174;LLM&#20013;&#33719;&#24471;&#35757;&#32451;&#26448;&#26009;&#65292;&#20294;LLM&#26080;&#27861;&#30452;&#25509;&#25191;&#34892;&#26816;&#32034;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While instruction-tuned Large Language Models (LLMs) excel at extracting information from text, they are not suitable for locating texts conforming to a given description in a large document collection (semantic retrieval). Similarity search over embedding vectors does allow to perform retrieval by query, but the similarity reflected in the embedding is ill-defined and non-consistent, and is sub-optimal for many use cases. What, then, is a good query representation for effective retrieval?  We identify the well defined and consistent task of retrieving sentences based on abstract descriptions of their content. We demonstrate the inadequacy of current text embeddings and propose an alternative model that significantly improves when used in standard nearest neighbor search. The model is trained using positive and negative pairs sourced through prompting a LLM. While it is easy to source the training material from an LLM, the retrieval task cannot be performed by the LLM directly. This de
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.12421</link><description>&lt;p&gt;
&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-QA Evaluation. (arXiv:2305.12421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#35780;&#20272;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;QA-Eval&#21644;&#25968;&#25454;&#38598;EVOUNA&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#21644;&#25913;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#23545;&#20110;&#26410;&#26469;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#21457;&#23637;&#21644;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23545;&#24320;&#25918;&#24335;&#38382;&#31572;&#65288;Open-QA&#65289;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#35813;&#20219;&#21153;&#21487;&#20197;&#30452;&#25509;&#20272;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20107;&#23454;&#24615;&#12290;&#30446;&#21069;&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#20154;&#24037;&#35780;&#20272;&#20173;&#28982;&#26159;&#26368;&#21487;&#38752;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#35780;&#20272;QA&#35780;&#20272;&#65288;QA-Eval&#65289;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#65292;&#26088;&#22312;&#35780;&#20272;AI&#29983;&#25104;&#30340;&#31572;&#26696;&#19982;Open-QA&#20013;&#30340;&#26631;&#20934;&#31572;&#26696;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#32467;&#26524;&#26469;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#37027;&#20123;&#19982;&#20154;&#24037;&#35780;&#20272;&#20855;&#26377;&#39640;&#24230;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#35748;&#20026;&#23427;&#20204;&#26356;&#21487;&#38752;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#32570;&#38519;&#20197;&#21450;&#25913;&#36827;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#20010;&#26032;&#30340;QA-Eval&#20219;&#21153;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;EVOUNA&#23558;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#35780;&#20272;&#24037;&#20855;&#30340;&#24320;&#21457;&#65292;&#24182;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#20855;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, Evaluating QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#20027;&#39064;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#30830;&#23450;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.12152</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#20027;&#39064;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Re-visiting Automated Topic Model Evaluation with Large Language Models. (arXiv:2305.12152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#20027;&#39064;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#30830;&#23450;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#23545;&#22823;&#22411;&#25991;&#26412;&#38598;&#21512;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#35780;&#20272;&#20027;&#39064;&#27169;&#22411;&#36755;&#20986;&#21644;&#30830;&#23450;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#19968;&#30452;&#26159;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#23578;&#26080;&#26377;&#25928;&#30340;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#36825;&#31181;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#33021;&#36866;&#24403;&#22320;&#35780;&#20272;&#20986;&#20135;&#29983;&#30340;&#20027;&#39064;&#65292;&#24182;&#19988;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#33021;&#21542;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#30340;&#20027;&#39064;&#25968;&#37327;&#12290;&#25105;&#20204;&#33258;&#21160;&#20026;&#25991;&#26723;&#20998;&#37197;&#26631;&#31614;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#26368;&#32431;&#20928;&#26631;&#31614;&#30340;&#37197;&#32622;&#65292;&#20197;&#36820;&#22238;&#21512;&#29702;&#30340;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are used to make sense of large text collections. However, automatically evaluating topic model output and determining the optimal number of topics both have been longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models to evaluate such output. We find that large language models appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. We then investigate whether we can use large language models to automatically determine the optimal number of topics. We automatically assign labels to documents and choosing configurations with the most pure labels returns reasonable values for the optimal number of topics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#28304;&#25991;&#26412;&#20013;&#30340;&#32534;&#36753;&#25805;&#20316;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11862</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32534;&#36753;&#25805;&#20316;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;
&lt;/p&gt;
&lt;p&gt;
Reducing Sequence Length by Predicting Edit Operations with Large Language Models. (arXiv:2305.11862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#28304;&#25991;&#26412;&#20013;&#30340;&#32534;&#36753;&#25805;&#20316;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#20851;&#27880;&#12290; LLMs&#20063;&#34987;&#29992;&#20110;&#26412;&#22320;&#24207;&#21015;&#36716;&#25442;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#65288;GEC&#65289;&#21644;&#24418;&#24335;&#39118;&#26684;&#36716;&#25442;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#28304;&#25991;&#26412;&#20013;&#30340;&#22823;&#37096;&#20998;&#35760;&#21495;&#20445;&#25345;&#19981;&#21464;&#12290; &#28982;&#32780;&#65292;&#29983;&#25104;&#25152;&#26377;&#30446;&#26631;&#35760;&#21495;&#26159;&#20302;&#25928;&#29575;&#30340;&#65292;&#22240;&#20026;&#30446;&#26631;&#35760;&#21495;&#30340;&#39044;&#27979;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#22312;&#39044;&#27979;&#21518;&#32493;&#35760;&#21495;&#26102;&#20986;&#29616;&#28798;&#38590;&#65292;&#24182;&#19988;&#38543;&#30528;&#30446;&#26631;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#26412;&#22320;&#24207;&#21015;&#36716;&#25442;&#20219;&#21153;&#30340;&#28304;&#25991;&#26412;&#30340;&#19968;&#32452;&#32534;&#36753;&#25805;&#20316;&#30340;&#26041;&#27861;&#12290; &#36890;&#36807;&#34920;&#31034;&#19968;&#20010;&#32534;&#36753;&#25805;&#20316;&#30340;&#28304;&#25991;&#26412;&#36328;&#24230;&#21644;&#26356;&#25913;&#30340;&#35760;&#21495;&#65292;&#25105;&#20204;&#21487;&#20197;&#20943;&#23569;&#30446;&#26631;&#24207;&#21015;&#30340;&#38271;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290; &#25105;&#20204;&#22312;&#32534;&#36753;&#25805;&#20316;&#30340;&#30417;&#30563;&#25968;&#25454;&#19978;&#24212;&#29992;&#25351;&#23548;&#35843;&#20248;LLMs&#12290; &#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance in various tasks and gained significant attention. LLMs are also used for local sequence transduction tasks, including grammatical error correction (GEC) and formality style transfer, where most tokens in a source text are kept unchanged. However, it is inefficient to generate all target tokens because a prediction error of a target token may cause a catastrophe in predicting subsequent tokens and because the computational cost grows quadratically with the target sequence length. This paper proposes to predict a set of edit operations for the source text for local sequence transduction tasks. Representing an edit operation with a span of the source text and changed tokens, we can reduce the length of the target sequence and thus the computational cost for inference. We apply instruction tuning for LLMs on the supervision data of edit operations. Experiments show that the proposed method achieves comparable performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65288;HELMA&#65289;&#65292;&#20854;&#20026;&#26631;&#20934;&#21270;&#21644;&#21487;&#38752;&#30340;&#20272;&#31639;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#34920;&#26126;&#20854;&#23384;&#22312;&#24187;&#35273;&#30340;&#39118;&#38505;&#24182;&#20026;&#37492;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11747</link><description>&lt;p&gt;
HELMA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65288;HELMA&#65289;&#65292;&#20854;&#20026;&#26631;&#20934;&#21270;&#21644;&#21487;&#38752;&#30340;&#20272;&#31639;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#34920;&#26126;&#20854;&#23384;&#22312;&#24187;&#35273;&#30340;&#39118;&#38505;&#24182;&#20026;&#37492;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#23481;&#26131;&#29983;&#25104;&#24187;&#35273;&#65292;&#21363;&#19982;&#28304;&#20869;&#23481;&#20914;&#31361;&#25110;&#26080;&#27861;&#36890;&#36807;&#20107;&#23454;&#30693;&#35782;&#36827;&#34892;&#39564;&#35777;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#20102;&#35299;LLMs&#20250;&#20135;&#29983;&#21738;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hallucination Evaluation for Large Language Models&#65288;HELMA&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#29983;&#25104;&#30340;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#24187;&#35273;&#26679;&#26412;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#21644;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#29983;&#25104;&#36825;&#20123;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#20004;&#27493;&#26694;&#26550;&#65292;&#21363;&#37319;&#26679;-&#36807;&#28388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;&#25351;&#20196;&#29983;&#25104;&#24187;&#35273;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#31034;&#20363;&#22686;&#24378;&#36807;&#28388;&#26041;&#27861;&#36873;&#25321;&#26368;&#22909;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32856;&#35831;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#21592;&#26469;&#27880;&#37322;ChatGPT&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;ChatGPT&#26377;&#19968;&#23450;&#30340;&#27010;&#29575;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#23384;&#22312;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;HELMA&#22522;&#20934;&#21487;&#20316;&#20026;&#35782;&#21035;&#21644;&#20943;&#36731;LLMs&#24187;&#35273;&#38382;&#39064;&#30340;&#26631;&#20934;&#21270;&#21487;&#38752;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, \ie content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HELMA) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, \ie sampling-then-filtering. Specifically, we first adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced filtering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and exist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#26469;&#34913;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#20851;&#38190;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.11550</link><description>&lt;p&gt;
&#36879;&#36807;&#34920;&#24449;&#38236;&#22836;&#30475;&#24453;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Viewing Knowledge Transfer in Multilingual Machine Translation Through a Representational Lens. (arXiv:2305.11550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11550
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#26469;&#34913;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#20851;&#38190;&#29305;&#24449;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#65292;&#20165;&#20165;&#20381;&#38752;&#32763;&#35793;&#36136;&#37327;&#24230;&#37327;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#36716;&#31227;&#26159;&#19981;&#22815;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#34920;&#24449;&#36716;&#31227;&#28508;&#21147;&#65288;RTP&#65289;&#65292;&#23427;&#27979;&#37327;&#35821;&#35328;&#20043;&#38388;&#30340;&#34920;&#24449;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RTP&#21487;&#20197;&#27979;&#37327;&#27491;&#21521;&#21644;&#36127;&#21521;&#36716;&#31227;&#65288;&#24178;&#25200;&#65289;&#65292;&#24182;&#21457;&#29616;RTP&#19982;&#32763;&#35793;&#36136;&#37327;&#21464;&#21270;&#24378;&#30456;&#20851;&#65292;&#34920;&#26126;&#30830;&#23454;&#23384;&#22312;&#36716;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#36716;&#31227;&#30456;&#20851;&#30340;&#25968;&#25454;&#21644;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#21457;&#29616;&#22810;&#36335;&#24182;&#34892;&#37325;&#21472;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#40092;&#26377;&#25506;&#32034;&#30340;&#29305;&#24449;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20351;&#29992;&#36741;&#21161;&#30456;&#20284;&#24615;&#25439;&#22833;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#36335;&#24182;&#34892;&#25968;&#25454;&#65292;&#40723;&#21169;&#34920;&#24449;&#22312;&#35821;&#35328;&#20043;&#38388;&#26356;&#20855;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#21644;&#27169;&#22411;&#35774;&#32622;&#20013;&#25552;&#39640;&#20102;&#20302;&#36164;&#28304;&#21644;&#20013;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We argue that translation quality alone is not a sufficient metric for measuring knowledge transfer in multilingual neural machine translation. To support this claim, we introduce Representational Transfer Potential (RTP), which measures representational similarities between languages. We show that RTP can measure both positive and negative transfer (interference), and find that RTP is strongly correlated with changes in translation quality, indicating that transfer does occur. Furthermore, we investigate data and language characteristics that are relevant for transfer, and find that multi-parallel overlap is an important yet under-explored feature. Based on this, we develop a novel training scheme, which uses an auxiliary similarity loss that encourages representations to be more invariant across languages by taking advantage of multi-parallel data. We show that our method yields increased translation quality for low- and mid-resource languages across multiple data and model setups.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10786</link><description>&lt;p&gt;
Ditto: &#19968;&#31181;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#31616;&#27905;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ditto: A Simple and Efficient Approach to Improve Sentence Embeddings. (arXiv:2305.10786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10786
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Ditto&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#35299;&#20915;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#35786;&#26029;&#20102;&#30001;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#20135;&#29983;&#30340;&#21477;&#23376;&#34920;&#31034;&#20013;&#23384;&#22312;&#30340;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#65292;&#27809;&#26377;&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;BERT&#20135;&#29983;&#30340;&#21477;&#23376;&#23884;&#20837;&#23384;&#22312;&#20559;&#21521;&#20110;&#38750;&#20449;&#24687;&#24615;&#21333;&#35789;&#30340;&#20559;&#35265;&#65292;&#38480;&#21046;&#20102;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#8212;&#8212;&#23545;&#35282;&#32447;&#27880;&#24847;&#21147;&#27744;&#21270;&#65288;Ditto&#65289;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#20272;&#35745;&#26435;&#37325;&#21333;&#35789;&#65292;&#24182;&#35745;&#31639;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21333;&#35789;&#34920;&#31034;&#30340;&#21152;&#26435;&#24179;&#22343;&#20540;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#12290;Ditto&#21487;&#20197;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#21518;&#22788;&#29702;&#25805;&#20316;&#12290;&#19982;&#20808;&#21069;&#30340;&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#30456;&#27604;&#65292;Ditto&#19981;&#28155;&#21152;&#21442;&#25968;&#20063;&#19981;&#38656;&#35201;&#20219;&#20309;&#23398;&#20064;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;Ditto&#21487;&#20197;&#32531;&#35299;&#21508;&#21521;&#24322;&#24615;&#38382;&#39064;&#24182;&#25552;&#39640;&#22312;STS&#20219;&#21153;&#20013;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior studies diagnose the anisotropy problem in sentence representations from pre-trained language models, e.g., BERT, without fine-tuning. Our analysis reveals that the sentence embeddings from BERT suffer from a bias towards uninformative words, limiting the performance in semantic textual similarity (STS) tasks. To address this bias, we propose a simple and efficient unsupervised approach, Diagonal Attention Pooling (Ditto), which weights words with model-based importance estimations and computes the weighted average of word representations from pre-trained models as sentence embeddings. Ditto can be easily applied to any pre-trained language model as a postprocessing operation. Compared to prior sentence embedding approaches, Ditto does not add parameters nor requires any learning. Empirical evaluations demonstrate that our proposed Ditto can alleviate the anisotropy problem and improve various pre-trained models on STS tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10713</link><description>&lt;p&gt;
&#24179;&#22374;&#24230;&#24863;&#30693;&#30340;Prompt&#36873;&#25321;&#33021;&#25552;&#39640;&#31934;&#24230;&#21644;&#26679;&#26412;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Flatness-Aware Prompt Selection Improves Accuracy and Sample Efficiency. (arXiv:2305.10713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;--Prompt&#24179;&#22374;&#24230;&#65292;&#21487;&#20197;&#20248;&#21270;&#35821;&#35328;&#25552;&#31034;&#36873;&#25321;&#65292;&#25552;&#39640;&#27169;&#22411;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#32467;&#21512;&#29616;&#26377;&#24230;&#37327;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#38271;&#65292;&#25552;&#31034;&#24050;&#25104;&#20026;&#35775;&#38382;&#23427;&#20204;&#30340;&#20027;&#35201;&#26041;&#24335;&#12290;&#36825;&#28608;&#21457;&#20102;&#33258;&#21160;&#36873;&#25321;&#26377;&#25928;&#35821;&#35328;&#25552;&#31034;&#31574;&#30053;&#30340;&#21457;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;Prompt&#24179;&#22374;&#24230;&#65292;&#19968;&#31181;&#37327;&#21270;&#35821;&#35328;&#25552;&#31034;&#39044;&#26399;&#25928;&#29992;&#30340;&#26032;&#24230;&#37327;&#12290;&#35813;&#24230;&#37327;&#21463;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#24179;&#22374;&#24230;&#27491;&#21017;&#21270;&#21551;&#21457;&#65292;&#37327;&#21270;&#27169;&#22411;&#23545;&#20854;&#21442;&#25968;&#25200;&#21160;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#35813;&#24230;&#37327;&#30340;&#29702;&#35770;&#22522;&#30784;&#21450;&#20854;&#19982;&#20854;&#20182;Prompt&#36873;&#25321;&#24230;&#37327;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20840;&#38754;&#20102;&#35299;&#29616;&#26377;&#26041;&#27861;&#12290;&#20174;&#32463;&#39564;&#19978;&#35762;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#23558;Prompt&#24179;&#22374;&#24230;&#19982;&#29616;&#26377;&#24230;&#37327;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;6&#20010;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#30340;&#24230;&#37327;&#20248;&#20110;&#20197;&#21069;&#30340;Prompt&#36873;&#25321;&#24230;&#37327;&#65292;&#24179;&#22343;&#31934;&#24230;&#25552;&#39640;5&#65285;&#65292;Pearson&#30456;&#20851;&#24615;&#25552;&#39640;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing capabilities of large language models, prompting them has become the dominant way to access them. This has motivated the development of strategies for automatically selecting effective language prompts. In this paper, we introduce prompt flatness, a new metric to quantify the expected utility of a language prompt. This metric is inspired by flatness regularization in statistical learning that quantifies the robustness of the model towards its parameter perturbations. We provide theoretical foundations for this metric and its relationship with other prompt selection metrics, providing a comprehensive understanding of existing methods. Empirically, we show that combining prompt flatness with existing metrics improves both performance and sample efficiency. Our metric outperforms the previous prompt selection metrics with an average increase of 5% in accuracy and 10% in Pearson correlation across 6 classification benchmarks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10434</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning the Visualness of Text Using Large Vision-Language Models. (arXiv:2305.10434v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10434
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22914;CLIP&#26469;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#65292;&#24182;&#25552;&#20986;fine-tuning&#31574;&#30053;&#65292;&#23558;&#38750;&#35270;&#35273;&#25991;&#26412;&#26144;&#23556;&#20026;NULL&#22270;&#20687;&#65292;&#21305;&#37197;&#35270;&#35273;&#25991;&#26412;&#19982;&#23545;&#24212;&#22270;&#20687;&#65292;&#20197;&#35299;&#38145;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26412;&#20250;&#22312;&#20154;&#20204;&#30340;&#33041;&#28023;&#20013;&#21576;&#29616;&#22270;&#20687;&#65292;&#32780;&#38750;&#35270;&#35273;&#25991;&#26412;&#21017;&#26080;&#27861;&#36798;&#21040;&#27492;&#25928;&#26524;&#12290;&#33258;&#21160;&#26816;&#27979;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#23558;&#26377;&#21161;&#20110;&#22312;&#25991;&#26412;&#20013;&#23884;&#20837;&#30456;&#20851;&#22270;&#20687;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;3620&#20010;&#33521;&#35821;&#21477;&#23376;&#21450;&#20854;&#22810;&#20010;&#20154;&#31867;&#27880;&#37322;&#32773;&#25552;&#20379;&#30340;&#35270;&#35273;&#24615;&#24471;&#20998;&#65292;&#24182;&#20351;&#29992;&#21253;&#21547;&#25991;&#26412;&#21644;&#35270;&#35273;&#36164;&#20135;&#30340;&#25991;&#26723;&#26469;&#21019;&#24314;&#36828;&#31243;&#30417;&#30563;&#35821;&#26009;&#24211;&#65292;&#20197;&#35780;&#20272;&#25991;&#26412;&#30340;&#35270;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual text evokes an image in a person's mind, while non-visual text fails to do so. A method to automatically detect visualness in text will unlock the ability to augment text with relevant images, as neural text-to-image generation and retrieval models operate on the implicit assumption that the input text is visual in nature. We curate a dataset of 3,620 English sentences and their visualness scores provided by multiple human annotators. Additionally, we use documents that contain text and visual assets to create a distantly supervised corpus of document text and associated images. We also propose a fine-tuning strategy that adapts large vision-language models like CLIP that assume a one-to-one correspondence between text and image to the task of scoring text visualness from text input alone. Our strategy involves modifying the model's contrastive learning objective to map text identified as non-visual to a common NULL image while matching visual text to their corresponding images 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20250;&#35758;&#25688;&#35201;&#26694;&#26550;SVB&#65292;&#36890;&#36807;&#19977;&#20010;&#36807;&#31243;&#21387;&#32553;&#20887;&#20313;&#20869;&#23481;&#24182;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#12290;&#20854;&#20013;&#65292;&#28369;&#21160;&#31383;&#21475;&#23545;&#35805;&#24674;&#22797;&#19982;&#35780;&#20998;&#12289;&#36890;&#36947;&#37325;&#35201;&#24615;&#24471;&#20998;&#25237;&#31080;&#21644;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#31561;&#31639;&#27861;&#29992;&#20110;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.07988</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#21477;&#23376;&#21387;&#32553;&#29992;&#20110;&#20250;&#35758;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20250;&#35758;&#25688;&#35201;&#26694;&#26550;SVB&#65292;&#36890;&#36807;&#19977;&#20010;&#36807;&#31243;&#21387;&#32553;&#20887;&#20313;&#20869;&#23481;&#24182;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#12290;&#20854;&#20013;&#65292;&#28369;&#21160;&#31383;&#21475;&#23545;&#35805;&#24674;&#22797;&#19982;&#35780;&#20998;&#12289;&#36890;&#36947;&#37325;&#35201;&#24615;&#24471;&#20998;&#25237;&#31080;&#21644;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#31561;&#31639;&#27861;&#29992;&#20110;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24635;&#32467;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#22240;&#20026;&#20250;&#35758;&#35821;&#26009;&#24211;&#36890;&#24120;&#28041;&#21450;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#20887;&#38271;&#23545;&#35805;&#24182;&#20805;&#26021;&#30528;&#20887;&#20313;&#21644;&#29712;&#30862;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVB&#65292;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#20250;&#35758;&#25688;&#35201;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#36807;&#31243;&#8220;&#21387;&#32553;&#8221;&#20887;&#20313;&#32780;&#20445;&#30041;&#37325;&#35201;&#20869;&#23481;&#65306;&#28369;&#21160;&#31383;&#21475;&#23545;&#35805;&#24674;&#22797;&#19982;&#35780;&#20998;&#12289;&#36890;&#36947;&#37325;&#35201;&#24615;&#24471;&#20998;&#25237;&#31080;&#21644;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#33258;&#30417;&#30563;&#33539;&#24335;&#19979;&#65292;&#28369;&#21160;&#31383;&#21475;&#35780;&#20998;&#26088;&#22312;&#20174;&#22810;&#20010;&#35270;&#35282;&#35780;&#20272;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#25237;&#31080;&#32858;&#21512;&#36825;&#20123;&#35780;&#20998;&#12290;&#20855;&#26377;&#39640;&#35780;&#20998;&#30340;&#26631;&#35760;&#23558;&#34987;&#35270;&#20026;&#26174;&#30528;&#20449;&#24687;&#24182;&#26631;&#35760;&#20026;&#8220;anchers&#8221;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20351;&#36755;&#20837;&#38271;&#24230;&#36866;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#24230;&#38480;&#21046;&#65292;&#37319;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional summarization model often fails to capture critical information in meeting transcripts, as meeting corpus usually involves multiple parties with lengthy conversations and is stuffed with redundant and trivial content. To tackle this problem, we present SVB, an effective and efficient framework for meeting summarization that `compress' the redundancy while preserving important content via three processes: sliding-window dialogue restoration and \textbf{S}coring, channel-wise importance score \textbf{V}oting, and relative positional \textbf{B}ucketing. Specifically, under the self-supervised paradigm, the sliding-window scoring aims to rate the importance of each token from multiple views. Then these ratings are aggregated by channel-wise voting. Tokens with high ratings will be regarded as salient information and labeled as \textit{anchors}. Finally, to tailor the lengthy input to an acceptable length for the language model, the relative positional bucketing algorithm i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZARA&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#23558;&#21512;&#29702;&#24615;&#21028;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#33258;&#21160;&#26500;&#24314;&#20266;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ZARA&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#19978;&#37117;&#34920;&#29616;&#20986;SOTA&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07355</link><description>&lt;p&gt;
ZARA&#65306;&#25913;&#36827;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#33258;&#29702;&#24615;
&lt;/p&gt;
&lt;p&gt;
ZARA: Improving Few-Shot Self-Rationalization for Small Language Models. (arXiv:2305.07355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ZARA&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#23558;&#21512;&#29702;&#24615;&#21028;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#33258;&#21160;&#26500;&#24314;&#20266;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;ZARA&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#19978;&#37117;&#34920;&#29616;&#20986;SOTA&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#29983;&#25104;&#32456;&#31471;&#20219;&#21153;&#31572;&#26696;&#21644;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#30340;&#35821;&#35328;&#27169;&#22411;&#34987;&#31216;&#20026;&#33258;&#25105;&#35299;&#37322;&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26377;&#29702;&#25454;&#30340;&#20363;&#23376;&#26469;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65292;&#23637;&#29616;&#20102;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25165;&#33021;&#21463;&#30410;&#20110;&#35299;&#37322;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#34987;&#33719;&#24471;&#12290;&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#35299;&#37322;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#33258;&#25105;&#35299;&#37322;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#37325;&#26032;&#25506;&#35752;&#20102;&#35299;&#37322;&#21644;&#31572;&#26696;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#21463;&#21040;&#20154;&#31867;&#22914;&#20309;&#35780;&#20272;&#35299;&#37322;&#30340;&#38544;&#21547;&#24605;&#32771;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;ZARA&#65292;&#21363;&#29702;&#24615;&#31572;&#26696;&#23545;&#30340;&#38646;&#26679;&#26412;&#22686;&#24378;&#65292;&#36890;&#36807;&#23558;&#21512;&#29702;&#24615;&#21028;&#26029;&#38382;&#39064;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#33258;&#21160;&#26500;&#24314;&#20266;&#24179;&#34892;&#25968;&#25454;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;FEB&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;ZARA&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#36136;&#37327;&#19978;&#37117;&#21462;&#24471;&#20102;SOTA&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) that jointly generate end-task answers as well as free-text rationales are known as self-rationalization models. Recent works demonstrate great performance gain for self-rationalization by few-shot prompting LMs with rationale-augmented exemplars. However, the ability to benefit from explanations only emerges with large-scale LMs, which have poor accessibility. In this work, we explore the less-studied setting of leveraging explanations for small LMs to improve few-shot self-rationalization. We first revisit the relationship between rationales and answers. Inspired by the implicit mental process of how human beings assess explanations, we present a novel approach, Zero-shot Augmentation of Rationale-Answer pairs (ZARA), to automatically construct pseudo-parallel data for self-training by reducing the problem of plausibility judgement to natural language inference. Experimental results show ZARA achieves SOTA performance on the FEB benchmark, for both the task accu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;XLT&#65292;&#29992;&#20110;&#25552;&#39640;LLMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#19981;&#21516;&#35821;&#35328;&#20013;&#20219;&#21153;&#24615;&#33021;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07004</link><description>&lt;p&gt;
LLM &#19981;&#21516;&#35821;&#35328;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;: &#36890;&#36807;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;&#25552;&#39640;&#22810;&#35821;&#35328;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Not All Languages Are Created Equal in LLMs: Improving Multilingual Capability by Cross-Lingual-Thought Prompting. (arXiv:2305.07004v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07004
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;&#26041;&#27861;&#65292;&#21517;&#20026;XLT&#65292;&#29992;&#20110;&#25552;&#39640;LLMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#24615;&#33021;&#65292;&#24182;&#20943;&#23569;&#19981;&#21516;&#35821;&#35328;&#20013;&#20219;&#21153;&#24615;&#33021;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#20855;&#26377;&#26174;&#33879;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#36328;&#35821;&#35328;&#24605;&#32500;&#25552;&#31034;(XLT)&#65292;&#20197;&#31995;&#32479;&#22320;&#25552;&#39640;LLMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;XLT&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#26495;&#25552;&#31034;&#65292;&#21487;&#20197;&#21050;&#28608;&#36328;&#35821;&#35328;&#21644;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20197;&#22686;&#24378;&#19981;&#21516;&#35821;&#35328;&#30340;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#28085;&#30422;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;7&#20010;&#20856;&#22411;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#20840;&#38754;&#35780;&#20272;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XLT&#19981;&#20165;&#26174;&#33879;&#25552;&#39640;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#26174;&#33879;&#20943;&#23569;&#20102;&#19981;&#21516;&#35821;&#35328;&#20013;&#27599;&#20010;&#20219;&#21153;&#24179;&#22343;&#24615;&#33021;&#21644;&#26368;&#20339;&#24615;&#33021;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;XLT&#22312;&#31639;&#26415;&#25512;&#29702;&#21644;&#24320;&#25918;&#22495;&#38382;&#31572;&#26041;&#38754;&#24102;&#26469;&#20102;&#36229;&#36807;10&#20010;&#30334;&#20998;&#28857;&#30340;&#24179;&#22343;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) demonstrate impressive multilingual capability, but their performance varies substantially across different languages. In this work, we introduce a simple yet effective method, called cross-lingual-thought prompting (XLT), to systematically improve the multilingual capability of LLMs. Specifically, XLT is a generic template prompt that stimulates cross-lingual and logical reasoning skills to enhance task performance across languages. We conduct comprehensive evaluations on 7 typical benchmarks related to reasoning, understanding, and generation tasks, covering both high-resource and low-resource languages. Experimental results show that XLT not only remarkably enhances the performance of various multilingual tasks but also significantly reduces the gap between the average performance and the best performance of each task in different languages. Notably, XLT brings over 10 points of average improvement in arithmetic reasoning and open-domain question-answeri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06983</link><description>&lt;p&gt;
&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Active Retrieval Augmented Generation. (arXiv:2305.06983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20102;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20855;&#26377;&#29702;&#35299;&#21644;&#29983;&#25104;&#35821;&#35328;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#23427;&#20204;&#24448;&#24448;&#20250;&#20135;&#29983;&#34394;&#20551;&#30340;&#21644;&#38169;&#35823;&#30340;&#36755;&#20986;&#12290;&#20174;&#22806;&#37096;&#30693;&#35782;&#36164;&#28304;&#20013;&#26816;&#32034;&#20449;&#24687;&#26469;&#22686;&#24378;LM&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26816;&#32034;&#22686;&#24378;&#30340;LM&#37319;&#29992;&#19968;&#31181;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#35774;&#32622;&#65292;&#20165;&#22522;&#20110;&#36755;&#20837;&#19968;&#27425;&#26816;&#32034;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#29983;&#25104;&#38271;&#25991;&#26412;&#30340;&#26356;&#26222;&#36941;&#30340;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#25910;&#38598;&#20449;&#24687;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#36807;&#21435;&#26377;&#19968;&#20123;&#26816;&#32034;&#20449;&#24687;&#65292;&#21516;&#26102;&#29983;&#25104;&#36755;&#20986;&#30340;&#21162;&#21147;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#20351;&#29992;&#21069;&#19968;&#20010;&#19978;&#19979;&#25991;&#20316;&#20026;&#26597;&#35810;&#65292;&#22312;&#22266;&#23450;&#30340;&#26102;&#38388;&#38388;&#38548;&#20869;&#26816;&#32034;&#25991;&#26723;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#24191;&#20041;&#35270;&#22270;&#65292;&#21363;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#20027;&#21160;&#20915;&#23450;&#20309;&#26102;&#20197;&#21450;&#20309;&#26102;&#20174;&#21738;&#37324;&#26816;&#32034;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21069;&#30651;&#24615;&#20027;&#21160;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;FLARE&#65289;&#65292;&#23427;&#36890;&#36807;&#20801;&#35768;&#29983;&#25104;&#22120;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#20027;&#21160;&#26597;&#35810;&#26816;&#32034;&#32452;&#20214;&#26469;&#26356;&#32039;&#23494;&#22320;&#38598;&#25104;&#20027;&#21160;&#26816;&#32034;&#21644;&#29983;&#25104;&#12290;FLARE&#22312;&#19968;&#32452;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#20027;&#21160;&#26816;&#32034;&#22312;&#25972;&#20010;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval-augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout the generation process is essential. There have been some past efforts to retrieve information multiple times while generating outputs, which mostly retrieve documents at fixed intervals using the previous context as queries. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval aug
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NLP4SGPAPERS&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26144;&#23556;&#12289;&#20219;&#21153;&#21450;&#26041;&#27861;&#30340;&#30830;&#23450;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#25972;&#20010;ACL&#25991;&#38598;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#23637;&#31034;&#20102;NLP4SG&#39046;&#22495;&#30340;&#20840;&#35980;&#12290;</title><link>http://arxiv.org/abs/2305.05471</link><description>&lt;p&gt;
&#36229;&#36234;&#21892;&#24847;&#65306;NLP&#29992;&#20110;&#31038;&#20250;&#20844;&#30410;&#30340;&#30740;&#31350;&#29616;&#29366;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NLP4SGPAPERS&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26144;&#23556;&#12289;&#20219;&#21153;&#21450;&#26041;&#27861;&#30340;&#30830;&#23450;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#25972;&#20010;ACL&#25991;&#38598;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#23637;&#31034;&#20102;NLP4SG&#39046;&#22495;&#30340;&#20840;&#35980;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20986;&#29616;&#22312;&#21508;&#31181;&#29992;&#20363;&#20013;&#12290;&#22312;&#20247;&#22810;&#30340;NLP&#24212;&#29992;&#20013;&#65292;&#35768;&#22810;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#21463;&#21040;&#28608;&#21169;&#65292;&#24076;&#26395;&#36890;&#36807;&#24037;&#20316;&#20855;&#26377;&#31215;&#26497;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#31526;&#21512;NLP for Social Good (NLP4SG)&#30340;&#26368;&#26032;&#20513;&#35758;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#24182;&#19981;&#24635;&#26159;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#24049;&#30340;&#30740;&#31350;&#24037;&#20316;&#22914;&#20309;&#35299;&#20915;&#24403;&#20170;&#30340;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;NLP4SGPAPERS&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#19977;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#31185;&#23398;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;NLP4SG&#35770;&#25991;&#65292;&#24182;&#36890;&#36807;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#23545;NLP4SG&#36827;&#34892;&#25551;&#36848;: (1)&#30830;&#23450;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;(2)&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;(SDGs)&#65292;&#20197;&#21450;(3)&#35782;&#21035;&#23427;&#20204;&#27491;&#22312;&#35299;&#20915;&#30340;&#20219;&#21153;&#21644;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#25972;&#20010;ACL&#25991;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;NLP4SG&#39046;&#22495;&#30340;&#40479;&#30640;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today's big social problems. Thus, in this paper, we introduce NLP4SGPAPERS, a scientific dataset with three associated tasks that can help identify NLP4SG papers and characterize the NLP4SG landscape by: (1) identifying the papers that address a social problem, (2) mapping them to the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying the task they are solving and the methods they are using. Using state-of-the-art NLP models, we address each of these tasks and use them on the entire ACL Anthology, resulting in a visualization workspace that gives resear
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#8221;&#30340;&#20219;&#21153;&#65292;&#30446;&#30340;&#22312;&#20110;&#20174;&#30693;&#35782;&#26469;&#28304;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26723;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#30340;IRP&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#35821;&#35328;&#27169;&#22411;&#30340;&#32570;&#28857;&#65292;&#23454;&#29616;&#20102;&#20869;&#23481;&#35268;&#21010;&#12289;&#20107;&#23454;&#36873;&#25321; &#21644;&#25913;&#20889;&#27493;&#39588;&#20998;&#21035;&#22788;&#29702;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#25991;&#26723;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.03276</link><description>&lt;p&gt;
&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#65306;&#27169;&#20223;&#12289;&#26816;&#32034;&#12289;&#25913;&#20889;
&lt;/p&gt;
&lt;p&gt;
Expository Text Generation: Imitate, Retrieve, Paraphrase. (arXiv:2305.03276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03276
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#8220;&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#8221;&#30340;&#20219;&#21153;&#65292;&#30446;&#30340;&#22312;&#20110;&#20174;&#30693;&#35782;&#26469;&#28304;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26723;&#65292;&#30740;&#31350;&#32773;&#25552;&#20986;&#30340;IRP&#26694;&#26550;&#33021;&#22815;&#20811;&#26381;&#35821;&#35328;&#27169;&#22411;&#30340;&#32570;&#28857;&#65292;&#23454;&#29616;&#20102;&#20869;&#23481;&#35268;&#21010;&#12289;&#20107;&#23454;&#36873;&#25321; &#21644;&#25913;&#20889;&#27493;&#39588;&#20998;&#21035;&#22788;&#29702;&#65292;&#20351;&#24471;&#29983;&#25104;&#30340;&#25991;&#26723;&#20855;&#26377;&#36739;&#39640;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#19994;&#24615;&#25991;&#26723;&#26159;&#21521;&#35835;&#32773;&#20256;&#36798;&#22797;&#26434;&#20449;&#24687;&#30340;&#37325;&#35201;&#36164;&#28304;&#12290;&#23613;&#31649;&#23427;&#20204;&#24456;&#26377;&#29992;&#65292;&#20294;&#25163;&#21160;&#32534;&#20889;&#19987;&#19994;&#24615;&#25991;&#29486;&#26159;&#32791;&#26102;&#19988;&#21171;&#21160;&#24378;&#24230;&#22823;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#23545;&#25152;&#28041;&#39046;&#22495;&#26377;&#28145;&#20837;&#20102;&#35299;&#12289;&#31934;&#24515;&#35268;&#21010;&#20869;&#23481;&#12289;&#20197;&#21450;&#33021;&#22815;&#20174;&#22810;&#20010;&#26469;&#28304;&#32508;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#20943;&#36731;&#36127;&#25285;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19987;&#19994;&#24615;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#65292;&#23427;&#26088;&#22312;&#20174;&#30693;&#35782;&#26469;&#28304;&#33258;&#21160;&#29983;&#25104;&#20934;&#30830;&#12289;&#20449;&#24687;&#20016;&#23500;&#30340;&#19987;&#19994;&#24615;&#25991;&#26723;&#12290;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;IRP&#35299;&#20915;&#20102;&#36825;&#20010;&#20219;&#21153;&#65292;IRP&#26159;&#19968;&#20010;&#36845;&#20195;&#26694;&#26550;&#65292;&#20811;&#26381;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#23616;&#38480;&#65292;&#20998;&#21035;&#22788;&#29702;&#20102;&#20869;&#23481;&#35268;&#21010;&#12289;&#20107;&#23454;&#36873;&#25321;&#21644;&#25913;&#20889;&#30340;&#27493;&#39588;&#12290;&#36890;&#36807;&#23545;&#19977;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;IRP&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#19987;&#19994;&#24615;&#25991;&#26723;&#65292;&#20934;&#30830;&#22320;&#21521;&#35835;&#32773;&#20256;&#36882;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expository documents are vital resources for conveying complex information to readers. Despite their usefulness, writing expository documents by hand is a time-consuming and labor-intensive process that requires knowledge of the domain of interest, careful content planning, and the ability to synthesize information from multiple sources. To ease these burdens, we introduce the task of expository text generation, which seeks to automatically generate an accurate and informative expository document from a knowledge source. We solve our task by developing IRP, an iterative framework that overcomes the limitations of language models and separately tackles the steps of content planning, fact selection, and rephrasing. Through experiments on three diverse datasets, we demonstrate that IRP produces high-quality expository documents that accurately inform readers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02996</link><description>&lt;p&gt;
&#24102;&#26377;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;CUR k-NN&#25628;&#32034;&#30340;&#33258;&#36866;&#24212;&#38170;&#23450;&#39033;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Adaptive Selection of Anchor Items for CUR-based k-NN search with Cross-Encoders. (arXiv:2305.02996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#38170;&#28857;&#36873;&#25321;&#26041;&#27861;&#65292;&#20197;&#25913;&#21892;ANNCUR&#27169;&#22411;&#20013;&#39640;&#21069;k&#39033;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#21484;&#22238;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25345;&#36739;&#23567;&#30340;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#19982;&#38543;&#26426;&#25277;&#26679;&#38170;&#28857;&#30456;&#24403;&#25110;&#32773;&#26356;&#22909;&#30340;k-NN&#21484;&#22238;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-encoder models, which jointly encode and score a query-item pair, are typically prohibitively expensive for k-nearest neighbor search. Consequently, k-NN search is performed not with a cross-encoder, but with a heuristic retrieve (e.g., using BM25 or dual-encoder) and re-rank approach. Recent work proposes ANNCUR (Yadav et al., 2022) which uses CUR matrix factorization to produce an embedding space for efficient vector-based search that directly approximates the cross-encoder without the need for dual-encoders. ANNCUR defines this shared query-item embedding space by scoring the test query against anchor items which are sampled uniformly at random. While this minimizes average approximation error over all items, unsuitably high approximation error on top-k items remains and leads to poor recall of top-k (and especially top-1) items. Increasing the number of anchor items is a straightforward way of improving the approximation error and hence k-NN recall of ANNCUR but at the cost o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.02239</link><description>&lt;p&gt;
&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#22312;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
The Benefits of Label-Description Training for Zero-Shot Text Classification. (arXiv:2305.02239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26631;&#27880;&#25551;&#36848;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#20998;&#31867;&#20013;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#26356;&#40065;&#26834;&#22320;&#22788;&#29702;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#20801;&#35768;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36716;&#31227;&#35821;&#20041;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#23567;&#30340;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25551;&#36848;&#20219;&#21153;&#26631;&#31614;&#12290;&#19982;&#36890;&#24120;&#26377;&#25991;&#26412;&#26631;&#27880;&#26631;&#31614;&#30340;&#24494;&#35843;&#25968;&#25454;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#21482;&#26159;&#29992;&#35821;&#35328;&#25551;&#36848;&#26631;&#31614;&#65292;&#20363;&#22914;&#20351;&#29992;&#19968;&#20123;&#30456;&#20851;&#26415;&#35821;&#12289;&#35789;&#20856;/&#30334;&#31185;&#20840;&#20070;&#26465;&#30446;&#21644;&#30701;&#27169;&#26495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20027;&#39064;&#21644;&#24773;&#24863;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#27604;&#38646;&#26679;&#26412;&#39640;15-17&#65285;&#32477;&#23545;&#20540;&#12290;&#23427;&#36824;&#26356;&#20855;&#26377;&#38646;&#26679;&#26412;&#20998;&#31867;&#25152;&#38656;&#36873;&#25321;&#30340;&#40065;&#26834;&#24615;&#65292;&#20363;&#22914;&#25552;&#31034;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#30340;&#27169;&#24335;&#20197;&#21450;&#20174;&#26631;&#31614;&#26144;&#23556;&#21040;&#27169;&#22411;&#35789;&#27719;&#34920;&#20013;&#30340;&#20196;&#29260;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#20165;&#25551;&#36848;&#26631;&#31614;&#20294;&#19981;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#65292;&#22240;&#27492;&#22312;&#20854;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#31867;&#30340;&#37325;&#28857;&#26356;&#19987;&#27880;&#20110;&#26631;&#31614;&#32780;&#19981;&#26159;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have improved zero-shot text classification by allowing the transfer of semantic knowledge from the training data in order to classify among specific label sets in downstream tasks. We propose a simple way to further improve zero-shot accuracies with minimal effort. We curate small finetuning datasets intended to describe the labels for a task. Unlike typical finetuning data, which has texts annotated with labels, our data simply describes the labels in language, e.g., using a few related terms, dictionary/encyclopedia entries, and short templates. Across a range of topic and sentiment datasets, our method is more accurate than zero-shot by 15-17% absolute. It is also more robust to choices required for zero-shot classification, such as patterns for prompting the model to classify and mappings from labels to tokens in the model's vocabulary. Furthermore, since our data merely describes the labels but does not use input texts, finetuning on it yields a model that p
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#31232;&#30095;&#28608;&#27963;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Mixture-of-experts&#27169;&#22411;&#21442;&#25968;&#20302;&#25928;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02176</link><description>&lt;p&gt;
&#21521;&#21442;&#25968;&#25928;&#29575;&#36808;&#36827;&#65306;&#20855;&#26377;&#21160;&#24577;&#33021;&#21147;&#30340;&#20998;&#23618;&#31232;&#30095;&#28608;&#27963;Transformer
&lt;/p&gt;
&lt;p&gt;
Towards Being Parameter-Efficient: A Stratified Sparsely Activated Transformer with Dynamic Capacity. (arXiv:2305.02176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#31232;&#30095;&#28608;&#27963;Transformer&#27169;&#22411;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#20196;&#29260;&#30340;&#33021;&#21147;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;Mixture-of-experts&#27169;&#22411;&#21442;&#25968;&#20302;&#25928;&#30340;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#36739;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#31232;&#30095;&#28608;&#27963;&#30340;Mixture-of-experts&#65288;MoE&#65289;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#20445;&#25345;&#20302;&#27599;&#20010;&#20196;&#29260;&#35745;&#31639;&#35201;&#27714;&#30340;&#21516;&#26102;&#26174;&#30528;&#22686;&#21152;&#21442;&#25968;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;MoE&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#21442;&#25968;&#20302;&#25928;&#30340;&#65292;&#22240;&#20026;&#38543;&#30528;&#19987;&#23478;&#25968;&#37327;&#22686;&#21152;&#65292;&#24615;&#33021;&#30340;&#25552;&#39640;&#20250;&#21464;&#23567;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#31181;&#21442;&#25968;&#20302;&#25928;&#26159;&#25152;&#26377;&#19987;&#23478;&#20855;&#26377;&#30456;&#21516;&#33021;&#21147;&#30340;&#32467;&#26524;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#28385;&#36275;&#19981;&#21516;&#20196;&#29260;&#25110;&#20219;&#21153;&#30340;&#19981;&#21516;&#22797;&#26434;&#24230;&#35201;&#27714;&#65292;&#20363;&#22914;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;&#22522;&#20110;&#20854;&#36164;&#28304;&#27700;&#24179;&#30340;&#35821;&#35328;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#23618;&#32467;&#26500;&#24182;&#21487;&#20197;&#20026;&#19981;&#21516;&#20196;&#29260;&#20998;&#37197;&#21160;&#24577;&#33021;&#21147;&#30340;Stratified Mixture of Experts&#65288;SMoE&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;SMoE&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#27169;&#22411;&#22312;&#20004;&#20010;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20248;&#20110;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;MoE&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-experts (MoE) models that employ sparse activation have demonstrated effectiveness in significantly increasing the number of parameters while maintaining low computational requirements per token. However, recent studies have established that MoE models are inherently parameter-inefficient as the improvement in performance diminishes with an increasing number of experts. We hypothesize this parameter inefficiency is a result of all experts having equal capacity, which may not adequately meet the varying complexity requirements of different tokens or tasks, e.g., in a multilingual setting, languages based on their resource levels might require different capacities. In light of this, we propose Stratified Mixture of Experts(SMoE) models, which feature a stratified structure and can assign dynamic capacity to different tokens. We demonstrate the effectiveness of SMoE on two multilingual machine translation benchmarks, where it outperforms multiple state-of-the-art MoE models. On
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#65292;&#36890;&#36807;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;&#21644;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#20302;&#30456;&#20851;&#24615;&#21644;&#20542;&#21521;&#20110;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#25104;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36229;&#36807;&#29616;&#26377;&#22522;&#20934;&#65292;&#20854;&#20013;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.02105</link><description>&lt;p&gt;
GPT-RE: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPT-RE: In-context Learning for Relation Extraction using Large Language Models. (arXiv:2305.02105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#65292;&#36890;&#36807;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;&#21644;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#20302;&#30456;&#20851;&#24615;&#21644;&#20542;&#21521;&#20110;&#38169;&#35823;&#20998;&#31867;&#30340;&#38382;&#39064;&#65292;&#25104;&#26524;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#36229;&#36807;&#29616;&#26377;&#22522;&#20934;&#65292;&#20854;&#20013;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#32467;&#26524;&#36798;&#21040;&#20102;&#26368;&#26032;&#30740;&#31350;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3&#65289;&#26377;&#21487;&#33021;&#21462;&#24471;&#31361;&#30772;&#24615;&#25104;&#23601;&#65292;&#20294;&#23427;&#20204;&#22312;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#20173;&#36828;&#36828;&#33853;&#21518;&#20110;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#65288;&#20363;&#22914;fine-tuned BERT&#65289;&#12290;  &#36825;&#26159;&#30001;&#20110;LLMs&#22312;RE&#26041;&#38754;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#32570;&#28857;:(1)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#26816;&#32034;&#21040;&#30340;&#28436;&#31034;&#20013;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;; (2)&#24378;&#28872;&#20542;&#21521;&#20110;&#38169;&#35823;&#22320;&#23558;NULL&#31034;&#20363;&#20998;&#31867;&#20026;&#20854;&#20182;&#39044;&#23450;&#20041;&#30340;&#26631;&#31614;&#12290;  &#26412;&#25991;&#25552;&#20986;&#20102;GPT-RE&#26469;&#24357;&#21512;LLMs&#21644;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290; GPT-RE&#36890;&#36807;&#65288;1&#65289;&#22312;&#28436;&#31034;&#26816;&#32034;&#20013;&#21152;&#20837;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#23454;&#20307;&#34920;&#31034;; &#65288;2&#65289;&#20351;&#29992;&#37329;&#26631;&#31614;&#35825;&#23548;&#30340;&#25512;&#29702;&#36923;&#36753;&#20016;&#23500;&#28436;&#31034;&#26469;&#25104;&#21151;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290; &#25105;&#20204;&#22312;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;RE&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;GPT-RE&#65292;&#24182;&#35266;&#23519;&#21040;GPT-RE&#19981;&#20165;&#25913;&#21892;&#20102;&#29616;&#26377;&#30340;GPT-3&#22522;&#20934;&#65292;&#32780;&#19988;&#25913;&#21892;&#20102;&#23436;&#20840;&#30417;&#30563;&#30340;&#22522;&#20934;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;GPT-RE&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#19977;&#20010;&#19978;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#30740;&#31350;&#25104;&#26524;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20851;&#31995;&#25277;&#21462;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In spite of the potential for ground-breaking achievements offered by large language models (LLMs) (e.g., GPT-3), they still lag significantly behind fully-supervised baselines (e.g., fine-tuned BERT) in relation extraction (RE). This is due to the two major shortcomings of LLMs in RE: (1) low relevance regarding entity and relation in retrieved demonstrations for in-context learning; and (2) the strong inclination to wrongly classify NULL examples into other pre-defined labels.  In this paper, we propose GPT-RE to bridge the gap between LLMs and fully-supervised baselines. GPT-RE successfully addresses the aforementioned issues by (1) incorporating task-specific entity representations in demonstration retrieval; and (2) enriching the demonstrations with gold label-induced reasoning logic. We evaluate GPT-RE on four widely-used RE datasets, and observe that GPT-RE achieves improvements over not only existing GPT-3 baselines, but also fully-supervised baselines. Specifically, GPT-RE ach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;</title><link>http://arxiv.org/abs/2305.01498</link><description>&lt;p&gt;
&#26088;&#22312;&#24635;&#32467;&#24102;&#26377;&#23618;&#27425;&#20851;&#31995;&#30340;&#22810;&#31687;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
Towards Summarizing Multiple Documents with Hierarchical Relationships. (arXiv:2305.01498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01498
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;PeerSum&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20803;&#35780;&#35770;&#29983;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#27169;&#22411;Rammer&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#25968;&#29616;&#23384;&#30340;&#22810;&#25991;&#26723;&#25688;&#35201;(MDS)&#25968;&#25454;&#38598;&#32570;&#23569;&#20154;&#24037;&#29983;&#25104;&#30340;&#12289;&#30495;&#23454;&#30340;(&#21363;&#38750;&#21512;&#25104;&#30340;)&#25688;&#35201;&#25110;&#32773;&#24102;&#26377;&#26174;&#24335;&#25991;&#26723;&#38388;&#20851;&#31995;&#30340;&#28304;&#25991;&#26723;&#12290;&#20026;&#20102;&#22686;&#24378;MDS&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;PeerSum&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#29983;&#25104;&#31185;&#23398;&#35770;&#25991;&#30340;&#20803;&#35780;&#35770;&#65292;&#20854;&#20013;&#20803;&#35780;&#35770;&#26159;&#23545;&#35780;&#35770;&#21644;&#30456;&#24212;&#35752;&#35770;&#30340;&#39640;&#24230;&#27010;&#25324;&#19988;&#30495;&#23454;&#30340;&#25688;&#35201;&#12290;&#36825;&#20123;&#28304;&#25991;&#26723;&#20855;&#26377;&#26174;&#24335;&#23618;&#27425;&#32467;&#26500;&#30340;&#20016;&#23500;&#25991;&#26723;&#38388;&#20851;&#31995;&#65292;&#21253;&#25324;&#20132;&#21449;&#24341;&#29992;&#21644;&#32463;&#24120;&#20986;&#29616;&#30340;&#20914;&#31361;&#12290;&#37492;&#20110;&#24456;&#23569;&#26377;&#30740;&#31350;&#37319;&#29992;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#25805;&#32437;&#26469;&#23558;&#23618;&#27425;&#20851;&#31995;&#32435;&#20837;MDS&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Rammer(&#20851;&#31995;&#24863;&#30693;&#22810;&#20219;&#21153;&#20803;&#35780;&#35770;&#29983;&#25104;&#22120;)&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#35780;&#35770;&#29983;&#25104;&#27169;&#22411;&#65292;&#20351;&#29992;&#22522;&#20110;&#23618;&#27425;&#20851;&#31995;&#30340;&#31232;&#30095;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#30446;&#26631;&#65292;&#21487;&#20197;&#39044;&#27979;&#22810;&#20010;&#24230;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most existing multi-document summarization (MDS) datasets lack human-generated and genuine (i.e., not synthetic) summaries or source documents with explicit inter-document relationships that a summary must capture. To enhance the capabilities of MDS systems we present PeerSum, a novel dataset for generating meta-reviews of scientific papers, where the meta-reviews are highly abstractive and genuine summaries of reviews and corresponding discussions. These source documents have rich inter-document relationships of an explicit hierarchical structure with cross-references and often feature conflicts. As there is a scarcity of research that incorporates hierarchical relationships into MDS systems through attention manipulation on pre-trained language models, we additionally present Rammer (Relationship-aware Multi-task Meta-review Generator), a meta-review generation model that uses sparse attention based on the hierarchical relationships and a multi-task objective that predicts several me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21517;&#23383;&#22635;&#31354;&#25104;&#21592;&#25512;&#26029;&#26597;&#35810;&#65292;&#35813;&#30740;&#31350;&#32771;&#21476;&#20102;ChatGPT&#21644;GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#35760;&#24518;&#20102;&#22823;&#37327;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#36825;&#25903;&#25345;&#20102;&#19968;&#20010;&#20351;&#29992;&#24050;&#30693;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#25918;&#27169;&#22411;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.00118</link><description>&lt;p&gt;
&#32842;&#22825;GPT/GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#30340;&#32771;&#21476;&#23398;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Speak, Memory: An Archaeology of Books Known to ChatGPT/GPT-4. (arXiv:2305.00118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00118
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21517;&#23383;&#22635;&#31354;&#25104;&#21592;&#25512;&#26029;&#26597;&#35810;&#65292;&#35813;&#30740;&#31350;&#32771;&#21476;&#20102;ChatGPT&#21644;GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#35760;&#24518;&#20102;&#22823;&#37327;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#36825;&#25903;&#25345;&#20102;&#19968;&#20010;&#20351;&#29992;&#24050;&#30693;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#25918;&#27169;&#22411;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#21517;&#23383;&#22635;&#31354;&#25104;&#21592;&#25512;&#26029;&#26597;&#35810;&#65292;&#23545;ChatGPT&#21644;GPT-4&#24050;&#30693;&#30340;&#22270;&#20070;&#36827;&#34892;&#25968;&#25454;&#32771;&#21476;&#23398;&#25512;&#26029;&#65292;&#21457;&#29616;OpenAI&#27169;&#22411;&#24050;&#32463;&#35760;&#24518;&#20102;&#22823;&#37327;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#26448;&#26009;&#65292;&#24182;&#19988;&#35760;&#24518;&#30340;&#31243;&#24230;&#19982;&#36825;&#20123;&#20070;&#31821;&#22312;&#32593;&#19978;&#20986;&#29616;&#30340;&#39057;&#29575;&#26377;&#20851;&#12290;&#36825;&#20123;&#27169;&#22411;&#35760;&#24518;&#26410;&#30693;&#30340;&#20070;&#31821;&#30340;&#33021;&#21147;&#20351;&#24471;&#25991;&#21270;&#20998;&#26512;&#30340;&#27979;&#37327;&#26377;&#25928;&#24615;&#35780;&#20272;&#21464;&#24471;&#22797;&#26434;&#65292;&#22240;&#20026;&#23427;&#20250;&#27745;&#26579;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#35760;&#24518;&#30340;&#20070;&#31821;&#19978;&#30340;&#34920;&#29616;&#36828;&#36828;&#20248;&#20110;&#26410;&#35760;&#24518;&#30340;&#20070;&#31821;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#36825;&#25903;&#25345;&#20102;&#19968;&#20010;&#20351;&#29992;&#24050;&#30693;&#35757;&#32451;&#25968;&#25454;&#30340;&#24320;&#25918;&#27169;&#22411;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we carry out a data archaeology to infer books that are known to ChatGPT and GPT-4 using a name cloze membership inference query. We find that OpenAI models have memorized a wide collection of copyrighted materials, and that the degree of memorization is tied to the frequency with which passages of those books appear on the web. The ability of these models to memorize an unknown set of books complicates assessments of measurement validity for cultural analytics by contaminating test data; we show that models perform much better on memorized books than on non-memorized books for downstream tasks. We argue that this supports a case for open models whose training data is known.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35266;&#23519;&#32422;20&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21518;&#33021;&#22815;&#25552;&#20379;&#26368;&#20339;&#25311;&#21512;&#65292;&#20854;surprisal&#20272;&#35745;&#33021;&#21147;&#33021;&#22815;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#21464;&#20307;&#20250;&#20986;&#29616;&#8220;&#20020;&#30028;&#28857;&#8221;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#22256;&#24785;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23548;&#33268;&#36739;&#24046;&#30340;&#20154;&#31867;&#25311;&#21512;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11389</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;Surprisal&#26368;&#20339;&#30340;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#30340;&#35757;&#32451;&#20196;&#29260;&#25968;&#32422;&#20026;20&#20159;
&lt;/p&gt;
&lt;p&gt;
Transformer-Based LM Surprisal Predicts Human Reading Times Best with About Two Billion Training Tokens. (arXiv:2304.11389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35266;&#23519;&#32422;20&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21518;&#33021;&#22815;&#25552;&#20379;&#26368;&#20339;&#25311;&#21512;&#65292;&#20854;surprisal&#20272;&#35745;&#33021;&#21147;&#33021;&#22815;&#26368;&#22909;&#22320;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#25910;&#25947;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#21464;&#20307;&#20250;&#20986;&#29616;&#8220;&#20020;&#30028;&#28857;&#8221;&#65292;&#20351;&#35821;&#35328;&#27169;&#22411;&#22256;&#24785;&#24230;&#19979;&#38477;&#65292;&#20174;&#32780;&#23548;&#33268;&#36739;&#24046;&#30340;&#20154;&#31867;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#19982;&#20854;surprisal&#20272;&#35745;&#33021;&#21147;&#39044;&#27979;&#20154;&#31867;&#38405;&#35835;&#26102;&#38388;&#30340;&#20851;&#31995;&#24471;&#20986;&#20102;&#30456;&#20114;&#30683;&#30462;&#30340;&#32467;&#35770;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#30740;&#31350;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23481;&#37327;&#30340;&#24040;&#22823;&#24046;&#36317;&#25152;&#33268;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#21464;&#20307;&#30340;Surprisal&#20272;&#35745;&#33021;&#21147;&#65292;&#36825;&#20123;&#21464;&#20307;&#22312;&#35757;&#32451;&#25968;&#25454;&#37327;&#21644;&#27169;&#22411;&#23481;&#37327;&#26041;&#38754;&#26377;&#31995;&#32479;&#21464;&#21270;&#65292;&#20197;&#25972;&#21512;&#36825;&#20123;&#21457;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20855;&#26377;&#29616;&#20195;&#27169;&#22411;&#23481;&#37327;&#30340;&#21464;&#20307;&#30340;Surprisal&#20272;&#35745;&#22312;&#35266;&#23519;&#32422;20&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21518;&#25552;&#20379;&#26368;&#20339;&#25311;&#21512;&#65292;&#27492;&#21518;&#23427;&#20204;&#24320;&#22987;&#20559;&#31163;&#19982;&#20154;&#31867;&#26399;&#26395;&#30456;&#31526;&#30340;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#26032;&#35757;&#32451;&#30340;&#36739;&#23567;&#27169;&#22411;&#21464;&#20307;&#22312;&#25910;&#25947;&#26102;&#26174;&#31034;&#20986;&#8220;&#20020;&#30028;&#28857;&#8221;&#65292;&#22312;&#27492;&#20043;&#21518;&#65292;&#35821;&#35328;&#27169;&#22411;&#22256;&#24785;&#24230;&#30340;&#19979;&#38477;&#24320;&#22987;&#23548;&#33268;&#36739;&#24046;&#30340;&#20154;&#31867;&#25311;&#21512;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Outlier Suppression+&#26694;&#26550;&#30340;&#36890;&#36947;&#32423;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#65292;&#20998;&#26512;&#24471;&#21040;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09145</link><description>&lt;p&gt;
Outlier Suppression+&#65306;&#36890;&#36807;&#31561;&#25928;&#21644;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#26469;&#20934;&#30830;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Outlier Suppression+: Accurate quantization of large language models by equivalent and optimal shifting and scaling. (arXiv:2304.09145v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09145
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Outlier Suppression+&#26694;&#26550;&#30340;&#36890;&#36947;&#32423;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#65292;&#20998;&#26512;&#24471;&#21040;&#26368;&#20248;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#37327;&#21270;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#19981;&#23545;&#31216;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;Transformer&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#38754;&#20020;&#30528;&#23384;&#22312;&#25439;&#23475;&#24615;&#31163;&#32676;&#20540;&#30340;&#37325;&#35201;&#38590;&#39064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31163;&#32676;&#20540;&#26159;&#19981;&#23545;&#31216;&#30340;&#24182;&#19988;&#38598;&#20013;&#22312;&#29305;&#23450;&#36890;&#36947;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Outlier Suppression+&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#36947;&#32423;&#21035;&#30340;&#31227;&#20301;&#21644;&#32553;&#25918;&#25805;&#20316;&#26469;&#28040;&#38500;&#19981;&#23545;&#31216;&#34920;&#31034;&#24182;&#32553;&#23567;&#26377;&#38382;&#39064;&#30340;&#36890;&#36947;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#25805;&#20316;&#21487;&#20197;&#26080;&#32541;&#22320;&#36801;&#31227;&#33267;&#21518;&#32493;&#27169;&#22359;&#32780;&#20445;&#25345;&#31561;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#37327;&#21270;&#20998;&#26512;&#20102;&#26368;&#20248;&#30340;&#31227;&#20301;&#21644;&#32553;&#25918;&#20540;&#65292;&#32771;&#34385;&#21040;&#19979;&#19968;&#23618;&#26435;&#37325;&#30340;&#19981;&#23545;&#31216;&#29305;&#24615;&#21644;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#36731;&#37327;&#32423;&#30340;&#26694;&#26550;&#21487;&#20197;&#22312;&#38745;&#24577;&#21644;&#26631;&#20934;&#30340;&#35757;&#32451;&#21518;&#37327;&#21270;&#35774;&#32622;&#19979;&#36896;&#25104;&#26368;&#23567;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23567;&#22411;&#27169;&#22411;&#21644;&#22823;&#22411;&#27169;&#22411;&#22914;GPT-2&#26041;&#38754;&#23454;&#29616;&#20102;&#25509;&#36817;&#28014;&#28857;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of transformer language models faces significant challenges due to the existence of detrimental outliers in activations. We observe that these outliers are asymmetric and concentrated in specific channels. To address this issue, we propose the Outlier Suppression+ framework. First, we introduce channel-wise shifting and scaling operations to eliminate asymmetric presentation and scale down problematic channels. We demonstrate that these operations can be seamlessly migrated into subsequent modules while maintaining equivalence. Second, we quantitatively analyze the optimal values for shifting and scaling, taking into account both the asymmetric property and quantization errors of weights in the next layer. Our lightweight framework can incur minimal performance degradation under static and standard post-training quantization settings. Comprehensive results across various tasks and models reveal that our approach achieves near-floating-point performance on both small models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08315</link><description>&lt;p&gt;
&#33606;&#26840;&#29611;&#29808;&#65306;&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21452;&#37325;&#20351;&#29992;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#20351;&#29992;&#26159;&#25351;&#26377;&#24847;&#23558;&#25216;&#26415;&#21644;&#31185;&#23398;&#25104;&#26524;&#29992;&#20110;&#26377;&#23475;&#30446;&#30340;&#30340;&#38382;&#39064;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#23578;&#26410;&#26126;&#30830;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;NLP&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#22312;&#31038;&#20250;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20869;&#37096;&#36816;&#34892;&#26041;&#24335;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21452;&#37325;&#20351;&#29992;&#30340;&#38382;&#39064;&#20197;&#21450;&#38480;&#21046;&#21452;&#37325;&#20351;&#29992;&#30340;&#28508;&#22312;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NLP&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#35813;&#38382;&#39064;&#30340;&#28145;&#24230;&#29702;&#35299;&#21644;&#35266;&#28857;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#30340;&#25903;&#25345;&#24773;&#20917;&#12290;&#26681;&#25454;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#23545;&#20182;&#20204;&#30340;&#30740;&#31350;&#30340;&#28508;&#22312;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#34920;&#31034;&#20851;&#20999;&#65292;&#20294;&#21482;&#37319;&#21462;&#26377;&#38480;&#30340;&#34892;&#21160;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12528</link><description>&lt;p&gt;
MEGA: &#22810;&#35821;&#35328;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35821;&#35328;&#29983;&#25104;&#65289;&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#24403;&#20170;AI&#31038;&#21306;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#35780;&#20272;&#29983;&#25104;AI&#26174;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#37117;&#38480;&#20110;&#33521;&#35821;&#65292;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#20840;&#38754;&#35780;&#20272; 8 &#39033;&#19981;&#21516;&#20219;&#21153;&#21644; 33 &#31181;&#35821;&#35328;&#30340;&#29983;&#25104;LLMs MEGA &#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#23558;&#29983;&#25104;LLMs&#30340;&#24615;&#33021;&#19982;&#36825;&#20123;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#19982;&#19978;&#19968;&#20195;LLMs&#30456;&#27604;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#25552;&#31034;&#23545;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#26041;&#38754;&#30340;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#65288;&#30693;&#35782;&#20914;&#31361;&#21644;&#39044;&#27979;&#30340;&#24323;&#26435;&#65289;&#65292;&#21457;&#29616;&#20102;&#22522;&#20110;&#35266;&#28857;&#30340;&#25552;&#31034;&#21644;&#34394;&#26500;&#28436;&#31034;&#20004;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.11315</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20934;&#30830;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Context-faithful Prompting for Large Language Models. (arXiv:2303.11315v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11315
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#25552;&#31034;&#23545;&#20110;&#35299;&#20915;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#26041;&#38754;&#30340;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#65288;&#30693;&#35782;&#20914;&#31361;&#21644;&#39044;&#27979;&#30340;&#24323;&#26435;&#65289;&#65292;&#21457;&#29616;&#20102;&#22522;&#20110;&#35266;&#28857;&#30340;&#25552;&#31034;&#21644;&#34394;&#26500;&#28436;&#31034;&#20004;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32534;&#30721;&#20102;&#20851;&#20110;&#19990;&#30028;&#20107;&#23454;&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#65292;&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#21442;&#25968;&#21270;&#30693;&#35782;&#30340;&#20381;&#36182;&#21487;&#33021;&#23548;&#33268;&#23427;&#20204;&#24573;&#35270;&#19978;&#19979;&#25991;&#32447;&#32034;&#65292;&#22312;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#20363;&#22914;&#30693;&#35782;&#33719;&#21462;&#20219;&#21153;&#65289;&#20013;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#21644;&#22686;&#24378;LLMs&#22312;&#20004;&#20010;&#26041;&#38754;&#30340;&#19978;&#19979;&#25991;&#20934;&#30830;&#24615;&#65306;&#30693;&#35782;&#20914;&#31361;&#21644;&#39044;&#27979;&#30340;&#24323;&#26435;&#12290;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#31574;&#30053;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22522;&#20110;&#35266;&#28857;&#30340;&#25552;&#31034;&#21644;&#34394;&#26500;&#28436;&#31034;&#26159;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#35266;&#28857;&#30340;&#25552;&#31034;&#23558;&#19978;&#19979;&#25991;&#37325;&#26032;&#26500;&#36896;&#20026;&#35762;&#36848;&#32773;&#30340;&#38472;&#36848;&#65292;&#24182;&#35810;&#38382;&#35762;&#36848;&#32773;&#30340;&#24847;&#35265;&#65292;&#32780;&#34394;&#26500;&#28436;&#31034;&#20351;&#29992;&#21253;&#21547;&#38169;&#35823;&#20107;&#23454;&#30340;&#23454;&#20363;&#65292;&#20197;&#25552;&#39640;&#22312;&#30693;&#35782;&#20914;&#31361;&#24773;&#20917;&#19979;&#30340;&#20934;&#30830;&#24615;&#12290;&#20004;&#31181;&#25216;&#26415;&#37117;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. W
&lt;/p&gt;</description></item><item><title>WiCE&#26159;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#34164;&#21547;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#22768;&#26126;&#21644;&#35777;&#25454;&#37197;&#23545;&#26500;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#22768;&#26126;&#20998;&#35299;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#31574;&#30053;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#39564;&#35777;&#21644;&#26816;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.01432</link><description>&lt;p&gt;
WiCE: &#29992;&#20110;&#32500;&#22522;&#30334;&#31185;&#20013;&#22768;&#26126;&#30340;&#30495;&#23454;&#19990;&#30028;&#34164;&#21547;
&lt;/p&gt;
&lt;p&gt;
WiCE: Real-World Entailment for Claims in Wikipedia. (arXiv:2303.01432v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01432
&lt;/p&gt;
&lt;p&gt;
WiCE&#26159;&#19968;&#20010;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#34164;&#21547;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;&#32500;&#22522;&#30334;&#31185;&#20013;&#30340;&#22768;&#26126;&#21644;&#35777;&#25454;&#37197;&#23545;&#26500;&#24314;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#22768;&#26126;&#20998;&#35299;&#31574;&#30053;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#31574;&#30053;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25552;&#21319;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#30340;&#39564;&#35777;&#21644;&#26816;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#34164;&#21547;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#20107;&#23454;&#26816;&#26597;&#12289;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#39044;&#35774;&#39564;&#35777;&#25110;&#25688;&#35201;&#35780;&#20272;&#31561;&#35774;&#32622;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#19982;&#29616;&#26377;&#30340;&#34164;&#21547;&#25968;&#25454;&#38598;&#23384;&#22312;&#26174;&#33879;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#23548;&#33268;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;WiCE&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#32454;&#31890;&#24230;&#25991;&#26412;&#34164;&#21547;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25552;&#21462;&#26469;&#33258;&#32500;&#22522;&#30334;&#31185;&#30340;&#33258;&#28982;&#22768;&#26126;&#21644;&#35777;&#25454;&#37197;&#23545;&#26469;&#26500;&#24314;&#12290;&#38500;&#20102;&#26631;&#20934;&#30340;&#22768;&#26126;&#32423;&#34164;&#21547;&#65292;WiCE&#36824;&#25552;&#20379;&#23545;&#22768;&#26126;&#23376;&#21477;&#30340;&#34164;&#21547;&#21028;&#26029;&#65292;&#20197;&#21450;&#25903;&#25345;&#27599;&#20010;&#23376;&#22768;&#26126;&#30340;&#26368;&#23567;&#35777;&#25454;&#21477;&#23376;&#23376;&#38598;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-3.5&#30340;&#33258;&#21160;&#22768;&#26126;&#20998;&#35299;&#31574;&#30053;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#31574;&#30053;&#22312;&#27979;&#35797;&#26102;&#20063;&#33021;&#26377;&#25928;&#25552;&#39640;&#34164;&#21547;&#27169;&#22411;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30495;&#23454;&#22768;&#26126;&#28041;&#21450;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#39564;&#35777;&#21644;&#26816;&#32034;&#38382;&#39064;&#65292;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual entailment models are increasingly applied in settings like fact-checking, presupposition verification in question answering, or summary evaluation. However, these represent a significant domain shift from existing entailment datasets, and models underperform as a result. We propose WiCE, a new fine-grained textual entailment dataset built on natural claim and evidence pairs extracted from Wikipedia. In addition to standard claim-level entailment, WiCE provides entailment judgments over sub-sentence units of the claim, and a minimal subset of evidence sentences that support each subclaim. To support this, we propose an automatic claim decomposition strategy using GPT-3.5 which we show is also effective at improving entailment models' performance on multiple datasets at test time. Finally, we show that real claims in our dataset involve challenging verification and retrieval problems that existing models fail to address.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;</title><link>http://arxiv.org/abs/2302.07268</link><description>&lt;p&gt;
AI&#32842;&#22825;&#21161;&#25163;&#21487;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AI Chat Assistants can Improve Conversations about Divisive Topics. (arXiv:2302.07268v4 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#21487;&#20197;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20182;&#20204;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#24110;&#21161;&#20154;&#20204;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#32447;&#20132;&#27969;&#25968;&#37327;&#27491;&#22312;&#36805;&#36895;&#22686;&#38271;&#12290;&#20294;&#26159;&#65292;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#28040;&#24687;&#24212;&#29992;&#31243;&#24207;&#21644;&#20854;&#20182;&#25968;&#23383;&#35770;&#22363;&#19978;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#20114;&#21160;&#21487;&#33021;&#20250;&#20135;&#29983;&#20998;&#35010;&#21644;&#20914;&#31361;&#12290;&#36825;&#31181;&#26377;&#27602;&#24615;&#22686;&#21152;&#20102;&#26497;&#21270;&#30340;&#31243;&#24230;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#65292;&#20405;&#34432;&#20102;&#22810;&#20803;&#21270;&#31038;&#20250;&#21457;&#23637;&#35299;&#20915;&#24433;&#21709;&#25152;&#26377;&#20154;&#30340;&#22797;&#26434;&#31038;&#20250;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#23398;&#32773;&#21644;&#27665;&#38388;&#31038;&#20250;&#32452;&#32455;&#25512;&#21160;&#24178;&#39044;&#25514;&#26045;&#65292;&#20351;&#38754;&#23545;&#38754;&#30340;&#23545;&#35805;&#19981;&#37027;&#20040;&#20855;&#26377;&#20998;&#35010;&#24615;&#25110;&#26356;&#20855;&#29983;&#20135;&#21147;&#65292;&#20294;&#23558;&#36825;&#20123;&#21162;&#21147;&#25193;&#23637;&#33267;&#22312;&#32447;&#21457;&#29983;&#30340;&#35768;&#22810;&#35805;&#35821;&#26159;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#35813;&#23454;&#39564;&#35777;&#26126;&#20102;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;&#20309;&#25913;&#21892;&#20851;&#20110;&#20998;&#35010;&#24615;&#35805;&#39064;&#30340;&#22312;&#32447;&#23545;&#35805;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#20379;&#22522;&#20110;&#35777;&#25454;&#30340;&#24314;&#35758;&#65292;&#20197;&#25913;&#21892;&#21442;&#19982;&#32773;&#22312;&#23545;&#35805;&#20013;&#24863;&#21463;&#21040;&#29702;&#35299;&#30340;&#24863;&#35273;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#24314;&#35758;&#30830;&#23454;&#21487;&#20197;&#24110;&#21161;&#20154;&#20204;&#25913;&#21892;&#23545;&#35805;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A rapidly increasing amount of human conversation occurs online. But divisiveness and conflict can fester in text-based interactions on social media platforms, in messaging apps, and on other digital forums. Such toxicity increases polarization and, importantly, corrodes the capacity of diverse societies to develop efficient solutions to complex social problems that impact everyone. Scholars and civil society groups promote interventions that can make interpersonal conversations less divisive or more productive in offline settings, but scaling these efforts to the amount of discourse that occurs online is extremely challenging. We present results of a large-scale experiment that demonstrates how online conversations about divisive topics can be improved with artificial intelligence tools. Specifically, we employ a large language model to make real-time, evidence-based recommendations intended to improve participants' perception of feeling understood in conversations. We find that these
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#25935;&#25463;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#12289;&#23450;&#21521;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#29305;&#23450;&#31574;&#30053;&#24555;&#36895;&#24320;&#21457;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20026;&#25903;&#25345;&#26356;&#23433;&#20840;&#30340;&#22312;&#32447;&#20132;&#27969;&#30340;&#27169;&#22411;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2302.06541</link><description>&lt;p&gt;
&#38754;&#21521;&#25152;&#26377;&#20154;&#30340;&#25935;&#25463;&#25991;&#26412;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards Agile Text Classifiers for Everyone. (arXiv:2302.06541v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#21644;&#35780;&#20272;&#20102;&#25935;&#25463;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#12289;&#23450;&#21521;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#29305;&#23450;&#31574;&#30053;&#24555;&#36895;&#24320;&#21457;&#20998;&#31867;&#22120;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#36825;&#20026;&#25903;&#25345;&#26356;&#23433;&#20840;&#30340;&#22312;&#32447;&#20132;&#27969;&#30340;&#27169;&#22411;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#24191;&#27867;&#29992;&#20110;&#20869;&#23481;&#23457;&#26680;&#65292;&#24182;&#19988;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35843;&#25972;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#36825;&#26159;&#23545;&#25968;&#23383;&#21161;&#29702;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#23433;&#20840;&#24615;&#30340;&#26085;&#30410;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#31574;&#30053;&#38656;&#35201;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65292;&#24182;&#19988;&#23433;&#20840;&#31574;&#30053;&#26412;&#36523;&#20063;&#21487;&#20197;&#36890;&#36807;&#36845;&#20195;&#21644;&#35843;&#25972;&#26469;&#25913;&#36827;&#12290;&#26412;&#25991;&#24341;&#20837;&#24182;&#35780;&#20272;&#20102;&#25935;&#25463;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23567;&#22411;&#12289;&#23450;&#21521;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#20026;&#29305;&#23450;&#31574;&#30053;&#24555;&#36895;&#24320;&#21457;&#20998;&#31867;&#22120;&#12290;&#22312;&#19977;&#20010;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#39046;&#22495;&#20351;&#29992;7&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;15&#20010;&#27880;&#37322;&#26041;&#26696;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;&#20351;&#29992;&#23613;&#23569;80&#20010;&#31034;&#20363;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;PaLM 62B&#65289;&#36827;&#34892;&#25552;&#31034;&#35843;&#25972;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#20026;&#25991;&#26412;&#20998;&#31867;&#24102;&#26469;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#29305;&#21035;&#26159;&#20026;&#25903;&#25345;&#26356;&#23433;&#20840;&#30340;&#22312;&#32447;&#20132;&#27969;&#30340;&#27169;&#22411;&#12290;&#19982;&#35797;&#22270;&#25910;&#38598;&#25968;&#30334;&#19975;&#20010;&#31034;&#20363;&#20197;&#21019;&#24314;&#36890;&#29992;&#23433;&#20840;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#21152;&#39640;&#25928;&#21644;&#28789;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based safety classifiers are widely used for content moderation and increasingly to tune generative language model behavior - a topic of growing concern for the safety of digital assistants and chatbots. However, different policies require different classifiers, and safety policies themselves improve from iteration and adaptation. This paper introduces and evaluates methods for agile text classification, whereby classifiers are trained using small, targeted datasets that can be quickly developed for a particular policy. Experimenting with 7 datasets from three safety-related domains, comprising 15 annotation schemes, led to our key finding: prompt-tuning large language models, like PaLM 62B, with a labeled dataset of as few as 80 examples can achieve state-of-the-art performance. We argue that this enables a paradigm shift for text classification, especially for models supporting safer online discourse. Instead of collecting millions of examples to attempt to create universal safe
&lt;/p&gt;</description></item><item><title>Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.04858</link><description>&lt;p&gt;
Re-ViLM: &#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#30340;&#26816;&#32034;&#22686;&#24378;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning. (arXiv:2302.04858v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04858
&lt;/p&gt;
&lt;p&gt;
Re-ViLM&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#65292;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#26032;&#25968;&#25454;&#65292;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#35270;&#35273;&#32534;&#30721;&#22120;&#65288;&#22914;Flamingo&#65289;&#30456;&#32467;&#21512;&#24050;&#32463;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#25152;&#26377;&#30693;&#35782;&#23384;&#20648;&#22312;&#20854;&#21442;&#25968;&#20013;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#24040;&#22823;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#24314;&#27169;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#21644;&#20016;&#23500;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#22312;&#34701;&#21512;&#26032;&#25968;&#25454;&#26041;&#38754;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#32791;&#26102;&#30340;&#24494;&#35843;&#36807;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;Re-ViLM&#65292;&#22522;&#20110;Flamingo&#26500;&#24314;&#65292;&#25903;&#25345;&#22312;&#38646;&#26679;&#26412;&#21644;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#20174;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#12290;&#36890;&#36807;&#23558;&#26576;&#20123;&#30693;&#35782;&#26126;&#30830;&#23384;&#20648;&#22312;&#22806;&#37096;&#25968;&#25454;&#24211;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#26356;&#26032;&#25968;&#25454;&#24211;&#26469;&#36731;&#26494;&#36866;&#24212;&#35780;&#20272;&#36807;&#31243;&#20013;&#30340;&#26032;&#25968;&#25454;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#31181;&#20132;&#38169;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#25968;&#25454;&#65292;&#20197;&#20419;&#36827;&#19978;&#19979;&#25991;&#20869;&#23569;&#26679;&#26412;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Augmenting pretrained language models (LMs) with a vision encoder (e.g., Flamingo) has obtained the state-of-the-art results in image-to-text generation. However, these models store all the knowledge within their parameters, thus often requiring enormous model parameters to model the abundant visual concepts and very rich textual descriptions. Additionally, they are inefficient in incorporating new data, requiring a computational-expensive fine-tuning process. In this work, we introduce a Retrieval-augmented Visual Language Model, Re-ViLM, built upon the Flamingo, that supports retrieving the relevant knowledge from the external database for zero and in-context few-shot image-to-text generations. By storing certain knowledge explicitly in the external database, our approach reduces the number of model parameters and can easily accommodate new data during evaluation by simply updating the database. We also construct an interleaved image and text data that facilitates in-context few-shot
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30740;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2302.04012</link><description>&lt;p&gt;
CodeLMSec&#22522;&#20934;&#65306;&#31995;&#32479;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
CodeLMSec Benchmark: Systematically Evaluating and Finding Security Vulnerabilities in Black-Box Code Language Models. (arXiv:2302.04012v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04012
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30740;&#31350;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35780;&#20272;&#21644;&#21457;&#29616;&#40657;&#30418;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29992;&#20110;&#33258;&#21160;&#20195;&#30721;&#29983;&#25104;&#22312;&#20960;&#20010;&#32534;&#31243;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#23427;&#20204;&#22312;&#31454;&#36187;&#32423;&#32534;&#31243;&#38382;&#39064;&#19978;&#30340;&#36827;&#23637;&#20351;&#23427;&#20204;&#25104;&#20026;AI&#36741;&#21161;&#23545;&#32534;&#31243;&#30340;&#37325;&#35201;&#25903;&#26609;&#65292;&#24037;&#20855;&#22914;GitHub Copilot&#24050;&#32463;&#25104;&#20026;&#25968;&#30334;&#19975;&#24320;&#21457;&#20154;&#21592;&#26085;&#24120;&#32534;&#31243;&#24037;&#20316;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26469;&#33258;&#20110;&#20114;&#32852;&#32593;&#65288;&#20363;&#22914;&#24320;&#28304;&#23384;&#20648;&#24211;&#65289;&#24182;&#19988;&#21487;&#33021;&#21547;&#26377;&#32570;&#38519;&#21644;&#23433;&#20840;&#28431;&#27934;&#12290;&#36825;&#20123;&#26410;&#32463;&#28040;&#27602;&#30340;&#35757;&#32451;&#25968;&#25454;&#21487;&#33021;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#28431;&#27934;&#24182;&#22312;&#20195;&#30721;&#29983;&#25104;&#36807;&#31243;&#20013;&#20256;&#25773;&#23427;&#20204;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24050;&#32463;&#24191;&#27867;&#35780;&#20272;&#20102;&#23427;&#20204;&#29983;&#25104;&#21151;&#33021;&#19978;&#27491;&#30830;&#31243;&#24207;&#30340;&#33021;&#21147;&#65292;&#20294;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;&#20173;&#28982;&#32570;&#20047;&#20840;&#38754;&#30340;&#35843;&#26597;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) for automatic code generation have achieved breakthroughs in several programming tasks. Their advances in competition-level programming problems have made them an essential pillar of AI-assisted pair programming, and tools such as GitHub Copilot have emerged as part of the daily programming workflow used by millions of developers. The training data for these models is usually collected from the Internet (e.g., from open-source repositories) and is likely to contain faults and security vulnerabilities. This unsanitized training data can cause the language models to learn these vulnerabilities and propagate them during the code generation procedure. While these models have been extensively assessed for their ability to produce functionally correct programs, there remains a lack of comprehensive investigations and benchmarks addressing the security aspects of these models.  In this work, we propose a method to systematically study the security issues of code l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2302.03693</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#30340;&#27010;&#24565;&#20195;&#25968;
&lt;/p&gt;
&lt;p&gt;
Concept Algebra for Score-Based Conditional Models. (arXiv:2302.03693v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03693
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#26465;&#20214;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#25968;&#23398;&#24418;&#24335;&#21270;&#34920;&#36798;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#34920;&#31034;&#31354;&#38388;&#23376;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;&#21033;&#29992;&#36825;&#20010;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25805;&#20316;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#20013;&#23398;&#20064;&#34920;&#31034;&#30340;&#32467;&#26500;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#32858;&#28966;&#20110;&#27010;&#24565;&#34987;&#32534;&#30721;&#20026;&#26576;&#31181;&#34920;&#31034;&#31354;&#38388;&#30340;&#23376;&#31354;&#38388;&#65288;&#25110;&#26041;&#21521;&#65289;&#30340;&#24605;&#24819;&#65292;&#24182;&#24320;&#21457;&#20102;&#36825;&#20010;&#24605;&#24819;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#12290;&#21033;&#29992;&#36825;&#20010;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26377;&#19968;&#20010;&#33258;&#28982;&#30340;&#34920;&#31034;&#36873;&#25321;&#20855;&#26377;&#36825;&#31181;&#24615;&#36136;&#65292;&#24182;&#19988;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#19982;&#32473;&#23450;&#27010;&#24565;&#23545;&#24212;&#30340;&#34920;&#31034;&#37096;&#20998;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#23545;&#34920;&#31034;&#30340;&#20195;&#25968;&#25805;&#20316;&#26469;&#25805;&#32437;&#27169;&#22411;&#25152;&#34920;&#36798;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#20351;&#29992;&#31283;&#23450;&#25193;&#25955;&#22312;&#25991;&#26412;&#24341;&#23548;&#22270;&#20687;&#29983;&#25104;&#30340;&#31034;&#20363;&#20013;&#28436;&#31034;&#20102;&#36825;&#20010;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper concerns the structure of learned representations in text-guided generative models, focusing on score-based models. Here, we focus on the idea that concepts are encoded as subspaces (or directions) of some representation space. We develop a mathematical formalization of this idea.Using this formalism, we show there's a natural choice of representation with this property, and we develop a simple method for identifying the part of the representation corresponding to a given concept. In particular, this allows us to manipulate the concepts expressed by the model through algebraic manipulation of the representation. We demonstrate the idea with examples text-guided image generation, using Stable Diffusion.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20803;&#25968;&#25454;&#23545;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;19&#20010;&#39046;&#22495;&#20013;&#36873;&#25321;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2302.03341</link><description>&lt;p&gt;
&#20803;&#25968;&#25454;&#23545;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#65306;&#36328;&#39046;&#22495;&#36328;&#27169;&#22411;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study. (arXiv:2302.03341v1 [cs.DL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20803;&#25968;&#25454;&#23545;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;19&#20010;&#39046;&#22495;&#20013;&#36873;&#25321;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#23398;&#20986;&#29256;&#29289;&#22312;&#32593;&#32476;&#19978;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36843;&#20999;&#38656;&#35201;&#20026;&#27599;&#31687;&#35770;&#25991;&#25171;&#19978;&#32454;&#31890;&#24230;&#20027;&#39064;&#26631;&#31614;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36319;&#36394;&#33258;&#24049;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#32780;&#19981;&#26159;&#28153;&#27809;&#22312;&#25972;&#20010;&#25991;&#29486;&#20013;&#12290;&#31185;&#23398;&#25991;&#29486;&#30340;&#26631;&#31614;&#21270;&#19981;&#20165;&#20165;&#26159;&#19968;&#20010;&#32431;&#31929;&#30340;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#22240;&#20026;&#32593;&#32476;&#19978;&#30340;&#35770;&#25991;&#36890;&#24120;&#38468;&#24102;&#20803;&#25968;&#25454;&#20449;&#24687;&#65292;&#22914;&#20250;&#35758;&#12289;&#20316;&#32773;&#21644;&#21442;&#32771;&#25991;&#29486;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#20316;&#20026;&#39069;&#22806;&#30340;&#20449;&#21495;&#26469;&#25512;&#26029;&#30456;&#20851;&#30340;&#26631;&#31614;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30740;&#31350;&#21033;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#23398;&#26415;&#35770;&#25991;&#20998;&#31867;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38480;&#20110;&#19968;&#20010;&#25110;&#20004;&#20010;&#31185;&#23398;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#29983;&#29289;&#21307;&#23398;&#65289;&#21644;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#20803;&#25968;&#25454;&#23545;19&#20010;&#39046;&#22495;&#31185;&#23398;&#25991;&#29486;&#26631;&#31614;&#21270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#31181;&#20195;&#34920;&#24615;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#22120;&#65288;&#35789;&#34955;&#27169;&#22411;&#12289;&#22522;&#20110;&#24207;&#21015;&#30340;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the exponential growth of scientific publications on the Web, there is a pressing need to tag each paper with fine-grained topics so that researchers can track their interested fields of study rather than drowning in the whole literature. Scientific literature tagging is beyond a pure multi-label text classification task because papers on the Web are prevalently accompanied by metadata information such as venues, authors, and references, which may serve as additional signals to infer relevant tags. Although there have been studies making use of metadata in academic paper classification, their focus is often restricted to one or two scientific fields (e.g., computer science and biomedicine) and to one specific model. In this work, we systematically study the effect of metadata on scientific literature tagging across 19 fields. We select three representative multi-label classifiers (i.e., a bag-of-words model, a sequence-based model, and a pre-trained language model) and explore t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26816;&#32034;&#24335;&#26041;&#27861;&#26469;&#20943;&#23569;Chatbots&#22238;&#24212;&#20013;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#23433;&#20840;&#30340;&#20869;&#23481;&#65292;&#35813;&#26041;&#27861;&#24615;&#33021;&#19982;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#21709;&#24212;&#23433;&#20840;&#24615;&#30340;&#37325;&#26032;&#25490;&#21517;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2302.00871</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25552;&#39640;&#23545;&#35805;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Using In-Context Learning to Improve Dialogue Safety. (arXiv:2302.00871v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26816;&#32034;&#24335;&#26041;&#27861;&#26469;&#20943;&#23569;Chatbots&#22238;&#24212;&#20013;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#23433;&#20840;&#30340;&#20869;&#23481;&#65292;&#35813;&#26041;&#27861;&#24615;&#33021;&#19982;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#23218;&#32654;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#21709;&#24212;&#23433;&#20840;&#24615;&#30340;&#37325;&#26032;&#25490;&#21517;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#23545;&#35805;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#25104;&#29087;&#65292;&#20154;&#20204;&#24320;&#22987;&#20851;&#27880;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#38382;&#39064;&#12290;&#36825;&#20123;&#31995;&#32479;&#24456;&#23481;&#26131;&#29983;&#25104;&#26377;&#27602;&#20869;&#23481;&#65292;&#36825;&#20123;&#20869;&#23481;&#36890;&#24120;&#20250;&#24310;&#32493;&#31038;&#20250;&#20559;&#35265;&#25110;&#21051;&#26495;&#21360;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26816;&#32034;&#24335;&#26041;&#27861;&#26469;&#20943;&#23569;Chatbots&#22238;&#24212;&#20013;&#20559;&#35265;&#21644;&#26377;&#27602;&#20869;&#23481;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#24335;&#26469;&#24341;&#23548;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#23433;&#20840;&#30340;&#20869;&#23481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24403;&#38024;&#23545;&#19981;&#23433;&#20840;&#30340;&#23545;&#35805;&#35821;&#22659;&#29983;&#25104;&#22238;&#31572;&#26102;&#65292;&#25105;&#20204;&#20250;&#20174;&#31867;&#20284;&#30340;&#23545;&#35805;&#35821;&#22659;&#20013;&#26816;&#32034;&#20986;&#23433;&#20840;&#22238;&#31572;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#21487;&#20197;&#19982;&#24378;&#22522;&#32447;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;&#20363;&#22914;&#65292;&#37319;&#29992;&#33258;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#26368;&#22909;&#30340;&#24494;&#35843;&#22522;&#32447;&#26041;&#27861;&#30456;&#36739;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#21482;&#33021;&#27604;&#8220;DiaSafety&#8221;&#20013;&#23545;&#19981;&#23433;&#20840;&#30340;&#23545;&#35805;&#35821;&#22659;&#29983;&#25104;&#23433;&#20840;&#22238;&#24212;&#22810;4.04%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36827;&#19968;&#27493;&#25552;&#39640;&#21709;&#24212;&#23433;&#20840;&#24615;&#30340;&#37325;&#26032;&#25490;&#21517;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large neural-based conversational models have become increasingly proficient dialogue agents, recent work has highlighted safety issues with these systems. For example, these systems can be goaded into generating toxic content, which often perpetuates social biases or stereotypes. We investigate a retrieval-based method for reducing bias and toxicity in responses from chatbots. It uses in-context learning to steer a model towards safer generations. Concretely, to generate a response to an unsafe dialogue context, we retrieve demonstrations of safe responses to similar dialogue contexts. We find our method performs competitively with strong baselines without requiring training. For instance, using automatic evaluation, we find our best fine-tuned baseline only generates safe responses to unsafe dialogue contexts from DiaSafety 4.04% more than our approach. Finally, we also propose a re-ranking procedure which can further improve response safeness.
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;</title><link>http://arxiv.org/abs/2301.12609</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#8776;&#26631;&#31614;&#24179;&#28369;&#65306;&#20107;&#23454;&#36824;&#26159;&#35884;&#35823;&#65311;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation $\approx$ Label Smoothing: Fact or Fallacy?. (arXiv:2301.12609v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12609
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#21644;&#26631;&#31614;&#24179;&#28369;&#34987;&#35748;&#20026;&#26159;&#31561;&#20215;&#30340;&#26041;&#27861;&#65292;&#20294;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#23545;&#27169;&#22411;&#32622;&#20449;&#24230;&#30340;&#24433;&#21709;&#26041;&#21521;&#23436;&#20840;&#30456;&#21453;&#12290;&#30693;&#35782;&#33976;&#39311;&#19981;&#20165;&#20256;&#36882;&#30693;&#35782;&#65292;&#36824;&#20256;&#36882;&#20102;&#33258;&#20449;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#21021;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#20174;&#19968;&#20010;&#27169;&#22411;&#21521;&#21478;&#19968;&#20010;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26041;&#27861;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#30693;&#35782;&#33976;&#39311;(KD)&#23454;&#38469;&#19978;&#26159;&#19968;&#31181;&#27491;&#21017;&#21270;&#30340;&#24418;&#24335;&#12290;&#26368;&#24378;&#26377;&#21147;&#30340;&#25903;&#25345;&#26469;&#33258;&#20110;&#23427;&#19982;&#26631;&#31614;&#24179;&#28369;(LS)&#26041;&#27861;&#30340;&#26126;&#26174;&#30456;&#20284;&#20043;&#22788;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#25152;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#39044;&#27979;&#32622;&#20449;&#24230;&#65292;&#37325;&#26032;&#32771;&#23519;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#20851;&#31995;&#12290;&#22312;&#28041;&#21450;&#19981;&#21516;&#35268;&#27169;&#27169;&#22411;&#30340;&#22235;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#26174;&#31034;&#20986;&#65306;(a)&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;KD&#21644;LS&#20250;&#23436;&#20840;&#30456;&#21453;&#22320;&#24433;&#21709;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#12290;(b) &#22312;KD&#20013;&#65292;&#23398;&#29983;&#19981;&#20165;&#32487;&#25215;&#30693;&#35782;&#65292;&#32780;&#19988;&#36824;&#20174;&#32769;&#24072;&#37027;&#37324;&#32487;&#25215;&#33258;&#20449;&#24515;&#65292;&#21152;&#24378;&#20102;&#20256;&#32479;&#30340;&#30693;&#35782;&#20256;&#36882;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originally proposed as a method for knowledge transfer from one model to another, some recent studies have suggested that knowledge distillation (KD) is in fact a form of regularization. Perhaps the strongest support of all for this new perspective comes from its apparent similarities with label smoothing (LS). Here we re-examine this stated equivalence between the two methods by comparing the predictive confidences of the models they train. Experiments on four text classification tasks involving models of different sizes show that: (a) In most settings, KD and LS drive model confidence in completely opposite directions, and (b) In KD, the student inherits not only its knowledge but also its confidence from the teacher, reinforcing the classical knowledge transfer view.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#23545;&#26368;&#36817;&#22312;NLP&#39046;&#22495;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#21516;&#20043;&#22788;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2301.09112</link><description>&lt;p&gt;
&#19981;&#21516;ially&#31169;&#23494;&#30340;&#33258;&#28982;&#35821;&#35328;&#27169;&#22411;: &#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Natural Language Models: Recent Advances and Future Directions. (arXiv:2301.09112v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09112
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#23545;&#26368;&#36817;&#22312;NLP&#39046;&#22495;&#20013;&#30340;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#24046;&#20998;&#38544;&#31169;&#28145;&#24230;&#23398;&#20064;&#30340;&#19981;&#21516;&#20043;&#22788;&#21644;&#39069;&#22806;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24212;&#29992;&#21487;&#33021;&#28041;&#21450;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#30340;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#20445;&#25252;&#25935;&#24863;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#22914;&#20309;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#26159;NLP&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#20445;&#25252;&#38544;&#31169;&#65292;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#25104;&#20026;&#20102;&#38544;&#31169;&#25968;&#25454;&#20998;&#26512;&#20013;&#38450;&#27490;&#37325;&#24314;&#25915;&#20987;&#21644;&#38450;&#25252;&#28508;&#22312;&#36793;&#32536;&#30693;&#35782;&#30340;&#20107;&#23454;&#26631;&#20934;&#25216;&#26415;&#12290;&#36817;&#24180;&#26469;&#65292;DP&#22312;NLP&#27169;&#22411;&#65288;DP-NLP&#65289;&#26041;&#38754;&#24050;&#32463;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20540;&#24471;&#36827;&#34892;&#20840;&#38754;&#30340;&#22238;&#39038;&#12290;&#26412;&#25991;&#39318;&#27425;&#23545;DP&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;NLP&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;DP&#28145;&#24230;&#23398;&#20064;&#30456;&#27604;&#65292;DP-NLP&#30340;&#19968;&#20123;&#24046;&#24322;&#21644;&#39069;&#22806;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19968;&#20123;&#29616;&#26377;&#30340;DP-NLP&#24037;&#20316;&#65292;&#24182;&#20174;&#19977;&#20010;&#26041;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#30340;&#21457;&#23637;&#65306;&#26799;&#24230;pe
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep learning have led to great success in various natural language processing (NLP) tasks. However, these applications may involve data that contain sensitive information. Therefore, how to achieve good performance while also protecting the privacy of sensitive data is a crucial challenge in NLP. To preserve privacy, Differential Privacy (DP), which can prevent reconstruction attacks and protect against potential side knowledge, is becoming a de facto technique for private data analysis. In recent years, NLP in DP models (DP-NLP) has been studied from different perspectives, which deserves a comprehensive review. In this paper, we provide the first systematic review of recent advances in DP deep learning models in NLP. In particular, we first discuss some differences and additional challenges of DP-NLP compared with the standard DP deep learning. Then, we investigate some existing work on DP-NLP and present its recent developments from three aspects: gradient pe
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39564;&#35777;&#38598;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#20219;&#21153;&#36866;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20855;&#26377;&#25913;&#36827;&#20316;&#29992;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.11680</link><description>&lt;p&gt;
&#24179;&#31283;&#33322;&#34892;&#65306;&#29992;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#25913;&#36827;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smooth Sailing: Improving Active Learning for Pre-trained Language Models with Representation Smoothness Analysis. (arXiv:2212.11680v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11680
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39564;&#35777;&#38598;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#24182;&#21457;&#29616;&#20219;&#21153;&#36866;&#24212;&#23545;&#20027;&#21160;&#23398;&#20064;&#20855;&#26377;&#25913;&#36827;&#20316;&#29992;&#12290;&#36825;&#39033;&#24037;&#20316;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#35777;&#26126;&#20102;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#23545;&#20110;&#25552;&#39640;&#20027;&#21160;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26631;&#27880;&#22797;&#26434;&#24615;&#65292;&#20197;&#32531;&#35299;&#26114;&#36149;&#30340;&#26631;&#27880;&#25104;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;AL&#19982;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#32467;&#21512;&#20351;&#29992;&#20855;&#26377;&#30410;&#22788;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#24433;&#21709;AL&#25928;&#26524;&#30340;&#23454;&#38469;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;AL&#26082;&#26377;&#25928;&#21448;&#23454;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#39564;&#35777;&#38598;&#30340;&#26089;&#20572;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;AL&#26041;&#27861;&#20013;&#23545;&#27604;&#38543;&#26426;&#25277;&#26679;&#65292;&#35266;&#23519;&#21040;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;&#36866;&#24212;&#25913;&#36827;&#20102;AL&#65292;&#32780;&#26631;&#20934;&#30340;&#30701;&#26399;&#24494;&#35843;&#22312;AL&#20013;&#24182;&#27809;&#26377;&#27604;&#38543;&#26426;&#25277;&#26679;&#25552;&#20379;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#34920;&#31034;&#24179;&#28369;&#24230;&#20998;&#26512;&#22312;AL&#20013;&#30340;&#26377;&#29992;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#20943;&#23569;&#20027;&#21160;&#23398;&#20064;&#20572;&#27490;&#26631;&#20934;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developed to alleviate prohibitive labeling costs, active learning (AL) methods aim to reduce label complexity in supervised learning. While recent work has demonstrated the benefit of using AL in combination with large pre-trained language models (PLMs), it has often overlooked the practical challenges that hinder the effectiveness of AL. We address these challenges by leveraging representation smoothness analysis to ensure AL is feasible, that is, both effective and practicable. Firstly, we propose an early stopping technique that does not require a validation set -- often unavailable in realistic AL conditions -- and observe significant improvements over random sampling across multiple datasets and AL methods. Further, we find that task adaptation improves AL, whereas standard short fine-tuning in AL does not provide improvements over random sampling. Our work demonstrates the usefulness of representation smoothness analysis for AL and introduces an AL stopping criterion that reduce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2212.10901</link><description>&lt;p&gt;
ALCAP: &#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
ALCAP: Alignment-Augmented Music Captioner. (arXiv:2212.10901v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#30340;&#38899;&#20048;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26174;&#24335;&#23398;&#20064;&#38899;&#39057;&#21644;&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#65292;&#21462;&#24471;&#20102;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#39046;&#20808;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38899;&#20048;&#27969;&#23186;&#20307;&#24179;&#21488;&#29992;&#20110;&#38899;&#20048;&#25628;&#32034;&#21644;&#25512;&#33616;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#38656;&#35201;&#26032;&#26041;&#27861;&#26469;&#35299;&#37322;&#38899;&#20048;&#65292;&#21516;&#26102;&#32771;&#34385;&#27468;&#35789;&#21644;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#31934;&#32454;&#35843;&#25972;&#23558;&#38899;&#20048;&#26144;&#23556;&#21040;&#23383;&#24149;&#35760;&#21495;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30340;&#21508;&#20010;&#32452;&#20214;&#65292;&#24573;&#30053;&#20102;&#38899;&#39057;&#21644;&#27468;&#35789;&#20043;&#38388;&#23545;&#24212;&#30340;&#28508;&#22312;&#30410;&#22788;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#26174;&#24335;&#23398;&#20064;&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23398;&#20064;&#38899;&#39057;-&#27468;&#35789;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#20351;&#27169;&#22411;&#25351;&#23548;&#23398;&#20064;&#26356;&#22909;&#30340;&#36328;&#27169;&#24577;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23383;&#24149;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#21644;&#32463;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#22312;&#20004;&#20010;&#38899;&#20048;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26032;&#30340;&#29366;&#24577;-&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing popularity of streaming media platforms for music search and recommendations has led to a need for novel methods for interpreting music that take into account both lyrics and audio. However, many previous works focus on refining individual components of encoder-decoder architecture that maps music to caption tokens, ignoring the potential benefits of correspondence between audio and lyrics. In this paper, we propose to explicitly learn the multimodal alignment through contrastive learning. By learning audio-lyrics correspondence, the model is guided to learn better cross-modal consistency, thus generating high-quality captions. We provide both theoretical and empirical results demonstrating the advantage of the proposed method, and achieve new state-of-the-art on two music captioning datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#30524;&#21160;&#25968;&#25454;&#12289;&#27880;&#37322;&#21644;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#39118;&#26684;&#30340;&#26174;&#33879;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30524;&#21160;&#25968;&#25454;&#22312;&#20154;&#31867;&#27880;&#37322;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20998;&#20013;&#36215;&#21040;&#20102;&#26725;&#26753;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2212.09873</link><description>&lt;p&gt;
&#30524;&#21160;&#25968;&#25454;&#12289;&#27880;&#37322;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#26174;&#33879;&#24615;&#39118;&#26684;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study on Textual Saliency of Styles from Eye Tracking, Annotations, and Language Models. (arXiv:2212.09873v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#30524;&#21160;&#25968;&#25454;&#12289;&#27880;&#37322;&#21644;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#26412;&#39118;&#26684;&#30340;&#26174;&#33879;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#30524;&#21160;&#25968;&#25454;&#22312;&#20154;&#31867;&#27880;&#37322;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#35780;&#20998;&#20013;&#36215;&#21040;&#20102;&#26725;&#26753;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#30524;&#21160;&#25968;&#25454;&#21644;&#20854;&#20182;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38544;&#24335;&#27979;&#37327;&#26041;&#27861;&#24341;&#20837;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27969;&#31243;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#20013;&#34164;&#21547;&#20102;&#23545;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#29420;&#29305;&#30340;&#27934;&#23519;&#21147;&#65292;&#21487;&#20197;&#34987;&#35821;&#35328;&#27169;&#22411;&#25152;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#24615;&#36136;&#20197;&#21450;&#22914;&#20309;&#22312;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#26368;&#22909;&#22320;&#21033;&#29992;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#35299;&#31572;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;eyeStyliency&#65292;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#20154;&#31867;&#23545;&#25991;&#20307;&#25991;&#26412;&#65288;&#22914;&#31036;&#35980;&#24615;&#65289;&#36827;&#34892;&#22788;&#29702;&#30340;&#30524;&#21160;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#25910;&#38598;&#30340;&#30524;&#21160;&#25968;&#25454;&#23545;&#25991;&#26412;&#36827;&#34892;&#39118;&#26684;&#26174;&#33879;&#24615;&#35780;&#20998;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#36825;&#31181;&#26174;&#33879;&#24615;&#25968;&#25454;&#19982;&#20154;&#31867;&#27880;&#37322;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#25351;&#26631;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#30524;&#21160;&#25968;&#25454;&#24456;&#29420;&#29305;&#65292;&#20294;&#23427;&#19982;&#20154;&#31867;&#27880;&#37322;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#35780;&#20998;&#20132;&#38598;&#65292;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#35270;&#35282;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#26725;&#26753;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in incorporating eye-tracking data and other implicit measures of human language processing into natural language processing (NLP) pipelines. The data from human language processing contain unique insight into human linguistic understanding that could be exploited by language models. However, many unanswered questions remain about the nature of this data and how it can best be utilized in downstream NLP tasks. In this paper, we present eyeStyliency, an eye-tracking dataset for human processing of stylistic text (e.g., politeness). We develop a variety of methods to derive style saliency scores over text using the collected eye dataset. We further investigate how this saliency data compares to both human annotation methods and model-based interpretability metrics. We find that while eye-tracking data is unique, it also intersects with both human annotations and model-based importance scores, providing a possible bridge between human- and machine-based perspecti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09724</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Retrieve-and-Read Framework for Knowledge Graph Link Prediction. (arXiv:2212.09724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#26469;&#35299;&#20915;&#29616;&#26377;&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#39318;&#20808;&#26816;&#32034;&#30456;&#20851;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#20351;&#29992;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#25552;&#20379;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#29616;&#26377;&#20107;&#23454;&#25512;&#26029;&#20986;&#26032;&#30340;&#20107;&#23454;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20351;&#29992;&#33410;&#28857;&#30340;&#22270;&#37051;&#22495;&#25552;&#20379;&#20102;&#27604;&#20165;&#20351;&#29992;&#26597;&#35810;&#20449;&#24687;&#26356;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;&#20256;&#32479;&#30340;KG&#38142;&#25509;&#39044;&#27979;&#30340;GNNs&#36981;&#24490;&#25972;&#20010;KG&#19978;&#30340;&#26631;&#20934;&#28040;&#24687;&#20256;&#36882;&#33539;&#24335;&#65292;&#36825;&#23548;&#33268;&#20102;&#20887;&#20313;&#35745;&#31639;&#12289;&#33410;&#28857;&#34920;&#31034;&#30340;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#22312;&#22823;&#35268;&#27169;&#19978;&#65292;&#20174;&#25972;&#20010;KG&#20013;&#32858;&#21512;&#26377;&#29992;&#30340;&#20449;&#24687;&#36827;&#34892;&#25512;&#29702;&#21464;&#24471;&#35745;&#31639;&#19978;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;KG&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#21644;&#38405;&#35835;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#26816;&#32034;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#19979;&#25991;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#23481;&#37327;&#38405;&#35835;&#22120;&#32852;&#21512;&#25512;&#29702;&#19978;&#19979;&#25991;&#21644;&#26597;&#35810;&#12290;&#20316;&#20026;&#25105;&#20204;&#26032;&#26694;&#26550;&#30340;&#23454;&#20363;&#21270;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;GNN&#20316;&#20026;r&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph (KG) link prediction aims to infer new facts based on existing facts in the KG. Recent studies have shown that using the graph neighborhood of a node via graph neural networks (GNNs) provides more useful information compared to just using the query information. Conventional GNNs for KG link prediction follow the standard message-passing paradigm on the entire KG, which leads to superfluous computation, over-smoothing of node representations, and also limits their expressive power. On a large scale, it becomes computationally expensive to aggregate useful information from the entire KG for inference. To address the limitations of existing KG link prediction frameworks, we propose a novel retrieve-and-read framework, which first retrieves a relevant subgraph context for the query and then jointly reasons over the context and the query with a high-capacity reader. As part of our exemplar instantiation for the new framework, we propose a novel Transformer-based GNN as the r
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2212.09702</link><description>&lt;p&gt;
&#35770;&#25991;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On Event Individuation for Document-Level Information Extraction. (arXiv:2212.09702v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09702
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38382;&#39064;&#9472;&#9472;&#20107;&#20214;&#20010;&#20307;&#21270;&#23545;&#20110;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#26159;&#21542;&#36866;&#29992;&#65292;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#22312;&#22788;&#29702;&#25972;&#20010;&#25991;&#20214;&#26041;&#38754;&#36234;&#26469;&#36234;&#29087;&#32451;&#65292;&#20256;&#32479;&#30340;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#20316;&#20026;&#25991;&#20214;&#32423;&#20449;&#24687;&#25552;&#21462;&#30340;&#22522;&#20934;&#20219;&#21153;&#20877;&#27425;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#20102;&#27169;&#26495;&#22635;&#20805;&#20219;&#21153;&#22312;&#36825;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#35813;&#20219;&#21153;&#35201;&#27714;&#23545;&#20107;&#20214;&#20010;&#20307;&#21270;&#38382;&#39064;&#25552;&#20379;&#26126;&#30830;&#30340;&#31572;&#26696;&#8212;&#8212;&#21363;&#21306;&#20998;&#19981;&#21516;&#30340;&#20107;&#20214;&#8212;&#8212;&#32780;&#21363;&#20351;&#26159;&#20154;&#31867;&#19987;&#23478;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#20063;&#23384;&#22312;&#20998;&#27495;&#12290;&#36890;&#36807;&#27880;&#37322;&#30740;&#31350;&#21644;&#35823;&#24046;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#24341;&#21457;&#20102;&#23545;&#27169;&#26495;&#22635;&#20805;&#24230;&#37327;&#30340;&#26377;&#25928;&#24615;&#12289;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#20197;&#21450;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#30340;&#25285;&#24551;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
As information extraction (IE) systems have grown more adept at processing whole documents, the classic task of template filling has seen renewed interest as benchmark for document-level IE. In this position paper, we call into question the suitability of template filling for this purpose. We argue that the task demands definitive answers to thorny questions of event individuation -- the problem of distinguishing distinct events -- about which even human experts disagree. Through an annotation study and error analysis, we show that this raises concerns about the usefulness of template filling metrics, the quality of datasets for the task, and the ability of models to learn it. Finally, we consider possible solutions.
&lt;/p&gt;</description></item><item><title>DiSTRICT&#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#27169;&#26495;&#30340;&#36890;&#29992;&#19978;&#19979;&#25991;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#24494;&#35843;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#26102;&#33719;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.02851</link><description>&lt;p&gt;
DiSTRICT: &#36890;&#36807;Retriever&#39537;&#21160;&#30340;&#19978;&#19979;&#25991;&#35843;&#20248;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DiSTRICT: Dialogue State Tracking with Retriever Driven In-Context Tuning. (arXiv:2212.02851v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02851
&lt;/p&gt;
&lt;p&gt;
DiSTRICT&#26159;&#19968;&#31181;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#27169;&#26495;&#30340;&#36890;&#29992;&#19978;&#19979;&#25991;&#35843;&#20248;&#26041;&#27861;&#65292;&#36890;&#36807;&#26816;&#32034;&#30456;&#20851;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#24494;&#35843;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#26102;&#33719;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;(DST)&#26159;&#20219;&#21153;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#23427;&#36890;&#36807;&#30830;&#23450;&#36827;&#34892;&#20013;&#23545;&#35805;&#20013;&#39044;&#23450;&#20041;&#27133;&#20301;&#30340;&#20540;&#26469;&#34920;&#31034;&#29992;&#25143;&#24847;&#22270;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#27169;&#26495;&#21644;&#39069;&#22806;&#30340;&#27133;&#20301;&#20449;&#24687;&#26469;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#25552;&#31034;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#24341;&#20986;&#27133;&#20301;&#30340;&#20540;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#38656;&#35201;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26032;&#30340;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiSTRICT&#65292;&#19968;&#31181;&#36866;&#29992;&#20110;DST&#30340;&#36890;&#29992;&#19978;&#19979;&#25991;&#35843;&#20248;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#26816;&#32034;&#19982;&#32473;&#23450;&#23545;&#35805;&#39640;&#24230;&#30456;&#20851;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#36827;&#34892;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#27169;&#26495;&#12290;&#22312;&#20351;&#29992;MultiWOZ&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#65292;DiSTRICT&#22312;&#21508;&#31181;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#20351;&#29992;&#36739;&#23567;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20174;&#32780;&#20026;&#32463;&#24120;&#36827;&#34892;&#23454;&#38469;&#37096;&#32626;&#30340;&#23454;&#38469;&#24773;&#20917;&#25552;&#20379;&#20102;&#37325;&#35201;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue State Tracking (DST), a key component of task-oriented conversation systems, represents user intentions by determining the values of pre-defined slots in an ongoing dialogue. Existing approaches use hand-crafted templates and additional slot information to fine-tune and prompt large pre-trained language models and elicit slot values from the dialogue context. Significant manual effort and domain knowledge is required to design effective prompts, limiting the generalizability of these approaches to new domains and tasks. In this work, we propose DiSTRICT, a generalizable in-context tuning approach for DST that retrieves highly relevant training examples for a given dialogue to fine-tune the model without any hand-crafted templates. Experiments with the MultiWOZ benchmark datasets show that DiSTRICT outperforms existing approaches in various zero-shot and few-shot settings using a much smaller model, thereby providing an important advantage for real-world deployments that often 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IRRGN&#30340;&#38544;&#24335;&#20851;&#31995;&#25512;&#29702;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21709;&#24212;&#36873;&#25321;&#20219;&#21153;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#38544;&#24335;&#25552;&#21462;&#35805;&#35821;&#20043;&#38388;&#20197;&#21450;&#35805;&#35821;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#21452;&#37325;&#27604;&#36739;&#24863;&#30693;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2212.00482</link><description>&lt;p&gt;
IRRGN: &#19968;&#31181;&#29992;&#20110;&#22810;&#36718;&#21709;&#24212;&#36873;&#25321;&#30340;&#38544;&#24335;&#20851;&#31995;&#25512;&#29702;&#22270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection. (arXiv:2212.00482v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IRRGN&#30340;&#38544;&#24335;&#20851;&#31995;&#25512;&#29702;&#22270;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21709;&#24212;&#36873;&#25321;&#20219;&#21153;&#12290;&#35813;&#32593;&#32476;&#21033;&#29992;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#38544;&#24335;&#25552;&#21462;&#35805;&#35821;&#20043;&#38388;&#20197;&#21450;&#35805;&#35821;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#21452;&#37325;&#27604;&#36739;&#24863;&#30693;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#21709;&#24212;&#36873;&#25321;&#30340;&#20219;&#21153;&#26159;&#20174;&#25152;&#26377;&#20505;&#36873;&#39033;&#20013;&#25214;&#21040;&#26368;&#20339;&#36873;&#39033;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#26356;&#20851;&#27880;&#20351;&#29992;&#26126;&#30830;&#30340;&#31639;&#27861;&#26469;&#24314;&#27169;&#35805;&#35821;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36825;&#20123;&#20851;&#31995;&#26159;&#30830;&#23450;&#24615;&#30340;&#12289;&#26377;&#38480;&#30340;&#21644;&#19981;&#28789;&#27963;&#30340;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#21040;&#25512;&#29702;&#20043;&#21069;&#21644;&#20043;&#21518;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#20851;&#31995;&#25512;&#29702;&#22270;&#32593;&#32476;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#23427;&#30001;&#35805;&#35821;&#20851;&#31995;&#25512;&#29702;&#22120;&#65288;URR&#65289;&#21644;&#36873;&#39033;&#21452;&#37325;&#27604;&#36739;&#22120;&#65288;ODC&#65289;&#32452;&#25104;&#12290;URR&#30340;&#30446;&#26631;&#26159;&#38544;&#24335;&#25552;&#21462;&#35805;&#35821;&#20043;&#38388;&#20197;&#21450;&#35805;&#35821;&#21644;&#36873;&#39033;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#20851;&#31995;&#22270;&#21367;&#31215;&#32593;&#32476;&#36827;&#34892;&#25512;&#29702;&#12290;ODC&#19987;&#27880;&#20110;&#36890;&#36807;&#21452;&#37325;&#27604;&#36739;&#24863;&#30693;&#36873;&#39033;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#25490;&#38500;&#22122;&#22768;&#36873;&#39033;&#30340;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22810;&#36718;&#23545;&#35805;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;MuTual&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#21709;&#24212;&#36873;&#25321;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of response selection in multi-turn dialogue is to find the best option from all candidates. In order to improve the reasoning ability of the model, previous studies pay more attention to using explicit algorithms to model the dependencies between utterances, which are deterministic, limited and inflexible. In addition, few studies consider differences between the options before and after reasoning. In this paper, we propose an Implicit Relational Reasoning Graph Network to address these issues, which consists of the Utterance Relational Reasoner (URR) and the Option Dual Comparator (ODC). URR aims to implicitly extract dependencies between utterances, as well as utterances and options, and make reasoning with relational graph convolutional networks. ODC focuses on perceiving the difference between the options through dual comparison, which can eliminate the interference of the noise options. Experimental results on two multi-turn dialogue reasoning benchmark datasets MuTual a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35821;&#27861;&#35268;&#21017;&#30340;&#35825;&#23548;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35821;&#27861;&#21487;&#26367;&#20195;&#24615;&#65292;&#33021;&#22815;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.16031</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#20381;&#23384;&#35821;&#27861;&#30340;&#35821;&#27861;&#21487;&#26367;&#20195;&#24615;
&lt;/p&gt;
&lt;p&gt;
Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35821;&#27861;&#35268;&#21017;&#30340;&#35825;&#23548;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35821;&#27861;&#21487;&#26367;&#20195;&#24615;&#65292;&#33021;&#22815;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#26159;&#26500;&#25104;&#20154;&#31867;&#35821;&#35328;&#40092;&#26126;&#21644;&#32452;&#21512;&#24615;&#30340;&#28508;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#31350;&#20102;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#21477;&#27861;&#20381;&#23384;&#20851;&#31995;&#34920;&#31034;&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35821;&#27861;&#35268;&#21017;&#30340;&#35825;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#27169;&#25311;&#21477;&#27861;&#26367;&#20195;&#24615;&#36825;&#20010;&#26356;&#20026;&#26222;&#36941;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#20005;&#26684;&#25353;&#29031;&#26631;&#27880;&#26550;&#26500;&#24314;&#27169;&#20381;&#23384;&#20851;&#31995;&#12290;&#36825;&#31181;&#20851;&#31995;&#20307;&#29616;&#20102;&#20107;&#23454;&#65292;&#21363;&#21477;&#27861;&#20381;&#23384;&#20851;&#31995;&#20004;&#31471;&#30340;&#21333;&#35789;&#21487;&#20197;&#34987;&#21516;&#19968;&#21477;&#27861;&#33539;&#30068;&#30340;&#21333;&#35789;&#25152;&#26367;&#25442;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#19968;&#32452;&#35821;&#27861;&#19981;&#21464;&#30340;&#21477;&#23376;&#12290;&#36825;&#20123;&#21477;&#23376;&#30340;&#34920;&#31034;&#34987;&#29992;&#20316;&#35299;&#26512;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#65292;&#20363;&#22914;&#22312;&#38271;&#36317;&#31163;&#20027;&#35859;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;78.3&#65285;&#30340;&#21484;&#22238;&#29575;&#65292;&#32780;&#20043;&#21069;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21482;&#26377;8.5&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we further explore the hypothesis that syntactic dependencies can be represented in the attention distributions of language models trained on text and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic substitutability. This property captures the fact that the words at either end of a syntactic dependency can be substituted with words from the same syntactic category, defining a set of syntactically-invariant sentences whose representations are then used as the basis for parsing. We demonstrate that our method results in both qualitative and quantitative gains, for example achieving 78.3% recall on a long-distance subject-verb agreement task vs. 8.5% with a previous unsupervis
&lt;/p&gt;</description></item><item><title>SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.13308</link><description>&lt;p&gt;
SciRepEval&#65306;&#19968;&#20010;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#22810;&#26684;&#24335;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SciRepEval: A Multi-Format Benchmark for Scientific Document Representations. (arXiv:2211.13308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13308
&lt;/p&gt;
&lt;p&gt;
SciRepEval&#26159;&#31532;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#25913;&#36827;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#21487;&#20197;&#20316;&#20026;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#36755;&#20837;&#29305;&#24449;&#65292;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#34920;&#31034;&#30340;&#29616;&#26377;&#22522;&#20934;&#26410;&#33021;&#25429;&#25417;&#21040;&#30456;&#20851;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; SciRepEval&#65292;&#31532;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#31185;&#23398;&#25991;&#29486;&#34920;&#31034;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;&#23427;&#21253;&#25324;&#22235;&#31181;&#26684;&#24335;&#30340; 25 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#29616;&#23454;&#24615;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013; 11 &#20010;&#26159;&#26032;&#20219;&#21153;&#65306;&#20998;&#31867;&#12289;&#22238;&#24402;&#12289;&#25490;&#21517;&#21644;&#25628;&#32034;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#22522;&#20934;&#26469;&#30740;&#31350;&#21644;&#25913;&#36827;&#31185;&#23398;&#25991;&#26723;&#34920;&#31034;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;&#20309;&#22312;&#20219;&#21153;&#26684;&#24335;&#26041;&#38754;&#32570;&#20047;&#27867;&#21270;&#24615;&#33021;&#65292;&#31616;&#21333;&#30340;&#22810;&#20219;&#21153;&#35757;&#32451;&#20063;&#19981;&#33021;&#25913;&#36827;&#23427;&#20204;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23398;&#20064;&#27599;&#20010;&#25991;&#26723;&#30340;&#22810;&#20010;&#23884;&#20837;&#65292;&#27599;&#20010;&#23884;&#20837;&#19987;&#38376;&#38024;&#23545;&#19981;&#21516;&#30340;&#26684;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#20219;&#21153;&#26684;&#24335;&#29305;&#23450;&#30340;&#25511;&#21046;&#20195;&#30721;&#21644;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learned representations of scientific documents can serve as valuable input features for downstream tasks, without the need for further fine-tuning. However, existing benchmarks for evaluating these representations fail to capture the diversity of relevant tasks. In response, we introduce SciRepEval, the first comprehensive benchmark for training and evaluating scientific document representations. It includes 25 challenging and realistic tasks, 11 of which are new, across four formats: classification, regression, ranking and search. We then use the benchmark to study and improve the generalization ability of scientific document representation models. We show how state-of-the-art models struggle to generalize across task formats, and that simple multi-task training fails to improve them. However, a new approach that learns multiple embeddings per document, each tailored to a different format, can improve performance. We experiment with task-format-specific control codes and adapters in 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;"&#24605;&#32500;&#31243;&#24207;"&#65288;PoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#36807;&#31243;&#34920;&#36798;&#20026;&#19968;&#20010;&#31243;&#24207;&#65292;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#36807;&#31243;&#20998;&#31163;&#24320;&#26469;&#65292;&#20197;&#25552;&#21319;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;PoT&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;CoT&#22312;&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;&#32422;12&#65285;&#12290;</title><link>http://arxiv.org/abs/2211.12588</link><description>&lt;p&gt;
&#24605;&#32500;&#20419;&#36827;&#31243;&#24207;&#65306;&#20026;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#20998;&#31163;&#24320;&#26469;
&lt;/p&gt;
&lt;p&gt;
Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks. (arXiv:2211.12588v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12588
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;"&#24605;&#32500;&#31243;&#24207;"&#65288;PoT&#65289;&#65292;&#36890;&#36807;&#23558;&#25512;&#29702;&#36807;&#31243;&#34920;&#36798;&#20026;&#19968;&#20010;&#31243;&#24207;&#65292;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#36807;&#31243;&#20998;&#31163;&#24320;&#26469;&#65292;&#20197;&#25552;&#21319;&#35299;&#20915;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20013;&#65292;PoT&#30456;&#27604;&#26368;&#26032;&#26041;&#27861;CoT&#22312;&#24615;&#33021;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;&#32422;12&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#30340;&#36880;&#27493;&#25512;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#24605;&#32500;&#38142;&#25552;&#31034;&#65288;CoT&#65289;&#26159;&#30446;&#21069;&#36825;&#20123;&#20219;&#21153;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;CoT&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25191;&#34892;&#22810;&#27493;&#39588;&#30340;&#25512;&#29702;&#21644;&#35745;&#31639;&#36807;&#31243;&#12290;&#20026;&#20102;&#23558;&#35745;&#31639;&#19982;&#25512;&#29702;&#20998;&#31163;&#24320;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;"&#24605;&#32500;&#31243;&#24207;"&#65288;PoT&#65289;&#65292;&#23427;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;&#20027;&#35201;&#26159;Codex&#65289;&#23558;&#25512;&#29702;&#36807;&#31243;&#34920;&#36798;&#20026;&#19968;&#20010;&#31243;&#24207;&#12290;&#35745;&#31639;&#36807;&#31243;&#34987;&#22996;&#25176;&#32473;&#22806;&#37096;&#35745;&#31639;&#26426;&#25191;&#34892;&#29983;&#25104;&#30340;&#31243;&#24207;&#20197;&#24471;&#20986;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#25968;&#23398;&#38382;&#39064;&#25968;&#25454;&#38598;&#65288;GSM&#65292;AQuA&#65292;SVAMP&#65292;TabMWP&#65292;MultiArith&#65289;&#21644;&#19977;&#20010;&#37329;&#34701;&#38382;&#31572;&#25968;&#25454;&#38598;&#65288;FinQA&#65292;ConvFinQA&#65292;TATQA&#65289;&#19978;&#35780;&#20272;&#20102;PoT&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;PoT&#30340;&#24179;&#22343;&#24615;&#33021;&#25552;&#21319;&#32422;&#20026;12&#65285;&#65292;&#22312;&#25152;&#26377;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#19978;&#12290;&#36890;&#36807;&#23558;PoT&#19982;&#33258;&#19968;&#33268;&#24615;&#32452;&#21512;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been significant progress in teaching language models to perform step-by-step reasoning to solve complex numerical reasoning tasks. Chain-of-thoughts prompting (CoT) is by far the state-of-art method for these tasks. CoT uses language models to perform both reasoning and computation in the multi-step `thought' process. To disentangle computation from reasoning, we propose `Program of Thoughts' (PoT), which uses language models (mainly Codex) to express the reasoning process as a program. The computation is relegated to an external computer, which executes the generated programs to derive the answer. We evaluate PoT on five math word problem datasets (GSM, AQuA, SVAMP, TabMWP, MultiArith) and three financial-QA datasets (FinQA, ConvFinQA, TATQA) for both few-shot and zero-shot setups. Under both few-shot and zero-shot settings, PoT can show an average performance gain over CoT by around 12\% across all the evaluated datasets. By combining PoT with self-consistency de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.11093</link><description>&lt;p&gt;
[RE]VER&#65306;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#20197;&#38416;&#36848;&#23454;&#20307;&#21644;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#36890;&#36807;&#29702;&#35299;&#23454;&#20307;&#21644;&#20851;&#31995;&#26469;&#20102;&#35299;&#19990;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose [RE]VER: A Unified Model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to repres
&lt;/p&gt;</description></item><item><title>CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.09935</link><description>&lt;p&gt;
CAPE: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
CAPE: Corrective Actions from Precondition Errors using Large Language Models. (arXiv:2211.09935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09935
&lt;/p&gt;
&lt;p&gt;
CAPE&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21069;&#32622;&#38169;&#35823;&#20013;&#32416;&#27491;&#34892;&#21160;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#65292;&#20351;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#26356;&#22810;&#20219;&#21153;&#65292;&#24182;&#25913;&#21892;&#20102;&#35745;&#21010;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#20026;&#35774;&#35745;&#26234;&#33021;&#26426;&#22120;&#20154;&#25552;&#20379;&#20102;&#19968;&#31181;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#34892;&#21160;&#22833;&#36133;&#26102;&#26080;&#27861;&#24674;&#22797;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#33021;&#23581;&#35797;&#37325;&#26032;&#25191;&#34892;&#22833;&#36133;&#30340;&#34892;&#21160;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#38169;&#35823;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65288;CAPE&#65289;&#65292;&#35797;&#22270;&#22312;&#35268;&#21010;&#36807;&#31243;&#20013;&#25552;&#20986;&#32416;&#27491;&#21069;&#32622;&#26465;&#20214;&#38169;&#35823;&#30340;&#34892;&#21160;&#12290;CAPE&#36890;&#36807;&#21033;&#29992;&#23569;&#26679;&#26412;&#25512;&#29702;&#20174;&#34892;&#21160;&#21069;&#32622;&#26465;&#20214;&#20013;&#25552;&#39640;&#20102;&#29983;&#25104;&#35745;&#21010;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#20855;&#36523;&#20195;&#29702;&#33021;&#22815;&#25191;&#34892;&#27604;&#22522;&#32447;&#26041;&#27861;&#26356;&#22810;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#30830;&#20445;&#35821;&#20041;&#27491;&#30830;&#24615;&#21644;&#26368;&#23567;&#21270;&#37325;&#26032;&#25552;&#31034;&#12290;&#22312;VirtualHome&#20013;&#65292;CAPE&#29983;&#25104;&#21487;&#25191;&#34892;&#30340;&#35745;&#21010;&#65292;&#24182;&#19988;&#30456;&#27604;SayCan&#65292;&#23558;&#20154;&#24037;&#26631;&#27880;&#30340;&#35745;&#21010;&#27491;&#30830;&#24230;&#25351;&#26631;&#20174;28.89%&#25552;&#39640;&#21040;49.63%&#12290;&#25105;&#20204;&#30340;&#25913;&#36827;&#20063;&#36866;&#29992;&#20110;&#19968;&#21488;&#37197;&#32622;&#20102;&#19968;&#32452;&#20197;&#35821;&#35328;&#20026;&#25351;&#23450;&#30340;&#25216;&#33021;&#21644;&#30456;&#20851;&#21069;&#32622;&#26465;&#20214;&#30340;&#27874;&#22763;&#39039;&#21160;&#21147;&#20844;&#21496;&#30340;Spot&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;CAPE&#25552;&#39640;&#20102;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting commonsense knowledge from a large language model (LLM) offers a path to designing intelligent robots. Existing approaches that leverage LLMs for planning are unable to recover when an action fails and often resort to retrying failed actions, without resolving the error's underlying cause.  We propose a novel approach (CAPE) that attempts to propose corrective actions to resolve precondition errors during planning. CAPE improves the quality of generated plans by leveraging few-shot reasoning from action preconditions. Our approach enables embodied agents to execute more tasks than baseline methods while ensuring semantic correctness and minimizing re-prompting. In VirtualHome, CAPE generates executable plans while improving a human-annotated plan correctness metric from 28.89% to 49.63% over SayCan. Our improvements transfer to a Boston Dynamics Spot robot initialized with a set of skills (specified in language) and associated preconditions, where CAPE improves the correctne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.08238</link><description>&lt;p&gt;
&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Exploiting Contrastive Learning and Numerical Evidence for Improving Confusing Legal Judgment Prediction. (arXiv:2211.08238v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#25968;&#23383;&#35777;&#25454;&#30340;&#26041;&#27861;&#25913;&#36827;&#28151;&#28102;&#30340;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#21033;&#29992;&#25968;&#23383;&#35777;&#25454;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21306;&#20998;&#20998;&#31867;&#38169;&#35823;&#21644;&#21033;&#29992;&#25968;&#23383;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#19968;&#20010;&#27861;&#24459;&#26696;&#20363;&#30340;&#20107;&#23454;&#25551;&#36848;&#25991;&#26412;&#65292;&#27861;&#24459;&#21028;&#20915;&#39044;&#27979;&#65288;LJP&#65289;&#26088;&#22312;&#39044;&#27979;&#26696;&#20363;&#30340;&#32618;&#21517;&#12289;&#27861;&#24459;&#26465;&#27454;&#21644;&#22788;&#32602;&#26399;&#38480;&#12290;LJP&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#22914;&#20309;&#21306;&#20998;&#28151;&#28102;&#30340;&#27861;&#24459;&#26696;&#20363;&#65292;&#20854;&#20013;&#21482;&#23384;&#22312;&#24494;&#22937;&#30340;&#25991;&#26412;&#24046;&#24322;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#20998;&#31867;&#25439;&#22833;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#30340;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#24573;&#30053;&#20102;&#20107;&#23454;&#25551;&#36848;&#20013;&#30340;&#25968;&#23383;&#65292;&#29992;&#20110;&#39044;&#27979;&#22788;&#32602;&#26399;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;moco&#30340;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#65292;&#20197;&#23398;&#20064;&#21487;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#24182;&#25506;&#32034;&#26500;&#24314;&#27491;&#20363;&#23545;&#30340;&#26368;&#20339;&#31574;&#30053;&#65292;&#20174;&#32780;&#21516;&#26102;&#26377;&#21033;&#20110;LJP&#30340;&#19977;&#20010;&#23376;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#21033;&#29992;&#27861;&#24459;&#26696;&#20363;&#20013;&#30340;&#25968;&#23383;&#26469;&#39044;&#27979;&#26576;&#20123;&#26696;&#20363;&#30340;&#22788;&#32602;&#26399;&#38480;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#30001;&#39044;&#35757;&#32451;&#25968;&#20540;&#27169;&#22411;&#32534;&#30721;&#30340;&#25552;&#21462;&#30340;&#29359;&#32618;&#37329;&#39069;&#23545;&#20107;&#23454;&#25551;&#36848;&#30340;&#34920;&#31034;&#12290;&#23545;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the fact description text of a legal case, legal judgment prediction (LJP) aims to predict the case's charge, law article and penalty term. A core problem of LJP is how to distinguish confusing legal cases, where only subtle text differences exist. Previous studies fail to distinguish different classification errors with a standard cross-entropy classification loss, and ignore the numbers in the fact description for predicting the term of penalty. To tackle these issues, in this work, first, we propose a moco-based supervised contrastive learning to learn distinguishable representations, and explore the best strategy to construct positive example pairs to benefit all three subtasks of LJP simultaneously. Second, in order to exploit the numbers in legal cases for predicting the penalty terms of certain cases, we further enhance the representation of the fact description with extracted crime amounts which are encoded by a pre-trained numeracy model. Extensive experiments on public 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35780;&#20272;&#27969;&#31243;&#26469;&#27979;&#35797;&#27169;&#22411;&#23545;&#35270;&#35273;&#22330;&#26223;&#12289;&#25991;&#26412;&#21644;&#30456;&#20851;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26631;&#20934;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.05895</link><description>&lt;p&gt;
&#29702;&#35299;ME&#65311;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#35273;&#24120;&#35782;&#30340;&#22810;&#27169;&#24577;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Understanding ME? Multimodal Evaluation for Fine-grained Visual Commonsense. (arXiv:2211.05895v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05895
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35780;&#20272;&#27969;&#31243;&#26469;&#27979;&#35797;&#27169;&#22411;&#23545;&#35270;&#35273;&#22330;&#26223;&#12289;&#25991;&#26412;&#21644;&#30456;&#20851;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#35813;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26631;&#20934;&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#24120;&#35782;&#29702;&#35299;&#35201;&#27714;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#35201;&#29702;&#35299;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#36824;&#35201;&#22312;&#20004;&#32773;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#21442;&#32771;&#65292;&#20197;&#23436;&#20840;&#34701;&#21512;&#21644;&#29702;&#35299;&#25152;&#25551;&#36848;&#30340;&#35270;&#35273;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#24182;&#22312;&#35270;&#35273;&#24120;&#35782;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#35780;&#20272;&#25968;&#25454;&#36164;&#28304;&#65292;&#25105;&#20204;&#26080;&#27861;&#30830;&#23450;&#27169;&#22411;&#26159;&#21542;&#30495;&#27491;&#29702;&#35299;&#35270;&#35273;&#22330;&#26223;&#21644;&#28508;&#22312;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;&#20026;&#20102;&#25552;&#20379;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#35780;&#20272;&#65288;ME&#65289;&#27969;&#31243;&#65292;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;-&#31572;&#26696;&#23545;&#26469;&#27979;&#35797;&#27169;&#22411;&#23545;&#35270;&#35273;&#22330;&#26223;&#12289;&#25991;&#26412;&#21644;&#30456;&#20851;&#30693;&#35782;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#23637;&#31034;&#65292;&#20351;&#29992;ME&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#26631;&#20934;VCR&#35780;&#20272;&#20013;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#27604;&#36739;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#21457;&#29616;&#65306;&#65288;1&#65289;&#35821;&#20041;&#20302;&#32423;&#20449;&#24687;&#21487;&#20197;&#24110;&#21161;&#23398;&#20064;&#39640;&#32423;&#20449;&#24687;&#65292;&#20294;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#35821;&#20041;&#29702;&#35299;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual commonsense understanding requires Vision Language (VL) models to not only understand image and text but also cross-reference in-between to fully integrate and achieve comprehension of the visual scene described. Recently, various approaches have been developed and have achieved high performance on visual commonsense benchmarks. However, it is unclear whether the models really understand the visual scene and underlying commonsense knowledge due to limited evaluation data resources. To provide an in-depth analysis, we present a Multimodal Evaluation (ME) pipeline to automatically generate question-answer pairs to test models' understanding of the visual scene, text, and related knowledge. We then take a step further to show that training with the ME data boosts the model's performance in standard VCR evaluation. Lastly, our in-depth analysis and comparison reveal interesting findings: (1) semantically low-level information can assist the learning of high-level information but not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;HuBERT&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#33976;&#39311;&#65292;&#20197;&#25913;&#21892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#21028;&#21035;&#25439;&#22833;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24494;&#22411;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.15631</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#33976;&#39311;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Effective Distillation of Self-Supervised Speech Models for Automatic Speech Recognition. (arXiv:2210.15631v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#23558;HuBERT&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#33976;&#39311;&#65292;&#20197;&#25913;&#21892;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#12290;&#36890;&#36807;&#30740;&#31350;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#32467;&#26500;&#65292;&#24182;&#24341;&#20837;&#21028;&#21035;&#25439;&#22833;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#24494;&#22411;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#35821;&#38899;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#26356;&#21916;&#27426;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#26469;&#22686;&#21152;&#24314;&#27169;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#36807;&#22823;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#32780;&#38480;&#21046;&#20102;&#20854;&#28508;&#22312;&#24212;&#29992;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#30340;&#24494;&#22411;&#21270;&#24050;&#25104;&#20026;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#30340;&#37325;&#35201;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22522;&#20110;HuBERT&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#26377;&#25928;&#33976;&#39311;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20934;&#32447;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30340;&#23398;&#29983;&#27169;&#22411;&#32467;&#26500;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#20316;&#20026;&#23545;&#20808;&#21069;&#24037;&#20316;&#20013;&#24191;&#27867;&#37319;&#29992;&#30340;&#22238;&#24402;&#25439;&#22833;&#30340;&#34917;&#20805;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21028;&#21035;&#25439;&#22833;&#26469;&#22686;&#24378;HuBERT&#30340;&#33976;&#39311;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#22330;&#26223;&#19979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#36827;&#34892;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed great strides in self-supervised learning (SSL) on the speech processing. The SSL model is normally pre-trained on a great variety of unlabelled data and a large model size is preferred to increase the modeling capacity. However, this might limit its potential applications due to the expensive computation and memory costs introduced by the oversize model. Miniaturization for SSL models has become an important research direction of practical value. To this end, we explore the effective distillation of HuBERT-based SSL models for automatic speech recognition (ASR). First, in order to establish a strong baseline, a comprehensive study on different student model structures is conducted. On top of this, as a supplement to the regression loss widely adopted in previous works, a discriminative loss is introduced for HuBERT to enhance the distillation performance, especially in low-resource scenarios. In addition, we design a simple and effective algorithm to distil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20197;&#23454;&#29616;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.15500</link><description>&lt;p&gt;
COFFEE: &#21487;&#35299;&#37322;&#24615;&#25512;&#33616;&#20013;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
COFFEE: Counterfactual Fairness for Personalized Text Generation in Explainable Recommendation. (arXiv:2210.15500v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#22312;&#35299;&#37322;&#29983;&#25104;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#20197;&#23454;&#29616;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#25968;&#23383;&#29983;&#27963;&#20013;&#65292;&#20010;&#24615;&#21270;&#25991;&#26412;&#29983;&#25104;&#65288;PTG&#65289;&#24050;&#25104;&#20026;&#19968;&#20010;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;PTG&#27169;&#22411;&#35757;&#32451;&#30340;&#29992;&#25143;&#32534;&#20889;&#25991;&#26412;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#23558;&#19981;&#21516;&#27700;&#24179;&#30340;&#35821;&#35328;&#36136;&#37327;&#19982;&#29992;&#25143;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#20851;&#32852;&#36215;&#26469;&#12290;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#36825;&#31181;&#20559;&#35265;&#65292;&#24182;&#22312;&#29983;&#25104;&#19982;&#29992;&#25143;&#30340;&#21463;&#20445;&#25252;&#23646;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#26102;&#24310;&#32493;&#19981;&#24179;&#31561;&#65292;&#23548;&#33268;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#26381;&#21153;&#26102;&#20986;&#29616;&#19981;&#20844;&#24179;&#30340;&#23545;&#24453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20010;&#24615;&#21270;&#35299;&#37322;&#29983;&#25104;&#20013;PTG&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#25104;&#30340;&#35299;&#37322;&#20013;&#30340;&#20559;&#35265;&#21450;&#20854;&#20844;&#24179;&#24615;&#24433;&#21709;&#12290;&#20026;&#20102;&#20419;&#36827;&#20844;&#24179;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#35299;&#37322;&#29983;&#25104;&#20013;&#30340;&#24230;&#37327;&#29305;&#23450;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models become increasingly integrated into our digital lives, Personalized Text Generation (PTG) has emerged as a pivotal component with a wide range of applications. However, the bias inherent in user written text, often used for PTG model training, can inadvertently associate different levels of linguistic quality with users' protected attributes. The model can inherit the bias and perpetuate inequality in generating text w.r.t. users' protected attributes, leading to unfair treatment when serving users. In this work, we investigate fairness of PTG in the context of personalized explanation generation for recommendations. We first discuss the biases in generated explanations and their fairness implications. To promote fairness, we introduce a general framework to achieve measure-specific counterfactual fairness in explanation generation. Extensive experiments and human evaluations demonstrate the effectiveness of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31038;&#20250;&#35268;&#33539;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19968;&#32452;&#26631;&#27880;&#32773;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#35780;&#20215;&#26469;&#39044;&#27979;&#31038;&#20250;&#35268;&#33539;&#30340;&#35748;&#30693;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20010;&#24615;&#21270;&#26041;&#27861;&#22312;&#39044;&#27979;&#31038;&#20250;&#35268;&#33539;&#35748;&#30693;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#30340;&#23376;&#38598;&#20013;&#30340;&#34920;&#29616;&#26377;&#25152;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.14531</link><description>&lt;p&gt;
&#32479;&#19968;&#25968;&#25454;&#35266;&#28857;&#21644;&#20010;&#24615;&#21270;: &#24212;&#29992;&#20110;&#31038;&#20250;&#35268;&#33539;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unifying Data Perspectivism and Personalization: An Application to Social Norms. (arXiv:2210.14531v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31038;&#20250;&#35268;&#33539;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#19968;&#32452;&#26631;&#27880;&#32773;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#35780;&#20215;&#26469;&#39044;&#27979;&#31038;&#20250;&#35268;&#33539;&#30340;&#35748;&#30693;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20010;&#24615;&#21270;&#26041;&#27861;&#22312;&#39044;&#27979;&#31038;&#20250;&#35268;&#33539;&#35748;&#30693;&#26041;&#38754;&#20855;&#26377;&#36739;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#22312;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#30340;&#23376;&#38598;&#20013;&#30340;&#34920;&#29616;&#26377;&#25152;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#19981;&#20877;&#20351;&#29992;&#21333;&#19968;&#30340;&#30495;&#23454;&#26631;&#31614;&#65292;&#36817;&#26399;&#30740;&#31350;&#30528;&#37325;&#20110;&#22914;&#20309;&#34920;&#31034;&#21644;&#39044;&#27979;&#19968;&#32452;&#26631;&#27880;&#32773;&#30340;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26631;&#27880;&#32773;&#30340;&#20449;&#24687;&#24448;&#24448;&#24456;&#23569;&#25110;&#27809;&#26377;&#20102;&#35299;&#65292;&#25110;&#32773;&#26631;&#27880;&#32773;&#30340;&#25968;&#37327;&#24456;&#23569;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19968;&#32452;13k&#26631;&#27880;&#32773;&#23545;210k&#39033;&#31038;&#20250;&#35268;&#33539;&#30340;&#35780;&#20215;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#26631;&#27880;&#32773;&#24314;&#27169;&#30340;&#23454;&#39564;&#35774;&#35745;&#65292;&#27604;&#36739;&#20102;&#20010;&#24615;&#21270;&#26041;&#27861;&#23545;&#39044;&#27979;&#31038;&#20250;&#35268;&#33539;&#35748;&#30693;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#19981;&#21516;&#31038;&#20132;&#24773;&#22659;&#30340;&#23376;&#38598;&#36827;&#34892;&#34920;&#29616;&#20998;&#26512;&#65292;&#36825;&#20123;&#24773;&#22659;&#26681;&#25454;&#20914;&#31361;&#21508;&#26041;&#20043;&#38388;&#20851;&#31995;&#30340;&#20146;&#23494;&#24230;&#32780;&#24322;&#65292;&#35780;&#20272;&#20010;&#24615;&#21270;&#26041;&#27861;&#22312;&#21738;&#20123;&#26041;&#38754;&#30340;&#25928;&#26524;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instead of using a single ground truth for language processing tasks, several recent studies have examined how to represent and predict the labels of the set of annotators. However, often little or no information about annotators is known, or the set of annotators is small. In this work, we examine a corpus of social media posts about conflict from a set of 13k annotators and 210k judgements of social norms. We provide a novel experimental setup that applies personalization methods to the modeling of annotators and compare their effectiveness for predicting the perception of social norms. We further provide an analysis of performance across subsets of social situations that vary by the closeness of the relationship between parties in conflict, and assess where personalization helps the most.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12770</link><description>&lt;p&gt;
&#20851;&#20110;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#25968;&#25454;&#21463;&#38480;&#24494;&#35843;&#20013;&#23427;&#20204;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#29992;&#20174;&#19968;&#33324;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23558;&#20854;&#24494;&#35843;&#21040;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#65292;&#24182;&#20351;&#29992;&#26032;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#23454;&#36341;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#26159;&#22312;&#33647;&#29289;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#21644;&#36890;&#36807;&#24494;&#35843;BERT-based LLMs&#65288;&#21253;&#25324;BERT-base&#12289;BioBERT&#21644;ClinicalBERT&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#19982;&#24102;&#26377;CRF&#23618;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;CRF&#23618;&#23545;&#25152;&#26377;&#31070;&#32463;&#27169;&#22411;&#37117;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#65307;2&#65289;&#22312;&#20351;&#29992;&#23439;&#24179;&#22343;F1&#23545;BIO-strict&#36328;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24494;&#35843;&#30340;LLMs&#33719;&#24471;&#20102;0.83+&#30340;&#24471;&#20998;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;TransformerCRF&#27169;&#22411;&#24471;&#20998;&#20026;0.78+&#65292;&#35777;&#26126;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#21487;&#35299;&#37322;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#22312;&#20219;&#21153;&#24615;&#33021;&#21644;&#20559;&#24046;&#32531;&#35299;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#21644;&#26356;&#20844;&#24179;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2210.07440</link><description>&lt;p&gt;
InterFair: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20844;&#24179;&#21487;&#35299;&#37322;&#39044;&#27979;&#30340;&#21435;&#20559;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
InterFair: Debiasing with Natural Language Feedback for Fair Interpretable Predictions. (arXiv:2210.07440v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07440
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#21487;&#35299;&#37322;&#30340;&#21435;&#20559;&#26041;&#27861;&#65292;&#36890;&#36807;&#29992;&#25143;&#21453;&#39304;&#22312;&#20219;&#21153;&#24615;&#33021;&#21644;&#20559;&#24046;&#32531;&#35299;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#21644;&#26356;&#20844;&#24179;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;NLP&#27169;&#22411;&#21435;&#20559;&#26041;&#27861;&#36890;&#24120;&#19987;&#27880;&#20110;&#38548;&#31163;&#19982;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35748;&#20026;&#19968;&#20010;&#26377;&#21033;&#30340;&#21435;&#20559;&#26041;&#27861;&#24212;&#35813;&#8220;&#20844;&#24179;&#22320;&#8221;&#20351;&#29992;&#25935;&#24863;&#20449;&#24687;&#65292;&#24182;&#19988;&#33021;&#22815;&#35299;&#37322;&#20854;&#20351;&#29992;&#21407;&#22240;&#65292;&#32780;&#19981;&#26159;&#30450;&#30446;&#28040;&#38500;&#23427;&#12290;&#36825;&#31181;&#20844;&#24179;&#24179;&#34913;&#24448;&#24448;&#26159;&#20027;&#35266;&#30340;&#65292;&#19988;&#22312;&#31639;&#27861;&#19978;&#23454;&#29616;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#20132;&#20114;&#35774;&#32622;&#65292;&#24182;&#20351;&#29992;&#20923;&#32467;&#30340;&#39044;&#27979;&#27169;&#22411;&#23637;&#31034;&#20102;&#29992;&#25143;&#33021;&#22815;&#36890;&#36807;&#25552;&#20379;&#21453;&#39304;&#26469;&#22312;&#20219;&#21153;&#24615;&#33021;&#21644;&#20559;&#24046;&#32531;&#35299;&#20043;&#38388;&#23454;&#29616;&#26356;&#22909;&#21644;&#26356;&#20844;&#24179;&#30340;&#24179;&#34913;&#12290;&#22312;&#19968;&#31181;&#35774;&#32622;&#20013;&#65292;&#29992;&#25143;&#36890;&#36807;&#19982;&#27979;&#35797;&#31034;&#20363;&#30340;&#20132;&#20114;&#65292;&#36827;&#19968;&#27493;&#20943;&#23567;&#20102;&#35299;&#37322;&#20013;&#30340;&#20559;&#24046;&#65288;5-8%&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30456;&#21516;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#12290;&#22312;&#21478;&#19968;&#31181;&#35774;&#32622;&#20013;&#65292;&#20154;&#31867;&#21453;&#39304;&#33021;&#22815;&#20174;&#36755;&#20837;&#20013;&#23558;&#30456;&#20851;&#20559;&#24046;&#21644;&#39044;&#27979;&#20449;&#24687;&#30456;&#20998;&#31163;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#21331;&#36234;&#30340;&#20559;&#24046;&#32531;&#35299;&#21644;&#25913;&#21892;&#30340;&#20219;&#21153;&#24615;&#33021;&#65288;4-5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Debiasing methods in NLP models traditionally focus on isolating information related to a sensitive attribute (e.g., gender or race). We instead argue that a favorable debiasing method should use sensitive information 'fairly,' with explanations, rather than blindly eliminating it. This fair balance is often subjective and can be challenging to achieve algorithmically. We explore two interactive setups with a frozen predictive model and show that users able to provide feedback can achieve a better and fairer balance between task performance and bias mitigation. In one setup, users, by interacting with test examples, further decreased bias in the explanations (5-8%) while maintaining the same prediction accuracy. In the other setup, human feedback was able to disentangle associated bias and predictive information from the input leading to superior bias mitigation and improved task performance (4-5%) simultaneously.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixEncoder&#30340;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#21477;&#23545;&#24314;&#27169;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;113&#20493;&#30340;&#21477;&#23545;&#21305;&#37197;&#21152;&#36895;&#65292;&#19982;&#26356;&#26114;&#36149;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2210.05261</link><description>&lt;p&gt;
&#19968;&#27425;&#23601;&#22815;&#20102;&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#29992;&#20110;&#24555;&#36895;&#21477;&#23545;&#24314;&#27169;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Once is Enough: A Light-Weight Cross-Attention for Fast Sentence Pair Modeling. (arXiv:2210.05261v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MixEncoder&#30340;&#26032;&#33539;&#24335;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#21477;&#23545;&#24314;&#27169;&#12290;&#35813;&#33539;&#24335;&#36890;&#36807;&#36731;&#37327;&#32423;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#20102;&#36229;&#36807;113&#20493;&#30340;&#21477;&#23545;&#21305;&#37197;&#21152;&#36895;&#65292;&#19982;&#26356;&#26114;&#36149;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;&#30456;&#27604;&#65292;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21477;&#23545;&#24314;&#27169;&#20219;&#21153;&#65288;&#22914;&#31572;&#26696;&#36873;&#25321;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65289;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#36755;&#20837;&#21477;&#23545;&#19978;&#25191;&#34892;&#20132;&#21449;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#32534;&#30721;&#22120;&#21644;&#21518;&#26399;&#20132;&#20114;&#30340;&#26550;&#26500;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#35745;&#31639;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#34920;&#36798;&#24615;&#21644;&#35745;&#31639;&#36895;&#24230;&#20043;&#38388;&#30340;&#24179;&#34913;&#20173;&#38656;&#26356;&#22909;&#30340;&#21327;&#35843;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;MixEncoder&#29992;&#20110;&#39640;&#25928;&#30340;&#21477;&#23545;&#24314;&#27169;&#12290;MixEncoder&#21253;&#21547;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#32534;&#30721;&#26597;&#35810;&#26102;&#21482;&#36827;&#34892;&#19968;&#27425;&#65292;&#21516;&#26102;&#24182;&#34892;&#24314;&#27169;&#26597;&#35810;-&#20505;&#36873;&#20132;&#20114;&#12290;&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;MixEncoder&#21487;&#20197;&#23558;&#21477;&#23545;&#21305;&#37197;&#21152;&#36895;&#36229;&#36807;113&#20493;&#65292;&#21516;&#26102;&#23454;&#29616;&#19982;&#26356;&#26114;&#36149;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved great success on sentence pair modeling tasks, such as answer selection and natural language inference (NLI). These models generally perform cross-attention over input pairs, leading to prohibitive computational costs. Recent studies propose dual-encoder and late interaction architectures for faster computation. However, the balance between the expressive of cross-attention and computation speedup still needs better coordinated. To this end, this paper introduces a novel paradigm MixEncoder for efficient sentence pair modeling. MixEncoder involves a light-weight cross-attention mechanism. It conducts query encoding only once while modeling the query-candidate interaction in parallel. Extensive experiments conducted on four tasks demonstrate that our MixEncoder can speed up sentence pairing by over 113x while achieving comparable performance as the more expensive cross-attention models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#32763;&#35793;&#20026;&#21487;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;</title><link>http://arxiv.org/abs/2208.08374</link><description>&lt;p&gt;
&#20174;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#20013;&#32763;&#35793;&#25112;&#30053;&#24847;&#22270;&#30340;&#35745;&#31639;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
A Computational Interface to Translate Strategic Intent from Unstructured Language in a Low-Data Setting. (arXiv:2208.08374v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#25509;&#21475;&#65292;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#32763;&#35793;&#20026;&#21487;&#25191;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#28041;&#21450;&#21040;&#20154;&#31867;&#21644;AI&#31995;&#32479;&#21327;&#21516;&#23436;&#25104;&#20219;&#21153;&#30340;&#28151;&#21512;&#21457;&#36215;&#35774;&#23450;&#12290;&#23613;&#31649;&#22312;&#36890;&#36807;&#35821;&#35328;&#31934;&#30830;&#25351;&#23450;&#20195;&#29702;&#23436;&#25104;&#20219;&#21153;&#30340;&#20302;&#32423;&#35268;&#33539;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#24037;&#20316;&#65292;&#20294;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#35299;&#37322;&#20154;&#31867;&#25351;&#25381;&#23448;&#30340;&#39640;&#32423;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#32570;&#20047;&#12290;&#20174;&#35821;&#35328;&#20013;&#35299;&#26512;&#25112;&#30053;&#24847;&#22270;&#23558;&#20351;&#33258;&#20027;&#31995;&#32479;&#33021;&#22815;&#26681;&#25454;&#29992;&#25143;&#30340;&#35745;&#21010;&#29420;&#31435;&#36816;&#34892;&#65292;&#32780;&#26080;&#38656;&#39057;&#32321;&#30340;&#25351;&#23548;&#25110;&#25351;&#20196;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#33021;&#22815;&#23558;&#38750;&#32467;&#26500;&#21270;&#35821;&#35328;&#31574;&#30053;&#36716;&#21270;&#20026;&#21487;&#34892;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#24418;&#24335;&#30340;&#35745;&#31639;&#25509;&#21475;&#12290;&#21033;&#29992;&#19968;&#20010;&#28216;&#25103;&#29615;&#22659;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;1000&#22810;&#20010;&#31034;&#20363;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#35821;&#35328;&#31574;&#30053;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#30446;&#26631;&#21644;&#32422;&#26463;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25512;&#26029;&#25112;&#30053;&#24847;&#22270;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#20154;&#31867;&#35299;&#37322;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-world tasks involve a mixed-initiative setup, wherein humans and AI systems collaboratively perform a task. While significant work has been conducted towards enabling humans to specify, through language, exactly how an agent should complete a task (i.e., low-level specification), prior work lacks on interpreting the high-level strategic intent of the human commanders. Parsing strategic intent from language will allow autonomous systems to independently operate according to the user's plan without frequent guidance or instruction. In this paper, we build a computational interface capable of translating unstructured language strategies into actionable intent in the form of goals and constraints. Leveraging a game environment, we collect a dataset of over 1000 examples, mapping language strategies to the corresponding goals and constraints, and show that our model, trained on this dataset, significantly outperforms human interpreters in inferring strategic intent (i.e., goals an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2207.08012</link><description>&lt;p&gt;
&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.08012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#20803;&#21453;&#28216;&#25103;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#26469;&#25903;&#25345;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23637;&#31034;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21033;&#29992;&#32452;&#21512;&#24615;&#20174;&#36807;&#21435;&#30340;&#32463;&#39564;&#20013;&#25512;&#24191;&#21040;&#26032;&#39062;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#20551;&#35774;&#25105;&#20204;&#30340;&#32463;&#39564;&#21487;&#20197;&#20998;&#35299;&#20026;&#22522;&#26412;&#30340;&#21407;&#23376;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#20197;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#37325;&#26032;&#32452;&#21512;&#65292;&#20197;&#25903;&#25345;&#25105;&#20204;&#21442;&#19982;&#26032;&#39062;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#23398;&#20064;&#20197;&#32452;&#21512;&#26041;&#24335;&#27867;&#21270;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#30340;&#34892;&#20026;&#31216;&#20026;&#32452;&#21512;&#23398;&#20064;&#34892;&#20026;&#65288;CLBs&#65289;&#12290;&#23398;&#20064;CLBs&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#35299;&#20915;&#32465;&#23450;&#38382;&#39064;&#65288;BP&#65289;&#12290;&#23613;&#31649;&#36825;&#26159;&#20154;&#31867;&#36731;&#26494;&#23436;&#25104;&#30340;&#26234;&#33021;&#22766;&#20030;&#65292;&#20294;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#25105;&#20204;&#24314;&#35758;&#24320;&#21457;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#30740;&#31350;&#20195;&#29702;&#21830;&#36890;&#36807;&#35299;&#20915;BP&#30340;&#39046;&#22495;&#26080;&#20851;&#29256;&#26412;&#26469;&#23637;&#31034;CLBs&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21463;&#21040;&#25351;&#20195;&#28216;&#25103;&#30340;&#35821;&#35328;&#28044;&#29616;&#21644;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#25193;&#23637;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Human beings use compositionality to generalise from past experiences to novel experiences. We assume a separation of our experiences into fundamental atomic components that can be recombined in novel ways to support our ability to engage with novel experiences. We frame this as the ability to learn to generalise compositionally, and we will refer to behaviours making use of this ability as compositional learning behaviours (CLBs). A central problem to learning CLBs is the resolution of a binding problem (BP). While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents. Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#27604;&#29305;&#24065;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23558;&#30456;&#20851;&#36164;&#20135;&#12289;&#25216;&#26415;&#25351;&#26631;&#21644;Twitter&#20869;&#23481;&#20316;&#20026;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#37329;&#34701;&#35789;&#27719;&#34920;&#30340;&#21477;&#32423;FinBERT&#23884;&#20837;&#65292;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25512;&#25991;&#20013;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#27874;&#21160;&#12290;</title><link>http://arxiv.org/abs/2206.00648</link><description>&lt;p&gt;
PreBit -- &#19968;&#31181;&#21033;&#29992;Twitter FinBERT&#23884;&#20837;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#27604;&#29305;&#24065;&#30340;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
PreBit -- A multimodal model with Twitter FinBERT embeddings for extreme price movement prediction of Bitcoin. (arXiv:2206.00648v2 [q-fin.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#27604;&#29305;&#24065;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23558;&#30456;&#20851;&#36164;&#20135;&#12289;&#25216;&#26415;&#25351;&#26631;&#21644;Twitter&#20869;&#23481;&#20316;&#20026;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#37329;&#34701;&#35789;&#27719;&#34920;&#30340;&#21477;&#32423;FinBERT&#23884;&#20837;&#65292;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#25512;&#25991;&#20013;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#39044;&#27979;&#27604;&#29305;&#24065;&#30340;&#20215;&#26684;&#27874;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#29305;&#24065;&#20197;&#20854;&#19981;&#26029;&#22686;&#38271;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#23637;&#31034;&#20102;&#33258;&#20854;&#35806;&#29983;&#20197;&#26469;&#30340;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#24615;&#12290;&#36825;&#31181;&#27874;&#21160;&#24615;&#65292;&#21152;&#19978;&#20854;&#21435;&#20013;&#24515;&#21270;&#30340;&#24615;&#36136;&#65292;&#20351;&#27604;&#29305;&#24065;&#30456;&#23545;&#20110;&#26356;&#20256;&#32479;&#30340;&#36164;&#20135;&#26356;&#23481;&#26131;&#21463;&#21040;&#25237;&#26426;&#20132;&#26131;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#23558;&#21508;&#31181;&#30456;&#20851;&#36164;&#20135;&#12289;&#25216;&#26415;&#25351;&#26631;&#20197;&#21450;Twitter&#20869;&#23481;&#20316;&#20026;&#36755;&#20837;&#12290;&#22312;&#19968;&#39033;&#28145;&#20837;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26469;&#33258;&#22823;&#20247;&#23545;&#27604;&#29305;&#24065;&#30340;&#31038;&#20132;&#23186;&#20307;&#35752;&#35770;&#26159;&#21542;&#20855;&#26377;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20174;2015&#24180;&#21040;2021&#24180;&#27599;&#22825;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;&#27604;&#29305;&#24065;&#8221;&#30340;5,000&#26465;&#25512;&#25991;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;PreBit&#65292;&#24182;&#23558;&#20854;&#22312;&#32593;&#19978;&#25552;&#20379;&#12290;&#22312;&#25105;&#20204;&#30340;&#28151;&#21512;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22312;&#37329;&#34701;&#35789;&#27719;&#34920;&#19978;&#39044;&#35757;&#32451;&#30340;&#21477;&#32423;FinBERT&#23884;&#20837;&#65292;&#20197;&#20415;&#20197;&#21487;&#29702;&#35299;&#30340;&#26041;&#24335;&#25429;&#25417;&#25512;&#25991;&#30340;&#20840;&#37096;&#20869;&#23481;&#24182;&#23558;&#20854;&#25552;&#20379;&#32473;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#19968;&#31181;&#21367;&#31215;&#23618;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#21487;&#20197;&#25552;&#21462;&#25512;&#25991;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#24182;&#22312;&#27169;&#22411;&#20013;&#36827;&#34892;&#26497;&#31471;&#20215;&#26684;&#27874;&#21160;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bitcoin, with its ever-growing popularity, has demonstrated extreme price volatility since its origin. This volatility, together with its decentralised nature, make Bitcoin highly subjective to speculative trading as compared to more traditional assets. In this paper, we propose a multimodal model for predicting extreme price fluctuations. This model takes as input a variety of correlated assets, technical indicators, as well as Twitter content. In an in-depth study, we explore whether social media discussions from the general public on Bitcoin have predictive power for extreme price movements. A dataset of 5,000 tweets per day containing the keyword `Bitcoin' was collected from 2015 to 2021. This dataset, called PreBit, is made available online. In our hybrid model, we use sentence-level FinBERT embeddings, pretrained on financial lexicons, so as to capture the full contents of the tweets and feed it to the model in an understandable way. By combining these embeddings with a Convoluti
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2205.12422</link><description>&lt;p&gt;
&#38388;&#25509;&#36890;&#36807;&#20027;&#21160;&#31034;&#20363;&#20026;&#38750;&#31243;&#24207;&#21592;&#28155;&#21152;&#26631;&#31614;&#31243;&#24207;&#65306;&#20197;Text-to-SQL&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Labeling Programs with Non-Programmers Indirectly via Active Examples: A Case Study with Text-to-SQL. (arXiv:2205.12422v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12422
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;APEL&#26694;&#26550;&#65292;&#38750;&#31243;&#24207;&#21592;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#38388;&#25509;&#36873;&#25321;&#22797;&#26434;&#31243;&#24207;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#30340;&#27880;&#37322;&#65292;&#24182;&#19988;&#22312;&#37325;&#26032;&#27880;&#37322;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#26102;&#36798;&#21040;&#20102;&#19982;&#19987;&#23478;&#30456;&#21516;&#30340;&#20934;&#30830;&#24230;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#30340;&#32454;&#24494;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#31243;&#24207;&#21592;&#33021;&#21542;&#20351;&#29992;&#22797;&#26434;&#31243;&#24207;&#23545;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#36827;&#34892;&#27880;&#37322;&#20197;&#34920;&#31034;&#20854;&#21547;&#20041;&#65311;&#25105;&#20204;&#20171;&#32461;&#20102;APEL&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#38750;&#31243;&#24207;&#21592;&#20174;&#30001;&#31181;&#23376;&#35821;&#20041;&#35299;&#26512;&#22120;&#65288;&#20363;&#22914;Codex&#65289;&#29983;&#25104;&#30340;&#20505;&#36873;&#31243;&#24207;&#20013;&#36873;&#25321;&#12290;&#30001;&#20110;&#20182;&#20204;&#26080;&#27861;&#29702;&#35299;&#20505;&#36873;&#31243;&#24207;&#65292;&#25105;&#20204;&#35201;&#27714;&#20182;&#20204;&#36890;&#36807;&#26816;&#26597;&#31243;&#24207;&#30340;&#36755;&#20837;-&#36755;&#20986;&#31034;&#20363;&#26469;&#38388;&#25509;&#36873;&#25321;&#12290;&#23545;&#20110;&#27599;&#20010;&#34920;&#36798;&#24335;&#65292;APEL&#20027;&#21160;&#25628;&#32034;&#19968;&#20010;&#31616;&#21333;&#30340;&#36755;&#20837;&#65292;&#20505;&#36873;&#31243;&#24207;&#22312;&#36825;&#20010;&#36755;&#20837;&#19978;&#26356;&#20542;&#21521;&#20110;&#20135;&#29983;&#19981;&#21516;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;&#23427;&#21482;&#35201;&#27714;&#38750;&#31243;&#24207;&#21592;&#36873;&#25321;&#36866;&#24403;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#25512;&#26029;&#20986;&#21738;&#20010;&#31243;&#24207;&#26159;&#27491;&#30830;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#35843;&#20248;&#35299;&#26512;&#22120;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#25307;&#21215;&#20102;&#20154;&#31867;&#38750;&#31243;&#24207;&#21592;&#20351;&#29992;APEL&#37325;&#26032;&#27880;&#37322;SPIDER&#65292;&#19968;&#20010;&#25991;&#26412;&#21040;SQL&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#19982;&#21407;&#22987;&#19987;&#23478;&#27880;&#37322;&#32773;&#30456;&#21516;&#30340;&#27880;&#37322;&#20934;&#30830;&#24230;&#65288;75%&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#21407;&#22987;&#27880;&#37322;&#20013;&#35768;&#22810;&#24494;&#23567;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can non-programmers annotate natural language utterances with complex programs that represent their meaning? We introduce APEL, a framework in which non-programmers select among candidate programs generated by a seed semantic parser (e.g., Codex). Since they cannot understand the candidate programs, we ask them to select indirectly by examining the programs' input-ouput examples. For each utterance, APEL actively searches for a simple input on which the candidate programs tend to produce different outputs. It then asks the non-programmers only to choose the appropriate output, thus allowing us to infer which program is correct and could be used to fine-tune the parser. As a first case study, we recruited human non-programmers to use APEL to re-annotate SPIDER, a text-to-SQL dataset. Our approach achieved the same annotation accuracy as the original expert annotators (75%) and exposed many subtle errors in the original annotations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26469;&#33258;&#36755;&#20837;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.01845</link><description>&lt;p&gt;
&#24102;&#26377;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#20027;&#39064;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds. (arXiv:2205.01845v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.01845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#26469;&#33258;&#36755;&#20837;&#35821;&#26009;&#24211;&#30340;&#23616;&#37096;&#35821;&#20041;&#30456;&#32467;&#21512;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#21457;&#29616;&#28508;&#22312;&#20027;&#39064;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#35838;&#39064;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35774;&#32622;&#65292;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#21033;&#29992;&#29992;&#25143;&#25351;&#23548;&#65292;&#25152;&#20197;&#23427;&#20204;&#21457;&#29616;&#30340;&#20027;&#39064;&#21487;&#33021;&#19981;&#31526;&#21512;&#29992;&#25143;&#30340;&#29305;&#23450;&#20852;&#36259;&#12290;&#34429;&#28982;&#23384;&#22312;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#31181;&#23376;&#35789;&#26469;&#21457;&#29616;&#20027;&#39064;&#20195;&#34920;&#35789;&#30340;&#31181;&#23376;&#24341;&#23548;&#20027;&#39064;&#21457;&#29616;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36739;&#23569;&#20851;&#27880;&#20004;&#20010;&#22240;&#32032;&#65306;(1)&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#30340;&#23384;&#22312;&#21644;(2)&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#31181;&#23376;&#24341;&#23548;&#20027;&#39064;&#21457;&#29616;&#30340;&#20219;&#21153;&#25512;&#24191;&#21040;&#20801;&#35768;&#26410;&#30331;&#24405;&#35789;&#31181;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;SeeTopic&#65292;&#22312;&#20854;&#20013;PLM&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#20174;&#36755;&#20837;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#30340;&#23616;&#37096;&#35821;&#20041;&#21487;&#20197;&#30456;&#20114;&#21463;&#30410;&#12290;&#22312;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#19977;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;SeeTopic&#22312;&#20027;&#39064;&#36830;&#36143;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Discovering latent topics from text corpora has been studied for decades. Many existing topic models adopt a fully unsupervised setting, and their discovered topics may not cater to users' particular interests due to their inability of leveraging user guidance. Although there exist seed-guided topic discovery approaches that leverage user-provided seeds to discover topic-representative terms, they are less concerned with two factors: (1) the existence of out-of-vocabulary seeds and (2) the power of pre-trained language models (PLMs). In this paper, we generalize the task of seed-guided topic discovery to allow out-of-vocabulary seeds. We propose a novel framework, named SeeTopic, wherein the general knowledge of PLMs and the local semantics learned from the input corpus can mutually benefit each other. Experiments on three real datasets from different domains demonstrate the effectiveness of SeeTopic in terms of topic coherence, accuracy, and diversity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#20013;&#25991;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#12289;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;28&#20159;&#21442;&#25968;&#30340;EVA2.0&#27169;&#22411;&#65292;&#20854;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#35752;&#35770;&#20102;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2203.09313</link><description>&lt;p&gt;
EVA2.0&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30740;&#31350;&#24320;&#25918;&#39046;&#22495;&#30340;&#20013;&#25991;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EVA2.0: Investigating Open-Domain Chinese Dialogue Systems with Large-Scale Pre-Training. (arXiv:2203.09313v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#24320;&#25918;&#39046;&#22495;&#20013;&#25991;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#12289;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#20855;&#26377;28&#20159;&#21442;&#25968;&#30340;EVA2.0&#27169;&#22411;&#65292;&#20854;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#36824;&#35752;&#35770;&#20102;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#22312;&#26500;&#24314;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21457;&#24067;&#30340;&#23545;&#35805;&#27169;&#22411;&#30340;&#23545;&#35805;&#24615;&#33021;&#23637;&#31034;&#21644;&#35780;&#20272;&#65292;&#24573;&#35270;&#20102;&#19968;&#20123;&#20851;&#38190;&#22240;&#32032;&#23545;&#20110;&#24378;&#22823;&#30340;&#31867;&#20154;&#32842;&#22825;&#26426;&#22120;&#20154;&#29305;&#21035;&#22312;&#20013;&#25991;&#22330;&#26223;&#19979;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#26410;&#26366;&#25506;&#32034;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#25968;&#25454;&#36136;&#37327;&#25511;&#21046;&#12289;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#12289;&#35757;&#32451;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;EVA2.0&#65292;&#19968;&#20010;&#20855;&#26377;28&#20159;&#21442;&#25968;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#24320;&#25918;&#39046;&#22495;&#20013;&#25991;&#23545;&#35805;&#27169;&#22411;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;EVA2.0&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#24320;&#28304;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#35752;&#35770;&#20102;&#35813;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22823;&#35268;&#27169;&#20013;&#25991;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-training has shown remarkable performance in building open-domain dialogue systems. However, previous works mainly focus on showing and evaluating the conversational performance of the released dialogue model, ignoring the discussion of some key factors towards a powerful human-like chatbot, especially in Chinese scenarios. In this paper, we conduct extensive experiments to investigate these under-explored factors, including data quality control, model architecture designs, training approaches, and decoding strategies. We propose EVA2.0, a large-scale pre-trained open-domain Chinese dialogue model with 2.8 billion parameters, and will make our models and codes publicly available. Automatic and human evaluations show that EVA2.0 significantly outperforms other open-source counterparts. We also discuss the limitations of this work by presenting some failure cases and pose some future research directions on large-scale Chinese open-domain dialogue systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;MICoL&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2202.05932</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Metadata-Induced Contrastive Learning for Zero-Shot Multi-Label Text Classification. (arXiv:2202.05932v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.05932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65288;MICoL&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#25991;&#26723;&#19982;&#20854;&#30456;&#20851;&#26631;&#31614;&#20851;&#32852;&#36215;&#26469;&#65292;&#24182;&#20174;&#19968;&#20010;&#22823;&#30340;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#38590;&#20197;&#33719;&#21462;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#38598;&#20013;&#23384;&#22312;&#38271;&#23614;&#26631;&#31614;&#20998;&#24067;&#65288;&#21363;&#35768;&#22810;&#26631;&#31614;&#21482;&#20986;&#29616;&#20960;&#27425;&#65289;&#12290;&#26412;&#25991;&#30740;&#31350;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#22823;&#35268;&#27169;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#65292;&#36825;&#19981;&#38656;&#35201;&#20219;&#20309;&#24102;&#26377;&#26631;&#31614;&#30340;&#27880;&#37322;&#25991;&#26723;&#65292;&#21482;&#20381;&#36182;&#20110;&#26631;&#31614;&#30340;&#34920;&#38754;&#21517;&#31216;&#21644;&#25551;&#36848;&#12290;&#20026;&#20102;&#35757;&#32451;&#19968;&#20010;&#35745;&#31639;&#25991;&#26723;&#19982;&#26631;&#31614;&#20043;&#38388;&#30456;&#20284;&#24230;&#24471;&#20998;&#30340;&#20998;&#31867;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#25968;&#25454;&#24341;&#23548;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;MICoL&#65289;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#22522;&#20110;&#25991;&#26412;&#30340;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#19981;&#21516;&#65292;MICoL&#21033;&#29992;&#20102;&#24191;&#27867;&#21487;&#29992;&#20110;Web&#19978;&#30340;&#25991;&#26723;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#20316;&#32773;&#65292;&#20250;&#35758;&#65292;&#30740;&#31350;&#35770;&#25991;&#30340;&#24341;&#29992;&#65289;&#26469;&#25512;&#23548;&#20986;&#30456;&#20284;&#30340;&#25991;&#26723;&#23545;&#12290;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;
&lt;/p&gt;
&lt;p&gt;
Large-scale multi-label text classification (LMTC) aims to associate a document with its relevant labels from a large candidate set. Most existing LMTC approaches rely on massive human-annotated training data, which are often costly to obtain and suffer from a long-tailed label distribution (i.e., many labels occur only a few times in the training set). In this paper, we study LMTC under the zero-shot setting, which does not require any annotated documents with labels and only relies on label surface names and descriptions. To train a classifier that calculates the similarity score between a document and a label, we propose a novel metadata-induced contrastive learning (MICoL) method. Different from previous text-based contrastive learning techniques, MICoL exploits document metadata (e.g., authors, venues, and references of research papers), which are widely available on the Web, to derive similar document-document pairs. Experimental results on two large-scale datasets show that: (1)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ENGINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#20307;&#24863;&#30693;&#30340;&#25991;&#31456;&#29983;&#25104;&#21644;&#26816;&#32034;&#12290;&#26694;&#26550;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#27169;&#22359;&#21644;&#23454;&#20307;&#24863;&#30693;&#26426;&#21046;&#65292;&#23558;&#21629;&#21517;&#23454;&#20307;&#26126;&#30830;&#22320;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#31456;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2112.05917</link><description>&lt;p&gt;
&#23637;&#31034;&#12289;&#20889;&#20316;&#21644;&#26816;&#32034;: &#23454;&#20307;&#24863;&#30693;&#30340;&#25991;&#31456;&#29983;&#25104;&#21644;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Show, Write, and Retrieve: Entity-aware Article Generation and Retrieval. (arXiv:2112.05917v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.05917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ENGINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#20307;&#24863;&#30693;&#30340;&#25991;&#31456;&#29983;&#25104;&#21644;&#26816;&#32034;&#12290;&#26694;&#26550;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#27169;&#22359;&#21644;&#23454;&#20307;&#24863;&#30693;&#26426;&#21046;&#65292;&#23558;&#21629;&#21517;&#23454;&#20307;&#26126;&#30830;&#22320;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#31456;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#29702;&#35299;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#65292;&#20363;&#22914;&#25991;&#31456;&#29983;&#25104;&#25110;&#22270;&#20687;&#21040;&#25991;&#31456;&#30340;&#26816;&#32034;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#25991;&#31456;&#20013;&#30340;&#25152;&#26377;&#26631;&#35760;&#36827;&#34892;&#32479;&#19968;&#32534;&#30721;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#22914;&#29702;&#35299;&#26032;&#38395;&#25253;&#36947;&#65292;&#36825;&#20123;&#25991;&#31456;&#37117;&#22522;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#20107;&#20214;&#65292;&#21487;&#33021;&#24341;&#29992;&#20102;&#35768;&#22810;&#38590;&#20197;&#20934;&#30830;&#35782;&#21035;&#21644;&#39044;&#27979;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;ENtity-aware article GeneratIoN and rEtrieval (ENGINE)&#26694;&#26550;&#65292;&#23558;&#21629;&#21517;&#23454;&#20307;&#26126;&#30830;&#22320;&#34701;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;ENGINE&#26694;&#26550;&#26377;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#21629;&#21517;&#23454;&#20307;&#25552;&#21462;&#27169;&#22359;&#65292;&#29992;&#20110;&#20174;&#25991;&#31456;&#30340;&#20803;&#25968;&#25454;&#21644;&#23884;&#20837;&#22270;&#20687;&#20013;&#25552;&#21462;&#21629;&#21517;&#23454;&#20307;&#65307;&#19968;&#20010;&#23454;&#20307;&#24863;&#30693;&#26426;&#21046;&#65292;&#22686;&#24378;&#27169;&#22411;&#35782;&#21035;&#21644;&#39044;&#27979;&#23454;&#20307;&#21517;&#31216;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;GoodNews&#12289;VisualNews&#21644;WikiText&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Article comprehension is an important challenge in natural language processing with many applications such as article generation or image-to-article retrieval. Prior work typically encodes all tokens in articles uniformly using pretrained language models. However, in many applications, such as understanding news stories, these articles are based on real-world events and may reference many named entities that are difficult to accurately recognize and predict by language models. To address this challenge, we propose an ENtity-aware article GeneratIoN and rEtrieval (ENGINE) framework, to explicitly incorporate named entities into language models. ENGINE has two main components: a named-entity extraction module to extract named entities from both metadata and embedded images associated with articles, and an entity-aware mechanism that enhances the model's ability to recognize and predict entity names. We conducted experiments on three public datasets: GoodNews, VisualNews, and WikiText, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RATE&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25972;&#21512;&#20854;&#20182;&#29305;&#24449;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#23454;&#26102;&#20301;&#32622;&#20272;&#35745;&#20013;&#30340;&#25991;&#26412;&#29305;&#24449;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2111.06515</link><description>&lt;p&gt;
RATE: &#20811;&#26381;&#23454;&#26102;&#20301;&#32622;&#20272;&#35745;&#20013;&#25991;&#26412;&#29305;&#24449;&#30340;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
RATE: Overcoming Noise and Sparsity of Textual Features in Real-Time Location Estimation. (arXiv:2111.06515v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RATE&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#25972;&#21512;&#20854;&#20182;&#29305;&#24449;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#23454;&#26102;&#20301;&#32622;&#20272;&#35745;&#20013;&#30340;&#25991;&#26412;&#29305;&#24449;&#22122;&#22768;&#21644;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#30340;&#23454;&#26102;&#20301;&#32622;&#25512;&#26029;&#26159;&#19968;&#20123;&#31354;&#38388;&#24212;&#29992;&#65288;&#22914;&#26412;&#22320;&#25628;&#32034;&#21644;&#20107;&#20214;&#26816;&#27979;&#65289;&#30340;&#22522;&#30784;&#12290;&#34429;&#28982;&#25512;&#25991;&#25991;&#26412;&#26159;&#20301;&#32622;&#20272;&#35745;&#20013;&#26368;&#24120;&#29992;&#30340;&#29305;&#24449;&#65292;&#20294;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#21463;&#21040;&#25991;&#26412;&#29305;&#24449;&#22122;&#22768;&#25110;&#31232;&#30095;&#24615;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#20316;&#20026;&#26500;&#24314;&#27169;&#22359;&#65292;&#20197;&#34920;&#24449;&#22320;&#29702;&#20027;&#39064;&#21464;&#21270;&#21644;&#35789;&#27719;&#21464;&#21270;&#65292;&#20174;&#32780;&#19981;&#20877;&#30452;&#25509;&#20351;&#29992;&#8220;one-hot&#8221;&#32534;&#30721;&#21521;&#37327;&#12290;&#25105;&#20204;&#36824;&#32467;&#21512;&#36890;&#36807;Twitter&#27969;API&#25552;&#21462;&#30340;&#20854;&#20182;&#29305;&#24449;&#26469;&#20811;&#26381;&#22122;&#22768;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;RATE&#31639;&#27861;&#22312;&#22320;&#21306;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#21644;&#32428;&#24230;&#32463;&#24230;&#22238;&#24402;&#30340;&#24179;&#22343;&#36317;&#31163;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#20960;&#31181;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time location inference of social media users is the fundamental of some spatial applications such as localized search and event detection. While tweet text is the most commonly used feature in location estimation, most of the prior works suffer from either the noise or the sparsity of textual features. In this paper, we aim to tackle these two problems. We use topic modeling as a building block to characterize the geographic topic variation and lexical variation so that "one-hot" encoding vectors will no longer be directly used. We also incorporate other features which can be extracted through the Twitter streaming API to overcome the noise problem. Experimental results show that our RATE algorithm outperforms several benchmark methods, both in the precision of region classification and the mean distance error of latitude and longitude regression.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MotifClass&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#39640;&#38454;&#20803;&#25968;&#25454;&#20449;&#24687;&#26469;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#27169;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#22270;&#26696;&#25551;&#36848;&#20803;&#25968;&#25454;&#32452;&#21512;&#20197;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#31867;&#21035;&#25351;&#31034;&#24847;&#20041;&#30340;&#22270;&#26696;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2111.04022</link><description>&lt;p&gt;
MotifClass: &#22522;&#20110;&#39640;&#38454;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MotifClass: Weakly Supervised Text Classification with Higher-order Metadata Information. (arXiv:2111.04022v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.04022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MotifClass&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#39640;&#38454;&#20803;&#25968;&#25454;&#20449;&#24687;&#26469;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#24314;&#27169;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21033;&#29992;&#22270;&#26696;&#25551;&#36848;&#20803;&#25968;&#25454;&#32452;&#21512;&#20197;&#25429;&#25417;&#39640;&#38454;&#32467;&#26500;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#31867;&#21035;&#25351;&#31034;&#24847;&#20041;&#30340;&#22270;&#26696;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65292;&#26088;&#22312;&#23558;&#25991;&#26412;&#25991;&#26723;&#20998;&#31867;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#31867;&#21035;&#65292;&#20165;&#20351;&#29992;&#31867;&#21035;&#34920;&#38754;&#21517;&#31216;&#65292;&#32780;&#26080;&#38656;&#25552;&#20379;&#20219;&#20309;&#26631;&#27880;&#30340;&#35757;&#32451;&#25991;&#26723;&#12290;&#30446;&#21069;&#22823;&#37096;&#20998;&#29616;&#26377;&#20998;&#31867;&#22120;&#21033;&#29992;&#27599;&#20010;&#25991;&#26723;&#20013;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#25991;&#26723;&#38468;&#24102;&#21508;&#31181;&#31867;&#22411;&#30340;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#20316;&#32773;&#12289;&#20986;&#22788;&#21644;&#30740;&#31350;&#35770;&#25991;&#30340;&#24180;&#20221;&#65289;&#12290;&#36825;&#20123;&#20803;&#25968;&#25454;&#21450;&#20854;&#32452;&#21512;&#21487;&#33021;&#20316;&#20026;&#24378;&#26377;&#21147;&#30340;&#31867;&#21035;&#25351;&#26631;&#65292;&#29992;&#20110;&#36741;&#21161;&#25991;&#26412;&#20998;&#31867;&#12290;&#26412;&#25991;&#36890;&#36807;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#24314;&#27169;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#22270;&#26696;&#25551;&#36848;&#20803;&#25968;&#25454;&#32452;&#21512;&#20197;&#26377;&#25928;&#25429;&#25417;&#32593;&#32476;&#20013;&#30340;&#39640;&#38454;&#32467;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MotifClass&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#65288;1&#65289;&#36873;&#25321;&#20855;&#26377;&#31867;&#21035;&#25351;&#31034;&#24847;&#20041;&#30340;&#22270;&#26696;&#23454;&#20363;&#65292;&#65288;2&#65289;&#26816;&#32034;&#21644;&#29983;&#25104;&#20803;&#25968;&#25454;&#30340;&#34920;&#31034;&#20197;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of weakly supervised text classification, which aims to classify text documents into a set of pre-defined categories with category surface names only and without any annotated training document provided. Most existing classifiers leverage textual information in each document. However, in many domains, documents are accompanied by various types of metadata (e.g., authors, venue, and year of a research paper). These metadata and their combinations may serve as strong category indicators in addition to textual contents. In this paper, we explore the potential of using metadata to help weakly supervised text classification. To be specific, we model the relationships between documents and metadata via a heterogeneous information network. To effectively capture higher-order structures in the network, we use motifs to describe metadata combinations. We propose a novel framework, named MotifClass, which (1) selects category-indicative motif instances, (2) retrieves and gen
&lt;/p&gt;</description></item><item><title>C-PCFGs&#22312;&#26080;&#30417;&#30563;&#30701;&#35821;&#32467;&#26500;&#35821;&#27861;&#24402;&#32435;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#25968;&#25454;&#26174;&#31034;&#65292;C-PCFGs&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#33021;&#36866;&#24212;&#19981;&#21516;&#21477;&#23376;&#38271;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#32423;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#26368;&#20339;&#37197;&#32622;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2103.02298</link><description>&lt;p&gt;
&#22797;&#21512;&#27010;&#29575;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Compound PCFGs. (arXiv:2103.02298v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.02298
&lt;/p&gt;
&lt;p&gt;
C-PCFGs&#22312;&#26080;&#30417;&#30563;&#30701;&#35821;&#32467;&#26500;&#35821;&#27861;&#24402;&#32435;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#25968;&#25454;&#26174;&#31034;&#65292;C-PCFGs&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#33021;&#36866;&#24212;&#19981;&#21516;&#21477;&#23376;&#38271;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#32423;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#26368;&#20339;&#37197;&#32622;&#19981;&#19968;&#23450;&#36866;&#29992;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#21512;&#27010;&#29575;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861; (C-PCFGs) &#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#30701;&#35821;&#32467;&#26500;&#35821;&#27861;&#24402;&#32435;&#26041;&#38754;&#21462;&#24471;&#20102;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22522;&#20110;&#22270;&#34920;&#30340;&#34920;&#31034;&#21644;&#25512;&#29702;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#36739;&#39640;&#65292;&#23545;C-PCFGs&#36827;&#34892;&#20840;&#38754;&#30340;&#30740;&#31350;&#23384;&#22312;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#24555;&#36895;&#30340; C-PCFGs &#23454;&#29616;&#65292;&#23545;\citet{kim-etal-2019-compound}&#30340;&#35780;&#20272;&#36827;&#34892;&#20102;&#34917;&#20805;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#33521;&#25991;&#26641;&#24211;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#20462;&#21098;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1)C-PCFGs&#20855;&#26377;&#39640;&#25928;&#21033;&#29992;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#22312;&#22788;&#29702;&#26410;&#35265;&#36807;&#30340;&#21477;&#23376;/&#25104;&#20998;&#38271;&#24230;&#26102;&#20063;&#33021;&#36827;&#34892;&#27867;&#21270;&#65307;(2)C-PCFGs&#22312;&#29983;&#25104;&#32456;&#32467;&#31526;&#35268;&#21017;&#27010;&#29575;&#26102;&#20805;&#20998;&#21033;&#29992;&#20102;&#21477;&#23376;&#32423;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#23545;C-PCFGs&#36827;&#34892;&#20102;&#22810;&#35821;&#35328;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#32463;&#36807;&#22312;&#33521;&#25991;&#19978;&#35843;&#35797;&#30340;C-PCFGs&#30340;&#26368;&#20339;&#37197;&#32622;&#24182;&#19981;&#24635;&#33021;&#36866;&#29992;&#20110;&#24418;&#24577;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compound probabilistic context-free grammars (C-PCFGs) have recently established a new state of the art for unsupervised phrase-structure grammar induction. However, due to the high space and time complexities of chart-based representation and inference, it is difficult to investigate C-PCFGs comprehensively. In this work, we rely on a fast implementation of C-PCFGs to conduct an evaluation complementary to that of~\citet{kim-etal-2019-compound}. We start by analyzing and ablating C-PCFGs on English treebanks. Our findings suggest that (1) C-PCFGs are data-efficient and can generalize to unseen sentence/constituent lengths; and (2) C-PCFGs make the best use of sentence-level information in generating preterminal rule probabilities. We further conduct a multilingual evaluation of C-PCFGs. The experimental results show that the best configurations of C-PCFGs, which are tuned on English, do not always generalize to morphology-rich languages.
&lt;/p&gt;</description></item><item><title>MATCH&#26159;&#19968;&#20010;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#20803;&#25968;&#25454;&#21644;&#23618;&#27425;&#20449;&#24687;&#12290;&#23427;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#24335;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.07349</link><description>&lt;p&gt;
MATCH: &#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MATCH: Metadata-Aware Text Classification in A Large Hierarchy. (arXiv:2102.07349v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.07349
&lt;/p&gt;
&lt;p&gt;
MATCH&#26159;&#19968;&#20010;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#23618;&#27425;&#25991;&#26412;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#20102;&#20803;&#25968;&#25454;&#21644;&#23618;&#27425;&#20449;&#24687;&#12290;&#23427;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#36890;&#36807;&#19981;&#21516;&#30340;&#27491;&#21017;&#21270;&#26041;&#24335;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26159;&#25351;&#23558;&#27599;&#20010;&#32473;&#23450;&#30340;&#25991;&#26723;&#20998;&#37197;&#32473;&#19982;&#26631;&#31614;&#38598;&#26368;&#30456;&#20851;&#30340;&#26631;&#31614;&#30340;&#38382;&#39064;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#25552;&#20379;&#32473;&#23450;&#25991;&#26723;&#30340;&#20803;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#23618;&#27425;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#21482;&#20851;&#27880;&#24314;&#27169;&#25991;&#26412;&#20449;&#24687;&#65292;&#23545;&#20110;&#21033;&#29992;&#20803;&#25968;&#25454;&#25110;&#23618;&#27425;&#32467;&#26500;&#20449;&#21495;&#30340;&#23581;&#35797;&#36739;&#23569;&#65292;&#24182;&#38750;&#20004;&#32773;&#20860;&#39038;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35268;&#33539;&#21270;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#22823;&#22411;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#19979;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#25968;&#19975;&#20010;&#26631;&#31614;&#65289;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MATCH&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#19968;&#20010;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#20803;&#25968;&#25454;&#21644;&#23618;&#27425;&#20449;&#24687;&#12290;&#20026;&#20102;&#32467;&#21512;&#20803;&#25968;&#25454;&#65292;&#25105;&#20204;&#22312;&#30456;&#21516;&#31354;&#38388;&#20013;&#39044;&#35757;&#32451;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#30340;&#23884;&#20837;&#65292;&#24182;&#21033;&#29992;&#20840;&#36830;&#25509;&#27880;&#24847;&#21147;&#26469;&#25429;&#25417;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20026;&#20102;&#21033;&#29992;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#24335;&#26469;&#27491;&#21017;&#21270;&#21442;&#25968;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification refers to the problem of assigning each given document its most relevant labels from the label set. Commonly, the metadata of the given documents and the hierarchy of the labels are available in real-world applications. However, most existing studies focus on only modeling the text information, with a few attempts to utilize either metadata or hierarchy signals, but not both of them. In this paper, we bridge the gap by formalizing the problem of metadata-aware text classification in a large label hierarchy (e.g., with tens of thousands of labels). To address this problem, we present the MATCH solution -- an end-to-end framework that leverages both metadata and hierarchy information. To incorporate metadata, we pre-train the embeddings of text and metadata in the same space and also leverage the fully-connected attentions to capture the interrelations between them. To leverage the label hierarchy, we propose different ways to regularize the parameters and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#25972;&#21512;&#36215;&#26469;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;HiMeCat&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#22312;&#21482;&#26377;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#25991;&#26723;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2010.13556</link><description>&lt;p&gt;
&#20998;&#23618;&#20803;&#25968;&#25454;&#24863;&#30693;&#30340;&#24369;&#30417;&#30563;&#19979;&#25991;&#26723;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Metadata-Aware Document Categorization under Weak Supervision. (arXiv:2010.13556v2 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.13556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#23558;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#25972;&#21512;&#36215;&#26469;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;HiMeCat&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#22312;&#21482;&#26377;&#23569;&#37327;&#35757;&#32451;&#26679;&#26412;&#21644;&#20803;&#25968;&#25454;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39640;&#25928;&#25991;&#26723;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26222;&#36941;&#23384;&#22312;&#23618;&#27425;&#21270;&#20027;&#39064;&#32467;&#26500;&#65292;&#23558;&#25991;&#26723;&#20998;&#31867;&#21040;&#32473;&#23450;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20013;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#12290;&#23613;&#31649;&#30456;&#20851;&#30740;&#31350;&#22312;&#23436;&#20840;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26723;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21482;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#65288;1&#65289;&#26631;&#27880;&#38750;&#24120;&#26114;&#36149;&#65292;&#38590;&#20197;&#33719;&#21462;&#21040;&#24456;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#65307;&#65288;2&#65289;&#25991;&#26723;&#38468;&#24102;&#20803;&#25968;&#25454;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#24369;&#30417;&#30563;&#19979;&#25972;&#21512;&#26631;&#31614;&#23618;&#27425;&#12289;&#20803;&#25968;&#25454;&#21644;&#25991;&#26412;&#20449;&#21495;&#36827;&#34892;&#25991;&#26723;&#20998;&#31867;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;HiMeCat&#65292;&#19968;&#20010;&#22522;&#20110;&#23884;&#20837;&#24335;&#29983;&#25104;&#26694;&#26550;&#65292;&#29992;&#20110;&#25105;&#20204;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#21512;&#34920;&#31034;&#23398;&#20064;&#27169;&#22359;&#65292;&#21487;&#20197;&#21516;&#26102;&#24314;&#27169;&#31867;&#21035;&#20381;&#36182;&#20851;&#31995;&#12289;&#20803;&#25968;&#25454;&#20449;&#24687;&#21644;&#25991;&#26412;&#35821;&#20041;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#27169;&#22359;&#65292;&#29992;&#20110;&#20998;&#23618;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Categorizing documents into a given label hierarchy is intuitively appealing due to the ubiquity of hierarchical topic structures in massive text corpora. Although related studies have achieved satisfying performance in fully supervised hierarchical document classification, they usually require massive human-annotated training data and only utilize text information. However, in many domains, (1) annotations are quite expensive where very few training samples can be acquired; (2) documents are accompanied by metadata information. Hence, this paper studies how to integrate the label hierarchy, metadata, and text signals for document categorization under weak supervision. We develop HiMeCat, an embedding-based generative framework for our task. Specifically, we propose a novel joint representation learning module that allows simultaneous modeling of category dependencies, metadata information and textual semantics, and we introduce a data augmentation module that hierarchically synthesize
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2005.00624</link><description>&lt;p&gt;
&#25991;&#26412;&#19982;&#20803;&#25968;&#25454;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Minimally Supervised Categorization of Text with Metadata. (arXiv:2005.00624v3 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.00624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#23454;&#29616;&#20102;&#22312;&#26631;&#31614;&#31232;&#32570;&#24773;&#20917;&#19979;&#30340;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20998;&#31867;&#26159;&#23558;&#20027;&#39064;&#26631;&#31614;&#20998;&#37197;&#32473;&#27599;&#20010;&#25991;&#26723;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20256;&#32479;&#30417;&#30563;&#25991;&#26723;&#20998;&#31867;&#30740;&#31350;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36739;&#23569;&#20851;&#27880;&#20004;&#20010;&#23454;&#38469;&#38382;&#39064;&#65306;&#65288;1&#65289;&#23384;&#22312;&#20803;&#25968;&#25454;&#65306;&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#25991;&#26412;&#20276;&#38543;&#30528;&#21508;&#31181;&#38468;&#21152;&#20449;&#24687;&#65292;&#20363;&#22914;&#20316;&#32773;&#21644;&#26631;&#31614;&#12290;&#36825;&#20123;&#20803;&#25968;&#25454;&#20316;&#20026;&#26377;&#21147;&#30340;&#20027;&#39064;&#25351;&#31034;&#22120;&#65292;&#24212;&#35813;&#34987;&#21033;&#29992;&#21040;&#20998;&#31867;&#26694;&#26550;&#20013;&#65307;&#65288;2&#65289;&#26631;&#31614;&#31232;&#32570;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26377;&#26631;&#31614;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#26114;&#36149;&#30340;&#65292;&#38656;&#35201;&#21482;&#20351;&#29992;&#23569;&#37327;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20998;&#31867;&#12290;&#37492;&#20110;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MetaCat&#65292;&#19968;&#31181;&#20351;&#29992;&#20803;&#25968;&#25454;&#36827;&#34892;&#26368;&#23567;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#26469;&#25551;&#36848;&#21333;&#35789;&#12289;&#25991;&#26723;&#12289;&#26631;&#31614;&#21644;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26681;&#25454;&#29983;&#25104;&#27169;&#22411;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#21644;&#20803;&#25968;&#25454;&#23884;&#20837;&#21040;&#20998;&#31867;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document categorization, which aims to assign a topic label to each document, plays a fundamental role in a wide variety of applications. Despite the success of existing studies in conventional supervised document classification, they are less concerned with two real problems: (1) the presence of metadata: in many domains, text is accompanied by various additional information such as authors and tags. Such metadata serve as compelling topic indicators and should be leveraged into the categorization framework; (2) label scarcity: labeled training samples are expensive to obtain in some cases, where categorization needs to be performed using only a small set of annotated data. In recognition of these two challenges, we propose MetaCat, a minimally supervised framework to categorize text with metadata. Specifically, we develop a generative process describing the relationships between words, documents, labels, and metadata. Guided by the generative model, we embed text and metadata into th
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/1912.05957</link><description>&lt;p&gt;
&#25991;&#26412;&#20316;&#20026;&#29615;&#22659;:&#19968;&#31181;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text as Environment: A Deep Reinforcement Learning Text Readability Assessment Model. (arXiv:1912.05957v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1912.05957
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#21644;&#21322;&#30417;&#30563;&#20449;&#21495;&#26469;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#21487;&#20197;&#26174;&#33879;&#20419;&#36827;&#20449;&#24687;&#30340;&#20934;&#30830;&#34920;&#36798;&#12290;&#25991;&#26412;&#21487;&#35835;&#24615;&#35780;&#20272;&#30340;&#21046;&#23450;&#28041;&#21450;&#23545;&#25991;&#26412;&#30340;&#26377;&#24847;&#20041;&#30340;&#23646;&#24615;&#36827;&#34892;&#35782;&#21035;&#65292;&#32780;&#19981;&#35770;&#20854;&#38271;&#24230;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#25991;&#26412;&#30340;&#21487;&#29702;&#35299;&#24615;&#65292;&#20351;&#29992;&#20102;&#22797;&#26434;&#30340;&#29305;&#24449;&#21644;&#27169;&#22411;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#39640;&#25928;&#35780;&#20272;&#25991;&#26412;&#21487;&#35835;&#24615;&#30340;&#38382;&#39064;&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#36890;&#36807;&#20351;&#29992;&#30828;&#27880;&#24847;&#21147;&#30340;&#20027;&#21160;&#25512;&#29702;&#25216;&#26415;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#36755;&#20837;&#25991;&#26412;&#21644;&#35745;&#31639;&#36164;&#28304;&#12290;&#36890;&#36807;&#20351;&#29992;&#21322;&#30417;&#30563;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#26368;&#23569;&#30340;&#25991;&#26412;&#26469;&#30830;&#23450;&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#12290;&#23558;&#35813;&#27169;&#22411;&#19982;Weebit&#21644;&#21073;&#26725;&#32771;&#35797;&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the readability of a text can significantly facilitate the precise expression of information in written form. The formulation of text readability assessment involves the identification of meaningful properties of the text regardless of its length. Sophisticated features and models are used to evaluate the comprehensibility of texts accurately. Despite this, the problem of assessing texts' readability efficiently remains relatively untouched. The efficiency of state-of-the-art text readability assessment models can be further improved using deep reinforcement learning models. Using a hard attention-based active inference technique, the proposed approach makes efficient use of input text and computational resources. Through the use of semi-supervised signals, the reinforcement learning model uses the minimum amount of text in order to determine text's readability. A comparison of the model on Weebit and Cambridge Exams with state-of-the-art models, such as the BERT text readab
&lt;/p&gt;</description></item><item><title>HiGitClass&#26159;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;GitHub&#20179;&#24211;&#30340;&#20998;&#23618;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#24102;&#26377;&#20851;&#38190;&#35789;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#30417;&#30563;&#12290;&#26694;&#26550;&#35299;&#20915;&#20102;&#22810;&#27169;&#24335;&#20449;&#21495;&#12289;&#30417;&#30563;&#31232;&#32570;&#24615;&#21644;&#30417;&#30563;&#26684;&#24335;&#19981;&#21305;&#37197;&#31561;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/1910.07115</link><description>&lt;p&gt;
HiGitClass: &#22522;&#20110;&#20851;&#38190;&#35789;&#30340;GitHub&#20179;&#24211;&#30340;&#20998;&#23618;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HiGitClass: Keyword-Driven Hierarchical Classification of GitHub Repositories. (arXiv:1910.07115v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1910.07115
&lt;/p&gt;
&lt;p&gt;
HiGitClass&#26159;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;GitHub&#20179;&#24211;&#30340;&#20998;&#23618;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#24102;&#26377;&#20851;&#38190;&#35789;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#30417;&#30563;&#12290;&#26694;&#26550;&#35299;&#20915;&#20102;&#22810;&#27169;&#24335;&#20449;&#21495;&#12289;&#30417;&#30563;&#31232;&#32570;&#24615;&#21644;&#30417;&#30563;&#26684;&#24335;&#19981;&#21305;&#37197;&#31561;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GitHub&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#20195;&#30721;&#20998;&#20139;&#21644;&#31185;&#23398;&#20132;&#27969;&#24179;&#21488;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#20179;&#24211;&#25968;&#37327;&#24222;&#22823;&#65292;&#38656;&#35201;&#22522;&#20110;&#20027;&#39064;&#36827;&#34892;&#25628;&#32034;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#20027;&#39064;&#26631;&#31614;&#21151;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;GitHub&#20179;&#24211;&#37117;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#65292;&#38480;&#21046;&#20102;&#25628;&#32034;&#21644;&#22522;&#20110;&#20027;&#39064;&#30340;&#20998;&#26512;&#30340;&#25928;&#29992;&#12290;&#26412;&#30740;&#31350;&#23558;&#33258;&#21160;&#20179;&#24211;&#20998;&#31867;&#38382;&#39064;&#23450;&#20301;&#20026;&#22522;&#20110;&#20851;&#38190;&#35789;&#30340;&#20998;&#23618;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#24102;&#26377;&#20851;&#38190;&#35789;&#30340;&#26631;&#31614;&#23618;&#27425;&#32467;&#26500;&#20316;&#20026;&#30417;&#30563;&#12290;&#36825;&#31181;&#35774;&#32622;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#33021;&#36866;&#24212;&#29992;&#25143;&#30340;&#38656;&#27714;&#65292;&#32771;&#34385;&#21040;&#20027;&#39064;&#26631;&#31614;&#30340;&#19981;&#21516;&#31890;&#24230;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#20154;&#21147;&#25237;&#20837;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#65288;1&#65289;&#22810;&#27169;&#24335;&#20449;&#21495;&#30340;&#23384;&#22312;&#65307;&#65288;2&#65289;&#30417;&#30563;&#31232;&#32570;&#24615;&#21644;&#20559;&#35265;&#65307;&#65288;3&#65289;&#30417;&#30563;&#26684;&#24335;&#19981;&#21305;&#37197;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiGitClass&#26694;&#26550;&#65292;&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;heterogeneou
&lt;/p&gt;
&lt;p&gt;
GitHub has become an important platform for code sharing and scientific exchange. With the massive number of repositories available, there is a pressing need for topic-based search. Even though the topic label functionality has been introduced, the majority of GitHub repositories do not have any labels, impeding the utility of search and topic-based analysis. This work targets the automatic repository classification problem as keyword-driven hierarchical classification. Specifically, users only need to provide a label hierarchy with keywords to supply as supervision. This setting is flexible, adaptive to the users' needs, accounts for the different granularity of topic labels and requires minimal human effort. We identify three key challenges of this problem, namely (1) the presence of multi-modal signals; (2) supervision scarcity and bias; (3) supervision format mismatch. In recognition of these challenges, we propose the HiGitClass framework, comprising of three modules: heterogeneou
&lt;/p&gt;</description></item></channel></rss>