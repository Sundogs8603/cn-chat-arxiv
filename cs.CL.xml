<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#30456;&#20851;&#20449;&#24687;&#20301;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#24615;&#33021;&#26368;&#20339;&#65292;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#22312;&#38271;&#25991;&#26412;&#30340;&#20013;&#38388;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#36755;&#20837;&#25991;&#26412;&#36234;&#38271;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#65292;&#24182;&#19988;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.03172</link><description>&lt;p&gt;
&#36855;&#22833;&#22312;&#20013;&#38388;&#65306;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#38271;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Lost in the Middle: How Language Models Use Long Contexts. (arXiv:2307.03172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#24403;&#30456;&#20851;&#20449;&#24687;&#20301;&#20110;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#24615;&#33021;&#26368;&#20339;&#65292;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#22312;&#38271;&#25991;&#26412;&#30340;&#20013;&#38388;&#35775;&#38382;&#30456;&#20851;&#20449;&#24687;&#26102;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#36755;&#20837;&#25991;&#26412;&#36234;&#38271;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#25552;&#20379;&#20102;&#26032;&#30340;&#35748;&#35782;&#65292;&#24182;&#19988;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23558;&#38271;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#65292;&#20294;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#26377;&#25928;&#22320;&#20351;&#29992;&#36739;&#38271;&#30340;&#25991;&#26412;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#20004;&#20010;&#38656;&#35201;&#22312;&#36755;&#20837;&#25991;&#26412;&#20013;&#35782;&#21035;&#30456;&#20851;&#20449;&#24687;&#30340;&#20219;&#21153;&#65288;&#22810;&#25991;&#26723;&#38382;&#31572;&#21644;&#38190;&#20540;&#26816;&#32034;&#65289;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30456;&#20851;&#20449;&#24687;&#20986;&#29616;&#22312;&#36755;&#20837;&#25991;&#26412;&#30340;&#24320;&#22836;&#25110;&#32467;&#23614;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#36890;&#24120;&#26368;&#20339;&#65307;&#32780;&#24403;&#27169;&#22411;&#38656;&#35201;&#35775;&#38382;&#38271;&#25991;&#26412;&#20013;&#30340;&#20013;&#38388;&#30456;&#20851;&#20449;&#24687;&#26102;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#23545;&#20110;&#19987;&#38376;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#27169;&#22411;&#65292;&#24403;&#36755;&#20837;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#38271;&#26102;&#65292;&#24615;&#33021;&#20063;&#20250;&#22823;&#24133;&#38477;&#20302;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20351;&#29992;&#36755;&#20837;&#25991;&#26412;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#38271;&#25991;&#26412;&#27169;&#22411;&#25552;&#20379;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.
&lt;/p&gt;</description></item><item><title>Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.03170</link><description>&lt;p&gt;
Focused Transformer: &#21453;&#24046;&#35757;&#32451;&#23545;&#19978;&#19979;&#25991;&#32553;&#25918;&#36827;&#34892;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Focused Transformer: Contrastive Training for Context Scaling. (arXiv:2307.03170v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03170
&lt;/p&gt;
&lt;p&gt;
Focused Transformer&#36890;&#36807;&#21453;&#24046;&#35757;&#32451;&#20248;&#21270;&#20102;&#19978;&#19979;&#25991;&#32553;&#25918;&#38382;&#39064;&#65292;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#19978;&#19979;&#25991;&#21270;&#30340;&#26041;&#24335;&#21560;&#32435;&#26032;&#30340;&#20449;&#24687;&#65292;&#20294;&#30001;&#20110;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#28508;&#21147;&#36890;&#24120;&#21463;&#21040;&#38480;&#21046;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#26159;&#20026;&#27880;&#24847;&#21147;&#23618;&#25552;&#20379;&#35775;&#38382;&#22806;&#37096;&#23384;&#20648;&#22120;&#30340;&#33021;&#21147;&#65292;&#35813;&#23384;&#20648;&#22120;&#30001;&#65288;&#38190;&#65292;&#20540;&#65289;&#23545;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25991;&#26723;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#30456;&#20851;&#38190;&#19982;&#26080;&#20851;&#38190;&#30340;&#27604;&#20363;&#20943;&#23569;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#26080;&#20851;&#38190;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#21517;&#20026;&#20998;&#24515;&#38382;&#39064;&#30340;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#19982;&#19981;&#21516;&#35821;&#20041;&#20540;&#30456;&#20851;&#32852;&#30340;&#38190;&#21487;&#33021;&#37325;&#21472;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#21306;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Focused Transformer&#65288;FoT&#65289;&#65292;&#19968;&#31181;&#21463;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#65288;&#38190;&#65292;&#20540;&#65289;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20351;&#19978;&#19979;&#25991;&#38271;&#24230;&#24471;&#20197;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#29616;&#26377;&#22823;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03135</link><description>&lt;p&gt;
&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#24615;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#25945;&#24072;&#27169;&#22411;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#36229;&#20986;&#20998;&#24067;&#21487;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#35268;&#27169;&#21644;&#35745;&#31639;&#35201;&#27714;&#20351;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#21644;&#26102;&#38388;&#25935;&#24863;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#27169;&#22411;&#21387;&#32553;&#26159;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#24555;&#30340;&#27169;&#22411;&#20197;&#20445;&#25345;&#36739;&#22823;&#27169;&#22411;&#24615;&#33021;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#34920;&#31034;&#21387;&#32553;&#21040;&#36731;&#37327;&#32423;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#36807;&#31243;&#65292;&#20351;&#29992;&#23567;&#22411;&#25110;&#20013;&#22411;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#21487;&#27867;&#21270;&#30340;&#24320;&#25918;&#35789;&#27719;&#38382;&#39064;&#65292;&#36825;&#22312;&#20197;&#24448;&#30340;&#27169;&#22411;&#21387;&#32553;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#20102;&#12290;&#25105;&#20204;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#20004;&#20010;&#21407;&#21017;&#26469;&#22686;&#24378;&#23398;&#29983;&#27169;&#22411;&#30340;OOD&#21487;&#27867;&#21270;&#24615;&#65306;&#65288;1&#65289;&#26356;&#22909;&#22320;&#27169;&#20223;&#25945;&#24072;&#30340;&#35270;&#35273;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#22312;&#35270;&#35273;&#35821;&#35328;&#23545;&#40784;&#26041;&#38754;&#35880;&#24910;&#22320;&#20419;&#36827;&#26356;&#22909;&#30340;&#19968;&#33268;&#24615;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20016;&#23500;&#23398;&#29983;&#27169;&#22411;&#30340;&#33258;&#20030;&#23398;&#20064;&#21644;&#25968;&#25454;&#25193;&#20805;&#26469;&#25552;&#39640;OOD&#21487;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large vision-language models have achieved outstanding performance, but their size and computational requirements make their deployment on resource-constrained devices and time-sensitive tasks impractical. Model distillation, the process of creating smaller, faster models that maintain the performance of larger models, is a promising direction towards the solution. This paper investigates the distillation of visual representations in large teacher vision-language models into lightweight student models using a smallor mid-scale dataset. Notably, this study focuses on open-vocabulary out-of-distribution (OOD) generalization, a challenging problem that has been overlooked in previous model distillation literature. We propose two principles from vision and language modality perspectives to enhance student's OOD generalization: (1) by better imitating teacher's visual representation space, and carefully promoting better coherence in vision-language alignment with the teacher; (2) by enric
&lt;/p&gt;</description></item><item><title>T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03132</link><description>&lt;p&gt;
T-MARS&#65306;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#26469;&#25913;&#21892;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
T-MARS: Improving Visual Representations by Circumventing Text Feature Learning. (arXiv:2307.03132v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03132
&lt;/p&gt;
&lt;p&gt;
T-MARS&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#36991;&#25991;&#26412;&#29305;&#24449;&#23398;&#20064;&#65292;&#25913;&#21892;&#20102;&#35270;&#35273;&#34920;&#31034;&#30340;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#19982;&#22270;&#20687;&#37325;&#21472;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#32593;&#32476;&#26469;&#28304;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#20026;&#23398;&#20064;&#36890;&#29992;&#35270;&#35273;&#34920;&#31034;&#30340;&#26032;&#26041;&#27861;&#25552;&#20379;&#20102;&#21160;&#21147;&#65292;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#24443;&#24213;&#25913;&#21464;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35782;&#21035;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#20915;&#31574;&#38382;&#39064;&#26159;&#22914;&#20309;&#31579;&#36873;&#36825;&#20123;&#26085;&#30410;&#24222;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#31579;&#36873;&#26041;&#27861;&#65292;&#20854;&#21160;&#26426;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#36817;40%&#30340;LAION&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#19982;&#35828;&#26126;&#23384;&#22312;&#37325;&#21472;&#30340;&#25991;&#26412;&#12290;&#30452;&#35273;&#19978;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#20250;&#28010;&#36153;&#36164;&#28304;&#65292;&#22240;&#20026;&#23427;&#40723;&#21169;&#27169;&#22411;&#36827;&#34892;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#32780;&#19981;&#26159;&#23398;&#20064;&#35270;&#35273;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#23558;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#21435;&#38500;&#20063;&#21487;&#33021;&#28010;&#36153;&#65292;&#22240;&#20026;&#36825;&#20250;&#20002;&#24323;&#21253;&#21547;&#35270;&#35273;&#29305;&#24449;&#30340;&#22270;&#20687;&#65288;&#38500;&#20102;&#37325;&#21472;&#30340;&#25991;&#26412;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large web-sourced multimodal datasets have powered a slew of new methods for learning general-purpose visual representations, advancing the state of the art in computer vision and revolutionizing zero- and few-shot recognition. One crucial decision facing practitioners is how, if at all, to curate these ever-larger datasets. For example, the creators of the LAION-5B dataset chose to retain only image-caption pairs whose CLIP similarity score exceeded a designated threshold. In this paper, we propose a new state-of-the-art data filtering approach motivated by our observation that nearly 40% of LAION's images contain text that overlaps significantly with the caption. Intuitively, such data could be wasteful as it incentivizes models to perform optical character recognition rather than learning visual features. However, naively removing all such data could also be wasteful, as it throws away images that contain visual features (in addition to overlapping text). Our simple and scalable app
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#31995;&#32479;&#24615;&#22320;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#21508;&#31181;&#33258;&#21160;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;BLEURT&#21644;BARTScore&#31561;&#24230;&#37327;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#23545;&#25239;&#32763;&#35793;&#29616;&#35937;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#40065;&#26834;&#24615;&#32570;&#38519;&#20027;&#35201;&#30001;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#21644;&#24230;&#37327;&#33539;&#24335;&#30340;&#20542;&#21521;&#24341;&#36215;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#32422;&#26463;&#65292;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03131</link><description>&lt;p&gt;
BLEURT&#20855;&#26377;&#36890;&#29992;&#32763;&#35793;&#33021;&#21147;&#65306;&#22522;&#20110;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#30340;&#33258;&#21160;&#24230;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
BLEURT Has Universal Translations: An Analysis of Automatic Metrics by Minimum Risk Training. (arXiv:2307.03131v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#31995;&#32479;&#24615;&#22320;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#21508;&#31181;&#33258;&#21160;&#24230;&#37327;&#65292;&#24182;&#21457;&#29616;&#20102;BLEURT&#21644;BARTScore&#31561;&#24230;&#37327;&#20013;&#23384;&#22312;&#30340;&#36890;&#29992;&#23545;&#25239;&#32763;&#35793;&#29616;&#35937;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#40065;&#26834;&#24615;&#32570;&#38519;&#20027;&#35201;&#30001;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#21644;&#24230;&#37327;&#33539;&#24335;&#30340;&#20542;&#21521;&#24341;&#36215;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#32422;&#26463;&#65292;&#21487;&#20197;&#25552;&#39640;&#24230;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24230;&#37327;&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;n-gram&#24230;&#37327;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26368;&#36817;&#20986;&#29616;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24230;&#37327;&#30340;&#21457;&#23637;&#28526;&#27969;&#65292;&#37325;&#28857;&#22312;&#20110;&#27979;&#37327;&#21477;&#23376;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31070;&#32463;&#24230;&#37327;&#34429;&#28982;&#19982;&#20154;&#24037;&#35780;&#20272;&#30456;&#20851;&#24615;&#26356;&#39640;&#65292;&#20294;&#24120;&#24120;&#34987;&#35748;&#20026;&#26159;&#24102;&#26377;&#28508;&#22312;&#20559;&#35265;&#19988;&#38590;&#20197;&#26816;&#27979;&#30340;&#40657;&#30418;&#23376;&#12290;&#26412;&#30740;&#31350;&#20174;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#25351;&#23548;&#35282;&#24230;&#65292;&#31995;&#32479;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22810;&#31181;&#20027;&#27969;&#21644;&#21069;&#27839;&#30340;&#33258;&#21160;&#24230;&#37327;&#12290;&#36890;&#36807;&#26368;&#23567;&#39118;&#38505;&#35757;&#32451;&#65288;MRT&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;&#24230;&#37327;&#23384;&#22312;&#40065;&#26834;&#24615;&#32570;&#38519;&#65292;&#20363;&#22914;BLEURT&#21644;BARTScore&#20013;&#23384;&#22312;&#36890;&#29992;&#23545;&#25239;&#32763;&#35793;&#29616;&#35937;&#12290;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#40065;&#26834;&#24615;&#32570;&#38519;&#20027;&#35201;&#26377;&#20004;&#20010;&#21407;&#22240;&#65306;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#20559;&#24046;&#21644;&#24230;&#37327;&#33539;&#24335;&#30340;&#20542;&#21521;&#12290;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#32422;&#26463;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#24230;&#37327;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic metrics play a crucial role in machine translation. Despite the widespread use of n-gram-based metrics, there has been a recent surge in the development of pre-trained model-based metrics that focus on measuring sentence semantics. However, these neural metrics, while achieving higher correlations with human evaluations, are often considered to be black boxes with potential biases that are difficult to detect. In this study, we systematically analyze and compare various mainstream and cutting-edge automatic metrics from the perspective of their guidance for training machine translation systems. Through Minimum Risk Training (MRT), we find that certain metrics exhibit robustness defects, such as the presence of universal adversarial translations in BLEURT and BARTScore. In-depth analysis suggests two main causes of these robustness deficits: distribution biases in the training datasets, and the tendency of the metric paradigm. By incorporating token-level constraints, we enhan
&lt;/p&gt;</description></item><item><title>VisKoP&#26159;&#19968;&#31181;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#34701;&#20837;&#21040;&#30693;&#35782;&#24211;&#26597;&#35810;&#30340;&#32534;&#36753;&#21644;&#35843;&#35797;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35270;&#35273;&#30693;&#35782;&#23548;&#21521;&#32534;&#31243;&#30340;&#24179;&#21488;&#12290;&#23427;&#19981;&#20165;&#25552;&#20379;&#20102;&#31070;&#32463;&#31243;&#24207;&#24402;&#32435;&#27169;&#22359;&#65292;&#36824;&#23558;&#31243;&#24207;&#26144;&#23556;&#20026;&#22270;&#24418;&#20803;&#32032;&#65292;&#20351;&#20854;&#26131;&#20110;&#32534;&#36753;&#21644;&#35843;&#35797;&#12290;&#36890;&#36807;&#25552;&#20379;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#21644;&#39640;&#25928;&#30340;&#25191;&#34892;&#24341;&#25806;&#65292;VisKoP&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#38382;&#31572;&#26102;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03130</link><description>&lt;p&gt;
VisKoP&#65306;&#38754;&#21521;&#20132;&#20114;&#24335;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#35270;&#35273;&#30693;&#35782;&#23548;&#21521;&#32534;&#31243;
&lt;/p&gt;
&lt;p&gt;
VisKoP: Visual Knowledge oriented Programming for Interactive Knowledge Base Question Answering. (arXiv:2307.03130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03130
&lt;/p&gt;
&lt;p&gt;
VisKoP&#26159;&#19968;&#31181;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;&#20154;&#31867;&#34701;&#20837;&#21040;&#30693;&#35782;&#24211;&#26597;&#35810;&#30340;&#32534;&#36753;&#21644;&#35843;&#35797;&#20013;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35270;&#35273;&#30693;&#35782;&#23548;&#21521;&#32534;&#31243;&#30340;&#24179;&#21488;&#12290;&#23427;&#19981;&#20165;&#25552;&#20379;&#20102;&#31070;&#32463;&#31243;&#24207;&#24402;&#32435;&#27169;&#22359;&#65292;&#36824;&#23558;&#31243;&#24207;&#26144;&#23556;&#20026;&#22270;&#24418;&#20803;&#32032;&#65292;&#20351;&#20854;&#26131;&#20110;&#32534;&#36753;&#21644;&#35843;&#35797;&#12290;&#36890;&#36807;&#25552;&#20379;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#21644;&#39640;&#25928;&#30340;&#25191;&#34892;&#24341;&#25806;&#65292;VisKoP&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#38382;&#31572;&#26102;&#34920;&#29616;&#20986;&#39640;&#25928;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VisKoP&#30340;&#35270;&#35273;&#30693;&#35782;&#23548;&#21521;&#32534;&#31243;&#24179;&#21488;&#65292;&#23427;&#26159;&#19968;&#20010;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#31995;&#32479;&#65292;&#23558;&#20154;&#31867;&#34701;&#20837;&#21040;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#26597;&#35810;&#30340;&#32534;&#36753;&#21644;&#35843;&#35797;&#20013;&#12290;VisKoP&#19981;&#20165;&#25552;&#20379;&#20102;&#19968;&#20010;&#31070;&#32463;&#31243;&#24207;&#24402;&#32435;&#27169;&#22359;&#65292;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#21270;&#20026;&#30693;&#35782;&#23548;&#21521;&#30340;&#31243;&#24207;&#35821;&#35328;&#65288;KoPL&#65289;&#65292;&#36824;&#23558;KoPL&#31243;&#24207;&#26144;&#23556;&#20026;&#22270;&#24418;&#20803;&#32032;&#12290;KoPL&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#22270;&#24418;&#25805;&#20316;&#36827;&#34892;&#32534;&#36753;&#65292;&#20363;&#22914;&#25302;&#21160;&#20197;&#28155;&#21152;&#30693;&#35782;&#25805;&#20316;&#31526;&#21644;&#20351;&#29992;&#27133;&#22635;&#20805;&#20197;&#25351;&#23450;&#25805;&#20316;&#31526;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;VisKoP&#36824;&#20026;&#20854;&#30693;&#35782;&#24211;&#27169;&#24335;&#25552;&#20379;&#20102;&#33258;&#21160;&#34917;&#20840;&#21151;&#33021;&#65292;&#24182;&#36890;&#36807;&#26816;&#26597;&#20013;&#38388;&#32467;&#26524;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#35843;&#35797;KoPL&#31243;&#24207;&#12290;&#20026;&#20102;&#20415;&#20110;&#22312;&#20159;&#32423;&#21035;&#30340;&#23454;&#38469;&#30693;&#35782;&#24211;&#38382;&#31572;&#19978;&#36827;&#34892;&#65292;&#25105;&#20204;&#20026;&#21518;&#31471;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;KoPL&#25191;&#34892;&#24341;&#25806;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;VisKoP&#38750;&#24120;&#39640;&#25928;&#65292;&#29992;&#25143;&#20132;&#20114;&#21487;&#20197;&#20462;&#22797;&#22823;&#37096;&#20998;&#38169;&#35823;&#30340;KoPL&#31243;&#24207;&#20197;&#33719;&#21462;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Visual Knowledge oriented Programming platform (VisKoP), a knowledge base question answering (KBQA) system that integrates human into the loop to edit and debug the knowledge base (KB) queries. VisKoP not only provides a neural program induction module, which converts natural language questions into knowledge oriented program language (KoPL), but also maps KoPL programs into graphical elements. KoPL programs can be edited with simple graphical operators, such as dragging to add knowledge operators and slot filling to designate operator arguments. Moreover, VisKoP provides auto-completion for its knowledge base schema and users can easily debug the KoPL program by checking its intermediate results. To facilitate the practical KBQA on a million-entity-level KB, we design a highly efficient KoPL execution engine for the back-end. Experiment results show that VisKoP is highly efficient and user interaction can fix a large portion of wrong KoPL programs to acquire the correct ans
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20540;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#21517;&#21644;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#20855;&#26377;&#29305;&#23450;&#20851;&#31995;&#38408;&#20540;&#20197;&#19978;&#30340;&#23545;&#35937;&#21487;&#20197;&#36798;&#21040;49.5%&#30340;F1&#24471;&#20998;&#65292;&#36825;&#23545;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20540;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35813;&#30740;&#31350;&#20026;&#20174;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#20013;&#25552;&#21462;&#20851;&#31995;&#30693;&#35782;&#24320;&#36767;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2307.03122</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20540;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Extracting Multi-valued Relations from Language Models. (arXiv:2307.03122v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03122
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#22810;&#20540;&#20851;&#31995;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25490;&#21517;&#21644;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36873;&#25321;&#20855;&#26377;&#29305;&#23450;&#20851;&#31995;&#38408;&#20540;&#20197;&#19978;&#30340;&#23545;&#35937;&#21487;&#20197;&#36798;&#21040;49.5%&#30340;F1&#24471;&#20998;&#65292;&#36825;&#23545;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#20540;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#32780;&#35328;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#35813;&#30740;&#31350;&#20026;&#20174;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#20013;&#25552;&#21462;&#20851;&#31995;&#30693;&#35782;&#24320;&#36767;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#34920;&#26126;&#23427;&#20204;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#27599;&#20010;&#20027;&#39064;-&#20851;&#31995;&#23545;&#20013;&#30340;&#21333;&#20010;&#23545;&#35937;&#65292;&#23613;&#31649;&#36890;&#24120;&#26377;&#22810;&#20010;&#23545;&#35937;&#26159;&#27491;&#30830;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20998;&#26512;&#36825;&#20123;&#34920;&#31034;&#20197;&#20102;&#35299;&#23427;&#20204;&#20135;&#29983;&#22810;&#23545;&#35937;&#20851;&#31995;&#30693;&#35782;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#25490;&#21517;-&#36873;&#25321;&#20219;&#21153;&#12290;&#23545;&#20110;&#25490;&#21517;&#20505;&#36873;&#23545;&#35937;&#65292;&#25105;&#20204;&#35780;&#20272;&#29616;&#26377;&#30340;&#25552;&#31034;&#25216;&#26415;&#24182;&#25552;&#20986;&#20102;&#34701;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#26032;&#25216;&#26415;&#12290;&#22312;&#36873;&#25321;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#20855;&#26377;&#39640;&#20110;&#23398;&#20064;&#21040;&#30340;&#20851;&#31995;&#29305;&#23450;&#38408;&#20540;&#30340;&#23545;&#35937;&#21487;&#20197;&#36798;&#21040;49.5%&#30340;F1&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;&#20351;&#29992;LMs&#36827;&#34892;&#22810;&#20540;&#27133;&#20301;&#22635;&#20805;&#20219;&#21153;&#30340;&#22256;&#38590;&#65292;&#24182;&#20026;&#20174;&#28508;&#22312;&#35821;&#35328;&#34920;&#31034;&#20013;&#25552;&#21462;&#20851;&#31995;&#30693;&#35782;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread usage of latent language representations via pre-trained language models (LMs) suggests that they are a promising source of structured knowledge. However, existing methods focus only on a single object per subject-relation pair, even though often multiple objects are correct. To overcome this limitation, we analyze these representations for their potential to yield materialized multi-object relational knowledge. We formulate the problem as a rank-then-select task. For ranking candidate objects, we evaluate existing prompting techniques and propose new ones incorporating domain knowledge. Among the selection methods, we find that choosing objects with a likelihood above a learned relation-specific threshold gives a 49.5% F1 score. Our results highlight the difficulty of employing LMs for the multi-valued slot-filling task and pave the way for further research on extracting relational knowledge from latent language representations.
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#30340;&#37325;&#35201;&#24615;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#24378;&#35843;&#12290;&#20026;&#20102;&#20811;&#26381;&#24050;&#26377;&#22522;&#20934;&#27979;&#35797;&#30340;&#38480;&#21046;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;KoRC&#65292;&#22312;&#30693;&#35782;&#35206;&#30422;&#21644;&#31572;&#26696;&#26684;&#24335;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KoRC&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#25991;&#26412;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03115</link><description>&lt;p&gt;
KoRC: &#38754;&#21521;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#30340;&#30693;&#35782;&#23548;&#21521;&#38405;&#35835;&#29702;&#35299;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
KoRC: Knowledge oriented Reading Comprehension Benchmark for Deep Text Understanding. (arXiv:2307.03115v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03115
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#30340;&#37325;&#35201;&#24615;&#22312;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#24378;&#35843;&#12290;&#20026;&#20102;&#20811;&#26381;&#24050;&#26377;&#22522;&#20934;&#27979;&#35797;&#30340;&#38480;&#21046;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;KoRC&#65292;&#22312;&#30693;&#35782;&#35206;&#30422;&#21644;&#31572;&#26696;&#26684;&#24335;&#19978;&#20855;&#26377;&#20248;&#21183;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KoRC&#21487;&#20197;&#24110;&#21161;&#25913;&#36827;&#25991;&#26412;&#29702;&#35299;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#37117;&#24378;&#35843;&#20102;&#28145;&#24230;&#25991;&#26412;&#29702;&#35299;&#23545;&#20110;&#32473;&#23450;&#25991;&#26723;&#21644;&#25991;&#26412;&#20197;&#22806;&#30340;&#20808;&#39564;&#30693;&#35782;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#36935;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#30340;&#38480;&#21046;&#12290;&#19968;&#26041;&#38754;&#65292;&#22823;&#22810;&#25968;&#22522;&#20934;&#27979;&#35797;&#38656;&#35201;&#20154;&#24037;&#27880;&#37322;&#30693;&#35782;&#65292;&#23548;&#33268;&#20102;&#30693;&#35782;&#35206;&#30422;&#30340;&#38480;&#21046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#20204;&#36890;&#24120;&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#36873;&#39033;&#25110;&#36328;&#24230;&#20316;&#20026;&#31572;&#26696;&#65292;&#23548;&#33268;&#20102;&#29421;&#31364;&#30340;&#31572;&#26696;&#31354;&#38388;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21517;&#20026;KoRC&#12290;&#19982;&#20808;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;KoRC&#20855;&#26377;&#20004;&#20010;&#20248;&#28857;&#65292;&#21363;&#24191;&#27867;&#30340;&#30693;&#35782;&#35206;&#30422;&#21644;&#28789;&#27963;&#30340;&#31572;&#26696;&#26684;&#24335;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#28023;&#37327;&#30693;&#35782;&#24211;&#26469;&#25351;&#23548;&#27880;&#37322;&#32773;&#25110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26500;&#24314;&#26377;&#35265;&#22320;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#26631;&#31614;&#20316;&#20026;&#26368;&#32456;&#31572;&#26696;&#65292;&#32780;&#19981;&#26159;&#36328;&#24230;&#25110;&#36873;&#39033;&#12290;&#25105;&#20204;&#22312;KoRC&#19978;&#27979;&#35797;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
Deep text understanding, which requires the connections between a given document and prior knowledge beyond its text, has been highlighted by many benchmarks in recent years. However, these benchmarks have encountered two major limitations. On the one hand, most of them require human annotation of knowledge, which leads to limited knowledge coverage. On the other hand, they usually use choices or spans in the texts as the answers, which results in narrow answer space. To overcome these limitations, we build a new challenging benchmark named KoRc in this paper. Compared with previous benchmarks, KoRC has two advantages, i.e., broad knowledge coverage and flexible answer format. Specifically, we utilize massive knowledge bases to guide annotators or large language models (LLMs) to construct knowledgable questions. Moreover, we use labels in knowledge bases rather than spans or choices as the final answers. We test state-of-the-art models on KoRC and the experimental results show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2307.03109</link><description>&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Evaluation of Large Language Models. (arXiv:2307.03109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#35780;&#20272;&#20219;&#21153;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#26412;&#25991;&#20026;&#31038;&#20250;&#23618;&#38754;&#23545;LLMs&#28508;&#22312;&#39118;&#38505;&#30340;&#29702;&#35299;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#32780;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#22312;&#30740;&#31350;&#21644;&#26085;&#24120;&#20351;&#29992;&#20013;&#32487;&#32493;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#20204;&#30340;&#35780;&#20272;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#19981;&#20165;&#22312;&#20219;&#21153;&#27700;&#24179;&#19978;&#65292;&#32780;&#19988;&#22312;&#31038;&#20250;&#23618;&#38754;&#19978;&#65292;&#20197;&#26356;&#22909;&#22320;&#20102;&#35299;&#23427;&#20204;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#37324;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#21162;&#21147;&#26469;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#26469;&#30740;&#31350;LLMs&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;LLMs&#30340;&#36825;&#20123;&#35780;&#20272;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20851;&#38190;&#32500;&#24230;&#65306;&#35780;&#20272;&#20160;&#20040;&#12289;&#22312;&#21738;&#37324;&#35780;&#20272;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#35780;&#20272;&#20219;&#21153;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#19968;&#33324;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#25512;&#29702;&#12289;&#21307;&#23398;&#24212;&#29992;&#12289;&#20262;&#29702;&#23398;&#12289;&#25945;&#32946;&#12289;&#33258;&#28982;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#12289;&#20195;&#29702;&#24212;&#29992;&#21644;&#20854;&#20182;&#39046;&#22495;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#28145;&#20837;&#25506;&#35752;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20934;&#31572;&#26696;&#26469;&#22238;&#31572;&#8220;&#22312;&#21738;&#37324;&#8221;&#21644;&#8220;&#22914;&#20309;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and bench
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03104</link><description>&lt;p&gt;
&#20351;&#29992;&#36866;&#37197;&#22120;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Efficient Domain Adaptation of Sentence Embeddings using Adapters. (arXiv:2307.03104v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#26469;&#39640;&#25928;&#22495;&#33258;&#36866;&#24212;&#21477;&#23376;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#23884;&#20837;&#20351;&#25105;&#20204;&#33021;&#22815;&#25429;&#25417;&#30701;&#25991;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#22823;&#22810;&#25968;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#26159;&#38024;&#23545;&#19968;&#33324;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22240;&#27492;&#65292;&#35201;&#22312;&#29305;&#23450;&#39046;&#22495;&#20013;&#20351;&#29992;&#21477;&#23376;&#23884;&#20837;&#65292;&#24517;&#39035;&#23558;&#27169;&#22411;&#36866;&#24212;&#20110;&#35813;&#39046;&#22495;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36825;&#26159;&#36890;&#36807;&#23545;&#24863;&#20852;&#36259;&#30340;&#22495;&#23545;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#26469;&#23454;&#29616;&#30340;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#26356;&#26032;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#26435;&#37325;&#65292;&#20351;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#19978;&#35201;&#27714;&#36739;&#39640;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#36731;&#37327;&#32423;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#21333;&#29420;&#20026;&#27599;&#20010;&#30446;&#26631;&#39046;&#22495;&#24494;&#35843;&#25972;&#20010;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#12290;&#36825;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#19981;&#38656;&#35201;&#24494;&#35843;&#25152;&#26377;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21482;&#35757;&#32451;&#23569;&#37327;&#30340;&#39069;&#22806;&#21442;&#25968;&#65292;&#21516;&#26102;&#20445;&#25345;&#24213;&#23618;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;&#30340;&#26435;&#37325;&#19981;&#21464;&#12290;&#35757;&#32451;&#29305;&#23450;&#39046;&#22495;&#30340;&#36866;&#37197;&#22120;&#21487;&#20197;&#22987;&#32456;&#20351;&#29992;&#21516;&#19968;&#27169;&#22411;&#24182;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#33719;&#24471;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence embeddings enable us to capture the semantic similarity of short texts. Most sentence embedding models are trained for general semantic textual similarity (STS) tasks. Therefore, to use sentence embeddings in a particular domain, the model must be adapted to it in order to achieve good results. Usually, this is done by fine-tuning the entire sentence embedding model for the domain of interest. While this approach yields state-of-the-art results, all of the model's weights are updated during fine-tuning, making this method resource-intensive. Therefore, instead of fine-tuning entire sentence embedding models for each target domain individually, we propose to train lightweight adapters. These domain-specific adapters do not require fine-tuning all underlying sentence embedding model parameters. Instead, we only train a small number of additional parameters while keeping the weights of the underlying sentence embedding model fixed. Training domain-specific adapters allows always 
&lt;/p&gt;</description></item><item><title>OpenDelta&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;delta&#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#12290;&#23427;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03084</link><description>&lt;p&gt;
OpenDelta: &#19968;&#31181;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#24211;
&lt;/p&gt;
&lt;p&gt;
OpenDelta: A Plug-and-play Library for Parameter-efficient Adaptation of Pre-trained Models. (arXiv:2307.03084v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03084
&lt;/p&gt;
&lt;p&gt;
OpenDelta&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;delta&#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#12290;&#23427;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#35843;&#25972;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#32780;&#26080;&#38656;&#20462;&#25913;&#27169;&#22411;&#30340;&#20195;&#30721;&#65292;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411; (PTMs) &#30340;&#35268;&#27169;&#32473;&#35843;&#25972;&#19979;&#28216;&#20219;&#21153;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#20840;&#21442;&#25968;&#24494;&#35843;&#28041;&#21450;&#39640;&#26114;&#30340;&#20248;&#21270;&#24320;&#38144;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026; "delta &#35843;&#25972;"&#65292;&#21363;&#20165;&#26356;&#26032;&#19968;&#23567;&#37096;&#20998;&#21442;&#25968;&#65292;&#31216;&#20026; "delta &#27169;&#22359;"&#65292;&#21516;&#26102;&#20445;&#25345;&#20027;&#24178;&#27169;&#22411;&#30340;&#21442;&#25968;&#22266;&#23450;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#23454;&#29616;&#30452;&#25509;&#20462;&#25913;&#20027;&#24178; PTMs &#30340;&#20195;&#30721;&#65292;&#24182;&#20026;&#27599;&#20010; PTM &#30828;&#32534;&#30721;&#29305;&#23450;&#30340; delta &#35843;&#25972;&#26041;&#27861;&#65292;delta &#35843;&#25972;&#30340;&#23454;&#29992;&#24615;&#21644;&#28789;&#27963;&#24615;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; OpenDelta&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#24211;&#65292;&#36890;&#36807;&#25552;&#20379;&#21508;&#31181; delta &#35843;&#25972;&#26041;&#27861;&#30340;&#21363;&#25554;&#21363;&#29992;&#23454;&#29616;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26032;&#25216;&#26415;&#28040;&#38500;&#20102;&#20462;&#25913;&#20027;&#24178; PTMs &#20195;&#30721;&#30340;&#38656;&#27714;&#65292;&#20351; OpenDelta &#21487;&#20197;&#19982;&#19981;&#21516;&#30340;&#12289;&#29978;&#33267;&#26159;&#26032;&#30340; PTMs &#20860;&#23481;&#12290;OpenDelta &#30340;&#35774;&#35745;&#31616;&#21333;&#12289;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scale of large pre-trained models (PTMs) poses significant challenges in adapting to downstream tasks due to the high optimization overhead and storage costs associated with full-parameter fine-tuning. To address this, many studies explore parameter-efficient tuning methods, also framed as "delta tuning", which updates only a small subset of parameters, known as "delta modules", while keeping the backbone model's parameters fixed. However, the practicality and flexibility of delta tuning have been limited due to existing implementations that directly modify the code of the backbone PTMs and hard-code specific delta tuning methods for each PTM. In this paper, we present OpenDelta, an open-source library that overcomes these limitations by providing a plug-and-play implementation of various delta tuning methods. Our novel techniques eliminate the need to modify the backbone PTMs' code, making OpenDelta compatible with different, even novel PTMs. OpenDelta is designed to be simple, mo
&lt;/p&gt;</description></item><item><title>DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;</title><link>http://arxiv.org/abs/2307.03067</link><description>&lt;p&gt;
DeepOnto: &#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
DeepOnto: A Python Package for Ontology Engineering with Deep Learning. (arXiv:2307.03067v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03067
&lt;/p&gt;
&lt;p&gt;
DeepOnto&#26159;&#19968;&#20010;Python&#21253;&#65292;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#26412;&#20307;&#24037;&#31243;&#12290;&#23427;&#36890;&#36807;&#38598;&#25104;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;&#26412;&#20307;API&#65292;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24037;&#20855;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#65292;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22914;PyTorch&#21644;Tensorflow&#20027;&#35201;&#26159;&#20026;Python&#24320;&#21457;&#30340;&#65292;&#32780;&#24191;&#27867;&#20351;&#29992;&#30340;&#26412;&#20307;API&#65288;&#22914;OWL API&#21644;Jena&#65289;&#20027;&#35201;&#26159;&#22522;&#20110;Java&#30340;&#12290;&#20026;&#20102;&#26041;&#20415;&#26080;&#32541;&#38598;&#25104;&#36825;&#20123;&#26694;&#26550;&#21644;API&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deeponto&#65292;&#19968;&#20010;&#19987;&#20026;&#26412;&#20307;&#24037;&#31243;&#35774;&#35745;&#30340;Python&#21253;&#12290;&#35813;&#21253;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;&#24191;&#27867;&#35748;&#21487;&#21644;&#21487;&#38752;&#30340;OWL API&#30340;&#26680;&#24515;&#26412;&#20307;&#22788;&#29702;&#27169;&#22359;&#65292;&#20197;&#26356;&#8220;Pythonic&#8221;&#30340;&#26041;&#24335;&#23553;&#35013;&#20854;&#22522;&#26412;&#29305;&#24615;&#65292;&#24182;&#25193;&#23637;&#20854;&#21151;&#33021;&#20197;&#21253;&#25324;&#20854;&#20182;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#35821;&#35328;&#21270;&#12289;&#35268;&#33539;&#21270;&#12289;&#25237;&#24433;&#31561;&#12290;&#22522;&#20110;&#36825;&#20010;&#27169;&#22359;&#65292;Deeponto&#25552;&#20379;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#31639;&#27861;&#65292;&#25903;&#25345;&#21508;&#31181;&#26412;&#20307;&#24037;&#31243;&#20219;&#21153;&#65292;&#20363;&#22914;&#26412;&#20307;&#23545;&#40784;&#21644;&#23436;&#25104;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Applying deep learning techniques, particularly language models (LMs), in ontology engineering has raised widespread attention. However, deep learning frameworks like PyTorch and Tensorflow are predominantly developed for Python programming, while widely-used ontology APIs, such as the OWL API and Jena, are primarily Java-based. To facilitate seamless integration of these frameworks and APIs, we present Deeponto, a Python package designed for ontology engineering. The package encompasses a core ontology processing module founded on the widely-recognised and reliable OWL API, encapsulating its fundamental features in a more "Pythonic" manner and extending its capabilities to include other essential components including reasoning, verbalisation, normalisation, projection, and more. Building on this module, Deeponto offers a suite of tools, resources, and algorithms that support various ontology engineering tasks, such as ontology alignment and completion, by harnessing deep learning meth
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2307.03042</link><description>&lt;p&gt;
LLaMA&#22312;&#20020;&#24202;&#39046;&#22495;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain. (arXiv:2307.03042v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#35757;&#32451;&#20102;&#19968;&#20010;&#19987;&#38376;&#36866;&#37197;&#20020;&#24202;&#39046;&#22495;&#30340;LLaMA-LoRA&#27169;&#22411;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#20854;&#19982;Downstream LLaMA-LoRA&#36866;&#37197;&#22120;&#36827;&#34892;&#34701;&#21512;&#65292;&#20197;&#23454;&#29616;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#21040;&#26032;&#39046;&#22495;&#65292;&#22914;&#20020;&#24202;&#24212;&#29992;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#24040;&#22823;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#36341;&#24615;&#36234;&#26469;&#36234;&#34987;&#35777;&#26126;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#22320;&#24494;&#35843;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#21442;&#25968;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#39046;&#22495;&#36866;&#24212;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20020;&#24202;LLaMA-LoRA&#65292;&#36825;&#26159;&#19968;&#20010;&#26500;&#24314;&#22312;&#24320;&#28304;LLaMA&#27169;&#22411;&#19978;&#30340;PEFT&#36866;&#37197;&#22120;&#23618;&#12290;&#20020;&#24202;LLaMA-LoRA&#20351;&#29992;&#20174;MIMIC-IV&#25968;&#25454;&#24211;&#20013;&#33719;&#21462;&#30340;&#20020;&#24202;&#35760;&#24405;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#20026;&#20020;&#24202;&#39046;&#22495;&#35774;&#35745;&#30340;&#19987;&#29992;&#36866;&#37197;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;PEFT&#26694;&#26550;&#65292;&#23558;&#20020;&#24202;LLaMA-LoRA&#19982;Downstream LLaMA-LoRA&#36827;&#34892;&#34701;&#21512;&#65292;&#21518;&#32773;&#26159;&#21478;&#19968;&#20010;&#19987;&#20026;&#19979;&#28216;&#20219;&#21153;&#35774;&#35745;&#30340;PEFT&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapting pretrained language models to novel domains, such as clinical applications, traditionally involves retraining their entire set of parameters. However, this approach is increasingly proven to be impractical owing to the substantial computational requirements associated with training such large language models. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) techniques offer a viable solution by selectively fine-tuning a small subset of additional parameters, significantly reducing the computational requirements for domain adaptation. In this study, we propose Clinical LLaMA-LoRA, a PEFT adapter layer built upon the open-sourced LLaMA model. Clinical LLaMA-LoRA is trained using clinical notes obtained from the MIMIC-IV database, thereby creating a specialised adapter designed for the clinical domain. Additionally, we propose a two-step PEFT framework which fuses Clinical LLaMA-LoRA with Downstream LLaMA-LoRA, another PEFT adapter specialised for downstream tasks. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#32447;&#24615;&#25193;&#23637;&#31639;&#27861;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.03027</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#37325;&#35201;&#24615;&#23398;&#20064;&#25913;&#21892;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Retrieval-Augmented Large Language Models via Data Importance Learning. (arXiv:2307.03027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#32447;&#24615;&#25193;&#23637;&#31639;&#27861;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#20854;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#21098;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#20363;&#22914;&#22312;&#38382;&#39064;&#22238;&#31572;&#21644;&#25968;&#25454;&#34917;&#20840;&#31561;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#30340;&#24615;&#33021;&#21463;&#21040;&#20854;&#22522;&#30784;&#26816;&#32034;&#35821;&#26009;&#30340;&#25968;&#25454;&#36136;&#37327;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#32447;&#24615;&#25193;&#23637;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#26816;&#32034;&#21040;&#30340;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#22810;&#32447;&#24615;&#25193;&#23637;&#20013;&#23384;&#22312;&#25351;&#25968;&#32423;&#30340;&#39033;&#65292;&#26412;&#25991;&#30340;&#19968;&#20010;&#20851;&#38190;&#36129;&#29486;&#26159;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#65292;&#33021;&#22815;&#31934;&#30830;&#35745;&#31639;&#20855;&#26377;&#21152;&#27861;&#25928;&#29992;&#20989;&#25968;&#21644;&#39564;&#35777;&#38598;&#30340;&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#28857;&#22312;&#26816;&#32034;&#35821;&#26009;&#20013;&#30340;&#25968;&#25454;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#30340;&#65288;&#949;&#65292;&#948;&#65289;-&#36817;&#20284;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20165;&#20462;&#21098;&#25110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmentation enables large language models to take advantage of external knowledge, for example on tasks like question answering and data imputation. However, the performance of such retrieval-augmented models is limited by the data quality of their underlying retrieval corpus. In this paper, we propose an algorithm based on multilinear extension for evaluating the data importance of retrieved data points. There are exponentially many terms in the multilinear extension, and one key contribution of this paper is a polynomial time algorithm that computes exactly, given a retrieval-augmented model with an additive utility function and a validation set, the data importance of data points in the retrieval corpus using the multilinear extension of the model's utility function. We further proposed an even more efficient ({\epsilon}, {\delta})-approximation algorithm. Our experimental results illustrate that we can enhance the performance of large language models by only pruning or r
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#23613;&#31649;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#65292;&#31572;&#26696;&#20173;&#28982;&#34987;&#26356;&#39640;&#22320;&#35780;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;</title><link>http://arxiv.org/abs/2307.03025</link><description>&lt;p&gt;
&#39118;&#26684;&#32988;&#36807;&#23454;&#36136;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Style Over Substance: Evaluation Biases for Large Language Models. (arXiv:2307.03025v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03025
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#20154;&#31867;&#21644;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#65292;&#24182;&#21457;&#29616;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#21363;&#23613;&#31649;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#65292;&#31572;&#26696;&#20173;&#28982;&#34987;&#26356;&#39640;&#22320;&#35780;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#20934;&#30830;&#21644;&#20840;&#38754;&#35780;&#20272;&#23427;&#20204;&#30340;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#19978;&#65292;&#20154;&#31867;&#35780;&#20272;&#34987;&#35748;&#20026;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#23558;&#26368;&#20808;&#36827;&#30340;LLMs&#32435;&#20837;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#20316;&#20026;&#20154;&#31867;&#35780;&#22996;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21644;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#33021;&#21147;&#31243;&#24230;&#20173;&#28982;&#19981;&#30830;&#23450;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20247;&#21253;&#20154;&#31867;&#35780;&#22996;&#21644;&#22522;&#20110;LLMs&#30340;&#35780;&#22996;&#22312;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#36755;&#20986;&#26102;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#25925;&#24847;&#26377;&#32570;&#38519;&#30340;&#26426;&#22120;&#29983;&#25104;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#20107;&#23454;&#19978;&#30340;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#26356;&#22823;&#30340;&#21361;&#38505;&#65292;&#20294;&#24102;&#26377;&#20107;&#23454;&#38169;&#35823;&#30340;&#31572;&#26696;&#20173;&#28982;&#27604;&#38271;&#24230;&#36807;&#30701;&#25110;&#21253;&#21547;&#35821;&#27861;&#38169;&#35823;&#30340;&#31572;&#26696;&#35780;&#20998;&#26356;&#39640;&#12290;&#36825;&#31361;&#26174;&#20102;&#35780;&#20272;&#36807;&#31243;&#20013;&#23384;&#22312;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) continue to advance, accurately and comprehensively evaluating their performance becomes increasingly challenging. Conventionally, human evaluations are considered the gold standard in natural language generation. Recent advancements incorporate state-of-the-art LLMs as proxies for human judges in evaluation processes. Nonetheless, the extent to which humans and LLMs are capable evaluators remains uncertain. This study aims to investigate the behavior of both crowd-sourced human and LLM-based judges when comparing outputs from different models. To accomplish this, we curate a dataset comprising intentionally flawed machine-generated answers. Our findings indicate that despite the potentially greater danger posed by factual errors, answers with factual errors were still rated more favorably compared to answers that were too short or contained grammatical errors. This highlights a concerning bias in the evaluation process. To address this issue, we propose
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#21322;&#29615;&#21152;&#26435;Earley&#20998;&#26512;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#35268;&#27169;&#35821;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21152;&#36895;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#35821;&#27861;&#30340;&#39044;&#22788;&#29702;&#21644;&#25512;&#29702;&#24490;&#29615;&#30340;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2307.02982</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21322;&#29615;&#21152;&#26435;Earley&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Efficient Semiring-Weighted Earley Parsing. (arXiv:2307.02982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#21322;&#29615;&#21152;&#26435;Earley&#20998;&#26512;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#35268;&#27169;&#35821;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31181;&#21152;&#36895;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#35821;&#27861;&#30340;&#39044;&#22788;&#29702;&#21644;&#25512;&#29702;&#24490;&#29615;&#30340;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;Earley (1970)&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#35821;&#27861;&#20998;&#26512;&#31639;&#27861;&#30340;&#21442;&#32771;&#25551;&#36848;&#65292;&#21253;&#25324;&#22810;&#31181;&#21152;&#36895;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21253;&#25324;&#20174;Earley&#30340;$O(N^3|G||R|)$&#21040;$O(N^3|G|)$&#30340;&#24050;&#30693;&#26368;&#22351;&#24773;&#20917;&#36816;&#34892;&#26102;&#25913;&#36827;&#65292;&#21518;&#32773;&#21487;&#20197;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20986;&#29616;&#30340;&#22823;&#35268;&#27169;&#35821;&#27861;&#30340;&#38382;&#39064;&#12290;&#20854;&#20013;&#65292;$N$&#26159;&#21477;&#23376;&#30340;&#38271;&#24230;&#65292;$|R|$&#26159;$G$&#20013;&#30340;&#20135;&#29983;&#24335;&#25968;&#37327;&#65292;$|G|$&#26159;&#36825;&#20123;&#20135;&#29983;&#24335;&#30340;&#24635;&#38271;&#24230;&#12290;&#24403;&#20197;&#21333;&#20010;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;$M$&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#24335;&#34920;&#31034;&#35821;&#27861;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36816;&#34892;&#26102;&#20026;$O(N^3|M|)$&#65292;&#20854;&#20013;$|M|\leq|G|$&#30340;&#29256;&#26412;&#65288;&#37096;&#20998;&#26159;&#26032;&#39062;&#30340;&#65289;&#12290;&#25105;&#20204;&#20180;&#32454;&#22788;&#29702;&#20102;&#21322;&#29615;&#21152;&#26435;&#25512;&#29702;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#23545;&#35821;&#27861;&#36827;&#34892;&#39044;&#22788;&#29702;&#20197;&#28040;&#38500;&#25512;&#29702;&#24490;&#29615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;Stolcke (1995)&#30340;&#26041;&#27861;&#20197;&#35745;&#31639;&#21477;&#23376;&#21069;&#32512;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a reference description, in the form of a deduction system, of Earley's (1970) context-free parsing algorithm with various speed-ups. Our presentation includes a known worst-case runtime improvement from Earley's $O (N^3|G||R|)$, which is unworkable for the large grammars that arise in natural language processing, to $O (N^3|G|)$, which matches the runtime of CKY on a binarized version of the grammar $G$. Here $N$ is the length of the sentence, $|R|$ is the number of productions in $G$, and $|G|$ is the total length of those productions. We also provide a version that achieves runtime of $O (N^3|M|)$ with $|M| \leq |G|$ when the grammar is represented compactly as a single finite-state automaton $M$ (this is partly novel). We carefully treat the generalization to semiring-weighted deduction, preprocessing the grammar like Stolcke (1995) to eliminate deduction cycles, and further generalize Stolcke's method to compute the weights of sentence prefixes. We also provide
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#21270;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#24050;&#26377;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#19978;&#29983;&#25104;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#35937;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#24230;&#37327;&#26469;&#20248;&#21270;&#36328;&#25991;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.02971</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
On the Cultural Gap in Text-to-Image Generation. (arXiv:2307.02971v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02971
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#25991;&#21270;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#21270;&#22522;&#20934;&#65292;&#36890;&#36807;&#20998;&#26512;&#24050;&#26377;&#27169;&#22411;&#22312;&#35813;&#22522;&#20934;&#19978;&#29983;&#25104;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#35937;-&#25991;&#26412;&#23545;&#40784;&#30340;&#22810;&#27169;&#24577;&#24230;&#37327;&#26469;&#20248;&#21270;&#36328;&#25991;&#21270;&#27169;&#22411;&#30340;&#24494;&#35843;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#20013;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#24847;&#22806;&#21453;&#26144;&#20102;&#25991;&#21270;&#24046;&#36317;&#65292;&#24403;&#36755;&#20837;&#25991;&#26412;&#30340;&#25991;&#21270;&#20803;&#32032;&#24456;&#23569;&#20986;&#29616;&#22312;&#35757;&#32451;&#38598;&#20013;&#26102;&#65292;&#36825;&#34920;&#26126;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#24046;&#24322;&#12290;&#23613;&#31649;&#21508;&#31181;T2I&#27169;&#22411;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#20294;&#26159;&#38543;&#24847;&#30340;&#20363;&#23376;&#65292;&#20294;&#26159;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#22522;&#20934;&#26469;&#31995;&#32479;&#35780;&#20272;T2I&#27169;&#22411;&#29983;&#25104;&#36328;&#25991;&#21270;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#32508;&#21512;&#35780;&#20272;&#26631;&#20934;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36328;&#25991;&#21270;&#65288;C3&#65289;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#21487;&#20197;&#35780;&#20272;&#27169;&#22411;&#23545;&#30446;&#26631;&#25991;&#21270;&#30340;&#36866;&#24212;&#24615;&#12290;&#36890;&#36807;&#20998;&#26512;&#22312;C3&#22522;&#20934;&#19978;&#30001;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26377;&#32570;&#38519;&#30340;&#22270;&#20687;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#29305;&#23450;&#30340;&#25991;&#21270;&#23545;&#35937;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#23545;&#35937;&#19982;&#25991;&#26412;&#23545;&#40784;&#30340;&#26032;&#22411;&#22810;&#27169;&#24577;&#24230;&#37327;&#65292;&#29992;&#20110;&#36807;&#28388;&#30446;&#26631;&#25991;&#21270;&#20013;&#30340;&#24494;&#35843;&#25968;&#25454;&#65292;&#29992;&#20110;&#20248;&#21270;&#36328;&#25991;&#21270;&#33021;&#21147;&#30340;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
One challenge in text-to-image (T2I) generation is the inadvertent reflection of culture gaps present in the training data, which signifies the disparity in generated image quality when the cultural elements of the input text are rarely collected in the training set. Although various T2I models have shown impressive but arbitrary examples, there is no benchmark to systematically evaluate a T2I model's ability to generate cross-cultural images. To bridge the gap, we propose a Challenging Cross-Cultural (C3) benchmark with comprehensive evaluation criteria, which can assess how well-suited a model is to a target culture. By analyzing the flawed images generated by the Stable Diffusion model on the C3 benchmark, we find that the model often fails to generate certain cultural objects. Accordingly, we propose a novel multi-modal metric that considers object-text alignment to filter the fine-tuning data in the target culture, which is used to fine-tune a T2I model to improve cross-cultural g
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LEA&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#25171;&#23383;&#38169;&#35823;&#30340;&#21477;&#23376;&#30456;&#20284;&#24615;&#40065;&#26834;&#24615;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#24341;&#20837;&#35789;&#27719;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#25991;&#26412;&#22122;&#38899;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#25171;&#23383;&#38169;&#35823;&#23548;&#33268;&#30340;&#26631;&#35760;&#20998;&#24067;&#20559;&#31227;&#12290;</title><link>http://arxiv.org/abs/2307.02912</link><description>&lt;p&gt;
LEA: &#20351;&#29992;&#35789;&#27719;&#27880;&#24847;&#20559;&#24046;&#25552;&#39640;&#23545;&#25171;&#23383;&#38169;&#35823;&#30340;&#21477;&#23376;&#30456;&#20284;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
LEA: Improving Sentence Similarity Robustness to Typos Using Lexical Attention Bias. (arXiv:2307.02912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02912
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LEA&#27169;&#22359;&#65292;&#29992;&#20110;&#25552;&#39640;&#23545;&#25171;&#23383;&#38169;&#35823;&#30340;&#21477;&#23376;&#30456;&#20284;&#24615;&#40065;&#26834;&#24615;&#12290;&#35813;&#27169;&#22359;&#36890;&#36807;&#24341;&#20837;&#35789;&#27719;&#30456;&#20284;&#24615;&#26469;&#35299;&#20915;&#25991;&#26412;&#22122;&#38899;&#38382;&#39064;&#65292;&#24182;&#36991;&#20813;&#20102;&#25171;&#23383;&#38169;&#35823;&#23548;&#33268;&#30340;&#26631;&#35760;&#20998;&#24067;&#20559;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22122;&#38899;&#65292;&#22914;&#25171;&#23383;&#38169;&#35823;&#25110;&#32553;&#20889;&#65292;&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#38382;&#39064;&#65292;&#20250;&#23545;&#22823;&#22810;&#25968;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#32431;&#21464;&#21387;&#22120;&#27169;&#22411;&#36896;&#25104;&#24809;&#32602;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20063;&#36866;&#29992;&#20110;&#21477;&#23376;&#30456;&#20284;&#24615;&#65292;&#36825;&#26159;&#22810;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#27604;&#22914;&#21305;&#37197;&#12289;&#26816;&#32034;&#25110;&#37322;&#20041;&#12290;&#21487;&#20197;&#20351;&#29992;&#20132;&#21449;&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#21477;&#23376;&#30456;&#20284;&#24615;&#65292;&#20854;&#20013;&#20004;&#20010;&#21477;&#23376;&#22312;&#36755;&#20837;&#20013;&#36830;&#25509;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#20043;&#21069;&#35299;&#20915;&#22122;&#38899;&#38382;&#39064;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#23637;&#31034;&#20102;&#22312;&#22788;&#29702;&#19982;&#35757;&#32451;&#26679;&#26412;&#30456;&#20284;&#30340;&#25439;&#22351;&#26679;&#26412;&#26102;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#21463;&#21040;&#25171;&#23383;&#38169;&#35823;&#24341;&#36215;&#30340;&#26631;&#35760;&#20998;&#24067;&#20559;&#31227;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#24863;&#30693;&#27880;&#24847;&#27169;&#22359;&#65288;LEA&#65289;&#26469;&#35299;&#20915;&#25991;&#26412;&#22122;&#38899;&#38382;&#39064;&#65292;&#35813;&#27169;&#22359;&#22312;&#20004;&#20010;&#21477;&#23376;&#20013;&#30340;&#35789;&#20043;&#38388;&#24341;&#20837;&#20102;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21407;&#22987;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;token&#20998;&#24067;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual noise, such as typos or abbreviations, is a well-known issue that penalizes vanilla Transformers for most downstream tasks. We show that this is also the case for sentence similarity, a fundamental task in multiple domains, e.g. matching, retrieval or paraphrasing. Sentence similarity can be approached using cross-encoders, where the two sentences are concatenated in the input allowing the model to exploit the inter-relations between them. Previous works addressing the noise issue mainly rely on data augmentation strategies, showing improved robustness when dealing with corrupted samples that are similar to the ones used for training. However, all these methods still suffer from the token distribution shift induced by typos. In this work, we propose to tackle textual noise by equipping cross-encoders with a novel LExical-aware Attention module (LEA) that incorporates lexical similarities between words in both sentences. By using raw text similarities, our approach avoids the to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#24847;&#22823;&#21033;&#26412;&#22303;&#35762;&#32773;&#30340;&#23545;&#27604;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#27169;&#22411;&#26159;&#21542;&#33021;&#25512;&#26029;&#35789;&#27719;&#35821;&#20041;&#21644;&#23436;&#25104;&#24418;&#24577;&#21477;&#27861;&#27169;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#22312;&#25429;&#25417;&#20154;&#31867;&#35821;&#20041;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02910</link><description>&lt;p&gt;
GilBERTo&#20013;&#30340;&#20027;&#21160;&#24615;&#21644;&#24184;&#31119;&#24863;&#65306;&#23545;&#35748;&#30693;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Agentivit\`a e telicit\`a in GilBERTo: implicazioni cognitive. (arXiv:2307.02910v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#19982;&#24847;&#22823;&#21033;&#26412;&#22303;&#35762;&#32773;&#30340;&#23545;&#27604;&#23454;&#39564;&#65292;&#25506;&#31350;&#20102;&#27169;&#22411;&#26159;&#21542;&#33021;&#25512;&#26029;&#35789;&#27719;&#35821;&#20041;&#21644;&#23436;&#25104;&#24418;&#24577;&#21477;&#27861;&#27169;&#24335;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#27169;&#22411;&#22312;&#25429;&#25417;&#20154;&#31867;&#35821;&#20041;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#31350;&#22522;&#20110;Transformer&#30340;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#26029;&#35789;&#27719;&#35821;&#20041;&#24182;&#21033;&#29992;&#27492;&#20449;&#24687;&#23436;&#25104;&#24418;&#24577;&#21477;&#27861;&#27169;&#24335;&#12290;&#32771;&#34385;&#30340;&#35821;&#20041;&#23646;&#24615;&#26159;&#24310;&#32493;&#24615;&#65288;&#20063;&#19982;&#30830;&#23450;&#24615;&#30456;&#32467;&#21512;&#65289;&#21644;&#20027;&#21160;&#24615;&#12290;&#20108;&#32773;&#22343;&#22312;&#35821;&#20041;&#21644;&#24418;&#24577;&#21477;&#27861;&#20043;&#38388;&#36215;&#20316;&#29992;&#65306;&#23427;&#20204;&#22312;&#35821;&#20041;&#19978;&#30830;&#23450;&#65292;&#22312;&#21477;&#27861;&#19978;&#32534;&#30721;&#12290;&#36825;&#20123;&#20219;&#21153;&#21516;&#26102;&#20132;&#32473;&#20102;&#35745;&#31639;&#27169;&#22411;&#21644;&#19968;&#32452;&#24847;&#22823;&#21033;&#26412;&#22303;&#35762;&#32773;&#12290;&#27604;&#36739;&#36825;&#20004;&#32452;&#25968;&#25454;&#21487;&#20197;&#35753;&#25105;&#20204;&#25506;&#31350;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25429;&#25417;&#21040;&#20154;&#31867;&#35821;&#20041;&#33021;&#21147;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of this study is to investigate whether a Transformer-based neural language model infers lexical semantics and use this information for the completion of morphosyntactic patterns. The semantic properties considered are telicity (also combined with definiteness) and agentivity. Both act at the interface between semantics and morphosyntax: they are semantically determined and syntactically encoded. The tasks were submitted to both the computational model and a group of Italian native speakers. The comparison between the two groups of data allows us to investigate to what extent neural language models capture significant aspects of human semantic competence.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#25233;&#37057;&#30151;&#20250;&#25913;&#21464;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#36825;&#31181;&#27934;&#23519;&#21147;&#21487;&#20197;&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25233;&#37057;&#30151;&#26816;&#27979;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02892</link><description>&lt;p&gt;
&#25233;&#37057;&#30151;&#23545;&#35821;&#38899;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#20135;&#29983;&#24433;&#21709;: &#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#36895;&#24230;&#21644;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
The Relationship Between Speech Features Changes When You Get Depressed: Feature Correlations for Improving Speed and Performance of Depression Detection. (arXiv:2307.02892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#25233;&#37057;&#30151;&#20250;&#25913;&#21464;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992;&#36825;&#31181;&#27934;&#23519;&#21147;&#21487;&#20197;&#36890;&#36807;&#25913;&#36827;&#29305;&#24449;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#25233;&#37057;&#30151;&#26816;&#27979;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25233;&#37057;&#30151;&#20250;&#25913;&#21464;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#34920;&#26126;&#21033;&#29992;&#36825;&#26679;&#30340;&#27934;&#23519;&#21147;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;SVM&#21644;LSTMs&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#22120;&#30340;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#12290;&#23454;&#39564;&#26159;&#22312;Androids Corpus&#19978;&#36827;&#34892;&#30340;&#65292;&#36825;&#26159;&#19968;&#20010;&#28041;&#21450;112&#21517;&#35828;&#35805;&#32773;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;58&#21517;&#30001;&#19987;&#19994;&#31934;&#31070;&#30149;&#23398;&#23478;&#35786;&#26029;&#20026;&#25233;&#37057;&#30151;&#30340;&#20154;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23454;&#39564;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#36895;&#24230;&#21644;&#24615;&#33021;&#26041;&#38754;&#37117;&#24471;&#21040;&#20102;&#25913;&#21892;&#65292;&#19982;&#20351;&#29992;&#29305;&#24449;&#21521;&#37327;&#30456;&#27604;&#65292;&#20351;&#29992;&#29305;&#24449;&#30456;&#20851;&#24615;&#30697;&#38453;&#20316;&#20026;&#36755;&#20837;&#26102;&#65292;&#38169;&#35823;&#29575;&#30456;&#23545;&#20943;&#23569;&#20102;23.1&#65285;&#21040;26.6&#65285;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#27169;&#22411;&#12290;&#21487;&#33021;&#30340;&#35299;&#37322;&#26159;&#65292;&#22312;&#25233;&#37057;&#30340;&#35828;&#35805;&#32773;&#20013;&#65292;&#29305;&#24449;&#30456;&#20851;&#24615;&#30697;&#38453;&#20284;&#20046;&#26356;&#21152;&#22810;&#21464;&#12290;&#30456;&#24212;&#22320;&#65292;&#36825;&#31181;&#29616;&#35937;&#21487;&#20197;&#34987;&#35270;&#20026;&#25233;&#37057;&#30151;&#30340;&#19968;&#20010;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work shows that depression changes the correlation between features extracted from speech. Furthermore, it shows that using such an insight can improve the training speed and performance of depression detectors based on SVMs and LSTMs. The experiments were performed over the Androids Corpus, a publicly available dataset involving 112 speakers, including 58 people diagnosed with depression by professional psychiatrists. The results show that the models used in the experiments improve in terms of training speed and performance when fed with feature correlation matrices rather than with feature vectors. The relative reduction of the error rate ranges between 23.1% and 26.6% depending on the model. The probable explanation is that feature correlation matrices appear to be more variable in the case of depressed speakers. Correspondingly, such a phenomenon can be thought of as a depression marker.
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#27861;&#24459;&#20998;&#31867;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#29992;SetFit&#24494;&#35843;&#30340;&#27169;&#22411;&#27604;&#26222;&#36890;&#24494;&#35843;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;LIME&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#29305;&#24449;&#30340;&#35748;&#30693;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#27861;&#24459;&#19978;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#24182;&#23545;&#20998;&#31867;&#32467;&#26524;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02882</link><description>&lt;p&gt;
&#23545;&#27604;&#23601;&#26159;&#20320;&#25152;&#38656;&#30340;&#19968;&#20999;
&lt;/p&gt;
&lt;p&gt;
Contrast Is All You Need. (arXiv:2307.02882v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02882
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#27861;&#24459;&#20998;&#31867;&#22330;&#26223;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#20351;&#29992;SetFit&#24494;&#35843;&#30340;&#27169;&#22411;&#27604;&#26222;&#36890;&#24494;&#35843;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;LIME&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#29305;&#24449;&#30340;&#35748;&#30693;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#27861;&#24459;&#19978;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#24182;&#23545;&#20998;&#31867;&#32467;&#26524;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#25968;&#25454;&#31232;&#32570;&#30340;&#20998;&#31867;&#22330;&#26223;&#65292;&#20854;&#20013;&#21487;&#29992;&#30340;&#26631;&#35760;&#27861;&#24459;&#25968;&#25454;&#24456;&#23569;&#19988;&#19981;&#24179;&#34913;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20102;&#20004;&#20010;&#24494;&#35843;&#30446;&#26631;&#65306;SetFit&#65288;&#21477;&#23376;&#36716;&#25442;&#22120;&#24494;&#35843;&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#35774;&#32622;&#65292;&#20197;&#21450;&#22312;&#27861;&#24459;&#26465;&#27454;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#26222;&#36890;&#24494;&#35843;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;LIME&#65288;&#23616;&#37096;&#21487;&#35299;&#37322;&#27169;&#22411;&#26080;&#20851;&#35299;&#37322;&#65289;&#27604;&#36739;&#20102;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#20197;&#26597;&#30475;&#21738;&#20123;&#29305;&#23450;&#29305;&#24449;&#23545;&#27169;&#22411;&#30340;&#20998;&#31867;&#20915;&#31574;&#26377;&#36129;&#29486;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#30340;&#26222;&#36890;&#24494;&#35843;&#30456;&#27604;&#65292;&#20351;&#29992;SetFit&#30340;&#23545;&#27604;&#35774;&#32622;&#34920;&#29616;&#26356;&#22909;&#12290;LIME&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#21319;&#23545;&#27491;&#38754;&#21644;&#36127;&#38754;&#29305;&#24449;&#30340;&#35748;&#30693;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#27861;&#24459;&#19978;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#24182;&#23545;&#20998;&#31867;&#32467;&#26524;&#26377;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#23545;&#27604;&#30446;&#26631;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#20284;&#20046;&#26356;&#33258;&#20449;&#22320;&#22522;&#20110;&#27861;&#24459;&#20449;&#24687;&#36827;&#34892;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we analyze data-scarce classification scenarios, where available labeled legal data is small and imbalanced, potentially hurting the quality of the results. We focused on two finetuning objectives; SetFit (Sentence Transformer Finetuning), a contrastive learning setup, and a vanilla finetuning setup on a legal provision classification task. Additionally, we compare the features that are extracted with LIME (Local Interpretable Model-agnostic Explanations) to see which particular features contributed to the model's classification decisions. The results show that a contrastive setup with SetFit performed better than vanilla finetuning while using a fraction of the training samples. LIME results show that the contrastive learning approach helps boost both positive and negative features which are legally informative and contribute to the classification results. Thus a model finetuned with a contrastive objective seems to base its decisions more confidently on legally informa
&lt;/p&gt;</description></item><item><title>ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2307.02863</link><description>&lt;p&gt;
ValiTex -- &#19968;&#31181;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#32479;&#19968;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ValiTex -- a uniform validation framework for computational text-based measures of social science constructs. (arXiv:2307.02863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02863
&lt;/p&gt;
&lt;p&gt;
ValiTex&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#23427;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#30340;&#20256;&#32479;&#65292;&#36890;&#36807;&#27010;&#24565;&#27169;&#22411;&#21644;&#21160;&#24577;&#26816;&#26597;&#34920;&#25552;&#20379;&#20102;&#39564;&#35777;&#30340;&#32467;&#26500;&#21644;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22914;&#20309;&#39564;&#35777;&#35745;&#31639;&#25991;&#26412;&#30340;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#24230;&#37327;&#30340;&#25351;&#23548;&#26159;&#20998;&#25955;&#30340;&#12290;&#34429;&#28982;&#23398;&#32773;&#20204;&#26222;&#36941;&#35748;&#35782;&#21040;&#39564;&#35777;&#20182;&#20204;&#30340;&#25991;&#26412;&#24230;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#20182;&#20204;&#36890;&#24120;&#32570;&#20047;&#20849;&#21516;&#30340;&#26415;&#35821;&#21644;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ValiTex&#30340;&#26032;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#23398;&#32773;&#20204;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#26469;&#24230;&#37327;&#31038;&#20250;&#31185;&#23398;&#26500;&#24314;&#12290;&#35813;&#26694;&#26550;&#20511;&#37492;&#20102;&#24515;&#29702;&#27979;&#37327;&#23398;&#20013;&#38271;&#26399;&#23384;&#22312;&#30340;&#20256;&#32479;&#65292;&#21516;&#26102;&#25193;&#23637;&#20102;&#26694;&#26550;&#20197;&#36866;&#29992;&#20110;&#35745;&#31639;&#25991;&#26412;&#20998;&#26512;&#30340;&#30446;&#30340;&#12290;ValiTex&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#19968;&#20010;&#26159;&#27010;&#24565;&#27169;&#22411;&#65292;&#19968;&#20010;&#26159;&#21160;&#24577;&#26816;&#26597;&#34920;&#12290;&#27010;&#24565;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#25351;&#23548;&#39564;&#35777;&#30340;&#19981;&#21516;&#38454;&#27573;&#65292;&#21160;&#24577;&#26816;&#26597;&#34920;&#23450;&#20041;&#20102;&#20855;&#20307;&#30340;&#39564;&#35777;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#20102;&#21738;&#20123;&#27493;&#39588;&#21487;&#33021;&#34987;&#35748;&#20026;&#26159;&#25512;&#33616;&#30340;&#65288;&#21363;&#25552;&#20379;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#39564;&#35777;&#35777;&#25454;&#65289;&#25110;&#21487;&#36873;&#30340;&#65288;&#21363;&#23545;&#25552;&#20379;&#39069;&#22806;&#20449;&#24687;&#26377;&#29992;&#30340;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Guidance on how to validate computational text-based measures of social science constructs is fragmented. Whereas scholars are generally acknowledging the importance of validating their text-based measures, they often lack common terminology and a unified framework to do so. This paper introduces a new validation framework called ValiTex, designed to assist scholars to measure social science constructs based on textual data. The framework draws on a long-established tradition within psychometrics while extending the framework for the purpose of computational text analysis. ValiTex consists of two components, a conceptual model, and a dynamic checklist. Whereas the conceptual model provides a general structure along distinct phases on how to approach validation, the dynamic checklist defines specific validation steps and provides guidance on which steps might be considered recommendable (i.e., providing relevant and necessary validation evidence) or optional (i.e., useful for providing 
&lt;/p&gt;</description></item><item><title>NatLogAttack&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#36923;&#36753;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36827;&#34892;&#20445;&#25345;&#26631;&#31614;&#21644;&#32763;&#36716;&#26631;&#31614;&#30340;&#25915;&#20987;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25915;&#20987;&#27169;&#22411;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.02849</link><description>&lt;p&gt;
NatLogAttack: &#19968;&#20010;&#29992;&#33258;&#28982;&#36923;&#36753;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#25915;&#20987;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NatLogAttack: A Framework for Attacking Natural Language Inference Models with Natural Logic. (arXiv:2307.02849v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02849
&lt;/p&gt;
&lt;p&gt;
NatLogAttack&#26159;&#19968;&#20010;&#29992;&#33258;&#28982;&#36923;&#36753;&#23545;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#25915;&#20987;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36827;&#34892;&#20445;&#25345;&#26631;&#31614;&#21644;&#32763;&#36716;&#26631;&#31614;&#30340;&#25915;&#20987;&#65292;&#24182;&#30456;&#27604;&#29616;&#26377;&#25915;&#20987;&#27169;&#22411;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#33258;&#20174;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#22987;&#23601;&#26159;&#19968;&#20010;&#20013;&#24515;&#35805;&#39064;&#12290;&#36817;&#24180;&#26469;&#22312;&#20998;&#24067;&#24335;&#34920;&#31034;&#21644;&#31070;&#32463;&#32593;&#32476;&#19978;&#21462;&#24471;&#30340;&#36827;&#23637;&#25345;&#32493;&#25913;&#36827;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#36890;&#36807;&#30495;&#27491;&#30340;&#25512;&#29702;&#26469;&#24471;&#20986;&#32467;&#35770;&#65292;&#36824;&#26159;&#20381;&#36182;&#20110;&#34394;&#20551;&#30340;&#30456;&#20851;&#24615;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#23545;&#25239;&#24615;&#25915;&#20987;&#24050;&#32463;&#35777;&#26126;&#26159;&#35780;&#20272;&#21463;&#23475;&#27169;&#22411;&#30340;&#33268;&#21629;&#24369;&#28857;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22522;&#20110;&#36923;&#36753;&#24418;&#24335;&#20027;&#20041;&#24320;&#21457;&#25915;&#20987;&#27169;&#22411;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;NatLogAttack&#26469;&#25191;&#34892;&#22260;&#32469;&#33258;&#28982;&#36923;&#36753;&#30340;&#31995;&#32479;&#24615;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#36861;&#28335;&#21040;&#20122;&#37324;&#22763;&#22810;&#24503;&#19977;&#27573;&#35770;&#24182;&#19988;&#19982;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23494;&#20999;&#30456;&#20851;&#30340;&#32463;&#20856;&#36923;&#36753;&#24418;&#24335;&#12290;&#35813;&#25552;&#35758;&#30340;&#26694;&#26550;&#21487;&#20197;&#36827;&#34892;&#20445;&#25345;&#26631;&#31614;&#21644;&#32763;&#36716;&#26631;&#31614;&#30340;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#25915;&#20987;&#27169;&#22411;&#30456;&#27604;&#65292;NatLogAttack&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning has been a central topic in artificial intelligence from the beginning. The recent progress made on distributed representation and neural networks continues to improve the state-of-the-art performance of natural language inference. However, it remains an open question whether the models perform real reasoning to reach their conclusions or rely on spurious correlations. Adversarial attacks have proven to be an important tool to help evaluate the Achilles' heel of the victim models. In this study, we explore the fundamental problem of developing attack models based on logic formalism. We propose NatLogAttack to perform systematic attacks centring around natural logic, a classical logic formalism that is traceable back to Aristotle's syllogism and has been closely developed for natural language inference. The proposed framework renders both label-preserving and label-flipping attacks. We show that compared to the existing attack models, NatLogAttack generates better adversarial 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02839</link><description>&lt;p&gt;
&#20351;&#29992;&#36827;&#21270;&#35843;&#20248;&#22686;&#24378;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing LLM with Evolutionary Fine Tuning for News Summary Generation. (arXiv:2307.02839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#20351;&#29992;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#65292;&#36890;&#36807;&#36827;&#21270;&#35843;&#20248;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26159;&#24773;&#25253;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#20840;&#38754;&#30340;&#20449;&#24687;&#65292;&#24110;&#21161;&#20154;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#23545;&#22797;&#26434;&#30340;&#29616;&#23454;&#20107;&#20214;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#27169;&#22411;&#26412;&#36523;&#21644;&#35757;&#32451;&#25968;&#25454;&#37327;&#30340;&#38480;&#21046;&#65292;&#20197;&#21450;&#25991;&#26412;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#20351;&#24471;&#20934;&#30830;&#29983;&#25104;&#21487;&#38752;&#20449;&#24687;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20855;&#26377;&#24378;&#22823;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;LLM&#36827;&#34892;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#30340;&#26032;&#33539;&#24335;&#12290;&#25105;&#20204;&#21033;&#29992;LLM&#20174;&#26032;&#38395;&#27573;&#33853;&#20013;&#25552;&#21462;&#22810;&#20010;&#32467;&#26500;&#21270;&#20107;&#20214;&#27169;&#24335;&#65292;&#36890;&#36807;&#36951;&#20256;&#31639;&#27861;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#36873;&#25321;&#26368;&#36866;&#24212;&#30340;&#20107;&#20214;&#27169;&#24335;&#36755;&#20837;LLM&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#38395;&#25688;&#35201;&#29983;&#25104;&#22120;(NSG)&#26469;&#36873;&#25321;&#21644;&#36827;&#21270;&#20107;&#20214;&#27169;&#24335;&#32676;&#20307;&#65292;&#24182;&#29983;&#25104;&#26032;&#38395;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
News summary generation is an important task in the field of intelligence analysis, which can provide accurate and comprehensive information to help people better understand and respond to complex real-world events. However, traditional news summary generation methods face some challenges, which are limited by the model itself and the amount of training data, as well as the influence of text noise, making it difficult to generate reliable information accurately. In this paper, we propose a new paradigm for news summary generation using LLM with powerful natural language understanding and generative capabilities. We use LLM to extract multiple structured event patterns from the events contained in news paragraphs, evolve the event pattern population with genetic algorithm, and select the most adaptive event pattern to input into the LLM to generate news summaries. A News Summary Generator (NSG) is designed to select and evolve the event pattern populations and generate news summaries. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#38646;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#36870;&#21521;&#25552;&#31034;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26410;&#35265;&#27133;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2307.02830</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#30340;&#29983;&#25104;&#24335;&#38646;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#19982;&#36870;&#21521;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Generative Zero-Shot Prompt Learning for Cross-Domain Slot Filling with Inverse Prompting. (arXiv:2307.02830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#38646;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#38024;&#23545;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#36870;&#21521;&#25552;&#31034;&#31574;&#30053;&#21644;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#26410;&#35265;&#27133;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#26088;&#22312;&#23558;&#26631;&#35760;&#30340;&#28304;&#39046;&#22495;&#30693;&#35782;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#23545;&#27133;&#25551;&#36848;&#21644;&#31034;&#20363;&#36827;&#34892;&#32534;&#30721;&#65292;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#35268;&#21017;&#35774;&#35745;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#27169;&#26495;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27867;&#21270;&#33021;&#21147;&#25110;&#40065;&#26834;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#24335;&#38646;&#26679;&#26412;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#65292;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#26356;&#20855;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36870;&#21521;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#21306;&#20998;&#19981;&#21516;&#30340;&#27133;&#31867;&#22411;&#20197;&#36991;&#20813;&#22810;&#37325;&#39044;&#27979;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#65292;&#36890;&#36807;&#20165;&#35757;&#32451;&#36739;&#23569;&#30340;&#25552;&#31034;&#21442;&#25968;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#26410;&#35265;&#27133;&#19978;&#30340;&#26174;&#33879;&#25552;&#21319;&#65288;+13.44% F1&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot cross-domain slot filling aims to transfer knowledge from the labeled source domain to the unlabeled target domain. Existing models either encode slot descriptions and examples or design handcrafted question templates using heuristic rules, suffering from poor generalization capability or robustness. In this paper, we propose a generative zero-shot prompt learning framework for cross-domain slot filling, both improving generalization and robustness than previous work. Besides, we introduce a novel inverse prompting strategy to distinguish different slot types to avoid the multiple prediction problem, and an efficient prompt-tuning strategy to boost higher performance by only training fewer prompt parameters. Experiments and analysis demonstrate the effectiveness of our proposed framework, especially huge improvements (+13.44% F1) on the unseen slots.
&lt;/p&gt;</description></item><item><title>&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02796</link><description>&lt;p&gt;
VerifAI&#65306;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
VerifAI: Verified Generative AI. (arXiv:2307.02796v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02796
&lt;/p&gt;
&lt;p&gt;
&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20998;&#26512;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#65292;&#26469;&#24314;&#31435;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36755;&#20986;&#30340;&#26356;&#22362;&#23454;&#22522;&#30784;&#65292;&#24182;&#35299;&#20915;&#38169;&#35823;&#20449;&#24687;&#20256;&#25773;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#23545;&#20110;&#20854;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#20173;&#22312;&#22686;&#38271;&#12290;&#36825;&#31181;&#19981;&#20934;&#30830;&#24615;&#21487;&#33021;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#38169;&#35823;&#20915;&#31574;&#65292;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#65292;&#20405;&#29359;&#38544;&#31169;&#65292;&#27861;&#24459;&#36131;&#20219;&#31561;&#12290;&#34429;&#28982;&#24050;&#32463;&#22312;&#36827;&#34892;&#24212;&#23545;&#36825;&#20123;&#39118;&#38505;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#23454;&#36341;&#65292;&#22914;&#36879;&#26126;&#24230;&#65292;&#38544;&#31169;&#20445;&#25252;&#65292;&#20559;&#35265;&#32531;&#35299;&#20197;&#21450;&#31038;&#20250;&#21644;&#29615;&#22659;&#36131;&#20219;&#31561;&#65292;&#20294;&#30001;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#38169;&#35823;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#20174;&#25968;&#25454;&#31649;&#29702;&#30340;&#35282;&#24230;&#39564;&#35777;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#26159;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#26032;&#20852;&#38382;&#39064;&#12290;&#36825;&#21253;&#25324;&#20998;&#26512;&#26469;&#33258;&#22810;&#27169;&#24577;&#25968;&#25454;&#28246;&#30340;&#24213;&#23618;&#25968;&#25454;&#65292;&#21253;&#25324;&#25991;&#26412;&#25991;&#20214;&#65292;&#34920;&#26684;&#21644;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#35780;&#20272;&#20854;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#35780;&#20272;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36755;&#20986;&#22880;&#23450;&#26356;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24110;&#21161;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36755;&#20986;&#39564;&#35777;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI has made significant strides, yet concerns about the accuracy and reliability of its outputs continue to grow. Such inaccuracies can have serious consequences such as inaccurate decision-making, the spread of false information, privacy violations, legal liabilities, and more. Although efforts to address these risks are underway, including explainable AI and responsible AI practices such as transparency, privacy protection, bias mitigation, and social and environmental responsibility, misinformation caused by generative AI will remain a significant challenge. We propose that verifying the outputs of generative AI from a data management perspective is an emerging issue for generative AI. This involves analyzing the underlying data from multi-modal data lakes, including text files, tables, and knowledge graphs, and assessing its quality and consistency. By doing so, we can establish a stronger foundation for evaluating the outputs of generative AI models. Such an approach ca
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.02792</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23545;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#24212;&#35813;&#20570;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Should Data Science Education Do with Large Language Models?. (arXiv:2307.02792v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02792
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#21644;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27169;&#24335;&#65292;&#20174;&#21160;&#25163;&#32534;&#30721;&#21644;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#36716;&#21464;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#31561;&#30340;&#24555;&#36895;&#21457;&#23637;&#27491;&#22312;&#25913;&#21464;&#25968;&#25454;&#31185;&#23398;&#21644;&#32479;&#35745;&#23398;&#12290;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#24037;&#20855;&#21487;&#20197;&#31616;&#21270;&#22797;&#26434;&#30340;&#27969;&#31243;&#65292;&#20174;&#32780;&#37325;&#22609;&#20102;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#35282;&#33394;&#12290;&#25105;&#20204;&#35748;&#20026;LLM&#27491;&#22312;&#36716;&#21464;&#25968;&#25454;&#31185;&#23398;&#23478;&#30340;&#36131;&#20219;&#65292;&#23558;&#20182;&#20204;&#30340;&#37325;&#28857;&#20174;&#21160;&#25163;&#32534;&#30721;&#12289;&#25968;&#25454;&#25972;&#29702;&#21644;&#36827;&#34892;&#26631;&#20934;&#20998;&#26512;&#36716;&#21464;&#20026;&#35780;&#20272;&#21644;&#31649;&#29702;&#36825;&#20123;&#33258;&#21160;&#21270;AI&#25191;&#34892;&#30340;&#20998;&#26512;&#12290;&#36825;&#31181;&#35282;&#33394;&#30340;&#28436;&#21464;&#31867;&#20284;&#20110;&#20174;&#36719;&#20214;&#24037;&#31243;&#24072;&#36716;&#21464;&#20026;&#20135;&#21697;&#32463;&#29702;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20351;&#29992;LLM&#22312;&#25968;&#25454;&#31185;&#23398;&#26696;&#20363;&#30740;&#31350;&#20013;&#35828;&#26126;&#20102;&#36825;&#31181;&#36716;&#21464;&#12290;&#36825;&#20123;&#21457;&#23637;&#35201;&#27714;&#25968;&#25454;&#31185;&#23398;&#25945;&#32946;&#26377;&#24847;&#20041;&#22320;&#21457;&#23637;&#12290;&#25945;&#32946;&#26041;&#27861;&#29616;&#22312;&#24517;&#39035;&#26356;&#21152;&#27880;&#37325;&#22521;&#20859;&#23398;&#29983;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#65292;&#22914;LLM&#21551;&#21457;&#30340;&#21019;&#36896;&#21147;&#12289;&#25209;&#21028;&#24615;&#24605;&#32500;&#12289;AI&#24341;&#23548;&#30340;&#32534;&#31243;&#12290;LLM&#36824;&#21487;&#20197;&#22312;&#35838;&#22530;&#19978;&#36215;&#21040;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20316;&#20026;&#20114;&#21160;&#24335;&#25945;&#23398;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The rapid advances of large language models (LLMs), such as ChatGPT, are revolutionizing data science and statistics. These state-of-the-art tools can streamline complex processes. As a result, it reshapes the role of data scientists. We argue that LLMs are transforming the responsibilities of data scientists, shifting their focus from hands-on coding, data-wrangling and conducting standard analyses to assessing and managing analyses performed by these automated AIs. This evolution of roles is reminiscent of the transition from a software engineer to a product manager. We illustrate this transition with concrete data science case studies using LLMs in this paper. These developments necessitate a meaningful evolution in data science education. Pedagogy must now place greater emphasis on cultivating diverse skillsets among students, such as LLM-informed creativity, critical thinking, AI-guided programming. LLMs can also play a significant role in the classroom as interactive teaching and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;PATTERNREFRAME&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#22914;&#20309;&#29983;&#25104;&#22823;&#37327;&#30340;&#20010;&#24615;&#21270;&#32451;&#20064;&#26448;&#26009;&#21644;&#24314;&#35758;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#37325;&#26500;&#26080;&#30410;&#24605;&#32500;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.02768</link><description>&lt;p&gt;
&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#12289;&#35782;&#21035;&#21644;&#37325;&#26500;&#26080;&#30410;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Training Models to Generate, Recognize, and Reframe Unhelpful Thoughts. (arXiv:2307.02768v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#25968;&#25454;&#38598;PATTERNREFRAME&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#25506;&#35752;&#22914;&#20309;&#29983;&#25104;&#22823;&#37327;&#30340;&#20010;&#24615;&#21270;&#32451;&#20064;&#26448;&#26009;&#21644;&#24314;&#35758;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#37325;&#26500;&#26080;&#30410;&#24605;&#32500;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#35768;&#22810;&#35748;&#30693;&#26041;&#27861;&#26469;&#25552;&#39640;&#24184;&#31119;&#24863;&#65292;&#20363;&#22914;&#35782;&#21035;&#21644;&#37325;&#26500;&#26080;&#30410;&#24605;&#32500;&#65292;&#24471;&#21040;&#20102;&#30456;&#24403;&#22810;&#30340;&#23454;&#35777;&#25903;&#25345;&#65292;&#20294;&#22312;&#33258;&#21161;&#26684;&#24335;&#19979;&#20173;&#32570;&#20047;&#24191;&#27867;&#37319;&#32435;&#12290;&#20854;&#20013;&#19968;&#20010;&#21407;&#22240;&#26159;&#32570;&#20047;&#20805;&#20998;&#20855;&#20307;&#19988;&#22810;&#26679;&#21270;&#30340;&#19987;&#38376;&#32451;&#20064;&#26448;&#26009;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#26469;&#20135;&#29983;&#22823;&#37327;&#30340;&#32451;&#20064;&#26448;&#26009;&#65292;&#23637;&#31034;&#19982;&#29305;&#23450;&#19978;&#19979;&#25991;&#21305;&#37197;&#30340;&#24120;&#35265;&#26080;&#30410;&#24605;&#32500;&#27169;&#24335;&#65292;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#31215;&#26497;&#37325;&#26500;&#24314;&#35758;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PATTERNREFRAME&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#32422;1&#19975;&#20010;&#20197;&#32473;&#23450;&#35282;&#33394;&#20026;&#26465;&#20214;&#30340;&#21253;&#21547;&#26080;&#30410;&#24605;&#32500;&#27169;&#24335;&#30340;&#20363;&#23376;&#65292;&#24182;&#38468;&#24102;&#22823;&#32422;2.7&#19975;&#20010;&#31215;&#26497;&#30340;&#37325;&#26500;&#24314;&#35758;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#24403;&#21069;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#21644;/&#25110;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#29616;&#26377;&#27169;&#22411;&#24050;&#32463;&#21487;&#20197;&#25104;&#20026;&#19968;&#20010;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#24110;&#21161;&#29983;&#25104;&#37327;&#36523;&#23450;&#21046;&#30340;&#32451;&#20064;&#26448;&#26009;&#21644;&#20551;&#35774;&#65292;&#19988;&#19981;&#38656;&#25110;&#21482;&#38656;&#26368;&#23569;&#30340;&#20154;&#24037;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many cognitive approaches to well-being, such as recognizing and reframing unhelpful thoughts, have received considerable empirical support over the past decades, yet still lack truly widespread adoption in self-help format. A barrier to that adoption is a lack of adequately specific and diverse dedicated practice material. This work examines whether current language models can be leveraged to both produce a virtually unlimited quantity of practice material illustrating standard unhelpful thought patterns matching specific given contexts, and generate suitable positive reframing proposals. We propose PATTERNREFRAME, a novel dataset of about 10k examples of thoughts containing unhelpful thought patterns conditioned on a given persona, accompanied by about 27k positive reframes. By using this dataset to train and/or evaluate current models, we show that existing models can already be powerful tools to help generate an abundance of tailored practice material and hypotheses, with no or min
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#27169;&#20010;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#26469;&#20934;&#30830;&#35782;&#21035;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#36866;&#23452;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#20851;&#31995;&#26412;&#36523;&#22914;&#20309;&#20316;&#20026;&#26263;&#21547;&#35268;&#33539;&#30340;&#21151;&#33021;&#20197;&#21450;&#22312;&#19981;&#21516;&#23545;&#35805;&#35774;&#32622;&#20013;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.02763</link><description>&lt;p&gt;
&#24744;&#30340;&#20276;&#20387;&#38656;&#35201;&#19987;&#19994;&#24110;&#21161;&#65306;&#36890;&#36807;&#24314;&#27169;&#31038;&#20132;&#20851;&#31995;&#30830;&#23450;&#28040;&#24687;&#30340;&#19978;&#19979;&#25991;&#36866;&#24403;&#24615;
&lt;/p&gt;
&lt;p&gt;
Your spouse needs professional help: Determining the Contextual Appropriateness of Messages through Modeling Social Relationships. (arXiv:2307.02763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#24314;&#27169;&#20010;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#26469;&#20934;&#30830;&#35782;&#21035;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#36866;&#23452;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#20851;&#31995;&#26412;&#36523;&#22914;&#20309;&#20316;&#20026;&#26263;&#21547;&#35268;&#33539;&#30340;&#21151;&#33021;&#20197;&#21450;&#22312;&#19981;&#21516;&#23545;&#35805;&#35774;&#32622;&#20013;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#20154;&#38469;&#20132;&#27969;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#38656;&#35201;&#29702;&#35299;&#28040;&#24687;&#25152;&#35828;&#30340;&#31038;&#20132;&#32972;&#26223;&#21644;&#35268;&#33539;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35782;&#21035;&#27492;&#31867;&#20132;&#27969;&#20013;&#20882;&#29359;&#20869;&#23481;&#30340;&#26041;&#27861;&#24456;&#22823;&#31243;&#24230;&#19978;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#65292;&#21482;&#26377;&#23569;&#25968;&#26041;&#27861;&#32771;&#34385;&#21040;&#31038;&#21306;&#35268;&#33539;&#25110;&#20808;&#21069;&#23545;&#35805;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26174;&#24335;&#22320;&#24314;&#27169;&#20010;&#20307;&#20043;&#38388;&#30340;&#31038;&#20132;&#20851;&#31995;&#26469;&#35782;&#21035;&#19981;&#36866;&#24403;&#20132;&#27969;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20855;&#26377;&#19978;&#19979;&#25991;&#21028;&#26029;&#36866;&#23452;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#24456;&#22909;&#22320;&#25972;&#21512;&#20851;&#31995;&#20449;&#24687;&#26469;&#20934;&#30830;&#22320;&#35782;&#21035;&#32473;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#36866;&#23452;&#24615;&#12290;&#21033;&#29992;&#22312;&#32447;&#23545;&#35805;&#21644;&#30005;&#24433;&#23545;&#35805;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20851;&#31995;&#26412;&#36523;&#20316;&#20026;&#26263;&#21547;&#35268;&#33539;&#30340;&#21151;&#33021;&#65292;&#24182;&#37327;&#21270;&#20102;&#19981;&#21516;&#23545;&#35805;&#35774;&#32622;&#20013;&#38656;&#35201;&#19978;&#19979;&#25991;&#25935;&#24863;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding interpersonal communication requires, in part, understanding the social context and norms in which a message is said. However, current methods for identifying offensive content in such communication largely operate independent of context, with only a few approaches considering community norms or prior conversation as context. Here, we introduce a new approach to identifying inappropriate communication by explicitly modeling the social relationship between the individuals. We introduce a new dataset of contextually-situated judgments of appropriateness and show that large language models can readily incorporate relationship information to accurately identify appropriateness in a given context. Using data from online conversations and movie dialogues, we provide insight into how the relationships themselves function as implicit norms and quantify the degree to which context-sensitivity is needed in different conversation settings. Further, we also demonstrate that contextua
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.02762</link><description>&lt;p&gt;
PRD: &#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations. (arXiv:2307.02762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PRD&#31639;&#27861;&#65292;&#21033;&#29992;&#21516;&#34892;&#35780;&#32423;&#21644;&#35752;&#35770;&#25913;&#21892;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#33258;&#25105;&#25552;&#21319;&#21644;&#20301;&#32622;&#20559;&#35265;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22238;&#31572;&#36136;&#37327;&#22312;&#33258;&#21160;&#21270;&#26041;&#38754;&#24456;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#24182;&#20027;&#35201;&#20351;&#29992;LLMs&#20316;&#20026;&#26080;&#21442;&#32771;&#24230;&#37327;&#34913;&#24320;&#25918;&#24335;&#38382;&#39064;&#22238;&#31572;&#30340;&#21442;&#32771;&#25351;&#26631;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20182;&#20204;&#20197;&#34987;&#35748;&#20026;&#26159;&#8220;&#26368;&#24378;&#8221;&#30340;LLM&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#23545;&#20505;&#36873;&#27169;&#22411;&#30340;&#31572;&#26696;&#36827;&#34892;&#20004;&#20004;&#27604;&#36739;&#24182;&#25552;&#20379;&#25490;&#21517;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30452;&#35266;&#30340;&#26041;&#27861;&#23384;&#22312;&#22810;&#20010;&#38382;&#39064;&#65292;&#20363;&#22914;&#24102;&#26469;&#33258;&#25105;&#25552;&#21319;&#65288;&#38738;&#30544;&#33258;&#24049;&#30340;&#31572;&#26696;&#65289;&#21644;&#20301;&#32622;&#20559;&#35265;&#12290;&#25105;&#20204;&#20174;&#25945;&#32946;&#39046;&#22495;&#65288;Cho and MacArthur, 2011&#65307;Walsh, 2014&#65289;&#20013;&#27762;&#21462;&#35265;&#35299;&#21644;&#25945;&#35757;&#65292;&#25913;&#36827;&#20102;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#21516;&#34892;&#35780;&#32423;&#65288;PR&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#32771;&#34385;&#27599;&#20010;&#21516;&#34892;LLM&#23545;&#25152;&#26377;&#31572;&#26696;&#23545;&#30340;&#20004;&#20004;&#20559;&#22909;&#65292;&#24182;&#36755;&#20986;&#27169;&#22411;&#30340;&#26368;&#32456;&#25490;&#21517;&#65307;&#20197;&#21450;&#65288;2&#65289;&#21516;&#34892;&#35752;&#35770;&#65288;PD&#65289;&#65292;&#22312;&#20854;&#20013;&#25105;&#20204;&#20419;&#20351;&#20004;&#20010;LLMs&#36827;&#34892;&#35752;&#35770;&#24182;&#23581;&#35797;&#23601;&#20004;&#20010;&#20559;&#22909;&#36798;&#25104;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the quality of responses generated by different modern large language models (LLMs) are hard to evaluate and compare automatically. Recent studies suggest and predominantly use LLMs as a reference-free metric for open-ended question answering. More specifically, they use the recognized "strongest" LLM as the evaluator, which conducts pairwise comparisons of candidate models' answers and provides a ranking score. However, this intuitive method has multiple problems, such as bringing in self-enhancement (favoring its own answers) and positional bias. We draw insights and lessons from the educational domain (Cho and MacArthur, 2011; Walsh, 2014) to improve LLM-based evaluations. Specifically, we propose the (1) peer rank (PR) algorithm that takes into account each peer LLM's pairwise preferences of all answer pairs, and outputs a final ranking of models; and (2) peer discussion (PD), where we prompt two LLMs to discuss and try to reach a mutual agreement on preferences of two an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;Reddit&#19978;&#20998;&#26512;&#20102;&#20004;&#26041;&#23545;&#35805;&#20027;&#39064;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#20102;LSM&#22312;&#23545;&#35805;&#20013;&#30340;&#24046;&#24322;&#20197;&#21450;&#19982;&#31038;&#21306;&#25351;&#26631;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#29702;&#35299;&#31038;&#21306;&#21160;&#24577;&#26102;&#23545;&#35805;&#21442;&#19982;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02758</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#31038;&#21306;&#20013;&#25506;&#32034;&#35821;&#35328;&#39118;&#26684;&#21305;&#37197;&#65306;&#31038;&#20250;&#32972;&#26223;&#21644;&#23545;&#35805;&#21160;&#24577;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Linguistic Style Matching in Online Communities: The Role of Social Context and Conversation Dynamics. (arXiv:2307.02758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;Reddit&#19978;&#20998;&#26512;&#20102;&#20004;&#26041;&#23545;&#35805;&#20027;&#39064;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#65292;&#30740;&#31350;&#20102;LSM&#22312;&#23545;&#35805;&#20013;&#30340;&#24046;&#24322;&#20197;&#21450;&#19982;&#31038;&#21306;&#25351;&#26631;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#29702;&#35299;&#31038;&#21306;&#21160;&#24577;&#26102;&#23545;&#35805;&#21442;&#19982;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#30340;&#35821;&#35328;&#39118;&#26684;&#21305;&#37197;&#21487;&#20197;&#21453;&#26144;&#20986;&#31038;&#20250;&#24433;&#21709;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#22914;&#26435;&#21147;&#25110;&#35828;&#26381;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#31867;&#20284;Reddit&#31561;&#24179;&#21488;&#19978;&#65292;LSM&#19982;&#22312;&#32447;&#27807;&#36890;&#32467;&#26524;&#30340;&#20851;&#31995;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Reddit&#20013;&#20004;&#26041;&#23545;&#35805;&#20027;&#39064;&#30340;&#22823;&#37327;&#35821;&#26009;&#24211;&#65292;&#24182;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;&#39118;&#26684;&#65306;&#21151;&#33021;&#35789;&#30340;&#20351;&#29992;&#21644;&#24418;&#24335;&#21270;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#31038;&#20132;&#22240;&#32032;&#22312;Reddit&#23545;&#35805;&#20013;LSM&#27700;&#24179;&#30340;&#24046;&#24322;&#65306;&#24086;&#23376;&#21644;&#23376;&#31038;&#21306;&#29305;&#24449;&#12289;&#23545;&#35805;&#28145;&#24230;&#12289;&#29992;&#25143;&#36164;&#21382;&#21644;&#35780;&#35770;&#30340;&#20105;&#35758;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#31038;&#21306;&#31105;&#20196;&#21518;&#22833;&#21435;&#22320;&#20301;&#21518;LSM&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;LSM&#22312;Reddit&#23545;&#35805;&#20013;&#19982;&#20960;&#20010;&#31038;&#21306;&#25351;&#26631;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26263;&#31034;&#20102;&#22312;&#20102;&#35299;&#31038;&#21306;&#21160;&#24577;&#26102;&#29702;&#35299;&#23545;&#35805;&#21442;&#19982;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic style matching (LSM) in conversations can be reflective of several aspects of social influence such as power or persuasion. However, how LSM relates to the outcomes of online communication on platforms such as Reddit is an unknown question. In this study, we analyze a large corpus of two-party conversation threads in Reddit where we identify all occurrences of LSM using two types of style: the use of function words and formality. Using this framework, we examine how levels of LSM differ in conversations depending on several social factors within Reddit: post and subreddit features, conversation depth, user tenure, and the controversiality of a comment. Finally, we measure the change of LSM following loss of status after community banning. Our findings reveal the interplay of LSM in Reddit conversations with several community metrics, suggesting the importance of understanding conversation engagement when understanding community dynamics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#26816;&#32034;&#27169;&#22411;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#25991;&#26723;&#38598;&#65292;&#20294;&#21487;&#20197;&#35775;&#38382;&#25551;&#36848;&#30446;&#26631;&#39046;&#22495;&#30340;&#31616;&#35201;&#25991;&#26412;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.02740</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#39046;&#22495;&#25551;&#36848;&#30340;&#23494;&#38598;&#26816;&#32034;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Dense Retrieval Adaptation using Target Domain Description. (arXiv:2307.02740v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20449;&#24687;&#26816;&#32034;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#26816;&#32034;&#27169;&#22411;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#25991;&#26723;&#38598;&#65292;&#20294;&#21487;&#20197;&#35775;&#38382;&#25551;&#36848;&#30446;&#26631;&#39046;&#22495;&#30340;&#31616;&#35201;&#25991;&#26412;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#39046;&#22495;&#36866;&#24212;&#26159;&#23558;&#26816;&#32034;&#27169;&#22411;&#36866;&#24212;&#20110;&#25968;&#25454;&#20998;&#24067;&#19982;&#28304;&#39046;&#22495;&#19981;&#21516;&#30340;&#26032;&#39046;&#22495;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38598;&#20013;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#25991;&#26723;&#38598;&#65292;&#25110;&#32773;&#26159;&#30417;&#30563;&#65288;&#36890;&#24120;&#26159;&#23569;&#26679;&#26412;&#65289;&#39046;&#22495;&#36866;&#24212;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#23427;&#20204;&#36824;&#21487;&#20197;&#35775;&#38382;&#30446;&#26631;&#39046;&#22495;&#20013;&#65288;&#26377;&#38480;&#30340;&#65289;&#26631;&#35760;&#25968;&#25454;&#12290;&#36824;&#23384;&#22312;&#19968;&#20123;&#30740;&#31350;&#33268;&#21147;&#20110;&#25913;&#21892;&#27809;&#26377;&#36866;&#24212;&#30340;&#26816;&#32034;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20449;&#24687;&#26816;&#32034;&#20013;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#31867;&#26032;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19982;&#38646;&#26679;&#26412;&#35774;&#32622;&#31867;&#20284;&#65292;&#25105;&#20204;&#20551;&#35774;&#26816;&#32034;&#27169;&#22411;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#25991;&#26723;&#38598;&#65292;&#20294;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#31616;&#35201;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#35828;&#26126;&#30446;&#26631;&#39046;&#22495;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39046;&#22495;&#23646;&#24615;&#30340;&#20998;&#31867;&#23398;&#65292;&#29992;&#20110;&#29702;&#35299;&#28304;&#39046;&#22495;&#21487;&#20197;&#36866;&#24212;&#21040;&#30446;&#26631;&#39046;&#22495;&#30340;&#19981;&#21516;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In information retrieval (IR), domain adaptation is the process of adapting a retrieval model to a new domain whose data distribution is different from the source domain. Existing methods in this area focus on unsupervised domain adaptation where they have access to the target document collection or supervised (often few-shot) domain adaptation where they additionally have access to (limited) labeled data in the target domain. There also exists research on improving zero-shot performance of retrieval models with no adaptation. This paper introduces a new category of domain adaptation in IR that is as-yet unexplored. Here, similar to the zero-shot setting, we assume the retrieval model does not have access to the target document collection. In contrast, it does have access to a brief textual description that explains the target domain. We define a taxonomy of domain attributes in retrieval tasks to understand different properties of a source domain that can be adapted to a target domain
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02738</link><description>&lt;p&gt;
RecallM:&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#38382;&#39064;&#22238;&#31572;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
RecallM: An Architecture for Temporal Context Understanding and Question Answering. (arXiv:2307.02738v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RecallM&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#23558;&#20026;&#36830;&#32493;&#23398;&#20064;&#12289;&#22797;&#26434;&#25512;&#29702;&#21644;&#23398;&#20064;&#24207;&#21015;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#25171;&#19979;&#22522;&#30784;&#12290;&#21019;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#26041;&#27861;&#23454;&#29616;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#19987;&#27880;&#20110;&#20026;AGI&#31995;&#32479;&#21019;&#24314;&#21487;&#36866;&#24212;&#21644;&#21487;&#26356;&#26032;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#23637;&#31034;&#20102;RecallM&#26550;&#26500;&#30340;&#22909;&#22788;&#65292;&#29305;&#21035;&#26159;&#23427;&#25552;&#20379;&#30340;&#25913;&#36827;&#30340;&#26102;&#38388;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ideal long-term memory mechanism for Large Language Model (LLM) based chatbots, would lay the foundation for continual learning, complex reasoning and allow sequential and temporal dependencies to be learnt. Creating this type of memory mechanism is an extremely challenging problem. In this paper we explore different methods of achieving the effect of long-term memory. We propose a new architecture focused on creating adaptable and updatable long-term memory for AGI systems. We demonstrate through various experiments the benefits of the RecallM architecture, particularly the improved temporal understanding it provides.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#24615;&#12289;&#38382;&#31572;&#12289;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#12290;&#36890;&#36807;&#23545;RoBERTa&#36827;&#34892;&#36731;&#37327;&#32423;&#24494;&#35843;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02729</link><description>&lt;p&gt;
&#25991;&#26412;&#23545;&#40784;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29992;&#20110;&#28023;&#37327;NLP&#20219;&#21153;&#30340;&#32479;&#19968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text Alignment Is An Efficient Unified Model for Massive NLP Tasks. (arXiv:2307.02729v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25991;&#26412;&#23545;&#40784;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#24615;&#12289;&#38382;&#31572;&#12289;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#12290;&#36890;&#36807;&#23545;RoBERTa&#36827;&#34892;&#36731;&#37327;&#32423;&#24494;&#35843;&#65292;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#26356;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#34987;&#35774;&#35745;&#20026;&#19979;&#19968;&#20010;&#35789;&#35821;&#39044;&#27979;&#30340;&#20989;&#25968;&#65292;&#22312;&#24191;&#27867;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#20855;&#26377;&#24191;&#27867;&#24615;&#65292;&#20294;&#19979;&#19968;&#20010;&#35789;&#35821;&#39044;&#27979;&#23545;&#20110;&#35768;&#22810;&#20219;&#21153;&#26469;&#35828;&#36890;&#24120;&#19981;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#34920;&#36798;&#26041;&#24335;&#65292;&#38656;&#35201;&#26497;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#25968;&#30334;&#20159;&#32423;&#21035;&#65289;&#65292;&#26377;&#26102;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#23454;&#38469;&#19978;&#65292;&#26500;&#24314;&#26356;&#39640;&#25928;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#21487;&#21462;&#30340;&#8212;&#8212;&#23613;&#31649;&#19981;&#22815;&#36890;&#29992;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#36866;&#29992;&#20110;&#22823;&#37327;&#38382;&#39064;&#30340;&#23376;&#38598;&#65292;&#24182;&#20197;&#26356;&#23567;&#30340;&#27169;&#22411;&#35268;&#27169;&#23454;&#29616;&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25991;&#26412;&#23545;&#40784;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#30340;&#32479;&#19968;&#27169;&#22411;&#65292;&#29992;&#20110;&#28041;&#21450;&#25991;&#26412;&#34164;&#21547;&#12289;&#30456;&#20284;&#24615;&#12289;&#38382;&#31572;&#65288;&#21644;&#21487;&#22238;&#31572;&#24615;&#65289;&#12289;&#20107;&#23454;&#19968;&#33268;&#24615;&#31561;&#20851;&#38190;&#20219;&#21153;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#32473;&#23450;&#19968;&#23545;&#25991;&#26412;&#65292;&#35813;&#27169;&#22411;&#27979;&#37327;&#23427;&#20204;&#20043;&#38388;&#20449;&#24687;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;RoBERTa&#65288;3.55&#20159;&#21442;&#25968;&#65289;&#36827;&#34892;&#36731;&#37327;&#32423;&#24494;&#35843;&#26469;&#23454;&#20363;&#21270;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#65288;Align&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), typically designed as a function of next-word prediction, have excelled across extensive NLP tasks. Despite the generality, next-word prediction is often not an efficient formulation for many of the tasks, demanding an extreme scale of model parameters (10s or 100s of billions) and sometimes yielding suboptimal performance. In practice, it is often desirable to build more efficient models -- despite being less versatile, they still apply to a substantial subset of problems, delivering on par or even superior performance with much smaller model sizes. In this paper, we propose text alignment as an efficient unified model for a wide range of crucial tasks involving text entailment, similarity, question answering (and answerability), factual consistency, and so forth. Given a pair of texts, the model measures the degree of alignment between their information. We instantiate an alignment model (Align) through lightweight finetuning of RoBERTa (355M parameters)
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#65292;&#20197;&#24212;&#29992;&#20110;&#35774;&#22791;&#20869;&#20851;&#38190;&#35789;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#36164;&#28304;&#24773;&#20917;&#19979;&#23558;&#30693;&#35782;&#20174;&#22797;&#26434;&#27169;&#22411;&#20256;&#36882;&#32473;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02720</link><description>&lt;p&gt;
&#22522;&#20110;&#35774;&#22791;&#38480;&#21046;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#21450;&#30693;&#35782;&#33976;&#39311;&#65288;arXiv:2307.02720v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation. (arXiv:2307.02720v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02720
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#26550;&#26500;&#65292;&#20197;&#24212;&#29992;&#20110;&#35774;&#22791;&#20869;&#20851;&#38190;&#35789;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#36164;&#28304;&#24773;&#20917;&#19979;&#23558;&#30693;&#35782;&#20174;&#22797;&#26434;&#27169;&#22411;&#20256;&#36882;&#32473;&#36731;&#37327;&#32423;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#33258;&#30417;&#30563;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20294;&#22312;&#35774;&#22791;&#20869;&#39044;&#31639;&#38480;&#21046;&#21644;&#26377;&#20559;&#24046;&#30340;&#25968;&#25454;&#38598;&#25910;&#38598;&#19979;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20851;&#38190;&#35789;&#26816;&#27979;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#33976;&#39311;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#65288;S3RL&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#22791;&#20869;&#20851;&#38190;&#35789;&#26816;&#27979;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#35270;&#22270;&#20114;&#30456;&#20851;&#33976;&#39311;&#21644;&#25945;&#24072;&#30340;&#30721;&#26412;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#20174;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#27169;&#22411;&#20013;&#20256;&#36882;&#30693;&#35782;&#21040;&#26356;&#23567;&#12289;&#36731;&#37327;&#32423;&#30340;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;16.6k&#23567;&#26102;&#30340;&#20869;&#37096;&#25968;&#25454;&#38598;&#65292;&#22312;Alexa&#30340;&#20851;&#38190;&#35789;&#26816;&#27979;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#27491;&#24120;&#21644;&#22122;&#22768;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#22312;&#35774;&#22791;&#36164;&#28304;&#38480;&#21046;&#19979;&#26500;&#24314;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.
&lt;/p&gt;</description></item><item><title>CFSum&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#30340;&#31895;&#21040;&#32454;&#36129;&#29486;&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#35745;&#31639;&#21644;&#21033;&#29992;&#22270;&#20687;&#22312;&#25688;&#35201;&#20013;&#30340;&#19981;&#21516;&#36129;&#29486;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02716</link><description>&lt;p&gt;
CFSum: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#30340;&#31895;&#21040;&#32454;&#36129;&#29486;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
CFSum: A Coarse-to-Fine Contribution Network for Multimodal Summarization. (arXiv:2307.02716v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02716
&lt;/p&gt;
&lt;p&gt;
CFSum&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#30340;&#31895;&#21040;&#32454;&#36129;&#29486;&#32593;&#32476;&#65292;&#33021;&#22815;&#20934;&#30830;&#35745;&#31639;&#21644;&#21033;&#29992;&#22270;&#20687;&#22312;&#25688;&#35201;&#20013;&#30340;&#19981;&#21516;&#36129;&#29486;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25688;&#35201;&#36890;&#24120;&#23384;&#22312;&#35270;&#35273;&#27169;&#24577;&#36129;&#29486;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#25688;&#35201;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#35774;&#35745;&#19981;&#21516;&#27169;&#24577;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#32780;&#24573;&#35270;&#20102;&#35270;&#35273;&#27169;&#24577;&#26377;&#29992;&#30340;&#33258;&#36866;&#24212;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31895;&#21040;&#32454;&#36129;&#29486;&#32593;&#32476;&#29992;&#20110;&#22810;&#27169;&#24577;&#25688;&#35201;&#65288;CFSum&#65289;&#65292;&#20197;&#32771;&#34385;&#22270;&#20687;&#22312;&#25688;&#35201;&#20013;&#30340;&#19981;&#21516;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#28040;&#38500;&#26080;&#29992;&#22270;&#20687;&#30340;&#24178;&#25200;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39044;&#36807;&#28388;&#27169;&#22359;&#26469;&#33293;&#24323;&#26080;&#29992;&#22270;&#20687;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#20934;&#30830;&#20351;&#29992;&#26377;&#29992;&#22270;&#20687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#23618;&#27425;&#30340;&#35270;&#35273;&#34917;&#20805;&#27169;&#22359;&#65292;&#35789;&#32423;&#21644;&#30701;&#35821;&#32423;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35745;&#31639;&#22270;&#20687;&#36129;&#29486;&#24182;&#29992;&#20110;&#24341;&#23548;&#25991;&#26412;&#21644;&#35270;&#35273;&#27169;&#24577;&#30340;&#27880;&#24847;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CFSum&#22312;&#26631;&#20934;&#22522;&#20934;&#19978;&#26126;&#26174;&#20248;&#20110;&#22810;&#20010;&#24378;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal summarization usually suffers from the problem that the contribution of the visual modality is unclear. Existing multimodal summarization approaches focus on designing the fusion methods of different modalities, while ignoring the adaptive conditions under which visual modalities are useful. Therefore, we propose a novel Coarse-to-Fine contribution network for multimodal Summarization (CFSum) to consider different contributions of images for summarization. First, to eliminate the interference of useless images, we propose a pre-filter module to abandon useless images. Second, to make accurate use of useful images, we propose two levels of visual complement modules, word level and phrase level. Specifically, image contributions are calculated and are adopted to guide the attention of both textual and visual modalities. Experimental results have shown that CFSum significantly outperforms multiple strong baselines on the standard benchmark. Furthermore, the analysis verifies th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#21147;&#23398;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26641;&#32467;&#26500;&#30340;Strahler&#25968;&#30340;&#19978;&#19979;&#38480;&#65292;&#21457;&#29616;&#23427;&#20960;&#20046;&#24635;&#26159;3&#25110;4&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#22788;&#29702;&#21477;&#23376;&#25152;&#38656;&#35760;&#24518;&#37327;&#30340;&#19979;&#38480;&#12290;&#21516;&#26102;&#65292;&#23545;&#38543;&#26426;&#26641;&#36827;&#34892;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;Strahler&#25968;&#30340;&#22686;&#38271;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29305;&#24449;&#30340;&#32479;&#35745;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2307.02697</link><description>&lt;p&gt;
Strahler&#25968;&#30340;&#32479;&#35745;&#21147;&#23398;&#65306;&#22522;&#20110;&#38543;&#26426;&#21644;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Statistical Mechanics of Strahler Number via Random and Natural Language Sentences. (arXiv:2307.02697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32479;&#35745;&#21147;&#23398;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26641;&#32467;&#26500;&#30340;Strahler&#25968;&#30340;&#19978;&#19979;&#38480;&#65292;&#21457;&#29616;&#23427;&#20960;&#20046;&#24635;&#26159;3&#25110;4&#65292;&#24182;&#35777;&#26126;&#23427;&#26159;&#22788;&#29702;&#21477;&#23376;&#25152;&#38656;&#35760;&#24518;&#37327;&#30340;&#19979;&#38480;&#12290;&#21516;&#26102;&#65292;&#23545;&#38543;&#26426;&#26641;&#36827;&#34892;&#30340;&#20998;&#26512;&#25581;&#31034;&#20986;Strahler&#25968;&#30340;&#22686;&#38271;&#27169;&#24335;&#65292;&#25581;&#31034;&#20102;&#23427;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29305;&#24449;&#30340;&#32479;&#35745;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Strahler&#25968;&#26368;&#21021;&#34987;&#25552;&#20986;&#29992;&#20110;&#25551;&#36848;&#27827;&#27969;&#20998;&#25903;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25214;&#21040;&#20102;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35745;&#31639;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26641;&#32467;&#26500;&#30340;Strahler&#25968;&#19978;&#19979;&#38480;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#32467;&#26500;&#21487;&#20197;&#22312;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#32479;&#35745;&#21147;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#35821;&#27861;&#27880;&#37322;&#25968;&#25454;&#30340;&#32463;&#39564;&#24615;&#27979;&#37327;&#65292;&#26174;&#31034;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#30340;Strahler&#25968;&#20960;&#20046;&#24635;&#26159;3&#25110;4&#65292;&#19982;Strahler&#65288;1957&#24180;&#65289;&#21644;Horton&#65288;1945&#24180;&#65289;&#25253;&#36947;&#30340;&#27827;&#27969;&#20998;&#27969;&#24773;&#20917;&#31867;&#20284;&#12290;&#20174;&#35813;&#25968;&#20540;&#30340;&#29702;&#35770;&#35266;&#28857;&#20986;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#23427;&#26159;&#22312;&#29305;&#23450;&#27169;&#22411;&#19979;&#22788;&#29702;&#21477;&#23376;&#25152;&#38656;&#35760;&#24518;&#37327;&#30340;&#19979;&#38480;&#12290;&#23545;&#38543;&#26426;&#26641;&#36827;&#34892;&#30340;&#25968;&#23398;&#20998;&#26512;&#36827;&#19968;&#27493;&#20551;&#35774;&#20102;Strahler&#25968;&#30340;&#24615;&#36136;&#65292;&#25581;&#31034;&#20986;&#23427;&#24182;&#38750;&#24120;&#25968;&#32780;&#26159;&#20197;&#23545;&#25968;&#24418;&#24335;&#22686;&#38271;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;Strahler&#25968;&#20316;&#20026;&#25551;&#36848;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#29305;&#24449;&#30340;&#32479;&#35745;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Strahler number was originally proposed to characterize the complexity of river bifurcation and has found various applications. This article proposes computation of the Strahler number's upper and lower limits for natural language sentence tree structures, which are available in a large dataset allowing for statistical mechanics analysis.  Through empirical measurements across grammatically annotated data, the Strahler number of natural language sentences is shown to be almost always 3 or 4, similar to the case of river bifurcation as reported by Strahler (1957) and Horton (1945).  From the theory behind the number, we show that it is the lower limit of the amount of memory required to process sentences under a particular model. A mathematical analysis of random trees provides a further conjecture on the nature of the Strahler number, revealing that it is not a constant but grows logarithmically. This finding uncovers the statistical basics behind the Strahler number as a character
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#28436;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#38480;&#21046;&#19982;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.02690</link><description>&lt;p&gt;
&#20351;&#29992;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#25193;&#23637;&#19978;&#19979;&#25991;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
Scaling In-Context Demonstrations with Structured Attention. (arXiv:2307.02690v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#28436;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#38480;&#21046;&#19982;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20852;&#36215;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#20174;&#23569;&#25968;&#28436;&#31034;&#20013;&#8220;&#23398;&#20064;&#8221;&#25191;&#34892;&#20219;&#21153;&#32780;&#26080;&#38656;&#36827;&#34892;&#21442;&#25968;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#33021;&#21147;&#21463;&#21040;&#27169;&#22411;&#26550;&#26500;&#30340;&#38480;&#21046;&#65306;1&#65289;&#30001;&#20110;&#20301;&#32622;&#23884;&#20837;&#65292;&#28436;&#31034;&#30340;&#20351;&#29992;&#21463;&#21040;&#26368;&#22823;&#21477;&#23376;&#38271;&#24230;&#30340;&#38480;&#21046;&#65307;2&#65289;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#38459;&#30861;&#29992;&#25143;&#26377;&#25928;&#20351;&#29992;&#26356;&#22810;&#30340;&#28436;&#31034;&#65307;3&#65289;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23545;&#28436;&#31034;&#30340;&#39034;&#24207;&#25935;&#24863;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#26356;&#22909;&#30340;&#26550;&#26500;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SAICL&#65288;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#65289;&#65292;&#23427;&#36890;&#36807;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#26367;&#25442;&#20840;&#27880;&#24847;&#21147;&#65292;&#24182;&#28040;&#38500;&#20102;&#20010;&#21035;&#28436;&#31034;&#20043;&#38388;&#19981;&#24517;&#35201;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#21516;&#26102;&#20351;&#27169;&#22411;&#23545;&#28436;&#31034;&#30340;&#25490;&#21015;&#19981;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of large language models (LLMs) highlights their ability to perform in-context learning, i.e., "learning" to perform a task from a few demonstrations in the context without any parameter updates. However, their capabilities of in-context learning are limited by the model architecture: 1) the use of demonstrations is constrained by a maximum sentence length due to positional embeddings; 2) the quadratic complexity of attention hinders users from using more demonstrations efficiently; 3) LLMs are shown to be sensitive to the order of the demonstrations. In this work, we tackle these challenges by proposing a better architectural design for in-context learning. We propose SAICL (Structured Attention for In-Context Learning), which replaces the full-attention by a structured attention mechanism designed for in-context learning, and removes unnecessary dependencies between individual demonstrations, while making the model invariant to the permutation of demonstrations. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;NESTA&#65292;&#36890;&#36807;&#23398;&#20064;&#25277;&#35937;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20316;&#20026;&#31574;&#30053;&#65292;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.02689</link><description>&lt;p&gt;
&#23398;&#20064;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#30340;&#31526;&#21495;&#35268;&#21017;&#20197;&#36827;&#34892;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Symbolic Rules over Abstract Meaning Representations for Textual Reinforcement Learning. (arXiv:2307.02689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#30340;&#25991;&#26412;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;NESTA&#65292;&#36890;&#36807;&#23398;&#20064;&#25277;&#35937;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20316;&#20026;&#31574;&#30053;&#65292;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#27604;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26356;&#22909;&#30340;&#27867;&#21270;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#36890;&#24120;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#23884;&#20837;&#24335;&#34920;&#31034;&#23398;&#20064;&#26080;&#27861;&#35299;&#37322;&#30340;&#31574;&#30053;&#65292;&#24448;&#24448;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#30693;&#30340;&#28216;&#25103;&#12290;&#32780;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21033;&#29992;&#20013;&#38388;&#24418;&#24335;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#20174;&#20869;&#37096;&#21487;&#35299;&#37322;&#24615;&#12289;&#23545;&#35757;&#32451;&#25968;&#25454;&#35201;&#27714;&#36739;&#23569;&#20197;&#21450;&#22312;&#26410;&#30693;&#25968;&#25454;&#24773;&#20917;&#19979;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#20248;&#21183;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#30340;NEuro-Symbolic Textual Agent (NESTA)&#26041;&#27861;&#65292;&#23427;&#23558;&#36890;&#29992;&#30340;&#35821;&#20041;&#35299;&#26512;&#22120;&#19982;&#35268;&#21017;&#24402;&#32435;&#31995;&#32479;&#30456;&#32467;&#21512;&#65292;&#23398;&#20064;&#25277;&#35937;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20316;&#20026;&#31574;&#30053;&#12290;&#25105;&#20204;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;NESTA&#26041;&#27861;&#22312;&#26410;&#30693;&#27979;&#35797;&#28216;&#25103;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#33021;&#22815;&#20174;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based reinforcement learning agents have predominantly been neural network-based models with embeddings-based representation, learning uninterpretable policies that often do not generalize well to unseen games. On the other hand, neuro-symbolic methods, specifically those that leverage an intermediate formal representation, are gaining significant attention in language understanding tasks. This is because of their advantages ranging from inherent interpretability, the lesser requirement of training data, and being generalizable in scenarios with unseen data. Therefore, in this paper, we propose a modular, NEuro-Symbolic Textual Agent (NESTA) that combines a generic semantic parser with a rule induction system to learn abstract interpretable rules as policies. Our experiments on established text-based game benchmarks show that the proposed NESTA method outperforms deep reinforcement learning-based techniques by achieving better generalization to unseen test games and learning from 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#20351;&#29992;&#35270;&#39057;&#21644;&#26631;&#27880;&#65292;&#32780;&#26159;&#22312;&#27979;&#35797;&#26102;&#20165;&#20248;&#21270;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25991;&#26412;&#21644;&#26102;&#21051;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#25551;&#36848;&#20107;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.02682</link><description>&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25991;&#26412;&#21644;&#26102;&#21051;&#23454;&#29616;&#38646;&#26679;&#26412;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Dense Video Captioning by Jointly Optimizing Text and Moment. (arXiv:2307.02682v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#20351;&#29992;&#35270;&#39057;&#21644;&#26631;&#27880;&#65292;&#32780;&#26159;&#22312;&#27979;&#35797;&#26102;&#20165;&#20248;&#21270;&#36755;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#30340;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#25991;&#26412;&#21644;&#26102;&#21051;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#22320;&#23450;&#20301;&#21644;&#25551;&#36848;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26159;&#19968;&#39033;&#23558;&#26377;&#24847;&#20041;&#30340;&#26102;&#21051;&#23450;&#20301;&#21644;&#30456;&#20851;&#23383;&#24149;&#29983;&#25104;&#24212;&#29992;&#20110;&#35270;&#39057;&#20013;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#26114;&#36149;&#30340;&#24102;&#26377;&#26631;&#27880;&#30340;&#35270;&#39057;&#29255;&#27573;&#21644;&#25991;&#26412;&#30340;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#38477;&#20302;&#26631;&#27880;&#25104;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#26412;&#23494;&#38598;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;ZeroTA&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#20219;&#20309;&#35270;&#39057;&#25110;&#26631;&#27880;&#65292;&#32780;&#26159;&#36890;&#36807;&#20165;&#22312;&#36755;&#20837;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#22312;&#27979;&#35797;&#26102;&#23450;&#20301;&#21644;&#25551;&#36848;&#27599;&#20010;&#36755;&#20837;&#35270;&#39057;&#20013;&#30340;&#20107;&#20214;&#12290;&#36825;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#34920;&#31034;&#35270;&#39057;&#20013;&#30340;&#26102;&#38388;&#27573;&#30340;&#36719;&#26102;&#21051;&#25513;&#30721;&#65292;&#24182;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#32512;&#21442;&#25968;&#36827;&#34892;&#32852;&#21512;&#20248;&#21270;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#32852;&#21512;&#20248;&#21270;&#36890;&#36807;&#26368;&#22823;&#21270;&#29983;&#25104;&#25991;&#26412;&#19982;&#35270;&#39057;&#20013;&#30340;&#26576;&#20010;&#26102;&#21051;&#20043;&#38388;&#30340;&#21305;&#37197;&#20998;&#25968;&#65292;&#23558;&#22266;&#23450;&#30340;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;&#21363;GPT-2&#65289;&#19982;&#22266;&#23450;&#30340;&#35270;&#35273;-&#35821;&#35328;&#23545;&#27604;&#27169;&#22411;&#65288;&#21363;CLIP&#65289;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#23545;&#19968;&#30340;&#26102;&#38388;IoU&#25439;&#22833;&#65292;&#20351;&#19968;&#32452;&#36719;&#26102;&#21051;&#25513;&#30721;&#20043;&#38388;&#36827;&#34892;&#27604;&#23545;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense video captioning, a task of localizing meaningful moments and generating relevant captions for videos, often requires a large, expensive corpus of annotated video segments paired with text. In an effort to minimize the annotation cost, we propose ZeroTA, a novel method for dense video captioning in a zero-shot manner. Our method does not require any videos or annotations for training; instead, it localizes and describes events within each input video at test time by optimizing solely on the input. This is accomplished by introducing a soft moment mask that represents a temporal segment in the video and jointly optimizing it with the prefix parameters of a language model. This joint optimization aligns a frozen language generation model (i.e., GPT-2) with a frozen vision-language contrastive model (i.e., CLIP) by maximizing the matching score between the generated text and a moment within the video. We also introduce a pairwise temporal IoU loss to let a set of soft moment masks c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#25972;&#24418;&#25163;&#26415;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#21019;&#24314;&#29305;&#24449;&#12289;&#23398;&#20064;&#21644;&#29983;&#25104;&#20027;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#33258;&#21160;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2307.02640</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#31038;&#20132;&#23186;&#20307;&#25972;&#24418;&#25163;&#26415;&#24086;&#23376;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Sentiment Analysis of Plastic Surgery Social Media Posts. (arXiv:2307.02640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#23545;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#25972;&#24418;&#25163;&#26415;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#21019;&#24314;&#29305;&#24449;&#12289;&#23398;&#20064;&#21644;&#29983;&#25104;&#20027;&#39064;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#30340;&#33258;&#21160;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#29992;&#25143;&#24086;&#23376;&#30340;&#22823;&#37327;&#25910;&#38598;&#20027;&#35201;&#30001;&#20110;&#25991;&#26412;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36895;&#24230;&#65292;&#32780;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#29992;&#20363;&#20013;&#26410;&#34987;&#20351;&#29992;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26159;AI&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#21033;&#29992;&#25991;&#26723;&#38598;&#21512;&#65288;&#31216;&#20026;&#35821;&#26009;&#24211;&#65289;&#23545;&#35745;&#31639;&#26426;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#30340;&#35757;&#32451;&#12290;&#21033;&#29992;&#35789;&#39057;-&#36870;&#25991;&#26723;&#39057;&#29575;&#65288;TF-IDF&#65289;&#30340;&#35789;&#25490;&#21517;&#26041;&#27861;&#65292;&#22312;&#25991;&#26723;&#20043;&#38388;&#21019;&#24314;&#29305;&#24449;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#30340;&#20998;&#26512;&#65292;&#21363;&#26426;&#22120;&#23398;&#20064;&#22312;&#27809;&#26377;&#20154;&#24037;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#25991;&#26723;&#36827;&#34892;&#20998;&#32452;&#12290;&#38024;&#23545;&#25317;&#26377;&#25968;&#21315;&#20010;&#29305;&#24449;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#21033;&#29992;t-&#20998;&#24067;&#38543;&#26426;&#37051;&#22495;&#23884;&#20837;&#65288;t-SNE&#65289;&#12289;K-&#22343;&#20540;&#32858;&#31867;&#21644;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#24067;&#65288;LDA&#65289;&#26469;&#23398;&#20064;&#39030;&#32423;&#35789;&#27719;&#24182;&#29983;&#25104;Reddit&#21644;Twitter&#30340;&#21512;&#24182;&#35821;&#26009;&#24211;&#30340;&#20027;&#39064;&#12290;&#21033;&#29992;&#38750;&#24120;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#26080;&#30417;&#30563;&#20998;&#26512;&#30340;&#24212;&#29992;&#32467;&#26524;&#65292;&#20351;&#24471;&#35745;&#31639;&#26426;&#33021;&#22815;&#33258;&#21160;&#36827;&#34892;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The massive collection of user posts across social media platforms is primarily untapped for artificial intelligence (AI) use cases based on the sheer volume and velocity of textual data. Natural language processing (NLP) is a subfield of AI that leverages bodies of documents, known as corpora, to train computers in human-like language understanding. Using a word ranking method, term frequency-inverse document frequency (TF-IDF), to create features across documents, it is possible to perform unsupervised analytics, machine learning (ML) that can group the documents without a human manually labeling the data. For large datasets with thousands of features, t-distributed stochastic neighbor embedding (t-SNE), k-means clustering and Latent Dirichlet allocation (LDA) are employed to learn top words and generate topics for a Reddit and Twitter combined corpus. Using extremely simple deep learning models, this study demonstrates that the applied results of unsupervised analysis allow a comput
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SkipDecode&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26631;&#35760;&#32423;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#65292;&#33021;&#22815;&#19982;&#25209;&#37327;&#25512;&#29702;&#21644;KV&#32531;&#23384;&#26080;&#32541;&#37197;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.02628</link><description>&lt;p&gt;
SkipDecode&#65306;&#29992;&#20110;&#39640;&#25928;LLM&#25512;&#29702;&#30340;&#33258;&#22238;&#24402;&#36339;&#36291;&#35299;&#30721;&#26041;&#27861;&#65288;arXiv:2307.02628v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference. (arXiv:2307.02628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SkipDecode&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26631;&#35760;&#32423;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#65292;&#33021;&#22815;&#19982;&#25209;&#37327;&#25512;&#29702;&#21644;KV&#32531;&#23384;&#26080;&#32541;&#37197;&#21512;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36880;&#20010;&#29983;&#25104;&#26631;&#35760;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#65292;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24310;&#36831;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20351;&#29992;&#26089;&#26399;&#36864;&#20986;&#31574;&#30053;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#36825;&#20123;&#31574;&#30053;&#33021;&#22815;&#22312;&#19981;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#23436;&#25972;&#35745;&#31639;&#22270;&#30340;&#24773;&#20917;&#19979;&#21152;&#24555;&#25991;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26631;&#35760;&#32423;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22312;&#22312;&#32447;&#25512;&#29702;&#20013;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#36731;&#26131;&#24212;&#29992;&#20110;&#25209;&#37327;&#25512;&#29702;&#21644;&#38190;&#20540;&#32531;&#23384;&#12290;&#36825;&#26159;&#22240;&#20026;&#23427;&#20204;&#24517;&#39035;&#31561;&#21040;&#25209;&#37327;&#20013;&#30340;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#36864;&#20986;&#21518;&#25165;&#33021;&#20572;&#27490;&#35745;&#31639;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#36825;&#31181;&#25216;&#26415;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26631;&#35760;&#32423;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;SkipDecode&#65292;&#23427;&#33021;&#22815;&#19982;&#25209;&#37327;&#25512;&#29702;&#21644;KV&#32531;&#23384;&#26080;&#32541;&#37197;&#21512;&#12290;&#23427;&#20811;&#26381;&#20102;&#20808;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive large language models (LLMs) have made remarkable progress in various natural language generation tasks. However, they incur high computation cost and latency resulting from the autoregressive token-by-token generation. To address this issue, several approaches have been proposed to reduce computational cost using early-exit strategies. These strategies enable faster text generation using reduced computation without applying the full computation graph to each token. While existing token-level early exit methods show promising results for online inference, they cannot be readily applied for batch inferencing and Key-Value caching. This is because they have to wait until the last token in a batch exits before they can stop computing. This severely limits the practical application of such techniques. In this paper, we propose a simple and effective token-level early exit method, SkipDecode, designed to work seamlessly with batch inferencing and KV caching. It overcomes prio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#21644;&#28176;&#36827;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#36807;&#31243;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2307.02615</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#31867;&#21551;&#21457;&#30340;&#28176;&#36827;&#23545;&#40784;&#21644;&#27604;&#36739;&#23398;&#20064;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Human Inspired Progressive Alignment and Comparative Learning for Grounded Word Acquisition. (arXiv:2307.02615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02615
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#21644;&#28176;&#36827;&#23545;&#40784;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#30340;&#36807;&#31243;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#35813;&#26041;&#27861;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#35821;&#35328;&#20064;&#24471;&#26159;&#19968;&#31181;&#39640;&#25928;&#12289;&#21463;&#30417;&#30563;&#21644;&#25345;&#32493;&#30340;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#20511;&#37492;&#20102;&#20154;&#31867;&#23156;&#20799;&#20064;&#24471;&#31532;&#19968;&#38376;&#35821;&#35328;&#30340;&#26041;&#24335;&#65292;&#36890;&#36807;&#27604;&#36739;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#35789;&#27719;&#33719;&#21462;&#30340;&#35745;&#31639;&#36807;&#31243;&#12290;&#21463;&#35748;&#30693;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#20351;&#35745;&#31639;&#27169;&#22411;&#33021;&#22815;&#27604;&#36739;&#21508;&#31181;&#23646;&#24615;&#30340;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#65292;&#23398;&#20064;&#36807;&#28388;&#20986;&#24182;&#25552;&#21462;&#20849;&#21516;&#30340;&#20449;&#24687;&#29992;&#20110;&#27599;&#20010;&#20849;&#20139;&#30340;&#35821;&#35328;&#26631;&#31614;&#12290;&#25105;&#20204;&#23558;&#35789;&#27719;&#33719;&#21462;&#26694;&#26550;&#23450;&#20041;&#20026;&#26082;&#21253;&#25324;&#20449;&#24687;&#36807;&#28388;&#36807;&#31243;&#65292;&#20063;&#21253;&#25324;&#34920;&#24449;-&#31526;&#21495;&#26144;&#23556;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#19981;&#28041;&#21450;&#22266;&#23450;&#30340;&#35789;&#27719;&#37327;&#22823;&#23567;&#65292;&#20063;&#19981;&#28041;&#21450;&#26377;&#21306;&#20998;&#24615;&#30340;&#30446;&#26631;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25345;&#32493;&#39640;&#25928;&#22320;&#23398;&#20064;&#26356;&#22810;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#25511;&#21046;&#23454;&#39564;&#20013;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#25928;&#22320;&#25345;&#32493;&#23398;&#20064;&#22522;&#20110;&#32463;&#39564;&#30340;&#35789;&#27719;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human language acquisition is an efficient, supervised, and continual process. In this work, we took inspiration from how human babies acquire their first language, and developed a computational process for word acquisition through comparative learning. Motivated by cognitive findings, we generated a small dataset that enables the computation models to compare the similarities and differences of various attributes, learn to filter out and extract the common information for each shared linguistic label. We frame the acquisition of words as not only the information filtration process, but also as representation-symbol mapping. This procedure does not involve a fixed vocabulary size, nor a discriminative objective, and allows the models to continually learn more concepts efficiently. Our results in controlled experiments have shown the potential of this approach for efficient continual learning of grounded words.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2307.02599</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#20010;&#31354;&#26684;&#32469;&#36807;ChatGPT&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evade ChatGPT Detectors via A Single Space. (arXiv:2307.02599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#21069;&#30340;ChatGPT&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#32780;&#19968;&#20010;&#39069;&#22806;&#30340;&#31354;&#26684;&#25104;&#20026;&#20102;&#35268;&#36991;&#26816;&#27979;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#24102;&#26469;&#20102;&#38761;&#21629;&#24615;&#30340;&#31038;&#20250;&#20215;&#20540;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20110;AI&#29983;&#25104;&#20869;&#23481;&#28389;&#29992;&#30340;&#25285;&#24551;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#26816;&#27979;&#20986;&#20869;&#23481;&#26159;&#30001;ChatGPT&#29983;&#25104;&#36824;&#26159;&#20154;&#31867;&#29983;&#25104;&#30340;&#12290;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#26159;&#24314;&#31435;&#22312;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#30340;&#20551;&#35774;&#19978;&#30340;&#12290;&#36825;&#20123;&#24046;&#36317;&#36890;&#24120;&#26159;&#36890;&#36807;&#32479;&#35745;&#20449;&#24687;&#25110;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36136;&#30097;&#20102;&#26816;&#27979;&#22120;&#20013;&#30340;&#20998;&#24067;&#24046;&#36317;&#20551;&#35774;&#12290;&#25105;&#20204;&#21457;&#29616;&#26816;&#27979;&#22120;&#19981;&#33021;&#26377;&#25928;&#22320;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#21644;&#39118;&#26684;&#24046;&#36317;&#12290;&#30456;&#21453;&#65292;"&#24494;&#23567;&#30340;&#24046;&#24322;"&#65292;&#22914;&#39069;&#22806;&#30340;&#19968;&#20010;&#31354;&#26684;&#65292;&#22312;&#26816;&#27979;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpaceInfi&#31574;&#30053;&#26469;&#35268;&#36991;&#26816;&#27979;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#31574;&#30053;&#22312;&#22810;&#20010;&#22522;&#20934;&#21644;&#26816;&#27979;&#22120;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23545;&#20026;&#20160;&#20040;SpaceInfi&#33021;&#25104;&#21151;&#35268;&#36991;&#26816;&#27979;&#25552;&#20379;&#20102;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT brings revolutionary social value but also raises concerns about the misuse of AI-generated content. Consequently, an important question is how to detect whether content is generated by ChatGPT or by human. Existing detectors are built upon the assumption that there are distributional gaps between human-generated and AI-generated content. These gaps are typically identified using statistical information or classifiers. Our research challenges the distributional gap assumption in detectors. We find that detectors do not effectively discriminate the semantic and stylistic gaps between human-generated and AI-generated content. Instead, the "subtle differences", such as an extra space, become crucial for detection. Based on this discovery, we propose the SpaceInfi strategy to evade detection. Experiments demonstrate the effectiveness of this strategy across multiple benchmarks and detectors. We also provide a theoretical explanation for why SpaceInfi is successful in evading perple
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.02570</link><description>&lt;p&gt;
&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;
&lt;/p&gt;
&lt;p&gt;
Named Entity Inclusion in Abstractive Text Summarization. (arXiv:2307.02570v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02570
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#20013;&#21629;&#21517;&#23454;&#20307;&#36951;&#28431;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#35757;&#32451;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#30340;&#21253;&#21547;&#24773;&#20917;&#65292;&#25552;&#39640;&#20102;&#25688;&#35201;&#30340;&#20934;&#30830;&#24615;&#21644;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#35768;&#22810;&#24403;&#21069;&#25277;&#35937;&#25991;&#26412;&#25688;&#35201;&#22120;&#30340;&#32570;&#28857;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#30340;&#36951;&#28431;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#35758;&#37319;&#29992;&#23450;&#21046;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#30340;&#27880;&#24847;&#21147;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;RoBERTa&#26469;&#30830;&#23450;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35813;&#27169;&#22411;&#23545;&#25991;&#26412;&#20013;&#30340;&#21629;&#21517;&#23454;&#20307;&#36827;&#34892;&#23631;&#34109;&#65292;&#20877;&#20351;&#29992;BART&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#37325;&#24314;&#12290;&#25509;&#19979;&#26469;&#65292;&#23558;BART&#27169;&#22411;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#39044;&#35757;&#32451;&#26041;&#27861;&#25913;&#21892;&#20102;&#21629;&#21517;&#23454;&#20307;&#21253;&#21547;&#30340;&#31934;&#30830;&#24230;&#21644;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the named entity omission - the drawback of many current abstractive text summarizers. We suggest a custom pretraining objective to enhance the model's attention on the named entities in a text. At first, the named entity recognition model RoBERTa is trained to determine named entities in the text. After that, this model is used to mask named entities in the text and the BART model is trained to reconstruct them. Next, the BART model is fine-tuned on the summarization task. Our experiments showed that this pretraining approach improves named entity inclusion precision and recall metrics.
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#24515;&#33039;&#30149;&#21644;&#34880;&#31649;&#30149;&#29702;&#23398;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22238;&#31572;&#25361;&#25112;&#24615;&#22810;&#39033;&#36873;&#25321;&#39064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#21307;&#23398;&#29983;&#65292;&#26174;&#31034;&#20102;ChatGPT&#22312;&#25552;&#20379;&#20934;&#30830;&#31572;&#26696;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.02518</link><description>&lt;p&gt;
&#20998;&#26512;ChatGPT&#22312;&#24515;&#33039;&#30149;&#21644;&#34880;&#31649;&#30149;&#29702;&#23398;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Performance of ChatGPT in Cardiology and Vascular Pathologies. (arXiv:2307.02518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02518
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;ChatGPT&#22312;&#24515;&#33039;&#30149;&#21644;&#34880;&#31649;&#30149;&#29702;&#23398;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22238;&#31572;&#25361;&#25112;&#24615;&#22810;&#39033;&#36873;&#25321;&#39064;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#21307;&#23398;&#29983;&#65292;&#26174;&#31034;&#20102;ChatGPT&#22312;&#25552;&#20379;&#20934;&#30830;&#31572;&#26696;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#31456;&#26088;&#22312;&#20998;&#26512;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;&#24515;&#33039;&#30149;&#21644;&#34880;&#31649;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#22312;&#22238;&#31572;&#25361;&#25112;&#24615;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#20934;&#30830;&#24615;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;Siamois-QCM&#24179;&#21488;&#30340;190&#20010;&#38382;&#39064;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#30446;&#26631;&#26159;&#35780;&#20272;ChatGPT&#22312;&#21307;&#23398;&#25945;&#32946;&#20013;&#19982;&#20004;&#21517;&#25104;&#32489;&#20248;&#31168;&#30340;&#21307;&#23398;&#29983;&#30456;&#27604;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;190&#36947;&#38382;&#39064;&#20013;&#33719;&#24471;&#20102;175&#20010;&#27491;&#30830;&#31572;&#26696;&#65292;&#20934;&#30830;&#29575;&#20026;92.10\%&#65292;&#32780;&#20004;&#21517;&#23398;&#29983;&#20998;&#21035;&#24471;&#20998;163&#21644;159&#65292;&#20934;&#30830;&#29575;&#20998;&#21035;&#20026;85.78\%&#21644;82.63\%&#12290;&#36825;&#20123;&#32467;&#26524;&#23637;&#31034;&#20102;ChatGPT&#22312;&#24515;&#33039;&#30149;&#21644;&#34880;&#31649;&#30149;&#29702;&#23398;&#39046;&#22495;&#20855;&#26377;&#20934;&#30830;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article aims to analyze the performance of ChatGPT, a large language model developed by OpenAI, in the context of cardiology and vascular pathologies. The study evaluated the accuracy of ChatGPT in answering challenging multiple-choice questions (QCM) using a dataset of 190 questions from the Siamois-QCM platform. The goal was to assess ChatGPT potential as a valuable tool in medical education compared to two well-ranked students of medicine. The results showed that ChatGPT outperformed the students, scoring 175 out of 190 correct answers with a percentage of 92.10\%, while the two students achieved scores of 163 and 159 with percentages of 85.78\% and 82.63\%, respectively. These results showcase how ChatGPT has the potential to be highly effective in the fields of cardiology and vascular pathologies by providing accurate answers to relevant questions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#22238;&#39038;&#20102;&#22823;&#22411;&#20195;&#30721;&#35757;&#32451;&#30340;transformer-based&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#25688;&#35201;&#12289;&#32570;&#38519;&#26816;&#27979;&#31561;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#23558;NLP&#25216;&#26415;&#19982;&#36719;&#20214;&#33258;&#28982;&#21270;&#30456;&#32467;&#21512;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2307.02503</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#21644;&#29702;&#35299;&#22823;&#22411;&#20195;&#30721;&#29992;&#20110;AI&#36741;&#21161;&#32534;&#31243;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review. (arXiv:2307.02503v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02503
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#22238;&#39038;&#20102;&#22823;&#22411;&#20195;&#30721;&#35757;&#32451;&#30340;transformer-based&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#25688;&#35201;&#12289;&#32570;&#38519;&#26816;&#27979;&#31561;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#23558;NLP&#25216;&#26415;&#19982;&#36719;&#20214;&#33258;&#28982;&#21270;&#30456;&#32467;&#21512;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#25991;&#29486;&#65292;&#29305;&#21035;&#20851;&#27880;&#20351;&#29992;&#22823;&#22411;&#20195;&#30721;&#36827;&#34892;&#35757;&#32451;&#30340;&#22522;&#20110;transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;AI&#36741;&#21161;&#32534;&#31243;&#20219;&#21153;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#32463;&#36807;&#36719;&#20214;&#33258;&#28982;&#21270;&#22686;&#24378;&#30340;LLMs&#22312;&#20419;&#36827;AI&#36741;&#21161;&#32534;&#31243;&#24212;&#29992;&#26041;&#38754;&#36215;&#21040;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#20195;&#30721;&#34917;&#20840;&#12289;&#20195;&#30721;&#32763;&#35793;&#12289;&#20195;&#30721;&#20248;&#21270;&#12289;&#20195;&#30721;&#25688;&#35201;&#12289;&#32570;&#38519;&#26816;&#27979;&#21644;&#20811;&#38534;&#26816;&#27979;&#12290;&#20854;&#20013;&#33879;&#21517;&#30340;&#24212;&#29992;&#21253;&#25324;&#30001;OpenAI&#30340;Codex&#21644;DeepMind AlphaCode&#39537;&#21160;&#30340;GitHub Copilot&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#20027;&#35201;&#30340;LLMs&#21450;&#20854;&#22312;&#19982;AI&#36741;&#21161;&#32534;&#31243;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#23558;NLP&#25216;&#26415;&#19982;&#36719;&#20214;&#33258;&#28982;&#21270;&#30456;&#32467;&#21512;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#65292;&#24182;&#35752;&#35770;&#20102;&#25193;&#23637;AI&#36741;&#21161;&#32534;&#31243;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23398;&#26234;&#33021;&#20307;&#21644;&#25968;&#23398;&#23884;&#20837;&#20316;&#20026;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22522;&#22240;&#32452;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;GPT&#30340;&#24037;&#20316;&#27969;&#23558;&#25991;&#29486;&#20013;&#30340;&#26041;&#31243;&#36716;&#25442;&#20026;LaTeX&#21644;Python&#26684;&#24335;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#21644;&#20132;&#20114;&#24335;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2307.02502</link><description>&lt;p&gt;
&#25968;&#23398;&#26234;&#33021;&#20307;&#65306;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#12289;&#25968;&#23398;&#23884;&#20837;&#21644;&#22522;&#22240;&#32452;&#23398;
&lt;/p&gt;
&lt;p&gt;
Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics. (arXiv:2307.02502v1 [q-bio.OT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25968;&#23398;&#26234;&#33021;&#20307;&#21644;&#25968;&#23398;&#23884;&#20837;&#20316;&#20026;&#35299;&#20915;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#22522;&#22240;&#32452;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;GPT&#30340;&#24037;&#20316;&#27969;&#23558;&#25991;&#29486;&#20013;&#30340;&#26041;&#31243;&#36716;&#25442;&#20026;LaTeX&#21644;Python&#26684;&#24335;&#65292;&#20197;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#21644;&#20132;&#20114;&#24335;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21487;&#20197;&#36890;&#36807;&#26356;&#26131;&#20110;&#29702;&#35299;&#30340;&#25968;&#23398;&#30693;&#35782;&#26469;&#25552;&#21319;&#12290;&#38500;&#20102;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#20197;&#22806;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20063;&#22312;&#32534;&#31243;&#12289;&#31639;&#27861;&#21457;&#29616;&#21644;&#23450;&#29702;&#35777;&#26126;&#26041;&#38754;&#24471;&#21040;&#24212;&#29992;&#65292;&#20294;&#23427;&#20204;&#22312;&#22522;&#22240;&#32452;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#36739;&#22823;&#12290;&#26412;&#39033;&#30446;&#24341;&#20837;&#20102;&#25968;&#23398;&#26234;&#33021;&#20307;&#21644;&#25968;&#23398;&#23884;&#20837;&#20316;&#20026;&#8220;&#25968;&#23398;&#25705;&#23572;&#23450;&#24459;&#8221;&#30340;&#26032;&#36827;&#23637;&#65292;&#20351;&#29992;&#22522;&#20110;GPT&#30340;&#24037;&#20316;&#27969;&#23558;&#25991;&#29486;&#20013;&#30340;&#26041;&#31243;&#36716;&#25442;&#20026;LaTeX&#21644;Python&#26684;&#24335;&#12290;&#34429;&#28982;&#23384;&#22312;&#35768;&#22810;&#25968;&#23383;&#26041;&#31243;&#34920;&#31034;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#33258;&#21160;&#21270;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#24037;&#20855;&#12290;LLM&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#25552;&#20379;&#20102;&#35821;&#35328;&#29992;&#25143;&#30028;&#38754;&#65292;&#24182;&#20026;&#22823;&#35268;&#27169;&#30340;AI&#36741;&#21161;&#35745;&#31639;&#22522;&#30784;&#35774;&#26045;&#25552;&#20379;&#20102;&#24418;&#24335;&#21270;&#35821;&#35328;&#12290;&#37492;&#20110;&#26080;&#38480;&#30340;&#24418;&#24335;&#21487;&#33021;&#31354;&#38388;&#65292;&#19982;&#25968;&#23398;&#20114;&#21160;&#30340;&#25968;&#23398;&#26234;&#33021;&#20307;&#26377;&#21487;&#33021;&#20351;&#25105;&#20204;&#20174;&#8220;&#22823;&#25968;&#25454;&#8221;&#36716;&#21521;&#8220;&#22823;&#25968;&#23398;&#8221;&#12290;&#25968;&#23398;&#19982;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#20855;&#26377;&#21487;&#20197;&#36890;&#36807;&#35777;&#26126;&#26469;&#39564;&#35777;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#22312;&#24212;&#29992;&#33539;&#22260;&#19978;&#26356;&#21152;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement in generative AI could be boosted with more accessible mathematics. Beyond human-AI chat, large language models (LLMs) are emerging in programming, algorithm discovery, and theorem proving, yet their genomics application is limited. This project introduces Math Agents and mathematical embedding as fresh entries to the "Moore's Law of Mathematics", using a GPT-based workflow to convert equations from literature into LaTeX and Python formats. While many digital equation representations exist, there's a lack of automated large-scale evaluation tools. LLMs are pivotal as linguistic user interfaces, providing natural language access for human-AI chat and formal languages for large-scale AI-assisted computational infrastructure. Given the infinite formal possibility spaces, Math Agents, which interact with math, could potentially shift us from "big data" to "big math". Math, unlike the more flexible natural language, has properties subject to proof, enabling its use beyond tr
&lt;/p&gt;</description></item><item><title>mPLUG-DocOwl&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#12290;&#23427;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#12289;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#21644;&#25991;&#26723;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25552;&#21319;&#20102;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02499</link><description>&lt;p&gt;
mPLUG-DocOwl: &#27169;&#22359;&#21270;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding. (arXiv:2307.02499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02499
&lt;/p&gt;
&lt;p&gt;
mPLUG-DocOwl&#26159;&#19968;&#31181;&#27169;&#22359;&#21270;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#12290;&#23427;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#35821;&#35328;&#12289;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#21644;&#25991;&#26723;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#65292;&#25552;&#21319;&#20102;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#29702;&#35299;&#26159;&#25351;&#20174;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#23383;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#12289;&#20998;&#26512;&#21644;&#29702;&#35299;&#20449;&#24687;&#65292;&#20363;&#22914;&#32593;&#39029;&#12290;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#21253;&#25324;mPLUG-Owl&#65292;&#24050;&#32463;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#23454;&#29616;&#26080;OCR&#30340;&#25991;&#26412;&#35782;&#21035;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#39046;&#22495;&#20869;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#24573;&#35270;OCR&#32454;&#31890;&#24230;&#29305;&#24449;&#65292;&#22914;&#22797;&#26434;&#30340;&#34920;&#26684;&#25110;&#22823;&#22359;&#25991;&#26412;&#65292;&#36825;&#20123;&#29305;&#24449;&#23545;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;mPLUG-Owl&#25552;&#20986;&#20102;mPLUG-DocOwl&#65292;&#29992;&#20110;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#31181;&#35270;&#35273;-&#25991;&#26412;&#29702;&#35299;&#20219;&#21153;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#38024;&#23545;&#35821;&#35328;&#12289;&#36890;&#29992;&#35270;&#35273;-&#35821;&#35328;&#21644;&#25991;&#26723;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#26469;&#22686;&#24378;&#26080;OCR&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document understanding refers to automatically extract, analyze and comprehend information from various types of digital documents, such as a web page. Existing Multi-model Large Language Models (MLLMs), including mPLUG-Owl, have demonstrated promising zero-shot capabilities in shallow OCR-free text recognition, indicating their potential for OCR-free document understanding. Nevertheless, without in-domain training, these models tend to ignore fine-grained OCR features, such as sophisticated tables or large blocks of text, which are essential for OCR-free document understanding. In this paper, we propose mPLUG-DocOwl based on mPLUG-Owl for OCR-free document understanding. Specifically, we first construct a instruction tuning dataset featuring a wide range of visual-text understanding tasks. Then, we strengthen the OCR-free document understanding ability by jointly train the model on language-only, general vision-and-language, and document instruction tuning dataset with our unified ins
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#35777;&#26126;&#35268;&#21010;&#20013;&#30340;&#28436;&#32462;&#21487;&#21152;&#24615;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#39640;&#25928;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#30340;&#21069;&#25552;&#38472;&#36848;&#24635;&#21644;&#25509;&#36817;&#20110;&#22522;&#20110;&#36825;&#20123;&#21069;&#25552;&#30340;&#32467;&#35770;&#23884;&#20837;&#12290;&#20174;&#32780;&#35777;&#26126;&#20102;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#23384;&#22312;&#12290;</title><link>http://arxiv.org/abs/2307.02472</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#35777;&#26126;&#35268;&#21010;&#30340;&#28436;&#32462;&#21487;&#21152;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deductive Additivity for Planning of Natural Language Proofs. (arXiv:2307.02472v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#35777;&#26126;&#35268;&#21010;&#20013;&#30340;&#28436;&#32462;&#21487;&#21152;&#24615;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#39640;&#25928;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#30340;&#21069;&#25552;&#38472;&#36848;&#24635;&#21644;&#25509;&#36817;&#20110;&#22522;&#20110;&#36825;&#20123;&#21069;&#25552;&#30340;&#32467;&#35770;&#23884;&#20837;&#12290;&#20174;&#32780;&#35777;&#26126;&#20102;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35774;&#35745;&#29992;&#20110;&#22810;&#27493;&#39588;&#21629;&#39064;&#39564;&#35777;&#30340;&#33258;&#28982;&#35821;&#35328;&#31995;&#32479;&#36890;&#24120;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26816;&#32034;&#19968;&#32452;&#30456;&#20851;&#30340;&#21069;&#25552;&#38472;&#36848;&#65288;&#35268;&#21010;&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#36825;&#20123;&#38472;&#36848;&#20013;&#29983;&#25104;&#26032;&#30340;&#32467;&#35770;&#65288;&#28436;&#32462;&#65289;&#12290;&#35268;&#21010;&#38454;&#27573;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;Transformer&#25805;&#20316;&#65292;&#24182;&#19988;&#26080;&#27861;&#25193;&#23637;&#21040;&#20219;&#24847;&#25968;&#37327;&#30340;&#21069;&#25552;&#38472;&#36848;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#19982;&#28436;&#32462;&#25512;&#29702;&#20860;&#23481;&#30340;&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#39640;&#25928;&#30340;&#35268;&#21010;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23884;&#20837;&#31354;&#38388;&#26159;&#21542;&#20855;&#26377;&#25105;&#20204;&#31216;&#20043;&#20026;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#29305;&#24615;&#65306;&#21069;&#25552;&#38472;&#36848;&#23884;&#20837;&#30340;&#24635;&#21644;&#24212;&#35813;&#25509;&#36817;&#22522;&#20110;&#36825;&#20123;&#21069;&#25552;&#30340;&#32467;&#35770;&#30340;&#23884;&#20837;&#12290;&#38500;&#20102;&#26469;&#33258;GPT3&#30340;&#24494;&#35843;&#23884;&#20837;&#21644;&#26469;&#33258;BM25&#30340;&#31232;&#30095;&#23884;&#20837;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22810;&#31181;&#29616;&#25104;&#30340;&#23494;&#38598;&#23884;&#20837;&#28304;&#12290;&#25105;&#20204;&#22312;&#20869;&#22312;&#19978;&#30740;&#31350;&#20102;&#23884;&#20837;&#27169;&#22411;&#65292;&#35780;&#20272;&#20102;&#28436;&#32462;&#21487;&#21152;&#24615;&#30340;&#23646;&#24615;&#26159;&#21542;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current natural language systems designed for multi-step claim validation typically operate in two phases: retrieve a set of relevant premise statements using heuristics (planning), then generate novel conclusions from those statements using a large language model (deduction). The planning step often requires expensive Transformer operations and does not scale to arbitrary numbers of premise statements. In this paper, we investigate whether an efficient planning heuristic is possible via embedding spaces compatible with deductive reasoning. Specifically, we evaluate whether embedding spaces exhibit a property we call deductive additivity: the sum of premise statement embeddings should be close to embeddings of conclusions based on those premises. We explore multiple sources of off-the-shelf dense embeddings in addition to fine-tuned embeddings from GPT3 and sparse embeddings from BM25. We study embedding models both intrinsically, evaluating whether the property of deductive additivity
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#32034;&#21644;&#25490;&#21517;&#20256;&#36798;&#25233;&#37057;&#30151;&#29366;&#30340;&#21477;&#23376;&#12290;&#37319;&#29992;&#35821;&#20041;&#25628;&#32034;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#21477;&#23376;&#19982;BDI-II&#30151;&#29366;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#12290;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#27604;BDI-II&#21709;&#24212;&#26356;&#20016;&#23500;&#21644;&#35821;&#20041;&#22810;&#26679;&#21270;&#65292;&#33021;&#26377;&#25928;&#22686;&#21152;&#25968;&#25454;&#21644;&#24494;&#35843;&#19979;&#28216;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.02313</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#32034;&#25233;&#37057;&#30151;&#29366;
&lt;/p&gt;
&lt;p&gt;
Utilizing ChatGPT Generated Data to Retrieve Depression Symptoms from Social Media. (arXiv:2307.02313v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;ChatGPT&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#20013;&#26816;&#32034;&#21644;&#25490;&#21517;&#20256;&#36798;&#25233;&#37057;&#30151;&#29366;&#30340;&#21477;&#23376;&#12290;&#37319;&#29992;&#35821;&#20041;&#25628;&#32034;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#36827;&#34892;&#21477;&#23376;&#19982;BDI-II&#30151;&#29366;&#30340;&#30456;&#20851;&#24615;&#25490;&#21517;&#12290;&#32467;&#26524;&#26174;&#31034;&#21512;&#25104;&#25968;&#25454;&#27604;BDI-II&#21709;&#24212;&#26356;&#20016;&#23500;&#21644;&#35821;&#20041;&#22810;&#26679;&#21270;&#65292;&#33021;&#26377;&#25928;&#22686;&#21152;&#25968;&#25454;&#21644;&#24494;&#35843;&#19979;&#28216;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BLUE&#22242;&#38431;&#22312;eRisk Lab&#20219;&#21153;&#20013;&#25628;&#32034;&#25233;&#37057;&#30151;&#29366;&#30340;&#36129;&#29486;&#12290;&#35813;&#20219;&#21153;&#21253;&#25324;&#20174;BDI-II&#38382;&#21367;&#20013;&#26816;&#32034;&#21644;&#25490;&#21517;Reddit&#31038;&#20132;&#23186;&#20307;&#21477;&#23376;&#65292;&#36825;&#20123;&#21477;&#23376;&#20256;&#36798;&#20102;&#25233;&#37057;&#30151;&#29366;&#12290;&#37492;&#20110;LLMs&#25552;&#20379;&#30340;&#21512;&#25104;&#25968;&#25454;&#24050;&#34987;&#35777;&#26126;&#26159;&#22686;&#21152;&#25968;&#25454;&#21644;&#24494;&#35843;&#19979;&#28216;&#27169;&#22411;&#30340;&#21487;&#38752;&#26041;&#27861;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;ChatGPT&#20026;&#27599;&#20010;BDI-II&#38382;&#21367;&#30340;&#30151;&#29366;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25552;&#31034;&#65292;&#20351;&#29983;&#25104;&#30340;&#25968;&#25454;&#27604;&#27599;&#20010;&#38382;&#39064;&#30340;BDI-II&#21709;&#24212;&#26356;&#20016;&#23500;&#21644;&#35821;&#20041;&#22810;&#26679;&#21270;&#65292;&#21516;&#26102;&#21253;&#21547;&#20102;Reddit&#19978;&#20998;&#20139;&#32463;&#21382;&#26356;&#31169;&#23494;&#26041;&#24335;&#20013;&#29305;&#26377;&#30340;&#24773;&#24863;&#21644;&#36726;&#20107;&#32463;&#21382;&#12290;&#25105;&#20204;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24230;&#25191;&#34892;&#35821;&#20041;&#25628;&#32034;&#24182;&#23545;&#21477;&#23376;&#19982;BDI-II&#30151;&#29366;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65288;MentalRoBERTa&#21644;MPNet&#30340;&#21464;&#20307;&#65289;&#36827;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present the contribution of the BLUE team in the eRisk Lab task on searching for symptoms of depression. The task consists of retrieving and ranking Reddit social media sentences that convey symptoms of depression from the BDI-II questionnaire. Given that synthetic data provided by LLMs have been proven to be a reliable method for augmenting data and fine-tuning downstream models, we chose to generate synthetic data using ChatGPT for each of the symptoms of the BDI-II questionnaire. We designed a prompt such that the generated data contains more richness and semantic diversity than the BDI-II responses for each question and, at the same time, contains emotional and anecdotal experiences that are specific to the more intimate way of sharing experiences on Reddit. We perform semantic search and rank the sentences' relevance to the BDI-II symptoms by cosine similarity. We used two state-of-the-art transformer-based models (MentalRoBERTa and a variant of MPNet) for embeddi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37319;&#29992;Transformer&#30340;&#36716;&#25442;&#30340;&#21407;&#22411;&#37325;&#26500;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;RNN&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#25289;&#19969;&#35821;&#21644;&#27721;&#35821;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#32034;&#20102;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#31995;&#32479;&#21457;&#32946;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2307.01896</link><description>&lt;p&gt;
&#36716;&#25442;&#30340;&#21407;&#22411;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Transformed Protoform Reconstruction. (arXiv:2307.01896v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01896
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#37319;&#29992;Transformer&#30340;&#36716;&#25442;&#30340;&#21407;&#22411;&#37325;&#26500;&#27169;&#22411;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;RNN&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#22312;&#25289;&#19969;&#35821;&#21644;&#27721;&#35821;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#25506;&#32034;&#20102;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#31995;&#32479;&#21457;&#32946;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#22411;&#37325;&#26500;&#26159;&#25512;&#26029;&#19968;&#32452;&#23376;&#35821;&#35328;&#20013;&#25152;&#20986;&#29616;&#30340;&#35821;&#32032;&#25110;&#35789;&#27719;&#22312;&#31062;&#20808;&#35821;&#20013;&#30340;&#24773;&#20917;&#30340;&#20219;&#21153;&#12290;Meloni&#31561;&#20154;&#65288;2021&#65289;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;RNN&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#19982;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#22312;&#25289;&#19969;&#35821;&#21407;&#22411;&#37325;&#26500;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#26356;&#26032;&#20102;&#20182;&#20204;&#30340;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65306;Transformer&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#21363;&#35206;&#30422;&#20102;5&#31181;&#35821;&#35328;&#30340;8000&#20010;&#21516;&#28304;&#35789;&#30340;&#32599;&#26364;&#35821;&#25968;&#25454;&#38598;&#21644;&#28085;&#30422;&#20102;39&#31181;&#35821;&#35328;&#21464;&#20307;&#30340;800+&#21516;&#28304;&#35789;&#30340;&#20013;&#22269;&#25968;&#25454;&#38598;&#65288;Hou 2004&#65289;&#65292;&#22312;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#24230;&#37327;&#25351;&#26631;&#19978;&#32988;&#36807;&#20102;&#20182;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#28508;&#22312;&#30340;&#31995;&#32479;&#21457;&#32946;&#20449;&#21495;&#25506;&#32034;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/cmu-llab/acl-2023&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al. (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01715</link><description>&lt;p&gt;
&#31526;&#21512;&#30446;&#26631;&#65306;&#20351;&#29992;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#22312;CTC&#27169;&#22411;&#20013;&#20248;&#21270;&#25152;&#38656;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#25554;&#20837;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#20248;&#21270;CTC&#27169;&#22411;&#20013;&#30340;&#25152;&#38656;&#23646;&#24615;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#34917;&#20805;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#65292;&#24182;&#19981;&#38656;&#35201;&#20462;&#25913;CTC&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#26159;&#35757;&#32451;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#24191;&#27867;&#20351;&#29992;&#30340;&#20934;&#21017;&#12290;&#23427;&#36890;&#36807;&#23558;&#23436;&#32654;&#23545;&#40784;&#65288;&#20135;&#29983;&#22522;&#26412;&#20107;&#23454;&#65289;&#30340;&#36793;&#38469;&#21270;&#26469;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#24207;&#21015;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#31216;&#20026;&#23545;&#20854;&#65292;&#20197;&#20195;&#20215;&#19981;&#23436;&#32654;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#23545;&#40784;&#30340;&#20108;&#20803;&#21306;&#20998;&#26080;&#27861;&#25429;&#25417;&#21040;&#22312;&#20854;&#20182;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20854;&#20182;&#20851;&#38190;&#23545;&#40784;&#23646;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Align With Purpose}$&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;CTC&#26465;&#20214;&#19979;&#35757;&#32451;&#27169;&#22411;&#20013;&#25152;&#38656;&#23646;&#24615;&#30340;$\textbf{&#36890;&#29992;&#25554;&#20837;&#24335;&#26694;&#26550;}$&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#39069;&#22806;&#30340;&#25439;&#22833;&#39033;&#26469;&#34917;&#20805;CTC&#26469;&#20248;&#20808;&#32771;&#34385;&#31526;&#21512;&#25152;&#38656;&#23646;&#24615;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#24178;&#39044;CTC&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#22815;&#36731;&#26494;&#20248;&#21270;&#21508;&#31181;&#23646;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#21306;&#20998;&#23436;&#32654;&#21644;&#19981;&#23436;&#32654;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) is a widely used criterion for training supervised sequence-to-sequence (seq2seq) models. It enables learning the relations between input and output sequences, termed alignments, by marginalizing over perfect alignments (that yield the ground truth), at the expense of imperfect alignments. This binary differentiation of perfect and imperfect alignments falls short of capturing other essential alignment properties that hold significance in other real-world applications. Here we propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play framework}$ for enhancing a desired property in models trained with the CTC criterion. We do that by complementing the CTC with an additional loss term that prioritizes alignments according to a desired property. Our method does not require any intervention in the CTC loss function, enables easy optimization of a variety of properties, and allows differentiation between both perfect and imperfect al
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00209</link><description>&lt;p&gt;
&#22270;&#20687;&#30340;&#37325;&#35201;&#24615;&#65306;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#30340;&#26032;&#25968;&#25454;&#38598;&#21644;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#21644;&#22270;&#20687;&#20316;&#20026;&#20004;&#31181;&#27169;&#24577;&#36827;&#34892;&#30740;&#31350;&#12290;&#21516;&#26102;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#22840;&#24352;&#26816;&#27979;&#30340;&#36328;&#39046;&#22495;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22840;&#24352;&#65292;&#21363;&#22840;&#22823;&#20854;&#35789;&#65292;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#29616;&#35937;&#12290;&#22840;&#24352;&#26816;&#27979;&#26159;&#29702;&#35299;&#20154;&#31867;&#34920;&#36798;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#24050;&#32463;&#26377;&#20960;&#39033;&#20851;&#20110;&#22840;&#24352;&#26816;&#27979;&#30340;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#30340;&#30740;&#31350;&#21482;&#20851;&#27880;&#25991;&#26412;&#27169;&#24577;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#20351;&#29992;&#21508;&#31181;&#27169;&#24577;&#65288;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#31561;&#65289;&#26469;&#34920;&#36798;&#22840;&#24352;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22810;&#27169;&#24577;&#22840;&#24352;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#24494;&#21338;&#65288;&#20013;&#22269;&#30340;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#65289;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#19968;&#20123;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#24494;&#21338;&#30340;&#25991;&#26412;&#21644;&#22270;&#20687;&#35270;&#20026;&#20004;&#31181;&#27169;&#24577;&#65292;&#25506;&#32034;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#22312;&#22840;&#24352;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#32534;&#30721;&#22120;&#22312;&#36825;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#20174;&#20116;&#20010;&#19981;&#21516;&#30340;&#20027;&#39064;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection of hyperbole is an important part of understanding human expression. There have been several studies on hyperbole detection, but most of which focus on text modality only. However, with the development of social media, people can create hyperbolic expressions with various modalities, including text, images, videos, etc. In this paper, we focus on multimodal hyperbole detection. We create a multimodal detection dataset\footnote{The dataset will be released to the community.} from Weibo (a Chinese social media) and carry out some studies on it. We treat the text and image from a piece of weibo as two modalities and explore the role of text and image for hyperbole detection. Different pre-trained multimodal encoders are also evaluated on this downstream task to show their performance. Besides, since this dataset is constructed from five different topics, we also evaluate the cross-domain performance of different 
&lt;/p&gt;</description></item><item><title>&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#25913;&#36827;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.17649</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#19981;&#29702;&#24819;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Biomedical Language Models are Robust to Sub-optimal Tokenization. (arXiv:2306.17649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17649
&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23545;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#20998;&#21106;&#26041;&#24335;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36825;&#23545;&#20110;&#25913;&#36827;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19968;&#33324;&#30340;&#33521;&#35821;&#30456;&#21453;&#65292;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20013;&#30340;&#35768;&#22810;&#27010;&#24565;&#26159;&#30001;&#29983;&#29289;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#35774;&#35745;&#30340;&#65292;&#30446;&#30340;&#26159;&#35201;&#31934;&#30830;&#19988;&#31616;&#26126;&#12290;&#36890;&#24120;&#36890;&#36807;&#23558;&#26377;&#24847;&#20041;&#30340;&#29983;&#29289;&#21307;&#23398;&#35789;&#32032;&#36830;&#25509;&#36215;&#26469;&#21019;&#24314;&#26032;&#30340;&#35821;&#20041;&#21333;&#20301;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#20195;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411; (LMs) &#26159;&#20351;&#29992;&#20174;&#22823;&#35268;&#27169;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#32479;&#35745;&#20013;&#23548;&#20986;&#30340;&#26631;&#20934;&#39046;&#22495;&#29305;&#23450;&#26631;&#35760;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#32780;&#27809;&#26377;&#26126;&#30830;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#30340;&#31896;&#38468;&#24615;&#29305;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#21457;&#29616;&#26631;&#20934;&#36890;&#29992;&#39046;&#22495;&#21644;&#29983;&#29289;&#21307;&#23398;&#26631;&#35760;&#22120;&#22312;&#23558;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20998;&#21106;&#25104;&#26377;&#24847;&#20041;&#30340;&#32452;&#25104;&#37096;&#20998;&#26041;&#38754;&#33021;&#21147;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20551;&#35774;&#20351;&#29992;&#19968;&#31181;&#26356;&#20934;&#30830;&#20998;&#21106;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#26631;&#35760;&#22120;&#23558;&#20351;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#28041;&#21450;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#30340;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035; (NER) &#21644;&#23454;&#20307;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
As opposed to general English, many concepts in biomedical terminology have been designed in recent history by biomedical professionals with the goal of being precise and concise. This is often achieved by concatenating meaningful biomedical morphemes to create new semantic units. Nevertheless, most modern biomedical language models (LMs) are pre-trained using standard domain-specific tokenizers derived from large scale biomedical corpus statistics without explicitly leveraging the agglutinating nature of biomedical language. In this work, we first find that standard open-domain and biomedical tokenizers are largely unable to segment biomedical terms into meaningful components. Therefore, we hypothesize that using a tokenizer which segments biomedical terminology more accurately would enable biomedical LMs to improve their performance on downstream biomedical NLP tasks, especially ones which involve biomedical terms directly such as named entity recognition (NER) and entity linking. Su
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.16564</link><description>&lt;p&gt;
&#36890;&#36807;Pareto Optimal&#33258;&#30417;&#30563;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#26657;&#20934;&#21644;&#38169;&#35823;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Automatic Calibration and Error Correction for Large Language Models via Pareto Optimal Self-Supervision. (arXiv:2306.16564v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#21709;&#24212;&#36827;&#34892;&#31995;&#32479;&#26657;&#20934;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#26159;&#20934;&#30830;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#22686;&#38271;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#31561;&#20851;&#38190;&#39046;&#22495;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26657;&#20934;LLM&#21709;&#24212;&#30340;&#32622;&#20449;&#27700;&#24179;&#65292;&#23545;&#20110;&#33258;&#21160;&#26816;&#27979;&#38169;&#35823;&#24182;&#20419;&#36827;&#20154;&#26426;&#21327;&#20316;&#39564;&#35777;&#33267;&#20851;&#37325;&#35201;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#26657;&#20934;&#20449;&#21495;&#26469;&#28304;&#26159;&#19987;&#23478;&#25351;&#23450;&#30340;&#32534;&#31243;&#30417;&#30563;&#65292;&#36890;&#24120;&#20855;&#26377;&#36739;&#20302;&#30340;&#25104;&#26412;&#65292;&#20294;&#20063;&#26377;&#20854;&#33258;&#36523;&#30340;&#23616;&#38480;&#24615;&#65292;&#22914;&#22122;&#22768;&#21644;&#35206;&#30422;&#33539;&#22260;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;Pareto Optimal&#33258;&#30417;&#30563;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21487;&#29992;&#30340;&#32534;&#31243;&#30417;&#30563;&#26469;&#31995;&#32479;&#22320;&#26657;&#20934;LLM&#21709;&#24212;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#21709;&#24212;&#29983;&#25104;&#39118;&#38505;&#35780;&#20998;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;&#36825;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#35843;&#21644;&#27169;&#22411;&#26469;&#23454;&#29616;&#65292;&#23558;LLM&#36755;&#20986;&#19982;&#20854;&#20182;&#21487;&#29992;&#30340;&#30417;&#30563;&#26469;&#28304;&#30456;&#21327;&#35843;&#65292;&#23558;&#26356;&#19981;&#30830;&#23450;&#30340;&#21709;&#24212;&#20998;&#37197;&#26356;&#39640;&#30340;&#39118;&#38505;&#35780;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities out of box for a wide range of applications, yet accuracy still remains a major growth area, especially in mission-critical domains such as biomedicine. An effective method to calibrate the confidence level on LLM responses is essential to automatically detect errors and facilitate human-in-the-loop verification. An important source of calibration signals stems from expert-stipulated programmatic supervision, which is often available at low cost but has its own limitations such as noise and coverage. In this paper, we introduce a Pareto optimal self-supervision framework that can leverage available programmatic supervision to systematically calibrate LLM responses by producing a risk score for every response, without any additional manual efforts. This is accomplished by learning a harmonizer model to align LLM output with other available supervision sources, which would assign higher risk scores to more uncertain L
&lt;/p&gt;</description></item><item><title>2023&#24180;&#21809;&#22768;&#36716;&#25442;&#25361;&#25112;&#36187;&#65288;SVCC&#65289;&#30340;&#26368;&#26032;&#29256;&#26412;&#26088;&#22312;&#27604;&#36739;&#21644;&#20102;&#35299;&#19981;&#21516;&#30340;&#21809;&#22768;&#36716;&#25442;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#21548;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#39030;&#23574;&#31995;&#32479;&#36798;&#21040;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#33258;&#28982;&#24230;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#30446;&#26631;&#21457;&#38899;&#32773;&#30340;&#30456;&#20284;&#24230;&#35780;&#20998;&#12290;&#36328;&#22495;SVC&#27604;&#22495;&#20869;SVC&#26356;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2306.14422</link><description>&lt;p&gt;
2023&#24180;&#21809;&#22768;&#36716;&#25442;&#25361;&#25112;&#36187;
&lt;/p&gt;
&lt;p&gt;
The Singing Voice Conversion Challenge 2023. (arXiv:2306.14422v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14422
&lt;/p&gt;
&lt;p&gt;
2023&#24180;&#21809;&#22768;&#36716;&#25442;&#25361;&#25112;&#36187;&#65288;SVCC&#65289;&#30340;&#26368;&#26032;&#29256;&#26412;&#26088;&#22312;&#27604;&#36739;&#21644;&#20102;&#35299;&#19981;&#21516;&#30340;&#21809;&#22768;&#36716;&#25442;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#21548;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#21457;&#29616;&#34429;&#28982;&#39030;&#23574;&#31995;&#32479;&#36798;&#21040;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#33258;&#28982;&#24230;&#65292;&#20294;&#26410;&#33021;&#36798;&#21040;&#30446;&#26631;&#21457;&#38899;&#32773;&#30340;&#30456;&#20284;&#24230;&#35780;&#20998;&#12290;&#36328;&#22495;SVC&#27604;&#22495;&#20869;SVC&#26356;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#22768;&#38899;&#36716;&#25442;&#25361;&#25112;&#65288;VCC&#65289;&#31995;&#21015;&#30340;&#26368;&#26032;&#29256;&#26412;&#65292;&#36825;&#26159;&#19968;&#20010;&#27599;&#20004;&#24180;&#20030;&#34892;&#19968;&#27425;&#30340;&#31185;&#23398;&#27963;&#21160;&#65292;&#26088;&#22312;&#27604;&#36739;&#21644;&#20102;&#35299;&#22522;&#20110;&#20849;&#21516;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#22768;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#31995;&#32479;&#12290;&#20170;&#24180;&#65292;&#25105;&#20204;&#25226;&#37325;&#28857;&#36716;&#21521;&#20102;&#21809;&#22768;&#36716;&#25442;&#65288;SVC&#65289;&#65292;&#22240;&#27492;&#23558;&#25361;&#25112;&#21629;&#21517;&#20026;&#21809;&#22768;&#36716;&#25442;&#25361;&#25112;&#65288;SVCC&#65289;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#20004;&#20010;&#20219;&#21153;&#65292;&#21363;&#22495;&#20869;&#21644;&#36328;&#22495;SVC&#12290;&#25361;&#25112;&#25345;&#32493;&#20102;&#20004;&#20010;&#26376;&#65292;&#25105;&#20204;&#20849;&#25910;&#21040;&#20102;26&#20010;&#25552;&#20132;&#65292;&#20854;&#20013;&#21253;&#25324;2&#20010;&#22522;&#20934;&#31995;&#32479;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#20247;&#21253;&#21548;&#21147;&#27979;&#35797;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#20004;&#20010;&#20219;&#21153;&#65292;&#34429;&#28982;&#39030;&#23574;&#31995;&#32479;&#23454;&#29616;&#20102;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#33258;&#28982;&#24230;&#65292;&#20294;&#27809;&#26377;&#22242;&#38431;&#33021;&#22815;&#33719;&#24471;&#19982;&#30446;&#26631;&#21457;&#38899;&#32773;&#19968;&#26679;&#39640;&#30340;&#30456;&#20284;&#24230;&#35780;&#20998;&#12290;&#21516;&#26102;&#65292;&#27491;&#22914;&#39044;&#26399;&#30340;&#37027;&#26679;&#65292;&#36328;&#22495;SVC&#27604;&#22495;&#20869;SVC&#26356;&#21152;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#30456;&#20284;&#24230;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29616;&#26377;&#30340;&#23458;&#35266;&#35780;&#27979;&#26041;&#27861;&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#20027;&#35266;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#24456;&#23569;&#30340;&#25351;&#26631;&#33021;&#22815;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the latest iteration of the voice conversion challenge (VCC) series, a bi-annual scientific event aiming to compare and understand different voice conversion (VC) systems based on a common dataset. This year we shifted our focus to singing voice conversion (SVC), thus named the challenge the Singing Voice Conversion Challenge (SVCC). A new database was constructed for two tasks, namely in-domain and cross-domain SVC. The challenge was run for two months, and in total we received 26 submissions, including 2 baselines. Through a large-scale crowd-sourced listening test, we observed that for both tasks, although human-level naturalness was achieved by the top system, no team was able to obtain a similarity score as high as the target speakers. Also, as expected, cross-domain SVC is harder than in-domain SVC, especially in the similarity aspect. We also investigated whether existing objective measurements were able to predict perceptual performance, and found that only few of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2306.14096</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Chinese Fine-Grained Financial Sentiment Analysis with Large Language Models. (arXiv:2306.14096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#30340;&#26032;&#22411;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#24182;&#20351;&#29992;&#29616;&#26377;&#24320;&#28304;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#35813;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#36827;&#30495;&#23454;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#23454;&#20307;&#32423;&#21035;&#30340;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#26159;&#24773;&#24863;&#20998;&#26512;&#30340;&#37325;&#35201;&#23376;&#20219;&#21153;&#65292;&#30446;&#21069;&#38754;&#20020;&#30528;&#20247;&#22810;&#25361;&#25112;&#12290;&#20854;&#20013;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26469;&#33258;&#20110;&#32570;&#20047;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37329;&#34701;&#25991;&#26412;&#24773;&#24863;&#20998;&#26512;&#30340;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#36825;&#38480;&#21046;&#20102;&#24320;&#21457;&#26377;&#25928;&#25991;&#26412;&#22788;&#29702;&#25216;&#26415;&#25152;&#38656;&#30340;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#35821;&#35328;&#27169;&#24335;&#21305;&#37197;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#24191;&#27867;&#30340;&#20013;&#25991;&#32454;&#31890;&#24230;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#25968;&#25454;&#38598;FinChina SA&#65292;&#29992;&#20110;&#20225;&#19994;&#39044;&#35686;&#12290;&#25105;&#20204;&#23545;&#27969;&#34892;&#30340;&#29616;&#26377;&#24320;&#28304;LLMs&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#21644;&#23454;&#39564;&#12290;&#25105;&#20204;&#22362;&#20449;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23558;&#25104;&#20026;&#25512;&#21160;&#30495;&#23454;&#19990;&#30028;&#37329;&#34701;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#25506;&#32034;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity-level fine-grained sentiment analysis in the financial domain is a crucial subtask of sentiment analysis and currently faces numerous challenges. The primary challenge stems from the lack of high-quality and large-scale annotated corpora specifically designed for financial text sentiment analysis, which in turn limits the availability of data necessary for developing effective text processing techniques. Recent advancements in large language models (LLMs) have yielded remarkable performance in natural language processing tasks, primarily centered around language pattern matching. In this paper, we propose a novel and extensive Chinese fine-grained financial sentiment analysis dataset, FinChina SA, for enterprise early warning. We thoroughly evaluate and experiment with well-known existing open-source LLMs using our dataset. We firmly believe that our dataset will serve as a valuable resource to advance the exploration of real-world financial sentiment analysis tasks, which shoul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;LLMs&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#23427;&#20204;&#27491;&#22312;&#22686;&#24378;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35299;&#37322;&#33021;&#21147;&#12289;&#25552;&#21319;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20197;&#21450;&#31616;&#21270;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.06767</link><description>&lt;p&gt;
ChatGPT&#21644;LLMs&#23545;&#21307;&#23398;&#24433;&#20687;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#24433;&#21709;&#65306;&#35266;&#28857;&#21644;&#24212;&#29992;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
The Impact of ChatGPT and LLMs on Medical Imaging Stakeholders: Perspectives and Use Cases. (arXiv:2306.06767v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;ChatGPT&#21644;LLMs&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#65292;&#23427;&#20204;&#27491;&#22312;&#22686;&#24378;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35299;&#37322;&#33021;&#21147;&#12289;&#25552;&#21319;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#20197;&#21450;&#31616;&#21270;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;OpenAI ChatGPT&#22312;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;&#20511;&#21161;&#20844;&#20849;&#25968;&#25454;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#27491;&#22312;&#22686;&#24378;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#21319;&#24739;&#32773;&#19982;&#21307;&#29983;&#20043;&#38388;&#30340;&#27807;&#36890;&#65292;&#24182;&#31616;&#21270;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#65292;&#29992;&#20110;&#23637;&#31034;LLMs&#19982;&#21307;&#23398;&#24433;&#20687;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#22797;&#26434;&#20114;&#21160;&#65292;&#21253;&#25324;&#20225;&#19994;&#12289;&#20445;&#38505;&#26426;&#26500;&#12289;&#25919;&#24220;&#12289;&#30740;&#31350;&#26426;&#26500;&#21644;&#21307;&#38498;&#65288;&#34987;&#31216;&#20026;BIGR-H&#65289;&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#12289;&#31034;&#20363;&#24212;&#29992;&#26696;&#20363;&#20197;&#21450;&#23545;&#24191;&#27867;&#24433;&#21709;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#35752;&#35770;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#39640;&#22312;AI&#39537;&#21160;&#21307;&#30103;&#20445;&#20581;&#26102;&#20195;&#30340;&#25112;&#30053;&#35268;&#21010;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#35752;&#35770;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates the transformative potential of Large Language Models (LLMs), such as OpenAI ChatGPT, in medical imaging. With the aid of public data, these models, which possess remarkable language understanding and generation capabilities, are augmenting the interpretive skills of radiologists, enhancing patient-physician communication, and streamlining clinical workflows. The paper introduces an analytic framework for presenting the complex interactions between LLMs and the broader ecosystem of medical imaging stakeholders, including businesses, insurance entities, governments, research institutions, and hospitals (nicknamed BIGR-H). Through detailed analyses, illustrative use cases, and discussions on the broader implications and future directions, this perspective seeks to raise discussion in strategic planning and decision-making in the era of AI-enabled healthcare.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04504</link><description>&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#35780;&#20272;ChatGPT&#65306;&#19982;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#30340;&#38646;&#26679;&#20363;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers. (arXiv:2306.04504v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#35757;&#32451;&#26679;&#26412;&#36739;&#23567;&#26102;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#12290;&#30001;&#27492;&#34920;&#26126;ChatGPT&#20855;&#26377;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#25104;&#20026;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;&#20854;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20808;&#21069;&#30340;&#24037;&#20316;&#23578;&#26410;&#30740;&#31350;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;ChatGPT&#22312;&#21508;&#31181;&#22522;&#20934;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#22914;&#20851;&#31995;&#25552;&#21462;&#12289;&#25991;&#26723;&#20998;&#31867;&#12289;&#38382;&#31572;&#21644;&#25688;&#35201;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#23545;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#30340;&#24037;&#20316;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#22312;&#35757;&#32451;&#38598;&#36739;&#23567;&#30340;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#38646;&#26679;&#20363;ChatGPT&#29978;&#33267;&#20248;&#20110;&#20808;&#36827;&#30340;&#31934;&#35843;&#29983;&#25104;&#24335;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#22914;BioGPT&#21644;BioBART&#12290;&#36825;&#34920;&#26126;ChatGPT&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#39044;&#35757;&#32451;&#20351;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#19987;&#19994;&#24615;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20855;&#26377;&#25104;&#20026;&#21508;&#31181;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a large language model developed by OpenAI. Despite its impressive performance across various tasks, no prior work has investigated its capability in the biomedical domain yet. To this end, this paper aims to evaluate the performance of ChatGPT on various benchmark biomedical tasks, such as relation extraction, document classification, question answering, and summarization. To the best of our knowledge, this is the first work that conducts an extensive evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on our evaluation that in biomedical datasets that have smaller training sets, zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's pre-training on large text corpora makes it quite specialized even in the biomedical domain. Our findings demonstrate that ChatGPT has the potential to be a valuable tool for various tasks in the biomedical domain that la
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.18486</link><description>&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#31995;&#32479;&#30740;&#31350;&#21644;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets. (arXiv:2305.18486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978; ChatGPT &#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#12290;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914; ChatGPT &#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24320;&#21457;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38590;&#20197;&#23558;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20135;&#20986;&#19982;&#22522;&#26412;&#20107;&#23454;&#36827;&#34892;&#27604;&#36739;&#65292;&#22240;&#27492;&#20854;&#22312;&#22522;&#20934;&#23398;&#26415;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#23545; ChatGPT &#22312;&#21253;&#25324;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#20559;&#35265;&#26816;&#27979;&#21644;&#20262;&#29702;&#32771;&#34385;&#31561;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#24443;&#24213;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312; 140 &#20010;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102; ChatGPT&#65292;&#24182;&#20998;&#26512;&#20102;&#20854;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#30340; 255K &#27425;&#21709;&#24212;&#65292;&#36825;&#20351;&#25105;&#20204;&#30340;&#24037;&#20316;&#25104;&#20026;&#20102;&#22312; NLP &#22522;&#20934;&#27979;&#35797;&#20013;&#23545; ChatGPT &#36827;&#34892;&#30340;&#26368;&#22823;&#35780;&#20272;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#39564;&#35777; ChatGPT &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#65292;&#24182;&#20026;&#20351;&#29992; LLM &#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#19968;&#31181;&#26032;&#30340;&#36856;&#21457;&#33021;&#21147;&#65292;&#21363;&#36981;&#24490;&#22810;&#20010;&#26597;&#35810;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) such as ChatGPT has brought a lot of attention recently. However, their evaluation in the benchmark academic datasets remains under-explored due to the difficulty of evaluating the generative outputs produced by this model against the ground truth. In this paper, we aim to present a thorough evaluation of ChatGPT's performance on diverse academic datasets, covering tasks like question-answering, text summarization, code generation, commonsense reasoning, mathematical problem-solving, machine translation, bias detection, and ethical considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze 255K responses it generates in these datasets. This makes our work the largest evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate the strengths and weaknesses of ChatGPT in various tasks and provide insights for future research using LLMs. We also report a new emergent ability to follow multi-query instruct
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#26657;&#20934;&#21518;&#30340;Transformer&#27169;&#22411;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#29366;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.16797</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#21387;&#21147;&#21644;&#25233;&#37057;&#35782;&#21035;&#27169;&#22411;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Calibration of Transformer-based Models for Identifying Stress and Depression in Social Media. (arXiv:2305.16797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20351;&#29992;&#26657;&#20934;&#21518;&#30340;Transformer&#27169;&#22411;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#29366;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#24555;&#33410;&#22863;&#30340;&#29983;&#27963;&#20013;&#65292;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#30340;&#21457;&#30149;&#29575;&#21576;&#29616;&#19978;&#21319;&#36235;&#21183;&#12290;&#31038;&#20132;&#23186;&#20307;&#20026;&#26089;&#26399;&#21457;&#29616;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#20171;&#32461;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#24182;&#35757;&#32451;&#27973;&#23618;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#12290;&#20854;&#20182;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25110;Transformer&#27169;&#22411;&#12290;&#23613;&#31649;Transformer&#27169;&#22411;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#25429;&#25417;&#20016;&#23500;&#30340;&#23454;&#38469;&#30693;&#35782;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26088;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#30740;&#31350;&#65292;&#20294;&#27809;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#36825;&#20123;&#20462;&#25913;&#26469;&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#26816;&#27979;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20854;&#39044;&#27979;&#20013;&#30340;&#32622;&#20449;&#24230;&#21487;&#38752;&#24615;&#23545;&#20110;&#39640;&#39118;&#38505;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23578;&#26410;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#32771;&#34385;&#27169;&#22411;&#30340;&#26657;&#20934;&#12290;&#20026;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36890;&#36807;&#27169;&#22411;&#26657;&#20934;&#26469;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21387;&#21147;&#21644;&#25233;&#37057;&#30151;&#29366;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's fast-paced world, the rates of stress and depression present a surge. Social media provide assistance for the early detection of mental health conditions. Existing methods mainly introduce feature extraction approaches and train shallow machine learning classifiers. Other researches use deep neural networks or transformers. Despite the fact that transformer-based models achieve noticeable improvements, they cannot often capture rich factual knowledge. Although there have been proposed a number of studies aiming to enhance the pretrained transformer-based models with extra information or additional modalities, no prior work has exploited these modifications for detecting stress and depression through social media. In addition, although the reliability of a machine learning model's confidence in its predictions is critical for high-risk applications, there is no prior work taken into consideration the model calibration. To resolve the above issues, we present the first study i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#20102;&#35821;&#38899;&#20132;&#27969;&#20013;&#33258;&#21160;&#25233;&#37057;&#30151;&#26816;&#27979;&#35757;&#32451;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#25233;&#37057;&#30151;&#26816;&#27979;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;DAIC-WOZ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22522;&#20110;&#30495;&#23454;ASR&#30340;&#26368;&#20808;&#36827;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12263</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#35821;&#38899;&#20132;&#27969;&#20013;&#33258;&#25105;&#30417;&#30563;&#34920;&#31034;&#27861;&#22312;&#25233;&#37057;&#30151;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representations in speech-based depression detection. (arXiv:2305.12263v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#20102;&#35821;&#38899;&#20132;&#27969;&#20013;&#33258;&#21160;&#25233;&#37057;&#30151;&#26816;&#27979;&#35757;&#32451;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#23558;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#24773;&#24863;&#35782;&#21035;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#25233;&#37057;&#30151;&#26816;&#27979;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;DAIC-WOZ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22522;&#20110;&#30495;&#23454;ASR&#30340;&#26368;&#20808;&#36827;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#35821;&#38899;&#20132;&#27969;&#20013;&#33258;&#21160;&#25233;&#37057;&#30151;&#26816;&#27979;&#65288;SDD&#65289;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#12290;&#39318;&#20808;&#65292;&#23545;&#20174;&#19981;&#21516;&#23618;&#27425;&#30340;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#20013;&#24471;&#21040;&#30340;SSL&#34920;&#31034;&#36827;&#34892;&#20102;SDD&#20998;&#26512;&#65292;&#20174;&#32780;&#20026;&#25233;&#37057;&#30151;&#26816;&#27979;&#25552;&#20379;&#20102;&#21512;&#36866;&#30340;&#25351;&#26631;&#35265;&#35299;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#65292;&#20174;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#24773;&#24863;&#35782;&#21035;&#36716;&#31227;&#30693;&#35782;&#21040;SDD&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23558;ASR&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#19982;ASR&#25991;&#26412;&#20449;&#24687;&#30456;&#32467;&#21512;&#26102;&#65292;&#20351;&#29992;oracle&#21644;ASR&#36716;&#24405;&#20135;&#29983;&#20102;&#31867;&#20284;&#30340;SDD&#24615;&#33021;&#12290;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#31034;&#65292;&#22312;DAIC-WOZ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#22522;&#20110;&#30495;&#23454;ASR&#30340;&#26368;&#20808;&#36827;&#30340;SDD&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes handling training data sparsity in speech-based automatic depression detection (SDD) using foundation models pre-trained with self-supervised learning (SSL). An analysis of SSL representations derived from different layers of pre-trained foundation models is first presented for SDD, which provides insight to suitable indicator for depression detection. Knowledge transfer is then performed from automatic speech recognition (ASR) and emotion recognition to SDD by fine-tuning the foundation models. Results show that the uses of oracle and ASR transcriptions yield similar SDD performance when the hidden representations of the ASR model is incorporated along with the ASR textual information. By integrating representations from multiple foundation models, state-of-the-art SDD results based on real ASR were achieved on the DAIC-WOZ dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#27979;&#37327;&#20102;&#25919;&#27835;&#20559;&#35265;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#23558;&#31038;&#20250;&#20559;&#35265;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#23548;&#33268;NLP&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08283</link><description>&lt;p&gt;
&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#21040;&#35821;&#35328;&#27169;&#22411;&#20877;&#21040;&#19979;&#28216;&#20219;&#21153;&#65306;&#36861;&#36394;&#23548;&#33268;&#19981;&#20844;&#24179;NLP&#27169;&#22411;&#30340;&#25919;&#27835;&#20559;&#35265;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models. (arXiv:2305.08283v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#27979;&#37327;&#20102;&#25919;&#27835;&#20559;&#35265;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#27169;&#22411;&#23384;&#22312;&#25919;&#27835;&#20542;&#21521;&#65292;&#24182;&#23558;&#31038;&#20250;&#20559;&#35265;&#20256;&#36882;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#20174;&#32780;&#23548;&#33268;NLP&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;(LMs)&#26159;&#39044;&#35757;&#32451;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#19978;&#30340;&#65292;&#21253;&#25324;&#26032;&#38395;&#12289;&#35752;&#35770;&#35770;&#22363;&#12289;&#20070;&#31821;&#21644;&#22312;&#32447;&#30334;&#31185;&#20840;&#20070;&#31561;&#12290;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#30456;&#24403;&#19968;&#37096;&#20998;&#21253;&#25324;&#35266;&#28857;&#21644;&#35282;&#24230;&#65292;&#19968;&#26041;&#38754;&#36190;&#25196;&#27665;&#20027;&#21644;&#24605;&#24819;&#22810;&#26679;&#24615;&#65292;&#21478;&#19968;&#26041;&#38754;&#20855;&#26377;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24320;&#21457;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;(1)&#27979;&#37327;&#22522;&#20110;&#27492;&#31867;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;LMs&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#27839;&#31038;&#20250;&#21644;&#32463;&#27982;&#36724;&#65292;&#20197;&#21450;(2)&#34913;&#37327;&#22522;&#20110;&#25919;&#27835;&#20559;&#35265;&#30340;LMs&#35757;&#32451;&#30340;&#19979;&#28216;NLP&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#26088;&#22312;&#23454;&#35777;&#37327;&#21270;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#25919;&#27835;(&#31038;&#20250;&#12289;&#32463;&#27982;)&#20559;&#35265;&#23545;&#39640;&#39118;&#38505;&#31038;&#20250;&#23548;&#21521;&#20219;&#21153;&#20844;&#27491;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;LMs&#30830;&#23454;&#23384;&#22312;&#25919;&#27835;&#20542;&#21521;&#65292;&#21152;&#24378;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#26497;&#21270;&#65292;&#23558;&#31038;&#20250;&#20559;&#35265;&#20256;&#25773;&#21040;&#20167;&#24680;&#35328;&#35770;&#39044;&#27979;&#21644;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#22120;&#20013;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#30740;&#31350;&#30340;&#24847;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#24320;&#21457;&#26356;&#20844;&#27491;&#12289;&#26356;&#26080;&#20559;NLP&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are pretrained on diverse data sources, including news, discussion forums, books, and online encyclopedias. A significant portion of this data includes opinions and perspectives which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure political biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings that reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and misinformation detectors. We discuss the implications of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#30456;&#27604;&#65292;&#20174;T5-XXL&#33976;&#39311;&#21040;T5-Small&#20960;&#20046;&#24635;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;</title><link>http://arxiv.org/abs/2305.01645</link><description>&lt;p&gt;
&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#65306;&#33976;&#39311;&#36824;&#26159;&#26631;&#27880;&#65311;
&lt;/p&gt;
&lt;p&gt;
Distill or Annotate? Cost-Efficient Fine-Tuning of Compact Models. (arXiv:2305.01645v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21387;&#32553;&#27169;&#22411;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#21387;&#32553;&#27169;&#22411;&#30456;&#27604;&#65292;&#20174;T5-XXL&#33976;&#39311;&#21040;T5-Small&#20960;&#20046;&#24635;&#26159;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#27169;&#22411;&#30340;&#24494;&#35843;&#34429;&#28982;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#25104;&#26412;&#39640;&#19988;&#20250;&#20135;&#29983;&#30899;&#25490;&#25918;&#12290;&#30693;&#35782;&#33976;&#39311;&#24050;&#34987;&#35777;&#26126;&#26159;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#30340;&#23454;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#33976;&#39311;&#36807;&#31243;&#26412;&#36523;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#20351;&#29992;&#22266;&#23450;&#39044;&#31639;&#26500;&#24314;&#21387;&#32553;&#27169;&#22411;&#12290;&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;T5-XXL&#65288;11B&#65289;&#33976;&#39311;&#21040;T5-Small&#65288;60M&#65289;&#20960;&#20046;&#24635;&#26159;&#27604;&#25163;&#21160;&#26631;&#27880;&#26356;&#22810;&#30340;&#24494;&#35843;&#25968;&#25454;&#20197;&#30452;&#25509;&#35757;&#32451;&#19968;&#20010;&#21387;&#32553;&#27169;&#22411;&#65288;T5-Small&#65288;60M&#65289;&#65289;&#26356;&#20855;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#26368;&#22823;&#21270;&#25928;&#29992;&#30340;&#26368;&#20339;&#33976;&#39311;&#37327;&#22240;&#20219;&#21153;&#32780;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large models is highly effective, however, inference using these models can be expensive and produces carbon emissions. Knowledge distillation has been shown to be a practical solution to reduce inference costs, but the distillation process itself requires significant computational resources. Rather than buying or renting GPUs to fine-tune, then distill a large model, an NLP practitioner who needs a compact model might also choose to simply allocate an available budget to hire annotators and manually label additional fine-tuning data. In this paper, we investigate how to most efficiently use a fixed budget to build a compact model. Through our extensive experiments on six diverse NLP tasks, we find that distilling from T5-XXL (11B) to T5-Small (60M) leads to almost always a cost-efficient option compared to annotating more data to directly train a compact model (T5-Small (60M)). We further demonstrate that the optimal amount of distillation that maximizes utility varies acr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#21147;&#21450;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.13010</link><description>&lt;p&gt;
&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#65306;&#25105;&#20204;&#33021;&#21542;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#26368;&#20339;&#32467;&#26524;&#65311;
&lt;/p&gt;
&lt;p&gt;
Unstructured and structured data: Can we have the best of both worlds with large language models?. (arXiv:2304.13010v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#21147;&#21450;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#26080;&#32467;&#26500;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#24182;&#27010;&#36848;&#20102;&#26500;&#24314;&#36866;&#29992;&#20110;&#20004;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38382;&#31572;&#31995;&#32479;&#25152;&#28041;&#21450;&#30340;&#19968;&#20123;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an opinion on the potential of using large language models to query on both unstructured and structured data. It also outlines some research challenges related to the topic of building question-answering systems for both types of data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07729</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#30340;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Generation of Highlights from Research Papers Using Pointer-Generator Networks and SciBERT Embeddings. (arXiv:2302.07729v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07729
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#21644;SciBERT&#23884;&#20837;&#26469;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#20142;&#28857;&#30340;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30740;&#31350;&#20142;&#28857;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#35768;&#22810;&#30740;&#31350;&#25991;&#31456;&#37117;&#20197;&#30740;&#31350;&#20142;&#28857;&#20316;&#20026;&#21069;&#35328;&#65292;&#20197;&#24635;&#32467;&#35770;&#25991;&#30340;&#20027;&#35201;&#21457;&#29616;&#12290;&#20142;&#28857;&#19981;&#20165;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#35770;&#25991;&#30340;&#36129;&#29486;&#65292;&#36824;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#22686;&#21152;&#20102;&#25991;&#31456;&#30340;&#21487;&#21457;&#29616;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#30740;&#31350;&#35770;&#25991;&#30340;&#29305;&#23450;&#27573;&#33853;&#30340;&#24773;&#20917;&#19979;&#33258;&#21160;&#26500;&#24314;&#30740;&#31350;&#20142;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#35206;&#30422;&#26426;&#21046;&#21644;&#19978;&#19979;&#25991;&#23884;&#20837;&#23618;&#30340;&#25351;&#38024;&#29983;&#25104;&#32593;&#32476;&#65292;&#23558;&#36755;&#20837;&#26631;&#35760;&#32534;&#30721;&#20026;SciBERT&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;CSPubSum&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#36824;&#25552;&#20986;&#20102;MixSub&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#30740;&#31350;&#20142;&#28857;&#30340;&#26032;&#30340;&#36328;&#23398;&#31185;&#35770;&#25991;&#35821;&#26009;&#24211;&#12290;&#23545;&#20110;CSPubSum&#21644;MixSub&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#23545;&#20110;&#30456;&#20851;&#21464;&#20307;&#21644;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#26368;&#20339;&#24615;&#33021;&#12290;&#22312;CSPubSum&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#35770;&#25991;&#30340;&#25688;&#35201;&#20316;&#20026;&#36755;&#20837;&#26102;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays many research articles are prefaced with research highlights to summarize the main findings of the paper. Highlights not only help researchers precisely and quickly identify the contributions of a paper, they also enhance the discoverability of the article via search engines. We aim to automatically construct research highlights given certain segments of a research paper. We use a pointer-generator network with coverage mechanism and a contextual embedding layer at the input that encodes the input tokens into SciBERT embeddings. We test our model on a benchmark dataset, CSPubSum, and also present MixSub, a new multi-disciplinary corpus of papers for automatic research highlight generation. For both CSPubSum and MixSub, we have observed that the proposed model achieves the best performance compared to related variants and other models proposed in the literature. On the CSPubSum dataset, our model achieves the best performance when the input is only the abstract of a paper as op
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#21453;&#23545;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#36807;&#24230;&#21516;&#29702;&#24515;&#22238;&#24212;&#29992;&#25143;&#24773;&#32490;&#30340;&#35266;&#28857;&#65292;&#24378;&#35843;&#38656;&#35880;&#24910;&#32771;&#34385;&#23545;&#29992;&#25143;&#24773;&#32490;&#30340;&#22238;&#24212;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2212.10983</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#35828;&#8220;&#19981;&#8221;: &#21453;&#23545;&#26377;&#21516;&#29702;&#24515;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Computer says "No": The Case Against Empathetic Conversational AI. (arXiv:2212.10983v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10983
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#21453;&#23545;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#36807;&#24230;&#21516;&#29702;&#24515;&#22238;&#24212;&#29992;&#25143;&#24773;&#32490;&#30340;&#35266;&#28857;&#65292;&#24378;&#35843;&#38656;&#35880;&#24910;&#32771;&#34385;&#23545;&#29992;&#25143;&#24773;&#32490;&#30340;&#22238;&#24212;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#26159;&#20154;&#31867;&#35748;&#30693;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#23427;&#20204;&#19981;&#20165;&#25351;&#23548;&#30528;&#25105;&#20204;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#65292;&#20063;&#25351;&#23548;&#30528;&#25105;&#20204;&#22312;&#20854;&#20013;&#30340;&#34892;&#21160;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#24773;&#32490;&#30340;&#25242;&#24944;&#25110;&#28608;&#24594;&#24182;&#38750;&#26080;&#20851;&#32039;&#35201;&#12290;&#36817;&#26399;&#22312;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#23545;&#29992;&#25143;&#20570;&#20986;&#26377;&#21516;&#29702;&#24515;&#30340;&#22238;&#24212;&#65292;&#39564;&#35777;&#21644;&#25242;&#24944;&#20182;&#20204;&#30340;&#24773;&#32490;&#65292;&#21363;&#20351;&#27809;&#26377;&#30495;&#23454;&#30340;&#22522;&#30784;&#12290;&#36825;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24773;&#32490;&#35843;&#33410;&#21487;&#33021;&#23545;&#29992;&#25143;&#21644;&#31038;&#20250;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#65292;&#36235;&#21521;&#20110;&#23558;&#24184;&#31119;&#23450;&#20041;&#20026;&#20165;&#20165;&#26159;&#8220;&#36127;&#38754;&#8221;&#24773;&#32490;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#24517;&#39035;&#35880;&#24910;&#32771;&#34385;&#26159;&#21542;&#20197;&#21450;&#22914;&#20309;&#22238;&#24212;&#29992;&#25143;&#30340;&#24773;&#32490;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotions are an integral part of human cognition and they guide not only our understanding of the world but also our actions within it. As such, whether we soothe or flame an emotion is not inconsequential. Recent work in conversational AI has focused on responding empathetically to users, validating and soothing their emotions without a real basis. This AI-aided emotional regulation can have negative consequences for users and society, tending towards a one-noted happiness defined as only the absence of "negative" emotions. We argue that we must carefully consider whether and how to respond to users' emotions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2212.09811</link><description>&lt;p&gt;
&#39640;&#25928;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#19987;&#23478;&#21024;&#20943;
&lt;/p&gt;
&lt;p&gt;
Memory-efficient NLLB-200: Language-specific Expert Pruning of a Massively Multilingual Machine Translation Model. (arXiv:2212.09811v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33410;&#32422;&#20869;&#23384;&#30340;NLLB-200&#27169;&#22411;&#20462;&#21098;&#26041;&#27861;&#65292;&#21487;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#31227;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#23545;&#20110;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#21452;&#35821;&#32763;&#35793;&#31995;&#32479;&#30456;&#27604;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#21487;&#20197;&#32763;&#35793;&#25104;&#22810;&#31181;&#35821;&#35328;&#65292;&#24182;&#20174;&#30693;&#35782;&#36716;&#31227;&#20013;&#33719;&#30410;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#21463;&#21040;&#22810;&#35821;&#35328;&#24615;&#30340;&#38480;&#21046;&#65292;&#38500;&#38750;&#36827;&#34892;&#22823;&#35268;&#27169;&#25193;&#23637;&#65292;&#21542;&#21017;&#20250;&#22686;&#21152;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#31232;&#30095;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#26159;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22823;&#24133;&#22686;&#21152;&#27169;&#22411;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#21457;&#24067;&#30340;NLLB-200&#26159;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#20363;&#23376;&#12290;&#23427;&#28085;&#30422;&#20102;202&#31181;&#35821;&#35328;&#65292;&#20294;&#20165;&#25512;&#29702;&#23601;&#38656;&#35201;&#33267;&#23569;&#22235;&#20010;32GB&#30340;GPU&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#21098;&#26041;&#27861;&#65292;&#20801;&#35768;&#21024;&#38500;&#22810;&#36798;80&#65285;&#30340;&#19987;&#23478;&#65292;&#20294;&#32763;&#35793;&#36136;&#37327;&#20960;&#20046;&#27809;&#26377;&#25439;&#22833;&#65292;&#36825;&#20351;&#24471;&#22312;&#21333;&#20010;32GB&#30340;GPU&#19978;&#36816;&#34892;&#35813;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#20462;&#21098;&#24230;&#37327;&#25351;&#26631;&#21487;&#20197;&#35782;&#21035;&#20986;&#35821;&#35328;&#29305;&#23450;&#30340;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Compared to conventional bilingual translation systems, massively multilingual machine translation is appealing because a single model can translate into multiple languages and benefit from knowledge transfer for low resource languages. On the other hand, massively multilingual models suffer from the curse of multilinguality, unless scaling their size massively, which increases their training and inference costs. Sparse Mixture-of-Experts models are a way to drastically increase model capacity without the need for a proportional amount of computing. The recently released NLLB-200 is an example of such a model. It covers 202 languages but requires at least four 32GB GPUs just for inference. In this work, we propose a pruning method that allows the removal of up to 80\% of experts with a negligible loss in translation quality, which makes it feasible to run the model on a single 32GB GPU. Further analysis suggests that our pruning metrics allow to identify language-specific experts and p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25506;&#32034;&#20102;&#20854;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.00552</link><description>&lt;p&gt;
&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20013;&#26377;&#25928;&#22320;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
An Effective Employment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#25506;&#32034;&#20102;&#20854;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#23578;&#24453;&#25506;&#32034;&#21644;&#20998;&#26512;&#12290;&#22914;&#20309;&#27491;&#30830;&#21512;&#29702;&#22320;&#26500;&#24314;&#27491;&#36127;&#26679;&#26412;&#26159;&#23545;&#27604;&#23398;&#20064;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#32780;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#21457;&#29616;&#23545;&#27604;&#23545;&#35937;&#26356;&#21152;&#22256;&#38590;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20116;&#20010;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#21253;&#25324;&#20005;&#26684;&#23545;&#27604;&#25439;&#22833; (SCL)&#12289;&#20869;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833; (ICL)&#12289;Jaccard&#30456;&#20284;&#24230;&#23545;&#27604;&#25439;&#22833;(JSCL)&#12289;Jaccard&#30456;&#20284;&#24230;&#27010;&#29575;&#23545;&#27604;&#25439;&#22833;(JSPCL)&#21644;&#36880;&#27493;&#26631;&#31614;&#23545;&#27604;&#25439;&#22833;(SLCL)&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25506;&#32034;&#20102;&#23545;&#27604;&#23398;&#20064;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#37096;&#32626;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#25552;&#20379;&#20102;&#19968;&#22871;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of contrastive learning technology in natural language processing tasks is yet to be explored and analyzed. How to construct positive and negative samples correctly and reasonably is the core challenge of contrastive learning. It is even harder to discover contrastive objects in multi-label text classification tasks. There are very few contrastive losses proposed previously. In this paper, we investigate the problem from a different angle by proposing five novel contrastive losses for multi-label text classification tasks. These are Strict Contrastive Loss (SCL), Intra-label Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive Loss (SLCL). We explore the effectiveness of contrastive learning for multi-label text classification tasks by the employment of these novel losses and provide a set of baseline models for deploying contrastive learning techniques on specific t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#36716;&#21270;&#20026;&#24369;&#30417;&#30563;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#30495;&#27491;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#25193;&#23637;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#26102;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02499</link><description>&lt;p&gt;
&#20855;&#26377;&#30495;&#27491;&#38646;-shot&#33021;&#21147;&#30340;&#24369;&#30417;&#30563;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Weakly-Supervised Streaming Multilingual Speech Model with Truly Zero-Shot Capability. (arXiv:2211.02499v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24369;&#30417;&#30563;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65292;&#21033;&#29992;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#36716;&#21270;&#20026;&#24369;&#30417;&#30563;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#30495;&#27491;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#25193;&#23637;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#26102;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#26500;&#24314;&#30340;&#27969;&#24335;&#22810;&#35821;&#35328;&#35821;&#38899;&#27169;&#22411;&#65288;SM2&#65289;&#30340;&#24037;&#20316;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#22810;&#31181;&#21475;&#35821;&#35821;&#35328;&#36716;&#24405;&#25110;&#32763;&#35793;&#20026;&#30446;&#26631;&#35821;&#35328;&#30340;&#25991;&#26412;&#12290;SM2&#30340;&#26680;&#24515;&#26159;Transformer Transducer&#65292;&#20855;&#26377;&#39640;&#24230;&#27969;&#24335;&#22788;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#26381;&#21153;&#23558;&#35821;&#38899;&#35782;&#21035;&#35821;&#26009;&#24211;&#20013;&#30340;&#36716;&#24405;&#36716;&#21270;&#32780;&#25104;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#26469;&#35757;&#32451;SM2&#27169;&#22411;&#65292;&#32780;&#38750;&#20154;&#24037;&#26631;&#35760;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#12290;&#21033;&#29992;&#26469;&#33258;25&#31181;&#35821;&#35328;&#30340;35.1&#19975;&#23567;&#26102;&#30340;&#21311;&#21517;&#35821;&#38899;&#35757;&#32451;&#25968;&#25454;&#65292;SM2&#27169;&#22411;&#30340;&#35821;&#38899;&#32763;&#35793;&#36136;&#37327;&#19982;&#26576;&#20123;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#38750;&#27969;&#24335;&#35821;&#38899;&#27169;&#22411;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#25193;&#23637;&#21040;&#26032;&#30340;&#30446;&#26631;&#35821;&#35328;&#26102;&#65292;SM2&#20855;&#26377;&#30495;&#27491;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#21487;&#20197;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#35265;&#36807;&#30340;{&#28304;&#35821;&#38899;&#65292;&#30446;&#26631;&#25991;&#26412;}&#23545;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce our work of building a Streaming Multilingual Speech Model (SM2), which can transcribe or translate multiple spoken languages into texts of the target language. The backbone of SM2 is Transformer Transducer, which has high streaming capability. Instead of human labeled speech translation (ST) data, SM2 models are trained using weakly supervised data generated by converting the transcriptions in speech recognition corpora with a machine translation service. With 351 thousand hours of anonymized speech training data from 25 languages, SM2 models achieve comparable or even better ST quality than some recent popular large-scale non-streaming speech models. More importantly, we show that SM2 has the truly zero-shot capability when expanding to new target languages, yielding high quality ST results for {source-speech, target-text} pairs that are not seen during training.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65292;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#28151;&#28102;&#35789;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#32416;&#27491;&#30340;&#20934;&#30830;&#24615;&#21644;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.13823</link><description>&lt;p&gt;
&#22522;&#20110;&#36870;&#23545;&#27604;&#23398;&#20064;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Chinese Spelling Check Framework Based on Reverse Contrastive Learning. (arXiv:2210.13823v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13823
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36870;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;&#29992;&#20110;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65292;&#36890;&#36807;&#22686;&#24378;&#27169;&#22411;&#23545;&#28151;&#28102;&#35789;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#26816;&#27979;&#21644;&#32416;&#27491;&#30340;&#20934;&#30830;&#24615;&#21644;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#26159;&#19968;&#39033;&#26816;&#27979;&#21644;&#32416;&#27491;&#20013;&#25991;&#25991;&#26412;&#20013;&#25340;&#20889;&#38169;&#35823;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30740;&#31350;&#26088;&#22312;&#22686;&#24378;&#25991;&#26412;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#22810;&#28304;&#20449;&#24687;&#25552;&#39640;&#27169;&#22411;&#30340;&#26816;&#27979;&#21644;&#32416;&#27491;&#33021;&#21147;&#65292;&#20294;&#23545;&#20110;&#25913;&#21892;&#27169;&#22411;&#21306;&#20998;&#28151;&#28102;&#35789;&#30340;&#33021;&#21147;&#24182;&#26410;&#32473;&#20104;&#36275;&#22815;&#20851;&#27880;&#12290;&#23545;&#27604;&#23398;&#20064;&#36817;&#24180;&#26469;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#25104;&#20026;&#19968;&#31181;&#20027;&#35201;&#25216;&#26415;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#26368;&#23567;&#21270;&#30456;&#20284;&#26679;&#26412;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#26694;&#26550;&#65292;&#21253;&#25324;&#35821;&#35328;&#34920;&#31034;&#12289;&#25340;&#20889;&#26816;&#26597;&#21644;&#36870;&#23545;&#27604;&#23398;&#20064;&#19977;&#20010;&#27169;&#22359;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36870;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#26126;&#30830;&#35201;&#27714;&#27169;&#22411;&#26368;&#23567;&#21270;&#30456;&#20284;&#20363;&#23376;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21363;&#38899;&#31526;&#21644;&#35270;&#35273;&#19978;&#26131;&#28151;&#28102;&#30340;&#23383;&#31526;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#25552;&#39640;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#30340;&#20934;&#30830;&#24615;&#21644;&#32416;&#38169;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chinese spelling check is a task to detect and correct spelling mistakes in Chinese text. Existing research aims to enhance the text representation and use multi-source information to improve the detection and correction capabilities of models, but does not pay too much attention to improving their ability to distinguish between confusable words. Contrastive learning, whose aim is to minimize the distance in representation space between similar sample pairs, has recently become a dominant technique in natural language processing. Inspired by contrastive learning, we present a novel framework for Chinese spelling checking, which consists of three modules: language representation, spelling check and reverse contrastive learning. Specifically, we propose a reverse contrastive learning strategy, which explicitly forces the model to minimize the agreement between the similar examples, namely, the phonetically and visually confusable characters. Experimental results show that our framework i
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32445;&#32422;&#23458;&#28459;&#30011;&#23383;&#24149;&#27604;&#36187;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#23545;&#24189;&#40664;&#30340;&#8220;&#29702;&#35299;&#8221;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#22810;&#27169;&#24577;&#27169;&#22411;&#36824;&#26159;&#20165;&#25991;&#26412;&#27169;&#22411;&#65292;&#22312;&#37197;&#23545;&#31505;&#35805;&#21644;&#28459;&#30011;&#12289;&#35782;&#21035;&#33719;&#32988;&#23383;&#24149;&#20197;&#21450;&#35299;&#37322;&#33719;&#32988;&#23383;&#24149;&#20026;&#20160;&#20040;&#26377;&#36259;&#30340;&#20219;&#21153;&#19978;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2209.06293</link><description>&lt;p&gt;
Android&#20204;&#26159;&#21542;&#20250;&#31505;&#30005;&#23376;&#32650;&#65311;&#28304;&#33258;&#32445;&#32422;&#23458;&#28459;&#30011;&#23383;&#24149;&#27604;&#36187;&#30340;&#24189;&#40664;&#8220;&#29702;&#35299;&#8221;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do Androids Laugh at Electric Sheep? Humor "Understanding" Benchmarks from The New Yorker Caption Contest. (arXiv:2209.06293v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06293
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32445;&#32422;&#23458;&#28459;&#30011;&#23383;&#24149;&#27604;&#36187;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;AI&#27169;&#22411;&#23545;&#24189;&#40664;&#30340;&#8220;&#29702;&#35299;&#8221;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#22810;&#27169;&#24577;&#27169;&#22411;&#36824;&#26159;&#20165;&#25991;&#26412;&#27169;&#22411;&#65292;&#22312;&#37197;&#23545;&#31505;&#35805;&#21644;&#28459;&#30011;&#12289;&#35782;&#21035;&#33719;&#32988;&#23383;&#24149;&#20197;&#21450;&#35299;&#37322;&#33719;&#32988;&#23383;&#24149;&#20026;&#20160;&#20040;&#26377;&#36259;&#30340;&#20219;&#21153;&#19978;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#31505;&#35805;&#65292;&#20294;&#23427;&#20204;&#30495;&#27491;&#8220;&#29702;&#35299;&#8221;&#24189;&#40664;&#21527;&#65311;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#20219;&#21153;&#25361;&#25112;AI&#27169;&#22411;&#65292;&#36825;&#20123;&#20219;&#21153;&#28304;&#33258;&#20110;&#32445;&#32422;&#23458;&#28459;&#30011;&#23383;&#24149;&#27604;&#36187;&#65306;&#23558;&#31505;&#35805;&#19982;&#28459;&#30011;&#37197;&#23545;&#12289;&#35782;&#21035;&#33719;&#32988;&#23383;&#24149;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#33719;&#32988;&#30340;&#23383;&#24149;&#24456;&#26377;&#36259;&#12290;&#36825;&#20123;&#20219;&#21153;&#36880;&#28176;&#21253;&#21547;&#20102;&#8220;&#29702;&#35299;&#8221;&#28459;&#30011;&#30340;&#26356;&#22797;&#26434;&#26041;&#38754;&#65307;&#20851;&#38190;&#22240;&#32032;&#26159;&#22270;&#20687;&#19982;&#23383;&#24149;&#20043;&#38388;&#30340;&#22797;&#26434;&#12289;&#24120;&#24120;&#20196;&#20154;&#24778;&#35766;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#39057;&#32321;&#21253;&#21547;&#23545;&#20154;&#31867;&#32463;&#39564;&#21644;&#25991;&#21270;&#30340;&#38388;&#25509;&#21644;&#23500;&#26377;&#29609;&#21619;&#30340;&#26263;&#31034;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#27169;&#24577;&#21644;&#20165;&#25991;&#26412;&#27169;&#22411;&#65306;&#21069;&#32773;&#30452;&#25509;&#38754;&#23545;&#28459;&#30011;&#22270;&#20687;&#36827;&#34892;&#25361;&#25112;&#65292;&#32780;&#21518;&#32773;&#21017;&#32473;&#20986;&#20102;&#22810;&#26041;&#38754;&#30340;&#35270;&#35273;&#22330;&#26223;&#25551;&#36848;&#20197;&#27169;&#25311;&#20154;&#31867;&#32423;&#21035;&#30340;&#35270;&#35273;&#29702;&#35299;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#37117;&#38754;&#20020;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#37197;&#23545;&#20219;&#21153;&#19978;&#30340;&#20934;&#30830;&#29575;&#27604;&#20154;&#31867;&#34920;&#29616;&#20302;30&#20010;&#30334;&#20998;&#28857;&#65292;&#21363;&#20415;&#22312;&#25552;&#20379;&#20102;&#22270;&#20687;&#25551;&#36848;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large neural networks can now generate jokes, but do they really "understand" humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of "understanding" a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided gr
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#21487;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#30456;&#27604;&#23384;&#22312;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#21644;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#32553;&#23567;&#36825;&#19968;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2204.09269</link><description>&lt;p&gt;
&#38024;&#23545;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21450;&#20854;&#25193;&#23637;&#30340;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Non-Autoregressive Generation for Neural Machine Translation and Beyond. (arXiv:2204.09269v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09269
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#20854;&#20182;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#38750;&#33258;&#22238;&#24402;&#29983;&#25104;&#21487;&#20197;&#21152;&#24555;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#30456;&#27604;&#23384;&#22312;&#32763;&#35793;&#20934;&#30830;&#24615;&#30340;&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#21508;&#31181;&#26041;&#27861;&#21644;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#32553;&#23567;&#36825;&#19968;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#29983;&#25104;&#39318;&#27425;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#25552;&#20986;&#65292;&#26088;&#22312;&#21152;&#36895;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#24341;&#36215;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;NAR&#29983;&#25104;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#26426;&#22120;&#32763;&#35793;&#30340;&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#19982;&#20854;&#23545;&#24212;&#30340;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#29983;&#25104;&#30456;&#27604;&#65292;&#20854;&#32763;&#35793;&#20934;&#30830;&#24615;&#26377;&#25152;&#38477;&#20302;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26032;&#27169;&#22411;&#21644;&#31639;&#27861;&#24050;&#34987;&#35774;&#35745;/&#25552;&#20986;&#20197;&#24357;&#34917;NAR&#29983;&#25104;&#19982;AR&#29983;&#25104;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#26041;&#38754;&#30340;&#21508;&#31181;&#38750;&#33258;&#22238;&#24402;&#32763;&#35793;&#65288;NAT&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#35843;&#26597;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;NAT&#30340;&#21162;&#21147;&#20998;&#25104;&#20102;&#20960;&#20010;&#26041;&#21521;&#65292;&#21253;&#25324;&#25968;&#25454;&#22788;&#29702;&#12289;&#24314;&#27169;&#26041;&#27861;&#12289;&#35757;&#32451;&#26631;&#20934;&#12289;&#35299;&#30721;&#31639;&#27861;&#20197;&#21450;&#26469;&#33258;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30410;&#22788;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31616;&#35201;&#22238;&#39038;&#20102;NAR&#27169;&#22411;&#30340;&#20854;&#20182;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive (NAR) generation, which is first proposed in neural machine translation (NMT) to speed up inference, has attracted much attention in both machine learning and natural language processing communities. While NAR generation can significantly accelerate inference speed for machine translation, the speedup comes at the cost of sacrificed translation accuracy compared to its counterpart, autoregressive (AR) generation. In recent years, many new models and algorithms have been designed/proposed to bridge the accuracy gap between NAR generation and AR generation. In this paper, we conduct a systematic survey with comparisons and discussions of various non-autoregressive translation (NAT) models from different aspects. Specifically, we categorize the efforts of NAT into several groups, including data manipulation, modeling methods, training criterion, decoding algorithms, and the benefit from pre-trained models. Furthermore, we briefly review other applications of NAR models 
&lt;/p&gt;</description></item></channel></rss>