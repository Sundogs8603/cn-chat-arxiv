<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://rss.arxiv.org/abs/2311.05112</link><description>&lt;p&gt;
&#21307;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;&#65306;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models in Medicine: Principles, Applications, and Challenges
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2311.05112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#21516;&#26102;&#22238;&#31572;&#20102;&#21307;&#23398;LLMs&#30340;&#26500;&#24314;&#12289;&#19979;&#28216;&#24615;&#33021;&#12289;&#23454;&#38469;&#24212;&#29992;&#12289;&#25361;&#25112;&#20197;&#21450;&#26356;&#22909;&#26500;&#24314;&#21644;&#21033;&#29992;&#30340;&#38382;&#39064;&#12290;&#26088;&#22312;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#25552;&#20379;&#35265;&#35299;&#21644;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#20854;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#21307;&#23398;&#20013;&#65292;LLMs&#22312;&#21327;&#21161;&#21307;&#29983;&#36827;&#34892;&#24739;&#32773;&#25252;&#29702;&#26041;&#38754;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#21307;&#23398;&#20013;LLMs&#30340;&#21407;&#29702;&#12289;&#24212;&#29992;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#22238;&#31572;&#20102;&#20197;&#19979;&#20855;&#20307;&#38382;&#39064;&#65306;1&#65289;&#22914;&#20309;&#26500;&#24314;&#21307;&#23398;LLMs&#65311;2&#65289;&#20160;&#20040;&#26159;&#21307;&#23398;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#35780;&#20272;&#25351;&#26631;&#65311;3&#65289;&#22312;&#29616;&#23454;&#20020;&#24202;&#23454;&#36341;&#20013;&#65292;&#22914;&#20309;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;4&#65289;&#20351;&#29992;&#21307;&#23398;LLMs&#20250;&#20986;&#29616;&#21738;&#20123;&#25361;&#25112;&#65311;5&#65289;&#22914;&#20309;&#26356;&#22909;&#22320;&#26500;&#24314;&#21644;&#21033;&#29992;&#21307;&#23398;LLMs&#65311;&#26412;&#32508;&#36848;&#26088;&#22312;&#25552;&#20379;&#20851;&#20110;&#21307;&#23398;&#20013;LLMs&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#30340;&#35265;&#35299;&#65292;&#24182;&#20316;&#20026;&#26500;&#24314;&#26377;&#25928;&#30340;&#21307;&#23398;LLMs&#30340;&#23454;&#29992;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#32500;&#25252;&#24182;&#23450;&#26399;&#26356;&#26032;&#19968;&#20010;&#28165;&#21333;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. LLMs in medicine to assist physicians for patient care are emerging as a promising research direction in both artificial intelligence and clinical medicine. This review provides a comprehensive overview of the principles, applications, and challenges faced by LLMs in medicine. We address the following specific questions: 1) How should medical LLMs be built? 2) What are the measures for the downstream performance of medical LLMs? 3) How should medical LLMs be utilized in real-world clinical practice? 4) What challenges arise from the use of medical LLMs? and 5) How should we better construct and utilize medical LLMs? This review aims to provide insights into the opportunities and challenges of LLMs in medicine, and serve as a practical resource for constructing effective medical LLMs. We also maintain and regularly updated list of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2402.04291</link><description>&lt;p&gt;
BiLLM: &#25512;&#21160;LLMs&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
BiLLM: Pushing the Limit of Post-Training Quantization for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04291
&lt;/p&gt;
&lt;p&gt;
BiLLM&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLMs&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#30340;&#26435;&#37325;&#21644;&#20248;&#21270;&#20108;&#20540;&#21270;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20986;&#33394;&#30340;&#36890;&#29992;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20294;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#26377;&#24456;&#22823;&#30340;&#38656;&#27714;&#12290;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20108;&#20540;&#21270;&#21487;&#20197;&#23558;&#27169;&#22411;&#26435;&#37325;&#26497;&#22823;&#22320;&#20943;&#23569;&#21040;&#20165;1&#20301;&#65292;&#38477;&#20302;&#20102;&#26114;&#36149;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#37327;&#21270;&#25216;&#26415;&#22312;&#36229;&#20302;&#20301;&#23485;&#19979;&#26080;&#27861;&#20445;&#25345;LLM&#30340;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BiLLM&#65292;&#36825;&#26159;&#19968;&#31181;&#38024;&#23545;&#39044;&#35757;&#32451;LLM&#23450;&#21046;&#30340;&#24320;&#21019;&#24615;&#30340;1&#20301;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#26696;&#12290;&#22522;&#20110;LLMs&#30340;&#26435;&#37325;&#20998;&#24067;&#65292;BiLLM&#39318;&#20808;&#35782;&#21035;&#21644;&#32467;&#26500;&#36873;&#25321;&#37325;&#35201;&#30340;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#26377;&#25928;&#30340;&#20108;&#20540;&#21270;&#27531;&#24046;&#36924;&#36817;&#31574;&#30053;&#26469;&#26368;&#23567;&#21270;&#21387;&#32553;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#38750;&#37325;&#35201;&#26435;&#37325;&#30340;&#38047;&#24418;&#20998;&#24067;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20339;&#20998;&#21106;&#25628;&#32034;&#26041;&#27861;&#65292;&#20197;&#20934;&#30830;&#22320;&#23558;&#23427;&#20204;&#20998;&#32452;&#21644;&#20108;&#20540;&#21270;&#12290;BiLLM&#39318;&#27425;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24230;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large language models (LLMs) exhibit exceptional general language processing capabilities but come with significant demands on memory and computational resources. As a powerful compression technology, binarization can extremely reduce model weights to a mere 1 bit, lowering the expensive computation and memory requirements. However, existing quantization techniques fall short of maintaining LLM performance under ultra-low bit-widths. In response to this challenge, we present BiLLM, a groundbreaking 1-bit post-training quantization scheme tailored for pretrained LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally selects salient weights, and minimizes the compression loss through an effective binary residual approximation strategy. Moreover, considering the bell-shaped distribution of the non-salient weights, we propose an optimal splitting search to group and binarize them accurately. BiLLM achieving for the first time high-accuracy infere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01766</link><description>&lt;p&gt;
LLM&#25237;&#31080;&#65306;&#20154;&#31867;&#36873;&#25321;&#21644;AI&#38598;&#20307;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
LLM Voting: Human Choices and AI Collective Decision Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#25581;&#31034;&#20102;LLMs&#19982;&#20154;&#31867;&#22312;&#20915;&#31574;&#21644;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#21035;&#26159;OpenAI&#30340;GPT4&#21644;LLaMA2&#30340;&#25237;&#31080;&#34892;&#20026;&#65292;&#24182;&#19982;&#20154;&#31867;&#25237;&#31080;&#27169;&#24335;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36827;&#34892;&#20154;&#31867;&#25237;&#31080;&#23454;&#39564;&#20197;&#24314;&#31435;&#20154;&#31867;&#20559;&#22909;&#30340;&#22522;&#20934;&#65292;&#24182;&#19982;LLM&#20195;&#29702;&#36827;&#34892;&#24179;&#34892;&#23454;&#39564;&#12290;&#30740;&#31350;&#32858;&#28966;&#20110;&#38598;&#20307;&#32467;&#26524;&#21644;&#20010;&#20307;&#20559;&#22909;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#21644;LLMs&#20043;&#38388;&#22312;&#20915;&#31574;&#21644;&#22266;&#26377;&#20559;&#35265;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLMs&#22312;&#20559;&#22909;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#30456;&#27604;&#20154;&#31867;&#36873;&#27665;&#30340;&#22810;&#26679;&#20559;&#22909;&#65292;LLMs&#26377;&#26356;&#36235;&#21521;&#20110;&#19968;&#33268;&#36873;&#25321;&#30340;&#20542;&#21521;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#25237;&#31080;&#36741;&#21161;&#20013;&#20351;&#29992;LLMs&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#21516;&#36136;&#21270;&#30340;&#38598;&#20307;&#32467;&#26524;&#65292;&#24378;&#35843;&#20102;&#35880;&#24910;&#23558;LLMs&#25972;&#21512;&#21040;&#27665;&#20027;&#36807;&#31243;&#20013;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the voting behaviors of Large Language Models (LLMs), particularly OpenAI's GPT4 and LLaMA2, and their alignment with human voting patterns. Our approach included a human voting experiment to establish a baseline for human preferences and a parallel experiment with LLM agents. The study focused on both collective outcomes and individual preferences, revealing differences in decision-making and inherent biases between humans and LLMs. We observed a trade-off between preference diversity and alignment in LLMs, with a tendency towards more uniform choices as compared to the diverse preferences of human voters. This finding indicates that LLMs could lead to more homogenized collective outcomes when used in voting assistance, underscoring the need for cautious integration of LLMs into democratic processes.
&lt;/p&gt;</description></item><item><title>&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#24635;&#32467;&#20986;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#12290;&#36825;&#20123;&#29305;&#23450;&#20260;&#23475;&#28041;&#21450;&#21040;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.01708</link><description>&lt;p&gt;
&#19981;&#26159;&#25105;&#30340;&#22768;&#38899;&#65281;&#35821;&#38899;&#29983;&#25104;&#22120;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#20260;&#23475;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not My Voice! A Taxonomy of Ethical and Safety Harms of Speech Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01708
&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#24635;&#32467;&#20986;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#12290;&#36825;&#20123;&#29305;&#23450;&#20260;&#23475;&#28041;&#21450;&#21040;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24191;&#27867;&#37319;&#29992;&#35821;&#38899;&#29983;&#25104;&#25216;&#26415;&#32473;&#31038;&#20250;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#37325;&#22823;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#65292;&#20127;&#38656;&#35299;&#20915;&#12290;&#20363;&#22914;&#65292;&#22312;&#32654;&#22269;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#19982;&#35686;&#23519;&#21463;&#21040;&#24694;&#20316;&#21095;&#34989;&#20987;&#26377;&#20851;&#65292;&#21311;&#21517;&#34892;&#20026;&#32773;&#21046;&#36896;&#21512;&#25104;&#30340;&#22768;&#38899;&#25171;&#30005;&#35805;&#32473;&#35686;&#23519;&#65292;&#35201;&#27714;&#20851;&#38381;&#23398;&#26657;&#21644;&#21307;&#38498;&#65292;&#25110;&#32773;&#20197;&#26292;&#21147;&#25163;&#27573;&#36827;&#20837;&#26080;&#36764;&#24066;&#27665;&#30340;&#23478;&#20013;&#12290;&#36825;&#26679;&#30340;&#20107;&#20214;&#34920;&#26126;&#65292;&#22810;&#27169;&#24577;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#39118;&#38505;&#21644;&#20260;&#23475;&#24182;&#19981;&#23384;&#22312;&#20110;&#23396;&#31435;&#29366;&#24577;&#65292;&#32780;&#26159;&#28304;&#20110;&#22810;&#20010;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20043;&#38388;&#30340;&#20114;&#21160;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35821;&#38899;&#29983;&#25104;&#20107;&#20214;&#65292;&#30740;&#31350;&#29305;&#23450;&#20260;&#23475;&#27169;&#24335;&#30340;&#20986;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#29305;&#23450;&#20260;&#23475;&#21487;&#20197;&#26681;&#25454;&#21463;&#24433;&#21709;&#20010;&#20307;&#30340;&#26333;&#20809;&#31243;&#24230;&#36827;&#34892;&#20998;&#31867;&#65292;&#21363;&#20182;&#20204;&#26159;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#20027;&#20307;&#12289;&#19982;&#20043;&#20114;&#21160;&#12289;&#21463;&#20854;&#24433;&#21709;&#25110;&#34987;&#25490;&#38500;&#22312;&#22806;&#12290;&#21516;&#26679;&#65292;&#29305;&#23450;&#20260;&#23475;&#20063;&#19982;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#25216;&#26415;&#31995;&#32479;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid and wide-scale adoption of AI to generate human speech poses a range of significant ethical and safety risks to society that need to be addressed. For example, a growing number of speech generation incidents are associated with swatting attacks in the United States, where anonymous perpetrators create synthetic voices that call police officers to close down schools and hospitals, or to violently gain access to innocent citizens' homes. Incidents like this demonstrate that multimodal generative AI risks and harms do not exist in isolation, but arise from the interactions of multiple stakeholders and technical AI systems. In this paper we analyse speech generation incidents to study how patterns of specific harms arise. We find that specific harms can be categorised according to the exposure of affected individuals, that is to say whether they are a subject of, interact with, suffer due to, or are excluded from speech generation systems. Similarly, specific harms are also a con
&lt;/p&gt;</description></item><item><title>&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2401.16092</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#25918;&#22823;&#20102;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#24182;&#19988;&#20462;&#27491;&#24037;&#31243;&#21487;&#33021;&#26080;&#27861;&#24110;&#21161;&#24744;
&lt;/p&gt;
&lt;p&gt;
Multilingual Text-to-Image Generation Magnifies Gender Stereotypes and Prompt Engineering May Not Help You
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.16092
&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#23384;&#22312;&#24615;&#21035;&#20559;&#35265;&#65307;&#36890;&#36807;MAGBIG&#35780;&#20272;&#27169;&#22411;&#26102;&#65292;&#21457;&#29616;&#27169;&#22411;&#23545;&#19981;&#21516;&#35821;&#35328;&#20855;&#26377;&#37325;&#35201;&#24046;&#24322;&#65307;&#25105;&#20204;&#21628;&#21505;&#30740;&#31350;&#22810;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#28040;&#38500;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#36136;&#37327;&#12289;&#28789;&#27963;&#24615;&#21644;&#25991;&#26412;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#65292;&#24182;&#22240;&#27492;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#36890;&#36807;&#25913;&#21892;&#22810;&#35821;&#35328;&#33021;&#21147;&#65292;&#26356;&#22810;&#30340;&#31038;&#32676;&#29616;&#22312;&#21487;&#20197;&#35775;&#38382;&#36825;&#31181;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#27491;&#22914;&#25105;&#20204;&#23558;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#19982;&#21333;&#35821;&#27169;&#22411;&#19968;&#26679;&#21463;&#21040;(&#24615;&#21035;)&#20559;&#35265;&#30340;&#22256;&#25200;&#12290;&#27492;&#22806;&#65292;&#20154;&#20204;&#33258;&#28982;&#26399;&#26395;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#25552;&#20379;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#20294;&#20107;&#23454;&#24182;&#38750;&#22914;&#27492;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#37325;&#35201;&#30340;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#20419;&#36827;&#27809;&#26377;&#24615;&#21035;&#20559;&#35265;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#30340;&#26032;&#22522;&#20934;MAGBIG&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;T2I&#27169;&#22411;&#26159;&#21542;&#36890;&#36807;MAGBIG&#25918;&#22823;&#20102;&#24615;&#21035;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#35821;&#35328;&#25552;&#31034;&#35831;&#27714;&#29305;&#23450;&#32844;&#19994;&#25110;&#29305;&#36136;&#30340;&#20154;&#20687;&#22270;&#20687;(&#20351;&#29992;&#24418;&#23481;&#35789;)&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#34920;&#26126;&#27169;&#22411;&#20559;&#31163;&#20102;&#35268;&#33539;&#30340;&#20551;&#35774;&#65292;...
&lt;/p&gt;
&lt;p&gt;
Text-to-image generation models have recently achieved astonishing results in image quality, flexibility, and text alignment and are consequently employed in a fast-growing number of applications. Through improvements in multilingual abilities, a larger community now has access to this kind of technology. Yet, as we will show, multilingual models suffer similarly from (gender) biases as monolingual models. Furthermore, the natural expectation is that these models will provide similar results across languages, but this is not the case and there are important differences between languages. Thus, we propose a novel benchmark MAGBIG intending to foster research in multilingual models without gender bias. We investigate whether multilingual T2I models magnify gender bias with MAGBIG. To this end, we use multilingual prompts requesting portrait images of persons of a certain occupation or trait (using adjectives). Our results show not only that models deviate from the normative assumption th
&lt;/p&gt;</description></item><item><title>ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.10225</link><description>&lt;p&gt;
ChatQA: &#26500;&#24314;GPT-4&#32423;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10225
&lt;/p&gt;
&lt;p&gt;
ChatQA&#26159;&#19968;&#31995;&#21015;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#65292;&#21487;&#20197;&#36798;&#21040;GPT-4&#32423;&#21035;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20351;&#29992;&#23494;&#38598;&#26816;&#32034;&#22120;&#36827;&#34892;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;ChatQA-70B&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#24179;&#22343;&#24471;&#20998;&#36229;&#36807;&#20102;GPT-4&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#26469;&#33258;OpenAI GPT&#27169;&#22411;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChatQA&#65292;&#19968;&#31995;&#21015;&#20855;&#26377;GPT-4&#32423;&#21035;&#20934;&#30830;&#24615;&#30340;&#23545;&#35805;&#38382;&#31572;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#25351;&#20196;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#38646;-shot&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#22788;&#29702;&#23545;&#35805;&#38382;&#31572;&#20013;&#30340;&#26816;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#22810;&#36718;&#38382;&#31572;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23494;&#38598;&#26816;&#32034;&#22120;&#30340;&#24494;&#35843;&#65292;&#36825;&#26679;&#21487;&#20197;&#25552;&#20379;&#19982;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#26597;&#35810;&#37325;&#20889;&#27169;&#22411;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#22823;&#22823;&#38477;&#20302;&#37096;&#32626;&#25104;&#26412;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;ChatQA-70B&#21487;&#20197;&#22312;10&#20010;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#24179;&#22343;&#20998;&#19978;&#36229;&#36807;GPT-4&#65288;54.14 vs. 53.90&#65289;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;OpenAI GPT&#27169;&#22411;&#30340;&#20219;&#20309;&#21512;&#25104;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce ChatQA, a family of conversational question answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we propose a two-stage instruction tuning method that can significantly improve the zero-shot conversational QA results from large language models (LLMs). To handle retrieval in conversational QA, we fine-tune a dense retriever on a multi-turn QA dataset, which provides comparable results to using the state-of-the-art query rewriting model while largely reducing deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10 conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#24230;&#21306;&#20998;&#24615;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2311.12373</link><description>&lt;p&gt;
&#36229;&#36234;&#22270;&#28789;: &#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond Turing: A Comparative Analysis of Approaches for Detecting Machine-Generated Text. (arXiv:2311.12373v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.12373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#27604;&#20102;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#39640;&#24230;&#21306;&#20998;&#24615;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#21306;&#20998;&#20154;&#31867;&#29983;&#25104;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#35770;&#25991;&#23545;&#19977;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#35780;&#20272;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65306;&#20256;&#32479;&#30340;&#27973;&#23618;&#23398;&#20064;&#12289;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24494;&#35843;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#19978;&#32463;&#36807;&#20005;&#26684;&#27979;&#35797;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#35780;&#20272;&#23427;&#20204;&#22312;&#21306;&#20998;&#20154;&#31867;&#21644;&#26426;&#22120;&#26500;&#36896;&#35821;&#35328;&#26041;&#38754;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#32467;&#26524;&#26174;&#31034;&#19981;&#21516;&#26041;&#27861;&#30340;&#24615;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#22240;&#27492;&#24378;&#35843;&#20102;&#22312;&#36825;&#19968;&#20851;&#38190;NLP&#39046;&#22495;&#30340;&#25345;&#32493;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#20026;&#26410;&#26469;&#26088;&#22312;&#21019;&#24314;&#24378;&#22823;&#19988;&#39640;&#24230;&#21306;&#20998;&#24615;&#27169;&#22411;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Significant progress has been made on text generation by pre-trained language models (PLMs), yet distinguishing between human and machine-generated text poses an escalating challenge. This paper offers an in-depth evaluation of three distinct methods used to address this task: traditional shallow learning, Language Model (LM) fine-tuning, and Multilingual Model fine-tuning. These approaches are rigorously tested on a wide range of machine-generated texts, providing a benchmark of their competence in distinguishing between human-authored and machine-authored linguistic constructs. The results reveal considerable differences in performance across methods, thus emphasizing the continued need for advancement in this crucial area of NLP. This study offers valuable insights and paves the way for future research aimed at creating robust and highly discriminative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#20799;&#31461;&#35821;&#38899;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23558;Whisper&#31995;&#32479;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26174;&#31034;&#20102;&#34920;&#29616;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2309.07927</link><description>&lt;p&gt;
Kid-Whisper: &#21161;&#21147;&#22635;&#34917;&#20799;&#31461;&#19982;&#25104;&#20154;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#24046;&#36317;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults. (arXiv:2309.07927v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#21644;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#26469;&#25913;&#36827;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#23545;&#20799;&#31461;&#35821;&#38899;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;&#23558;Whisper&#31995;&#32479;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#65292;&#26174;&#31034;&#20102;&#34920;&#29616;&#21487;&#34892;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;Whisper&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#31995;&#32479;&#22312;&#36275;&#22815;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36827;&#23637;&#24182;&#19981;&#36866;&#29992;&#20110;&#20799;&#31461;ASR&#65292;&#21407;&#22240;&#26159;&#36866;&#29992;&#20110;&#20799;&#31461;&#30340;&#19987;&#29992;&#25968;&#25454;&#24211;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#19988;&#20799;&#31461;&#35821;&#38899;&#20855;&#26377;&#19982;&#25104;&#20154;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21033;&#29992;My Science Tutor&#65288;MyST&#65289;&#20799;&#31461;&#35821;&#38899;&#35821;&#26009;&#24211;&#25552;&#39640;Whisper&#35782;&#21035;&#20799;&#31461;&#35821;&#38899;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#22312;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#26356;&#26377;&#25928;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#22686;&#24378;&#20102;MyST&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25913;&#36827;&#20799;&#31461;ASR&#24615;&#33021;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#32467;&#26524;&#23637;&#31034;&#20102;&#23558;Whisper&#26377;&#25928;&#25972;&#21512;&#21040;&#20799;&#31461;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#21487;&#34892;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn't readily extend to ASR for children due to the limited availability of suitable child-specific databases and the distinct characteristics of children's speech. A recent study investigated leveraging the My Science Tutor (MyST) children's speech corpus to enhance Whisper's performance in recognizing children's speech. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We also highlight important challenges towards improving children's ASR performance. The results showcase the viable and efficient integration of Whisper for effective children's speech recognition.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23391;&#21152;&#25289;&#35821;&#20013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#24635;&#32467;&#21644;&#25193;&#20805;&#25216;&#26415;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#37325;&#26041;&#27861;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24635;&#32467;&#21644;&#25193;&#20805;&#22312;&#23391;&#21152;&#25289;&#35821;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06979</link><description>&lt;p&gt;
&#35299;&#20915;&#23391;&#21152;&#25289;&#35821;&#20013;&#30340;&#20551;&#26032;&#38395;&#38382;&#39064;&#65306;&#25581;&#31034;&#24635;&#32467;&#19982;&#25193;&#20805;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models. (arXiv:2307.06979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23391;&#21152;&#25289;&#35821;&#20013;&#20551;&#26032;&#38395;&#30340;&#26816;&#27979;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#24635;&#32467;&#21644;&#25193;&#20805;&#25216;&#26415;&#65292;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22235;&#37325;&#26041;&#27861;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24635;&#32467;&#21644;&#25193;&#20805;&#22312;&#23391;&#21152;&#25289;&#35821;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#21644;&#22312;&#32447;&#26032;&#38395;&#26469;&#28304;&#30340;&#20852;&#36215;&#65292;&#20551;&#26032;&#38395;&#24050;&#25104;&#20026;&#20840;&#29699;&#24615;&#30340;&#37325;&#22823;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#20687;&#23391;&#21152;&#25289;&#35821;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#26816;&#27979;&#20551;&#26032;&#38395;&#22312;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#24635;&#32467;&#21644;&#25193;&#20805;&#25216;&#26415;&#20197;&#21450;&#20116;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#20998;&#31867;&#23391;&#21152;&#25289;&#35821;&#30340;&#20551;&#26032;&#38395;&#25991;&#31456;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#23558;&#33521;&#35821;&#26032;&#38395;&#25991;&#31456;&#36827;&#34892;&#32763;&#35793;&#65292;&#24182;&#20351;&#29992;&#25193;&#20805;&#25216;&#26415;&#26469;&#35299;&#20915;&#20551;&#26032;&#38395;&#25991;&#31456;&#30340;&#19981;&#36275;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#30528;&#37325;&#20110;&#36890;&#36807;&#24635;&#32467;&#26032;&#38395;&#26469;&#35299;&#20915;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#20196;&#29260;&#38271;&#24230;&#38480;&#21046;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24635;&#32467;&#21644;&#25193;&#20805;&#22312;&#23391;&#21152;&#25289;&#35821;&#20551;&#26032;&#38395;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#29420;&#31435;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#24403;&#23558;BanglaBERT&#22522;&#30784;&#27169;&#22411;&#19982;&#25193;&#20805;&#25216;&#26415;&#30456;&#32467;&#21512;&#26102;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of social media and online news sources, fake news has become a significant issue globally. However, the detection of fake news in low resource languages like Bengali has received limited attention in research. In this paper, we propose a methodology consisting of four distinct approaches to classify fake news articles in Bengali using summarization and augmentation techniques with five pre-trained language models. Our approach includes translating English news articles and using augmentation techniques to curb the deficit of fake news articles. Our research also focused on summarizing the news to tackle the token length limitation of BERT based models. Through extensive experimentation and rigorous evaluation, we show the effectiveness of summarization and augmentation in the case of Bengali fake news detection. We evaluated our models using three separate test datasets. The BanglaBERT Base model, when combined with augmentation techniques, achieved an impressive accurac
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.01879</link><description>&lt;p&gt;
VisualGPTScore: &#22810;&#27169;&#24577;&#29983;&#25104;&#39044;&#35757;&#32451;&#20998;&#25968;&#30340;&#35270;&#35273;&#35821;&#20041;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores. (arXiv:2306.01879v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01879
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;VisualGPTScore&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#35745;&#31639;&#65292;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; VisualGPTScore &#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#29983;&#25104;&#20998;&#25968;&#26469;&#25429;&#25417;&#25991;&#26412;&#26631;&#39064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22270;&#20687;&#26465;&#20214;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#20687;&#19978;&#36816;&#31639;&#12290;&#19982;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#30340;VLM&#21482;&#26159;&#26080;&#24847;&#20041;&#30340;&#21333;&#35789;&#34955;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340; VisualGPTScore &#22312; ARO &#21644; Crepe &#31561;&#26368;&#36817;&#25552;&#20986;&#30340;&#22270;&#20687;&#25991;&#26412;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20102;&#39030;&#23574;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#20854;&#20855;&#22791;&#32452;&#21512;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) discriminatively pre-trained with contrastive image-text matching losses such as $P(\text{match}|\text{text}, \text{image})$ have been criticized for lacking compositional understanding. This means they might output similar scores even if the original caption is rearranged into a different semantic statement. To address this, we propose to use the ${\bf V}$isual ${\bf G}$enerative ${\bf P}$re-${\bf T}$raining Score (${\bf VisualGPTScore}$) of $P(\text{text}|\text{image})$, a $\textit{multimodal generative}$ score that captures the likelihood of a text caption conditioned on an image using an image-conditioned language model. Contrary to the belief that VLMs are mere bag-of-words models, our off-the-shelf VisualGPTScore demonstrates top-tier performance on recently proposed image-text retrieval benchmarks like ARO and Crepe that assess compositional reasoning. Furthermore, we factorize VisualGPTScore into a product of the $\textit{marginal}$ P(text) and the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09651</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#25351;&#23548;&#26377;&#21161;&#20110;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Tailoring Instructions to Student's Learning Levels Boosts Knowledge Distillation. (arXiv:2305.09651v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20010;&#24615;&#21270;&#25351;&#23548;&#30340;&#23398;&#20064;&#25216;&#26415;&#65292;&#31216;&#20026;LGTM&#65292;&#20854;&#21033;&#29992;&#33976;&#39311;&#25928;&#24212;&#36873;&#25321;&#26679;&#26412;&#20197;&#22686;&#24378;&#23398;&#29983;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30740;&#31350;&#34920;&#26126;&#65292;&#33021;&#21147;&#36229;&#32676;&#30340;&#25945;&#24072;&#27169;&#22411;&#24182;&#19981;&#19968;&#23450;&#33021;&#22815;&#35753;&#23398;&#29983;&#27700;&#24179;&#24471;&#21040;&#25552;&#21319;&#65292;&#36825;&#20984;&#26174;&#20102;&#24403;&#21069;&#25945;&#24072;&#22521;&#35757;&#23454;&#36341;&#21644;&#26377;&#25928;&#30693;&#35782;&#20256;&#25480;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#25945;&#24072;&#22521;&#35757;&#36807;&#31243;&#30340;&#25351;&#23548;&#25928;&#26524;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#33976;&#39311;&#25928;&#24212;&#30340;&#27010;&#24565;&#65292;&#20197;&#30830;&#23450;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#23545;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#22909;&#25945;&#24072;&#24456;&#37325;&#35201;&#65288;LGTM&#65289;&#30340;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#65292;&#20197;&#23558;&#33976;&#39311;&#25928;&#24212;&#32435;&#20837;&#25945;&#24072;&#30340;&#23398;&#20064;&#36807;&#31243;&#20013;&#12290;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#21487;&#33021;&#25552;&#21319;&#23398;&#29983;&#27867;&#21270;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#30340;LGTM&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;6&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20248;&#20110;10&#20010;&#24120;&#35265;&#30340;&#30693;&#35782;&#33976;&#39311;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been commonly observed that a teacher model with superior performance does not necessarily result in a stronger student, highlighting a discrepancy between current teacher training practices and effective knowledge transfer. In order to enhance the guidance of the teacher training process, we introduce the concept of distillation influence to determine the impact of distillation from each training sample on the student's generalization ability. In this paper, we propose Learning Good Teacher Matters (LGTM), an efficient training technique for incorporating distillation influence into the teacher's learning process. By prioritizing samples that are likely to enhance the student's generalization ability, our LGTM outperforms 10 common knowledge distillation baselines on 6 text classification tasks in the GLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2304.03427</link><description>&lt;p&gt;
&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cleansing Jewel: A Neural Spelling Correction Model Built On Google OCR-ed Tibetan Manuscripts. (arXiv:2304.03427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#25991;&#23398;&#32773;&#22312;&#30740;&#31350;&#21382;&#21490;&#12289;&#23447;&#25945;&#21644;&#31038;&#20250;&#25919;&#27835;&#32467;&#26500;&#31561;&#26041;&#38754;&#32463;&#24120;&#20381;&#36182;&#20110;&#21476;&#20195;&#25163;&#31295;&#12290;&#34429;&#28982;OCR&#25216;&#26415;&#21487;&#20197;&#23558;&#36825;&#20123;&#23453;&#36149;&#25163;&#31295;&#25968;&#23383;&#21270;&#65292;&#20294;&#22810;&#25968;&#25163;&#31295;&#22240;&#30952;&#25439;&#32780;&#36807;&#26102;&#65292;OCR&#31243;&#24207;&#27809;&#21150;&#27861;&#35782;&#21035;&#32763;&#39029;&#30340;&#34394;&#28129;&#25110;&#27745;&#28173;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35895;&#27468;OCR&#25195;&#25551;&#30340;&#34255;&#25991;&#25163;&#31295;&#30340;&#31070;&#32463;&#25340;&#20889;&#32416;&#38169;&#27169;&#22411;&#65292;&#21487;&#20197;&#33258;&#21160;&#32416;&#27491;OCR&#36755;&#20986;&#20013;&#30340;&#22122;&#22768;&#12290;&#26412;&#25991;&#20998;&#20026;&#22235;&#20010;&#37096;&#20998;&#65306;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35757;&#32451;&#21644;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21407;&#22987;&#34255;&#25991;&#30005;&#23376;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#20004;&#32452;&#32467;&#26500;&#21270;&#25968;&#25454;&#26694;&#8212;&#8212;&#19968;&#32452;&#21305;&#37197;&#30340;&#29609;&#20855;&#25968;&#25454;&#21644;&#19968;&#32452;&#21305;&#37197;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;Transformer&#26550;&#26500;&#20013;&#23454;&#29616;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#26469;&#25191;&#34892;&#25340;&#20889;&#26657;&#27491;&#20219;&#21153;&#12290;&#26681;&#25454;&#25439;&#22833;&#21644;&#23383;&#31526;&#38169;&#35823;&#29575;&#65292;&#25105;&#20204;&#30340;Transformer + &#32622;&#20449;&#24230;&#24471;&#20998;&#26426;&#21046;&#27604;&#20854;&#20182;&#24120;&#29992;&#30340;&#25340;&#20889;&#26657;&#27491;&#31639;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholars in the humanities rely heavily on ancient manuscripts to study history, religion, and socio-political structures in the past. Many efforts have been devoted to digitizing these precious manuscripts using OCR technology, but most manuscripts were blemished over the centuries so that an Optical Character Recognition (OCR) program cannot be expected to capture faded graphs and stains on pages. This work presents a neural spelling correction model built on Google OCR-ed Tibetan Manuscripts to auto-correct OCR-ed noisy output. This paper is divided into four sections: dataset, model architecture, training and analysis. First, we feature-engineered our raw Tibetan etext corpus into two sets of structured data frames -- a set of paired toy data and a set of paired real data. Then, we implemented a Confidence Score mechanism into the Transformer architecture to perform spelling correction tasks. According to the Loss and Character Error Rate, our Transformer + Confidence score mechani
&lt;/p&gt;</description></item></channel></rss>