<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>DeepFilterNet&#26159;&#19968;&#20010;&#22522;&#20110;&#24863;&#30693;&#30340;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#38899;&#29983;&#20135;&#21644;&#24515;&#29702;&#22768;&#23398;&#24863;&#30693;&#39046;&#22495;&#30340;&#30693;&#35782;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#22312;&#21333;&#32447;&#31243;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#23454;&#29616;&#20102;0.19&#30340;&#23454;&#26102;&#22240;&#23376;&#65292;&#22312;&#21305;&#37197;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#22686;&#24378;&#22522;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.08227</link><description>&lt;p&gt;
DeepFilterNet: &#22522;&#20110;&#24863;&#30693;&#30340;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DeepFilterNet: Perceptually Motivated Real-Time Speech Enhancement. (arXiv:2305.08227v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08227
&lt;/p&gt;
&lt;p&gt;
DeepFilterNet&#26159;&#19968;&#20010;&#22522;&#20110;&#24863;&#30693;&#30340;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;&#26694;&#26550;&#65292;&#21033;&#29992;&#35821;&#38899;&#29983;&#20135;&#21644;&#24515;&#29702;&#22768;&#23398;&#24863;&#30693;&#39046;&#22495;&#30340;&#30693;&#35782;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#22312;&#21333;&#32447;&#31243;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#23454;&#29616;&#20102;0.19&#30340;&#23454;&#26102;&#22240;&#23376;&#65292;&#22312;&#21305;&#37197;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#22686;&#24378;&#22522;&#20934;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#30340;&#22810;&#24103;&#31639;&#27861;&#33021;&#22815;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#30701;&#26102;&#30456;&#20851;&#24615;&#12290;Deep Filtering(DF) &#25552;&#20986;&#20102;&#30452;&#25509;&#22312;&#39057;&#22495;&#20272;&#35745;&#22797;&#26434;&#28388;&#27874;&#22120;&#20197;&#21033;&#29992;&#36825;&#20123;&#30456;&#20851;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;DeepFilterNet&#36827;&#34892;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;&#30340;&#28436;&#31034;&#12290;DeepFilterNet&#30340;&#25928;&#29575;&#26159;&#36890;&#36807;&#21033;&#29992;&#35821;&#38899;&#29983;&#20135;&#21644;&#24515;&#29702;&#22768;&#23398;&#24863;&#30693;&#39046;&#22495;&#30340;&#30693;&#35782;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;&#26368;&#20808;&#36827;&#30340;&#35821;&#38899;&#22686;&#24378;&#22522;&#20934;&#65292;&#21516;&#26102;&#22312;&#21333;&#32447;&#31243;&#31508;&#35760;&#26412;&#30005;&#33041;&#19978;&#23454;&#29616;&#20102;0.19&#30340;&#23454;&#26102;&#22240;&#23376;&#12290;&#35813;&#26694;&#26550;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;&#26435;&#37325;&#24050;&#32463;&#21457;&#34920;&#20102;&#24320;&#28304;&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-frame algorithms for single-channel speech enhancement are able to take advantage from short-time correlations within the speech signal. Deep Filtering (DF) was proposed to directly estimate a complex filter in frequency domain to take advantage of these correlations. In this work, we present a real-time speech enhancement demo using DeepFilterNet. DeepFilterNet's efficiency is enabled by exploiting domain knowledge of speech production and psychoacoustic perception. Our model is able to match state-of-the-art speech enhancement benchmarks while achieving a real-time-factor of 0.19 on a single threaded notebook CPU. The framework as well as pretrained weights have been published under an open source license.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08208</link><description>&lt;p&gt;
&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Generalize for Cross-domain QA. (arXiv:2305.08208v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08208
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#36328;&#22495;&#38382;&#31572;&#27867;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;4.5%-7.9%&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36328;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#19968;&#30452;&#23384;&#22312;&#30528;&#36234;&#26469;&#36234;&#22823;&#30340;&#25285;&#24551;&#12290;&#24403;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21463;&#21040;&#20102;&#22686;&#21152;&#35757;&#32451;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#25552;&#31034;&#26041;&#27861;&#21644;&#32447;&#24615;&#25506;&#27979;&#20877;&#24494;&#35843;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#21644;&#23454;&#35777;&#19978;&#37117;&#34987;&#35777;&#26126;&#26377;&#25928;&#65292;&#21487;&#20197;&#22686;&#24378;&#20135;&#29983;&#24335;&#21644;&#21028;&#21035;&#24335;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1&#24471;&#20998;&#24179;&#22343;&#25552;&#39640;&#20102;4.5%-7.9%&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#21040;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#24182;&#20026;&#26410;&#20805;&#20998;&#24320;&#21457;&#30340;&#36328;&#22495;&#38382;&#31572;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22312;GitHub&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;*&#12290;
&lt;/p&gt;
&lt;p&gt;
There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#30693;&#35782;&#34701;&#21512;&#30340;&#35748;&#30693;&#21050;&#28608;&#23545;&#35805;&#31995;&#32479;&#65292;&#38754;&#21521;&#35748;&#30693;&#25439;&#20260;&#30340;&#32769;&#24180;&#20154;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;CSConv&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#24471;&#31995;&#32479;&#21487;&#20197;&#22312;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#30340;&#21516;&#26102;&#36827;&#34892;&#23545;&#35805;&#65292;&#24182;&#37319;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#39044;&#27979;&#30446;&#26631;&#21709;&#24212;&#30340;CS&#21407;&#21017;&#21644;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#20197;&#29983;&#25104;&#24320;&#25918;&#24335;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2305.08200</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#28304;&#30693;&#35782;&#34701;&#21512;&#30340;&#35748;&#30693;&#21050;&#28608;&#23545;&#35805;&#31995;&#32479;&#65292;&#38754;&#21521;&#35748;&#30693;&#25439;&#20260;&#30340;&#32769;&#24180;&#20154;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Stimulation Dialogue System with Multi-source Knowledge Fusion for Elders with Cognitive Impairment. (arXiv:2305.08200v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#30693;&#35782;&#34701;&#21512;&#30340;&#35748;&#30693;&#21050;&#28608;&#23545;&#35805;&#31995;&#32479;&#65292;&#38754;&#21521;&#35748;&#30693;&#25439;&#20260;&#30340;&#32769;&#24180;&#20154;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;CSConv&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20351;&#24471;&#31995;&#32479;&#21487;&#20197;&#22312;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#30340;&#21516;&#26102;&#36827;&#34892;&#23545;&#35805;&#65292;&#24182;&#37319;&#29992;&#22806;&#37096;&#30693;&#35782;&#26469;&#39044;&#27979;&#30446;&#26631;&#21709;&#24212;&#30340;CS&#21407;&#21017;&#21644;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#20197;&#29983;&#25104;&#24320;&#25918;&#24335;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35748;&#30693;&#38556;&#30861;&#30340;&#32769;&#24180;&#20154;&#20132;&#27969;&#26102;&#65292;&#35748;&#30693;&#21050;&#28608;&#65288;CS&#65289;&#26377;&#21161;&#20110;&#32500;&#25252;&#32769;&#24180;&#20154;&#30340;&#35748;&#30693;&#20581;&#24247;&#12290;&#25968;&#25454;&#31232;&#32570;&#26159;&#26500;&#24314;&#22522;&#20110;CS&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#20027;&#35201;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#20013;&#25991;&#26041;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20221;&#20013;&#22269;CS&#23545;&#35805;&#65288;CSConv&#65289;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#32422;2.6K&#32452;&#20855;&#26377;CS&#21407;&#21017;&#21644;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#26631;&#31614;&#30340;&#23545;&#35805;&#12290;&#22312;&#25552;&#20379;&#24773;&#24863;&#25903;&#25345;&#30340;&#21516;&#26102;&#32842;&#22825;&#26159;&#22823;&#22810;&#25968;&#29616;&#26377;&#35748;&#30693;&#23545;&#35805;&#31995;&#32479;&#25152;&#24573;&#35270;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#28304;&#30693;&#35782;&#34701;&#21512;&#30340;&#35748;&#30693;&#21050;&#28608;&#23545;&#35805;&#65288;CSD&#65289;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#22312;CS&#21407;&#21017;&#21644;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#25351;&#23548;&#19979;&#30340;&#24320;&#25918;&#24335;&#22238;&#22797;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#30340;&#28176;&#36827;&#24335;&#36974;&#30422;&#26041;&#27861;&#26469;&#23398;&#20064;&#32534;&#30721;&#22120;&#20316;&#20026;&#26377;&#25928;&#30340;&#20998;&#31867;&#22120;&#65292;&#36825;&#26159;&#39044;&#27979;&#30446;&#26631;&#21709;&#24212;&#30340;CS&#21407;&#21017;&#21644;&#24773;&#24863;&#25903;&#25345;&#31574;&#30053;&#30340;&#21069;&#25552;&#12290;&#28982;&#21518;&#65292;&#35299;&#30721;&#22120;&#19982;&#24863;&#30693;&#30340;CS&#21407;&#21017;&#30456;&#20114;&#20316;&#29992;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
When communicating with elders with cognitive impairment, cognitive stimulation (CS) help to maintain the cognitive health of elders. Data sparsity is the main challenge in building CS-based dialogue systems, particularly in the Chinese language. To fill this gap, we construct a Chinese CS conversation (CSConv) dataset, which contains about 2.6K groups of dialogues with CS principles and emotional support strategy labels. Making chit chat while providing emotional support is overlooked by the majority of existing cognitive dialogue systems. In this paper, we propose a multi-source knowledge fusion method for CS dialogue (CSD), to generate open-ended responses guided by the CS principle and emotional support strategy. We first use a progressive mask method based on external knowledge to learn encoders as effective classifiers, which is the prerequisite to predict the CS principle and emotional support strategy of the target response. Then a decoder interacts with the perceived CS princi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#21462;&#24471;&#19982;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30456;&#24403;&#30340;&#38169;&#35823;&#32416;&#27491;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.08195</link><description>&lt;p&gt;
&#23398;&#20064;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20197;&#36827;&#34892;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning to Simulate Natural Language Feedback for Interactive Semantic Parsing. (arXiv:2305.08195v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#36807;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26469;&#36827;&#34892;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#21462;&#24471;&#19982;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30456;&#24403;&#30340;&#38169;&#35823;&#32416;&#27491;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#24050;&#32463;&#25104;&#20026;&#27604;&#20256;&#32479;&#30340;&#19968;&#27425;&#35821;&#20041;&#35299;&#26512;&#26356;&#23454;&#29992;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#29992;&#25143;&#25552;&#20379;&#21453;&#39304;&#26469;&#32416;&#27491;&#35299;&#26512;&#22120;&#30340;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#30340;&#21453;&#39304;&#25968;&#25454;&#26469;&#35757;&#32451;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#22120;&#65292;&#36825;&#31181;&#26041;&#27861;&#20195;&#20215;&#39640;&#26114;&#19988;&#19981;&#21487;&#25193;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#21363;&#27169;&#25311;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20197;&#29992;&#20110;&#20132;&#20114;&#24335;&#35821;&#20041;&#35299;&#26512;&#12290;&#25105;&#20204;&#37197;&#21512;&#35813;&#20219;&#21153;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#39304;&#35780;&#20272;&#22120;&#12290;&#35813;&#35780;&#20272;&#22120;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#35780;&#20272;&#27169;&#25311;&#21453;&#39304;&#30340;&#36136;&#37327;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#21487;&#20197;&#20915;&#23450;&#26368;&#20339;&#30340;&#21453;&#39304;&#27169;&#25311;&#22120;&#12290;&#22312;&#19968;&#20010;&#25991;&#26412;&#21040;SQL&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#21453;&#39304;&#27169;&#25311;&#22120;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20197;&#22686;&#24378;&#29305;&#23450;&#35299;&#26512;&#22120;&#30340;&#38169;&#35823;&#32416;&#27491;&#33021;&#21147;&#12290;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#21453;&#39304;&#27169;&#25311;&#22120;&#21487;&#20197;&#24110;&#21161;&#36798;&#21040;&#19982;&#20351;&#29992;&#20195;&#20215;&#39640;&#26114;&#30340;&#23436;&#25972;&#20154;&#24037;&#27880;&#37322;&#21453;&#39304;&#25968;&#25454;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30456;&#24403;&#30340;&#38169;&#35823;&#32416;&#27491;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactive semantic parsing based on natural language (NL) feedback, where users provide feedback to correct the parser mistakes, has emerged as a more practical scenario than the traditional one-shot semantic parsing. However, prior work has heavily relied on human-annotated feedback data to train the interactive semantic parser, which is prohibitively expensive and not scalable. In this work, we propose a new task of simulating NL feedback for interactive semantic parsing. We accompany the task with a novel feedback evaluator. The evaluator is specifically designed to assess the quality of the simulated feedback, based on which we decide the best feedback simulator from our proposed variants. On a text-to-SQL dataset, we show that our feedback simulator can generate high-quality NL feedback to boost the error correction ability of a specific parser. In low-data settings, our feedback simulator can help achieve comparable error correction performance as trained using the costly, full
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#27880;&#37322;&#20102;&#24773;&#24863;&#26497;&#24615;&#30340;&#20811;&#32599;&#22320;&#20122;&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#24494;&#35843;&#26041;&#27861;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.08173</link><description>&lt;p&gt;
&#20811;&#32599;&#22320;&#20122;&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;&#65288;Cro-FiReDa&#65289;&#65306;&#19968;&#20221;&#27880;&#37322;&#20102;&#24773;&#24863;&#26497;&#24615;&#30340;&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Croatian Film Review Dataset (Cro-FiReDa): A Sentiment Annotated Dataset of Film Reviews. (arXiv:2305.08173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08173
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#27880;&#37322;&#20102;&#24773;&#24863;&#26497;&#24615;&#30340;&#20811;&#32599;&#22320;&#20122;&#30005;&#24433;&#35780;&#35770;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#24494;&#35843;&#26041;&#27861;&#30340;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Cro-FiReDa&#65292;&#19968;&#20010;&#22312;&#30005;&#24433;&#35780;&#35770;&#39046;&#22495;&#20013;&#23545;&#20811;&#32599;&#22320;&#20122;&#35821;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#36229;&#36807;10,000&#20010;&#21477;&#23376;&#65292;&#24182;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#38500;&#20102;&#20171;&#32461;&#25972;&#20010;&#27880;&#37322;&#36807;&#31243;&#22806;&#65292;&#25105;&#20204;&#36824;&#22522;&#20110;&#21464;&#21387;&#22120;&#24494;&#35843;&#26041;&#27861;&#25552;&#20379;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Cro-FiReDa, a sentiment- annotated dataset for Croatian in the domain of movie reviews. The dataset, which contains over 10,000 sentences, has been annotated at the sentence level. In addition to presenting the overall annotation process, we also present benchmark results based on the transformer- based fine-tuning approach
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;STORYWARS&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#21327;&#20316;&#25925;&#20107;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#20316;&#32773;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#26041;&#24335;&#24314;&#31435;&#20102; INSTRUCTSTORY &#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#22312;&#20840;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#22788;&#29702;&#25925;&#20107;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.08152</link><description>&lt;p&gt;
STORYWARS: &#19968;&#20010;&#21327;&#20316;&#25925;&#20107;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#21644;&#25351;&#20196;&#35843;&#25972;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
STORYWARS: A Dataset and Instruction Tuning Baselines for Collaborative Story Understanding and Generation. (arXiv:2305.08152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08152
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;STORYWARS&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#21327;&#20316;&#25925;&#20107;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#12290;&#20316;&#32773;&#36890;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;&#26041;&#24335;&#24314;&#31435;&#20102; INSTRUCTSTORY &#27169;&#22411;&#65292;&#25104;&#21151;&#22320;&#22312;&#20840;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#22788;&#29702;&#25925;&#20107;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#25925;&#20107;&#26159;&#30001;&#22810;&#21517;&#19981;&#21516;&#20889;&#20316;&#39118;&#26684;&#21644;&#24847;&#22270;&#30340;&#20316;&#32773;&#20849;&#21516;&#21019;&#20316;&#30340;&#25991;&#26412;&#65292;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#32570;&#20047;&#24320;&#25918;&#39046;&#22495;&#35821;&#26009;&#24211;&#65292;&#29702;&#35299;&#21644;&#29983;&#25104;&#27492;&#31867;&#25925;&#20107;&#20173;&#28982;&#26159;&#19968;&#20010;&#36739;&#23569;&#28041;&#36275;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102; STORYWARS&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22312;&#32447;&#24179;&#21488;&#19978;&#30340;9,400&#22810;&#20301;&#19981;&#21516;&#20316;&#32773;&#25776;&#20889;&#30340;&#36229;&#36807;40,000&#20010;&#21327;&#20316;&#25925;&#20107;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312; STORYWARS &#19978;&#35774;&#35745;&#20102;12&#31181;&#20219;&#21153;&#31867;&#22411;&#65292;&#21253;&#25324;7&#31181;&#29702;&#35299;&#21644;5&#31181;&#29983;&#25104;&#20219;&#21153;&#31867;&#22411;&#65292;&#24635;&#20849;&#25512;&#23548;&#20986;101&#31181;&#22810;&#26679;&#30340;&#25925;&#20107;&#30456;&#20851;&#20219;&#21153;&#20316;&#20026;&#35206;&#30422;&#25152;&#26377;&#20840;&#30417;&#30563;&#12289;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411; INSTRUCTSTORY&#65292;&#29992;&#20110;&#23637;&#31034;&#22788;&#29702;&#25925;&#20107;&#20219;&#21153;&#65292;&#24403;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#21462;&#24471;&#21331;&#36234;&#32467;&#26524;&#26102;&#65292;&#25351;&#20196;&#35843;&#25972;&#19981;&#20165;&#33021;&#22815;&#22312;&#20840;&#30417;&#30563;&#20219;&#21153;&#20013;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#24314;&#31435;&#24378;&#22823;&#30340;&#22810;&#20219;&#21153;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative stories, which are texts created through the collaborative efforts of multiple authors with different writing styles and intentions, pose unique challenges for NLP models. Understanding and generating such stories remains an underexplored area due to the lack of open-domain corpora. To address this, we introduce STORYWARS, a new dataset of over 40,000 collaborative stories written by 9,400 different authors from an online platform. We design 12 task types, comprising 7 understanding and 5 generation task types, on STORYWARS, deriving 101 diverse story-related tasks in total as a multi-task benchmark covering all fully-supervised, few-shot, and zero-shot scenarios. Furthermore, we present our instruction-tuned model, INSTRUCTSTORY, for the story tasks showing that instruction tuning, in addition to achieving superior results in zero-shot and few-shot scenarios, can also obtain the best performance on the fully-supervised tasks in STORYWARS, establishing strong multi-task b
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#37322;&#20041;&#29983;&#25104;&#22120;&#29983;&#25104;&#26367;&#20195;&#35789;&#20505;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#35299;&#30721;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08146</link><description>&lt;p&gt;
ParaLS&#65306;&#22522;&#20110;&#39044;&#35757;&#32451;&#37322;&#20041;&#29983;&#25104;&#22120;&#30340;&#35789;&#27719;&#26367;&#25442;
&lt;/p&gt;
&lt;p&gt;
ParaLS: Lexical Substitution via Pretrained Paraphraser. (arXiv:2305.08146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35789;&#27719;&#26367;&#25442;&#26041;&#27861;&#65292;&#20351;&#29992;&#37322;&#20041;&#29983;&#25104;&#22120;&#29983;&#25104;&#26367;&#20195;&#35789;&#20505;&#36873;&#39033;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#35299;&#30721;&#31574;&#30053;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#26367;&#25442; (LS) &#26088;&#22312;&#25214;&#21040;&#21477;&#23376;&#20013;&#30446;&#26631;&#35789;&#30340;&#36866;&#24403;&#26367;&#20195;&#35789;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340; LS &#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36890;&#36807;&#20998;&#26512;&#30446;&#26631;&#35789;&#21608;&#22260;&#30340;&#35821;&#22659;&#29983;&#25104;&#28508;&#22312;&#30340;&#26367;&#20195;&#35789;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#26367;&#20195;&#35789;&#26102;&#24448;&#24448;&#24573;&#35270;&#20102;&#21477;&#23376;&#21547;&#20041;&#30340;&#20445;&#30041;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#37322;&#20041;&#29983;&#25104;&#22120;&#20013;&#29983;&#25104;&#26367;&#20195;&#35789;&#20505;&#36873;&#39033;&#65292;&#22240;&#20026;&#37322;&#20041;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#37322;&#20041;&#21253;&#21547;&#20102;&#35789;&#27719;&#36873;&#25321;&#30340;&#21464;&#21270;&#24182;&#20445;&#30041;&#20102;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#30001;&#20110;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#36890;&#36807;&#24120;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#29983;&#25104;&#26367;&#20195;&#35789;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#20004;&#31181;&#31616;&#21333;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#35299;&#30721;&#36807;&#31243;&#20013;&#30446;&#26631;&#35789;&#30340;&#21464;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340; LS &#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence's meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the generated paraphrases from a paraphraser contain variations in word choice and preserve the sentence's meaning. Since we cannot directly generate the substitutes via commonly used decoding strategies, we propose two simple decoding strategies that focus on the variations of the target word during decoding. Experimental results show that our methods outperform state-of-the-art LS methods based on pre-trained language models on three benchmarks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.08135</link><description>&lt;p&gt;
&#21306;&#20998;&#20808;&#20110;&#22238;&#31572;&#65306;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#20316;&#20026;&#24120;&#35782;&#38382;&#31572;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#33719;&#21462;&#19981;&#21516;&#30340;&#30693;&#35782;&#65292;&#22312;&#26576;&#20123;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#26816;&#32034;&#30693;&#35782;&#30340;&#29305;&#24615;&#38480;&#21046;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#21516;&#26102;&#20174;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#21306;&#20998;&#24615;&#26041;&#38754;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPACE&#65292;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#32473;&#23450;&#20505;&#36873;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;</title><link>http://arxiv.org/abs/2305.08096</link><description>&lt;p&gt;
&#25506;&#31350;&#21644;&#25913;&#36827;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation. (arXiv:2305.08096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30693;&#35782;&#33976;&#39311;&#30340;&#26412;&#36136;&#65292;&#21363;&#26469;&#33258;&#20110;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#39044;&#27979;&#12290;&#21516;&#26102;&#65292;&#25351;&#20986;&#20102;&#24403;&#21069;&#22522;&#20110;&#35789;&#32423;&#21035;&#30340;&#30693;&#35782;&#33976;&#39311;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#8212;&#8212;Top-1 Information&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#39046;&#22495;&#20013;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22312;&#21738;&#37324;&#38544;&#34255;&#30340;&#38382;&#39064;&#20173;&#19981;&#28165;&#26970;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#30693;&#35782;&#33976;&#39311;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#23454;&#35777;&#35282;&#24230;&#25581;&#24320;&#20102;&#36825;&#20010;&#35868;&#22242;&#65292;&#24182;&#23637;&#31034;&#20102;&#30693;&#35782;&#26469;&#33258;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#65292;&#36825;&#20063;&#24110;&#21161;&#25105;&#20204;&#24314;&#31435;&#20102;&#35789;&#32423;&#21644;&#24207;&#21015;&#32423;&#33976;&#39311;&#20043;&#38388;&#30340;&#28508;&#22312;&#36830;&#25509;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#25351;&#20986;&#20102;&#22522;&#30784;&#35789;&#32423;&#33976;&#39311;&#20013;&#23384;&#22312;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#30693;&#35782;&#30340;&#24403;&#21069;&#30446;&#26631;&#26159;&#23558;&#27880;&#24847;&#21147;&#25193;&#25955;&#21040;&#25972;&#20010;&#20998;&#24067;&#19978;&#23398;&#20064;&#30693;&#35782;&#65292;&#20294;&#32570;&#20047;&#23545;&#26368;&#20851;&#38190;&#30340;top-1&#20449;&#24687;&#30340;&#29305;&#27530;&#22788;&#29702;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#22823;&#22810;&#25968;&#25945;&#24072;&#30340;top-1&#39044;&#27979;&#19982;&#22320;&#38754;&#23454;&#20917;&#26631;&#35760;&#37325;&#21472;&#65292;&#22240;&#27492;&#30693;&#35782;&#34987;&#40644;&#37329;&#20449;&#24687;&#25152;&#21344;&#25454;&#65292;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#28508;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\textbf{T}op-1 \textbf{I}nformation&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \textbf{T}op-1 \textbf{I}nformation \te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.08088</link><description>&lt;p&gt;
&#20351;&#22522;&#20110;&#25552;&#31034;&#30340;&#40657;&#30418;&#35843;&#20248;&#26356;&#21152;&#20016;&#23500;&#22810;&#24425;&#65306;&#20174;&#19977;&#20010;&#27491;&#20132;&#35282;&#24230;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives. (arXiv:2305.08088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BBT-RGB&#65292;&#19968;&#22871;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#65292;&#21253;&#25324;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#20197;&#21450;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24050;&#32463;&#23637;&#29616;&#20986;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35843;&#25972;&#36825;&#20123;&#27169;&#22411;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#24040;&#39069;&#30340;&#20195;&#20215;&#25110;&#30001;&#20110;&#21830;&#19994;&#32771;&#34385;&#32780;&#19981;&#21487;&#29992;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#40657;&#30418;&#35843;&#20248;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20248;&#21270;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#32780;&#19981;&#35775;&#38382;&#26799;&#24230;&#21644;&#38544;&#34255;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20316;&#21697;&#36824;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#19979;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;BBT-RGB&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22686;&#24378;&#40657;&#30418;&#20248;&#21270;&#25928;&#29575;&#21644;&#24615;&#33021;&#30340;&#30452;&#25509;&#19988;&#20114;&#34917;&#25216;&#26415;&#22871;&#20214;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#21363;&#25554;&#21363;&#29992;&#30340;&#32452;&#20214;&#65306;&#65288;1&#65289;&#20004;&#38454;&#27573;&#26080;&#23548;&#25968;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#21161;&#20110;&#24555;&#36895;&#25910;&#25947;&#24182;&#32531;&#35299;&#36807;&#25311;&#21512;&#65307;&#65288;2&#65289;&#33258;&#21160;&#35821;&#35328;&#36716;&#21270;&#22120;&#26500;&#24314;&#21450;&#20854;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26032;&#29992;&#27861;&#65307;&#65288;3&#65289;&#26356;&#22909;&#30340;&#25552;&#31034;&#21021;&#22987;&#21270;&#65292;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#35821;&#35328;&#23398;&#21160;&#26426;&#21477;&#27861;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initializ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38901;&#24459;&#20851;&#27880;&#21644;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#26469;&#21033;&#29992;&#38901;&#24459;&#29305;&#24449;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#22312;SLURP&#21644;STOP&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;8&#65285;&#21644;2&#65285;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.08067</link><description>&lt;p&gt;
&#21033;&#29992;&#38901;&#24459;&#20851;&#27880;&#21644;&#33976;&#39311;&#26469;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Improving End-to-End SLU performance with Prosodic Attention and Distillation. (arXiv:2305.08067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38901;&#24459;&#20851;&#27880;&#21644;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#26469;&#21033;&#29992;&#38901;&#24459;&#29305;&#24449;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#32447;&#26041;&#27861;&#22312;SLURP&#21644;STOP&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;8&#65285;&#21644;2&#65285;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#31471;&#21040;&#31471;&#35821;&#20041;&#29702;&#35299;&#65288;SLU&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#25110;&#35821;&#35328;&#27169;&#22411;&#21151;&#33021;&#26469;&#36827;&#34892;&#24847;&#22270;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#20013;&#30340;&#20854;&#20182;&#37325;&#35201;&#20449;&#24687;&#65292;&#22914;&#38901;&#24459;&#65292;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#38901;&#24459;&#20449;&#24687;&#21512;&#24182;&#21040;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20013;&#21487;&#20197;&#33719;&#24471;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38901;&#24459;&#20851;&#27880;&#65292;&#23427;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#24335;&#21033;&#29992;&#38901;&#24459;&#29305;&#24449;&#26469;&#29983;&#25104;&#36328;&#26102;&#38388;&#24103;&#30340;&#27880;&#24847;&#21147;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38901;&#24459;&#33976;&#39311;&#65292;&#26469;&#26126;&#30830;&#22320;&#23398;&#20064;&#22768;&#23398;&#32534;&#30721;&#22120;&#20013;&#30340;&#38901;&#24459;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36830;&#25509;&#38544;&#21547;&#30340;&#38901;&#24459;&#29305;&#24449;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#25552;&#39640;&#20102;&#22522;&#32447;&#32467;&#26524;&#65292;&#20854;&#20013;&#38901;&#24459;&#33976;&#39311;&#26041;&#27861;&#22312;SLURP&#21644;STOP&#25968;&#25454;&#38598;&#19978;&#30456;&#23545;&#20110;&#38901;&#24459;&#22522;&#32447;&#25552;&#39640;&#20102;8&#65285;&#21644;2&#65285;&#30340;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most End-to-End SLU methods depend on the pretrained ASR or language model features for intent prediction. However, other essential information in speech, such as prosody, is often ignored. Recent research has shown improved results in classifying dialogue acts by incorporating prosodic information. The margins of improvement in these methods are minimal as the neural models ignore prosodic features. In this work, we propose prosody-attention, which uses the prosodic features differently to generate attention maps across time frames of the utterance. Then we propose prosody-distillation to explicitly learn the prosodic information in the acoustic encoder rather than concatenating the implicit prosodic features. Both the proposed methods improve the baseline results, and the prosody-distillation method gives an intent classification accuracy improvement of 8\% and 2\% on SLURP and STOP datasets over the prosody baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#20351;&#29992;&#38382;&#39064;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#32467;&#26500;&#65292;&#26681;&#25454;&#38382;&#39064;SRL&#32467;&#26500;&#30340;&#21738;&#19968;&#37096;&#20998;&#34987;&#20851;&#27880;&#26469;&#20419;&#36827;&#36328;&#35270;&#39057;&#24103;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.08059</link><description>&lt;p&gt;
&#20107;&#20214;&#32423;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering. (arXiv:2305.08059v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#20351;&#29992;&#38382;&#39064;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#32467;&#26500;&#65292;&#26681;&#25454;&#38382;&#39064;SRL&#32467;&#26500;&#30340;&#21738;&#19968;&#37096;&#20998;&#34987;&#20851;&#27880;&#26469;&#20419;&#36827;&#36328;&#35270;&#39057;&#24103;&#30340;&#22797;&#26434;&#25512;&#29702;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#32423;&#35270;&#39057;&#38382;&#31572;&#38656;&#35201;&#36328;&#36234;&#35270;&#39057;&#20107;&#20214;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#65292;&#20197;&#33719;&#21462;&#25552;&#20379;&#26368;&#20339;&#31572;&#26696;&#25152;&#38656;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#27169;&#22411;&#24615;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#22312;&#20107;&#20214;&#23618;&#38754;&#19978;&#20351;&#29992;&#38382;&#39064;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#26174;&#24335;&#35821;&#20041;&#32852;&#31995;&#12290;&#26377;&#24517;&#35201;&#21033;&#29992;&#36825;&#31181;&#35821;&#20041;&#32852;&#31995;&#65292;&#20419;&#36827;&#36328;&#35270;&#39057;&#24103;&#30340;&#22797;&#26434;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#38382;&#31572;&#30340;&#35821;&#20041;&#24863;&#30693;&#21160;&#24577;&#22238;&#39038;-&#21069;&#30651;&#25512;&#29702;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22312;&#21160;&#24577;&#25512;&#29702;&#36807;&#31243;&#20013;&#26126;&#30830;&#20351;&#29992;&#38382;&#39064;&#30340;&#35821;&#20041;&#35282;&#33394;&#26631;&#27880;&#65288;SRL&#65289;&#32467;&#26500;&#65292;&#26681;&#25454;&#38382;&#39064;SRL&#32467;&#26500;&#30340;&#21738;&#19968;&#37096;&#20998;&#65288;agent&#12289;verb&#12289;patient&#31561;&#65289;&#34987;&#20851;&#27880;&#26469;&#20915;&#23450;&#26159;&#21542;&#31227;&#21160;&#21040;&#19979;&#19968;&#24103;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;EVQA&#25968;&#25454;&#38598;&#8212;&#8212;TrafficQA&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ProKnow&#30340;&#27010;&#24565;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#30340;&#23433;&#20840;&#24615;&#21644;&#19987;&#19994;&#24615;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#23433;&#20840;&#24615;&#12289;&#30693;&#35782;&#25429;&#33719;&#21644;&#21487;&#35299;&#37322;&#24615;&#26469;&#27169;&#25311;&#27969;&#31243;&#30693;&#35782;&#12290;&#22312;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;ProKnow&#24341;&#23548;&#30340;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08010</link><description>&lt;p&gt;
ProKnow&#65306;&#29992;&#20110;&#23433;&#20840;&#38480;&#21046;&#21644;&#21487;&#35299;&#37322;&#38382;&#39064;&#29983;&#25104;&#30340;&#27969;&#31243;&#30693;&#35782;&#22312;&#24515;&#29702;&#20581;&#24247;&#35786;&#26029;&#36741;&#21161;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
ProKnow: Process Knowledge for Safety Constrained and Explainable Question Generation for Mental Health Diagnostic Assistance. (arXiv:2305.08010v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ProKnow&#30340;&#27010;&#24565;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#30340;&#23433;&#20840;&#24615;&#21644;&#19987;&#19994;&#24615;&#12290;&#21516;&#26102;&#24320;&#21457;&#20102;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#23433;&#20840;&#24615;&#12289;&#30693;&#35782;&#25429;&#33719;&#21644;&#21487;&#35299;&#37322;&#24615;&#26469;&#27169;&#25311;&#27969;&#31243;&#30693;&#35782;&#12290;&#22312;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;ProKnow&#24341;&#23548;&#30340;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#26356;&#23433;&#20840;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#34394;&#25311;&#24515;&#29702;&#20581;&#24247;&#21161;&#25163;&#65288;VMHA&#65289;&#25552;&#20379;&#36741;&#23548;&#21644;&#24314;&#35758;&#25252;&#29702;&#12290;&#23427;&#20204;&#19981;&#36827;&#34892;&#24739;&#32773;&#35786;&#26029;&#21327;&#21161;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#23433;&#20840;&#21463;&#38480;&#21644;&#19987;&#19994;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#30340;&#22521;&#35757;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23450;&#20041;Proknow&#20026;&#19968;&#32452;&#26377;&#24207;&#20449;&#24687;&#65292;&#23427;&#26144;&#23556;&#21040;&#39046;&#22495;&#19987;&#23478;&#30340;&#22522;&#20110;&#35777;&#25454;&#30340;&#25351;&#21335;&#25110;&#27010;&#24565;&#29702;&#35299;&#31867;&#21035;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#30001;&#23433;&#20840;&#38480;&#21046;&#21644;Proknow&#24341;&#23548;&#30340;&#35786;&#26029;&#23545;&#35805;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#29983;&#25104;&#65288;NLG&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#24739;&#32773;&#20132;&#20114;&#25910;&#38598;&#35786;&#26029;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#26126;&#30830;&#22320;&#24314;&#27169;&#23433;&#20840;&#24615;&#65292;&#30693;&#35782;&#25429;&#33719;&#21644;&#21487;&#35299;&#37322;&#24615;&#26469;&#27169;&#25311;&#27969;&#31243;&#30693;&#35782;&#12290;&#20351;&#29992;ProKnow&#24341;&#23548;&#30340;LM&#22686;&#24378;&#26041;&#27861;&#22312;&#25233;&#37057;&#30151;&#21644;&#28966;&#34385;&#30151;&#20013;&#29983;&#25104;&#20102;89&#65285;&#26356;&#23433;&#20840;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Virtual Mental Health Assistants (VMHAs) provide counseling and suggestive care. They refrain from patient diagnostic assistance because they lack training in safety-constrained and specialized clinical process knowledge. In this work, we define Proknow as an ordered set of information that maps to evidence-based guidelines or categories of conceptual understanding to experts in a domain. We also introduce a new dataset of diagnostic conversations guided by safety constraints and Proknow that healthcare professionals use. We develop a method for natural language question generation (NLG) that collects diagnostic information from the patient interactively. We demonstrate the limitations of using state-of-the-art large-scale language models (LMs) on this dataset. Our algorithm models the process knowledge through explicitly modeling safety, knowledge capture, and explainability. LMs augmented with ProKnow guided method generated 89% safer questions in the depression and anxiety d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#30340;&#22810;&#31181;&#23433;&#20840;&#39118;&#38505;&#21644;&#32469;&#36807;&#20445;&#25252;&#25514;&#26045;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#21457;&#29616;&#21363;&#20351;&#20445;&#25252;&#25514;&#26045;&#23384;&#22312;&#65292;LLMs&#20173;&#23384;&#22312;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20026;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#25552;&#20379;&#20102;&#28508;&#22312;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.08005</link><description>&lt;p&gt;
&#36229;&#36234;&#20445;&#38556;&#65306;&#25506;&#32034;ChatGPT&#30340;&#23433;&#20840;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Beyond the Safeguards: Exploring the Security Risks of ChatGPT. (arXiv:2305.08005v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#30340;&#22810;&#31181;&#23433;&#20840;&#39118;&#38505;&#21644;&#32469;&#36807;&#20445;&#25252;&#25514;&#26045;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#21457;&#29616;&#21363;&#20351;&#20445;&#25252;&#25514;&#26045;&#23384;&#22312;&#65292;LLMs&#20173;&#23384;&#22312;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26412;&#30740;&#31350;&#20026;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#25552;&#20379;&#20102;&#28508;&#22312;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#23433;&#20840;&#24615;&#12289;&#23433;&#20840;&#39118;&#38505;&#21644;&#20262;&#29702;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;ChatGPT&#30456;&#20851;&#30340;&#19981;&#21516;&#31867;&#22411;&#23433;&#20840;&#39118;&#38505;&#30340;&#27010;&#36848;&#65292;&#21253;&#25324;&#24694;&#24847;&#25991;&#26412;&#21644;&#20195;&#30721;&#29983;&#25104;&#12289;&#31169;&#20154;&#25968;&#25454;&#25259;&#38706;&#12289;&#27450;&#35784;&#24615;&#26381;&#21153;&#12289;&#20449;&#24687;&#25628;&#38598;&#21644;&#29983;&#25104;&#19981;&#36947;&#24503;&#20869;&#23481;&#31561;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;ChatGPT&#20869;&#23481;&#36807;&#28388;&#22120;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#35752;&#20102;&#32469;&#36807;&#36825;&#20123;&#20445;&#25252;&#30340;&#28508;&#22312;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#24403;&#20445;&#25252;&#25514;&#26045;&#23384;&#22312;&#26102;LLMs&#20013;&#20173;&#23384;&#22312;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#39118;&#38505;&#12290;&#26681;&#25454;&#23433;&#20840;&#39118;&#38505;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#30340;&#28508;&#22312;&#31574;&#30053;&#65292;&#24182;&#21521;&#30740;&#31350;&#20154;&#21592;&#12289;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#34892;&#19994;&#19987;&#19994;&#20154;&#21592;&#20171;&#32461;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;LLMs&#25152;&#38754;&#20020;&#30340;&#22797;&#26434;&#23433;&#20840;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#26377;&#21161;&#20110;&#23545;LLMs&#24102;&#26469;&#30340;&#20262;&#29702;&#21644;&#23433;&#20840;&#38382;&#39064;&#36827;&#34892;&#25345;&#32493;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing popularity of large language models (LLMs) such as ChatGPT has led to growing concerns about their safety, security risks, and ethical implications. This paper aims to provide an overview of the different types of security risks associated with ChatGPT, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. We present an empirical study examining the effectiveness of ChatGPT's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in LLMs even when protections are in place. Based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by LLMs like ChatGPT. This study contributes to the ongoing discussion on the ethical and security 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;MultiClaim&#65292;&#29992;&#20110;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#30340;&#26816;&#32034;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21450;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.07991</link><description>&lt;p&gt;
&#22810;&#35821;&#31181;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#32034;&#24341;
&lt;/p&gt;
&lt;p&gt;
Multilingual Previously Fact-Checked Claim Retrieval. (arXiv:2305.07991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;MultiClaim&#65292;&#29992;&#20110;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#30340;&#26816;&#32034;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21450;&#19968;&#20010;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#23454;&#26680;&#26597;&#21592;&#24120;&#24120;&#21463;&#21040;&#38656;&#35201;&#36827;&#34892;&#26680;&#26597;&#30340;&#22312;&#32447;&#20869;&#23481;&#25968;&#37327;&#30340;&#38480;&#21046;&#12290;NLP&#21487;&#20197;&#36890;&#36807;&#26816;&#32034;&#19982;&#27491;&#22312;&#35843;&#26597;&#30340;&#20869;&#23481;&#30456;&#20851;&#30340;&#24050;&#23384;&#22312;&#30340;&#20107;&#23454;&#26680;&#26597;&#26469;&#24110;&#21161;&#20182;&#20204;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;MultiClaim&#8212;&#8212;&#29992;&#20110;&#20808;&#21069;&#20107;&#23454;&#26680;&#26597;&#26816;&#32034;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#30340;27&#31181;&#35821;&#35328;&#30340;28k&#31687;&#25991;&#31456;&#12289;39&#31181;&#35821;&#35328;&#30340;206k&#31687;&#30001;&#19987;&#19994;&#20107;&#23454;&#26680;&#26597;&#21592;&#25776;&#20889;&#30340;&#20107;&#23454;&#26680;&#26597;&#20197;&#21450;&#36825;&#20004;&#20010;&#32452;&#20043;&#38388;&#30340;31k&#20010;&#36830;&#25509;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#24191;&#27867;&#12289;&#35821;&#35328;&#22810;&#20803;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19981;&#21516;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#21450;&#20854;&#21508;&#33258;&#30340;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35780;&#20272;&#36825;&#26679;&#19968;&#20010;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#22312;&#35299;&#37322;&#32467;&#26524;&#20043;&#21069;&#38656;&#35201;&#37319;&#21462;&#36866;&#24403;&#30340;&#25514;&#26045;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#19968;&#31181;&#32463;&#36807;&#30417;&#30563;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#22312;&#36825;&#20010;&#26041;&#27861;&#19978;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fact-checkers are often hampered by the sheer amount of online content that needs to be fact-checked. NLP can help them by retrieving already existing fact-checks relevant to the content being investigated. This paper introduces a new multilingual dataset -- MultiClaim -- for previously fact-checked claim retrieval. We collected 28k posts in 27 languages from social media, 206k fact-checks in 39 languages written by professional fact-checkers, as well as 31k connections between these two groups. This is the most extensive and the most linguistically diverse dataset of this kind to date. We evaluated how different unsupervised methods fare on this dataset and its various dimensions. We show that evaluating such a diverse dataset has its complexities and proper care needs to be taken before interpreting the results. We also evaluated a supervised fine-tuning approach, improving upon the unsupervised method significantly.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20250;&#35758;&#25688;&#35201;&#26694;&#26550;SVB&#65292;&#36890;&#36807;&#19977;&#20010;&#36807;&#31243;&#21387;&#32553;&#20887;&#20313;&#20869;&#23481;&#24182;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#12290;&#20854;&#20013;&#65292;&#28369;&#21160;&#31383;&#21475;&#23545;&#35805;&#24674;&#22797;&#19982;&#35780;&#20998;&#12289;&#36890;&#36947;&#37325;&#35201;&#24615;&#24471;&#20998;&#25237;&#31080;&#21644;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#31561;&#31639;&#27861;&#29992;&#20110;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.07988</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#21477;&#23376;&#21387;&#32553;&#29992;&#20110;&#20250;&#35758;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Sentence Compression for Meeting Summarization. (arXiv:2305.07988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20250;&#35758;&#25688;&#35201;&#26694;&#26550;SVB&#65292;&#36890;&#36807;&#19977;&#20010;&#36807;&#31243;&#21387;&#32553;&#20887;&#20313;&#20869;&#23481;&#24182;&#20445;&#30041;&#20851;&#38190;&#20449;&#24687;&#12290;&#20854;&#20013;&#65292;&#28369;&#21160;&#31383;&#21475;&#23545;&#35805;&#24674;&#22797;&#19982;&#35780;&#20998;&#12289;&#36890;&#36947;&#37325;&#35201;&#24615;&#24471;&#20998;&#25237;&#31080;&#21644;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#31561;&#31639;&#27861;&#29992;&#20110;&#23454;&#29616;&#36825;&#20010;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#24635;&#32467;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#21040;&#20250;&#35758;&#35760;&#24405;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#22240;&#20026;&#20250;&#35758;&#35821;&#26009;&#24211;&#36890;&#24120;&#28041;&#21450;&#22810;&#20010;&#21442;&#19982;&#32773;&#30340;&#20887;&#38271;&#23545;&#35805;&#24182;&#20805;&#26021;&#30528;&#20887;&#20313;&#21644;&#29712;&#30862;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVB&#65292;&#19968;&#20010;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#20250;&#35758;&#25688;&#35201;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#20010;&#36807;&#31243;&#8220;&#21387;&#32553;&#8221;&#20887;&#20313;&#32780;&#20445;&#30041;&#37325;&#35201;&#20869;&#23481;&#65306;&#28369;&#21160;&#31383;&#21475;&#23545;&#35805;&#24674;&#22797;&#19982;&#35780;&#20998;&#12289;&#36890;&#36947;&#37325;&#35201;&#24615;&#24471;&#20998;&#25237;&#31080;&#21644;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#33258;&#30417;&#30563;&#33539;&#24335;&#19979;&#65292;&#28369;&#21160;&#31383;&#21475;&#35780;&#20998;&#26088;&#22312;&#20174;&#22810;&#20010;&#35270;&#35282;&#35780;&#20272;&#27599;&#20010;&#26631;&#35760;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#36890;&#36807;&#36890;&#36947;&#25237;&#31080;&#32858;&#21512;&#36825;&#20123;&#35780;&#20998;&#12290;&#20855;&#26377;&#39640;&#35780;&#20998;&#30340;&#26631;&#35760;&#23558;&#34987;&#35270;&#20026;&#26174;&#30528;&#20449;&#24687;&#24182;&#26631;&#35760;&#20026;&#8220;anchers&#8221;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20351;&#36755;&#20837;&#38271;&#24230;&#36866;&#21512;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#24230;&#38480;&#21046;&#65292;&#37319;&#29992;&#20102;&#30456;&#23545;&#20301;&#32622;&#20998;&#31665;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conventional summarization model often fails to capture critical information in meeting transcripts, as meeting corpus usually involves multiple parties with lengthy conversations and is stuffed with redundant and trivial content. To tackle this problem, we present SVB, an effective and efficient framework for meeting summarization that `compress' the redundancy while preserving important content via three processes: sliding-window dialogue restoration and \textbf{S}coring, channel-wise importance score \textbf{V}oting, and relative positional \textbf{B}ucketing. Specifically, under the self-supervised paradigm, the sliding-window scoring aims to rate the importance of each token from multiple views. Then these ratings are aggregated by channel-wise voting. Tokens with high ratings will be regarded as salient information and labeled as \textit{anchors}. Finally, to tailor the lengthy input to an acceptable length for the language model, the relative positional bucketing algorithm i
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#30340;&#24544;&#23454;&#20107;&#23454;&#24615;&#38169;&#35823;&#32416;&#27491;&#26694;&#26550;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21450;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24615;&#35780;&#20272;&#26631;&#20934;&#65292;&#36866;&#29992;&#20110;&#32500;&#25252;&#25991;&#26412;&#30693;&#35782;&#24211;&#21644;&#39044;&#38450;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2305.07982</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20449;&#23454;&#20107;&#23454;&#32416;&#38169;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Faithful Factual Error Correction. (arXiv:2305.07982v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07982
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#30340;&#24544;&#23454;&#20107;&#23454;&#24615;&#38169;&#35823;&#32416;&#27491;&#26694;&#26550;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#26041;&#27861;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21450;&#35299;&#37322;&#24615;&#21644;&#24544;&#23454;&#24615;&#35780;&#20272;&#26631;&#20934;&#65292;&#36866;&#29992;&#20110;&#32500;&#25252;&#25991;&#26412;&#30693;&#35782;&#24211;&#21644;&#39044;&#38450;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24544;&#23454;&#22320;&#32416;&#27491;&#20107;&#23454;&#24615;&#38169;&#35823;&#23545;&#20110;&#32500;&#25252;&#25991;&#26412;&#30693;&#35782;&#24211;&#30340;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#33267;&#20851;&#37325;&#35201;&#12290;&#20511;&#37492;&#20154;&#31867;&#35782;&#21035;&#21644;&#32416;&#27491;&#20107;&#23454;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21046;&#23450;&#26377;&#20851;&#36755;&#20837;&#22768;&#26126;&#30340;&#38382;&#39064;&#65292;&#26597;&#25214;&#32473;&#23450;&#35777;&#25454;&#20013;&#30340;&#27491;&#30830;&#31572;&#26696;&#65292;&#24182;&#26681;&#25454;&#20854;&#19982;&#35777;&#25454;&#30340;&#19968;&#33268;&#24615;&#35780;&#20272;&#27599;&#20010;&#32416;&#27491;&#30340;&#20449;&#23454;&#24615;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#26694;&#26550;&#22312;FEVER&#21644;SciFact&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#20013;&#27604;&#23436;&#20840;&#30417;&#30563;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36755;&#20986;&#26356;&#21152;&#24544;&#23454;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#26694;&#26550;&#30340;&#21487;&#20998;&#35299;&#24615;&#22825;&#28982;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25581;&#31034;&#35780;&#20272;&#20107;&#23454;&#38169;&#35823;&#20462;&#27491;&#30340;&#26368;&#21512;&#36866;&#24230;&#37327;&#26631;&#20934;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24120;&#29992;&#24230;&#37327;&#26631;&#20934;&#19982;&#19977;&#20010;&#19981;&#21516;&#32500;&#24230;&#30340;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21253;&#25324;&#21487;&#29702;&#35299;&#24615;&#21644;&#24544;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Faithfully correcting factual errors is critical for maintaining the integrity of textual knowledge bases and preventing hallucinations in sequence-to-sequence models. Drawing on humans' ability to identify and correct factual errors, we present a zero-shot framework that formulates questions about input claims, looks for correct answers in the given evidence, and assesses the faithfulness of each correction based on its consistency with the evidence. Our zero-shot framework outperforms fully-supervised approaches, as demonstrated by experiments on the FEVER and SciFact datasets, where our outputs are shown to be more faithful. More importantly, the decomposability nature of our framework inherently provides interpretability. Additionally, to reveal the most suitable metrics for evaluating factual error corrections, we analyze the correlation between commonly used metrics with human judgments in terms of three different dimensions regarding intelligibility and faithfulness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#23637;&#20102;&#19968;&#39033;&#26032;&#30340;&#8220;&#40560;&#27966;-&#40509;&#27966;&#8221;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;RoBERTa-large&#27169;&#22411;&#26500;&#24314;&#20102;FOMC&#25991;&#20214;&#21457;&#24067;&#26085;&#30340;&#36135;&#24065;&#25919;&#31574;&#31435;&#22330;&#25351;&#25968;&#65292;&#24182;&#23545;&#20854;&#23545;&#22269;&#24211;&#24066;&#22330;&#12289;&#32929;&#31080;&#24066;&#22330;&#21644;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.07972</link><description>&lt;p&gt;
&#21315;&#20159;&#32654;&#20803;&#30340;&#35805;&#35821;&#65306;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#20219;&#21153;&#21644;&#24066;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Trillion Dollar Words: A New Financial Dataset, Task &amp; Market Analysis. (arXiv:2305.07972v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07972
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#37329;&#34701;&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#23637;&#20102;&#19968;&#39033;&#26032;&#30340;&#8220;&#40560;&#27966;-&#40509;&#27966;&#8221;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;RoBERTa-large&#27169;&#22411;&#26500;&#24314;&#20102;FOMC&#25991;&#20214;&#21457;&#24067;&#26085;&#30340;&#36135;&#24065;&#25919;&#31574;&#31435;&#22330;&#25351;&#25968;&#65292;&#24182;&#23545;&#20854;&#23545;&#22269;&#24211;&#24066;&#22330;&#12289;&#32929;&#31080;&#24066;&#22330;&#21644;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#20844;&#24320;&#24066;&#22330;&#22996;&#21592;&#20250;&#65288;FOMC&#65289;&#30340;&#36135;&#24065;&#25919;&#31574;&#22768;&#26126;&#26159;&#37329;&#34701;&#24066;&#22330;&#22238;&#25253;&#30340;&#20027;&#35201;&#39537;&#21160;&#22240;&#32032;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20102;&#35299;&#36135;&#24065;&#25919;&#31574;&#22914;&#20309;&#24433;&#21709;&#37329;&#34701;&#24066;&#22330;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;FOMC&#28436;&#35762;&#12289;&#20250;&#35758;&#35760;&#24405;&#21644;&#26032;&#38395;&#21457;&#24067;&#20250;&#35760;&#24405;&#30340;&#26368;&#22823;&#21270;&#25991;&#26412;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153; - &#8220;&#40560;&#27966;-&#40509;&#27966;&#8221;&#20998;&#31867;&#65292;&#24182;&#22312;&#25552;&#35758;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#20351;&#29992;&#25928;&#26524;&#26368;&#22909;&#30340;&#27169;&#22411;&#65288;RoBERTa-large&#65289;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;FOMC&#25991;&#20214;&#21457;&#24067;&#26085;&#30340;&#36135;&#24065;&#25919;&#31574;&#31435;&#22330;&#25351;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#26500;&#24314;&#30340;&#25351;&#25968;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#23545;&#22269;&#24211;&#24066;&#22330;&#12289;&#32929;&#31080;&#24066;&#22330;&#21644;&#23439;&#35266;&#32463;&#27982;&#25351;&#26631;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#20195;&#30721;&#22312;Huggingface&#21644;GitHub&#19978;&#20197;CC BY-NC 4.0&#35768;&#21487;&#35777;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monetary policy pronouncements by Federal Open Market Committee (FOMC) are a major driver of financial market returns. We construct the largest tokenized and annotated dataset of FOMC speeches, meeting minutes, and press conference transcripts in order to understand how monetary policy influences financial markets. In this study, we develop a novel task of hawkish-dovish classification and benchmark various pre-trained language models on the proposed dataset. Using the best-performing model (RoBERTa-large), we construct a measure of monetary policy stance for the FOMC document release days. To evaluate the constructed measure, we study its impact on the treasury market, stock market, and macroeconomic indicators. Our dataset, models, and code are publicly available on Huggingface and GitHub under CC BY-NC 4.0 license.
&lt;/p&gt;</description></item><item><title>GPT-Sentinel&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20854;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#21306;&#20998;&#36825;&#20004;&#31181;&#25991;&#26412;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.07969</link><description>&lt;p&gt;
GPT-Sentinel&#65306;&#21306;&#20998;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#20869;&#23481;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content. (arXiv:2305.07969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07969
&lt;/p&gt;
&lt;p&gt;
GPT-Sentinel&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#65292;&#20854;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97&#65285;&#65292;&#24182;&#19988;&#25581;&#31034;&#20102;&#21306;&#20998;&#36825;&#20004;&#31181;&#25991;&#26412;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#32534;&#20889;&#25991;&#26412;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#21457;&#24067;&#20102;&#19968;&#20010;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;OpenGPTText&#65292;&#20854;&#20013;&#21253;&#21547;&#20351;&#29992;ChatGPT&#29983;&#25104;&#30340;&#37325;&#26032;&#34920;&#36848;&#20869;&#23481;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#12289;&#23454;&#29616;&#12289;&#35757;&#32451;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#20998;&#21035;&#20351;&#29992;Robustly Optimized BERT Pretraining Approach&#65288;RoBERTa&#65289;&#21644;Text-to-Text Transfer Transformer&#65288;T5&#65289;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#36890;&#36807;&#21508;&#31181;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#65292;&#20934;&#30830;&#29575;&#36229;&#36807;&#20102;97%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#21462;&#21644;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#21644;ChatGPT&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#20851;&#38190;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#29983;&#25104;&#25991;&#26412;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach for detecting ChatGPT-generated vs. human-written text using language models. To this end, we first collected and released a pre-processed dataset named OpenGPTText, which consists of rephrased content generated using ChatGPT. We then designed, implemented, and trained two different models for text classification, using Robustly Optimized BERT Pretraining Approach (RoBERTa) and Text-to-Text Transfer Transformer (T5), respectively. Our models achieved remarkable results, with an accuracy of over 97% on the test dataset, as evaluated through various metrics. Furthermore, we conducted an interpretability study to showcase our model's ability to extract and differentiate key features between human-written and ChatGPT-generated text. Our findings provide important insights into the effective use of language models to detect generated text.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07961</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models in Conversational Recommender Systems. (arXiv:2305.07961v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#65292;&#35299;&#20915;&#22312;&#35813;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38754;&#20020;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#36890;&#36807;&#21551;&#29992;&#23454;&#26102;&#30340;&#22810;&#36718;&#23545;&#35805;&#20351;&#29992;&#25143;&#26356;&#21152;&#36879;&#26126;&#21644;&#25484;&#25511;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#19982;&#20154;&#31867;&#23545;&#35805;&#33258;&#28982;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#19990;&#30028;&#30693;&#35782;&#21644;&#24120;&#35782;&#25512;&#29702;&#34701;&#20837;&#21040;&#35821;&#35328;&#29702;&#35299;&#20013;&#65292;&#36827;&#19968;&#27493;&#37322;&#25918;&#20102;&#36825;&#19968;&#33539;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#20013;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#26032;&#30340;&#25216;&#26415;&#25361;&#25112;&#65292;&#21253;&#25324;&#36866;&#24403;&#22320;&#29702;&#35299;&#21644;&#25511;&#21046;&#22797;&#26434;&#30340;&#23545;&#35805;&#21644;&#20174;&#22806;&#37096;&#20449;&#24687;&#28304;&#26816;&#32034;&#12290;&#30001;&#20110;&#22823;&#32780;&#19981;&#26029;&#22686;&#38271;&#30340;&#39033;&#30446;&#35821;&#26009;&#24211;&#21644;&#32570;&#20047;&#23545;&#35805;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#38382;&#39064;&#21152;&#21095;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#31471;&#21040;&#31471;&#22823;&#35268;&#27169;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#25143;&#20559;&#22909;&#29702;&#35299;&#12289;&#28789;&#27963;&#30340;&#23545;&#35805;&#31649;&#29702;&#21644;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#20316;&#20026;&#25972;&#20010;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#30340;&#26032;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Conversational Recommender System (CRS) offers increased transparency and control to users by enabling them to engage with the system through a real-time multi-turn dialogue. Recently, Large Language Models (LLMs) have exhibited an unprecedented ability to converse naturally and incorporate world knowledge and common-sense reasoning into language understanding, unlocking the potential of this paradigm. However, effectively leveraging LLMs within a CRS introduces new technical challenges, including properly understanding and controlling a complex conversation and retrieving from external sources of information. These issues are exacerbated by a large, evolving item corpus and a lack of conversational data for training. In this paper, we provide a roadmap for building an end-to-end large-scale CRS using LLMs. In particular, we propose new implementations for user preference understanding, flexible dialogue management and explainable recommendations as part of an integrated architecture
&lt;/p&gt;</description></item><item><title>AMTSS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#25903;&#25345;&#25104;&#26412;&#25928;&#30410;&#30340;&#35821;&#35328;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;XNLI&#25968;&#25454;&#38598;&#21644;AliExpress&#20013;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07928</link><description>&lt;p&gt;
AMTSS&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#25512;&#29702;&#30340;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference. (arXiv:2305.07928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07928
&lt;/p&gt;
&lt;p&gt;
AMTSS&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#30693;&#35782;&#33976;&#39311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#25903;&#25345;&#25104;&#26412;&#25928;&#30410;&#30340;&#35821;&#35328;&#25512;&#29702;&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;XNLI&#25968;&#25454;&#38598;&#21644;AliExpress&#20013;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23545;&#20110;&#25512;&#20986;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#25903;&#25345;&#22810;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35821;&#35328;&#25512;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AMTSS&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#25945;&#24072;&#21333;&#23398;&#29983;&#33976;&#39311;&#26694;&#26550;&#65292;&#23427;&#20801;&#35768;&#20174;&#22810;&#20010;&#32769;&#24072;&#20013;&#25552;&#28860;&#30693;&#35782;&#21040;&#19968;&#20010;&#23398;&#29983;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#31181;&#33258;&#36866;&#24212;&#23398;&#20064;&#31574;&#30053;&#21644;&#25945;&#24072;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20351;&#23398;&#29983;&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#26368;&#22823;&#36793;&#32536;&#32769;&#24072;&#20013;&#23398;&#20064;&#65292;&#24182;&#36731;&#26494;&#36866;&#24212;&#26032;&#35821;&#35328;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20849;&#20139;&#30340;&#23398;&#29983;&#32534;&#30721;&#22120;&#65292;&#20197;&#19981;&#21516;&#30340;&#25237;&#24433;&#23618;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#65292;&#36825;&#26377;&#21161;&#20110;&#22823;&#22823;&#38477;&#20302;&#24320;&#21457;&#21644;&#26426;&#22120;&#25104;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;AMTSS&#22312;&#20844;&#20849;XNLI&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#30340;&#30005;&#23376;&#21830;&#21153;&#34892;&#19994;&#25968;&#25454;&#38598;AliExpress&#65288;AE&#65289;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is of key importance to launching multilingual pre-trained language models for real applications. To support cost-effective language inference in multilingual settings, we propose AMTSS, an adaptive multi-teacher single-student distillation framework, which allows distilling knowledge from multiple teachers to a single student. We first introduce an adaptive learning strategy and teacher importance weight, which enables a student to effectively learn from max-margin teachers and easily adapt to new languages. Moreover, we present a shared student encoder with different projection layers in support of multiple languages, which contributes to largely reducing development and machine cost. Experimental results show that AMTSS gains competitive results on the public XNLI dataset and the realistic industrial dataset AliExpress (AE) in the E-commerce scenario.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RC3&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#21017;&#21270;&#23545;&#27604;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#23398;&#20064;&#65292;&#26377;&#25928;&#21033;&#29992;&#24369;&#23545;&#20934;&#30340;&#22810;&#35821;&#31181;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#20174;&#32780;&#22312;&#22810;&#35821;&#31181;V&amp;L&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07927</link><description>&lt;p&gt;
RC3: &#35268;&#21017;&#21270;&#23545;&#27604;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
RC3: Regularized Contrastive Cross-lingual Cross-modal Pre-training. (arXiv:2305.07927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RC3&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35268;&#21017;&#21270;&#23545;&#27604;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#23398;&#20064;&#65292;&#26377;&#25928;&#21033;&#29992;&#24369;&#23545;&#20934;&#30340;&#22810;&#35821;&#31181;&#22270;&#20687;-&#25991;&#26412;&#23545;&#65292;&#20174;&#32780;&#22312;&#22810;&#35821;&#31181;V&amp;L&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#35270;&#35273;-&#35821;&#35328;&#65288;V&amp;L&#65289;&#39044;&#35757;&#32451;&#24050;&#32463;&#22312;&#36328;&#19981;&#21516;&#35821;&#35328;&#21644;&#27169;&#24577;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#38480;&#21046;&#20102;V&amp;L&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#22810;&#35821;&#31181;&#29615;&#22659;&#20013;&#30340;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#35268;&#21017;&#21270;&#23545;&#27604;&#36328;&#35821;&#35328;&#36328;&#27169;&#24577;&#65288;RC^3&#65289;&#39044;&#35757;&#32451;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#24369;&#23545;&#20934;&#22810;&#35821;&#31181;&#22270;&#20687;-&#25991;&#26412;&#23545;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#19968;&#31181;&#35268;&#21017;&#21270;&#30340;&#36328;&#35821;&#35328;&#35270;&#35273;-&#25991;&#26412;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#65292;&#26681;&#25454;&#25991;&#26412;&#30456;&#20851;&#24615;&#32422;&#26463;&#24369;&#23545;&#20934;&#35270;&#35273;-&#25991;&#26412;&#36755;&#20837;&#30340;&#34920;&#31034;&#25509;&#36817;&#24230;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;V&amp;L&#39044;&#35757;&#32451;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#33521;&#35821;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#20294;&#25910;&#38598;&#21644;&#32763;&#35793;&#20005;&#26684;&#23545;&#40784;&#30340;&#22810;&#35821;&#31181;&#22270;&#20687;-&#25991;&#26412;&#23545;&#30340;&#25104;&#26412;&#36890;&#24120;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual vision-language (V&amp;L) pre-training has achieved remarkable progress in learning universal representations across different modalities and languages. In spite of recent success, there still remain challenges limiting further improvements of V&amp;L pre-trained models in multilingual settings. Particularly, current V&amp;L pre-training methods rely heavily on strictly-aligned multilingual image-text pairs generated from English-centric datasets through machine translation. However, the cost of collecting and translating such strictly-aligned datasets is usually unbearable. In this paper, we propose Regularized Contrastive Cross-lingual Cross-modal (RC^3) pre-training, which further exploits more abundant weakly-aligned multilingual image-text pairs. Specifically, we design a regularized cross-lingual visio-textual contrastive learning objective that constrains the representation proximity of weakly-aligned visio-textual inputs according to textual relevance. Besides, existing V&amp;L pr
&lt;/p&gt;</description></item><item><title>CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07922</link><description>&lt;p&gt;
CodeT5+: &#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#24320;&#25918;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07922
&lt;/p&gt;
&lt;p&gt;
CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#28304;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#26041;&#38754;&#26377;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#29305;&#23450;&#30340;&#26550;&#26500;(&#20165;&#32534;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;)&#25110;&#20381;&#36182;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12290;&#21069;&#19968;&#31181;&#33539;&#24335;&#21463;&#21040;&#24212;&#29992;&#28789;&#27963;&#24615;&#30340;&#38480;&#21046;&#65292;&#32780;&#22312;&#21518;&#19968;&#31181;&#33539;&#24335;&#20013;&#65292;&#27169;&#22411;&#34987;&#35270;&#20026;&#25152;&#26377;&#20219;&#21153;&#30340;&#21333;&#19968;&#31995;&#32479;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#20219;&#21153;&#30340;&#23376;&#38598;&#19978;&#24615;&#33021;&#19981;&#20248;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#19982;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#19981;&#30456;&#20851;&#65292;&#22240;&#27492;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CodeT5+&#8221;&#65292;&#36825;&#26159;&#19968;&#32452;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#20854;&#20013;&#32452;&#20214;&#27169;&#22359;&#21487;&#20197;&#28789;&#27963;&#32452;&#21512;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#26159;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#39044;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;CodeT5+&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#20195;&#30721;&#29305;&#23450;LLM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretrai
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.07912</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Model with Prompts for Temporal Knowledge Graph Completion. (arXiv:2305.07912v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07912
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#65292;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#36890;&#36807;&#36974;&#30422;&#31574;&#30053;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;TKGC&#65289;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22312;&#24050;&#30693;&#30340;&#26102;&#38388;&#25139;&#19978;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#32570;&#22833;&#37096;&#20998;&#30340;&#20107;&#23454;&#65292;&#24182;&#22312;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#34920;&#31034;&#65292;&#21516;&#26102;&#31895;&#30053;&#22320;&#25552;&#21462;&#26102;&#38388;&#25139;&#20013;&#30340;&#20449;&#24687;&#65292;&#24182;&#19981;&#20805;&#20998;&#21033;&#29992;&#20851;&#31995;&#20013;&#38544;&#21547;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;TKGC&#27169;&#22411;&#65292;&#21363;&#22522;&#20110;&#25552;&#31034;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PPT&#65289;&#12290;&#25105;&#20204;&#23558;&#19968;&#31995;&#21015;&#37319;&#26679;&#30340;&#22235;&#20803;&#32452;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#24182;&#23558;&#26102;&#38388;&#25139;&#20043;&#38388;&#30340;&#38388;&#38548;&#36716;&#25442;&#20026;&#19981;&#21516;&#30340;&#25552;&#31034;&#65292;&#20197;&#24418;&#25104;&#24102;&#26377;&#38544;&#21547;&#35821;&#20041;&#20449;&#24687;&#30340;&#36830;&#36143;&#21477;&#23376;&#12290;&#25105;&#20204;&#20351;&#29992;&#36974;&#30422;&#31574;&#30053;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#23558;TKGC&#20219;&#21153;&#36716;&#25442;&#20026;&#36974;&#30422;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#21644;&#24191;&#27867;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge graph completion (TKGC) is a crucial task that involves reasoning at known timestamps to complete the missing part of facts and has attracted more and more attention in recent years. Most existing methods focus on learning representations based on graph neural networks while inaccurately extracting information from timestamps and insufficiently utilizing the implied information in relations. To address these problems, we propose a novel TKGC model, namely Pre-trained Language Model with Prompts for TKGC (PPT). We convert a series of sampled quadruples into pre-trained language model inputs and convert intervals between timestamps into different prompts to make coherent sentences with implicit semantic information. We train our model with a masking strategy to convert TKGC task into a masked token prediction task, which can leverage the semantic information in pre-trained language models. Experiments on three benchmark datasets and extensive analysis demonstrate that 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07895</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;
&lt;/p&gt;
&lt;p&gt;
On the Hidden Mystery of OCR in Large Multimodal Models. (arXiv:2305.07895v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;&#29616;&#26377;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#34429;&#28982;&#22312;&#35821;&#20041;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#26377;&#38480;&#65292;&#23545;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26816;&#27979;&#33021;&#21147;&#20063;&#19981;&#36275;&#65292;&#19981;&#33021;&#19982;&#20256;&#32479;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#20173;&#38656;&#36827;&#19968;&#27493;&#25506;&#32034;&#23427;&#20204;&#22312;OCR&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#22823;&#22411;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#22810;&#27169;&#24577;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#25903;&#37197;&#24615;&#30340;&#35282;&#33394;&#12290;&#20851;&#20110;&#23427;&#20204;&#22312;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#20219;&#21153;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#20173;&#19981;&#22815;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#35782;&#21035;&#12289;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#35273;&#38382;&#31572;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20248;&#21155;&#21183;&#65292;&#23427;&#20204;&#20027;&#35201;&#20381;&#36182;&#20110;&#35821;&#20041;&#29702;&#35299;&#26469;&#35782;&#21035;&#21333;&#35789;&#65292;&#24182;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#23545;&#21333;&#20010;&#23383;&#31526;&#24418;&#29366;&#30340;&#24863;&#30693;&#12290;&#23427;&#20204;&#23545;&#25991;&#26412;&#38271;&#24230;&#28448;&#19981;&#20851;&#24515;&#65292;&#22312;&#26816;&#27979;&#22270;&#20687;&#30340;&#32454;&#31890;&#24230;&#29305;&#24449;&#26041;&#38754;&#20855;&#26377;&#26377;&#38480;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#24403;&#21069;&#26368;&#24378;&#22823;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20063;&#26080;&#27861;&#19982;&#20256;&#32479;&#25991;&#26412;&#20219;&#21153;&#30340;&#39046;&#22495;&#29305;&#23450;&#26041;&#27861;&#30456;&#21305;&#37197;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#20013;&#38754;&#20020;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#30340;&#22522;&#32447;&#32467;&#26524;&#25581;&#31034;&#20102;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;OCR&#30340;&#38544;&#31192;&#20043;&#35868;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large models have recently played a dominant role in natural language processing and multimodal vision-language learning. It remains less explored about their efficacy in text-related visual tasks. We conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, and key information extraction. Our findings reveal strengths and weaknesses in these models, which primarily rely on semantic understanding for word recognition and exhibit inferior perception of individual character shapes. They also display indifference towards text length and have limited capabilities in detecting fine-grained features in images. Consequently, these results demonstrate that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks. Most importantly, the baseline results showcased in this study
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07893</link><description>&lt;p&gt;
PESTS: &#27874;&#26031;&#35821;-&#33521;&#35821;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
PESTS: Persian_English Cross Lingual Corpus for Semantic Textual Similarity. (arXiv:2305.07893v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07893
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#27169;&#22411;PESTS&#65292;&#24182;&#36890;&#36807;&#27874;&#26031;&#35821;-&#33521;&#35821;&#30340;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#26469;&#39564;&#35777;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22791;&#21463;&#20851;&#27880;&#30340;&#32452;&#20214;&#12290;&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#35780;&#20272;&#21333;&#35789;&#12289;&#30701;&#35821;&#12289;&#27573;&#33853;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#24456;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#24230;&#37327;&#35201;&#27714;&#22312;&#28304;&#21644;&#30446;&#26631;&#35821;&#35328;&#20013;&#25552;&#20379;&#20855;&#26377;&#19968;&#23450;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#21477;&#23376;&#23545;&#12290;&#35768;&#22810;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#27169;&#22411;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#26469;&#24357;&#34917;&#36328;&#35821;&#35328;&#35821;&#26009;&#24211;&#19981;&#21487;&#29992;&#30340;&#19981;&#36275;&#65292;&#20294;&#26426;&#22120;&#32763;&#35793;&#30340;&#35823;&#24046;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24230;&#29305;&#24449;&#23454;&#29616;&#26426;&#22120;&#32763;&#35793;&#26102;&#65292;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the components of natural language processing that has received a lot of investigation recently is semantic textual similarity. In computational linguistics and natural language processing, assessing the semantic similarity of words, phrases, paragraphs, and texts is crucial. Calculating the degree of semantic resemblance between two textual pieces, paragraphs, or phrases provided in both monolingual and cross-lingual versions is known as semantic similarity. Cross lingual semantic similarity requires corpora in which there are sentence pairs in both the source and target languages with a degree of semantic similarity between them. Many existing cross lingual semantic similarity models use a machine translation due to the unavailability of cross lingual semantic similarity dataset, which the propagation of the machine translation error reduces the accuracy of the model. On the other hand, when we want to use semantic similarity features for machine translation the same machine t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#35758;&#23558;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24212;&#29992;&#24314;&#35758;&#65292;&#20197;&#21152;&#24378;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#24182;&#22686;&#24378;&#31038;&#20250;&#23545;&#20854;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.07882</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65306;&#23454;&#29616;&#8220;&#21463;&#20851;&#27880;&#30340;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#8221;&#26694;&#26550;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Dual Use Concerns of Generative AI and Large Language Models. (arXiv:2305.07882v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#35758;&#23558;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#24212;&#29992;&#24314;&#35758;&#65292;&#20197;&#21152;&#24378;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#24182;&#22686;&#24378;&#31038;&#20250;&#23545;&#20854;&#24433;&#21709;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#35758;&#23558;&#21407;&#26412;&#35774;&#35745;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#8220;&#21463;&#20851;&#27880;&#30340;&#21452;&#37325;&#20351;&#29992;&#30740;&#31350;&#8221;&#65288;DURC&#65289;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#20855;&#20307;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#36890;&#36807;&#22312;&#29983;&#29289;&#30740;&#31350;&#39046;&#22495;&#30340;&#20248;&#21183;&#21644;&#32570;&#28857;&#30340;&#35777;&#26126;&#65292;&#25105;&#20204;&#30456;&#20449;DURC&#26631;&#20934;&#21487;&#20197;&#20026;LLM&#37325;&#26032;&#23450;&#20041;&#65292;&#21487;&#33021;&#26377;&#21161;&#20110;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#27835;&#29702;&#12290;&#22312;&#20351;&#29992;DURC&#26694;&#26550;&#26102;&#38656;&#35201;&#26435;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#24378;&#35843;&#20854;&#22312;&#25552;&#39640;&#31038;&#20250;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#30340;&#35748;&#35782;&#26041;&#38754;&#30340;&#37325;&#35201;&#25919;&#27835;&#20316;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#19968;&#31995;&#21015;&#20855;&#20307;&#30340;&#24314;&#35758;&#65292;&#20197;&#23558;DURC&#26041;&#27861;&#24212;&#29992;&#20110;LLM&#30340;&#30740;&#31350;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest the implementation of the Dual Use Research of Concern (DURC) framework, originally designed for life sciences, to the domain of generative AI, with a specific focus on Large Language Models (LLMs). With its demonstrated advantages and drawbacks in biological research, we believe the DURC criteria can be effectively redefined for LLMs, potentially contributing to improved AI governance. Acknowledging the balance that must be struck when employing the DURC framework, we highlight its crucial political role in enhancing societal awareness of the impact of generative AI. As a final point, we offer a series of specific recommendations for applying the DURC approach to LLM research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#21644;&#22635;&#34917;&#31354;&#30333;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;GPT 4&#34920;&#29616;&#26368;&#20026;&#20248;&#24322;&#65292;&#36825;&#35828;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20415;&#20016;&#23500;&#25105;&#20204;&#23545;&#36807;&#21435;&#30340;&#29702;&#35299;&#21644;&#24357;&#21512;&#21382;&#21490;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07868</link><description>&lt;p&gt;
AI&#25216;&#26415;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#20013;&#30340;&#24212;&#29992;&#8212;&#8212;GPT 3.5&#12289;GPT4&#21644;GoogleBARD&#30340;&#27604;&#36739;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking. (arXiv:2305.07868v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#35780;&#20272;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#21644;&#22635;&#34917;&#31354;&#30333;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;GPT 4&#34920;&#29616;&#26368;&#20026;&#20248;&#24322;&#65292;&#36825;&#35828;&#26126;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;AI&#25216;&#26415;&#22312;&#21382;&#21490;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20415;&#20016;&#23500;&#25105;&#20204;&#23545;&#36807;&#21435;&#30340;&#29702;&#35299;&#21644;&#24357;&#21512;&#21382;&#21490;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#26102;&#20195;&#20449;&#24687;&#30340;&#36805;&#36895;&#25193;&#25955;&#20984;&#26174;&#20102;&#20934;&#30830;&#30340;&#21382;&#21490;&#35760;&#24405;&#21644;&#35299;&#37322;&#30340;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#20154;&#24037;&#26234;&#33021;&#24050;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#20854;&#22312;&#21382;&#21490;&#20107;&#23454;&#26680;&#26597;&#21644;&#22635;&#34917;&#31354;&#30333;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#19981;&#20026;&#20154;&#30693;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#65292;&#21363; GPT 3.5&#12289;GPT 4&#21644;GoogleBARD&#65292;&#23427;&#20204;&#33021;&#22815;&#26681;&#25454;&#32473;&#23450;&#25968;&#25454;&#39044;&#27979;&#21644;&#39564;&#35777;&#21382;&#21490;&#20107;&#20214;&#30340;&#34920;&#29616;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#19982;&#29616;&#23454;&#30340;&#36317;&#31163;&#65288;DTR&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#36755;&#20986;&#32467;&#26524;&#19982;&#24050;&#26377;&#30340;&#21382;&#21490;&#20107;&#23454;&#26159;&#21542;&#19968;&#33268;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;AI&#22312;&#21382;&#21490;&#30740;&#31350;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20854;&#20013;GPT 4&#34920;&#29616;&#26368;&#20026;&#20248;&#24322;&#12290;&#26412;&#25991;&#24378;&#35843;&#20102;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;AI&#22312;&#20016;&#23500;&#25105;&#20204;&#23545;&#36807;&#21435;&#30340;&#29702;&#35299;&#21644;&#24357;&#21512;&#21382;&#21490;&#30693;&#35782;&#24046;&#36317;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid proliferation of information in the digital era underscores the importance of accurate historical representation and interpretation. While artificial intelligence has shown promise in various fields, its potential for historical fact-checking and gap-filling remains largely untapped. This study evaluates the performance of three large language models LLMs GPT 3.5, GPT 4, and GoogleBARD in the context of predicting and verifying historical events based on given data. A novel metric, Distance to Reality (DTR), is introduced to assess the models' outputs against established historical facts. The results reveal a substantial potential for AI in historical studies, with GPT 4 demonstrating superior performance. This paper underscores the need for further research into AI's role in enriching our understanding of the past and bridging historical knowledge gaps.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#20998;&#26512;&#20102;&#19977;&#31181;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#20309;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#24448;&#24448;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#35328;&#23478;&#26063;&#26356;&#21152;&#25509;&#36817;&#65292;&#20294;&#23427;&#20204;&#20960;&#20046;&#21487;&#20197;&#34987;&#20174;&#20854;&#20182;&#35821;&#35328;&#23478;&#26063;&#30340;&#35821;&#35328;&#20998;&#31163;&#20986;&#26469;&#12290;&#21516;&#26102;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#26222;&#36941;&#19981;&#22914;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07839</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#20309;&#65306;&#20197;&#24179;&#31561;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
The Geometry of Multilingual Language Models: An Equality Lens. (arXiv:2305.07839v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#20998;&#26512;&#20102;&#19977;&#31181;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#20309;&#65292;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#24448;&#24448;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#35328;&#23478;&#26063;&#26356;&#21152;&#25509;&#36817;&#65292;&#20294;&#23427;&#20204;&#20960;&#20046;&#21487;&#20197;&#34987;&#20174;&#20854;&#20182;&#35821;&#35328;&#23478;&#26063;&#30340;&#35821;&#35328;&#20998;&#31163;&#20986;&#26469;&#12290;&#21516;&#26102;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#26222;&#36941;&#19981;&#22914;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#31034;&#23545;&#20110;&#29702;&#35299;&#23427;&#20204;&#30340;&#36328;&#35821;&#35328;&#23646;&#24615;&#12289;&#39044;&#27979;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20197;&#21450;&#35782;&#21035;&#21508;&#31181;&#35821;&#35328;&#20043;&#38388;&#30340;&#20219;&#20309;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#20998;&#26512;&#20102;&#19977;&#31181;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#20960;&#20309;&#65292;&#21457;&#29616;&#25152;&#26377;&#35821;&#35328;&#37117;&#30001;&#21807;&#19968;&#30340;&#20960;&#20309;&#24418;&#29366;&#34920;&#31034;&#12290;&#36890;&#36807;&#20351;&#29992;&#20960;&#20309;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#23613;&#31649;&#35821;&#35328;&#24448;&#24448;&#26681;&#25454;&#23427;&#20204;&#30340;&#35821;&#35328;&#23478;&#26063;&#26356;&#21152;&#25509;&#36817;&#65292;&#20294;&#26159;&#23427;&#20204;&#20960;&#20046;&#21487;&#20197;&#34987;&#20174;&#20854;&#20182;&#35821;&#35328;&#23478;&#26063;&#30340;&#35821;&#35328;&#20998;&#31163;&#20986;&#26469;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#36328;&#35821;&#35328;&#30456;&#20284;&#24230;&#25351;&#25968;&#26469;&#27979;&#37327;&#35821;&#20041;&#31354;&#38388;&#20013;&#35821;&#35328;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20219;&#20309;&#27169;&#22411;&#20013;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#31034;&#19981;&#22914;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#31034;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the representations of different languages in multilingual language models is essential for comprehending their cross-lingual properties, predicting their performance on downstream tasks, and identifying any biases across languages. In our study, we analyze the geometry of three multilingual language models in Euclidean space and find that all languages are represented by unique geometries. Using a geometric separability index we find that although languages tend to be closer according to their linguistic family, they are almost separable with languages from other families. We also introduce a Cross-Lingual Similarity Index to measure the distance of languages with each other in the semantic space. Our findings indicate that the low-resource languages are not represented as good as high resource languages in any of the models
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35789;&#39057;&#23545;&#32500;&#24230;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#31215;&#36317;&#31163;&#30340;&#38745;&#24577;&#35789;&#21521;&#37327;&#32500;&#24230;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35789;&#39057;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07826</link><description>&lt;p&gt;
&#22522;&#20110;&#28151;&#21512;&#31215;&#36317;&#31163;&#30340;&#38745;&#24577;&#35789;&#21521;&#37327;&#39057;&#29575;&#24863;&#30693;&#30340;&#32500;&#24230;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Frequency-aware Dimension Selection for Static Word Embedding by Mixed Product Distance. (arXiv:2305.07826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35789;&#39057;&#23545;&#32500;&#24230;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28151;&#21512;&#31215;&#36317;&#31163;&#30340;&#38745;&#24577;&#35789;&#21521;&#37327;&#32500;&#24230;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#38477;&#20302;&#35789;&#39057;&#30340;&#24433;&#21709;&#65292;&#23454;&#29616;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26080;&#27861;&#20351;&#29992;&#19978;&#19979;&#25991;&#30340;&#20219;&#21153;&#65292;&#38745;&#24577;&#35789;&#21521;&#37327;&#20173;&#28982;&#26377;&#29992;&#65292;&#22240;&#20026;&#22312;&#27809;&#26377;&#19978;&#19979;&#25991;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#27604;&#38745;&#24577;&#35789;&#21521;&#37327;&#34920;&#29616;&#24471;&#26356;&#24046;&#12290;&#23613;&#31649;&#32500;&#24230;&#26159;&#30830;&#23450;&#38745;&#24577;&#35789;&#21521;&#37327;&#36136;&#37327;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#20294;&#33258;&#21160;&#32500;&#24230;&#36873;&#25321;&#24456;&#23569;&#34987;&#35752;&#35770;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35789;&#39057;&#23545;&#32500;&#24230;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#24182;&#32463;&#39564;&#35777;&#26126;&#20102;&#35789;&#39057;&#26159;&#38750;&#24120;&#20851;&#38190;&#30340;&#65292;&#38656;&#35201;&#22312;&#32500;&#24230;&#36873;&#25321;&#26102;&#36827;&#34892;&#32771;&#37327;&#12290;&#22522;&#20110;&#36825;&#26679;&#30340;&#23454;&#35777;&#21457;&#29616;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32500;&#24230;&#36873;&#25321;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#31181;&#24230;&#37327;&#65288;&#28151;&#21512;&#31215;&#36317;&#31163;&#65292;MPD&#65289;&#26469;&#20026;&#23383;&#23884;&#20837;&#31639;&#27861;&#36873;&#25321;&#36866;&#24403;&#30340;&#32500;&#24230;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#20219;&#20309;&#35789;&#21521;&#37327;&#12290;&#36890;&#36807;&#23545;&#39044;&#27979;&#30697;&#38453;&#24212;&#29992;&#21518;&#22788;&#29702;&#20989;&#25968;&#65292;&#22522;&#20110;MPD&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#35789;&#39057;&#30340;&#24433;&#21709;&#12290;&#38024;&#23545;&#26080;&#19978;&#19979;&#25991;&#21644;&#26377;&#19978;&#19979;&#25991;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#35789;&#21521;&#37327;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Static word embedding is still useful, particularly for context-unavailable tasks, because in the case of no context available, pre-trained language models often perform worse than static word embeddings. Although dimension is a key factor determining the quality of static word embeddings, automatic dimension selection is rarely discussed. In this paper, we investigate the impact of word frequency on the dimension selection, and empirically find that word frequency is so vital that it needs to be taken into account during dimension selection. Based on such an empirical finding, this paper proposes a dimension selection method that uses a metric (Mixed Product Distance, MPD) to select a proper dimension for word embedding algorithms without training any word embedding. Through applying a post-processing function to oracle matrices, the MPD-based method can de-emphasize the impact of word frequency. Experiments on both context-unavailable and context-available tasks demonstrate the bette
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RepAL &#30340;&#31616;&#21333;&#26131;&#29992;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#20943;&#24369;&#21477;&#23376;&#23884;&#20837;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;&#21644;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#26080;&#39035;&#35757;&#32451;&#65292;&#21487;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.07824</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#26131;&#29992;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement. (arXiv:2305.07824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; RepAL &#30340;&#31616;&#21333;&#26131;&#29992;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;&#12290;&#36890;&#36807;&#20943;&#24369;&#21477;&#23376;&#23884;&#20837;&#20013;&#30340;&#20887;&#20313;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;&#21644;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;&#35813;&#26041;&#27861;&#26080;&#39035;&#35757;&#32451;&#65292;&#21487;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#29983;&#25104;&#21477;&#23376;&#30340;&#23884;&#20837;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35821;&#20041;&#21305;&#37197;&#21644;&#26816;&#32034;&#38382;&#39064;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; Representation ALchemy (RepAL)&#65292;&#36825;&#26159;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#29992;&#20110;&#22686;&#24378;&#21477;&#23376;&#34920;&#31034;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#12290;RepAL &#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#20943;&#24369;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21477;&#23376;&#23884;&#20837;&#20013;&#20887;&#20313;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; RepAL &#26159;&#19968;&#31181;&#19981;&#38656;&#35201;&#35757;&#32451;&#19988;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21477;&#23376;&#23398;&#20064;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#20197;&#29702;&#35299; RepAL&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating proper embedding of sentences through an unsupervised way is beneficial to semantic matching and retrieval problems in real-world scenarios. This paper presents Representation ALchemy (RepAL), an extremely simple post-processing method that enhances sentence representations. The basic idea in RepAL is to de-emphasize redundant information of sentence embedding generated by pre-trained models. Through comprehensive experiments, we show that RepAL is free of training and is a plug-and-play method that can be combined with most existing unsupervised sentence learning models. We also conducted in-depth analysis to understand RepAL.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.07804</link><description>&lt;p&gt;
Dr. LLaMA&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25913;&#21892;&#29305;&#23450;&#39046;&#22495;QA&#20013;&#30340;&#23567;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Dr. LLaMA: Improving Small Language Models in Domain-Specific QA via Generative Data Augmentation. (arXiv:2305.07804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07804
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#24494;&#35843;&#21518;&#20351;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#38543;&#30528;&#20854;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20063;&#38754;&#20020;&#30528;&#35745;&#31639;&#24320;&#38144;&#21644;&#25928;&#29575;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30001;&#20110;&#23481;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Dr. LLaMA&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#65292;&#32858;&#28966;&#21307;&#23398;&#38382;&#31572;&#20219;&#21153;&#21644;PubMedQA&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#23567;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LLM&#26377;&#25928;&#22320;&#32454;&#21270;&#21644;&#25193;&#23637;&#29616;&#26377;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#22312;&#24494;&#35843;&#21518;&#65292;&#20351;&#24471;&#23567;&#22411;&#27169;&#22411;&#22312;&#29305;&#23450;&#39046;&#22495;QA&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#29305;&#23450;&#39046;&#22495;&#38382;&#31572;&#20219;&#21153;&#20013;&#20351;&#29992;LLM&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#26368;&#32456;&#26088;&#22312;&#20026;&#19987;&#19994;&#24212;&#29992;&#21019;&#24314;&#26356;&#39640;&#25928;&#21644;&#33021;&#21147;&#26356;&#24378;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant strides in natural language processing but face challenges in terms of computational expense and inefficiency as they grow in size, especially in domain-specific tasks. Small Language Models (SLMs), on the other hand, often struggle in these tasks due to limited capacity and training data. In this paper, we introduce Dr. LLaMA, a method for improving SLMs through generative data augmentation using LLMs, focusing on medical question-answering tasks and the PubMedQA dataset. Our findings indicate that LLMs effectively refine and diversify existing question-answer pairs, resulting in improved performance of a much smaller model on domain-specific QA datasets after fine-tuning. This study highlights the challenges of using LLMs for domain-specific question answering and suggests potential research directions to address these limitations, ultimately aiming to create more efficient and capable models for specialized applications. We have als
&lt;/p&gt;</description></item><item><title>ACCENT&#26159;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20107;&#20214;-&#20851;&#31995;&#20803;&#32452;&#19982;CSKB&#30340;&#20860;&#23481;&#24615;&#35780;&#20272;&#21709;&#24212;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20215;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.07797</link><description>&lt;p&gt;
ACCENT:&#19968;&#31181;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#33258;&#21160;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
ACCENT: An Automatic Event Commonsense Evaluation Metric for Open-Domain Dialogue Systems. (arXiv:2305.07797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07797
&lt;/p&gt;
&lt;p&gt;
ACCENT&#26159;&#19968;&#31181;&#22522;&#20110;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20107;&#20214;-&#20851;&#31995;&#20803;&#32452;&#19982;CSKB&#30340;&#20860;&#23481;&#24615;&#35780;&#20272;&#21709;&#24212;&#65292;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#35780;&#20215;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24120;&#35782;&#25512;&#29702;&#22312;&#20154;&#31867;&#20132;&#27969;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#22240;&#27492;&#23545;&#20110;&#24320;&#25918;&#39046;&#22495;&#30340;&#23545;&#35805;&#31995;&#32479;&#26469;&#35828;&#26159;&#19968;&#20010;&#37325;&#35201;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#35780;&#20272;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#24120;&#35782;&#25512;&#29702;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20851;&#27880;&#20107;&#20214;&#24120;&#35782;&#65292;&#23427;&#32771;&#34385;&#20107;&#20214;&#21450;&#20854;&#20851;&#31995;&#65292;&#22312;&#23545;&#35805;&#21644;&#19968;&#33324;&#24120;&#35782;&#25512;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;ACCENT&#65292;&#19968;&#31181;&#21463;&#24120;&#35782;&#30693;&#35782;&#24211; (CSKBs) &#25480;&#26435;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#25351;&#26631;&#12290;ACCENT&#39318;&#20808;&#20174;&#23545;&#35805;&#20013;&#25552;&#21462;&#20107;&#20214;-&#20851;&#31995;&#20803;&#32452;&#65292;&#28982;&#21518;&#36890;&#36807;&#35745;&#31639;&#23427;&#20204;&#19982;CSKB&#30340;&#20860;&#23481;&#24615;&#26469;&#35780;&#20272;&#21709;&#24212;&#12290;&#20026;&#20102;&#35780;&#20272;ACCENT&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#24320;&#25918;&#22495;&#23545;&#35805;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20215;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ACCENT&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20107;&#20214;&#24120;&#35782;&#35780;&#20272;&#25351;&#26631;&#65292;&#27604;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Commonsense reasoning is omnipresent in human communications and thus is an important feature for open-domain dialogue systems. However, evaluating commonsense in dialogue systems is still an open challenge. We take the first step by focusing on event commonsense that considers events and their relations, and is crucial in both dialogues and general commonsense reasoning. We propose ACCENT, an event commonsense evaluation metric empowered by commonsense knowledge bases (CSKBs). ACCENT first extracts event-relation tuples from a dialogue, and then evaluates the response by scoring the tuples in terms of their compatibility with the CSKB. To evaluate ACCENT, we construct the first public event commonsense evaluation dataset for open-domain dialogues. Our experiments show that ACCENT is an efficient metric for event commonsense evaluation, which achieves higher correlations with human judgments than existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KLDivS&#21644;JSDivS&#36825;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#21487;&#35299;&#37322;&#22320;&#35780;&#20272;MLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.07795</link><description>&lt;p&gt;
&#26500;&#24314;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#31038;&#20250;&#20559;&#35265;&#30340;&#25972;&#20307;&#35780;&#20272;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Constructing Holistic Measures for Social Biases in Masked Language Models. (arXiv:2305.07795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KLDivS&#21644;JSDivS&#36825;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#65292;&#23558;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#21487;&#20197;&#26356;&#31283;&#23450;&#12289;&#21487;&#35299;&#37322;&#22320;&#35780;&#20272;MLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#65292;MLMs &#24456;&#21487;&#33021;&#21453;&#26144;&#29616;&#23454;&#20013;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#12290;&#36807;&#21435;&#25552;&#20986;&#30340;&#22823;&#22810;&#25968;&#35780;&#20272;&#25351;&#26631;&#37319;&#29992;&#19981;&#21516;&#30340;&#25513;&#30721;&#31574;&#30053;&#65292;&#35774;&#35745;&#20102;MLMs &#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#12290;&#36825;&#20123;&#25351;&#26631;&#32570;&#20047;&#23545;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#21464;&#21270;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#23558;MLMs&#36755;&#20986;&#30340;&#21051;&#26495;&#21360;&#35937;&#21644;&#21453;&#21051;&#26495;&#21360;&#35937;&#26679;&#26412;&#30340;&#23545;&#25968;&#20284;&#28982;&#20989;&#25968;&#35270;&#20026;&#39640;&#26031;&#20998;&#24067;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#35780;&#20272;&#25351;&#26631;&#8212;&#8212;Kullback Leibler &#25955;&#24230;&#24471;&#20998;&#65288;KLDivS&#65289;&#21644;Jensen Shannon &#36317;&#31163;&#24471;&#20998;&#65288;JSDivS&#65289;&#65292;&#20197;&#35780;&#20272;MLMs&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;StereoSet &#21644;CrowS-Pairs&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#36807;&#21435;&#25552;&#20986;&#30340;&#25351;&#26631;&#30456;&#27604;&#65292;KLDivS&#21644;JSDivS&#26356;&#21152;&#31283;&#23450;&#21644;&#21487;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Models (MLMs) have been successful in many natural language processing tasks. However, real-world stereotype biases are likely to be reflected in MLMs due to their learning from large text corpora. Most of the evaluation metrics proposed in the past adopt different masking strategies, designed with the log-likelihood of MLMs. They lack holistic considerations such as variance for stereotype bias and anti-stereotype bias samples. In this paper, the log-likelihoods of stereotype bias and anti-stereotype bias samples output by MLMs are considered Gaussian distributions. Two evaluation metrics, Kullback Leibler Divergence Score (KLDivS) and Jensen Shannon Divergence Score (JSDivS) are proposed to evaluate social biases in MLMs The experimental results on the public datasets StereoSet and CrowS-Pairs demonstrate that KLDivS and JSDivS are more stable and interpretable compared to the metrics proposed in the past.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07789</link><description>&lt;p&gt;
&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#19982;&#25191;&#34892;&#31572;&#26696;&#22797;&#26434;&#38382;&#39064;&#30340;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Answering Complex Questions over Text by Hybrid Question Parsing and Execution. (arXiv:2305.07789v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07789
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#38382;&#39064;&#35299;&#26512;&#21644;&#25191;&#34892;&#26694;&#26550;&#65292;&#22312;&#25991;&#23383;&#38382;&#31572;&#31995;&#32479;&#20013;&#23454;&#29616;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#36890;&#36807;&#35299;&#26512;&#38382;&#39064;&#20026;H&#34920;&#36798;&#24335;&#24182;&#35774;&#35745;&#28151;&#21512;&#25191;&#34892;&#22120;&#23454;&#29616;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#21644;&#25928;&#29575;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#30340;&#20027;&#23548;&#27169;&#24335;&#26159;&#22522;&#20110;&#31471;&#21040;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#22312;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#20294;&#22312;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#36825;&#19982;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#30693;&#35782;&#22270;&#35889;&#65289;&#19978;&#24191;&#27867;&#36866;&#24212;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#36716;&#25442;&#20026;&#36923;&#36753;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#26597;&#35810;&#24341;&#25806;&#36827;&#34892;&#25191;&#34892;&#12290;&#20026;&#20102;&#32467;&#21512;&#31070;&#32463;&#21644;&#31526;&#21495;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25991;&#26412;&#38382;&#31572;&#31995;&#32479;&#20013;&#36827;&#34892;&#35299;&#26512;&#21644;&#25191;&#34892;&#38382;&#39064;&#30340;&#26694;&#26550;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#20013;&#24515;&#25903;&#26609;&#65306;&#65288;1&#65289;&#25105;&#20204;&#23558;&#21508;&#31181;&#22797;&#26434;&#38382;&#39064;&#35299;&#26512;&#25104;&#20013;&#38388;&#34920;&#31034;&#65292;&#31216;&#20026;H&#34920;&#36798;&#24335;&#65292;&#23427;&#30001;&#31616;&#21333;&#38382;&#39064;&#32452;&#25104;&#21407;&#35821;&#21644;&#34920;&#31034;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#31526;&#21495;&#25805;&#20316;&#32452;&#25104;&#65307;&#65288;2&#65289;&#20026;&#20102;&#25191;&#34892;&#20135;&#29983;&#30340;H&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#28151;&#21512;&#25191;&#34892;&#22120;&#65292;&#23427;&#38598;&#25104;&#20102;&#30830;&#23450;&#35268;&#21017;&#26469;&#32763;&#35793;&#31526;&#21495;&#25805;&#20316;&#65292;&#19982;&#22788;&#29702;&#21407;&#22987;&#38382;&#39064;&#30340;&#25554;&#20837;&#31070;&#32463;&#27169;&#22359;&#12290;&#25105;&#20204;&#22312;&#21253;&#21547;&#22797;&#26434;&#38382;&#39064;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#25351;&#26631;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominant paradigm of textual question answering systems is based on end-to-end neural networks, which excels at answering natural language questions but falls short on complex ones. This stands in contrast to the broad adaptation of semantic parsing approaches over structured data sources (e.g., relational database, knowledge graphs), that convert natural language questions to logical forms and execute them with query engines. Towards combining the strengths of neural and symbolic methods, we propose a framework of question parsing and execution on textual QA. It comprises two central pillars: (1) We parse the question of varying complexity into an intermediate representation, named H-expression, which is composed of simple questions as the primitives and symbolic operations representing the relationships among them; (2) To execute the resulting H-expressions, we design a hybrid executor, which integrates the deterministic rules to translate the symbolic operations with a drop-in n
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;NL&#21040;TL&#30340;&#36716;&#25442;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#21487;&#20197;&#20934;&#30830;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#22320;&#36716;&#25442;&#22797;&#26434;&#30340;&#39640;&#32423;&#31995;&#32479;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2305.07766</link><description>&lt;p&gt;
NL2TL&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#26102;&#24577;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models. (arXiv:2305.07766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;NL&#21040;TL&#30340;&#36716;&#25442;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#21487;&#20197;&#20934;&#30830;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#22320;&#36716;&#25442;&#22797;&#26434;&#30340;&#39640;&#32423;&#31995;&#32479;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#36923;&#36753;&#65288;TL&#65289;&#21487;&#29992;&#20110;&#22312;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#20005;&#26684;&#25351;&#23450;&#22797;&#26434;&#30340;&#39640;&#32423;&#31995;&#32479;&#35268;&#33539;&#12290;&#30001;&#20110;&#32570;&#20047;&#36328;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#21644;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21644;TL&#20043;&#38388;&#30340;&#36716;&#25442;&#19968;&#30452;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#33521;&#25991;&#25351;&#20196;&#20174;NL&#21040;TL&#30340;&#36716;&#25442;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#22810;&#20010;&#38454;&#27573;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;NL-TL&#23545;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;LLMs&#21644;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#20855;&#26377;28K&#20010;NL-TL&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;NL&#21644;TL&#30340;&#25552;&#21319;&#29256;&#26412;&#19978;&#24494;&#35843;&#20102;T5&#27169;&#22411;&#65288;&#21363;&#65292;&#29305;&#23450;&#21407;&#23376;&#21629;&#39064;&#65288;AP&#65289;&#34987;&#38544;&#34255;&#65289;&#12290;&#22686;&#24378;&#30340;&#26222;&#36866;&#24615;&#28304;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#20351;&#29992;&#25552;&#21319;&#30340;NL-TL&#34920;&#24449;&#24120;&#35265;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#27809;&#26377;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#12290;2&#65289;&#22312;&#25968;&#25454;&#38598;&#21019;&#24314;&#20013;&#24212;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation 
&lt;/p&gt;</description></item><item><title>KALMRA&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#23427;&#25193;&#23637;&#20102;&#33521;&#35821;&#21629;&#20196;&#30340;&#21151;&#33021;&#65292;&#20197;&#20801;&#35768;&#22312;KALM&#20013;&#36827;&#34892;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.07763</link><description>&lt;p&gt;
&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#30693;&#35782;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Knowledge Authoring for Rules and Actions. (arXiv:2305.07763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07763
&lt;/p&gt;
&lt;p&gt;
KALMRA&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#23427;&#25193;&#23637;&#20102;&#33521;&#35821;&#21629;&#20196;&#30340;&#21151;&#33021;&#65292;&#20197;&#20801;&#35768;&#22312;KALM&#20013;&#36827;&#34892;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#65292;&#24182;&#36890;&#36807;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#26174;&#31034;&#20986;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#31995;&#32479;&#20197;&#20107;&#23454;&#21644;&#35268;&#21017;&#30340;&#24418;&#24335;&#25551;&#36848;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#27010;&#24565;&#21644;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#37096;&#32626;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#31995;&#32479;&#26102;&#65292;&#39046;&#22495;&#19987;&#23478;&#24448;&#24448;&#24456;&#38590;&#26500;&#24314;&#27491;&#30830;&#30340;&#36923;&#36753;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KALMRA&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#20197;&#23454;&#29616;&#22312;KALM&#20013;&#36827;&#34892;&#35268;&#21017;&#21644;&#25805;&#20316;&#30340;&#21019;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge representation and reasoning (KRR) systems describe and reason with complex concepts and relations in the form of facts and rules. Unfortunately, wide deployment of KRR systems runs into the problem that domain experts have great difficulty constructing correct logical representations of their domain knowledge. Knowledge engineers can help with this construction process, but there is a deficit of such specialists. The earlier Knowledge Authoring Logic Machine (KALM) based on Controlled Natural Language (CNL) was shown to have very high accuracy for authoring facts and questions. More recently, KALMFL, a successor of KALM, replaced CNL with factual English, which is much less restrictive and requires very little training from users. However, KALMFL has limitations in representing certain types of knowledge, such as authoring rules for multi-step reasoning or understanding actions with timestamps. To address these limitations, we propose KALMRA to enable authoring of rules and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2305.07759</link><description>&lt;p&gt;
TinyStories: &#35821;&#35328;&#27169;&#22411;&#33021;&#31616;&#23567;&#21040;&#20160;&#20040;&#31243;&#24230;&#21364;&#20381;&#28982;&#33021;&#22815;&#35762;&#36848;&#36830;&#36143;&#30340;&#33521;&#25991;&#25925;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#23567;&#22411;&#21270;&#26102;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; TinyStories &#30340;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35268;&#27169;&#23567;&#12289;&#22797;&#26434;&#24230;&#20302;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#30701;&#25925;&#20107;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;&#21361;&#38505;&#23398;&#29983;&#22238;&#22797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07709</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#27979;&#21361;&#38505;&#30340;&#23398;&#29983;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Using Language Models to Detect Alarming Student Responses. (arXiv:2305.07709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#35782;&#21035;&#21361;&#38505;&#23398;&#29983;&#22238;&#22797;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#37319;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#35782;&#21035;&#21361;&#38505;&#23398;&#29983;&#22238;&#22797;&#30340;&#31995;&#32479;&#30340;&#36827;&#23637;&#12290;&#35813;&#31995;&#32479;&#38598;&#25104;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#24179;&#21488;&#20013;&#65292;&#29992;&#20110;&#35780;&#20272;&#23398;&#29983;&#30340;&#22238;&#22797;&#26159;&#21542;&#34920;&#26126;&#20182;&#20204;&#23545;&#33258;&#24049;&#25110;&#20182;&#20154;&#26500;&#25104;&#23041;&#32961;&#12290;&#36825;&#20123;&#22238;&#22797;&#21487;&#33021;&#21253;&#25324;&#20851;&#20110;&#26292;&#21147;&#23041;&#32961;&#12289;&#20005;&#37325;&#25233;&#37057;&#12289;&#33258;&#26432;&#39118;&#38505;&#21644;&#34384;&#24453;&#25551;&#36848;&#30340;&#32454;&#33410;&#12290;&#26368;&#26032;&#27169;&#22411;&#26159;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;&#30001;&#23398;&#29983;&#22238;&#22797;&#21644;&#34917;&#20805;&#25991;&#26412;&#26500;&#25104;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#32780;&#25104;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#27604;&#27492;&#21069;&#29256;&#26412;&#30340;&#31995;&#32479;&#33021;&#22815;&#22823;&#24133;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article details the advances made to a system that uses artificial intelligence to identify alarming student responses. This system is built into our assessment platform to assess whether a student's response indicates they are a threat to themselves or others. Such responses may include details concerning threats of violence, severe depression, suicide risks, and descriptions of abuse. Driven by advances in natural language processing, the latest model is a fine-tuned language model trained on a large corpus consisting of student responses and supplementary texts. We demonstrate that the use of a language model delivers a substantial improvement in accuracy over the previous iterations of this system.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.07677</link><description>&lt;p&gt;
Masked Audio Text Encoders &#22312;&#22810;&#27169;&#24577;&#37325;&#25171;&#20998;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#20108;&#27425;&#25171;&#20998;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Masked Audio Text Encoder&#65288;MATE&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;&#21508;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#37325;&#26032;&#25171;&#20998;&#22120;&#23545;ASR&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#24456;&#26377;&#22909;&#22788;&#12290;&#19982;&#20165;&#25991;&#26412;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#22495;&#20869;&#25968;&#25454;&#32452;&#19978;&#65292;MATE &#21487;&#20197;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;4&#65285;-16&#65285;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#32452;&#19978;&#21487;&#23558;WER&#38477;&#20302;3&#65285;-7&#65285;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;0.8&#23567;&#26102;&#65289;&#65292;MATE&#23601;&#21487;&#20197;&#23558;WER&#27604;&#19968;&#27425;&#25171;&#20998;&#30340;&#22522;&#32447;&#38477;&#20302;8&#65285;-23&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#26426;&#22120;&#19982;&#20154;&#31867;&#20799;&#31461;&#22312;&#27169;&#20223;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#19981;&#21516;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#38656;&#35201;&#26356;&#22810;&#30340;&#20449;&#24687;&#25165;&#33021;&#36798;&#21040;&#23401;&#23376;&#25152;&#33021;&#20570;&#21040;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.07666</link><description>&lt;p&gt;
&#27169;&#20223;&#19982;&#21019;&#26032;&#65306;&#23401;&#23376;&#20204;&#33021;&#20570;&#21040;&#30340;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#23578;&#19981;&#33021;&#20570;&#21040;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?. (arXiv:2305.07666v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#26426;&#22120;&#19982;&#20154;&#31867;&#20799;&#31461;&#22312;&#27169;&#20223;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#19981;&#21516;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#38656;&#35201;&#26356;&#22810;&#30340;&#20449;&#24687;&#25165;&#33021;&#36798;&#21040;&#23401;&#23376;&#25152;&#33021;&#20570;&#21040;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#35328;&#35270;&#35273;&#27169;&#22411;&#26159;&#21542;&#26159;&#26234;&#33021;&#20307;&#19968;&#30452;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#35270;&#35282;&#65292;&#35748;&#20026;&#36825;&#20123;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#26159;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#22686;&#24378;&#25991;&#21270;&#20256;&#25773;&#30340;&#25991;&#21270;&#25216;&#26415;&#65292;&#26159;&#39640;&#25928;&#30340;&#27169;&#20223;&#24341;&#25806;&#12290;&#36890;&#36807;&#35780;&#20272;AI&#27169;&#22411;&#35774;&#35745;&#26032;&#24037;&#20855;&#21644;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;AI&#27169;&#22411;&#23545;&#27169;&#20223;&#21644;&#21019;&#26032;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#20154;&#31867;&#20799;&#31461;&#30340;&#21453;&#24212;&#36827;&#34892;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#30830;&#23450;&#20174;&#29305;&#23450;&#30340;&#23398;&#20064;&#25216;&#26415;&#21644;&#25968;&#25454;&#20013;&#21487;&#25512;&#23548;&#20986;&#21738;&#20123;&#29305;&#23450;&#34920;&#31034;&#21644;&#33021;&#21147;&#65292;&#20197;&#21450;&#21738;&#20123;&#30693;&#35782;&#25110;&#25216;&#33021;&#30340;&#31532;&#19968;&#27493;&#12290;&#20851;&#38190;&#22312;&#20110;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26426;&#22120;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#21644;&#22270;&#20687;&#65292;&#25165;&#33021;&#36798;&#21040;&#19968;&#20010;&#23401;&#23376;&#25152;&#33021;&#20570;&#21040;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. We argue that these artificial intelligence models are cultural technologies that enhance cultural transmission in the modern world, and are efficient imitation engines. We explore what AI models can tell us about imitation and innovation by evaluating their capacity to design new tools and discover novel causal structures, and contrast their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. Critically, our findings suggest that machines may need more than large scale language and images to achieve what a child can do.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.07375</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#26029;&#22120;&#21527;&#65311;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#65292;&#23454;&#39564;&#35777;&#26126;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#65292;&#20294;&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#23384;&#22312;&#20005;&#37325;&#30340;&#22240;&#26524;&#24187;&#35273;&#38382;&#39064;&#65292;&#23545;&#20110;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#20247;&#22810;NLP&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26032;&#20852;&#33021;&#21147;&#65292;&#20294;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#30340;&#34920;&#29616;&#22914;&#20309;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#23545;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#39318;&#27425;&#20840;&#38754;&#35780;&#20272;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ChatGPT&#19981;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#25512;&#29702;&#32773;&#65292;&#20294;&#26159;&#26159;&#19968;&#20010;&#22909;&#30340;&#22240;&#26524;&#35299;&#37322;&#32773;&#12290;&#27492;&#22806;&#65292;ChatGPT&#22312;&#22240;&#26524;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#20005;&#37325;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#22240;&#26524;&#20851;&#31995;&#21644;&#38750;&#22240;&#26524;&#20851;&#31995;&#30340;&#25253;&#21578;&#20559;&#35265;&#65292;&#20197;&#21450;ChatGPT&#30340;&#21319;&#32423;&#36807;&#31243;&#65292;&#22914;RLHF&#12290;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;COT&#65289;&#25216;&#26415;&#26041;&#38754;&#65292;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#36825;&#31181;&#22240;&#26524;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#22240;&#26524;&#25512;&#29702;&#33021;&#21147;&#23545;&#20110;&#22312;&#25552;&#31034;&#20013;&#34920;&#36798;&#22240;&#26524;&#27010;&#24565;&#30340;&#35789;&#35821;&#38750;&#24120;&#25935;&#24863;&#65292;&#24182;&#19988;&#23553;&#38381;&#25552;&#31034;&#27604;&#24320;&#25918;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#12290;&#23545;&#20110;&#21477;&#23376;&#20013;&#30340;&#20107;&#20214;&#65292;ChatGPT&#25797;&#38271;&#25429;&#25417;&#26126;&#30830;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning ability is crucial for numerous NLP applications. Despite the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear how well ChatGPT performs in causal reasoning. In this paper, we conduct the first comprehensive evaluation of the ChatGPT's causal reasoning capabilities. Experiments show that ChatGPT is not a good causal reasoner, but a good causal interpreter. Besides, ChatGPT has a serious hallucination on causal reasoning, possibly due to the reporting biases between causal and non-causal relationships in natural language, as well as ChatGPT's upgrading processes, such as RLHF. The In-Context Learning (ICL) and Chain-of-Though (COT) techniques can further exacerbate such causal hallucination. Additionally, the causal reasoning ability of ChatGPT is sensitive to the words used to express the causal concept in prompts, and close-ended prompts perform better than open-ended prompts. For events in sentences, ChatGPT excels at capturing explicit caus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#35789;&#27719;&#21305;&#37197;&#20316;&#20026;&#35780;&#20272;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#26377;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#38646;-shot&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.06984</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#35780;&#20272;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Evaluating Open-Domain Question Answering in the Era of Large Language Models. (arXiv:2305.06984v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#35789;&#27719;&#21305;&#37197;&#20316;&#20026;&#35780;&#20272;&#26041;&#27861;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#20316;&#29992;&#26377;&#38480;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#20013;&#19968;&#20010;&#38646;-shot&#27169;&#22411;&#30340;&#24615;&#33021;&#22823;&#24133;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#21305;&#37197;&#20173;&#26159;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#65288;QA&#65289;&#30340;&#20107;&#23454;&#35780;&#20272;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#19968;&#20010;&#21512;&#29702;&#30340;&#20505;&#36873;&#31572;&#26696;&#26410;&#20986;&#29616;&#22312;&#37329;&#26631;&#20934;&#31572;&#26696;&#21015;&#34920;&#20013;&#26102;&#65292;&#35789;&#27719;&#21305;&#37197;&#23436;&#20840;&#22833;&#36133;&#65292;&#38543;&#30528;&#25105;&#20204;&#20174;&#25277;&#21462;&#27169;&#22411;&#36716;&#21521;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#31181;&#24773;&#20917;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;QA&#20013;&#30340;&#26368;&#36817;&#25104;&#21151;&#21152;&#21095;&#20102;&#35789;&#27719;&#21305;&#37197;&#30340;&#22833;&#36133;&#65292;&#22240;&#20026;&#20505;&#36873;&#31572;&#26696;&#21464;&#24471;&#26356;&#38271;&#65292;&#22240;&#27492;&#19982;&#37329;&#26631;&#20934;&#31572;&#26696;&#30340;&#21305;&#37197;&#21464;&#24471;&#26356;&#21152;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32570;&#20047;&#20934;&#30830;&#30340;&#35780;&#20272;&#65292;&#24320;&#25918;&#39046;&#22495;QA&#30340;&#30495;&#27491;&#36827;&#23637;&#20173;&#28982;&#26410;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;NQ-open&#30340;&#19968;&#20010;&#23376;&#38598;&#19978;&#25163;&#21160;&#35780;&#20272;&#21508;&#31181;&#24320;&#25918;&#39046;&#22495;QA&#27169;&#22411;&#65288;&#21253;&#25324;LLMs&#65289;&#30340;&#31572;&#26696;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#25581;&#31034;&#65292;&#23613;&#31649;&#25152;&#26377;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#33021;&#34987;&#26174;&#30528;&#20302;&#20272;&#65292;&#20294;InstructGPT&#65288;&#38646;-shot&#65289;LLM&#30340;&#24615;&#33021;&#22686;&#21152;&#20102;&#36817;60&#65285;&#65292;&#20351;&#20854;&#19982;&#29616;&#26377;&#30340;&#39030;&#32423;&#27169;&#22411;&#24182;&#39550;&#40784;&#39537;&#65292;&#32780;I
&lt;/p&gt;
&lt;p&gt;
Lexical matching remains the de facto evaluation method for open-domain question answering (QA). Unfortunately, lexical matching fails completely when a plausible candidate answer does not appear in the list of gold answers, which is increasingly the case as we shift from extractive to generative models. The recent success of large language models (LLMs) for QA aggravates lexical matching failures since candidate answers become longer, thereby making matching with the gold answers even more challenging. Without accurate evaluation, the true progress in open-domain QA remains unknown. In this paper, we conduct a thorough analysis of various open-domain QA models, including LLMs, by manually evaluating their answers on a subset of NQ-open, a popular benchmark. Our assessments reveal that while the true performance of all models is significantly underestimated, the performance of the InstructGPT (zero-shot) LLM increases by nearly +60%, making it on par with existing top models, and the I
&lt;/p&gt;</description></item><item><title>COCKATIEL&#26159;&#19968;&#31181;&#36830;&#32493;&#27010;&#24565;&#25490;&#21517;&#24102;&#24402;&#22240;&#24615;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;NLP&#20998;&#31867;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#19988;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#25110;&#38656;&#35201;&#26032;&#27169;&#22411;&#65292;&#24050;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#26356;&#26377;&#20449;&#24687;&#37327;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.06754</link><description>&lt;p&gt;
COCKATIEL:&#29992;&#21487;&#35299;&#37322;&#20803;&#32032;&#23545;NLP&#20219;&#21153;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#36827;&#34892;&#36830;&#32493;&#27010;&#24565;&#25490;&#21517;&#24102;&#24402;&#22240;&#24615;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
COCKATIEL: COntinuous Concept ranKed ATtribution with Interpretable ELements for explaining neural net classifiers on NLP tasks. (arXiv:2305.06754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06754
&lt;/p&gt;
&lt;p&gt;
COCKATIEL&#26159;&#19968;&#31181;&#36830;&#32493;&#27010;&#24565;&#25490;&#21517;&#24102;&#24402;&#22240;&#24615;&#35299;&#37322;&#30340;&#25216;&#26415;&#65292;&#22522;&#20110;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;NLP&#20998;&#31867;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#19988;&#19981;&#20250;&#24433;&#21709;&#20934;&#30830;&#24615;&#25110;&#38656;&#35201;&#26032;&#27169;&#22411;&#65292;&#24050;&#35777;&#26126;&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#26356;&#26377;&#20449;&#24687;&#37327;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#22797;&#26434;&#65292;&#20854;&#22312;NLP&#20013;&#30340;&#20351;&#29992;&#34429;&#28982;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#65292;&#20294;&#20854;&#21487;&#35299;&#37322;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#36739;&#20026;&#26840;&#25163;&#12290;&#26368;&#36817;&#30340;&#20105;&#35770;&#34920;&#26126;&#65292;&#27880;&#24847;&#21147;&#22270;&#21644;&#24402;&#22240;&#26041;&#27861;&#19981;&#21487;&#38752;&#65292;&#32780;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#20171;&#32461;&#20102;&#20854;&#20013;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;COCKATIEL&#36825;&#19968;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#23427;&#26159;&#19968;&#31181;&#21518;&#26399;&#26041;&#27861;&#65292;&#22522;&#20110;&#27010;&#24565;&#65292;&#29992;&#20110;&#20174;&#32463;&#36807;NLP&#20998;&#31867;&#20219;&#21153;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#20013;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#65292;&#36890;&#36807;&#20351;&#29992;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#26469;&#21457;&#29616;&#27169;&#22411;&#21033;&#29992;&#26469;&#36827;&#34892;&#39044;&#27979;&#30340;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#25935;&#24863;&#24615;&#20998;&#26512;&#26469;&#20934;&#30830;&#20272;&#35745;&#27599;&#20010;&#27010;&#24565;&#23545;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#24213;&#23618;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25110;&#38656;&#35201;&#35757;&#32451;&#26032;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21333;&#19968;&#21644;&#22810;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;COCKATIEL&#27604;&#29616;&#26377;&#26041;&#27861;&#20135;&#29983;&#26356;&#26377;&#20449;&#24687;&#37327;&#21644;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures are complex and their use in NLP, while it has engendered many successes, makes their interpretability or explainability challenging. Recent debates have shown that attention maps and attribution methods are unreliable (Pruthi et al., 2019; Brunner et al., 2019). In this paper, we present some of their limitations and introduce COCKATIEL, which successfully addresses some of them. COCKATIEL is a novel, post-hoc, concept-based, model-agnostic XAI technique that generates meaningful explanations from the last layer of a neural net model trained on an NLP classification task by using Non-Negative Matrix Factorization (NMF) to discover the concepts the model leverages to make predictions and by exploiting a Sensitivity Analysis to estimate accurately the importance of each of these concepts for the model. It does so without compromising the accuracy of the underlying model or requiring a new one to be trained. We conduct experiments in single and multi-aspect sent
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06569</link><description>&lt;p&gt;
&#22914;&#20309;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#32034;&#24341;&#39033;&#30446;ID
&lt;/p&gt;
&lt;p&gt;
How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#36827;&#34892;&#20102;&#31995;&#32479;&#26816;&#26597;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23558;&#25512;&#33616;&#20219;&#21153;&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#25512;&#33616;&#12290;&#23427;&#36890;&#36807;&#30452;&#25509;&#29983;&#25104;&#24314;&#35758;&#30340;&#39033;&#30446;&#32780;&#19981;&#26159;&#35745;&#31639;&#20256;&#32479;&#25512;&#33616;&#27169;&#22411;&#20013;&#27599;&#20010;&#20505;&#36873;&#39033;&#30446;&#30340;&#25490;&#21517;&#24471;&#20998;&#65292;&#31616;&#21270;&#20102;&#25512;&#33616;&#31649;&#36947;&#65292;&#36991;&#20813;&#20102;&#22810;&#27573;&#36807;&#28388;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#22312;&#20915;&#23450;&#35201;&#25512;&#33616;&#21738;&#20123;&#39033;&#30446;&#26102;&#29983;&#25104;&#36807;&#38271;&#30340;&#25991;&#26412;&#65292;&#20026;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#21019;&#24314;LLM&#20860;&#23481;&#30340;&#39033;&#30446;ID&#26159;&#24517;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#30340;&#39033;&#30446;&#32034;&#24341;&#38382;&#39064;&#65292;&#20197;P5&#20026;&#20195;&#34920;&#30340;&#20027;&#24178;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#21508;&#31181;&#32034;&#24341;&#26041;&#27861;&#22797;&#21046;&#20854;&#32467;&#26524;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#20960;&#31181;&#24494;&#19981;&#36275;&#36947;&#30340;&#39033;&#30446;&#32034;&#24341;&#26041;&#27861;&#65288;&#22914;&#29420;&#31435;&#32034;&#24341;&#12289;&#26631;&#39064;&#32034;&#24341;&#21644;&#38543;&#26426;&#32034;&#24341;&#65289;&#30340;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32034;&#24341;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#32034;&#24341;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#32034;&#24341;&#26041;&#27861;&#22312;&#39033;&#30446;&#25512;&#33616;&#20934;&#30830;&#24615;&#21644;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32034;&#24341;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation foundation model utilizes large language models (LLM) for recommendation by converting recommendation tasks into natural language tasks. It enables generative recommendation which directly generates the item(s) to recommend rather than calculating a ranking score for each and every candidate item in traditional recommendation models, simplifying the recommendation pipeline from multi-stage filtering to single-stage filtering. To avoid generating excessively long text when deciding which item(s) to recommend, creating LLM-compatible item IDs is essential for recommendation foundation models. In this study, we systematically examine the item indexing problem for recommendation foundation models, using P5 as the representative backbone model and replicating its results with various indexing methods. To emphasize the importance of item indexing, we first discuss the issues of several trivial item indexing methods, such as independent indexing, title indexing, and random inde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#20102;&#35299;&#31243;&#24230;&#65292;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#22522;&#20110;&#36127;&#38754;&#30693;&#35782;&#30340;&#26377;&#25928;&#21477;&#23376;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#22312;&#22238;&#31572;&#26497;&#24615;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#31181;&#20449;&#24565;&#20914;&#31361;&#20027;&#35201;&#28304;&#20110;&#35821;&#35328;&#39044;&#35757;&#32451;&#26102;&#30340;&#32479;&#35745;&#24555;&#25463;&#26041;&#24335;&#21644;&#21542;&#23450;&#25253;&#21578;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.05976</link><description>&lt;p&gt;
&#35828;&#21040;&#20570;&#21040;! &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#26041;&#38754;&#23384;&#22312;&#22826;&#36807;&#20048;&#35266;&#30340;&#34920;&#36848;
&lt;/p&gt;
&lt;p&gt;
Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge. (arXiv:2305.05976v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#20102;&#35299;&#31243;&#24230;&#65292;&#21457;&#29616;LLMs&#22312;&#29983;&#25104;&#22522;&#20110;&#36127;&#38754;&#30693;&#35782;&#30340;&#26377;&#25928;&#21477;&#23376;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#22312;&#22238;&#31572;&#26497;&#24615;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#36825;&#31181;&#20449;&#24565;&#20914;&#31361;&#20027;&#35201;&#28304;&#20110;&#35821;&#35328;&#39044;&#35757;&#32451;&#26102;&#30340;&#32479;&#35745;&#24555;&#25463;&#26041;&#24335;&#21644;&#21542;&#23450;&#25253;&#21578;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22240;&#33021;&#22815;&#23384;&#20648;&#21644;&#21033;&#29992;&#27491;&#38754;&#30693;&#35782;&#32780;&#34987;&#24191;&#27867;&#30740;&#31350;&#12290;&#20294;&#26159;&#65292;&#36127;&#38754;&#30693;&#35782;&#65292;&#22914;&#8220;&#29422;&#23376;&#19981;&#29983;&#27963;&#22312;&#28023;&#27915;&#20013;&#8221;&#65292;&#20063;&#26159;&#19990;&#30028;&#19978;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#20294;&#24456;&#23569;&#22312;&#25991;&#26412;&#20013;&#26126;&#30830;&#25552;&#21040;&#12290;LLMs&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#20102;&#35299;&#22810;&#23569;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#23545;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#20102;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#38480;&#21046;&#30340;&#20851;&#38190;&#35789;&#21040;&#21477;&#23376;&#29983;&#25104;&#20219;&#21153;(CG)&#21644;&#19968;&#20010;&#24067;&#23572;&#22411;&#38382;&#31572;&#20219;&#21153;(QA)&#26469;&#25506;&#27979;LLMs&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#65292;LLMs&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#22522;&#20110;&#36127;&#38754;&#24120;&#35782;&#30693;&#35782;&#30340;&#26377;&#25928;&#21477;&#23376;&#65292;&#20294;&#23427;&#20204;&#21487;&#20197;&#27491;&#30830;&#22320;&#22238;&#31572;&#26497;&#24615;&#30340;&#26159;&#25110;&#21542;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;LLMs&#30340;&#20449;&#24565;&#20914;&#31361;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#38454;&#27573;&#30340;&#32479;&#35745;&#24555;&#25463;&#26041;&#24335;&#21644;&#21542;&#23450;&#25253;&#21578;&#20559;&#35265;&#24341;&#36215;&#20102;&#36825;&#31181;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been widely studied for their ability to store and utilize positive knowledge. However, negative knowledge, such as "lions don't live in the ocean", is also ubiquitous in the world but rarely mentioned explicitly in the text. What do LLMs know about negative knowledge? This work examines the ability of LLMs to negative commonsense knowledge. We design a constrained keywords-to-sentence generation task (CG) and a Boolean question-answering task (QA) to probe LLMs. Our experiments reveal that LLMs frequently fail to generate valid sentences grounded in negative commonsense knowledge, yet they can correctly answer polar yes-or-no questions. We term this phenomenon the belief conflict of LLMs. Our further analysis shows that statistical shortcuts and negation reporting bias from language modeling pre-training cause this conflict.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NLP4SGPAPERS&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26144;&#23556;&#12289;&#20219;&#21153;&#21450;&#26041;&#27861;&#30340;&#30830;&#23450;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#25972;&#20010;ACL&#25991;&#38598;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#23637;&#31034;&#20102;NLP4SG&#39046;&#22495;&#30340;&#20840;&#35980;&#12290;</title><link>http://arxiv.org/abs/2305.05471</link><description>&lt;p&gt;
&#36229;&#36234;&#21892;&#24847;&#65306;NLP&#29992;&#20110;&#31038;&#20250;&#20844;&#30410;&#30340;&#30740;&#31350;&#29616;&#29366;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NLP4SGPAPERS&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26144;&#23556;&#12289;&#20219;&#21153;&#21450;&#26041;&#27861;&#30340;&#30830;&#23450;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#25972;&#20010;ACL&#25991;&#38598;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#23637;&#31034;&#20102;NLP4SG&#39046;&#22495;&#30340;&#20840;&#35980;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20986;&#29616;&#22312;&#21508;&#31181;&#29992;&#20363;&#20013;&#12290;&#22312;&#20247;&#22810;&#30340;NLP&#24212;&#29992;&#20013;&#65292;&#35768;&#22810;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#21463;&#21040;&#28608;&#21169;&#65292;&#24076;&#26395;&#36890;&#36807;&#24037;&#20316;&#20855;&#26377;&#31215;&#26497;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#31526;&#21512;NLP for Social Good (NLP4SG)&#30340;&#26368;&#26032;&#20513;&#35758;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#24182;&#19981;&#24635;&#26159;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#24049;&#30340;&#30740;&#31350;&#24037;&#20316;&#22914;&#20309;&#35299;&#20915;&#24403;&#20170;&#30340;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;NLP4SGPAPERS&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#19977;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#31185;&#23398;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;NLP4SG&#35770;&#25991;&#65292;&#24182;&#36890;&#36807;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#23545;NLP4SG&#36827;&#34892;&#25551;&#36848;: (1)&#30830;&#23450;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;(2)&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;(SDGs)&#65292;&#20197;&#21450;(3)&#35782;&#21035;&#23427;&#20204;&#27491;&#22312;&#35299;&#20915;&#30340;&#20219;&#21153;&#21644;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#25972;&#20010;ACL&#25991;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;NLP4SG&#39046;&#22495;&#30340;&#40479;&#30640;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today's big social problems. Thus, in this paper, we introduce NLP4SGPAPERS, a scientific dataset with three associated tasks that can help identify NLP4SG papers and characterize the NLP4SG landscape by: (1) identifying the papers that address a social problem, (2) mapping them to the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying the task they are solving and the methods they are using. Using state-of-the-art NLP models, we address each of these tasks and use them on the entire ACL Anthology, resulting in a visualization workspace that gives resear
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.05280</link><description>&lt;p&gt;
VCSUM&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05280
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26032;&#38395;&#21644;&#32842;&#22825;&#25688;&#35201;&#30456;&#27604;&#65292;&#30001;&#20110;&#25968;&#25454;&#21463;&#38480;&#65292;&#20250;&#35758;&#25688;&#35201;&#30340;&#21457;&#23637;&#21463;&#21040;&#26497;&#22823;&#30340;&#20943;&#36895;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#25105;&#20204;&#22768;&#31216;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#22810;&#21151;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#20027;&#39064;&#21010;&#20998;&#12289;&#22836;&#26465;&#12289;&#20998;&#27573;&#25688;&#35201;&#12289;&#25972;&#20010;&#20250;&#35758;&#25688;&#35201;&#21644;&#26174;&#35201;&#21477;&#23376;&#31561;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;VCSum&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20851;&#20110;&#19981;&#21516;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;VCSum&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312; \url{https://github.com/hahahawu/VCSum} &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05050</link><description>&lt;p&gt;
ANALOGICAL- &#19968;&#31181;&#26032;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#31867;&#27604;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20197;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#20026;&#24418;&#24335;&#30340;&#31867;&#27604;&#22312;&#34913;&#37327;&#35832;&#22914;word2vec&#20043;&#31867;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20027;&#35201;&#26681;&#25454;GLUE&#21644;SuperGLUE&#31561;&#22522;&#20934;&#30340;&#22806;&#22312;&#37327;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#22312;LLMs&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#25991;&#26412;&#20013;&#32472;&#21046;&#31867;&#27604;&#30340;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#20197;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#23545;LLMs&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20998;&#21035;&#20026; (i)&#21333;&#35789;&#12289;(ii)&#21333;&#35789;vs&#21477;&#23376;&#12289;(iii)&#35821;&#27861;&#12289;(iv)&#21542;&#23450;&#12289;(v)&#34164;&#21547;&#21644;(vi)&#38544;&#21947;&#12290;&#21033;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;(&#20363;&#22914;&#65292;&#8220;&#25105;&#33021;&#35828;&#20004;&#31181;&#35821;&#35328;&#8221;&#24212;&#35813;&#26356;&#25509;&#36817;&#8220;&#25105;&#26159;&#21452;&#35821;&#30340;&#8221;&#65292;&#32780;&#8220;&#25105;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#21644;&#8220;&#25105;&#19981;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#24212;&#35813;&#26159;&#27491;&#20132;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MultiTACRED&#30340;&#22810;&#35821;&#35328;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#33258;&#21160;&#27880;&#37322;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;12&#31181;&#35821;&#35328;&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#32570;&#20047;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#21333;&#35821;RE&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#33521;&#25991;&#21407;&#29256;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.04582</link><description>&lt;p&gt;
MultiTACRED: TAC&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;&#30340;&#22810;&#35821;&#35328;&#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
MultiTACRED: A Multilingual Version of the TAC Relation Extraction Dataset. (arXiv:2305.04582v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;MultiTACRED&#30340;&#22810;&#35821;&#35328;&#29256;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#33258;&#21160;&#27880;&#37322;&#65292;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;12&#31181;&#35821;&#35328;&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#24773;&#20917;&#19979;&#20851;&#31995;&#25277;&#21462;&#39046;&#22495;&#32570;&#20047;&#36164;&#28304;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#32763;&#35793;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#21333;&#35821;RE&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#33521;&#25991;&#21407;&#29256;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26159;&#20449;&#24687;&#25552;&#21462;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#20854;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#25193;&#23637;&#21463;&#21040;&#32570;&#20047;&#19982;TACRED&#65288;Zhang&#31561;&#20154;&#65292;2017&#65289;&#31561;&#22823;&#22411;&#33521;&#35821;&#25968;&#25454;&#38598;&#30456;&#23218;&#32654;&#30340;&#30417;&#30563;&#36164;&#28304;&#30340;&#38480;&#21046;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;MultiTACRED&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#26469;&#33258;9&#20010;&#35821;&#31995;&#30340;12&#31181;&#35821;&#35328;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;TACRED&#23454;&#20363;&#21644;&#33258;&#21160;&#25237;&#24433;&#20854;&#23454;&#20307;&#27880;&#37322;&#32780;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32763;&#35793;&#21644;&#27880;&#37322;&#25237;&#24433;&#30340;&#36136;&#37327;&#65292;&#30830;&#23450;&#20102;&#38169;&#35823;&#31867;&#21035;&#65292;&#24182;&#22312;&#24120;&#35265;&#30340;&#36801;&#31227;&#23398;&#20064;&#22330;&#26223;&#20013;&#23454;&#39564;&#35780;&#20272;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#26426;&#22120;&#32763;&#35793;&#26159;&#20256;&#36882;RE&#23454;&#20363;&#30340;&#21487;&#34892;&#31574;&#30053;&#65292;&#27597;&#35821;&#20154;&#22763;&#21028;&#26029;&#36229;&#36807;83&#65285;&#30340;&#32763;&#35793;&#23454;&#20363;&#22312;&#35821;&#35328;&#21644;&#35821;&#20041;&#19978;&#37117;&#26159;&#21487;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#21333;&#35821;RE&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#33521;&#25991;&#21407;&#29256;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) is a fundamental task in information extraction, whose extension to multilingual settings has been hindered by the lack of supervised resources comparable in size to large English datasets such as TACRED (Zhang et al., 2017). To address this gap, we introduce the MultiTACRED dataset, covering 12 typologically diverse languages from 9 language families, which is created by machine-translating TACRED instances and automatically projecting their entity annotations. We analyze translation and annotation projection quality, identify error categories, and experimentally evaluate fine-tuned pretrained mono- and multilingual language models in common transfer learning scenarios. Our analyses show that machine translation is a viable strategy to transfer RE instances, with native speakers judging more than 83% of the translated instances to be linguistically and semantically acceptable. We find monolingual RE model performance to be comparable to the English original fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.04476</link><description>&lt;p&gt;
AlignSTS&#65306;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#23454;&#29616;&#35821;&#38899;&#36716;&#21809;
&lt;/p&gt;
&lt;p&gt;
AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#21809; (STS) &#20219;&#21153;&#26088;&#22312;&#22312;&#38754;&#23545;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26102;&#65292;&#29983;&#25104;&#19982;&#35821;&#38899;&#24405;&#38899;&#30456;&#23545;&#24212;&#30340;&#21809;&#27468;&#26679;&#26412;&#65306;&#22312;&#27809;&#26377;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#65288;&#21809;&#27468;&#65289;&#38899;&#39640;&#36718;&#24275;&#21644;&#28304;&#65288;&#35821;&#38899;&#65289;&#20869;&#23481;&#20043;&#38388;&#30340;&#23545;&#40784;&#38590;&#20197;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#23558;&#35821;&#38899;&#21464;&#21270;&#65288;&#22914;&#38899;&#39640;&#21644;&#20869;&#23481;&#65289;&#35270;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#21809;&#20986;&#26059;&#24459;&#30340;&#27468;&#35789;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;AlignSTS: 1&#65289;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#65292;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#20854;&#20013;&#33410;&#22863;&#34920;&#31034;&#20197;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;&#65292;&#24182;&#37327;&#21270;&#20026;&#31163;&#25955;&#31354;&#38388;&#65307;2&#65289;&#20351;&#29992;&#39044;&#27979;&#30340;&#33410;&#22863;&#34920;&#31034;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#65292;&#24182;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AlignSTS&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.03445</link><description>&lt;p&gt;
LMs&#22266;&#23432;&#38453;&#22320;&#65306;&#25506;&#31350;&#20855;&#36523;&#21270;&#23545;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models. (arXiv:2305.03445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#35843;&#26597;&#20102;&#20855;&#36523;&#21270;&#31574;&#30053;&#23545;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27604;&#21947;&#24615;&#35821;&#35328;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#22312;&#22788;&#29702;&#34892;&#20026;&#26356;&#20855;&#20307;&#21270;&#30340;&#38544;&#21947;&#24615;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#21947;&#35821;&#35328;&#26159;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#35299;&#37322;&#22522;&#20110;&#21333;&#35789;&#30340;&#20351;&#29992;&#26041;&#24335;&#20559;&#31163;&#20102;&#23427;&#20204;&#30340;&#24120;&#35268;&#39034;&#24207;&#21644;&#21547;&#20041;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#21487;&#20197;&#36731;&#26494;&#29702;&#35299;&#21644;&#35808;&#37322;&#38544;&#21947;&#12289;&#27604;&#21947;&#25110;&#20064;&#35821;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20174;&#20855;&#36523;&#38544;&#21947;&#20013;&#25512;&#23548;&#20986;&#26469;&#12290;&#35821;&#35328;&#26159;&#20855;&#36523;&#21270;&#30340;&#20195;&#29702;&#65292;&#22914;&#26524;&#38544;&#21947;&#26159;&#20256;&#32479;&#30340;&#21644;&#35789;&#27719;&#21270;&#30340;&#65292;&#37027;&#20040;&#19968;&#20010;&#27809;&#26377;&#36523;&#20307;&#30340;&#31995;&#32479;&#23601;&#26356;&#23481;&#26131;&#29702;&#35299;&#20855;&#36523;&#27010;&#24565;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#27604;&#21947;&#24615;&#21477;&#23376;&#30340;&#34892;&#21160;&#26356;&#20855;&#20307;&#21270;&#26102;&#65292;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#37322;&#38544;&#21947;&#21477;&#23376;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#25490;&#38500;&#20102;&#19982;&#20854;&#20182;&#29305;&#24449;&#65288;&#20363;&#22914;&#21333;&#35789;&#38271;&#24230;&#25110;&#20855;&#20307;&#24615;&#65289;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#38271;&#25991;&#26723;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#65292;&#22312;&#26032;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.03319</link><description>&lt;p&gt;
HiPool&#65306;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#38271;&#25991;&#26723;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
HiPool: Modeling Long Documents Using Graph Neural Networks. (arXiv:2305.03319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#27169;&#25311;&#38271;&#25991;&#26723;&#65292;&#35299;&#20915;&#20102;&#39034;&#24207;&#27169;&#22411;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#65292;&#22312;&#26032;&#25552;&#20986;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#32534;&#30721;&#38271;&#24207;&#21015;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#20173;&#21463;&#21040;&#39044;&#23450;&#20041;&#30340;&#26368;&#22823;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#25193;&#23637;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#22240;&#27492;&#65292;&#19968;&#20123;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#23618;&#27425;&#32467;&#26500;&#26469;&#24314;&#27169;&#38271;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22823;&#22810;&#25968;&#26159;&#23545;&#19978;&#23618;&#20351;&#29992;&#39034;&#24207;&#27169;&#22411;&#65292;&#38754;&#20020;&#30528;&#38271;&#26399;&#20381;&#36182;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#22266;&#23450;&#38271;&#24230;&#23545;&#24207;&#21015;&#36827;&#34892;&#20998;&#22359;&#65292;&#20197;&#27169;&#25311;&#21477;&#23376;&#32423;&#21035;&#30340;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#26469;&#27169;&#25311;&#21477;&#20869;&#21644;&#36328;&#21477;&#30340;&#20851;&#32852;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#38271;&#25991;&#26723;&#20998;&#31867;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#36739;&#23569;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20849;&#35745;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#26679;&#26412;&#24635;&#25968;&#36798;53000&#20010;&#65292;&#24179;&#22343;&#26631;&#35760;&#38271;&#24230;&#20026;4034&#20010;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoding long sequences in Natural Language Processing (NLP) is a challenging problem. Though recent pretraining language models achieve satisfying performances in many NLP tasks, they are still restricted by a pre-defined maximum length, making them challenging to be extended to longer sequences. So some recent works utilize hierarchies to model long sequences. However, most of them apply sequential models for upper hierarchies, suffering from long dependency issues. In this paper, we alleviate these issues through a graph-based method. We first chunk the sequence with a fixed length to model the sentence-level information. We then leverage graphs to model intra- and cross-sentence correlations with a new attention mechanism. Additionally, due to limited standard benchmarks for long document classification (LDC), we propose a new challenging benchmark, totaling six datasets with up to 53k samples and 4034 average tokens' length. Evaluation shows our model surpasses competitive baselin
&lt;/p&gt;</description></item><item><title>CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2305.00969</link><description>&lt;p&gt;
CryCeleb: &#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CryCeleb: A Speaker Verification Dataset Based on Infant Cry Sounds. (arXiv:2305.00969v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00969
&lt;/p&gt;
&lt;p&gt;
CryCeleb&#26159;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#35828;&#35805;&#20154;&#35748;&#35777;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#21487;&#29992;&#20110;&#30740;&#31350;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;Ubenwa CryCeleb&#25968;&#25454;&#38598;&#8212;&#8212;&#19968;&#20010;&#26631;&#35760;&#30340;&#23156;&#20799;&#21741;&#22768;&#25910;&#38598;&#65292;&#20197;&#21450;&#38468;&#24102;&#30340;CryCeleb 2023&#20219;&#21153;&#8212;&#8212;&#19968;&#20010;&#22522;&#20110;&#23156;&#20799;&#21741;&#22768;&#30340;&#20844;&#20849;&#35828;&#35805;&#20154;&#39564;&#35777;&#25361;&#25112;&#12290;&#25105;&#20204;&#37322;&#25918;&#20986;786&#21517;&#26032;&#29983;&#20799;&#36229;&#36807;6&#23567;&#26102;&#30340;&#25163;&#21160;&#20998;&#21106;&#21741;&#22768;&#65292;&#20197;&#40723;&#21169;&#23156;&#20799;&#21741;&#22768;&#20998;&#26512;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the Ubenwa CryCeleb dataset - a labeled collection of infant cries, and the accompanying CryCeleb 2023 task - a public speaker verification challenge based on infant cry sounds. We release for academic usage more than 6 hours of manually segmented cry sounds from 786 newborns to encourage research in infant cry analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.00382</link><description>&lt;p&gt;
&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#25991;&#26412;&#25551;&#36848;&#20013;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database. (arXiv:2305.00382v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25552;&#21462;&#21644;&#23454;&#20307;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#65292;&#20363;&#22914;&#28431;&#27934;&#35780;&#20272;&#21644;&#23041;&#32961;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#28431;&#27934;&#25968;&#25454;&#24211;&#30340;&#20449;&#24687;&#20013;&#26500;&#24314;&#28431;&#27934;&#30693;&#35782;&#22270;&#35889;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#12289;&#20197;&#21450;&#20351;&#29992;&#31070;&#32463;&#27169;&#22411;&#12289;&#21551;&#21457;&#24335;&#35268;&#21017;&#21644;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#30340;&#23454;&#20307;&#39044;&#27979;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#26377;&#21161;&#20110;&#35299;&#20915;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#35889;&#20013;&#32570;&#22833;&#23454;&#20307;&#30340;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.
&lt;/p&gt;</description></item><item><title>SweCTRL-Mini&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#23427;&#29983;&#25104;&#30340;&#25991;&#26412;&#27969;&#27966;&#65292;&#23436;&#20840;&#24320;&#25918;&#19979;&#36733;&#12290;&#29983;&#25104;&#33021;&#21147;&#27604;&#36739;GPT-3&#12290;</title><link>http://arxiv.org/abs/2304.13994</link><description>&lt;p&gt;
SweCTRL-Mini&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#25968;&#25454;&#36879;&#26126;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#30340;&#29790;&#20856;&#35821;&#35328;&#29256;
&lt;/p&gt;
&lt;p&gt;
SweCTRL-Mini: a data-transparent Transformer-based large language model for controllable text generation in Swedish. (arXiv:2304.13994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13994
&lt;/p&gt;
&lt;p&gt;
SweCTRL-Mini&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#25143;&#21487;&#20197;&#25511;&#21046;&#23427;&#29983;&#25104;&#30340;&#25991;&#26412;&#27969;&#27966;&#65292;&#23436;&#20840;&#24320;&#25918;&#19979;&#36733;&#12290;&#29983;&#25104;&#33021;&#21147;&#27604;&#36739;GPT-3&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SweCTRL-Mini&#65292;&#23427;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29790;&#20856;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19978;&#30340;&#25512;&#29702;&#21644;fine-tuning&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#30001;Keskar&#12289;McCann&#12289;Varshney&#12289;Xiong&#21644;Socher&#65288;2019&#65289;&#24320;&#21457;&#30340;CTRL&#20307;&#31995;&#32467;&#26500;&#65292;&#36825;&#24847;&#21619;&#30528;SweCTRL-Mini&#27169;&#22411;&#30340;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#22312;&#29983;&#25104;&#25552;&#31034;&#20013;&#25554;&#20837;&#29305;&#27530;&#26631;&#35760;&#26469;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#27966;&#12290;SweCTRL-Mini&#22312;&#29790;&#20856;&#37096;&#20998;mC4&#35821;&#26009;&#24211;&#21644;&#19968;&#32452;&#29790;&#20856;&#23567;&#35828;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;(1)&#25152;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#25991;&#26412;&#39044;&#22788;&#29702;&#27493;&#39588;&#30340;&#35814;&#32454;&#35828;&#26126;&#65292;&#20197;&#20351;&#21487;&#20197;&#26816;&#26597;&#29305;&#23450;&#30701;&#35821;/&#26469;&#28304;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#19968;&#37096;&#20998;;(2)&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#36827;&#34892;&#36776;&#21035;&#24615;&#20219;&#21153;&#30340;&#27169;&#22411;&#35780;&#20272;&#65292;&#20351;&#29992;&#20154;&#24037;&#35009;&#21028;&#36827;&#34892;&#29983;&#25104;&#24615;&#20219;&#21153;&#30340;&#35780;&#20272;;&#25105;&#20204;&#36824;&#23558;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#19982;GPT-3&#30340;&#29983;&#25104;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;SweCTRL-Mini &#26159;&#23436;&#20840;&#24320;&#25918;&#30340;&#65292;&#21487;&#20379;&#19979;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SweCTRL-Mini, a large Swedish language model that can be used for inference and fine-tuning on a single consumer-grade GPU. The model is based on the CTRL architecture by Keskar, McCann, Varshney, Xiong, and Socher (2019), which means that users of the SweCTRL-Mini model can control the genre of the generated text by inserting special tokens in the generation prompts. SweCTRL-Mini is trained on a subset of the Swedish part of the mC4 corpus and a set of Swedish novels. In this article, we provide (1) a detailed account of the utilized training data and text pre-processing steps, to the extent that it is possible to check whether a specific phrase/source was a part of the training data, and (2) an evaluation of the model on both discriminative tasks, using automatic evaluation methods, and generative tasks, using human referees. We also compare the generative capabilities of the model with those of GPT-3. SweCTRL-Mini is fully open and available for download.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.12986</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Measuring Massive Multitask Chinese Understanding. (arXiv:2304.12986v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#39033;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#27979;&#35797;&#28085;&#30422;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#21457;&#27491;&#34028;&#21187;&#21457;&#23637;&#65292;&#20294;&#32570;&#20047;&#30456;&#24212;&#30340;&#33021;&#21147;&#35780;&#20272;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#65292;&#20197;&#34913;&#37327;&#22823;&#22411;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#21307;&#23398;&#12289;&#27861;&#24459;&#12289;&#24515;&#29702;&#23398;&#21644;&#25945;&#32946;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26377;15&#20010;&#23376;&#20219;&#21153;&#65292;&#22312;&#25945;&#32946;&#39046;&#22495;&#26377;8&#20010;&#23376;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#24179;&#22343;&#27604;&#34920;&#29616;&#26368;&#24046;&#30340;&#27169;&#22411;&#39640;&#20986;&#36817;22&#20010;&#30334;&#20998;&#28857;&#12290;&#22312;&#22235;&#20010;&#20027;&#35201;&#39046;&#22495;&#20013;&#65292;&#25152;&#26377;&#27169;&#22411;&#30340;&#24179;&#22343;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#22343;&#26410;&#36229;&#36807;0.5&#12290;&#22312;&#23376;&#39046;&#22495;&#20013;&#65292;&#21482;&#26377;GPT-3.5-turbo&#27169;&#22411;&#22312;&#20020;&#24202;&#21307;&#23398;&#20013;&#23454;&#29616;&#20102;0.703&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#65292;&#36825;&#26159;&#25152;&#26377;&#27169;&#22411;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#20013;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25152;&#26377;&#27169;&#22411;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#34920;&#29616;&#37117;&#24456;&#24046;&#65292;&#26368;&#39640;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24230;&#20165;&#36798;&#21040;0.259&#12290;&#36890;&#36807;&#20840;&#38754;&#35780;&#20272;&#22810;&#20010;&#23398;&#31185;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#30340;&#30693;&#35782;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#24320;&#21457;&#26356;&#21152;&#22810;&#26679;&#21270;&#21644;&#22343;&#34913;&#30340;&#22810;&#20219;&#21153;&#20013;&#25991;&#29702;&#35299;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 22 percentage points on average. Across the four major domains, the average zero-shot accuracy of all models did not exceed 0.5. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.703 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.259. By comprehensively evaluating the breadth and depth of knowledge across multiple discipli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2304.12036</link><description>&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;&#26725;&#25509;&#24230;&#37325;&#35201;&#33410;&#28857;&#29983;&#25104;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Generating Post-hoc Explanations for Skip-gram-based Node Embeddings by Identifying Important Nodes with Bridgeness. (arXiv:2304.12036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;Skip-gram&#33410;&#28857;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#35745;&#31639;&#26725;&#25509;&#24230;&#35782;&#21035;&#37325;&#35201;&#33410;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;GRAPH-wGD&#65292;&#26377;&#25928;&#22320;&#25552;&#20379;&#20840;&#23616;&#24615;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#34920;&#31034;&#23398;&#20064;&#26159;&#32534;&#30721;&#36830;&#32493;&#30690;&#37327;&#31354;&#38388;&#20013;&#30340;&#20851;&#31995;&#20449;&#24687;&#21516;&#26102;&#20445;&#30041;&#32593;&#32476;&#22266;&#26377;&#23646;&#24615;&#21644;&#32467;&#26500;&#30340;&#37325;&#35201;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;DeepWalk&#12289;LINE&#12289;struc2vec&#12289;PTE&#12289;UserItem2vec&#21644;RWJBG&#31561;&#26080;&#30417;&#30563;&#33410;&#28857;&#23884;&#20837;&#26041;&#27861;&#20174;Skip-gram&#27169;&#22411;&#20013;&#20986;&#29616;&#65292;&#24182;&#22312;&#35832;&#22914;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#23884;&#20837;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29702;&#35770;&#30740;&#31350;&#65292;&#25552;&#20379;Skip-gram&#23884;&#20837;&#30340;&#21518;&#32493;&#35299;&#37322;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#21487;&#20197;&#36890;&#36807;&#22312;&#35889;&#32858;&#31867;&#24863;&#30693;&#23616;&#37096;&#25200;&#21160;&#19979;&#35745;&#31639;&#26725;&#25509;&#24230;&#26469;&#25214;&#21040;Skip-gram&#23884;&#20837;&#30340;&#20840;&#23616;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRAPH-wGD&#30340;&#26032;&#22411;&#22522;&#20110;&#26799;&#24230;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#20801;&#35768;&#26816;&#32034;top-q&#20840;&#23616;&#24615;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Node representation learning in a network is an important machine learning technique for encoding relational information in a continuous vector space while preserving the inherent properties and structures of the network. Recently, unsupervised node embedding methods such as DeepWalk, LINE, struc2vec, PTE, UserItem2vec, and RWJBG have emerged from the Skip-gram model and perform better performance in several downstream tasks such as node classification and link prediction than the existing relational models. However, providing post-hoc explanations of Skip-gram-based embeddings remains a challenging problem because of the lack of explanation methods and theoretical studies applicable for embeddings. In this paper, we first show that global explanations to the Skip-gram-based embeddings can be found by computing bridgeness under a spectral cluster-aware local perturbation. Moreover, a novel gradient-based explanation method, which we call GRAPH-wGD, is proposed that allows the top-q glo
&lt;/p&gt;</description></item><item><title>&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;(GPC)&#26159;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;&#12290;&#23427;&#21487;&#20197;&#22312;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#20013;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102; 5.8% &#30340; SuperGLUE &#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.05642</link><description>&lt;p&gt;
&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Global Prompt Cell: A Portable Control Module for Effective Prompt. (arXiv:2304.05642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05642
&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;(GPC)&#26159;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;&#12290;&#23427;&#21487;&#20197;&#22312;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#20013;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102; 5.8% &#30340; SuperGLUE &#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#25552;&#31034;&#21333;&#20803;(Global Prompt Cell, GPC)&#26159;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21487;&#31227;&#26893;&#25511;&#21046;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20923;&#32467;&#21442;&#25968;&#24182;&#22312;&#31532;&#19968;&#23618;&#30340;&#36755;&#20837;&#20013;&#25554;&#20837;&#21487;&#35757;&#32451;&#30340;&#23884;&#20837;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20449;&#24687;&#21033;&#29992;&#38382;&#39064;&#65292;GPC&#21487;&#20197;&#22312;&#25152;&#26377;&#32534;&#30721;&#22120;&#23618;&#20013;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25552;&#31034;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26222;&#36890;&#25552;&#31034;&#35843;&#25972;&#30456;&#27604;&#65292;GPC &#22312; SuperGLUE &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102; 5.8% &#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel approach to tuning pre-trained models, prompt tuning involves freezing the parameters in downstream tasks while inserting trainable embeddings into inputs in the first layer.However,previous methods have mainly focused on the initialization of prompt embeddings. The question of how to train and utilize prompt embeddings in a reasonable way has become aa limiting factor in the effectiveness of prompt tuning. To address this issue, we introduce the Global Prompt Cell (GPC), a portable control module for prompt tuning that selectively preserves prompt information across all encoder layers. Our experimental results demonstrate a 5.8% improvement on SuperGLUE datasets compared to vanilla prompt tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#21746;&#23398;&#21644;NLP&#22330;&#26223;&#20986;&#21457;&#65292;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#22312;NLP&#20013;&#26126;&#30830;&#30340;&#23450;&#20041;&#65292;&#38416;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#30340;&#31867;&#22411;&#21644;&#19968;&#31181;&#25512;&#29702;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#32463;&#20856;&#36923;&#36753;&#25512;&#29702;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21644;&#24120;&#35782;&#25512;&#29702;&#22235;&#20010;&#26041;&#38754;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.14725</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#32508;&#36848;&#8212;&#8212;&#20174;&#21746;&#23398;&#21040;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Natural Language Reasoning, A Survey. (arXiv:2303.14725v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#20174;&#21746;&#23398;&#21644;NLP&#22330;&#26223;&#20986;&#21457;&#65292;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#22312;NLP&#20013;&#26126;&#30830;&#30340;&#23450;&#20041;&#65292;&#38416;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#30340;&#31867;&#22411;&#21644;&#19968;&#31181;&#25512;&#29702;&#20998;&#31867;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#32463;&#20856;&#36923;&#36753;&#25512;&#29702;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21644;&#24120;&#35782;&#25512;&#29702;&#22235;&#20010;&#26041;&#38754;&#30340;&#30740;&#31350;&#29616;&#29366;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27010;&#24565;&#30340;&#26356;&#20026;&#28165;&#26224;&#30340;&#35270;&#35282;&#65292;&#26088;&#22312;&#20174;&#27010;&#24565;&#21644;&#23454;&#36341;&#20004;&#20010;&#26041;&#38754;&#25506;&#35752;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;&#22312;&#27010;&#24565;&#26041;&#38754;&#65292;&#26412;&#25991;&#26681;&#25454;&#21746;&#23398;&#21644;NLP&#22330;&#26223;&#25552;&#20379;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#22312;NLP&#20013;&#30340;&#26126;&#30830;&#23450;&#20041;&#65292;&#35752;&#35770;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#20219;&#21153;&#38656;&#35201;&#25512;&#29702;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#25512;&#29702;&#20998;&#31867;&#27861;&#12290;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#26412;&#25991;&#23545;NLP&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#20027;&#35201;&#28085;&#30422;&#20102;&#32463;&#20856;&#36923;&#36753;&#25512;&#29702;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#21644;&#24120;&#35782;&#25512;&#29702;&#12290;&#26412;&#25991;&#36824;&#23558;&#24378;&#22823;&#30340;&#22810;&#27493;&#25512;&#29702;&#27169;&#24335;&#8212;&#8212;&#21453;&#21521;&#25512;&#29702;&#31561;&#35270;&#20026;&#26410;&#26469;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#30740;&#31350;&#30340;&#37325;&#35201;&#26041;&#21521;&#65292;&#24182;&#23558;&#21487;&#25764;&#38144;&#25512;&#29702;&#20171;&#32461;&#20026;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26410;&#26469;&#26041;&#21521;&#20043;&#19968;&#12290;&#26412;&#25991;&#20851;&#27880;&#21333;&#27169;&#24577;&#38750;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#19981;&#21253;&#25324;&#31070;&#32463;&#31526;&#21495;&#25216;&#26415;&#21644;&#25968;&#23398;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey paper proposes a clearer view of natural language reasoning in the field of Natural Language Processing (NLP), both conceptually and practically. Conceptually, we provide a distinct definition for natural language reasoning in NLP, based on both philosophy and NLP scenarios, discuss what types of tasks require reasoning, and introduce a taxonomy of reasoning. Practically, we conduct a comprehensive literature review on natural language reasoning in NLP, mainly covering classical logical reasoning, natural language inference, multi-hop question answering, and commonsense reasoning. The paper also identifies and views backward reasoning, a powerful paradigm for multi-step reasoning, and introduces defeasible reasoning as one of the most important future directions in natural language reasoning research. We focus on single-modality unstructured natural language text, excluding neuro-symbolic techniques and mathematical reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;</title><link>http://arxiv.org/abs/2303.13465</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep RL with Hierarchical Action Exploration for Dialogue Generation. (arXiv:2303.13465v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#23618;&#34892;&#20026;&#25506;&#32034;&#65292;&#20174;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#34892;&#20026;&#37319;&#26679;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#34892;&#20026;&#31354;&#38388;&#26497;&#20854;&#24222;&#22823;&#65292;&#22240;&#27492;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#24517;&#39035;&#20351;&#29992;&#31574;&#30053;&#25913;&#36827;&#21644;&#34892;&#20026;&#37319;&#26679;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#26377;&#20215;&#20540;&#30340;&#22238;&#24212;&#38750;&#24120;&#31232;&#30095;&#65292;&#22240;&#27492;&#20351;&#29992;&#38543;&#26426;&#37319;&#26679;&#30340;&#36138;&#24515;&#31574;&#30053;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21452;&#31890;&#24230;&#30340; Q-function &#24182;&#36890;&#36807;&#25506;&#32034;&#26368;&#26377;&#21069;&#36884;&#30340;&#22238;&#24212;&#31867;&#21035;&#26469;&#32531;&#35299;&#36825;&#20010;&#23616;&#38480;&#24615;&#12290;&#35813;&#31639;&#27861;&#20174;&#35782;&#21035;&#20154;&#31867;&#24773;&#24863;&#32454;&#33410;&#30340;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#20013;&#36827;&#34892;&#31163;&#32447;&#23398;&#20064;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventionally, since the natural language action space is astronomical, approximate dynamic programming applied to dialogue generation involves policy improvement with action sampling. However, such a practice is inefficient for reinforcement learning (RL) because the eligible (high action value) responses are very sparse, and the greedy policy sustained by the random sampling is flabby. This paper shows that the performance of dialogue policy positively correlated with sampling size by theoretical and experimental. We introduce a novel dual-granularity Q-function to alleviate this limitation by exploring the most promising response category to intervene in the sampling. It extracts the actions following the grained hierarchy, which can achieve the optimum with fewer policy iterations. Our approach learns in the way of offline RL from multiple reward functions designed to recognize human emotional details. Empirical studies demonstrate that our algorithm outperforms the baseline metho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#32452; transformer &#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#30340;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#24471;&#19982;&#20154;&#31867;&#24456;&#30456;&#20284;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#23383;&#31526;&#21629;&#21517;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2303.12294</link><description>&lt;p&gt;
&#35780;&#20272;&#21464;&#25442;&#22120;&#27169;&#22411;&#21644;&#20154;&#31867;&#34892;&#20026;&#22312;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Transformer Models and Human Behaviors on Chinese Character Naming. (arXiv:2303.12294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#32452; transformer &#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#30340;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#20219;&#21153;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#24471;&#19982;&#20154;&#31867;&#24456;&#30456;&#20284;&#65292;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#23383;&#31526;&#21629;&#21517;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#23383;&#27597;&#35821;&#35328;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#26469;&#35299;&#37322;&#20154;&#31867;&#30340;&#23383;&#32032;-&#38899;&#32032;&#26144;&#23556;&#36807;&#31243;&#12290;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#25104;&#21151;&#22320;&#23398;&#20064;&#20102;&#23383;&#27597;&#23383;&#31526;&#20018;&#21450;&#20854;&#21457;&#38899;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32780;&#19988;&#36824;&#25429;&#25417;&#20102;&#20154;&#31867;&#22312;&#30701;&#26242;&#21333;&#35789;&#21629;&#21517;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#32452;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#22312;&#26410;&#30693;&#20013;&#25991;&#23383;&#31526;&#21629;&#21517;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#21644;&#20154;&#31867;&#30340;&#34892;&#20026;&#38750;&#24120;&#30456;&#20284;&#65292;&#23427;&#20204;&#22312;&#27599;&#20010;&#23383;&#31526;&#30340;&#20934;&#30830;&#24230;&#20998;&#24067;&#26041;&#38754;&#20855;&#26377;&#31867;&#20284;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#22312;&#31572;&#26696;&#19978;&#26377;&#24456;&#22823;&#30340;&#37325;&#21472;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#31572;&#26696;&#19982;&#20154;&#31867;&#30340;&#31572;&#26696;&#39640;&#24230;&#30456;&#20851;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#21464;&#25442;&#22120;&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25429;&#25417;&#20154;&#31867;&#30340;&#23383;&#31526;&#21629;&#21517;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models have been proposed to explain the grapheme-phoneme mapping process in humans for many alphabet languages. These models not only successfully learned the correspondence of the letter strings and their pronunciation, but also captured human behavior in nonce word naming tasks. How would the neural models perform for a non-alphabet language (e.g., Chinese) unknown character task? How well would the model capture human behavior? In this study, we evaluate a set of transformer models and compare their performances with human behaviors on an unknown Chinese character naming task. We found that the models and humans behaved very similarly, that they had similar accuracy distribution for each character, and had a substantial overlap in answers. In addition, the models' answers are highly correlated with humans' answers. These results suggested that the transformer models can well capture human's character naming behavior.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#24341;&#20837;&#20154;&#24037;&#20849;&#24773;&#30340;&#24605;&#24819;&#36827;&#20837;&#20154;&#26412;&#35774;&#35745;&#65292;&#20197;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.10583</link><description>&lt;p&gt;
&#20154;&#26412;&#35774;&#35745;&#20013;&#30340;&#20154;&#24037;&#20849;&#24773;&#65306;&#19968;&#20010;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Toward Artificial Empathy for Human-Centered Design: A Framework. (arXiv:2303.10583v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20174;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#24341;&#20837;&#20154;&#24037;&#20849;&#24773;&#30340;&#24605;&#24819;&#36827;&#20837;&#20154;&#26412;&#35774;&#35745;&#65292;&#20197;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#35774;&#35745;&#24072;&#36890;&#36807;&#21457;&#29616;&#26410;&#28385;&#36275;&#30340;&#38656;&#27714;&#21644;&#24320;&#21457;&#21019;&#26032;&#30340;&#27010;&#24565;&#20316;&#20026;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25506;&#32034;&#26426;&#20250;&#12290;&#20174;&#20154;&#26412;&#35774;&#35745;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35774;&#35745;&#24072;&#24517;&#39035;&#19982;&#20154;&#21457;&#23637;&#20849;&#24773;&#65292;&#25165;&#33021;&#30495;&#27491;&#29702;&#35299;&#20182;&#20204;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21457;&#23637;&#20849;&#24773;&#26159;&#19968;&#20010;&#22797;&#26434;&#32780;&#20027;&#35266;&#30340;&#36807;&#31243;&#65292;&#38750;&#24120;&#20381;&#36182;&#20110;&#35774;&#35745;&#24072;&#30340;&#20849;&#24773;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#20849;&#24773;&#30340;&#21457;&#23637;&#26159;&#30452;&#35273;&#24615;&#30340;&#65292;&#28508;&#22312;&#38656;&#27714;&#30340;&#21457;&#29616;&#24448;&#24448;&#26159;&#20598;&#28982;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#35265;&#35299;&#65292;&#20197;&#25351;&#20986;&#20197;&#20849;&#24773;&#20026;&#26680;&#24515;&#30340;&#20154;&#26412;&#35774;&#35745;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36328;&#23398;&#31185;&#30740;&#31350;&#65292;&#21253;&#25324;&#25968;&#25454;&#39537;&#21160;&#30340;&#29992;&#25143;&#30740;&#31350;&#12289;&#20849;&#24773;&#29702;&#35299;&#30340;&#21457;&#23637;&#21644;&#20154;&#24037;&#20849;&#24773;&#31561;&#30740;&#31350;&#39046;&#22495;&#12290;&#22522;&#20110;&#36825;&#20010;&#22522;&#30784;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#20849;&#24773;&#22312;&#20154;&#26412;&#35774;&#35745;&#20013;&#21487;&#20197;&#21457;&#25381;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#35774;&#35745;&#36807;&#31243;&#20013;&#24320;&#21457;&#20154;&#24037;&#20849;&#24773;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the early stages of the design process, designers explore opportunities by discovering unmet needs and developing innovative concepts as potential solutions. From a human-centered design perspective, designers must develop empathy with people to truly understand their needs. However, developing empathy is a complex and subjective process that relies heavily on the designer's empathetic capability. Therefore, the development of empathetic understanding is intuitive, and the discovery of underlying needs is often serendipitous. This paper aims to provide insights from artificial intelligence research to indicate the future direction of AI-driven human-centered design, taking into account the essential role of empathy. Specifically, we conduct an interdisciplinary investigation of research areas such as data-driven user studies, empathetic understanding development, and artificial empathy. Based on this foundation, we discuss the role that artificial empathy can play in human-centered 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20004;&#31181;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#20248;&#21270;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.06574</link><description>&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;: &#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Diffusion Models for Non-autoregressive Text Generation: A Survey. (arXiv:2303.06574v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#38750;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#20004;&#31181;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#21450;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#25991;&#26412;&#29983;&#25104;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#25512;&#29702;&#24310;&#36831;&#65292;&#20294;&#19981;&#24471;&#19981;&#29306;&#29298;&#29983;&#25104;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#19968;&#31867;&#28508;&#21464;&#37327;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#25193;&#25955;&#27169;&#22411;&#24050;&#34987;&#24341;&#20837;&#21040;NAR&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#25991;&#26412;&#29983;&#25104;&#36136;&#37327;&#12290;&#26412;&#32508;&#36848;&#23545;&#25193;&#25955;&#27169;&#22411;&#22312;NAR&#25991;&#26412;&#29983;&#25104;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#36827;&#34892;&#20102;&#22238;&#39038;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#36890;&#29992;&#23450;&#20041;&#21644;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;NAR&#29983;&#25104;&#20013;&#30340;&#20248;&#28857;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#29616;&#26377;&#25991;&#26412;&#25193;&#25955;&#24037;&#20316;&#20013;&#20004;&#31181;&#20027;&#27969;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22238;&#39038;&#20102;&#25193;&#25955;&#36807;&#31243;&#30340;&#20851;&#38190;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#20351;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;&#25991;&#26412;&#25968;&#25454;&#30340;&#20248;&#21270;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#65292;&#24182;&#20316;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-autoregressive (NAR) text generation has attracted much attention in the field of natural language processing, which greatly reduces the inference latency but has to sacrifice the generation accuracy. Recently, diffusion models, a class of latent variable generative models, have been introduced into NAR text generation, showing an improved text generation quality. In this survey, we review the recent progress in diffusion models for NAR text generation. As the background, we first present the general definition of diffusion models and the text diffusion models, and then discuss their merits for NAR generation. As the core content, we further introduce two mainstream diffusion models in existing work of text diffusion, and review the key designs of the diffusion process. Moreover, we discuss the utilization of pre-trained language models (PLMs) for text diffusion models and introduce optimization techniques for text data. Finally, we discuss several promising directions and conclude
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.01313</link><description>&lt;p&gt;
&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Double Permutation Equivariance for Knowledge Graph Completion. (arXiv:2302.01313v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#30340;KG&#34920;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#31070;&#32463;&#32593;&#32476;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;&#21452;&#25490;&#21015;&#31561;&#21464;&#24615;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#30693;&#35782;&#22270;&#35889;(KGs)&#24418;&#24335;&#21270;&#20026;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#65292;&#24182;&#31216;&#20043;&#20026;&#21452;&#20132;&#25442;&#23646;&#24615;&#22270;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#20108;&#20803;&#65288;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#65289;&#34920;&#31034;&#24517;&#39035;&#23545;&#33410;&#28857;&#21495;&#21644;&#36793;&#65288;&#21450;&#33410;&#28857;&#65289;&#23646;&#24615;&#65288;&#20851;&#31995;&#21644;&#33410;&#28857;&#29305;&#24449;&#65289;&#30340;&#25490;&#21015;&#31561;&#21464;&#12290;&#21452;&#37325;&#25490;&#21015;&#31561;&#21464;&#30340;KG&#34920;&#31034;&#22312;KG&#20013;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#31561;&#21464;&#24615;&#23545;&#20851;&#31995;&#30340;&#32467;&#26500;&#34920;&#31034;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;KG&#20013;&#25191;&#34892;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31561;&#21464;&#34920;&#31034;&#34013;&#22270;&#65292;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#22522;&#20110;GNN&#30340;&#21452;&#25490;&#21015;&#31561;&#21464;&#31070;&#32463;&#32467;&#26500;&#65292;&#22312;WN18RR&#12289;FB237&#21644;NELL995&#24402;&#32435;KG&#23436;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;Hits@10&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#24182;&#33021;&#22815;&#20934;&#30830;&#25191;&#34892;&#29616;&#26377;&#26041;&#27861;&#26080;&#27861;&#25191;&#34892;&#30340;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a formalization of Knowledge Graphs (KGs) as a new class of graphs that we denote doubly exchangeable attributed graphs, where node and pairwise (joint 2-node) representations must be equivariant to permutations of both node ids and edge (&amp; node) attributes (relations &amp; node features). Double-permutation equivariant KG representations open a new research direction in KGs. We show that this equivariance imposes a structural representation of relations that allows neural networks to perform complex logical reasoning tasks in KGs. Finally, we introduce a general blueprint for such equivariant representations and test a simple GNN-based double-permutation equivariant neural architecture that achieve state-of-the-art Hits@10 test accuracy in the WN18RR, FB237 and NELL995 inductive KG completion tasks, and can accurately perform logical reasoning tasks that no existing methods can perform, to the best of our knowledge.
&lt;/p&gt;</description></item><item><title>PCW&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#29616;&#25104;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#65292;&#23558;&#38271;&#19978;&#19979;&#25991;&#21010;&#20998;&#20026;&#22359;&#24182;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#37325;&#29992;&#20301;&#32622;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10947</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#34892;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Parallel Context Windows for Large Language Models. (arXiv:2212.10947v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10947
&lt;/p&gt;
&lt;p&gt;
PCW&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#29616;&#25104;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#65292;&#23558;&#38271;&#19978;&#19979;&#25991;&#21010;&#20998;&#20026;&#22359;&#24182;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#37325;&#29992;&#20301;&#32622;&#23884;&#20837;&#65292;&#25552;&#39640;&#20102;&#22788;&#29702;&#38271;&#25991;&#26412;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#38271;&#25991;&#26412;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21463;&#21040;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#38480;&#21046;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#21253;&#25324;&#35757;&#32451;&#19987;&#38376;&#30340;&#26550;&#26500;&#65292;&#20294;&#19981;&#33021;&#36731;&#26494;&#22320;&#24212;&#29992;&#20110;&#29616;&#25104;&#30340;LLM&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24182;&#34892;&#19978;&#19979;&#25991;&#31383;&#21475;&#65288;PCW&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;&#20219;&#20309;&#29616;&#25104;LLM&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38271;&#19978;&#19979;&#25991;&#21010;&#20998;&#20026;&#22359;&#65288;&#8220;&#31383;&#21475;&#8221;&#65289;&#65292;&#38480;&#21046;&#27880;&#24847;&#26426;&#21046;&#20165;&#22312;&#27599;&#20010;&#31383;&#21475;&#20869;&#24212;&#29992;&#65292;&#24182;&#36328;&#31383;&#21475;&#37325;&#29992;&#20301;&#32622;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#22312;750&#19975;&#21040;1780&#20159;&#20010;&#21442;&#25968;&#33539;&#22260;&#20869;&#30340;&#27169;&#22411;&#19978;&#27979;&#35797;PCW&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20854;&#22312;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#22312;&#20854;&#20182;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#22810;&#36339;&#38382;&#39064;&#21644;&#20351;&#29992;&#22810;&#20010;&#26816;&#32034;&#30340;&#26816;&#32034;&#22686;&#24378;&#22411;&#38382;&#31572;&#20013;&#23637;&#31034;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applied for processing long text, Large Language Models (LLMs) are limited by their context window. Existing efforts to address this limitation involve training specialized architectures, and cannot be easily applied to off-the-shelf LLMs. We present Parallel Context Windows (PCW), a method that alleviates the context window restriction for any off-the-shelf LLM without further training. The key to the approach is to carve a long context into chunks (``windows''), restrict the attention mechanism to apply only within each window, and re-use the positional embeddings across the windows. Our main results test the PCW approach on in-context learning with models that range in size between 750 million and 178 billion parameters, and show substantial improvements for tasks with diverse input and output spaces. We show additional benefits in other settings where long context windows may be beneficial: multi-hop questions and retrieval-augmented question answering with multiple retrieved 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19982;&#27969;&#34892;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#30456;&#20851;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#38750;&#32447;&#24615;&#30340;&#35282;&#33394;&#23545;&#35805;&#26641;&#29983;&#25104;&#65292;&#24182;&#35201;&#27714;&#23545;&#35805;&#24544;&#23454;&#20110;&#28216;&#25103;&#30340;&#35282;&#33394;&#24418;&#35937;&#21644;&#23454;&#20307;&#20851;&#31995;&#65292;&#23545;&#35805;&#24517;&#39035;&#20934;&#30830;&#21521;&#29609;&#23478;&#25581;&#31034;&#26032;&#30340;&#20219;&#21153;&#32454;&#33410;&#12290;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#32988;&#20219;&#36825;&#20010;&#20219;&#21153;&#20294;&#36824;&#26377;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2212.10618</link><description>&lt;p&gt;
&#38750;&#29609;&#23478;&#35282;&#33394;&#23545;&#35805;&#30340;&#26412;&#20307;&#35770;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Ontologically Faithful Generation of Non-Player Character Dialogues. (arXiv:2212.10618v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10618
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#19982;&#27969;&#34892;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#30456;&#20851;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#38750;&#32447;&#24615;&#30340;&#35282;&#33394;&#23545;&#35805;&#26641;&#29983;&#25104;&#65292;&#24182;&#35201;&#27714;&#23545;&#35805;&#24544;&#23454;&#20110;&#28216;&#25103;&#30340;&#35282;&#33394;&#24418;&#35937;&#21644;&#23454;&#20307;&#20851;&#31995;&#65292;&#23545;&#35805;&#24517;&#39035;&#20934;&#30830;&#21521;&#29609;&#23478;&#25581;&#31034;&#26032;&#30340;&#20219;&#21153;&#32454;&#33410;&#12290;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#32988;&#20219;&#36825;&#20010;&#20219;&#21153;&#20294;&#36824;&#26377;&#36827;&#19968;&#27493;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#19982;&#27969;&#34892;&#35270;&#39057;&#28216;&#25103;&#29615;&#22659;&#30456;&#20851;&#30340;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration) &#35201;&#27714;&#27169;&#22411;&#20135;&#29983;&#21453;&#26144;&#33258;&#28982;&#35821;&#35328;&#20013;&#35268;&#23450;&#20219;&#21153;&#21644;&#23454;&#20307;&#35268;&#33539;&#30340;&#35270;&#39057;&#28216;&#25103;&#35282;&#33394;&#20043;&#38388;&#23545;&#35805;&#26641;&#12290;KNUDGE &#26159;&#20174; Obsidian Entertainment's The Outer Worlds &#30340;&#28216;&#25103;&#25968;&#25454;&#20013;&#30452;&#25509;&#25552;&#21462;&#30340;&#21103;&#20219;&#21153;&#23545;&#35805;&#26500;&#24314;&#30340;&#65292;&#23548;&#33268;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#23384;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22797;&#26434;&#24615;&#65306;(1) &#23545;&#35805;&#26159;&#20998;&#25903;&#26641;&#65292;&#32780;&#19981;&#26159;&#32447;&#24615;&#30340;&#21457;&#35328;&#38142;; (2) &#35805;&#35821;&#24517;&#39035;&#24544;&#23454;&#20110;&#28216;&#25103;&#19990;&#30028;&#8212;&#8212;&#35282;&#33394;&#24418;&#35937;&#12289;&#32972;&#26223;&#21644;&#23454;&#20307;&#20851;&#31995;;&#20197;&#21450;(3) &#23545;&#35805;&#24517;&#39035;&#20934;&#30830;&#22320;&#21521;&#29609;&#23478;&#25581;&#31034;&#26032;&#30340;&#20219;&#21153;&#32454;&#33410;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#32452;&#20351;&#29992;&#30417;&#30563;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25216;&#26415;&#30340;&#31070;&#32463;&#29983;&#25104;&#27169;&#22411;&#30340;&#32467;&#26524;&#65307;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#32988;&#20219;&#36825;&#20010;&#20219;&#21153;&#20294;&#36824;&#23384;&#22312;&#25913;&#36827;&#31354;&#38388;&#65292;&#20197;&#24212;&#23545;&#21019;&#24314;&#36924;&#30495;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#23545;&#35805;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a language generation task grounded in a popular video game environment. KNUDGE (KNowledge Constrained User-NPC Dialogue GEneration) requires models to produce trees of dialogue between video game characters that accurately reflect quest and entity specifications stated in natural language. KNUDGE is constructed from side quest dialogues drawn directly from game data of Obsidian Entertainment's The Outer Worlds, leading to real-world complexities in generation: (1) dialogues are branching trees as opposed to linear chains of utterances; (2) utterances must remain faithful to the game lore- character personas, backstories, and entity relationships; and (3) a dialogue must accurately reveal new quest details to the human player. We report results for a set of neural generation models using supervised and in-context learning techniques; we find competent performance but room for future work addressing the challenges of creating realistic, game-quality dialogues.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20026;&#20803;&#20248;&#21270;&#22120;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#29702;&#35299;&#20026;&#38544;&#24335;&#24494;&#35843;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#19968;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2212.10559</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;GPT&#33021;&#22815;&#23398;&#20064;&#19978;&#19979;&#25991;&#65311;&#35821;&#35328;&#27169;&#22411;&#38544;&#24335;&#22320;&#20316;&#20026;&#20803;&#20248;&#21270;&#22120;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers. (arXiv:2212.10559v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20026;&#20803;&#20248;&#21270;&#22120;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#29702;&#35299;&#20026;&#38544;&#24335;&#24494;&#35843;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#20102;&#36825;&#19968;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#20960;&#20010;&#28436;&#31034;&#30340;&#36755;&#20837;-&#26631;&#31614;&#23545;&#65292;&#22312;&#27809;&#26377;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#30340;&#26631;&#31614;&#12290;&#23613;&#31649;&#22312;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20854;&#24037;&#20316;&#26426;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#23558;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#20026;&#20803;&#20248;&#21270;&#22120;&#65292;&#24182;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#29702;&#35299;&#20026;&#38544;&#24335;&#24494;&#35843;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;Transformer&#27880;&#24847;&#21147;&#26377;&#26799;&#24230;&#19979;&#38477;&#30340;&#21452;&#37325;&#24418;&#24335;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;ICL&#22914;&#19979;&#29702;&#35299;&#65306;GPT&#39318;&#20808;&#26681;&#25454;&#28436;&#31034;&#31034;&#20363;&#20135;&#29983;&#20803;&#26799;&#24230;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#20803;&#26799;&#24230;&#24212;&#29992;&#20110;&#21407;&#22987;GPT&#65292;&#26500;&#24314;ICL&#27169;&#22411;&#12290;&#25105;&#20204;&#20840;&#38754;&#27604;&#36739;&#20102;&#23454;&#38469;&#20219;&#21153;&#20013;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#26174;&#24335;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#25552;&#20379;&#20102;&#25903;&#25345;&#25105;&#20204;&#29702;&#35299;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#19982;&#26174;&#24335;&#24494;&#35843;&#31867;&#20284;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pretrained language models have shown surprising in-context learning (ICL) ability. With a few demonstration input-label pairs, they can predict the label for an unseen input without parameter updates. Despite the great success in performance, its working mechanism still remains an open question. In this paper, we explain language models as meta-optimizers and understand in-context learning as implicit finetuning. Theoretically, we figure out that Transformer attention has a dual form of gradient descent. On top of it, we understand ICL as follows: GPT first produces meta-gradients according to the demonstration examples, and then these meta-gradients are applied to the original GPT to build an ICL model. We comprehensively compare the behaviors of in-context learning and explicit finetuning on real tasks to provide empirical evidence that supports our understanding. Experimental results show that in-context learning behaves similarly to explicit finetuning from multiple perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#20887;&#20313;&#20449;&#24687;&#12289;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#23558;&#20854;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#27745;&#26579;&#25915;&#20987;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#21487;&#25552;&#39640;&#36817;20%&#12290;</title><link>http://arxiv.org/abs/2212.10002</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#38450;&#24481;&#35823;&#23548;&#24615;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Misinformation Attacks in Open-Domain Question Answering. (arXiv:2212.10002v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#20887;&#20313;&#20449;&#24687;&#12289;&#24182;&#36890;&#36807;&#26032;&#39062;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#23558;&#20854;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#24481;&#24320;&#25918;&#22495;&#38382;&#31572;&#31995;&#32479;&#20013;&#30340;&#27745;&#26579;&#25915;&#20987;&#65292;&#31934;&#30830;&#21305;&#37197;&#29575;&#21487;&#25552;&#39640;&#36817;20%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#25628;&#32034;&#38598;&#21512;&#36827;&#34892;&#30340;&#25932;&#23545;&#27745;&#26579;&#21487;&#33021;&#20250;&#23548;&#33268;&#29983;&#20135;&#31995;&#32479;&#30340;&#31934;&#24230;&#22823;&#24133;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#27809;&#26377;&#24037;&#20316;&#25552;&#20986;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#22823;&#22411;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#20887;&#20313;&#20449;&#24687;&#30340;&#30452;&#35273;&#12290;&#20026;&#20102;&#25214;&#21040;&#36825;&#20123;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#26597;&#35810;&#25193;&#20805;&#26469;&#25628;&#32034;&#21487;&#33021;&#22238;&#31572;&#21407;&#22987;&#38382;&#39064;&#30340;&#22810;&#26679;&#21270;&#27573;&#33853;&#38598;&#21512;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#19981;&#22826;&#21487;&#33021;&#34987;&#27745;&#26579;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#31181;&#26032;&#22411;&#30340;&#32622;&#20449;&#24230;&#26041;&#27861;&#65288;&#27604;&#36739;&#39044;&#27979;&#31572;&#26696;&#19982;&#20854;&#22312;&#26816;&#32034;&#21040;&#30340;&#19978;&#19979;&#25991;&#20013;&#20986;&#29616;&#30340;&#24773;&#20917;&#8212;&#8212;&#25105;&#20204;&#31216;&#20043;&#20026;&#31572;&#26696;&#20887;&#20313;&#32622;&#20449;&#24230;&#65292;&#21363;CAR&#65289;&#23558;&#36825;&#20123;&#26032;&#27573;&#33853;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#12290;&#36825;&#20123;&#26041;&#27861;&#20849;&#21516;&#26500;&#25104;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#24335;&#65292;&#29992;&#20110;&#38450;&#24481;&#27745;&#26579;&#25915;&#20987;&#65292;&#21487;&#22312;&#19981;&#21516;&#27700;&#24179;&#30340;&#25968;&#25454;&#27745;&#26579;/&#30693;&#35782;&#20914;&#31361;&#19979;&#25552;&#20379;&#36817;20&#65285;&#30340;&#31934;&#30830;&#21305;&#37197;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in open-domain question answering (ODQA) has shown that adversarial poisoning of the search collection can cause large drops in accuracy for production systems. However, little to no work has proposed methods to defend against these attacks. To do so, we rely on the intuition that redundant information often exists in large corpora. To find it, we introduce a method that uses query augmentation to search for a diverse set of passages that could answer the original question but are less likely to have been poisoned. We integrate these new passages into the model through the design of a novel confidence method, comparing the predicted answer to its appearance in the retrieved contexts (what we call \textit{Confidence from Answer Redundancy}, i.e. CAR). Together these methods allow for a simple but effective way to defend against poisoning attacks that provides gains of nearly 20\% exact match across varying levels of data poisoning/knowledge conflicts.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#25688;&#35201;&#34920;&#29616;&#25552;&#39640;&#65292;&#32553;&#23567;&#20854;&#19982;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2212.09593</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25688;&#35201;&#20877;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Summarization Re-ranking. (arXiv:2212.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09593
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#25688;&#35201;&#34920;&#29616;&#25552;&#39640;&#65292;&#32553;&#23567;&#20854;&#19982;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20219;&#21153;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#20852;&#36215;&#65292;&#20687;PEGASUS&#36825;&#26679;&#30340;&#25277;&#35937;&#25688;&#35201;&#27169;&#22411;&#22312;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#20013;&#25552;&#20379;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26080;&#30417;&#30563;&#27169;&#22411;&#30340;&#24615;&#33021;&#20173;&#28982;&#26126;&#26174;&#33853;&#21518;&#20110;&#23427;&#20204;&#30340;&#26377;&#30417;&#30563;&#23545;&#24212;&#29289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25688;&#35201;&#20877;&#25490;&#24207;&#26041;&#27861;&#65292;&#26088;&#22312;&#32553;&#23567;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#27169;&#22411;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23558;PEGASUS&#30340;&#30456;&#23545;&#24179;&#22343;ROUGE&#25552;&#39640;&#20102;&#26368;&#22810;7.27&#65285;&#65292;ChatGPT&#25552;&#39640;&#20102;&#26368;&#22810;6.86&#65285;&#65307;&#24182;&#19988;&#22312;30&#31181;&#38646;&#26679;&#26412;&#36716;&#31227;&#35774;&#32622;&#65288;&#22312;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#65292;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#65289;&#20013;&#65292;&#24179;&#22343;&#33719;&#24471;&#20102;7.51&#65285;&#30340;&#30456;&#23545;&#22686;&#30410;&#65288;&#20174;XSum&#21040;WikiHow&#26368;&#39640;&#21487;&#36798;23.73&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of task-specific pre-training objectives, abstractive summarization models like PEGASUS offer appealing zero-shot performance on downstream summarization tasks. However, the performance of such unsupervised models still lags significantly behind their supervised counterparts. Similarly to the supervised setup, we notice a very high variance in quality among summary candidates from these models while only one candidate is kept as the summary output. In this paper, we propose to re-rank summary candidates in an unsupervised manner, aiming to close the performance gap between unsupervised and supervised models. Our approach improves the unsupervised PEGASUS by up to 7.27% and ChatGPT by up to 6.86% relative mean ROUGE across four widely-adopted summarization benchmarks ; and achieves relative gains of 7.51% (up to 23.73% from XSum to WikiHow) averaged over 30 zero-shot transfer setups (finetuning on a dataset, evaluating on another).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVGRU&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27719;&#24635;&#21464;&#37327;&#26469;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#36718;&#23545;&#35805;&#22238;&#22797;&#65292;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.09086</link><description>&lt;p&gt;
PVGRU&#65306;&#36890;&#36807;Pseudo-Variational&#26426;&#21046;&#29983;&#25104;&#22810;&#26679;&#19988;&#30456;&#20851;&#30340;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
PVGRU: Generating Diverse and Relevant Dialogue Responses via Pseudo-Variational Mechanism. (arXiv:2212.09086v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09086
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;PVGRU&#30340;&#32452;&#20214;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#27719;&#24635;&#21464;&#37327;&#26469;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#65292;&#20174;&#32780;&#20248;&#21270;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22810;&#36718;&#23545;&#35805;&#22238;&#22797;&#65292;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#29983;&#25104;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#30340;&#22238;&#22797;&#29983;&#25104;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;RNN&#65288;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26368;&#21518;&#38544;&#34255;&#30340;&#29366;&#24577;&#26469;&#27719;&#24635;&#24207;&#21015;&#65292;&#36825;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#19981;&#21516;&#23545;&#35805;&#20013;&#35266;&#23519;&#21040;&#30340;&#24494;&#22937;&#21464;&#21270;&#65292;&#24182;&#19988;&#19981;&#33021;&#21306;&#20998;&#22312;&#26500;&#25104;&#26041;&#38754;&#30456;&#20284;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Pseudo-Variational Gated Recurrent Unit&#65288;PVGRU&#65289;&#32452;&#20214;&#65292;&#26080;&#38656;&#21518;&#39564;&#30693;&#35782;&#21363;&#21487;&#23558;&#27719;&#24635;&#21464;&#37327;&#24341;&#20837;GRU&#65292;&#20854;&#21487;&#20197;&#32858;&#21512;&#23376;&#24207;&#21015;&#30340;&#32047;&#31215;&#20998;&#24067;&#21464;&#21270;&#12290; PVGRU&#21487;&#20197;&#36890;&#36807;&#24635;&#32467;&#21464;&#37327;&#24863;&#30693;&#24494;&#22937;&#30340;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#20123;&#21464;&#21270;&#26159;&#36890;&#36807;&#35774;&#35745;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#21644;&#37325;&#26500;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;PVGRU&#26500;&#24314;&#20102;Pseudo-Variational Hierarchical Dialogue&#65288;PVHD&#65289;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PVGRU&#21487;&#20197;&#24191;&#27867;&#25552;&#39640;&#23545;&#35805;&#27169;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate response generation for multi-turn dialogue in generative-based chatbots. Existing generative models based on RNNs (Recurrent Neural Networks) usually employ the last hidden state to summarize the sequences, which makes models unable to capture the subtle variability observed in different dialogues and cannot distinguish the differences between dialogues that are similar in composition. In this paper, we propose a Pseudo-Variational Gated Recurrent Unit (PVGRU) component without posterior knowledge through introducing a recurrent summarizing variable into the GRU, which can aggregate the accumulated distribution variations of subsequences. PVGRU can perceive the subtle semantic variability through summarizing variables that are optimized by the devised distribution consistency and reconstruction objectives. In addition, we build a Pseudo-Variational Hierarchical Dialogue (PVHD) model based on PVGRU. Experimental results demonstrate that PVGRU can broadly improve the dive
&lt;/p&gt;</description></item><item><title>TRIP&#27169;&#22411;&#21033;&#29992;&#25991;&#26723;&#32423;&#19977;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#25913;&#36827;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.07752</link><description>&lt;p&gt;
&#25512;&#21160;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65306;TRIP&#19977;&#35282;&#24418;&#25991;&#26723;&#32423;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Advancing Multilingual Pre-training: TRIP Triangular Document-level Pre-training for Multilingual Language Models. (arXiv:2212.07752v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07752
&lt;/p&gt;
&lt;p&gt;
TRIP&#27169;&#22411;&#21033;&#29992;&#25991;&#26723;&#32423;&#19977;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#25913;&#36827;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#22810;&#39033;&#20219;&#21153;&#19978;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#39044;&#35757;&#32451;&#33719;&#24471;&#25104;&#21151;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#21253;&#21547;&#22810;&#31181;&#35821;&#35328;&#30340;&#21333;&#35821;&#35328;&#25991;&#26723;&#32423;&#35821;&#26009;&#24211;&#12289;&#21477;&#23376;&#32423;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#26377;&#26102;&#36824;&#21033;&#29992;&#21512;&#25104;&#30340;&#25991;&#26723;&#32423;&#21452;&#35821;&#35821;&#26009;&#24211;&#12290;&#36825;&#38459;&#30861;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20219;&#21153;&#65288;&#22914;&#25991;&#26723;&#32423;&#32763;&#35793;&#65289;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#25991;&#26723;&#32423;&#19977;&#35821;&#35328;&#24179;&#34892;&#35821;&#26009;&#24211;&#25913;&#36827;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;TRIP&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26032;&#39062;&#30340;&#19977;&#35282;&#24418;&#39044;&#35757;&#32451;&#27169;&#24335;&#21033;&#29992;&#19977;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#22312;&#21508;&#31181;&#36328;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of multilingual sequence-to-sequence pre-training, most existing approaches rely on document-level monolingual corpora in many different languages, sentence-level bilingual corpora,\footnote{In this paper, we use `bilingual corpora' to denote parallel corpora with `bilingual translation pairs' in many different language pairs, each consisting of two sentences/documents with the same meaning written in different languages. We use `trilingual corpora' to denote parallel corpora with `trilingual translation pairs' in many different language combinations, each consisting of three sentences/documents.} and sometimes synthetic document-level bilingual corpora. This hampers the performance with cross-lingual document-level tasks such as document-level translation. Therefore, we propose to mine and leverage document-level trilingual parallel corpora to improve sequence-to-sequence multilingual pre-training. We present \textbf{Tri}angular Document-level \textbf{P}re-training
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2212.07530</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#21407;&#22240;&#21644;&#35299;&#20915;&#26041;&#27861;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causes and Cures for Interference in Multilingual Translation. (arXiv:2212.07530v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07530
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#31350;&#20102;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#21457;&#29616;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#65292;&#21516;&#26102;&#21457;&#29616;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21487;&#20197;&#20174;&#19981;&#21516;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#21327;&#21516;&#20013;&#33719;&#30410;&#65292;&#20294;&#21516;&#26102;&#20063;&#20250;&#21463;&#21040;&#24178;&#25200;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#20808;&#36827;&#26041;&#27861;&#26088;&#22312;&#28040;&#38500;&#24178;&#25200;&#65292;&#20294;&#25105;&#20204;&#23545;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#23548;&#33268;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#20013;&#24178;&#25200;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#21270;&#35797;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24178;&#25200;&#65288;&#25110;&#21327;&#21516;&#65289;&#20027;&#35201;&#30001;&#27169;&#22411;&#22823;&#23567;&#12289;&#25968;&#25454;&#22823;&#23567;&#21644;&#27599;&#20010;&#35821;&#35328;&#23545;&#22312;&#24635;&#25968;&#25454;&#38598;&#20013;&#25152;&#21344;&#27604;&#20363;&#26469;&#20915;&#23450;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#27169;&#22411;&#30456;&#23545;&#20110;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38750;&#24120;&#23567;&#30340;&#26102;&#20505;&#65292;&#20250;&#20986;&#29616;&#20005;&#37325;&#30340;&#24178;&#25200;&#65292;&#32780;&#20351;&#29992;&#19981;&#21040;10&#20159;&#21442;&#25968;&#30340;&#26631;&#20934;Transformer&#37197;&#32622;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#24178;&#25200;&#24182;&#20419;&#36827;&#21327;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#35843;&#25972;&#37319;&#26679;&#28201;&#24230;&#20197;&#25511;&#21046;&#25968;&#25454;&#20013;&#27599;&#20010;&#35821;&#35328;&#23545;&#25152;&#21344;&#27604;&#20363;&#30340;&#26041;&#27861;&#26159;&#24179;&#34913;&#35821;&#35328;&#23545;&#20043;&#38388;&#20851;&#31995;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.03760</link><description>&lt;p&gt;
&#35821;&#35328;&#24314;&#27169;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65306;&#20016;&#23500;&#20219;&#21153;&#29305;&#23450;&#21644;&#20219;&#21153;&#26080;&#20851;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning. (arXiv:2212.03760v3 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#21487;&#20197;&#22312;&#19981;&#21516;&#25512;&#33616;&#20219;&#21153;&#20013;&#21462;&#24471;&#20248;&#24322;&#32467;&#26524;&#65292;&#24182;&#19988;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#26469;&#33258;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#30340;&#32479;&#19968;&#29992;&#25143;&#24314;&#27169;&#26694;&#26550;&#12290;&#20854;&#20013;&#35768;&#22810;&#21463;&#30410;&#20110;&#23558;&#29992;&#25143;&#34892;&#20026;&#24207;&#21015;&#20316;&#20026;&#32431;&#25991;&#26412;&#20351;&#29992;&#65292;&#20195;&#34920;&#30528;&#20219;&#20309;&#39046;&#22495;&#25110;&#31995;&#32479;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#32780;&#19981;&#22833;&#36890;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#19968;&#20010;&#38382;&#39064;&#20135;&#29983;&#20102;&#65306;&#29992;&#25143;&#21382;&#21490;&#35821;&#35328;&#24314;&#27169;&#33021;&#21542;&#24110;&#21161;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#65311;&#34429;&#28982;&#35821;&#35328;&#24314;&#27169;&#30340;&#22810;&#21151;&#33021;&#24615;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#20854;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#20173;&#26410;&#28145;&#20837;&#25506;&#35752;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30452;&#25509;&#24212;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#29992;&#25143;&#21382;&#21490;&#30340;&#35821;&#35328;&#24314;&#27169;&#22312;&#19981;&#21516;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#21487;&#20197;&#21462;&#24471;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;&#20219;&#21153;&#26080;&#20851;&#30340;&#29992;&#25143;&#21382;&#21490;&#36824;&#21487;&#20197;&#25552;&#20379;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20026;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#26410;&#30693;&#22495;&#21644;&#26381;&#21153;&#19978;&#20063;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have proposed unified user modeling frameworks that leverage user behavior data from various applications. Many of them benefit from utilizing users' behavior sequences as plain texts, representing rich information in any domain or system without losing generality. Hence, a question arises: Can language modeling for user history corpus help improve recommender systems? While its versatile usability has been widely investigated in many domains, its applications to recommender systems still remain underexplored. We show that language modeling applied directly to task-specific user histories achieves excellent results on diverse recommendation tasks. Also, leveraging additional task-agnostic user histories delivers significant performance benefits. We further demonstrate that our approach can provide promising transfer learning capabilities for a broad spectrum of real-world recommender systems, even on unseen domains and services.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;"&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#23884;&#20837;&#27169;&#22411;"&#65288;LCGE&#65289;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26102;&#38388;&#25935;&#24863;&#24615;&#34920;&#31034;&#65288;&#28041;&#21450;&#26102;&#24577;&#21644;&#22240;&#26524;&#24615;&#30340;&#20107;&#20214;&#34920;&#31034;&#65289;&#19982;&#24120;&#35782;&#35282;&#24230;&#19978;&#30340;&#26102;&#38388;&#26080;&#20851;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#34920;&#31034;&#20107;&#20214;&#30340;&#23454;&#26102;&#24615;&#21644;&#22240;&#26524;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.16865</link><description>&lt;p&gt;
&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Logic and Commonsense-Guided Temporal Knowledge Graph Completion. (arXiv:2211.16865v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;"&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#23884;&#20837;&#27169;&#22411;"&#65288;LCGE&#65289;&#65292;&#36890;&#36807;&#20849;&#21516;&#23398;&#20064;&#26102;&#38388;&#25935;&#24863;&#24615;&#34920;&#31034;&#65288;&#28041;&#21450;&#26102;&#24577;&#21644;&#22240;&#26524;&#24615;&#30340;&#20107;&#20214;&#34920;&#31034;&#65289;&#19982;&#24120;&#35782;&#35282;&#24230;&#19978;&#30340;&#26102;&#38388;&#26080;&#20851;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#34920;&#31034;&#20107;&#20214;&#30340;&#23454;&#26102;&#24615;&#21644;&#22240;&#26524;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#65288;TKG&#65289;&#23384;&#20648;&#19982;&#26102;&#38388;&#26377;&#20851;&#30340;&#25968;&#25454;&#20107;&#20214;&#12290;&#39044;&#27979;&#20107;&#20214;&#30001;&#20110;&#20854;&#26102;&#24577;&#29305;&#24615;&#32780;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#20197;&#24448;&#30340;TKG&#34917;&#20840;&#26041;&#27861;&#26080;&#27861;&#21516;&#26102;&#34920;&#31034;&#20107;&#20214;&#30340;&#23454;&#26102;&#24615;&#21644;&#22240;&#26524;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#36923;&#36753;&#21644;&#24120;&#35782;&#24341;&#23548;&#30340;&#23884;&#20837;&#27169;&#22411;&#8221;&#65288;LCGE&#65289;&#65292;&#33021;&#22815;&#20849;&#21516;&#23398;&#20064;&#20107;&#20214;&#30340;&#26102;&#38388;&#25935;&#24863;&#24615;&#34920;&#31034;&#65288;&#28041;&#21450;&#26102;&#24577;&#21644;&#22240;&#26524;&#24615;&#30340;&#20107;&#20214;&#34920;&#31034;&#65289;&#19982;&#24120;&#35782;&#35282;&#24230;&#19978;&#30340;&#26102;&#38388;&#26080;&#20851;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#35268;&#21017;&#23398;&#20064;&#31639;&#27861;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#35268;&#21017;&#24341;&#23548;&#30340;&#35859;&#35789;&#23884;&#20837;&#35268;&#33539;&#31574;&#30053;&#65292;&#23398;&#20064;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#36741;&#21161;&#24120;&#35782;&#30693;&#35782;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20107;&#20214;&#30340;&#21512;&#29702;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;TKGC&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
A temporal knowledge graph (TKG) stores the events derived from the data involving time. Predicting events is extremely challenging due to the time-sensitive property of events. Besides, the previous TKG completion (TKGC) approaches cannot represent both the timeliness and the causality properties of events, simultaneously. To address these challenges, we propose a Logic and Commonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive representation involving timeliness and causality of events, together with the time-independent representation of events from the perspective of commonsense. Specifically, we design a temporal rule learning algorithm to construct a rule-guided predicate embedding regularization strategy for learning the causality among events. Furthermore, we could accurately evaluate the plausibility of events via auxiliary commonsense knowledge. The experimental results of TKGC task illustrate the significant performance improvements of our model compar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;data2vec-aqc&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2211.01246</link><description>&lt;p&gt;
data2vec-aqc&#65306;&#22312;&#24072;&#29983;&#35757;&#32451;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#21161;&#25945;&#12290;
&lt;/p&gt;
&lt;p&gt;
data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup. (arXiv:2211.01246v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;data2vec-aqc&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#65292;&#35813;&#31639;&#27861;&#22312;&#35821;&#38899;&#35782;&#21035;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#8212;&#8212;data2vec-aqc&#65292;&#29992;&#20110;&#20174;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25913;&#36827;&#22312;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#37117;&#24456;&#26377;&#38480;&#30340;&#35821;&#38899;&#39046;&#22495;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#12290;&#22312;&#26368;&#36817;&#24341;&#20837;&#30340;data2vec&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#27169;&#22359;&#26469;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#12289;&#37327;&#21270;&#34920;&#31034;&#21644;&#32858;&#31867;&#30340;&#20248;&#21183;&#12290;&#36825;&#20123;&#27169;&#22359;&#20043;&#38388;&#30340;&#20132;&#20114;&#24110;&#21161;&#35299;&#20915;&#20102;&#20132;&#21449;&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#39069;&#22806;&#30340;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#12290;&#22312;&#27809;&#26377;&#20351;&#29992;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#24773;&#20917;&#19979;&#65292;data2vec-aqc&#22312;LibriSpeech&#30340;test-clean&#38598;&#21644;test-other&#38598;&#19978;&#30456;&#23545;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;data2vec&#31995;&#32479;&#20998;&#21035;&#21462;&#24471;&#20102;14.1&#65285;&#21644;20.9&#65285;&#30340;&#30456;&#23545;WER&#25552;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Switchboard&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#24494;&#35843;&#26102;&#20063;&#21487;&#20197;&#33719;&#24471;&#39640;&#36798;17.8&#65285;&#30340;&#30456;&#23545;WER&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec, we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8\% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/dat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#32622;&#26469;&#35780;&#20272;&#27169;&#22411;&#22312;&#33521;&#35821;&#36807;&#21435;&#24335;&#35789;&#24418;&#21464;&#21270;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#35268;&#21017;&#21160;&#35789;&#19978;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#38750;&#35268;&#21017;&#21160;&#35789;&#19978;&#30340;&#34920;&#29616;&#30053;&#26377;&#25552;&#39640;&#12290;&#19981;&#21516;&#34892;&#20026;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#31526;&#21495;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.09167</link><description>&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#21040;&#36798;&#37027;&#37324;&#65311;&#35780;&#20272;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#33521;&#35821;&#36807;&#21435;&#24335;&#35789;&#24418;&#21464;&#21270;&#30340;&#35748;&#30693;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we get there? Evaluating transformer neural networks as cognitive models for English past tense inflection. (arXiv:2210.09167v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#32622;&#26469;&#35780;&#20272;&#27169;&#22411;&#22312;&#33521;&#35821;&#36807;&#21435;&#24335;&#35789;&#24418;&#21464;&#21270;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#21457;&#29616;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#35268;&#21017;&#21160;&#35789;&#19978;&#26377;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#22312;&#38750;&#35268;&#21017;&#21160;&#35789;&#19978;&#30340;&#34920;&#29616;&#30053;&#26377;&#25552;&#39640;&#12290;&#19981;&#21516;&#34892;&#20026;&#34920;&#26126;&#27169;&#22411;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#31526;&#21495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#25484;&#25569;&#35821;&#35328;&#30340;&#20934;&#27491;&#21017;&#24615;&#19968;&#30452;&#23384;&#22312;&#20105;&#35758;&#12290;&#22312;&#20856;&#22411;&#30340;&#20934;&#27491;&#21017;&#24615;&#20219;&#21153;&#65292;&#33521;&#35821;&#30340;&#36807;&#21435;&#24335;&#35789;&#24418;&#21464;&#21270;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#25209;&#35780;&#65292;&#35748;&#20026;&#23427;&#21482;&#23398;&#20064;&#20102;&#26368;&#24120;&#35265;&#30340;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#35268;&#21017;&#30340;&#27169;&#24335;&#65292;&#22240;&#27492;&#26080;&#27861;&#23398;&#20064;&#27491;&#21017;&#21644;&#38750;&#27491;&#21017;&#30340;&#25277;&#35937;&#31867;&#21035;&#65292;&#19982;&#20154;&#31867;&#34920;&#29616;&#19981;&#21516;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#35774;&#32622;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20197;&#26816;&#26597;&#23427;&#20204;&#22312;&#36825;&#39033;&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#35268;&#21017;&#21160;&#35789;&#30340;&#20934;&#30830;&#29575;&#24456;&#39640;&#65292;&#23545;&#26410;&#35265;&#36807;&#30340;&#38750;&#35268;&#21017;&#21160;&#35789;&#30340;&#20934;&#30830;&#29575;&#30053;&#26377;&#25552;&#39640;&#12290;&#27169;&#22411;&#22312;&#35268;&#21017;&#21160;&#35789;&#19978;&#30340;&#34920;&#29616;&#21463;&#21040;&#31867;&#22411;&#39057;&#29575;&#21644;&#27604;&#29575;&#30340;&#24433;&#21709;&#65292;&#32780;&#19981;&#26159;&#20196;&#29260;&#39057;&#29575;&#21644;&#27604;&#29575;&#65292;&#32780;&#22312;&#38750;&#35268;&#21017;&#21160;&#35789;&#19978;&#21017;&#30456;&#21453;&#12290;&#27169;&#22411;&#22312;&#35268;&#21017;&#21160;&#35789;&#21644;&#38750;&#35268;&#21017;&#21160;&#35789;&#19978;&#19981;&#21516;&#30340;&#34892;&#20026;&#34920;&#26126;&#23427;&#20204;&#22312;&#21160;&#35789;&#30340;&#35268;&#21017;&#24615;&#26041;&#38754;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#31526;&#21495;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24494;&#24369;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is an ongoing debate on whether neural networks can grasp the quasi-regularities in languages like humans. In a typical quasi-regularity task, English past tense inflections, the neural network model has long been criticized that it learns only to generalize the most frequent pattern, but not the regular pattern, thus can not learn the abstract categories of regular and irregular and is dissimilar to human performance. In this work, we train a set of transformer models with different settings to examine their behavior on this task. The models achieved high accuracy on unseen regular verbs and some accuracy on unseen irregular verbs. The models' performance on the regulars is heavily affected by type frequency and ratio but not token frequency and ratio, and vice versa for the irregulars. The different behaviors on the regulars and irregulars suggest that the models have some degree of symbolic learning on the regularity of the verbs. In addition, the models are weakly correlated 
&lt;/p&gt;</description></item><item><title>KALM&#26159;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#25991;&#26723;&#32423;&#21035;&#21644;&#20840;&#23616;&#35821;&#22659;&#20013;&#32852;&#21512;&#21033;&#29992;&#30693;&#35782;&#65292;&#29992;&#20110;&#38271;&#25991;&#26723;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2210.04105</link><description>&lt;p&gt;
KALM: &#30693;&#35782;&#24863;&#30693;&#30340;&#26412;&#22320;&#12289;&#25991;&#26723;&#21644;&#20840;&#23616;&#32972;&#26223;&#38598;&#25104;&#65292;&#29992;&#20110;&#38271;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
KALM: Knowledge-Aware Integration of Local, Document, and Global Contexts for Long Document Understanding. (arXiv:2210.04105v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04105
&lt;/p&gt;
&lt;p&gt;
KALM&#26159;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#25991;&#26723;&#32423;&#21035;&#21644;&#20840;&#23616;&#35821;&#22659;&#20013;&#32852;&#21512;&#21033;&#29992;&#30693;&#35782;&#65292;&#29992;&#20110;&#38271;&#25991;&#26723;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LMs) &#30340;&#20986;&#29616;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#20110;&#23558;&#24120;&#35782;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#27880;&#20837;&#21040; LMs &#20013;&#20197;&#20934;&#22791;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#24037;&#20316;&#35797;&#22270;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65292;&#21363;&#31526;&#21495;&#30693;&#35782;&#34920;&#31034;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451; LMs&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#21033;&#29992;&#20102;&#22806;&#37096;&#30693;&#35782;&#65292;&#20294;&#30446;&#21069;&#22914;&#20309;&#20849;&#21516;&#32467;&#21512;&#20195;&#34920;&#19981;&#21516;&#35821;&#22659;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20174;&#26412;&#22320; (&#20363;&#22914;&#65292;&#21477;&#23376;)&#12289;&#25991;&#26723;&#32423;&#21035;&#12289;&#20840;&#23616;&#30693;&#35782;&#31561;&#26041;&#38754;&#36827;&#34892;&#30693;&#35782;&#20016;&#23500;&#30340;&#20132;&#25442;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#26679;&#30340;&#20016;&#23500;&#35821;&#22659;&#21270;&#23545;&#20110;&#38271;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#23588;&#20854;&#26377;&#30410;&#65292;&#22240;&#20026;&#26631;&#20934;&#39044;&#20808;&#35757;&#32451;&#30340; LMs &#36890;&#24120;&#21463;&#38480;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;&#32771;&#34385;&#21040;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; KALM&#65292;&#19968;&#31181;&#30693;&#35782;&#24863;&#30693;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#32852;&#21512;&#21033;&#29992;&#26412;&#22320;&#12289;&#25991;&#26723;&#32423;&#21035;&#21644;&#20840;&#23616;&#35821;&#22659;&#20013;&#30340;&#30693;&#35782;&#65292;&#29992;&#20110;&#38271;&#25991;&#26723;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of pretrained language models (LMs), increasing research efforts have been focusing on infusing commonsense and domain-specific knowledge to prepare LMs for downstream tasks. These works attempt to leverage knowledge graphs, the de facto standard of symbolic knowledge representation, along with pretrained LMs. While existing approaches have leveraged external knowledge, it remains an open question how to jointly incorporate knowledge graphs representing varying contexts, from local (e.g., sentence), to document-level, to global knowledge, to enable knowledge-rich exchange across these contexts. Such rich contextualization can be especially beneficial for long document understanding tasks since standard pretrained LMs are typically bounded by the input sequence length. In light of these challenges, we propose KALM, a Knowledge-Aware Language Model that jointly leverages knowledge in local, document-level, and global contexts for long document understanding. KALM first en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20132;&#21449;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#22686;&#24378;&#25955;&#24230;&#12289;&#20132;&#21449;&#23545;&#27604;&#25439;&#22833;&#21644;&#32858;&#31867;&#27169;&#22359;&#65292;&#30456;&#23545;&#20110;wav2vec 2.0&#22522;&#32447;&#22312;LibriSpeech&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;12%&#30340;WER&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.02592</link><description>&lt;p&gt;
CCC-wav2vec 2.0&#65306;&#22522;&#20110;&#32858;&#31867;&#30340;&#20132;&#21449;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CCC-wav2vec 2.0: Clustering aided Cross Contrastive Self-supervised learning of speech representations. (arXiv:2210.02592v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#20132;&#21449;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#35821;&#38899;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#22686;&#24378;&#25955;&#24230;&#12289;&#20132;&#21449;&#23545;&#27604;&#25439;&#22833;&#21644;&#32858;&#31867;&#27169;&#22359;&#65292;&#30456;&#23545;&#20110;wav2vec 2.0&#22522;&#32447;&#22312;LibriSpeech&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#36229;&#36807;12%&#30340;WER&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#24050;&#32463;&#24110;&#21161;&#25105;&#20204;&#20174;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21363;ccc-wav2vec 2.0&#65292;&#35813;&#31574;&#30053;&#20351;&#29992;&#32858;&#31867;&#21644;&#22522;&#20110;&#22686;&#24378;&#30340;&#20132;&#21449;&#23545;&#27604;&#25439;&#22833;&#20316;&#20026;&#20854;&#33258;&#30417;&#30563;&#30446;&#26631;&#12290;&#36890;&#36807;&#32858;&#31867;&#27169;&#22359;&#65292;&#25105;&#20204;&#38477;&#20302;&#20102;&#19982;&#27491;&#26679;&#26412;&#39640;&#24230;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#20132;&#21449;&#23545;&#27604;&#25439;&#22833;&#22312;&#21407;&#22987;&#26679;&#26412;&#30340;&#32534;&#30721;&#22120;&#36755;&#20986;&#21644;&#20854;&#22686;&#24378;&#29256;&#30340;&#37327;&#21270;&#22120;&#36755;&#20986;&#20043;&#38388;&#20197;&#21450;&#21453;&#20043;&#35745;&#31639;&#65292;&#20174;&#32780;&#20026;&#39044;&#35757;&#32451;&#31574;&#30053;&#24102;&#26469;&#20102;&#40065;&#26834;&#24615;&#12290;ccc-wav2vec 2.0&#22312;LibriSpeech&#30340;&#27979;&#35797;-&#24178;&#20928;&#38598;&#21644;&#27979;&#35797;-&#20854;&#20182;&#38598;&#19978;&#30456;&#23545;&#20110;wav2vec 2.0&#22522;&#32447;&#20998;&#21035;&#21462;&#24471;&#20102;15.6&#65285;&#21644;12.7&#65285;&#30340;&#30456;&#23545;WER&#25913;&#36827;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#35821;&#35328;&#27169;&#22411;&#12290;&#24403;&#22312;Switchboard&#19978;&#24494;&#35843;&#26102;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#30456;&#23545;&#20110;wav2vec 2.0&#22522;&#32447;&#21462;&#24471;&#39640;&#36798;14.9&#65285;&#30340;&#30456;&#23545;WER&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Self-Supervised Learning has helped reap the benefit of the scale from the available unlabeled data, the learning paradigms are continuously being bettered. We present a new pre-training strategy named ccc-wav2vec 2.0, which uses clustering and an augmentation-based cross-contrastive loss as its self-supervised objective. Through the clustering module, we scale down the influence of those negative examples that are highly similar to the positive. The Cross-Contrastive loss is computed between the encoder output of the original sample and the quantizer output of its augmentation and vice-versa, bringing robustness to the pre-training strategy. ccc-wav2vec 2.0 achieves up to 15.6% and 12.7% relative WER improvement over the baseline wav2vec 2.0 on the test-clean and test-other sets, respectively, of LibriSpeech, without the use of any language model. The proposed method also achieves up to 14.9% relative WER improvement over the baseline wav2vec 2.0 when fine-tuned on Switchboard d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.15206</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20160;&#20040;&#26356;&#36866;&#21512;&#38646;&#26679;&#26412;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes Pre-trained Language Models Better Zero-shot Learners?. (arXiv:2209.15206v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22522;&#20110;&#27492;&#25552;&#20986;&#20102;&#19968;&#20010;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#26469;&#35299;&#37322;prompt learning&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20256;&#32479;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#20250;&#22240;&#20026;&#36807;&#25311;&#21512;&#19981;&#20855;&#20195;&#34920;&#24615;&#30340;&#26631;&#27880;&#25968;&#25454;&#32780;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;prompt learning&#26356;&#26377;&#25928;&#30340;&#20551;&#35774;&#65292;&#22240;&#20026;&#23427;&#20351;&#24314;&#31435;&#22312;&#28023;&#37327;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#39046;&#22495;&#30456;&#20851;&#20154;&#31867;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#22810;&#22320;&#21442;&#19982;&#39044;&#27979;&#65292;&#20174;&#32780;&#20943;&#23569;&#23567;&#22411;&#35757;&#32451;&#38598;&#25552;&#20379;&#30340;&#26377;&#38480;&#26631;&#31614;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#65292;&#35821;&#35328;&#24046;&#24322;&#21487;&#20197;&#34913;&#37327;&#25552;&#31034;&#36136;&#37327;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#26356;&#20026;&#37325;&#35201;&#30340;&#26159;&#65292;&#21463;&#21040;&#29702;&#35770;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#27880;&#37322;&#26080;&#20851;&#30340;&#27169;&#26495;&#36873;&#25321;&#26041;&#27861;&#65292;&#21487;&#20197;&#20107;&#20808;&#8220;&#39044;&#27979;&#8221;&#25552;&#31034;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#20540;&#24471;&#40723;&#21169;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a theoretical framework to explain the efficacy of prompt learning in zero/few-shot scenarios. First, we prove that conventional pre-training and fine-tuning paradigm fails in few-shot scenarios due to overfitting the unrepresentative labelled data. We then detail the assumption that prompt learning is more effective because it empowers pre-trained language model that is built upon massive text corpora, as well as domain-related human knowledge to participate more in prediction and thereby reduces the impact of limited label information provided by the small training set. We further hypothesize that language discrepancy can measure the quality of prompting. Comprehensive experiments are performed to verify our assumptions. More remarkably, inspired by the theoretical framework, we propose an annotation-agnostic template selection method based on perplexity, which enables us to ``forecast'' the prompting performance in advance. This approach is especially encou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BMA-SBT &#30340;&#26032;&#22411;&#36890;&#29992;&#22810;&#35821;&#35328;&#21327;&#35758;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992; switched BT &#26041;&#27861;&#65292;&#20351;&#24471;&#19981;&#38656;&#35201;&#24179;&#34892;&#25968;&#25454;&#23601;&#21487;&#20197;&#24314;&#31435;&#22810;&#35821;&#35328;&#21327;&#35758;&#65292;&#21516;&#26102;&#20351;&#29992; Kullback-Leibler &#25955;&#24230;&#25439;&#22833;&#21452;&#21521;&#20248;&#21270;&#21327;&#35758;&#65292;&#22312; MNMT &#20219;&#21153;&#19978;&#26126;&#26174;&#25913;&#21892;&#20102;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2209.13940</link><description>&lt;p&gt;
&#36890;&#36807; Switched Back-translation &#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#21452;&#21521;&#21319;&#32423;&#30340;&#22810;&#35821;&#35328;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Revamping Multilingual Agreement Bidirectionally via Switched Back-translation for Multilingual Neural Machine Translation. (arXiv:2209.13940v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; BMA-SBT &#30340;&#26032;&#22411;&#36890;&#29992;&#22810;&#35821;&#35328;&#21327;&#35758;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992; switched BT &#26041;&#27861;&#65292;&#20351;&#24471;&#19981;&#38656;&#35201;&#24179;&#34892;&#25968;&#25454;&#23601;&#21487;&#20197;&#24314;&#31435;&#22810;&#35821;&#35328;&#21327;&#35758;&#65292;&#21516;&#26102;&#20351;&#29992; Kullback-Leibler &#25955;&#24230;&#25439;&#22833;&#21452;&#21521;&#20248;&#21270;&#21327;&#35758;&#65292;&#22312; MNMT &#20219;&#21153;&#19978;&#26126;&#26174;&#25913;&#21892;&#20102;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#35821;&#35328;&#21327;&#35758; (MA) &#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (MNMT) &#20013;&#26174;&#31034;&#20986;&#20854;&#37325;&#35201;&#24615;&#65292;&#20294;&#24403;&#21069;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#65306;(i)&#38656;&#35201;&#22810;&#20010;&#35821;&#35328;&#23545;&#20043;&#38388;&#30340;&#24179;&#34892;&#25968;&#25454;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#29616;&#23454;&#65292;&#24182;&#19988;(ii)&#22312;&#19968;&#20010;&#27169;&#26865;&#20004;&#21487;&#30340;&#26041;&#21521;&#19978;&#20248;&#21270;&#21327;&#35758;&#65292;&#36825;&#20250;&#22952;&#30861;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#36890;&#29992;&#22810;&#35821;&#35328;&#21327;&#35758;&#26694;&#26550;&#65292;&#21517;&#20026;&#32463;&#30001; Switched Back-translation &#30340;&#21452;&#21521;&#22810;&#35821;&#35328;&#21327;&#35758; (BMA-SBT)&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340; MNMT &#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026; switched BT &#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#21478;&#19968;&#31181;&#28304;&#35821;&#35328;&#20013;&#21019;&#24314;&#29992;&#20110;&#32763;&#35793;&#30446;&#26631;&#30340;&#21512;&#25104;&#25991;&#26412;&#65292;&#20351;&#24471;&#19981;&#38656;&#35201;&#21069;&#36848;&#30340;&#24179;&#34892;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992; Kullback-Leibler &#25955;&#24230;&#25439;&#22833;&#21452;&#21521;&#20248;&#21270;&#21327;&#35758;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;BMA-SBT &#22312; MNMT &#20219;&#21153;&#19978;&#26126;&#26174;&#25913;&#21892;&#20102;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the fact that multilingual agreement (MA) has shown its importance for multilingual neural machine translation (MNMT), current methodologies in the field have two shortages: (i) require parallel data between multiple language pairs, which is not always realistic and (ii) optimize the agreement in an ambiguous direction, which hampers the translation performance. We present \textbf{B}idirectional \textbf{M}ultilingual \textbf{A}greement via \textbf{S}witched \textbf{B}ack-\textbf{t}ranslation (\textbf{BMA-SBT}), a novel and universal multilingual agreement framework for fine-tuning pre-trained MNMT models, which (i) exempts the need for aforementioned parallel data by using a novel method called switched BT that creates synthetic text written in another source language using the translation target and (ii) optimizes the agreement bidirectionally with the Kullback-Leibler Divergence loss. Experiments indicate that BMA-SBT clearly improves the strong baselines on the task of MNMT 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24448;&#36820;&#32763;&#35793;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#38500;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#21518;&#65292;&#24448;&#36820;&#32763;&#35793;&#21487;&#20197;&#24688;&#24403;&#22320;&#21453;&#26144;&#21069;&#21521;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2209.07351</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#30340;&#24448;&#36820;&#32763;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rethinking Round-Trip Translation for Machine Translation Evaluation. (arXiv:2209.07351v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.07351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24448;&#36820;&#32763;&#35793;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#38500;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#21518;&#65292;&#24448;&#36820;&#32763;&#35793;&#21487;&#20197;&#24688;&#24403;&#22320;&#21453;&#26144;&#21069;&#21521;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#32763;&#35793;&#30340;&#33258;&#21160;&#35780;&#20272;&#20013;&#23384;&#22312;&#24179;&#34892;&#35821;&#26009;&#24211;&#30340;&#19981;&#36275;&#12290;&#24448;&#36820;&#32763;&#35793;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24039;&#22937;&#19988;&#31616;&#21333;&#30340;&#25216;&#26415;&#26469;&#32531;&#35299;&#24179;&#34892;&#35780;&#20272;&#35821;&#26009;&#24211;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#22312;&#32479;&#35745;&#26426;&#22120;&#32763;&#35793; (SMT) &#26102;&#20195;&#65292;&#21069;&#21521;&#32763;&#35793;&#21644;&#24448;&#36820;&#32763;&#35793;&#30340;&#35780;&#20272;&#24471;&#20998;&#20043;&#38388;&#23384;&#22312;&#30528;&#27169;&#31946;&#30340;&#30456;&#20851;&#24615;&#35266;&#23519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#24778;&#20154;&#30340;&#21457;&#29616;&#65306;&#24448;&#36820;&#32763;&#35793;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#32780;&#19981;&#38656;&#35201;&#21442;&#32771;&#25991;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102; SMT &#35780;&#20272;&#20013;&#30340;&#24448;&#36820;&#32763;&#35793;&#65292;&#25581;&#31034;&#20102;&#20854;&#38271;&#26399;&#23384;&#22312;&#30340;&#35823;&#35299;&#23454;&#36136;&#19978;&#26159;&#30001;&#22797;&#21046;&#26426;&#21046;&#24341;&#36215;&#30340;&#12290;&#22312;&#21435;&#38500;SMT&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#21518;&#65292;&#24448;&#36820;&#32763;&#35793;&#35780;&#20998;&#21487;&#20197;&#24688;&#24403;&#22320;&#21453;&#26144;&#21069;&#21521;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20462;&#27491;&#26159;&#36807;&#26399;&#30340;&#65292;&#22240;&#20026;&#24448;&#36820;&#32763;&#35793;&#21487;&#20197;&#20174;&#22810;&#20010;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20219;&#21153;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic evaluation on low-resource language translation suffers from a deficiency of parallel corpora. Round-trip translation could be served as a clever and straightforward technique to alleviate the requirement of the parallel evaluation corpus. However, there was an observation of obscure correlations between the evaluation scores by forward and round-trip translations in the era of statistical machine translation (SMT). In this paper, we report the surprising finding that round-trip translation can be used for automatic evaluation without the references. Firstly, our revisit on the round-trip translation in SMT evaluation unveils that its long-standing misunderstanding is essentially caused by copying mechanism. After removing copying mechanism in SMT, round-trip translation scores can appropriately reflect the forward translation performance. Then, we demonstrate the rectification is overdue as round-trip translation could benefit multiple machine translation evaluation tasks. T
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2209.06049</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65306;&#20197;&#21360;&#24230;&#27861;&#24459;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law. (arXiv:2209.06049v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#65292;&#37325;&#26032;&#35757;&#32451;&#21644;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#20004;&#20010;PLMs&#65292;&#21363;LegalBERT&#21644;CaseLawBERT&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20960;&#39033;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#20013;&#65292;&#23545;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#22686;&#22810;&#65292;&#29305;&#21035;&#26159;&#22312;&#27431;&#32654;&#27861;&#24459;&#25991;&#26412;&#26041;&#38754;&#65292;PLMs&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#31561;&#20854;&#20182;&#22269;&#23478;&#30340;&#27861;&#24459;&#25991;&#26412;&#20855;&#26377;&#24456;&#22810;&#29305;&#27530;&#29305;&#24449;&#65292;&#22240;&#27492;&#20063;&#38656;&#35201;&#22312;&#36825;&#20123;&#26041;&#38754;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#23581;&#35797;&#22312;&#21360;&#24230;&#27861;&#24459;&#39046;&#22495;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#21360;&#24230;&#27861;&#24459;&#25968;&#25454;&#19978;&#37325;&#26032;&#35757;&#32451;&#65288;&#32487;&#32493;&#39044;&#35757;&#32451;&#65289;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;&#27861;&#24459;PLMs, LegalBERT&#21644;CaseLawBERT&#65292;&#20197;&#21450;&#20351;&#29992;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#25991;&#26412;&#30340;&#35789;&#27719;&#34920;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#20102;&#19968;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;PLMs&#24212;&#29992;&#20110;&#19977;&#20010;&#22522;&#20934;&#27861;&#24459;NLP&#20219;&#21153;&#8212;&#8212;&#20174;&#20107;&#23454;&#20013;&#35782;&#21035;&#27861;&#24459;&#27861;&#35268;&#12289;&#23545;&#27861;&#38498;&#21028;&#20915;&#25991;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#65292;&#20197;&#21450;&#39044;&#27979;&#27861;&#38498;&#19978;&#35785;&#21028;&#20915;--&#22312;&#21360;&#24230;&#21644;&#38750;&#21360;&#24230;&#30340;&#25991;&#26412;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;StoryTrans&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#34892;&#25925;&#20107;&#20316;&#32773;&#39118;&#26684;&#36716;&#25442;&#30340;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#35821;&#31687;&#34920;&#31034;&#21644;&#21487;&#23398;&#20064;&#30340;&#39118;&#26684;&#23884;&#20837;&#23454;&#29616;&#28304;&#20869;&#23481;&#20449;&#24687;&#21040;&#30446;&#26631;&#39118;&#26684;&#30340;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20869;&#23481;&#22686;&#24378;&#27169;&#22359;&#20197;&#25552;&#39640;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#27979;&#20013;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2208.13423</link><description>&lt;p&gt;
StoryTrans: &#24102;&#35821;&#31687;&#34920;&#31034;&#21644;&#20869;&#23481;&#22686;&#24378;&#30340;&#38750;&#24179;&#34892;&#25925;&#20107;&#20316;&#32773;&#39118;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
StoryTrans: Non-Parallel Story Author-Style Transfer with Discourse Representations and Content Enhancing. (arXiv:2208.13423v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;StoryTrans&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#24179;&#34892;&#25925;&#20107;&#20316;&#32773;&#39118;&#26684;&#36716;&#25442;&#30340;&#20219;&#21153;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#35821;&#31687;&#34920;&#31034;&#21644;&#21487;&#23398;&#20064;&#30340;&#39118;&#26684;&#23884;&#20837;&#23454;&#29616;&#28304;&#20869;&#23481;&#20449;&#24687;&#21040;&#30446;&#26631;&#39118;&#26684;&#30340;&#36716;&#25442;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20869;&#23481;&#22686;&#24378;&#27169;&#22359;&#20197;&#25552;&#39640;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#27979;&#20013;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#24179;&#34892;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#21333;&#35789;&#25110;&#21477;&#23376;&#32423;&#21035;&#19978;&#65292;&#20363;&#22914;&#21477;&#23376;&#24773;&#24863;&#21644;&#24418;&#24335;&#36716;&#25442;&#65292;&#20294;&#24573;&#30053;&#20102;&#22312;&#35821;&#31687;&#32423;&#21035;&#19978;&#36739;&#38271;&#30340;&#39118;&#26684;&#36716;&#25442;&#12290;&#38271;&#25991;&#26412;&#36890;&#24120;&#28041;&#21450;&#26356;&#20026;&#22797;&#26434;&#30340;&#20316;&#32773;&#35821;&#35328;&#20559;&#22909;&#65292;&#20363;&#22914;&#35821;&#31687;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#34892;&#25925;&#20107;&#20316;&#32773;&#39118;&#26684;&#36716;&#25442;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#36755;&#20837;&#25925;&#20107;&#36716;&#25442;&#20026;&#25351;&#23450;&#20316;&#32773;&#30340;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#21517;&#20026;StoryTrans&#65292;&#23427;&#21033;&#29992;&#35821;&#31687;&#34920;&#31034;&#26469;&#25429;&#25417;&#28304;&#20869;&#23481;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#21487;&#23398;&#20064;&#39118;&#26684;&#23884;&#20837;&#23558;&#23427;&#20204;&#36716;&#25442;&#20026;&#30446;&#26631;&#39118;&#26684;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#35757;&#32451;&#30446;&#26631;&#26469;&#23558;&#39118;&#26684;&#29305;&#24449;&#20174;&#23398;&#20064;&#30340;&#35821;&#31687;&#34920;&#31034;&#20013;&#35299;&#24320;&#65292;&#20197;&#38450;&#27490;&#27169;&#22411;&#36864;&#21270;&#20026;&#33258;&#21160;&#32534;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24378;&#21270;&#20869;&#23481;&#20445;&#30041;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20869;&#23481;&#22686;&#24378;&#27169;&#22359;&#65292;&#25913;&#21892;&#20102;&#36755;&#20837;&#21644;&#36755;&#20986;&#25925;&#20107;&#20043;&#38388;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-parallel text style transfer is an important task in natural language generation. However, previous studies concentrate on the token or sentence level, such as sentence sentiment and formality transfer, but neglect long style transfer at the discourse level. Long texts usually involve more complicated author linguistic preferences such as discourse structures than sentences. In this paper, we formulate the task of non-parallel story author-style transfer, which requires transferring an input story into a specified author style while maintaining source semantics. To tackle this problem, we propose a generation model, named StoryTrans, which leverages discourse representations to capture source content information and transfer them to target styles with learnable style embeddings. We use an additional training objective to disentangle stylistic features from the learned discourse representation to prevent the model from degenerating to an auto-encoder. Moreover, to enhance content pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36866;&#24212;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24863;&#30693;&#26102;&#38388;&#30340;&#27169;&#26495;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;MLM&#24471;&#21040;DCWE&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22256;&#24785;&#24230;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.10734</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#26495;&#21270;&#26102;&#38388;&#36866;&#24212;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning Dynamic Contextualised Word Embeddings via Template-based Temporal Adaptation. (arXiv:2208.10734v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36866;&#24212;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#24863;&#30693;&#26102;&#38388;&#30340;&#27169;&#26495;&#29983;&#25104;&#25552;&#31034;&#65292;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;MLM&#24471;&#21040;DCWE&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#22256;&#24785;&#24230;&#21644;&#19979;&#28216;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#65288;DCWE&#65289;&#34920;&#31034;&#21333;&#35789;&#30340;&#35821;&#20041;&#38543;&#26102;&#38388;&#30340;&#21464;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24863;&#30693;&#26102;&#38388;&#30340;&#27169;&#26495;&#23398;&#20064;&#39044;&#35757;&#32451;&#25513;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#24471;&#21040;DCWE&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#19982;$C_1$&#21644;$C_2$&#30456;&#20851;&#30340;&#8220;&#26530;&#36724;&#8221;&#65288;pivot&#65289;&#26415;&#35821;&#21644;&#22312;&#21508;&#20010;&#24555;&#29031;&#20013;&#19982;&#29305;&#23450;&#26530;&#36724;&#26415;&#35821;&#30456;&#20851;&#30340;&#8220;&#38170;&#8221;&#65288;anchor&#65289;&#26415;&#35821;&#26469;&#29983;&#25104;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#23398;&#20064;&#26102;&#38388;&#25935;&#24863;&#30340;&#27169;&#26495;&#30340;&#26041;&#27861;&#65292;&#20174;$C_1$&#21644;$C_2$&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#26080;&#38656;&#20219;&#20309;&#20154;&#24037;&#30417;&#30563;&#12290;&#25509;&#19979;&#26469;&#65292;&#26412;&#25991;&#20351;&#29992;&#36825;&#20123;&#29983;&#25104;&#30340;&#25552;&#31034;&#36890;&#36807;&#24494;&#35843;&#26469;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;MLM&#21040;$T_2$&#12290;&#22810;&#20010;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36229;&#36807;&#24378;&#22522;&#32447;9.2&#28857;&#30340;&#21516;&#26102;&#38477;&#20302;&#20102;$C_1$&#27979;&#35797;&#21477;&#23376;&#30340;&#22256;&#24785;&#24230;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31561;&#19979;&#28216;&#20219;&#21153;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamic contextualised word embeddings (DCWEs) represent the temporal semantic variations of words. We propose a method for learning DCWEs by time-adapting a pretrained Masked Language Model (MLM) using time-sensitive templates. Given two snapshots $C_1$ and $C_2$ of a corpus taken respectively at two distinct timestamps $T_1$ and $T_2$, we first propose an unsupervised method to select (a) \emph{pivot} terms related to both $C_1$ and $C_2$, and (b) \emph{anchor} terms that are associated with a specific pivot term in each individual snapshot. We then generate prompts by filling manually compiled templates using the extracted pivot and anchor terms. Moreover, we propose an automatic method to learn time-sensitive templates from $C_1$ and $C_2$, without requiring any human supervision. Next, we use the generated prompts to adapt a pretrained MLM to $T_2$ by fine-tuning using those prompts. Multiple experiments show that our proposed method reduces the perplexity of test sentences in $C_
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#30340;CASE&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#29992;&#25143;&#30340;&#31895;&#32454;&#31890;&#24230;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#21305;&#37197;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20855;&#20849;&#24773;&#21644;&#20449;&#24687;&#24615;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2208.08845</link><description>&lt;p&gt;
CASE&#65306;&#35843;&#25972;&#31895;&#32454;&#31890;&#24230;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#21305;&#37197;&#20197;&#20135;&#29983;&#20849;&#24773;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation. (arXiv:2208.08845v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.08845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#30340;CASE&#27169;&#22411;&#65292;&#36890;&#36807;&#35843;&#25972;&#29992;&#25143;&#30340;&#31895;&#32454;&#31890;&#24230;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#21305;&#37197;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#20855;&#20849;&#24773;&#21644;&#20449;&#24687;&#24615;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#20132;&#27969;&#24212;&#35813;&#26159;&#20849;&#24773;&#35748;&#30693;&#21644;&#24773;&#24863;&#30340;&#26377;&#24847;&#35782;&#35843;&#25972;&#21644;&#20132;&#20114;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20849;&#24773;&#23545;&#35805;&#27169;&#22411;&#36890;&#24120;&#21482;&#32771;&#34385;&#24773;&#24863;&#26041;&#38754;&#25110;&#22312;&#35748;&#30693;&#21644;&#24773;&#24863;&#19978;&#21333;&#29420;&#22788;&#29702;&#65292;&#36825;&#38480;&#21046;&#20102;&#20849;&#24773;&#21709;&#24212;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#30340;CASE&#27169;&#22411;&#12290;&#23427;&#39318;&#20808;&#24314;&#31435;&#22312;&#24120;&#35782;&#35748;&#30693;&#22270;&#21644;&#24773;&#24863;&#27010;&#24565;&#22270;&#30340;&#22522;&#30784;&#19978;&#65292;&#28982;&#21518;&#22312;&#31895;&#32454;&#30340;&#20004;&#20010;&#23618;&#38754;&#19978;&#23545;&#29992;&#25143;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#36827;&#34892;&#21305;&#37197;&#12290;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;CASE&#20248;&#20110;&#20849;&#24773;&#23545;&#35805;&#30340;&#26368;&#26032;&#22522;&#32447;&#65292;&#24182;&#19988;&#21487;&#20197;&#20135;&#29983;&#26356;&#20855;&#20849;&#24773;&#21644;&#20449;&#24687;&#24615;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;DCASE 2022&#27604;&#36187;&#20013;&#21442;&#21152;&#20102;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;Clotho&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2207.04156</link><description>&lt;p&gt;
&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Automated Audio Captioning and Language-Based Audio Retrieval. (arXiv:2207.04156v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04156
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;DCASE 2022&#27604;&#36187;&#20013;&#21442;&#21152;&#20102;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#21644;&#22522;&#20110;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;Clotho&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;&#22522;&#32447;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#39033;&#30446;&#21442;&#21152;&#20102;DCASE 2022&#31454;&#36187;&#65288;&#20219;&#21153;6&#65289;&#65292;&#20854;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#21644;&#65288;2&#65289;&#22522;&#20110;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;&#12290;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#20026;&#38899;&#39057;&#26679;&#26412;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#22312;&#22266;&#23450;&#25968;&#25454;&#38598;&#20013;&#26597;&#25214;&#19982;&#32473;&#23450;&#25551;&#36848;&#30456;&#21305;&#37197;&#30340;&#38899;&#39057;&#26679;&#26412;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#20351;&#29992;&#20102;Clotho&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#38899;&#39057;&#23383;&#24149;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;BLEU1&#65292;BLEU2&#65292;BLEU3&#65292;ROUGEL&#65292;METEOR&#65292;CIDEr&#65292;SPICE&#21644;SPIDEr&#24471;&#20998;&#65292;&#32780;&#38899;&#39057;&#26816;&#32034;&#35780;&#20272;&#20102;R1&#65292;R5&#65292;R10&#21644;mARP10&#24471;&#20998;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20123;&#20462;&#25913;&#36825;&#20123;&#20219;&#21153;&#30340;&#22522;&#32447;&#27169;&#22411;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#38024;&#23545;&#33258;&#21160;&#38899;&#39057;&#23383;&#24149;&#30340;&#26368;&#32456;&#26550;&#26500;&#25509;&#36817;&#20110;&#22522;&#32447;&#24615;&#33021;&#65292;&#32780;&#25105;&#20204;&#38024;&#23545;&#22522;&#20110;&#35821;&#35328;&#30340;&#38899;&#39057;&#26816;&#32034;&#30340;&#27169;&#22411;&#24050;&#36229;&#36234;&#20102;&#20854;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This project involved participation in the DCASE 2022 Competition (Task 6) which had two subtasks: (1) Automated Audio Captioning and (2) Language-Based Audio Retrieval. The first subtask involved the generation of a textual description for audio samples, while the goal of the second was to find audio samples within a fixed dataset that match a given description. For both subtasks, the Clotho dataset was used. The models were evaluated on BLEU1, BLEU2, BLEU3, ROUGEL, METEOR, CIDEr, SPICE, and SPIDEr scores for audio captioning and R1, R5, R10 and mARP10 scores for audio retrieval. We have conducted a handful of experiments that modify the baseline models for these tasks. Our final architecture for Automated Audio Captioning is close to the baseline performance, while our model for Language-Based Audio Retrieval has surpassed its counterpart.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2206.08406</link><description>&lt;p&gt;
&#39044;&#27979;Twitter&#23545;&#35805;&#32447;&#31243;&#20013;&#30340;&#20167;&#24680;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Predicting Hate Intensity of Twitter Conversation Threads. (arXiv:2206.08406v3 [cs.SI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DRAGNET++, &#36890;&#36807;&#32771;&#34385;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#12289;&#20256;&#25773;&#32467;&#26500;&#21644;&#29992;&#25143;&#20132;&#20114;&#65292;&#20197;&#39044;&#27979;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#24182;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21487;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#25552;&#20379;&#22312;&#24694;&#24847;&#23545;&#35805;&#21319;&#32423;&#20043;&#21069;&#35782;&#21035;&#21644;&#31649;&#29702;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#25991;&#26159;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#20013;&#26368;&#31616;&#27905;&#30340;&#20132;&#27969;&#24418;&#24335;&#65292;&#19968;&#26465;&#25512;&#25991;&#26377;&#21487;&#33021;&#26159;&#23545;&#35805;&#20013;&#25171;&#36896;&#25110;&#30772;&#22351;&#35752;&#35770;&#30340;&#28508;&#22312;&#23186;&#20171;&#12290;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#23481;&#26131;&#33719;&#24471;&#65292;&#38459;&#27490;&#20854;&#20256;&#25773;&#23545;&#20110;&#31038;&#20132;&#23186;&#20307;&#20844;&#21496;&#21644;&#29992;&#25143;&#26469;&#35828;&#26159;&#26497;&#20026;&#37325;&#35201;&#30340;&#65292;&#21487;&#20197;&#25512;&#36827;&#33391;&#22909;&#30340;&#20132;&#27969;&#26041;&#24335;&#12290;&#30446;&#21069;&#38500;&#20102;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22806;&#65292;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#19987;&#27880;&#20110;&#20998;&#31867;&#21333;&#20010;&#25512;&#25991;&#65292;&#32780;&#24573;&#30053;&#20102;&#25512;&#25991;&#20043;&#38388;&#30340;&#23545;&#35805;&#32447;&#31243;/&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DRAGNET++&#65292;&#26088;&#22312;&#36890;&#36807;&#25512;&#25991;&#30340;&#22238;&#22797;&#38142;&#39044;&#27979;&#23427;&#21487;&#33021;&#24102;&#26469;&#30340;&#20167;&#24680;&#31243;&#24230;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#32447;&#31243;&#30340;&#35821;&#20041;&#21644;&#20256;&#25773;&#32467;&#26500;&#20197;&#21450;&#32447;&#31243;&#20013;&#30340;&#29992;&#25143;&#20132;&#20114;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;DRAGNET++&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35748;&#20026;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#21487;&#20197;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#39044;&#27979;&#21644;&#31649;&#29702;&#24694;&#24847;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating stru
&lt;/p&gt;</description></item><item><title>COOL&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#23616;&#37096;&#21477;&#27861;&#19978;&#19979;&#25991;&#65292;&#33021;&#22815;&#25913;&#36827;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38382;&#31572;&#12290;</title><link>http://arxiv.org/abs/2204.09593</link><description>&lt;p&gt;
COOL&#65306;&#19968;&#31181;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#21450;&#20854;&#22312;&#38382;&#31572;&#21644;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks. (arXiv:2204.09593v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.09593
&lt;/p&gt;
&lt;p&gt;
COOL&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#65292;&#29992;&#20110;&#32534;&#30721;&#23616;&#37096;&#21477;&#27861;&#19978;&#19979;&#25991;&#65292;&#33021;&#22815;&#25913;&#36827;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#19978;&#19979;&#25991;&#35266;&#23519;&#22120;&#36890;&#36807;&#28155;&#21152;&#19968;&#20010;&#23616;&#37096;&#27880;&#24847;&#21147;&#24418;&#24335;&#30340;&#35266;&#23519;&#27880;&#24847;&#21147;&#25913;&#36827;&#20102;&#35270;&#35273;Transformer&#30340;&#24615;&#33021;&#12290;&#19982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20854;&#20182;&#39046;&#22495;&#19968;&#26679;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#26500;&#25104;&#20102;&#22823;&#22810;&#25968;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35266;&#23519;&#27880;&#24847;&#21147;&#26426;&#21046;COOL&#12290;COOL&#28155;&#21152;&#21040;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#23618;&#20043;&#19978;&#65292;&#32771;&#34385;&#21333;&#35789;&#30340;&#25509;&#36817;&#24615;&#21644;&#26356;&#22810;&#30340;&#25104;&#23545;&#32422;&#26463;&#65292;&#32534;&#30721;&#23616;&#37096;&#21477;&#27861;&#19978;&#19979;&#25991;&#12290;&#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;COOL&#30340;&#23454;&#29616;&#36827;&#34892;&#27604;&#36739;&#23454;&#35777;&#24615;&#33021;&#35780;&#20272;&#65292;&#35777;&#23454;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#25913;&#36827;&#22522;&#32447;&#27169;&#22411;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.  In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.  We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.  A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PADA&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#38754;&#21152;&#20837;&#21098;&#26525;&#31574;&#30053;&#65292;&#20351;&#29992;CD-TAW&#26041;&#27861;&#20174;&#31934;&#32454;&#35843;&#25972;&#30340;OOT&#27169;&#22411;&#20013;&#33719;&#24471;&#21021;&#22987;&#21098;&#26525;&#25513;&#30721;&#65292;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.16965</link><description>&lt;p&gt;
PADA: &#22522;&#20110;&#21098;&#26525;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
PADA: Pruning Assisted Domain Adaptation for Self-Supervised Speech Representations. (arXiv:2203.16965v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16965
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PADA&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#38754;&#21152;&#20837;&#21098;&#26525;&#31574;&#30053;&#65292;&#20351;&#29992;CD-TAW&#26041;&#27861;&#20174;&#31934;&#32454;&#35843;&#25972;&#30340;OOT&#27169;&#22411;&#20013;&#33719;&#24471;&#21021;&#22987;&#21098;&#26525;&#25513;&#30721;&#65292;&#24182;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#21487;&#29992;&#20110;&#22810;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#24120;&#24120;&#20250;&#20986;&#29616;&#23545;&#26410;&#26631;&#27880;&#25968;&#25454;&#26469;&#28304;&#39046;&#22495;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#20026;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PADA (Pruning Assisted Domain Adaptation)&#65292;&#24182;&#20174;&#32463;&#36807;&#22823;&#37327;OOT(Out-of-domain)&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#20943;&#21435;&#22810;&#20313;&#30340;&#26435;&#37325;&#65292;&#20026;&#30446;&#26631;&#39046;&#22495;&#30340;ASR&#24494;&#35843;&#33150;&#20986;&#31354;&#38388;&#12290;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#21098;&#26525;&#31574;&#30053;&#26469;&#35782;&#21035;&#20887;&#20313;&#26435;&#37325;&#65292;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#26368;&#36817;&#21457;&#29616;&#30340;Task-Agnostic&#21644;Task-Aware&#21098;&#26525;&#23545;PADA&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#21518;&#32773;&#30340;&#21098;&#26525;&#33539;&#24335;&#65292;&#31216;&#20026;Cross-Domain Task-Aware Pruning(CD-TAW)&#12290;CD-TAW&#20174;&#31934;&#32454;&#35843;&#25972;&#30340;OOT&#27169;&#22411;&#20013;&#33719;&#24471;&#21021;&#22987;&#21098;&#26525;&#25513;&#30721;&#65292;&#36825;&#20351;&#20854;&#19982;&#26412;&#25991;&#20013;&#35752;&#35770;&#30340;&#21098;&#26525;&#31574;&#30053;&#25130;&#28982;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;PADA&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#21644;&#21033;&#29992;&#21098;&#26525;&#25216;&#26415;&#26469;&#28040;&#38500;&#19981;&#24517;&#35201;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;CD-TAW&#21098;&#26525;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While self-supervised speech representation learning (SSL) models serve a variety of downstream tasks, these models have been observed to overfit to the domain from which the unlabelled data originates. To alleviate this issue, we propose PADA (Pruning Assisted Domain Adaptation) and zero out redundant weights from models pre-trained on large amounts of out-of-domain (OOD) data. Intuitively, this helps to make space for the target-domain ASR finetuning. The redundant weights can be identified through various pruning strategies which have been discussed in detail as a part of this work. Specifically, we investigate the effect of the recently discovered Task-Agnostic and Task-Aware pruning on PADA and propose a new pruning paradigm based on the latter, which we call Cross-Domain Task-Aware Pruning (CD-TAW). CD-TAW obtains the initial pruning mask from a well fine-tuned OOD model, which makes it starkly different from the rest of the pruning strategies discussed in the paper. Our proposed
&lt;/p&gt;</description></item><item><title>AxoNN&#26159;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2110.13005</link><description>&lt;p&gt;
AxoNN: &#19968;&#31181;&#24322;&#27493;&#12289;&#28040;&#24687;&#39537;&#21160;&#30340;&#26497;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#24182;&#34892;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning. (arXiv:2110.13005v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13005
&lt;/p&gt;
&lt;p&gt;
AxoNN&#26159;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#38477;&#20302;GPU&#20869;&#23384;&#28040;&#32791;&#65292;&#21516;&#26102;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#23384;&#20648;&#22120;&#23481;&#37327;&#24050;&#36828;&#36828;&#36229;&#20986;&#29616;&#20195;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;DRAM&#23481;&#37327;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#22522;&#20110;GPU&#30340;&#38598;&#32676;&#19978;&#24320;&#21457;&#39640;&#25928;&#31639;&#27861;&#24182;&#34892;&#35757;&#32451;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#12290;&#22312;&#29616;&#20195;GPU&#19978;&#65292;&#35745;&#31639;&#30456;&#23545;&#24265;&#20215;&#65292;&#20026;&#20102;&#25552;&#21462;&#26368;&#22823;&#24615;&#33021;&#65292;&#35774;&#35745;&#21644;&#23454;&#29616;&#36825;&#20123;&#24182;&#34892;&#35757;&#32451;&#31639;&#27861;&#20013;&#26497;&#20854;&#39640;&#25928;&#30340;&#36890;&#20449;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AxoNN&#65292;&#19968;&#31181;&#21033;&#29992;&#24322;&#27493;&#24615;&#21644;&#28040;&#24687;&#39537;&#21160;&#25191;&#34892;&#35843;&#24230;&#27599;&#20010;GPU&#19978;&#31070;&#32463;&#32593;&#32476;&#25805;&#20316;&#30340;&#24182;&#34892;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#20174;&#32780;&#20943;&#23569;GPU&#31354;&#38386;&#26102;&#38388;&#65292;&#26368;&#22823;&#21270;&#30828;&#20214;&#25928;&#29575;&#12290;&#36890;&#36807;&#20351;&#29992;CPU&#20869;&#23384;&#20316;&#20026;&#20887;&#20313;&#31354;&#38388;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#23450;&#26399;&#21368;&#36733;&#25968;&#25454;&#65292;AxoNN&#33021;&#22815;&#23558;GPU&#20869;&#23384;&#28040;&#32791;&#38477;&#20302;4&#20493;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#23558;&#27599;&#20010;GPU&#30340;&#21442;&#25968;&#25968;&#37327;&#22686;&#21152;4&#20493;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#24517;&#38656;&#30340;GPU&#25968;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;</title><link>http://arxiv.org/abs/2106.01263</link><description>&lt;p&gt;
Uni-Encoder: &#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#24555;&#36895;&#20934;&#30830;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;
&lt;/p&gt;
&lt;p&gt;
Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems. (arXiv:2106.01263v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.01263
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21709;&#24212;&#36873;&#25321;&#33539;&#20363;Uni-Encoder&#65292;&#35299;&#20915;&#20102;Cross-Encoder&#22810;&#27425;&#32534;&#30721;&#30456;&#21516;&#19978;&#19979;&#25991;&#35745;&#31639;&#25104;&#26412;&#39640;&#21644;Poly-Encoder&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#35813;&#33539;&#20363;&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26679;&#26412;&#19982;&#25490;&#24207;&#26159;&#29616;&#20195;&#29983;&#25104;&#22411;&#23545;&#35805;&#31995;&#32479;&#30340;&#20851;&#38190;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#20174;&#29983;&#25104;&#30340;&#23569;&#37327;&#20505;&#36873;&#31572;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#31572;&#26696;&#26469;&#23454;&#29616;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25490;&#24207;&#26041;&#27861;&#20027;&#35201;&#20351;&#29992;&#31216;&#20026;&#20132;&#21449;&#32534;&#30721;&#22120;&#30340;&#32534;&#30721;&#33539;&#20363;&#65292;&#35813;&#32534;&#30721;&#22120;&#20998;&#21035;&#23545;&#27599;&#20010;&#19978;&#19979;&#25991;-&#20505;&#36873;&#23545;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#26681;&#25454;&#20854;&#36866;&#24212;&#24230;&#24471;&#20998;&#23545;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#12290;&#28982;&#32780;&#65292;&#20132;&#21449;&#32534;&#30721;&#22120;&#20026;&#27599;&#20010;&#20505;&#36873;&#37325;&#22797;&#32534;&#30721;&#30456;&#21516;&#30340;&#20887;&#38271;&#19978;&#19979;&#25991;&#65292;&#23548;&#33268;&#35745;&#31639;&#25104;&#26412;&#39640;&#12290;Poly-Encoder&#36890;&#36807;&#20943;&#23569;&#19978;&#19979;&#25991;&#21644;&#20505;&#36873;&#20043;&#38388;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#20294;&#20195;&#20215;&#26159;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#31216;&#20026;Uni-Encoder&#65292;&#23427;&#20687;&#20132;&#21449;&#32534;&#30721;&#22120;&#19968;&#26679;&#23436;&#20840;&#20851;&#27880;&#27599;&#20010;&#20505;&#36873;&#23545;&#65292;&#21516;&#26102;&#20687;Poly-&#32534;&#30721;&#22120;&#19968;&#26679;&#21482;&#32534;&#30721;&#19968;&#27425;&#19978;&#19979;&#25991;&#12290;Uni-Encoder&#22312;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#23545;&#25152;&#26377;&#20505;&#36873;&#19982;&#19978;&#19979;&#25991;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#38024;&#23545;&#25152;&#26377;&#20505;&#36873;&#20351;&#29992;&#30456;&#21516;&#30340;&#20301;&#32622;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates
&lt;/p&gt;</description></item><item><title>Quinductor&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#25968;&#25454;&#39537;&#21160;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#20381;&#23384;&#26641;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#27604;&#20043;&#21069;&#25991;&#29486;&#25253;&#36947;&#30340;QG&#22522;&#32447;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2103.10121</link><description>&lt;p&gt;
Quinductor:&#22522;&#20110;&#36890;&#29992;&#20381;&#23384;&#35821;&#27861;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#39537;&#21160;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quinductor: a multilingual data-driven method for generating reading-comprehension questions using Universal Dependencies. (arXiv:2103.10121v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10121
&lt;/p&gt;
&lt;p&gt;
Quinductor&#26159;&#19968;&#31181;&#22810;&#35821;&#35328;&#25968;&#25454;&#39537;&#21160;&#30340;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#20381;&#23384;&#26641;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65292;&#27604;&#20043;&#21069;&#25991;&#29486;&#25253;&#36947;&#30340;QG&#22522;&#32447;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20381;&#23384;&#26641;&#29983;&#25104;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#12289;&#22823;&#37096;&#20998;&#30830;&#23450;&#24615;&#21644;&#35757;&#32451;&#25104;&#26412;&#20302;&#24265;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#12290;&#34429;&#28982;&#20173;&#38656;&#35201;&#29305;&#23450;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#20294;&#20854;&#22823;&#23567;&#36828;&#36828;&#19981;&#21450;&#29616;&#20195;&#31070;&#32463;&#38382;&#31572;&#29983;&#25104;&#65288;QG&#65289;&#20307;&#31995;&#32467;&#26500;&#25152;&#38656;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36807;&#20102;&#20808;&#21069;&#25991;&#29486;&#25253;&#36947;&#30340;QG&#22522;&#32447;&#65292;&#24182;&#22312;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multilingual data-driven method for generating reading comprehension questions using dependency trees. Our method provides a strong, mostly deterministic, and inexpensive-to-train baseline for less-resourced languages. While a language-specific corpus is still required, its size is nowhere near those required by modern neural question generation (QG) architectures. Our method surpasses QG baselines previously reported in the literature and shows a good performance in terms of human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2102.12227</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#29992;&#20110;&#35770;&#36848;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Attentive Residual Networks for Argument Mining. (arXiv:2102.12227v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.12227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#21033;&#29992;&#38598;&#25104;&#26041;&#27861;&#12289;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#26080;&#38656;&#20551;&#35774;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#65292;&#25104;&#21151;&#24212;&#29992;&#20110;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#65292;&#25104;&#20026;&#20102;&#19968;&#31181;&#26082;&#36890;&#29992;&#21448;&#39640;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22810;&#20219;&#21153;&#27880;&#24847;&#21147;&#27531;&#24046;&#32593;&#32476;&#22312;&#22810;&#20010;&#35770;&#36848;&#25366;&#25496;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27531;&#24046;&#26550;&#26500;&#65292;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#12289;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#24182;&#20351;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#19981;&#23545;&#25991;&#26723;&#25110;&#35770;&#25454;&#32467;&#26500;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#29992;&#25143;&#29983;&#25104;&#35780;&#35770;&#12289;&#31185;&#23398;&#20986;&#29256;&#29289;&#21644;&#21149;&#35828;&#24615;&#35770;&#25991;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#20855;&#26377;&#26356;&#39640;&#35745;&#31639;&#21360;&#35760;&#25110;&#29305;&#23450;&#20110;&#35821;&#26009;&#24211;&#35774;&#35745;&#30340;&#26368;&#20808;&#36827;&#26550;&#26500;&#30340;&#24378;&#26377;&#21147;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#20195;&#34920;&#20102;&#36890;&#29992;&#24615;&#12289;&#24615;&#33021;&#31934;&#24230;&#21644;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#20043;&#38388;&#30340;&#26377;&#36259;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the use of residual networks and neural attention for multiple argument mining tasks. We propose a residual architecture that exploits attention, multi-task learning, and makes use of ensemble, without any assumption on document or argument structure. We present an extensive experimental evaluation on five different corpora of user-generated comments, scientific publications, and persuasive essays. Our results show that our approach is a strong competitor against state-of-the-art architectures with a higher computational footprint or corpus-specific design, representing an interesting compromise between generality, performance accuracy and reduced model size.
&lt;/p&gt;</description></item></channel></rss>