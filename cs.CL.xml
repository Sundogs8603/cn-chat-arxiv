<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2401.16184</link><description>&lt;p&gt;
&#20851;&#20110;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#35821;&#20041;&#23398;&#65306;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Semantics of LM Latent Space: A Vocabulary-defined Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2401.16184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;LM&#28508;&#22312;&#31354;&#38388;&#30340;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#65292;&#24341;&#20837;&#20102;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#21644;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25991;&#26412;&#29702;&#35299;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;(LM)&#30340;&#28508;&#22312;&#31354;&#38388;&#23545;&#20110;&#25913;&#36827;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#20998;&#26512;&#24448;&#24448;&#22312;&#25552;&#20379;&#22522;&#20110;&#27169;&#22411;&#30340;&#23545;LM&#35821;&#20041;&#30340;&#20998;&#31163;&#27934;&#23519;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#24182;&#24573;&#35270;&#20102;LM&#36866;&#24212;&#30340;&#37325;&#35201;&#26041;&#38754;&#12290;&#20026;&#20102;&#21709;&#24212;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20197;&#35789;&#27719;&#20026;&#23450;&#20041;&#30340;&#35821;&#20041;&#23398;&#65292;&#23427;&#22312;LM&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#31435;&#20102;&#19968;&#20010;&#21442;&#32771;&#26694;&#26550;&#65292;&#30830;&#20445;&#22522;&#20110;LM&#35789;&#27719;&#30340;&#20998;&#31163;&#35821;&#20041;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20132;&#32455;&#20998;&#26512;&#65292;&#21033;&#29992;LM&#35789;&#27719;&#26469;&#33719;&#24471;&#20197;&#27169;&#22411;&#20026;&#20013;&#24515;&#30340;&#27934;&#23519;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;logits&#30340;&#26032;&#25216;&#26415;&#65292;&#24378;&#35843;&#21487;&#24494;&#20998;&#24615;&#21644;&#23616;&#37096;&#31561;&#36317;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#31070;&#32463;&#32858;&#31867;&#27169;&#22359;&#65292;&#29992;&#20110;&#22312;LM&#36866;&#24212;&#36807;&#31243;&#20013;&#36827;&#34892;&#35821;&#20041;&#26657;&#20934;&#12290;&#36890;&#36807;&#22312;&#22810;&#31181;&#25991;&#26412;&#29702;&#35299;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#38754;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the latent space of language models (LM) is crucial to refining their performance and interpretability. Existing analyses often fall short in providing disentangled (model-centric) insights into LM semantics, and neglect essential aspects of LM adaption. In response, we introduce a pioneering method called vocabulary-defined semantics, which establishes a reference frame within the LM latent space, ensuring disentangled semantic analysis grounded in LM vocabulary. Our approach transcends prior entangled analysis, leveraging LM vocabulary for model-centric insights. Furthermore, we propose a novel technique to compute logits, emphasising differentiability and local isotropy, and introduce a neural clustering module for semantically calibrating data representations during LM adaptation. Through extensive experiments across diverse text understanding datasets, our approach outperforms state-of-the-art methods of retrieval-augmented generation and parameter-efficient finetuni
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#25581;&#31034;&#20102;&#31185;&#23398;&#25991;&#31456;&#24341;&#29992;&#20013;&#30340;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;&#65292;&#22686;&#24378;&#20102;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#30340;&#23458;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;</title><link>https://arxiv.org/abs/2404.01800</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#23545;&#31185;&#23398;&#25991;&#31456;&#24341;&#29992;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#65306;&#35782;&#21035;&#28508;&#22312;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;
&lt;/p&gt;
&lt;p&gt;
Sentiment Analysis of Citations in Scientific Articles Using ChatGPT: Identifying Potential Biases and Conflicts of Interest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01800
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#25581;&#31034;&#20102;&#31185;&#23398;&#25991;&#31456;&#24341;&#29992;&#20013;&#30340;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;&#65292;&#22686;&#24378;&#20102;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#30340;&#23458;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#25991;&#31456;&#22312;&#25512;&#21160;&#30693;&#35782;&#21457;&#23637;&#21644;&#25351;&#23548;&#30740;&#31350;&#26041;&#21521;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#35780;&#20272;&#31185;&#23398;&#25991;&#31456;&#30340;&#20851;&#38190;&#26041;&#38754;&#20043;&#19968;&#26159;&#23545;&#24341;&#25991;&#36827;&#34892;&#20998;&#26512;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#34987;&#24341;&#29992;&#20316;&#21697;&#30340;&#24433;&#21709;&#21644;&#25509;&#21463;&#31243;&#24230;&#30340;&#35265;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;ChatGPT&#65292;&#22312;&#31185;&#23398;&#25991;&#31456;&#20013;&#23545;&#24341;&#25991;&#36827;&#34892;&#20840;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#21019;&#26032;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;ChatGPT&#33021;&#22815;&#36776;&#21035;&#24341;&#25991;&#30340;&#24494;&#22937;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#24863;&#65292;&#25552;&#20379;&#23545;&#34987;&#24341;&#29992;&#20316;&#21697;&#30340;&#25509;&#21463;&#31243;&#24230;&#21644;&#24433;&#21709;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;ChatGPT&#30340;&#33021;&#21147;&#36824;&#21253;&#25324;&#26816;&#27979;&#24341;&#25991;&#20013;&#30340;&#28508;&#22312;&#20559;&#35265;&#21644;&#21033;&#30410;&#20914;&#31361;&#65292;&#22686;&#24378;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#30340;&#23458;&#35266;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39537;&#21160;&#24037;&#20855;&#22312;&#22686;&#24378;&#24341;&#25991;&#20998;&#26512;&#21644;&#25552;&#39640;&#31185;&#23398;&#25991;&#29486;&#35780;&#20272;&#26041;&#38754;&#30340;&#21464;&#38761;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01800v1 Announce Type: cross  Abstract: Scientific articles play a crucial role in advancing knowledge and informing research directions. One key aspect of evaluating scientific articles is the analysis of citations, which provides insights into the impact and reception of the cited works. This article introduces the innovative use of large language models, particularly ChatGPT, for comprehensive sentiment analysis of citations within scientific articles. By leveraging advanced natural language processing (NLP) techniques, ChatGPT can discern the nuanced positivity or negativity of citations, offering insights into the reception and impact of cited works. Furthermore, ChatGPT's capabilities extend to detecting potential biases and conflicts of interest in citations, enhancing the objectivity and reliability of scientific literature evaluation. This study showcases the transformative potential of artificial intelligence (AI)-powered tools in enhancing citation analysis and pr
&lt;/p&gt;</description></item><item><title>&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;</title><link>https://arxiv.org/abs/2404.01129</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20449;&#24687;&#24456;&#37325;&#35201;&#65306;&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#24341;&#20837;LLMs&#20197;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Structured Information Matters: Incorporating Abstract Meaning Representation into LLMs for Improved Open-Domain Dialogue Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01129
&lt;/p&gt;
&lt;p&gt;
&#23558;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#32467;&#21512;&#21040;LLMs&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#25913;&#21892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#33258;&#21160;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#21487;&#35757;&#32451;&#30340;&#35780;&#20272;&#25351;&#26631;&#36890;&#24120;&#26159;&#36890;&#36807;&#35757;&#32451;&#20855;&#26377;&#30495;&#27491;&#27491;&#20363;&#21644;&#38543;&#26426;&#36873;&#25321;&#30340;&#36127;&#20363;&#22238;&#22797;&#26469;&#35757;&#32451;&#30340;&#65292;&#23548;&#33268;&#23427;&#20204;&#20542;&#21521;&#20110;&#23558;&#26356;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#30340;&#22238;&#22797;&#20998;&#37197;&#26356;&#39640;&#30340;&#24471;&#20998;&#32473;&#23450;&#19968;&#20010;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#24615;&#30340;&#36127;&#38754;&#22238;&#22797;&#20855;&#26377;&#19982;&#19978;&#19979;&#25991;&#39640;&#20869;&#23481;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#22312;&#35821;&#20041;&#19978;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#35780;&#20272;&#36825;&#31867;&#22238;&#22797;&#65292;&#23548;&#33268;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#20302;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#26041;&#38754;&#26377;&#19968;&#23450;&#25928;&#26524;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#24615;&#36127;&#38754;&#31034;&#20363;&#26041;&#38754;&#36935;&#21040;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;&#29992;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#35780;&#20272;&#65292;&#23427;&#32467;&#21512;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;SLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01129v1 Announce Type: new  Abstract: Automatic open-domain dialogue evaluation has attracted increasing attention. Trainable evaluation metrics are commonly trained with true positive and randomly selected negative responses, resulting in a tendency for them to assign a higher score to the responses that share higher content similarity with a given context. However, adversarial negative responses possess high content similarity with the contexts whilst being semantically different. Therefore, existing evaluation metrics are not robust enough to evaluate such responses, resulting in low correlations with human judgments. While recent studies have shown some efficacy in utilizing Large Language Models (LLMs) for open-domain dialogue evaluation, they still encounter challenges in effectively handling adversarial negative examples. In this paper, we propose a simple yet effective framework for open-domain dialogue evaluation, which combines domain-specific language models (SLMs
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#31038;&#20250;&#36741;&#21161;&#26426;&#22120;&#20154;&#39046;&#22495;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#26032;&#24212;&#29992;&#65292;&#33021;&#22815;&#26174;&#33879;&#25193;&#23637;&#20854;&#33021;&#21147;&#65292;&#20294;&#20063;&#24102;&#26469;&#26032;&#30340;&#39118;&#38505;&#21644;&#36947;&#24503;&#20851;&#20999;&#12290;</title><link>https://arxiv.org/abs/2404.00938</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#20419;&#36827;&#26356;&#22909;&#30340;&#31038;&#20250;&#36741;&#21161;&#20154;&#26426;&#20132;&#20114;&#65306;&#31616;&#35201;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
How Can Large Language Models Enable Better Socially Assistive Human-Robot Interaction: A Brief Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00938
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#31038;&#20250;&#36741;&#21161;&#26426;&#22120;&#20154;&#39046;&#22495;&#24102;&#26469;&#20102;&#28508;&#22312;&#30340;&#26032;&#24212;&#29992;&#65292;&#33021;&#22815;&#26174;&#33879;&#25193;&#23637;&#20854;&#33021;&#21147;&#65292;&#20294;&#20063;&#24102;&#26469;&#26032;&#30340;&#39118;&#38505;&#21644;&#36947;&#24503;&#20851;&#20999;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#36741;&#21161;&#26426;&#22120;&#20154;&#65288;SARs&#65289;&#22312;&#20026;&#32769;&#24180;&#20154;&#12289;&#24739;&#26377;&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#30340;&#20799;&#31461;&#20197;&#21450;&#31934;&#31070;&#20581;&#24247;&#25361;&#25112;&#32773;&#31561;&#29305;&#27530;&#32676;&#20307;&#25552;&#20379;&#20010;&#24615;&#21270;&#35748;&#30693;&#24773;&#24863;&#25903;&#25345;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290; SAR&#30340;&#22823;&#37327;&#30740;&#31350;&#20316;&#21697;&#23637;&#31034;&#20102;&#20854;&#22312;&#20026;&#22312;&#23478;&#25552;&#20379;&#25903;&#25345;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#31181;&#25903;&#25345;&#21487;&#20197;&#34917;&#20805;&#30001;&#19987;&#19994;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#30340;&#35786;&#25152;&#27835;&#30103;&#65292;&#20351;&#36825;&#20123;&#24178;&#39044;&#25514;&#26045;&#26356;&#21152;&#26377;&#25928;&#21644;&#21487;&#35775;&#38382;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#25216;&#26415;&#25361;&#25112;&#65292;&#38459;&#30861;&#20102;SAR&#20171;&#23548;&#30340;&#20132;&#20114;&#21644;&#24178;&#39044;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#31038;&#20250;&#26234;&#33021;&#21644;&#21151;&#25928;&#12290; &#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;SAR&#39046;&#22495;&#20869;&#30340;&#26032;&#24212;&#29992;&#28508;&#21147;&#26377;&#25152;&#22686;&#21152;&#65292;&#21487;&#20197;&#26174;&#33879;&#25193;&#23637;SAR&#30340;&#24403;&#21069;&#33021;&#21147;&#12290; &#28982;&#32780;&#65292;&#25972;&#21512;LLM&#20250;&#24341;&#20837;&#26032;&#30340;&#39118;&#38505;&#21644;&#36947;&#24503;&#20851;&#20999;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00938v1 Announce Type: cross  Abstract: Socially assistive robots (SARs) have shown great success in providing personalized cognitive-affective support for user populations with special needs such as older adults, children with autism spectrum disorder (ASD), and individuals with mental health challenges. The large body of work on SAR demonstrates its potential to provide at-home support that complements clinic-based interventions delivered by mental health professionals, making these interventions more effective and accessible. However, there are still several major technical challenges that hinder SAR-mediated interactions and interventions from reaching human-level social intelligence and efficacy. With the recent advances in large language models (LLMs), there is an increased potential for novel applications within the field of SAR that can significantly expand the current capabilities of SARs. However, incorporating LLMs introduces new risks and ethical concerns that ha
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2404.00482</link><description>&lt;p&gt;
&#29992;&#20110;&#26031;&#25289;&#22827;&#35821;&#30340;&#36328;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual Named Entity Corpus for Slavic Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00482
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#29992;&#20110;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#21629;&#21517;&#23454;&#20307;&#35821;&#26009;&#24211;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#21010;&#20998;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#30340;&#35782;&#21035;&#12289;&#20998;&#31867;&#12289;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#21547;&#20845;&#31181;&#26031;&#25289;&#22827;&#35821;&#35328;&#65288;&#20445;&#21152;&#21033;&#20122;&#35821;&#12289;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#12289;&#26031;&#27931;&#25991;&#23612;&#20122;&#35821;&#12289;&#20420;&#35821;&#21644;&#20044;&#20811;&#20848;&#35821;&#65289;&#21629;&#21517;&#23454;&#20307;&#30340;&#35821;&#26009;&#24211;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;2017-2023&#24180;&#38388;&#26031;&#25289;&#22827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#35752;&#20250;&#30340;&#19968;&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;&#20102;5017&#20221;&#28085;&#30422;&#19971;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#25991;&#26723;&#26631;&#26377;&#20116;&#31867;&#21629;&#21517;&#23454;&#20307;&#65292;&#27599;&#20010;&#23454;&#20307;&#30001;&#31867;&#21035;&#12289;&#24341;&#29992;&#35789;&#21644;&#21807;&#19968;&#36328;&#35821;&#35328;&#26631;&#35782;&#31526;&#25551;&#36848;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#35757;&#32451;&#35843;&#25972;&#30340;&#25968;&#25454;&#38598;&#21010;&#20998; - &#21333;&#20010;&#20027;&#39064;&#21010;&#20998;&#21644;&#36328;&#20027;&#39064;&#21010;&#20998;&#12290;&#23545;&#20110;&#27599;&#20010;&#21010;&#20998;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#32622;&#20102;&#22522;&#20934;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#25552;&#21450;&#35782;&#21035;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;mT5-large&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#24341;&#29992;&#35789;&#21270;&#21644;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00482v1 Announce Type: cross  Abstract: This paper presents a corpus manually annotated with named entities for six Slavic languages - Bulgarian, Czech, Polish, Slovenian, Russian, and Ukrainian. This work is the result of a series of shared tasks, conducted in 2017-2023 as a part of the Workshops on Slavic Natural Language Processing. The corpus consists of 5 017 documents on seven topics. The documents are annotated with five classes of named entities. Each entity is described by a category, a lemma, and a unique cross-lingual identifier. We provide two train-tune dataset splits - single topic out and cross topics. For each split, we set benchmarks using a transformer-based neural network architecture with the pre-trained multilingual models - XLM-RoBERTa-large for named entity mention recognition and categorization, and mT5-large for named entity lemmatization and linking.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2404.00226</link><description>&lt;p&gt;
&#24819;&#35201;&#30340;&#35774;&#35745;&#65306;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Design as Desired: Utilizing Visual Question Answering for Multimodal Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#25152;&#38656;&#30149;&#29702;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#19987;&#23478;&#27880;&#37322;&#30340;&#38382;&#39064;-&#31572;&#26696;&#23545;&#35774;&#35745;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#22312;&#21307;&#30103;&#39046;&#22495;&#23637;&#31034;&#20102;&#20854;&#28508;&#21147;&#65292;&#20174;&#25104;&#23545;&#30340;&#21307;&#30103;&#25253;&#21578;&#20013;&#23398;&#20064;&#21307;&#23398;&#35270;&#35273;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39044;&#35757;&#32451;&#20219;&#21153;&#38656;&#35201;&#20020;&#24202;&#21307;&#29983;&#39069;&#22806;&#30340;&#27880;&#37322;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#26410;&#33021;&#26126;&#30830;&#24341;&#23548;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#30149;&#29702;&#29305;&#24449;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#36827;&#34892;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#22242;&#38431;&#65292;&#20197;&#24341;&#23548;&#26694;&#26550;&#19987;&#27880;&#20110;&#30446;&#26631;&#30149;&#29702;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21307;&#30103;&#25253;&#21578;&#20013;&#30340;&#25551;&#36848;&#35774;&#35745;&#20102;&#19982;&#19981;&#21516;&#30142;&#30149;&#30456;&#20851;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#36825;&#26377;&#21161;&#20110;&#26694;&#26550;&#22312;&#39044;&#35757;&#32451;&#20013;&#26080;&#38656;&#19987;&#23478;&#39069;&#22806;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#20934;&#25991;&#26412;&#29305;&#24449;&#36716;&#25442;&#22120;&#27169;&#22359;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#36716;&#25442;&#21040;&#25509;&#36817;&#25991;&#26412;&#39046;&#22495;&#30340;&#20934;&#25991;&#26412;&#31354;&#38388;&#26469;&#36741;&#21161;&#39044;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00226v1 Announce Type: cross  Abstract: Multimodal pre-training demonstrates its potential in the medical domain, which learns medical visual representations from paired medical reports. However, many pre-training tasks require extra annotations from clinicians, and most of them fail to explicitly guide the model to learn the desired features of different pathologies. To the best of our knowledge, we are the first to utilize Visual Question Answering (VQA) for multimodal pre-training to guide the framework focusing on targeted pathological features. In this work, we leverage descriptions in medical reports to design multi-granular question-answer pairs associated with different diseases, which assist the framework in pre-training without requiring extra annotations from experts. We also propose a novel pre-training framework with a quasi-textual feature transformer, a module designed to transform visual features into a quasi-textual space closer to the textual domain via a c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;</title><link>https://arxiv.org/abs/2404.00076</link><description>&lt;p&gt;
&#20351;&#29992;&#20498;&#32622;&#26631;&#31614;&#30340;&#21518;&#38376;&#26041;&#27861;&#65306;&#33039;&#26631;&#31614;&#32763;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Backdoor Approach with Inverted Labels Using Dirty Label-Flipping Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#65292;&#21033;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#22312;&#36873;&#23450;&#30340;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#23454;&#29616;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22768;&#38899;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#32463;&#24120;&#20351;&#29992;&#20844;&#20849;&#25110;&#31532;&#19977;&#26041;&#25968;&#25454;&#65292;&#36825;&#21487;&#33021;&#26159;&#19981;&#20934;&#30830;&#30340;&#12290;&#36825;&#20351;&#24471;&#35757;&#32451;&#22312;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#28508;&#22312;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#12290;&#22312;&#36825;&#31181;&#25915;&#20987;&#31867;&#22411;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#20351;&#29992;&#27602;&#21270;&#25968;&#25454;&#26469;&#35757;&#32451;DNN&#27169;&#22411;&#65292;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#21478;&#19968;&#31181;&#23545;&#25105;&#20204;&#30340;&#30740;&#31350;&#38750;&#24120;&#30456;&#20851;&#30340;&#25968;&#25454;&#27602;&#21270;&#25915;&#20987;&#31867;&#22411;&#26159;&#26631;&#31614;&#32763;&#36716;&#65292;&#25915;&#20987;&#32773;&#22312;&#20854;&#20013;&#25805;&#32437;&#25968;&#25454;&#23376;&#38598;&#30340;&#26631;&#31614;&#12290;&#24050;&#32463;&#35777;&#26126;&#65292;&#21363;&#20351;&#26159;&#33021;&#21147;&#26377;&#38480;&#30340;&#25915;&#20987;&#32773;&#65292;&#36825;&#20123;&#25915;&#20987;&#20063;&#21487;&#33021;&#26497;&#22823;&#22320;&#38477;&#20302;&#31995;&#32479;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;DirtyFlipping&#8221;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#33039;&#26631;&#31614;&#25216;&#26415;&#65292;&#8220;&#26631;&#31614;&#23545;&#26631;&#31614;&#8221;&#65292;&#22312;&#19982;&#30446;&#26631;&#31867;&#21035;&#30456;&#20851;&#30340;&#36873;&#23450;&#25968;&#25454;&#27169;&#24335;&#20013;&#36755;&#20837;&#35302;&#21457;&#22120;&#65288;&#25293;&#25163;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38544;&#34109;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00076v1 Announce Type: cross  Abstract: Audio-based machine learning systems frequently use public or third-party data, which might be inaccurate. This exposes deep neural network (DNN) models trained on such data to potential data poisoning attacks. In this type of assault, attackers can train the DNN model using poisoned data, potentially degrading its performance. Another type of data poisoning attack that is extremely relevant to our investigation is label flipping, in which the attacker manipulates the labels for a subset of data. It has been demonstrated that these assaults may drastically reduce system performance, even for attackers with minimal abilities. In this study, we propose a backdoor attack named 'DirtyFlipping', which uses dirty label techniques, "label-on-label", to input triggers (clapping) in the selected data patterns associated with the target class, thereby enabling a stealthy backdoor.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#32034;&#19981;&#21516;&#35805;&#35821;&#26631;&#27880;&#26694;&#26550;&#38388;&#30340;&#35805;&#35821;&#20851;&#31995;&#28165;&#21333;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20196</link><description>&lt;p&gt;
&#19981;&#21516;&#35805;&#35821;&#26631;&#27880;&#26694;&#26550;&#30340;&#35805;&#35821;&#20851;&#31995;&#33258;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Automatic Alignment of Discourse Relations of Different Discourse Annotation Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20196
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#32034;&#19981;&#21516;&#35805;&#35821;&#26631;&#27880;&#26694;&#26550;&#38388;&#30340;&#35805;&#35821;&#20851;&#31995;&#28165;&#21333;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#21322;&#33258;&#21160;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#35805;&#35821;&#35821;&#26009;&#24211;&#22522;&#20110;&#19981;&#21516;&#30340;&#26694;&#26550;&#36827;&#34892;&#26631;&#27880;&#65292;&#22312;&#21442;&#25968;&#21644;&#20851;&#31995;&#30340;&#23450;&#20041;&#20197;&#21450;&#32467;&#26500;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#23613;&#31649;&#34920;&#38754;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#36825;&#20123;&#26694;&#26550;&#20998;&#20139;&#35805;&#35821;&#20851;&#31995;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#36825;&#20123;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#19981;&#21516;&#26694;&#26550;&#20013;&#20351;&#29992;&#30340;&#20851;&#31995;&#28165;&#21333;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20010;&#38382;&#39064;&#26377;&#21161;&#20110;&#25972;&#21512;&#35805;&#35821;&#29702;&#35770;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#26694;&#26550;&#19979;&#26631;&#27880;&#30340;&#35805;&#35821;&#35821;&#26009;&#24211;&#30340;&#20114;&#25805;&#20316;&#24615;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#25506;&#32034;&#35805;&#35821;&#20851;&#31995;&#28165;&#21333;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#21463;&#21040;&#20102;&#35805;&#35821;&#21010;&#20998;&#26631;&#20934;&#30340;&#19981;&#21516;&#38480;&#21046;&#65292;&#36890;&#24120;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#21644;&#25163;&#21160;&#26816;&#26597;&#12290;&#19968;&#20123;&#21322;&#33258;&#21160;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#21516;&#26102;&#22312;&#22810;&#20010;&#26694;&#26550;&#20013;&#36827;&#34892;&#26631;&#27880;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20196v1 Announce Type: new  Abstract: Existing discourse corpora are annotated based on different frameworks, which show significant dissimilarities in definitions of arguments and relations and structural constraints. Despite surface differences, these frameworks share basic understandings of discourse relations. The relationship between these frameworks has been an open research question, especially the correlation between relation inventories utilized in different frameworks. Better understanding of this question is helpful for integrating discourse theories and enabling interoperability of discourse corpora annotated under different frameworks. However, studies that explore correlations between discourse relation inventories are hindered by different criteria of discourse segmentation, and expert knowledge and manual examination are typically needed. Some semi-automatic methods have been proposed, but they rely on corpora annotated in multiple frameworks in parallel. In 
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25298;&#32477;&#26426;&#21046;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#38752;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;RLKF&#12290;</title><link>https://arxiv.org/abs/2403.18349</link><description>&lt;p&gt;
&#25298;&#32477;&#25552;&#39640;&#21487;&#38752;&#24615;&#65306;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#30693;&#35782;&#21453;&#39304;&#35757;&#32451;LLMs&#25298;&#32477;&#26410;&#30693;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rejection Improves Reliability: Training LLMs to Refuse Unknown Questions Using RL from Knowledge Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18349
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25298;&#32477;&#26426;&#21046;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#38752;&#24615;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;RLKF&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#32463;&#24120;&#29983;&#25104;&#38169;&#35823;&#36755;&#20986;&#65292;&#34987;&#31216;&#20026;&#24187;&#24819;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#36776;&#21035;&#36229;&#20986;&#20854;&#30693;&#35782;&#33539;&#22260;&#30340;&#38382;&#39064;&#26102;&#30340;&#23616;&#38480;&#24615;&#12290;&#34429;&#28982;&#35299;&#20915;&#24187;&#24819;&#19968;&#30452;&#26159;&#30740;&#31350;&#30340;&#28966;&#28857;&#65292;&#20197;&#24448;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#27491;&#30830;&#24615;&#32780;&#26410;&#20805;&#20998;&#32771;&#34385;&#25298;&#32477;&#26426;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#25298;&#32477;&#30340;&#20316;&#29992;&#65292;&#24341;&#20837;&#20102;&#27169;&#22411;&#21487;&#38752;&#24615;&#30340;&#27010;&#24565;&#20197;&#21450;&#30456;&#24212;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#34913;&#37327;&#20102;&#27169;&#22411;&#22312;&#25552;&#20379;&#20934;&#30830;&#21709;&#24212;&#30340;&#21516;&#26102;&#65292;&#28789;&#27963;&#25298;&#32477;&#36229;&#20986;&#20854;&#30693;&#35782;&#36793;&#30028;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#24187;&#24819;&#12290;&#20026;&#20102;&#25552;&#39640;LLMs&#22266;&#26377;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLKF&#65289;&#30340;&#26032;&#23545;&#40784;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18349v1 Announce Type: new  Abstract: Large Language Models (LLMs) often generate erroneous outputs, known as hallucinations, due to their limitations in discerning questions beyond their knowledge scope. While addressing hallucination has been a focal point in research, previous efforts primarily concentrate on enhancing correctness without giving due consideration to the significance of rejection mechanisms. In this paper, we conduct a comprehensive examination of the role of rejection, introducing the notion of model reliability along with corresponding metrics. These metrics measure the model's ability to provide accurate responses while adeptly rejecting questions exceeding its knowledge boundaries, thereby minimizing hallucinations. To improve the inherent reliability of LLMs, we present a novel alignment framework called Reinforcement Learning from Knowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically determine the model's knowledge boundary and 
&lt;/p&gt;</description></item><item><title>Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18276</link><description>&lt;p&gt;
RankMamba&#65292;&#22312;Transformer&#26102;&#20195;&#23545;Mamba&#25991;&#26723;&#25490;&#21517;&#24615;&#33021;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
RankMamba, Benchmarking Mamba's Document Ranking Performance in the Era of Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18276
&lt;/p&gt;
&lt;p&gt;
Mamba&#27169;&#22411;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#32463;&#20856;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;--&#25991;&#26723;&#25490;&#21517;&#20013;&#23637;&#29616;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#32467;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#20449;&#24687;&#26816;&#32034;(IR)&#31561;&#22810;&#20010;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;--&#27880;&#24847;&#21147;&#65292;&#22312;&#35757;&#32451;&#20013;&#38656;&#35201;$O(n^2)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22312;&#25512;&#26029;&#20013;&#38656;&#35201;$O(n)$&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#35768;&#22810;&#24037;&#20316;&#24050;&#32463;&#25552;&#20986;&#25913;&#36827;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#27604;&#22914;Flash Attention&#21644;Multi-query Attention&#12290;&#21478;&#19968;&#26041;&#38754;&#30340;&#24037;&#20316;&#26088;&#22312;&#35774;&#35745;&#26032;&#30340;&#26426;&#21046;&#26469;&#21462;&#20195;&#27880;&#24847;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#27169;&#22411;&#32467;&#26500;--Mamba&#65292;&#22312;&#22810;&#20010;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#19982;Transformer&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18276v1 Announce Type: cross  Abstract: Transformer structure has achieved great success in multiple applied machine learning communities, such as natural language processing (NLP), computer vision (CV) and information retrieval (IR). Transformer architecture's core mechanism -- attention requires $O(n^2)$ time complexity in training and $O(n)$ time complexity in inference. Many works have been proposed to improve the attention mechanism's scalability, such as Flash Attention and Multi-query Attention. A different line of work aims to design new mechanisms to replace attention. Recently, a notable model structure -- Mamba, which is based on state space models, has achieved transformer-equivalent performance in multiple sequence modeling tasks.   In this work, we examine \mamba's efficacy through the lens of a classical IR task -- document ranking. A reranker model takes a query and a document as input, and predicts a scalar relevance score. This task demands the language mod
&lt;/p&gt;</description></item><item><title>MasonTigers&#22312;SemEval-2024 Task 1&#20013;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#65292;&#22312;&#22788;&#29702;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14990</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024&#20219;&#21153;1&#20013;&#30340;&#38598;&#25104;&#26041;&#27861;&#30740;&#31350;&#65306;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14990
&lt;/p&gt;
&lt;p&gt;
MasonTigers&#22312;SemEval-2024 Task 1&#20013;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#65292;&#22312;&#22788;&#29702;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#21442;&#19982;SemEval-2024&#20219;&#21153;1-&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#24037;&#20316;&#12290;&#35813;&#20219;&#21153;&#28085;&#30422;&#20102;&#28085;&#30422;&#20102;&#30417;&#30563;&#65288;Track A&#65289;&#12289;&#26080;&#30417;&#30563;&#65288;Track B&#65289;&#21644;&#36328;&#35821;&#35328;&#65288;Track C&#65289;&#26041;&#27861;&#65292;&#28041;&#21450;14&#31181;&#19981;&#21516;&#35821;&#35328;&#12290;MasonTigers&#26159;&#23569;&#25968;&#21516;&#26102;&#21442;&#19982;&#20102;&#19977;&#20010;track&#20013;&#25152;&#26377;&#35821;&#35328;&#30340;&#20004;&#25903;&#22242;&#38431;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Track A&#20013;&#25490;&#21517;&#20174;&#31532;11&#21040;&#31532;21&#65292;&#22312;Track B&#20013;&#25490;&#21517;&#20174;&#31532;1&#21040;&#31532;8&#65292;&#22312;Track C&#20013;&#25490;&#21517;&#20174;&#31532;5&#21040;&#31532;12&#12290;&#22312;&#36981;&#24490;&#29305;&#23450;&#20219;&#21153;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14990v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MMIDR&#26694;&#26550;&#65292;&#29992;&#20110;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.14171</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MMIDR&#26694;&#26550;&#65292;&#29992;&#20110;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#33258;&#21160;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25496;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#20197;&#25104;&#26412;&#25928;&#30410;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#26041;&#24335;&#25945;&#23548;LLMs&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMIDR&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25945;&#23548;LLMs&#20026;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#27969;&#30021;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#23558;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#24403;&#30340;&#25351;&#20196;&#25191;&#34892;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#35270;&#35282;&#21644;&#31649;&#36947;&#12290;&#35813;&#31649;&#36947;&#21253;&#25324;&#19968;&#20010;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#27169;&#22359;&#21644;&#19968;&#20010;&#35777;&#25454;&#26816;&#32034;&#27169;&#22359;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22788;&#29702;&#36807;&#30340;&#20869;&#23481;&#25552;&#31034;&#19987;&#26377;&#30340;LLMs&#20026;&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#25552;&#21462;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14171v1 Announce Type: new  Abstract: Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore
&lt;/p&gt;</description></item><item><title>&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#22352;&#26631;&#32423;&#25511;&#21046;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13801</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25919;&#31574;&#65306;&#19982;LLMs&#19968;&#36215;&#36827;&#34892;&#22352;&#26631;&#32423;&#20307;&#24577;&#25511;&#21046;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13801
&lt;/p&gt;
&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#22352;&#26631;&#32423;&#25511;&#21046;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;LLMs&#19968;&#36215;&#35299;&#20915;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;LLMs&#24050;&#32463;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#20013;&#32423;&#31574;&#30053;&#20195;&#30721;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#33719;&#21462;&#20219;&#21153;&#21644;&#22330;&#26223;&#23545;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21046;&#23450;&#34892;&#21160;&#35268;&#21010;&#65292;&#24182;&#36755;&#20986;&#22352;&#26631;&#32423;&#25511;&#21046;&#21629;&#20196;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20316;&#20026;&#25919;&#31574;&#30340;&#20013;&#38388;&#34920;&#31034;&#20195;&#30721;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25552;&#31034;&#20223;&#30495;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23454;&#39564;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#19982;&#32570;&#24109;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26377;&#28508;&#21147;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#20174;&#24050;&#30693;&#20219;&#21153;&#36716;&#31227;&#21040;&#20197;&#21069;&#26410;&#35265;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13801v1 Announce Type: cross  Abstract: We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11169</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32416;&#27491;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Correcting misinformation on social media with a large language model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11169
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MUSE&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#35775;&#38382;&#26368;&#26032;&#20449;&#24687;&#24182;&#35780;&#20272;&#21487;&#20449;&#24230;&#65292;&#20197;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#19978;&#35823;&#20449;&#24687;&#32416;&#27491;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20449;&#24687;&#20250;&#30772;&#22351;&#20844;&#20247;&#23545;&#31185;&#23398;&#21644;&#27665;&#20027;&#30340;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#19981;&#20934;&#30830;&#20449;&#24687;&#20250;&#36805;&#36895;&#20256;&#25773;&#12290;&#19987;&#23478;&#21644;&#26222;&#36890;&#20154;&#36890;&#36807;&#25163;&#21160;&#35782;&#21035;&#21644;&#35299;&#37322;&#19981;&#20934;&#30830;&#20449;&#24687;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#32416;&#27491;&#35823;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24456;&#38590;&#25193;&#23637;&#65292;&#36825;&#26159;&#19968;&#20010;&#25285;&#24551;&#65292;&#22240;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#31561;&#25216;&#26415;&#20351;&#35823;&#20449;&#24687;&#26356;&#23481;&#26131;&#29983;&#25104;&#12290;LLMs&#36824;&#20855;&#26377;&#22810;&#21151;&#33021;&#33021;&#21147;&#65292;&#21487;&#20197;&#21152;&#36895;&#32416;&#27491;&#35823;&#20449;&#24687;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30001;&#20110;&#32570;&#20047;&#26368;&#26032;&#20449;&#24687;&#12289;&#20542;&#21521;&#20110;&#29983;&#25104;&#20284;&#26159;&#32780;&#38750;&#30340;&#20869;&#23481;&#21644;&#24341;&#29992;&#20197;&#21450;&#26080;&#27861;&#22788;&#29702;&#22810;&#27169;&#24577;&#20449;&#24687;&#32780;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSE&#65292;&#36825;&#26159;&#19968;&#20010;&#24102;&#26377;&#26368;&#26032;&#20449;&#24687;&#35775;&#38382;&#21644;&#21487;&#20449;&#24230;&#35780;&#20272;&#30340;LLM&#12290;&#36890;&#36807;&#26816;&#32034;&#19978;&#19979;&#25991;&#35777;&#25454;&#21644;&#21453;&#39539;&#65292;MUSE&#21487;&#20197;&#25552;&#20379;&#20934;&#30830;&#21487;&#20449;&#30340;&#35299;&#37322;&#21644;&#21442;&#32771;&#12290;&#23427;&#36824;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11169v1 Announce Type: cross  Abstract: Misinformation undermines public trust in science and democracy, particularly on social media where inaccuracies can spread rapidly. Experts and laypeople have shown to be effective in correcting misinformation by manually identifying and explaining inaccuracies. Nevertheless, this approach is difficult to scale, a concern as technologies like large language models (LLMs) make misinformation easier to produce. LLMs also have versatile capabilities that could accelerate misinformation correction; however, they struggle due to a lack of recent information, a tendency to produce plausible but false content and references, and limitations in addressing multimodal information. To address these issues, we propose MUSE, an LLM augmented with access to and credibility evaluation of up-to-date information. By retrieving contextual evidence and refutations, MUSE can provide accurate and trustworthy explanations and references. It also describes 
&lt;/p&gt;</description></item><item><title>&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.09722</link><description>&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Prediction of readmission of patients by extracting biomedical concepts from clinical texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09722
&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#25552;&#21462;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#39044;&#27979;&#24739;&#32773;&#30340;&#20877;&#20837;&#38498;&#24773;&#20917;&#65292;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23384;&#22312;&#22823;&#37327;&#30340;&#30005;&#23376;&#20581;&#24247;&#25968;&#25454;&#20026;&#36827;&#34892;&#26088;&#22312;&#25913;&#21892;&#20026;&#24739;&#32773;&#25552;&#20379;&#30340;&#21307;&#30103;&#26381;&#21153;&#24182;&#38477;&#20302;&#21307;&#30103;&#31995;&#32479;&#25104;&#26412;&#30340;&#30740;&#31350;&#21019;&#36896;&#20102;&#28508;&#22312;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#21307;&#23398;&#39046;&#22495;&#22791;&#21463;&#20851;&#27880;&#30340;&#19968;&#20010;&#35805;&#39064;&#26159;&#35782;&#21035;&#20986;&#21018;&#20174;&#21307;&#38498;&#20986;&#38498;&#21518;&#21487;&#33021;&#24456;&#24555;&#20877;&#27425;&#20837;&#38498;&#30340;&#24739;&#32773;&#12290;&#36825;&#31181;&#35782;&#21035;&#21487;&#20197;&#24110;&#21161;&#21307;&#29983;&#36873;&#25321;&#36866;&#24403;&#30340;&#27835;&#30103;&#26041;&#27861;&#65292;&#20174;&#32780;&#20943;&#23569;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#30340;&#27604;&#29575;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#27835;&#30103;&#25104;&#26412;&#38477;&#20302;&#12290;&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#21033;&#29992;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#21644;&#23545;&#24739;&#32773;&#30005;&#23376;&#25991;&#20214;&#20013;&#30340;&#20986;&#38498;&#25253;&#21578;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#26469;&#39044;&#27979;&#24739;&#32773;&#20877;&#27425;&#20837;&#38498;&#24773;&#20917;&#12290;&#20026;&#27492;&#65292;&#20351;&#29992;&#20004;&#31181;&#26041;&#27861;&#35780;&#20272;&#20102;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65306;&#35789;&#34955;&#27169;&#22411;&#21644;&#27010;&#24565;&#34955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09722v1 Announce Type: cross  Abstract: Today, the existence of a vast amount of electronic health data has created potential capacities for conducting studies aiming to improve the medical services provided to patients and reduce the costs of the healthcare system. One of the topics that has been receiving attention in the field of medicine in recent years is the identification of patients who are likely to be re-hospitalized shortly after being discharged from the hospital. This identification can help doctors choose appropriate treatment methods, thereby reducing the rate of patient re-hospitalization and resulting in effective treatment cost reduction. In this study, the prediction of patient re-hospitalization using text mining approaches and the processing of discharge report texts in the patient's electronic file has been discussed. To this end, the performance of various machine learning models has been evaluated using two approaches: bag of word and bag of concept, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;</title><link>https://arxiv.org/abs/2403.09516</link><description>&lt;p&gt;
&#21033;&#29992;&#20856;&#22411;&#34920;&#31034;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#32780;&#19981;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Leveraging Prototypical Representations for Mitigating Social Bias without Demographic Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09516
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20856;&#22411;&#20154;&#21475;&#32479;&#35745;&#25991;&#26412;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#27491;&#21017;&#21270;&#39033;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#20943;&#36731;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#21516;&#26102;&#19981;&#38656;&#35201;&#20381;&#36182;&#26174;&#24335;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;&#36890;&#24120;&#38656;&#35201;&#35782;&#21035;&#19982;&#27599;&#20010;&#25968;&#25454;&#26679;&#26412;&#30456;&#20851;&#32852;&#30340;&#31038;&#20250;&#32676;&#20307;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAFair&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#38382;&#39064;&#12290;&#19982;&#20381;&#36182;&#26174;&#24335;&#20154;&#21475;&#32479;&#35745;&#26631;&#31614;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#27492;&#31867;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#39044;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20856;&#22411;&#25991;&#26412;&#65292;&#24182;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#21152;&#20837;&#19968;&#20010;&#27491;&#21017;&#21270;&#39033;&#26469;&#20943;&#36731;&#27169;&#22411;&#34920;&#31034;&#20013;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;&#27169;&#22411;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20043;&#21069;&#19981;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#20154;&#21475;&#32479;&#35745;&#26631;&#27880;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#20248;&#20110;&#24120;&#35265;&#30340;&#21435;&#20559;&#35265;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09516v1 Announce Type: new  Abstract: Mitigating social biases typically requires identifying the social groups associated with each data sample. In this paper, we present DAFair, a novel approach to address social bias in language models. Unlike traditional methods that rely on explicit demographic labels, our approach does not require any such information. Instead, we leverage predefined prototypical demographic texts and incorporate a regularization term during the fine-tuning process to mitigate bias in the model's representations. Our empirical results across two tasks and two models demonstrate the effectiveness of our method compared to previous approaches that do not rely on labeled data. Moreover, with limited demographic-annotated data, our approach outperforms common debiasing approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.09054</link><description>&lt;p&gt;
Keyformer&#65306;&#36890;&#36807;&#20851;&#38190;&#26631;&#35760;&#36873;&#25321;&#20943;&#23569;KV&#32531;&#23384;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#29983;&#25104;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Keyformer&#8221;&#30340;&#21019;&#26032;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#36873;&#25321;&#20851;&#38190;&#26631;&#35760;&#26469;&#20943;&#23569;KV&#32531;&#23384;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#22522;&#30784;&#26550;&#26500;&#12290;&#22312;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25512;&#26029;&#36807;&#31243;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#38454;&#27573;&#65306;&#25552;&#31034;&#22788;&#29702;&#21644;&#26631;&#35760;&#29983;&#25104;&#12290;&#26631;&#35760;&#29983;&#25104;&#65292;&#26500;&#25104;&#20102;&#22823;&#37096;&#20998;&#35745;&#31639;&#24037;&#20316;&#37327;&#65292;&#20027;&#35201;&#28041;&#21450;&#21521;&#37327;-&#30697;&#38453;&#20056;&#27861;&#21644;&#19982;&#38190;-&#20540;(KV)&#32531;&#23384;&#20132;&#20114;&#12290;&#30001;&#20110;&#20174;&#23384;&#20648;&#31995;&#32479;&#20256;&#36755;&#26435;&#37325;&#21644;KV&#32531;&#23384;&#20540;&#21040;&#35745;&#31639;&#21333;&#20803;&#30340;&#24320;&#38144;&#65292;&#36825;&#19968;&#38454;&#27573;&#21463;&#21040;&#20869;&#23384;&#24102;&#23485;&#30340;&#38480;&#21046;&#12290;&#36825;&#31181;&#20869;&#23384;&#29942;&#39048;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#21644;&#22823;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#24212;&#29992;&#20013;&#23588;&#20026;&#31361;&#20986;&#65292;&#36825;&#20004;&#32773;&#23545;LLMs&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;  &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25512;&#26029;&#26102;&#38388;&#26041;&#27861;&#8220;Keyformer&#8221;&#65292;&#20197;&#32531;&#35299;&#19982;KV&#32531;&#23384;&#22823;&#23567;&#21644;&#20869;&#23384;&#24102;&#23485;&#21033;&#29992;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;Keyformer&#21033;&#29992;&#20102;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#22823;&#32422;90
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09054v1 Announce Type: cross  Abstract: Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs.   This paper introduces "Keyformer", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.07865</link><description>&lt;p&gt;
&#36890;&#36807;&#20195;&#30721;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Safety Generalization Challenges of Large Language Models via Code
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;CodeAttack&#26694;&#26550;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23433;&#20840;&#27867;&#21270;&#65292;&#30740;&#31350;&#21457;&#29616;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#31561;&#26368;&#26032;&#27169;&#22411;&#23384;&#22312;&#20195;&#30721;&#36755;&#20837;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#24102;&#26469;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#23427;&#20204;&#28508;&#22312;&#35823;&#29992;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;CodeAttack&#65292;&#19968;&#20010;&#23558;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36716;&#25442;&#20026;&#20195;&#30721;&#36755;&#20837;&#30340;&#26694;&#26550;&#65292;&#20026;&#27979;&#35797;LLMs&#30340;&#23433;&#20840;&#27867;&#21270;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29615;&#22659;&#12290;&#25105;&#20204;&#23545;&#21253;&#25324;GPT-4&#12289;Claude-2&#21644;Llama-2&#31995;&#21015;&#22312;&#20869;&#30340;&#26368;&#26032;LLMs&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#20195;&#30721;&#36755;&#20837;&#23384;&#22312;&#20849;&#21516;&#30340;&#23433;&#20840;&#28431;&#27934;&#65306;CodeAttack&#22312;&#36229;&#36807;80%&#30340;&#26102;&#38388;&#20869;&#22987;&#32456;&#32469;&#36807;&#25152;&#26377;&#27169;&#22411;&#30340;&#23433;&#20840;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07865v1 Announce Type: cross  Abstract: The rapid advancement of Large Language Models (LLMs) has brought about remarkable capabilities in natural language processing but also raised concerns about their potential misuse. While strategies like supervised fine-tuning and reinforcement learning from human feedback have enhanced their safety, these methods primarily focus on natural languages, which may not generalize to other domains. This paper introduces CodeAttack, a framework that transforms natural language inputs into code inputs, presenting a novel environment for testing the safety generalization of LLMs. Our comprehensive studies on state-of-the-art LLMs including GPT-4, Claude-2, and Llama-2 series reveal a common safety vulnerability of these models against code input: CodeAttack consistently bypasses the safety guardrails of all models more than 80\% of the time. Furthermore, we find that a larger distribution gap between CodeAttack and natural language leads to we
&lt;/p&gt;</description></item><item><title>PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05297</link><description>&lt;p&gt;
PEEB&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#35821;&#35328;&#29942;&#39048;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05297
&lt;/p&gt;
&lt;p&gt;
PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#21253;&#21547;{text encoder&#24050;&#30693;&#30340;&#31867;&#21517;&#31216;}&#30340;&#25552;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CLIP&#22312;&#26032;&#31867;&#21035;&#25110;&#20854;&#21517;&#31216;&#24456;&#23569;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#40479;&#31867;&#30340;&#23398;&#21517;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38024;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEEB - &#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#65288;1&#65289;&#23558;&#31867;&#21035;&#21517;&#31216;&#34920;&#36798;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65307;&#21644;&#65288;2&#65289;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#35745;&#31639;&#29992;&#20110;&#20998;&#31867;&#30340;&#36923;&#36753;&#20998;&#25968;&#12290;&#22312;&#19968;&#20010;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#31867;&#21035;&#21517;&#31216;&#26159;&#26410;&#30693;&#30340;&#65292;PEEB&#22312;&#20934;&#30830;&#24615;&#19978;&#22823;&#24133;&#20248;&#20110;CLIP&#65288;&#32422;&#20026;10&#20493;&#65289;&#12290;&#19982;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;PEEB&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19978;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;88.80%&#20934;&#30830;&#29575;&#65289;&#65292;&#32780;&#19988;&#36824;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35753;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#20197;&#24418;&#25104;&#26032;&#30340;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04787</link><description>&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#21644;&#23436;&#21892;&#36807;&#21435;&#26469;&#19981;&#26029;&#28436;&#36827;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Ever-Evolving Memory by Blending and Refining the Past
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04787
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;CREEM&#65292;&#36890;&#36807;&#28151;&#21512;&#36807;&#21435;&#35760;&#24518;&#24182;&#24341;&#20837;&#23436;&#21892;&#36807;&#31243;&#26469;&#23454;&#29616;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31867;&#20284;&#20154;&#31867;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#26500;&#24314;&#38271;&#26399;&#35760;&#24518;&#33267;&#20851;&#37325;&#35201;&#12290;&#26500;&#24314;&#35760;&#24518;&#30340;&#19968;&#20010;&#22825;&#30495;&#26041;&#27861;&#21487;&#33021;&#21482;&#26159;&#21015;&#20986;&#24635;&#32467;&#30340;&#23545;&#35805;&#12290;&#28982;&#32780;&#65292;&#24403;&#35828;&#35805;&#32773;&#30340;&#29366;&#24577;&#38543;&#26102;&#38388;&#21464;&#21270;&#26102;&#65292;&#36825;&#26679;&#20570;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#24182;&#31215;&#32047;&#30683;&#30462;&#20449;&#24687;&#12290;&#35760;&#24518;&#20445;&#25345;&#26377;&#32452;&#32455;&#23545;&#20110;&#38477;&#20302;&#22238;&#24212;&#29983;&#25104;&#22120;&#30340;&#28151;&#20081;&#24456;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38271;&#26399;&#23545;&#35805;&#35760;&#24518;&#26041;&#26696;&#65292;CREEM&#12290;&#19982;&#20165;&#22522;&#20110;&#24403;&#21069;&#23545;&#35805;&#26500;&#24314;&#35760;&#24518;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#35760;&#24518;&#24418;&#25104;&#36807;&#31243;&#20013;&#28151;&#21512;&#36807;&#21435;&#30340;&#35760;&#24518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#21892;&#36807;&#31243;&#26469;&#22788;&#29702;&#22810;&#20313;&#25110;&#36807;&#26102;&#20449;&#24687;&#12290;&#36825;&#31181;&#21019;&#26032;&#24615;&#26041;&#27861;&#36890;&#36807;&#30830;&#20445;&#19968;&#20010;&#26356;&#21152;&#30693;&#24773;&#21644;&#21160;&#24577;&#28436;&#21464;&#30340;&#38271;&#26399;&#35760;&#24518;&#65292;&#26088;&#22312;&#25552;&#39640;&#32842;&#22825;&#26426;&#22120;&#20154;&#22238;&#24212;&#30340;&#25972;&#20307;&#25913;&#36827;&#21644;&#36830;&#36143;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04787v1 Announce Type: cross  Abstract: For a human-like chatbot, constructing a long-term memory is crucial. A naive approach for making a memory could be simply listing the summarized dialogue. However, this can lead to problems when the speaker's status change over time and contradictory information gets accumulated. It is important that the memory stays organized to lower the confusion for the response generator. In this paper, we propose a novel memory scheme for long-term conversation, CREEM. Unlike existing approaches that construct memory based solely on current sessions, our proposed model blending past memories during memory formation. Additionally, we introduce refining process to handle redundant or outdated information. This innovative approach seeks for overall improvement and coherence of chatbot responses by ensuring a more informed and dynamically evolving long-term memory.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.03894</link><description>&lt;p&gt;
IRCoder: &#20013;&#38388;&#34920;&#31034;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#24050;&#36805;&#36895;&#25104;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;LM&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20195;&#30721;-LMs&#65288;&#21363;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;LMs&#65289;&#30340;&#22810;&#35821;&#35328;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#22914;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#20107;&#21518;LM&#35843;&#25972;&#65292;&#20197;&#21450;&#21033;&#29992;&#21407;&#22987;&#25991;&#26412;&#20869;&#23481;&#20043;&#22806;&#30340;&#25968;&#25454;&#28304;&#65292;&#35201;&#31232;&#23569;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#20195;&#30721;-LMs&#20165;&#22312;&#28304;&#20195;&#30721;&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;&#36328;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#24182;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#21069;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;SLTrans&#65292;&#19968;&#20010;&#30001;&#36817;400&#19975;&#20010;&#33258;&#21253;&#21547;&#28304;&#20195;&#30721;&#25991;&#20214;&#32452;&#25104;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
&lt;/p&gt;</description></item><item><title>FaaF&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#23454;&#39564;&#35777;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#21644;&#38754;&#21521;RAG&#20107;&#23454;&#22238;&#24518;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;LM&#35782;&#21035;&#19981;&#25903;&#25345;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#25104;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.03888</link><description>&lt;p&gt;
FaaF&#65306;&#20316;&#20026;RAG&#31995;&#32479;&#35780;&#20272;&#30340;&#20107;&#23454;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
FaaF: Facts as a Function for the evaluation of RAG systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03888
&lt;/p&gt;
&lt;p&gt;
FaaF&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#23454;&#39564;&#35777;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#21644;&#38754;&#21521;RAG&#20107;&#23454;&#22238;&#24518;&#35780;&#20272;&#30340;&#26694;&#26550;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;LM&#35782;&#21035;&#19981;&#25903;&#25345;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#25928;&#29575;&#21644;&#25104;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21442;&#32771;&#36164;&#26009;&#20013;&#20934;&#30830;&#25552;&#21462;&#20107;&#23454;&#23545;&#20110;&#35780;&#20272;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#31995;&#32479;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#25506;&#26597;&#20102;&#26816;&#32034;&#21644;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#21487;&#38752;&#39640;&#25928;&#22320;&#25191;&#34892;&#36825;&#31181;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#36890;&#36807;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22120;&#36827;&#34892;&#20107;&#23454;&#39564;&#35777;&#65292;&#28982;&#32780;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#25110;&#19981;&#20934;&#30830;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#21487;&#38752;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FaaF&#65288;Facts as a Function&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;LM&#30340;&#20989;&#25968;&#35843;&#29992;&#33021;&#21147;&#21644;&#38754;&#21521;RAG&#20107;&#23454;&#22238;&#24518;&#35780;&#20272;&#30340;&#26694;&#26550;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;FaaF&#26174;&#30528;&#25552;&#39640;&#20102;LM&#35782;&#21035;&#25991;&#26412;&#20013;&#19981;&#25903;&#25345;&#20107;&#23454;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#38477;&#20302;&#20102;&#25104;&#26412;&#20960;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03888v1 Announce Type: new  Abstract: Factual recall from a reference source is crucial for evaluating the performance of Retrieval Augmented Generation (RAG) systems, as it directly probes into the quality of both retrieval and generation. However, it still remains a challenge to perform this evaluation reliably and efficiently. Recent work has focused on fact verification via prompting language model (LM) evaluators, however we demonstrate that these methods are unreliable in the presence of incomplete or inaccurate information. We introduce Facts as a Function (FaaF), a new approach to fact verification that utilizes the function calling abilities of LMs and a framework for RAG factual recall evaluation. FaaF substantially improves the ability of LMs to identify unsupported facts in text with incomplete information whilst improving efficiency and lowering cost by several times, compared to prompt-based approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.03689</link><description>&lt;p&gt;
&#36890;&#29992;&#21040;&#19987;&#19994;&#30340;&#30005;&#23376;&#21830;&#21153;LLMs&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
General2Specialized LLMs Translation for E-commerce
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03689
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;G2ST&#30340;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65292;&#36890;&#36807;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#23558;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20027;&#35201;&#22788;&#29702;&#36890;&#29992;&#39046;&#22495;&#30340;&#32763;&#35793;&#65292;&#24573;&#30053;&#20102;&#20855;&#26377;&#29305;&#27530;&#20889;&#20316;&#20844;&#24335;&#30340;&#39046;&#22495;&#65292;&#27604;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#27861;&#24459;&#25991;&#20214;&#12290;&#20197;&#30005;&#23376;&#21830;&#21153;&#20026;&#20363;&#65292;&#25991;&#26412;&#36890;&#24120;&#21253;&#21547;&#22823;&#37327;&#39046;&#22495;&#30456;&#20851;&#35789;&#27719;&#65292;&#24182;&#19988;&#23384;&#22312;&#26356;&#22810;&#30340;&#35821;&#27861;&#38382;&#39064;&#65292;&#36825;&#23548;&#33268;&#24403;&#21069;NMT&#26041;&#27861;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#19982;&#39046;&#22495;&#30456;&#20851;&#30340;&#36164;&#28304;&#65292;&#21253;&#25324;&#19968;&#32452;&#26415;&#35821;&#23545;&#65288;&#23545;&#40784;&#30340;&#20013;&#33521;&#21452;&#35821;&#26415;&#35821;&#65289;&#21644;&#19968;&#20010;&#38024;&#23545;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#36827;&#34892;&#27880;&#37322;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#27493;&#24494;&#35843;&#33539;&#24335;&#65288;&#21517;&#20026;G2ST&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#23545;&#27604;&#35821;&#20041;&#22686;&#24378;&#65292;&#20197;&#23558;&#19968;&#20010;&#36890;&#29992;NMT&#27169;&#22411;&#36716;&#25442;&#20026;&#19987;&#38376;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#30340;NMT&#27169;&#22411;&#12290;&#35813;&#33539;&#24335;&#36866;&#29992;&#20110;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;NMT&#27169;&#22411;&#12290;&#23545;&#30495;&#23454;&#30005;&#23376;&#21830;&#21153;&#26631;&#39064;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#20102;&#21331;&#36234;&#30340;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03689v1 Announce Type: cross  Abstract: Existing Neural Machine Translation (NMT) models mainly handle translation in the general domain, while overlooking domains with special writing formulas, such as e-commerce and legal documents. Taking e-commerce as an example, the texts usually include amounts of domain-related words and have more grammar problems, which leads to inferior performances of current NMT methods. To address these problems, we collect two domain-related resources, including a set of term pairs (aligned Chinese-English bilingual terms) and a parallel corpus annotated for the e-commerce domain. Furthermore, we propose a two-step fine-tuning paradigm (named G2ST) with self-contrastive semantic enhancement to transfer one general NMT model to the specialized NMT model for e-commerce. The paradigm can be used for the NMT models based on Large language models (LLMs). Extensive evaluations on real e-commerce titles demonstrate the superior translation quality and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.16311</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#30340;&#20013;&#25991;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Chinese Sentence Pattern Parsing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#23558;&#28304;&#39046;&#22495;&#21477;&#27861;&#35268;&#21017;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#21477;&#24335;&#32467;&#26500;&#35299;&#26512;&#22120;&#23545;&#21508;&#31181;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#30340;&#25928;&#26524;&#20248;&#20110;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#21477;&#24335;&#32467;&#26500;&#65288;SPS&#65289;&#35299;&#26512;&#26159;&#19968;&#31181;&#20027;&#35201;&#29992;&#20110;&#35821;&#35328;&#25945;&#23398;&#30340;&#21477;&#27861;&#20998;&#26512;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;SPS&#35299;&#26512;&#22120;&#20027;&#35201;&#20381;&#36182;&#20110;&#25945;&#31185;&#20070;&#35821;&#26009;&#24211;&#36827;&#34892;&#35757;&#32451;&#65292;&#32570;&#20047;&#36328;&#39046;&#22495;&#33021;&#21147;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#25105;&#35757;&#32451;&#26694;&#26550;&#20869;&#12290;&#20174;&#28304;&#39046;&#22495;&#20013;&#25552;&#21462;&#37096;&#20998;&#21477;&#27861;&#35268;&#21017;&#65292;&#19982;&#30446;&#26631;&#39046;&#22495;&#21477;&#23376;&#32467;&#21512;&#21160;&#24577;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#22686;&#24378;&#20102;&#35299;&#26512;&#22120;&#23545;&#19981;&#21516;&#39046;&#22495;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#22312;&#25945;&#31185;&#20070;&#21644;&#26032;&#38395;&#39046;&#22495;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;F1&#25351;&#26631;&#27604;&#22522;&#20110;&#35268;&#21017;&#30340;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986;1.68&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16311v1 Announce Type: cross  Abstract: Sentence Pattern Structure (SPS) parsing is a syntactic analysis method primarily employed in language teaching.Existing SPS parsers rely heavily on textbook corpora for training, lacking cross-domain capability.To overcome this constraint, this paper proposes an innovative approach leveraging large language models (LLMs) within a self-training framework. Partial syntactic rules from a source domain are combined with target domain sentences to dynamically generate training data, enhancing the adaptability of the parser to diverse domains.Experiments conducted on textbook and news domains demonstrate the effectiveness of the proposed method, outperforming rule-based baselines by 1.68 points on F1 metrics.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Few-shot Learning&#21644;SBERT Fine-tuning&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;94.1%&#12290;</title><link>https://arxiv.org/abs/2402.15755</link><description>&lt;p&gt;
&#36890;&#36807;Few-shot Learning&#21644;SBERT Fine-tuning&#36827;&#34892;&#29273;&#31185;&#20005;&#37325;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Dental Severity Assessment through Few-shot Learning and SBERT Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15755
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Few-shot Learning&#21644;SBERT Fine-tuning&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#35813;&#26041;&#27861;&#22312;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#20005;&#37325;&#24615;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20934;&#30830;&#29575;&#39640;&#36798;94.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29273;&#31185;&#30142;&#30149;&#20005;&#37325;&#24433;&#21709;&#30528;&#30456;&#24403;&#19968;&#37096;&#20998;&#20154;&#21475;&#65292;&#23548;&#33268;&#21508;&#31181;&#20581;&#24247;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#30340;&#25972;&#20307;&#24184;&#31119;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#23558;&#33258;&#21160;&#21270;&#31995;&#32479;&#25972;&#21512;&#21040;&#21475;&#33108;&#20445;&#20581;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20026;&#35299;&#20915;&#35786;&#26029;&#22256;&#38590;&#12289;&#25928;&#29575;&#20302;&#19979;&#21644;&#21475;&#33108;&#30142;&#30149;&#35786;&#26029;&#20013;&#30340;&#38169;&#35823;&#31561;&#25361;&#25112;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24403;&#21307;&#29983;&#20204;&#38590;&#20197;&#39044;&#27979;&#25110;&#35786;&#26029;&#30142;&#30149;&#30340;&#26089;&#26399;&#38454;&#27573;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#23588;&#20854;&#26377;&#29992;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#25253;&#21578;&#26469;&#30830;&#23450;&#21475;&#33108;&#20581;&#24247;&#38382;&#39064;&#30340;&#20005;&#37325;&#31243;&#24230;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;Few-shot learning&#32467;&#21512;SBERT&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#27169;&#22411;&#65292;&#36798;&#21040;94.1%&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15755v1 Announce Type: new  Abstract: Dental diseases have a significant impact on a considerable portion of the population, leading to various health issues that can detrimentally affect individuals' overall well-being. The integration of automated systems in oral healthcare has become increasingly crucial. Machine learning approaches offer a viable solution to address challenges such as diagnostic difficulties, inefficiencies, and errors in oral disease diagnosis. These methods prove particularly useful when physicians struggle to predict or diagnose diseases at their early stages. In this study, thirteen different machine learning, deep learning, and large language models were employed to determine the severity level of oral health issues based on radiologists' reports. The results revealed that the Few-shot learning with SBERT and Multi-Layer Perceptron model outperformed all other models across various experiments, achieving an impressive accuracy of 94.1% as the best r
&lt;/p&gt;</description></item><item><title>&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.14710</link><description>&lt;p&gt;
IEPile: &#25366;&#25496;&#22823;&#35268;&#27169;&#22522;&#20110;&#27169;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
IEPile: Unearthing Large-Scale Schema-Based Information Extraction Corpus
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14710
&lt;/p&gt;
&lt;p&gt;
&#21457;&#24067;&#20102;IEPile&#65292;&#19968;&#20010;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#30340;&#32508;&#21512;&#21452;&#35821;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#25277;&#21462;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#22312;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#26159;&#25552;&#21319;LLMs&#29305;&#23450;&#33021;&#21147;&#30340;&#20851;&#38190;&#65292;&#32780;&#24403;&#21069;&#30340;IE&#25968;&#25454;&#38598;&#24448;&#24448;&#35268;&#27169;&#36739;&#23567;&#12289;&#20998;&#25955;&#19988;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27169;&#24335;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IEPile&#65292;&#19968;&#20010;&#32508;&#21512;&#30340;&#21452;&#35821;&#65288;&#33521;&#25991;&#21644;&#20013;&#25991;&#65289;IE&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#32422;0.32B&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#21644;&#28165;&#29702;33&#20010;&#29616;&#26377;IE&#25968;&#25454;&#38598;&#26500;&#24314;IEPile&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;&#25351;&#20196;&#29983;&#25104;&#26469;&#25366;&#25496;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#12290;&#22312;LLaMA&#21644;Baichuan&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;IEPile&#21487;&#20197;&#25552;&#39640;LLMs&#22312;IE&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#36164;&#28304;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24076;&#26395;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14710v1 Announce Type: cross  Abstract: Large Language Models (LLMs) demonstrate remarkable potential across various domains; however, they exhibit a significant performance gap in Information Extraction (IE). Note that high-quality instruction data is the vital key for enhancing the specific capabilities of LLMs, while current IE datasets tend to be small in scale, fragmented, and lack standardized schema. To this end, we introduce IEPile, a comprehensive bilingual (English and Chinese) IE instruction corpus, which contains approximately 0.32B tokens. We construct IEPile by collecting and cleaning 33 existing IE datasets, and introduce schema-based instruction generation to unearth a large-scale corpus. Experimental results on LLaMA and Baichuan demonstrate that using IEPile can enhance the performance of LLMs for IE, especially the zero-shot generalization. We open-source the resource and pre-trained models, hoping to provide valuable support to the NLP community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2402.14207</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#36741;&#21161;&#25776;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14207
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;STORM&#30340;&#20889;&#20316;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#21512;&#25104;&#20027;&#39064;&#27010;&#35201;&#65292;&#20197;&#36741;&#21161;&#20174;&#22836;&#24320;&#22987;&#20889;&#31867;&#20284;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#22836;&#24320;&#22987;&#25776;&#20889;&#22522;&#20110;&#20107;&#23454;&#21644;&#26377;&#26465;&#29702;&#30340;&#38271;&#31687;&#25991;&#31456;&#65292;&#20351;&#20854;&#22312;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#19982;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#21487;&#23218;&#32654;&#12290;&#36825;&#19968;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#22312;&#25776;&#20889;&#21069;&#38454;&#27573;&#25552;&#20986;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22914;&#20309;&#30740;&#31350;&#20027;&#39064;&#24182;&#20934;&#22791;&#22823;&#32434;&#20197;&#20415;&#25776;&#20889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STORM&#65292;&#19968;&#20010;&#29992;&#20110;&#36890;&#36807;&#26816;&#32034;&#21644;&#22810;&#35270;&#35282;&#25552;&#38382;&#36827;&#34892;&#20027;&#39064;&#27010;&#35201;&#21512;&#25104;&#30340;&#20889;&#20316;&#31995;&#32479;&#12290;STORM&#27169;&#25311;&#20102;&#25776;&#20889;&#21069;&#38454;&#27573;&#65292;&#20854;&#20013;&#65288;1&#65289;&#21457;&#29616;&#30740;&#31350;&#32473;&#23450;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#35266;&#28857;&#65292;&#65288;2&#65289;&#27169;&#25311;&#20250;&#35805;&#65292;&#25776;&#20889;&#25345;&#26377;&#19981;&#21516;&#35266;&#28857;&#30340;&#20316;&#32773;&#21521;&#22522;&#20110;&#21487;&#20449;&#20114;&#32852;&#32593;&#26469;&#28304;&#30340;&#20027;&#39064;&#19987;&#23478;&#25552;&#38382;&#65292;&#65288;3&#65289;&#25972;&#29702;&#25910;&#38598;&#21040;&#30340;&#20449;&#24687;&#20197;&#21019;&#24314;&#22823;&#32434;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;FreshWiki&#65292;&#19968;&#20010;&#21253;&#21547;&#26368;&#26032;&#39640;&#36136;&#37327;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21046;&#23450;&#20102;&#22823;&#32434;&#35780;&#20272;&#25351;&#26631;&#20197;&#35780;&#20272;&#25776;&#20889;&#21069;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14207v1 Announce Type: cross  Abstract: We study how to apply large language models to write grounded and organized long-form articles from scratch, with comparable breadth and depth to Wikipedia pages. This underexplored problem poses new challenges at the pre-writing stage, including how to research the topic and prepare an outline prior to writing. We propose STORM, a writing system for the Synthesis of Topic Outlines through Retrieval and Multi-perspective Question Asking. STORM models the pre-writing stage by (1) discovering diverse perspectives in researching the given topic, (2) simulating conversations where writers carrying different perspectives pose questions to a topic expert grounded on trusted Internet sources, (3) curating the collected information to create an outline.   For evaluation, we curate FreshWiki, a dataset of recent high-quality Wikipedia articles, and formulate outline assessments to evaluate the pre-writing stage. We further gather feedback from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10184</link><description>&lt;p&gt;
&#37325;&#22609;RLHF&#20013;&#30340;&#20449;&#24687;&#32467;&#26500;&#65306;&#22522;&#20110;&#22270;&#35770;&#30340;&#22870;&#21169;&#27867;&#21270;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Rethinking Information Structures in RLHF: Reward Generalization from a Graph Theory Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#65292;&#20174;&#22270;&#35770;&#30340;&#35270;&#35282;&#25552;&#20986;&#20102;RLHF&#20013;&#22870;&#21169;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#35299;&#20915;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#25104;&#26412;&#26631;&#27880;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#65288;RLHF&#65289;&#23384;&#22312;&#19968;&#20010;&#19977;&#38590;&#38382;&#39064;&#65306;&#39640;&#24230;&#22810;&#26679;&#30340;&#29615;&#22659;&#12289;&#20302;&#26631;&#27880;&#25104;&#26412;&#21644;&#21487;&#38752;&#30340;&#23545;&#40784;&#24615;&#33021;&#20043;&#38388;&#30340;&#19981;&#20860;&#23481;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35774;&#35745;&#22870;&#21169;&#24314;&#27169;&#36807;&#31243;&#20013;&#30340;&#25968;&#25454;&#38598;&#20449;&#24687;&#32467;&#26500;&#26469;&#32531;&#35299;&#36825;&#31181;&#19981;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;RLHF&#36807;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#23558;&#20854;&#25551;&#32472;&#20026;&#25991;&#26412;&#20998;&#24067;&#19978;&#30340;&#33258;&#21160;&#32534;&#30721;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#24418;&#24335;&#21270;&#20102;RLHF&#30446;&#26631;&#65292;&#21363;&#30830;&#20445;&#20154;&#31867;&#20559;&#22909;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34892;&#20026;&#20043;&#38388;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#12290;&#22522;&#20110;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;RLHF&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#20449;&#24687;&#32467;&#26500;&#30340;&#24615;&#33021;&#24433;&#21709;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#29702;&#35299;&#22870;&#21169;&#24314;&#27169;&#38454;&#27573;&#20013;&#30340;&#22870;&#21169;&#27867;&#21270;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#22270;&#35770;&#30340;&#26041;&#27861;&#26469;&#24314;&#27169;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#27867;&#21270;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10184v1 Announce Type: cross  Abstract: There is a trilemma in reinforcement learning from human feedback (RLHF): the incompatibility between highly diverse contexts, low labeling cost, and reliable alignment performance. Here we aim to mitigate such incompatibility through the design of dataset information structures during reward modeling. Specifically, we first reexamine the RLHF process and propose a theoretical framework portraying it as an autoencoding process over text distributions. Our framework formalizes the RLHF objective of ensuring distributional consistency between human preference and large language model (LLM) behavior. Building on this framework, we then systematically investigate the performance impact of information structure in the reward modeling stage of RLHF. To further understand reward generalization in the reward modeling stage, we introduce a new method based on random graph theory that models generalization in the semantic space. A key insight of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.07689</link><description>&lt;p&gt;
OrderBkd: &#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#36827;&#34892;&#30340;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
OrderBkd: Textual backdoor attack through repositioning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#23454;&#26045;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#19982;&#24050;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#30456;&#27604;&#65292;&#22312;&#25915;&#20987;&#25104;&#21151;&#29575;&#12289;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31532;&#19977;&#26041;&#25968;&#25454;&#38598;&#21644;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;NLP&#31995;&#32479;&#26500;&#25104;&#23041;&#32961;&#65292;&#21487;&#33021;&#38544;&#34255;&#21518;&#38376;&#25915;&#20987;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#26041;&#24335;&#21253;&#25324;&#25554;&#20837;&#26631;&#35760;&#25110;&#21477;&#23376;&#37325;&#36848;&#31561;&#27745;&#26579;&#25968;&#25454;&#26679;&#26412;&#65292;&#36825;&#35201;&#20040;&#25913;&#21464;&#20102;&#21407;&#22987;&#25991;&#26412;&#30340;&#35821;&#20041;&#65292;&#35201;&#20040;&#21487;&#20197;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#25105;&#20204;&#19982;&#20197;&#24448;&#24037;&#20316;&#30340;&#20027;&#35201;&#21306;&#21035;&#22312;&#20110;&#65292;&#25105;&#20204;&#20351;&#29992;&#37325;&#26032;&#23450;&#20301;&#21477;&#23376;&#20013;&#30340;&#20004;&#20010;&#21333;&#35789;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#36890;&#36807;&#35774;&#35745;&#24182;&#24212;&#29992;&#22522;&#20110;&#35789;&#24615;&#30340;&#35268;&#21017;&#26469;&#36873;&#25321;&#36825;&#20123;&#26631;&#35760;&#65292;&#25105;&#20204;&#22312;SST-2&#21644;AG&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22312;&#22256;&#24785;&#24230;&#21644;&#19982;&#24178;&#20928;&#26679;&#26412;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25915;&#20987;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#23545;ONION&#38450;&#24481;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;&#35770;&#25991;&#20013;&#30340;&#25152;&#26377;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.com/alekseevskaia/OrderBkd&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of third-party datasets and pre-trained machine learning models poses a threat to NLP systems due to possibility of hidden backdoor attacks. Existing attacks involve poisoning the data samples such as insertion of tokens or sentence paraphrasing, which either alter the semantics of the original texts or can be detected. Our main difference from the previous work is that we use the reposition of a two words in a sentence as a trigger. By designing and applying specific part-of-speech (POS) based rules for selecting these tokens, we maintain high attack success rate on SST-2 and AG classification datasets while outperforming existing attacks in terms of perplexity and semantic similarity to the clean samples. In addition, we show the robustness of our attack to the ONION defense method. All the code and data for the paper can be obtained at https://github.com/alekseevskaia/OrderBkd.
&lt;/p&gt;</description></item><item><title>&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05964</link><description>&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformer Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05964
&lt;/p&gt;
&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#26159;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#37492;&#20110;Transformer&#30340;&#29420;&#29305;&#26550;&#26500;&#65292;&#20855;&#26377;&#20132;&#26367;&#30340;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#27169;&#22359;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#12290;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#30340;&#25928;&#29575;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#35843;&#30740;&#25552;&#20379;&#20102;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05359</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#27835;&#31243;&#24207;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#38382;&#39064;&#27714;&#35299;&#36827;&#34892;&#24341;&#23548;
&lt;/p&gt;
&lt;p&gt;
Guiding Large Language Models with Divide-and-Conquer Program for Discerning Problem Solving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05359
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;LLM&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22240;&#20854;&#24191;&#27867;&#30340;&#24212;&#29992;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24403;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#22914;&#24605;&#32500;&#38142;&#65292;&#21487;&#20197;&#37322;&#25918;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22788;&#29702;&#28041;&#21450;&#37325;&#22797;&#23376;&#20219;&#21153;&#21644;/&#25110;&#20855;&#26377;&#27450;&#39575;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65288;&#22914;&#31639;&#26415;&#35745;&#31639;&#21644;&#25991;&#31456;&#32423;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#65292;&#29616;&#26377;&#30340;&#25552;&#31034;&#31574;&#30053;&#35201;&#20040;&#34920;&#29616;&#20986;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#65292;&#35201;&#20040;&#30001;&#24187;&#35273;&#24341;&#21457;&#20013;&#38388;&#38169;&#35823;&#12290;&#20026;&#20102;&#20351;LLM&#23545;&#36825;&#20123;&#20013;&#38388;&#38169;&#35823;&#26356;&#20855;&#36776;&#21035;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20998;&#27835;&#31243;&#24207;&#24341;&#23548;LLM&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#30830;&#20445;&#20248;&#36234;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#20219;&#21153;&#20998;&#35299;&#12289;&#23376;&#20219;&#21153;&#35299;&#20915;&#21644;&#35299;&#20915;&#32452;&#35013;&#36807;&#31243;&#30340;&#20998;&#31163;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#21487;&#20197;&#24341;&#23548;LLM&#25193;&#23637;&#22266;&#23450;&#28145;&#24230;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Foundation models, such as Large language Models (LLMs), have attracted significant amount of interest due to their large number of applications. Existing works show that appropriate prompt design, such as Chain-of-Thoughts, can unlock LLM's powerful capacity in diverse areas. However, when handling tasks involving repetitive sub-tasks and/or deceptive contents, such as arithmetic calculation and article-level fake news detection, existing prompting strategies either suffers from insufficient expressive power or intermediate errors triggered by hallucination. To make LLM more discerning to such intermediate errors, we propose to guide LLM with a Divide-and-Conquer program that simultaneously ensures superior expressive power and disentangles task decomposition, sub-task resolution, and resolution assembly process. Theoretic analysis reveals that our strategy can guide LLM to extend the expressive power of fixed-depth Transformer. Experiments indicate that our proposed method can achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05125</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#19982;LLMs
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Clinical Trial Patient Matching with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;LLMs&#24320;&#21457;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#20020;&#24202;&#35797;&#39564;&#24739;&#32773;&#21305;&#37197;&#31995;&#32479;&#65292;&#21487;&#20197;&#39640;&#25928;&#35780;&#20272;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20837;&#36873;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#25552;&#31034;&#31574;&#30053;&#21644;&#26816;&#32034;&#27969;&#31243;&#25552;&#39640;&#20102;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#20020;&#24202;&#35797;&#39564;&#21305;&#37197;&#26159;&#25512;&#20986;&#26032;&#33647;&#30340;&#20851;&#38190;&#38590;&#39064;&#12290;&#30446;&#21069;&#65292;&#35782;&#21035;&#31526;&#21512;&#35797;&#39564;&#20837;&#36873;&#26631;&#20934;&#30340;&#24739;&#32773;&#26159;&#39640;&#24230;&#25163;&#21160;&#30340;&#65292;&#27599;&#20301;&#24739;&#32773;&#38656;&#33457;&#36153;&#38271;&#36798;1&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#31579;&#36873;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#29702;&#35299;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23427;&#20204;&#22312;&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32473;&#23450;&#19968;&#20010;&#24739;&#32773;&#30340;&#30149;&#21490;&#20316;&#20026;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#26102;&#65292;&#35780;&#20272;&#35813;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#19968;&#32452;&#21253;&#21547;&#26631;&#20934;&#65288;&#20063;&#20197;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#25351;&#23450;&#65289;&#12290;&#25105;&#20204;&#30340;&#38646;&#26679;&#26412;&#31995;&#32479;&#22312;n2c2 2018&#38431;&#21015;&#36873;&#25321;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24471;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#19968;&#31181;&#25552;&#31034;&#31574;&#30053;&#65292;&#25913;&#21892;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#25454;&#21644;&#25104;&#26412;&#25928;&#29575;&#65292;&#35813;&#31574;&#30053;&#19982;&#29616;&#29366;&#30456;&#27604;&#21487;&#20197;&#23558;&#24739;&#32773;&#21305;&#37197;&#26102;&#38388;&#21644;&#25104;&#26412;&#38477;&#20302;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#24320;&#21457;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26816;&#32034;&#27969;&#31243;&#65292;&#20943;&#23569;&#20102;&#21305;&#37197;&#28040;&#38500;&#30340;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Matching patients to clinical trials is a key unsolved challenge in bringing new drugs to market. Today, identifying patients who meet a trial's eligibility criteria is highly manual, taking up to 1 hour per patient. Automated screening is challenging, however, as it requires understanding unstructured clinical text. Large language models (LLMs) offer a promising solution. In this work, we explore their application to trial matching. First, we design an LLM-based system which, given a patient's medical history as unstructured clinical text, evaluates whether that patient meets a set of inclusion criteria (also specified as free text). Our zero-shot system achieves state-of-the-art scores on the n2c2 2018 cohort selection benchmark. Second, we improve the data and cost efficiency of our method by identifying a prompting strategy which matches patients an order of magnitude faster and more cheaply than the status quo, and develop a two-stage retrieval pipeline that reduces the number of 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;</title><link>https://arxiv.org/abs/2402.04854</link><description>&lt;p&gt;
&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#23398;&#26415;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Tree-structured Knowledge Graph For Academic Insight Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04854
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26641;&#29366;&#30693;&#35782;&#22270;&#35889;&#21644;&#25512;&#33616;&#31995;&#32479;&#65292;&#24110;&#21161;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#36275;&#65292;&#24182;&#35299;&#20915;&#20102;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#24102;&#26469;&#30340;&#22256;&#24785;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32570;&#20047;&#30740;&#31350;&#22521;&#35757;&#30340;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#30740;&#31350;&#35843;&#26597;&#19968;&#30452;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#36825;&#20123;&#30740;&#31350;&#32773;&#22312;&#30701;&#26102;&#38388;&#20869;&#24456;&#38590;&#29702;&#35299;&#20182;&#20204;&#30740;&#31350;&#20027;&#39064;&#20869;&#30340;&#26041;&#21521;&#65292;&#20197;&#21450;&#21457;&#29616;&#26032;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#20026;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#25552;&#20379;&#30452;&#35266;&#30340;&#24110;&#21161;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#25552;&#20379;&#30456;&#20851;&#30340;&#30693;&#35782;&#22270;&#35889;(KG)&#24182;&#25512;&#33616;&#30456;&#20851;&#30340;&#23398;&#26415;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23548;&#33322;&#30693;&#35782;&#22270;&#35889;&#20027;&#35201;&#20381;&#36182;&#20110;&#30740;&#31350;&#39046;&#22495;&#30340;&#20851;&#38190;&#23383;&#65292;&#24120;&#24120;&#26080;&#27861;&#28165;&#26970;&#22320;&#21576;&#29616;&#22810;&#20010;&#30456;&#20851;&#35770;&#25991;&#20043;&#38388;&#30340;&#36923;&#36753;&#23618;&#27425;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#23398;&#26415;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#20165;&#20165;&#20381;&#36182;&#20110;&#39640;&#25991;&#26412;&#30456;&#20284;&#24615;&#65292;&#36825;&#21487;&#33021;&#20250;&#35753;&#30740;&#31350;&#20154;&#21592;&#22256;&#24785;&#20026;&#20160;&#20040;&#25512;&#33616;&#20102;&#29305;&#23450;&#30340;&#25991;&#31456;&#12290;&#20182;&#20204;&#21487;&#33021;&#32570;&#20047;&#20102;&#35299;&#20851;&#20110;&#20182;&#20204;&#24076;&#26395;&#33719;&#24471;&#30340;"&#38382;&#39064;&#35299;&#20915;"&#21644;"&#38382;&#39064;&#21457;&#29616;"&#20043;&#38388;&#30340;&#35265;&#35299;&#36830;&#25509;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25903;&#25345;&#21021;&#23398;&#32773;&#30740;&#31350;&#32773;&#36827;&#34892;&#30740;&#31350;&#35843;&#30740;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research surveys have always posed a challenge for beginner researchers who lack of research training. These researchers struggle to understand the directions within their research topic, and the discovery of new research findings within a short time. One way to provide intuitive assistance to beginner researchers is by offering relevant knowledge graphs(KG) and recommending related academic papers. However, existing navigation knowledge graphs primarily rely on keywords in the research field and often fail to present the logical hierarchy among multiple related papers clearly. Moreover, most recommendation systems for academic papers simply rely on high text similarity, which can leave researchers confused as to why a particular article is being recommended. They may lack of grasp important information about the insight connection between "Issue resolved" and "Issue finding" that they hope to obtain. To address these issues, this study aims to support research insight surveys for begi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;VlogQA&#65306;&#36234;&#21335;&#21475;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;VlogQA&#26159;&#19968;&#20010;&#22522;&#20110;&#26469;&#33258;YouTube&#30340;&#21095;&#26412;&#25991;&#26723;&#30340;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#20027;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#21462;&#24471;&#20102;75.34%&#30340;&#26368;&#39640;F1&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.02655</link><description>&lt;p&gt;
VlogQA: &#36234;&#21335;&#21475;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VlogQA: Task, Dataset, and Baseline Models for Vietnamese Spoken-Based Machine Reading Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VlogQA&#65306;&#36234;&#21335;&#21475;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#12289;&#25968;&#25454;&#38598;&#21644;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#20219;&#21153;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;VlogQA&#26159;&#19968;&#20010;&#22522;&#20110;&#26469;&#33258;YouTube&#30340;&#21095;&#26412;&#25991;&#26723;&#30340;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#20027;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#21462;&#24471;&#20102;75.34%&#30340;&#26368;&#39640;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#36234;&#21335;&#21475;&#35821;&#35821;&#26009;&#24211;&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#30340;&#35265;&#35299;&#12290;&#29616;&#26377;&#30340;&#36234;&#21335;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#35821;&#26009;&#24211;&#20027;&#35201;&#20851;&#27880;&#27491;&#24335;&#30340;&#20070;&#38754;&#25991;&#26723;&#65292;&#22914;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#12289;&#22312;&#32447;&#25253;&#32440;&#25110;&#25945;&#31185;&#20070;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;VlogQA&#21253;&#21547;&#20102;10,076&#20010;&#38382;&#31572;&#23545;&#65292;&#22522;&#20110;&#20174;YouTube&#33719;&#21462;&#30340;1,230&#20221;&#21095;&#26412;&#25991;&#26723;&#65292;YouTube&#26159;&#19968;&#20010;&#21253;&#21547;&#20102;&#29992;&#25143;&#19978;&#20256;&#20869;&#23481;&#30340;&#24191;&#27867;&#36164;&#28304;&#65292;&#28085;&#30422;&#20102;&#39135;&#29289;&#21644;&#26053;&#34892;&#31561;&#20027;&#39064;&#12290;&#36890;&#36807;&#25429;&#25417;&#36234;&#21335;&#26412;&#22303;&#20154;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#21475;&#35821;&#34920;&#36798;&#65292;&#36825;&#26159;&#36234;&#21335;&#30740;&#31350;&#20013;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#35282;&#33853;&#65292;&#35813;&#35821;&#26009;&#24211;&#20026;&#26410;&#26469;&#36234;&#21335;&#35821;&#38405;&#35835;&#29702;&#35299;&#20219;&#21153;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;F1&#20998;&#25968;&#20026;75.34%&#65292;&#34920;&#26126;&#20102;&#20854;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the development process of a Vietnamese spoken language corpus for machine reading comprehension (MRC) tasks and provides insights into the challenges and opportunities associated with using real-world data for machine reading comprehension tasks. The existing MRC corpora in Vietnamese mainly focus on formal written documents such as Wikipedia articles, online newspapers, or textbooks. In contrast, the VlogQA consists of 10,076 question-answer pairs based on 1,230 transcript documents sourced from YouTube -- an extensive source of user-uploaded content, covering the topics of food and travel. By capturing the spoken language of native Vietnamese speakers in natural settings, an obscure corner overlooked in Vietnamese research, the corpus provides a valuable resource for future research in reading comprehension tasks for the Vietnamese language. Regarding performance evaluation, our deep-learning models achieved the highest F1 score of 75.34% on the test set, indicat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2401.17390</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Customizing Language Model Responses with Contrastive In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#23450;&#21046;&#35821;&#35328;&#27169;&#22411;&#22238;&#22797;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#21644;&#36127;&#38754;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#23398;&#20250;&#22914;&#20309;&#22238;&#36991;&#36127;&#38754;&#29305;&#24449;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23558;LLMs&#19982;&#25105;&#20204;&#30340;&#24847;&#22270;&#23545;&#40784;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#24403;&#25105;&#20204;&#24076;&#26395;&#29983;&#25104;&#20248;&#20110;&#20854;&#20182;&#20869;&#23481;&#30340;&#20869;&#23481;&#65292;&#25110;&#32773;&#24403;&#25105;&#20204;&#24076;&#26395;LLMs&#20197;&#19968;&#31181;&#38590;&#20197;&#25551;&#36848;&#30340;&#39118;&#26684;&#25110;&#35821;&#27668;&#36827;&#34892;&#22238;&#24212;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#31034;&#20363;&#26469;&#26356;&#22909;&#22320;&#25551;&#36848;&#25105;&#20204;&#30340;&#24847;&#22270;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#25552;&#20379;&#27491;&#38754;&#31034;&#20363;&#26469;&#35828;&#26126;&#30495;&#23454;&#30340;&#24847;&#22270;&#65292;&#20197;&#21450;&#36127;&#38754;&#31034;&#20363;&#26469;&#23637;&#31034;&#25105;&#20204;&#24076;&#26395;LLMs&#36991;&#20813;&#30340;&#29305;&#24449;&#12290;&#36127;&#38754;&#31034;&#20363;&#21487;&#20197;&#20174;&#26631;&#35760;&#25968;&#25454;&#20013;&#26816;&#32034;&#65292;&#30001;&#20154;&#24037;&#32534;&#20889;&#65292;&#25110;&#30001;LLMs&#33258;&#21160;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#20043;&#21069;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#20998;&#26512;&#36825;&#20123;&#31034;&#20363;&#65292;&#20197;&#25945;&#20250;&#33258;&#24049;&#36991;&#20813;&#20160;&#20040;&#12290;&#36825;&#20010;&#25512;&#29702;&#27493;&#39588;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19982;&#29992;&#25143;&#38656;&#27714;&#30456;&#20851;&#30340;&#36866;&#24403;&#34920;&#36798;&#65292;&#24182;&#24341;&#23548;&#20854;&#29983;&#25104;&#26356;&#22909;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are becoming increasingly important for machine learning applications. However, it can be challenging to align LLMs with our intent, particularly when we want to generate content that is preferable over others or when we want the LLM to respond in a certain style or tone that is hard to describe. To address this challenge, we propose an approach that uses contrastive examples to better describe our intent. This involves providing positive examples that illustrate the true intent, along with negative examples that show what characteristics we want LLMs to avoid. The negative examples can be retrieved from labeled data, written by a human, or generated by the LLM itself. Before generating an answer, we ask the model to analyze the examples to teach itself what to avoid. This reasoning step provides the model with the appropriate articulation of the user's need and guides it towards generting a better answer. We tested our approach on both synthesized and real
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLsM&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#38544;&#20889;&#26415;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35805;&#35821;&#29305;&#24449;&#30340;&#38544;&#20889;&#25991;&#26412;&#65292;&#25552;&#39640;&#20102;&#38544;&#34109;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15656</link><description>&lt;p&gt;
LLsM: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
LLsM: Generative Linguistic Steganography with Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLsM&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#38544;&#20889;&#26415;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35805;&#35821;&#29305;&#24449;&#30340;&#38544;&#20889;&#25991;&#26412;&#65292;&#25552;&#39640;&#20102;&#38544;&#34109;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38544;&#20889;&#26415;&#65288;LS&#65289;&#26088;&#22312;&#26681;&#25454;&#31192;&#23494;&#20449;&#24687;&#29983;&#25104;&#38544;&#20889;&#25991;&#26412;&#65288;stego&#65289;&#12290;&#21482;&#26377;&#25480;&#26435;&#25509;&#25910;&#32773;&#25165;&#33021;&#23519;&#35273;&#25991;&#26412;&#20013;&#31192;&#23494;&#30340;&#23384;&#22312;&#24182;&#25552;&#21462;&#20986;&#26469;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#26696;&#29983;&#25104;&#30340;&#38544;&#20889;&#25991;&#26412;&#21487;&#25511;&#24615;&#36739;&#24046;&#65292;&#24456;&#38590;&#21253;&#21547;&#29305;&#23450;&#30340;&#35805;&#35821;&#29305;&#24449;&#65292;&#22914;&#39118;&#26684;&#12290;&#32467;&#26524;&#65292;&#38544;&#20889;&#25991;&#26412;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65292;&#21361;&#21450;&#38544;&#34109;&#36890;&#20449;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LLsM&#65292;&#31532;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;LS&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;&#20016;&#23500;&#35805;&#35821;&#29305;&#24449;&#30340;&#22823;&#35268;&#27169;&#26500;&#24314;&#25968;&#25454;&#38598;&#23545;LLaMA2&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;&#24494;&#35843;&#21518;&#30340;LLM&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35805;&#35821;&#29305;&#24449;&#30340;&#25991;&#26412;&#12290;&#28982;&#21518;&#23558;&#35805;&#35821;&#20316;&#20026;&#24341;&#23548;&#20449;&#24687;&#21644;&#31192;&#23494;&#19968;&#36215;&#36755;&#20837;&#32473;&#24494;&#35843;&#21518;&#30340;LLM&#65292;&#24418;&#24335;&#20026;&#8220;Prompt&#8221;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26500;&#24314;&#30340;&#20505;&#36873;&#27744;&#23558;&#36827;&#34892;&#33539;&#22260;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.09702</link><description>&lt;p&gt;
&#25512;&#29702;&#38142;&#19978;&#30340;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#65306;&#27169;&#22411;&#22312;&#27809;&#26377;&#24187;&#35273;&#30340;&#24773;&#20917;&#19979;&#33021;&#36208;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30001;&#35821;&#20041;&#20851;&#32852;&#24341;&#36215;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#25552;&#31034;&#20013;&#26159;&#21542;&#20250;&#22240;&#20026;&#26576;&#20123;&#20851;&#38190;&#23383;/&#23454;&#20307;&#20559;&#35265;&#32780;&#37319;&#21462;&#25463;&#24452;&#65292;&#32780;&#19981;&#26159;&#36981;&#24490;&#27491;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EureQA&#30340;&#26032;&#22411;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20174;LLMs&#20250;&#20197;&#32477;&#23545;&#30830;&#23450;&#24615;&#27491;&#30830;&#22238;&#31572;&#30340;&#38382;&#39064;&#24320;&#22987;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#29992;&#35777;&#25454;&#21477;&#23376;&#36974;&#34109;&#37325;&#35201;&#23454;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#25214;&#21040;&#26681;&#25454;&#35777;&#25454;&#38142;&#26465;&#36974;&#34109;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09702v2 Announce Type: replace-cross  Abstract: Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#21644;&#32508;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23567;&#35821;&#35328;&#27169;&#22411;&#32487;&#25215;&#19981;&#23436;&#21892;&#25512;&#29702;&#21644;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.09214</link><description>&lt;p&gt;
Mind's Mirror: &#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#21644;&#32508;&#21512;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Mind's Mirror: Distilling Self-Evaluation Capability and Comprehensive Thinking from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#21644;&#32508;&#21512;&#24605;&#32500;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#23567;&#35821;&#35328;&#27169;&#22411;&#32487;&#25215;&#19981;&#23436;&#21892;&#25512;&#29702;&#21644;&#24187;&#35273;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#21644;&#35745;&#31639;&#38656;&#27714;&#22312;&#32771;&#34385;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#26102;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#26041;&#27861;&#35770;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;LLMs&#20013;&#30340;&#33258;&#25105;&#35780;&#20272;&#33021;&#21147;&#25552;&#28860;&#21040;SLMs&#20013;&#65292;&#26088;&#22312;&#20943;&#36731;&#20174;LLMs&#32487;&#25215;&#30340;&#38169;&#35823;&#25512;&#29702;&#21644;&#24187;&#35273;&#30340;&#19981;&#33391;&#24433;&#21709;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20513;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;CoTs&#21644;&#33258;&#25105;&#35780;&#20272;&#36755;&#20986;&#26469;&#25552;&#28860;&#26356;&#20840;&#38754;&#30340;&#24605;&#32500;&#65292;&#20197;&#30830;&#20445;&#26356;&#20026;&#24443;&#24213;&#21644;&#20581;&#22766;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09214v2 Announce Type: replace  Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language processing. However, the massive scale and computational demands of these models present formidable challenges when considering their practical deployment in resource-constrained environments. While techniques such as chain-of-thought (CoT) distillation have displayed promise in distilling LLMs into small language models (SLMs), there is a risk that distilled SLMs may still inherit flawed reasoning and hallucinations from LLMs. To address these issues, we propose a twofold methodology: First, we introduce a novel method for distilling the self-evaluation capability from LLMs into SLMs, aiming to mitigate the adverse effects of flawed reasoning and hallucinations inherited from LLMs. Second, we advocate for distilling more comprehensive thinking by incorporating multiple distinct CoTs and self-evaluation outputs, to ensure a more thorough and robust
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;ReNeLLM&#65292;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#26102;&#38388;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2311.08268</link><description>&lt;p&gt;
&#20266;&#35013;&#25104;&#32650;&#30340;&#29436;&#65306;&#26222;&#36941;&#30340;&#23884;&#22871;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#36731;&#26494;&#24858;&#24324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#26377;&#25928;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;ReNeLLM&#65292;&#26174;&#33879;&#25552;&#39640;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#26088;&#22312;&#25552;&#20379;&#26377;&#29992;&#21644;&#23433;&#20840;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#34987;&#31216;&#20026;&#8220;&#36234;&#29425;&#8221;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#21487;&#20197;&#35268;&#36991;&#20445;&#38556;&#25514;&#26045;&#65292;&#23548;&#33268;LLMs&#29983;&#25104;&#28508;&#22312;&#26377;&#23475;&#20869;&#23481;&#12290;&#25506;&#32034;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#26356;&#22909;&#22320;&#25581;&#31034;LLMs&#30340;&#24369;&#28857;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#23548;&#25105;&#20204;&#23433;&#20840;&#22320;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#26041;&#27861;&#35201;&#20040;&#36973;&#21463;&#22797;&#26434;&#30340;&#25163;&#24037;&#35774;&#35745;&#65292;&#35201;&#20040;&#38656;&#35201;&#22312;&#20854;&#20182;&#30333;&#30418;&#27169;&#22411;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#27867;&#21270;&#24615;&#25110;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36234;&#29425;&#25552;&#31034;&#25915;&#20987;&#27010;&#25324;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#25552;&#31034;&#37325;&#20889;&#21644;&#65288;2&#65289;&#22330;&#26223;&#23884;&#22871;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReNeLLM&#65292;&#19968;&#20010;&#21033;&#29992;LLMs&#33258;&#36523;&#29983;&#25104;&#26377;&#25928;&#36234;&#29425;&#25552;&#31034;&#30340;&#33258;&#21160;&#26694;&#26550;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#22522;&#32447;&#30456;&#27604;&#65292;ReNeLLM&#26174;&#33879;&#25552;&#39640;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#22823;&#22823;&#20943;&#23569;&#20102;&#26102;&#38388;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08268v2 Announce Type: replace  Abstract: Large Language Models (LLMs), such as ChatGPT and GPT-4, are designed to provide useful and safe responses. However, adversarial prompts known as 'jailbreaks' can circumvent safeguards, leading LLMs to generate potentially harmful content. Exploring jailbreak prompts can help to better reveal the weaknesses of LLMs and further steer us to secure them. Unfortunately, existing jailbreak methods either suffer from intricate manual design or require optimization on other white-box models, compromising generalization or efficiency. In this paper, we generalize jailbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting. Based on this, we propose ReNeLLM, an automatic framework that leverages LLMs themselves to generate effective jailbreak prompts. Extensive experiments demonstrate that ReNeLLM significantly improves the attack success rate while greatly reducing the time cost compared to existing baselines. Ou
&lt;/p&gt;</description></item><item><title>&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.07092</link><description>&lt;p&gt;
&#25581;&#31034;&#30495;&#30456;&#65306;&#27450;&#39575;&#35821;&#35328;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
To Tell The Truth: Language of Deception and Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07092
&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#20998;&#26512;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#20154;&#31867;&#20027;&#20307;&#20855;&#26377;&#31867;&#20284;&#30340;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-cross &#25688;&#35201;&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#38169;&#35823;&#20449;&#24687;&#28183;&#36879;&#21040;&#22312;&#32447;&#35752;&#35770;&#20013;&#65292;&#28982;&#32780;&#20154;&#20204;&#33021;&#22815;&#20174;&#36825;&#31181;&#27450;&#39575;&#24615;&#25991;&#26412;&#20869;&#23481;&#20013;&#36776;&#21035;&#30495;&#30456;&#30340;&#35777;&#25454;&#21364;&#24456;&#23569;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#26723;&#26032;&#39062;&#30340;&#30005;&#35270;&#28216;&#25103;&#33410;&#30446;&#25968;&#25454;&#65292;&#20854;&#20013;&#39640;&#39118;&#38505;&#29615;&#22659;&#20013;&#30456;&#20114;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#30446;&#26631;&#30340;&#20010;&#20307;&#20043;&#38388;&#30340;&#23545;&#35805;&#23548;&#33268;&#35854;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#27450;&#39575;&#35821;&#35328;&#28508;&#22312;&#21487;&#39564;&#35777;&#35821;&#35328;&#32447;&#32034;&#22312;&#23458;&#35266;&#30495;&#30456;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#65292;&#36825;&#26159;&#20197;&#24448;&#22522;&#20110;&#25991;&#26412;&#30340;&#27450;&#39575;&#25968;&#25454;&#38598;&#20013;&#32570;&#23569;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#24449;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23384;&#22312;&#19968;&#31867;&#25506;&#27979;&#22120;&#65288;&#31639;&#27861;&#65289;&#65292;&#20854;&#30495;&#30456;&#26816;&#27979;&#24615;&#33021;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#20284;&#65292;&#21363;&#20351;&#21069;&#32773;&#21482;&#20351;&#29992;&#35821;&#35328;&#32447;&#32034;&#65292;&#32780;&#21518;&#32773;&#21017;&#36890;&#36807;&#23436;&#20840;&#35775;&#38382;&#25152;&#26377;&#28508;&#22312;&#32447;&#32034;&#28304;&#65288;&#35821;&#35328;&#21644;&#35270;&#21548;&#65289;&#36827;&#34892;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#19978;&#65292;&#37319;&#29992;&#29942;&#39048;&#26694;&#26550;&#26469;&#23398;&#20064;&#21487;&#36776;&#21035;&#30340;&#32447;&#32034;&#65292;&#20197;&#30830;&#23450;&#30495;&#30456;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07092v2 Announce Type: replace-cross  Abstract: Text-based misinformation permeates online discourses, yet evidence of people's ability to discern truth from such deceptive textual content is scarce. We analyze a novel TV game show data where conversations in a high-stake environment between individuals with conflicting objectives result in lies. We investigate the manifestation of potentially verifiable language cues of deception in the presence of objective truth, a distinguishing feature absent in previous text-based deception datasets. We show that there exists a class of detectors (algorithms) that have similar truth detection performance compared to human subjects, even when the former accesses only the language cues while the latter engages in conversations with complete access to all potential sources of cues (language and audio-visual). Our model, built on a large language model, employs a bottleneck framework to learn discernible cues to determine truth, an act of 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2310.10648</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#27169;&#22411;&#24357;&#34917;&#26032;&#25163;&#19982;&#19987;&#23478;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#20197;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#36741;&#23548;&#35268;&#27169;&#21270;&#20173;&#28982;&#26159;&#25945;&#32946;&#20013;&#30340;&#19968;&#39033;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#38656;&#27714;&#22686;&#38271;&#65292;&#35768;&#22810;&#24179;&#21488;&#32856;&#29992;&#26032;&#25163;&#23548;&#24072;&#65292;&#20182;&#20204;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#25945;&#32946;&#24037;&#20316;&#32773;&#19981;&#21516;&#65292;&#38590;&#20197;&#35299;&#20915;&#23398;&#29983;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#26080;&#27861;&#25235;&#20303;&#20027;&#35201;&#30340;&#23398;&#20064;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#20043;&#38388;&#30693;&#35782;&#24046;&#36317;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Bridge&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#23558;&#19987;&#23478;&#30340;&#28508;&#22312;&#24605;&#32500;&#36807;&#31243;&#36716;&#21270;&#20026;&#32416;&#27491;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#19987;&#23478;&#35782;&#21035;(A)&#23398;&#29983;&#30340;&#38169;&#35823;&#12289;(B)&#32416;&#27491;&#31574;&#30053;&#21644;(C)&#29983;&#25104;&#22238;&#24212;&#20043;&#21069;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;700&#20010;&#30495;&#23454;&#36741;&#23548;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26631;&#27880;&#20102;&#20182;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#19987;&#23478;&#30340;&#20915;&#31574;&#27169;&#22411;&#23545;LLMs&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65306;&#22238;&#24212;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10648v2 Announce Type: replace-cross  Abstract: Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01605</link><description>&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#39044;&#27979;&#30340;&#24544;&#23454;&#21644;&#31283;&#20581;&#30340;&#26412;&#22320;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Faithful and Robust Local Interpretability for Textual Predictions. (arXiv:2311.01605v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01605
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FRED&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24182;&#19988;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#20013;&#24471;&#21040;&#20449;&#20219;&#21644;&#37096;&#32626;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#27169;&#22411;&#30340;&#26041;&#27861;&#36890;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#32570;&#20047;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20063;&#19981;&#33021;&#20445;&#35777;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;FRED&#65288;Faithful and Robust Explainer for textual Documents&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#25991;&#26412;&#39044;&#27979;&#12290;FRED&#21487;&#20197;&#35782;&#21035;&#25991;&#26723;&#20013;&#30340;&#20851;&#38190;&#35789;&#65292;&#24403;&#36825;&#20123;&#35789;&#34987;&#31227;&#38500;&#26102;&#23545;&#39044;&#27979;&#32467;&#26524;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#27491;&#24335;&#30340;&#23450;&#20041;&#21644;&#23545;&#21487;&#35299;&#37322;&#20998;&#31867;&#22120;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#30830;&#31435;&#20102;FRED&#30340;&#21487;&#38752;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;FRED&#22312;&#25552;&#20379;&#23545;&#25991;&#26412;&#27169;&#22411;&#30340;&#28145;&#20837;&#35265;&#35299;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpretability is essential for machine learning models to be trusted and deployed in critical domains. However, existing methods for interpreting text models are often complex, lack solid mathematical foundations, and their performance is not guaranteed. In this paper, we propose FRED (Faithful and Robust Explainer for textual Documents), a novel method for interpreting predictions over text. FRED identifies key words in a document that significantly impact the prediction when removed. We establish the reliability of FRED through formal definitions and theoretical analyses on interpretable classifiers. Additionally, our empirical evaluation against state-of-the-art methods demonstrates the effectiveness of FRED in providing insights into text models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SALMONN&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;/&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#30452;&#25509;&#22788;&#29702;&#21644;&#29702;&#35299;&#26222;&#36890;&#38899;&#39057;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.13289</link><description>&lt;p&gt;
SALMONN&#65306;&#36808;&#21521;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36890;&#29992;&#21548;&#35273;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SALMONN: Towards Generic Hearing Abilities for Large Language Models. (arXiv:2310.13289v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SALMONN&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#20102;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;/&#38899;&#39057;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#30452;&#25509;&#22788;&#29702;&#21644;&#29702;&#35299;&#26222;&#36890;&#38899;&#39057;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#35821;&#38899;&#21644;&#38899;&#39057;&#20219;&#21153;&#19978;&#21462;&#24471;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#21487;&#20197;&#35828;&#26159;&#29289;&#29702;&#19990;&#30028;&#20013;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#30340;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#33021;&#21147;&#65292;&#23427;&#28041;&#21450;&#21040;&#23545;&#33267;&#23569;&#19977;&#31181;&#22768;&#38899;&#31867;&#22411;&#65288;&#35821;&#38899;&#12289;&#38899;&#39057;&#20107;&#20214;&#21644;&#38899;&#20048;&#65289;&#30340;&#26222;&#36890;&#21548;&#35273;&#20449;&#24687;&#30340;&#24863;&#30693;&#21644;&#29702;&#35299;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SALMONN&#65292;&#19968;&#31181;&#35821;&#38899;&#38899;&#39057;&#35821;&#35328;&#38899;&#20048;&#24320;&#25918;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#35821;&#38899;&#21644;&#38899;&#39057;&#32534;&#30721;&#22120;&#38598;&#25104;&#21040;&#21333;&#19968;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#36827;&#34892;&#26500;&#24314;&#12290;SALMONN&#20351;&#24471;LLM&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#21644;&#29702;&#35299;&#26222;&#36890;&#38899;&#39057;&#36755;&#20837;&#65292;&#22312;&#35757;&#32451;&#20013;&#22312;&#35768;&#22810;&#35821;&#38899;&#21644;&#38899;&#39057;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#22914;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#12289;&#22522;&#20110;&#21548;&#35273;&#20449;&#24687;&#30340;&#38382;&#39064;&#22238;&#31572;&#12289;&#24773;&#24863;&#35782;&#21035;&#12289;&#35828;&#35805;&#20154;&#39564;&#35777;&#20197;&#21450;&#38899;&#20048;&#21644;&#38899;&#39057;&#23383;&#24149;&#31561;&#12290;SALMONN&#36824;&#20855;&#26377;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#30340;&#21508;&#31181;&#26032;&#33021;&#21147;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#23545;&#26410;&#35757;&#32451;&#35821;&#35328;&#30340;&#35821;&#38899;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hearing is arguably an essential ability of artificial intelligence (AI) agents in the physical world, which refers to the perception and understanding of general auditory information consisting of at least three types of sounds: speech, audio events, and music. In this paper, we propose SALMONN, a speech audio language music open neural network, built by integrating a pre-trained text-based large language model (LLM) with speech and audio encoders into a single multimodal model. SALMONN enables the LLM to directly process and understand general audio inputs and achieve competitive performances on a number of speech and audio tasks used in training, such as automatic speech recognition and translation, auditory-information-based question answering, emotion recognition, speaker verification, and music and audio captioning \textit{etc.} SALMONN also has a diverse set of emergent abilities unseen in the training, which includes but is not limited to speech translation to untrained languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>REMARK-LLM&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#30340;&#40065;&#26834;&#39640;&#25928;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;-based&#28040;&#24687;&#32534;&#30721;&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#35299;&#30721;&#27169;&#22359;&#20197;&#21450;&#20248;&#21270;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#26469;&#20445;&#25252;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#24694;&#24847;&#21033;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.12362</link><description>&lt;p&gt;
REMARK-LLM:&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#39640;&#25928;&#30340;&#27700;&#21360;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models. (arXiv:2310.12362v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12362
&lt;/p&gt;
&lt;p&gt;
REMARK-LLM&#26159;&#19968;&#31181;&#38024;&#23545;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#30340;&#40065;&#26834;&#39640;&#25928;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;-based&#28040;&#24687;&#32534;&#30721;&#12289;&#37325;&#26032;&#21442;&#25968;&#21270;&#21644;&#35299;&#30721;&#27169;&#22359;&#20197;&#21450;&#20248;&#21270;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#26469;&#20445;&#25252;&#29983;&#25104;&#20869;&#23481;&#30340;&#23436;&#25972;&#24615;&#21644;&#38450;&#27490;&#24694;&#24847;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REMARK-LLM&#30340;&#26032;&#22411;&#39640;&#25928;&#12289;&#24378;&#40065;&#26834;&#24615;&#30340;&#27700;&#21360;&#26694;&#26550;&#65292;&#19987;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#35774;&#35745;&#12290;&#20351;&#29992;LLMs&#21512;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#24191;&#27867;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#37325;&#35201;&#30340;&#30693;&#35782;&#20135;&#26435;&#65288;IP&#65289;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#20869;&#23481;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#21033;&#29992;&#65292;&#21253;&#25324;&#22403;&#22334;&#37038;&#20214;&#21644;&#25220;&#34989;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;REMARK-LLM&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;i&#65289;&#22522;&#20110;&#23398;&#20064;&#30340;&#28040;&#24687;&#32534;&#30721;&#27169;&#22359;&#65292;&#23558;&#20108;&#36827;&#21046;&#31614;&#21517;&#27880;&#20837;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65307;&#65288;ii&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#27169;&#22359;&#65292;&#23558;&#28040;&#24687;&#32534;&#30721;&#30340;&#23494;&#38598;&#20998;&#24067;&#36716;&#25442;&#20026;&#27700;&#21360;&#25991;&#26412;&#26631;&#35760;&#30340;&#31232;&#30095;&#20998;&#24067;&#65307;&#65288;iii&#65289;&#19987;&#38376;&#29992;&#20110;&#31614;&#21517;&#25552;&#21462;&#30340;&#35299;&#30721;&#27169;&#22359;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20248;&#21270;&#30340;&#27874;&#26463;&#25628;&#32034;&#31639;&#27861;&#65292;&#20197;&#20445;&#35777;&#29983;&#25104;&#20869;&#23481;&#30340;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;REMARK-LLM&#32463;&#36807;&#20005;&#26684;&#30340;&#35757;&#32451;&#65292;&#20197;&#40723;&#21169;&#35821;&#20041;&#23436;&#25972;&#24615;&#30340;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity
&lt;/p&gt;</description></item><item><title>QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.08041</link><description>&lt;p&gt;
QLLM: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#39640;&#25928;&#20302;&#20301;&#23485;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08041
&lt;/p&gt;
&lt;p&gt;
QLLM&#26159;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20854;&#25152;&#38656;&#36164;&#28304;&#36807;&#22823;&#65292;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#24212;&#29992;&#12290;&#34429;&#28982;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;Quantization-Aware Training&#65292;QAT&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#30340;&#35757;&#32451;&#25104;&#26412;&#36807;&#39640;&#65292;&#22240;&#27492;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;Post-Training Quantization&#65292;PTQ&#65289;&#25104;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26356;&#23454;&#38469;&#30340;&#26041;&#27861;&#12290;&#22312;&#29616;&#26377;&#30740;&#31350;&#20013;&#65292;&#29305;&#23450;&#36890;&#36947;&#20013;&#30340;&#28608;&#27963;&#31163;&#32676;&#20540;&#34987;&#35748;&#20026;&#26159;&#23548;&#33268;&#21518;&#35757;&#32451;&#37327;&#21270;&#20934;&#30830;&#24615;&#19979;&#38477;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QLLM&#65292;&#19968;&#31181;&#20026;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;&#30340;&#20934;&#30830;&#39640;&#25928;&#30340;&#20302;&#20301;&#23485;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;QLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#36890;&#36947;&#37325;&#32452;&#25216;&#26415;&#65292;&#23558;&#31163;&#32676;&#20540;&#30340;&#22823;&#23567;&#37325;&#26032;&#20998;&#37197;&#32473;&#20854;&#20182;&#36890;&#36947;&#65292;&#20174;&#32780;&#20943;&#36731;&#23427;&#20204;&#23545;&#37327;&#21270;&#33539;&#22260;&#30340;&#24433;&#21709;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36890;&#36807;&#36890;&#36947;&#25286;&#20998;&#21644;&#36890;&#36947;&#32452;&#35013;&#65292;&#22312;&#20445;&#35777;&#20302;&#20301;&#23485;&#30340;&#24773;&#20917;&#19979;&#23558;&#31163;&#32676;&#36890;&#36947;&#20998;&#35299;&#25104;&#22810;&#20010;&#23376;&#36890;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in NLP, but their demands hinder their widespread deployment. While Quantization-Aware Training (QAT) offers a solution, its extensive training costs make Post-Training Quantization (PTQ) a more practical approach for LLMs. In existing studies, activation outliers in particular channels are identified as the bottleneck to PTQ accuracy. They propose to transform the magnitudes from activations to weights, which however offers limited alleviation or suffers from unstable gradients, resulting in a severe performance drop at low-bitwidth. In this paper, we propose QLLM, an accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM introduces an adaptive channel reassembly technique that reallocates the magnitude of outliers to other channels, thereby mitigating their impact on the quantization range. This is achieved by channel disassembly and channel assembly, which first breaks down the outlier channels into several sub-channels to ensure a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.00322</link><description>&lt;p&gt;
&#32418;&#38431;&#28216;&#25103;&#65306;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Game: A Game-Theoretic Framework for Red Teaming Language Models. (arXiv:2310.00322v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;&#21338;&#24328;&#35770;&#20998;&#26512;&#20102;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#26469;&#25552;&#20379;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#37096;&#32626;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24517;&#39035;&#31526;&#21512;&#26377;&#30410;&#21644;&#26080;&#23475;&#24615;&#30340;&#26631;&#20934;&#65292;&#20174;&#32780;&#23454;&#29616;LLM&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#30340;&#19968;&#33268;&#24615;&#12290;&#32418;&#38431;&#25216;&#26415;&#26159;&#23454;&#29616;&#36825;&#19968;&#26631;&#20934;&#30340;&#20851;&#38190;&#36884;&#24452;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#20381;&#36182;&#20110;&#25163;&#21160;&#32418;&#38431;&#35774;&#35745;&#21644;&#21551;&#21457;&#24335;&#23545;&#25239;&#25552;&#31034;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;&#21644;&#20248;&#21270;&#12290;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#25968;&#23398;&#24418;&#24335;&#21270;&#65292;&#38480;&#21046;&#20102;&#22312;&#21487;&#37327;&#21270;&#24230;&#37327;&#21644;&#25910;&#25947;&#20445;&#35777;&#19979;&#23545;LLM&#36827;&#34892;&#22810;&#26679;&#25915;&#20987;&#31574;&#30053;&#30340;&#25506;&#32034;&#21644;&#20248;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32418;&#38431;&#28216;&#25103;&#65288;RTG&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#21338;&#24328;&#35770;&#26694;&#26550;&#12290;RTG&#26088;&#22312;&#20998;&#26512;&#32418;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;RLM&#65289;&#19982;&#34013;&#38431;&#35821;&#35328;&#27169;&#22411;&#65288;BLM&#65289;&#20043;&#38388;&#30340;&#22810;&#36718;&#25915;&#38450;&#20114;&#21160;&#12290;&#22312;RTG&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#35821;&#20041;&#31354;&#38388;&#22810;&#26679;&#24615;&#24230;&#37327;&#30340;&#28216;&#25103;&#21270;&#32418;&#38431;&#27714;&#35299;&#22120;&#65288;GRTS&#65289;&#12290;GRTS&#26159;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#32418;&#38431;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#32418;&#38431;&#28216;&#25103;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deployable Large Language Models (LLMs) must conform to the criterion of helpfulness and harmlessness, thereby achieving consistency between LLMs outputs and human values. Red-teaming techniques constitute a critical way towards this criterion. Existing work rely solely on manual red team designs and heuristic adversarial prompts for vulnerability detection and optimization. These approaches lack rigorous mathematical formulation, thus limiting the exploration of diverse attack strategy within quantifiable measure and optimization of LLMs under convergence guarantees. In this paper, we present Red-teaming Game (RTG), a general game-theoretic framework without manual annotation. RTG is designed for analyzing the multi-turn attack and defense interactions between Red-team language Models (RLMs) and Blue-team Language Model (BLM). Within the RTG, we propose Gamified Red-teaming Solver (GRTS) with diversity measure of the semantic space. GRTS is an automated red teaming technique to solve 
&lt;/p&gt;</description></item><item><title>CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.16671</link><description>&lt;p&gt;
&#25581;&#31192;CLIP&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Demystifying CLIP Data. (arXiv:2309.16671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16671
&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#25104;&#21151;&#20027;&#35201;&#24402;&#21151;&#20110;&#20854;&#25968;&#25454;&#32780;&#38750;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#20803;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#24341;&#20837;&#20102;MetaCLIP&#65292;&#35813;&#26041;&#27861;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#20013;&#29983;&#25104;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#65292;&#25552;&#20379;&#20102;&#26356;&#21152;&#35814;&#32454;&#30340;&#25968;&#25454;&#20449;&#24687;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;MetaCLIP&#22312;&#22788;&#29702;400M&#20010;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#26102;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#26159;&#19968;&#31181;&#25512;&#21160;&#35745;&#31639;&#26426;&#35270;&#35273;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#20026;&#29616;&#20195;&#35782;&#21035;&#31995;&#32479;&#21644;&#29983;&#25104;&#27169;&#22411;&#27880;&#20837;&#20102;&#27963;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;CLIP&#25104;&#21151;&#30340;&#20027;&#35201;&#22240;&#32032;&#26159;&#20854;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#27169;&#22411;&#26550;&#26500;&#25110;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;CLIP&#21482;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#25968;&#25454;&#21644;&#22914;&#20309;&#25910;&#38598;&#25968;&#25454;&#30340;&#38750;&#24120;&#26377;&#38480;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#20854;&#20182;&#30740;&#31350;&#21162;&#21147;&#36890;&#36807;&#20351;&#29992;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#36807;&#28388;&#26469;&#37325;&#29616;CLIP&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24847;&#22312;&#25581;&#31034;CLIP&#30340;&#25968;&#25454;&#25972;&#29702;&#26041;&#27861;&#65292;&#24182;&#22312;&#20844;&#24320;&#32473;&#31038;&#21306;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20803;&#25968;&#25454;&#25972;&#29702;&#30340;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;MetaCLIP&#65289;&#12290;MetaCLIP&#36890;&#36807;&#23545;&#20803;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24179;&#34913;&#65292;&#20174;&#21407;&#22987;&#25968;&#25454;&#27744;&#21644;&#20803;&#25968;&#25454;&#65288;&#20174;CLIP&#30340;&#27010;&#24565;&#20013;&#24471;&#20986;&#65289;&#20013;&#20135;&#29983;&#19968;&#20010;&#24179;&#34913;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20005;&#26684;&#38548;&#31163;&#20102;&#27169;&#22411;&#21644;&#35757;&#32451;&#35774;&#32622;&#65292;&#20165;&#19987;&#27880;&#20110;&#25968;&#25454;&#12290;MetaCLIP&#24212;&#29992;&#20110;&#21253;&#21547;400M&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#23545;&#30340;CommonCrawl&#65292;&#24182;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pre-training (CLIP) is an approach that has advanced research and applications in computer vision, fueling modern recognition systems and generative models. We believe that the main ingredient to the success of CLIP is its data and not the model architecture or pre-training objective. However, CLIP only provides very limited information about its data and how it has been collected, leading to works that aim to reproduce CLIP's data by filtering with its model parameters. In this work, we intend to reveal CLIP's data curation approach and in our pursuit of making it open to the community introduce Metadata-Curated Language-Image Pre-training (MetaCLIP). MetaCLIP takes a raw data pool and metadata (derived from CLIP's concepts) and yields a balanced subset over the metadata distribution. Our experimental study rigorously isolates the model and training settings, concentrating solely on data. MetaCLIP applied to CommonCrawl with 400M image-text data pairs outper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PerNee&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#20013;&#24515;&#20803;&#32032;&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#29616;&#26377;NEE&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20013;&#24515;&#20803;&#32032;&#21452;&#37325;&#36523;&#20221;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#25552;&#39640;NEE&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12960</link><description>&lt;p&gt;
&#22522;&#20110;&#20013;&#24515;&#20803;&#32032;&#35782;&#21035;&#30340;&#23884;&#22871;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Nested Event Extraction upon Pivot Element Recogniton. (arXiv:2309.12960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PerNee&#30340;&#26032;&#27169;&#22411;&#65292;&#36890;&#36807;&#35782;&#21035;&#20013;&#24515;&#20803;&#32032;&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#35299;&#20915;&#20102;&#29616;&#26377;NEE&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#20013;&#24515;&#20803;&#32032;&#21452;&#37325;&#36523;&#20221;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#25552;&#39640;NEE&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#20107;&#20214;&#25277;&#21462;&#65288;NEE&#65289;&#26088;&#22312;&#25552;&#21462;&#21253;&#21547;&#20854;&#20182;&#20107;&#20214;&#20316;&#20026;&#20854;&#21442;&#25968;&#30340;&#22797;&#26434;&#20107;&#20214;&#32467;&#26500;&#12290;&#23884;&#22871;&#20107;&#20214;&#28041;&#21450;&#19968;&#31181;&#31216;&#20026;&#20013;&#24515;&#20803;&#32032;&#65288;PEs&#65289;&#30340;&#20803;&#32032;&#65292;&#23427;&#21516;&#26102;&#20316;&#20026;&#22806;&#37096;&#20107;&#20214;&#30340;&#21442;&#25968;&#21644;&#20869;&#37096;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#65292;&#24182;&#23558;&#23427;&#20204;&#36830;&#25509;&#25104;&#23884;&#22871;&#32467;&#26500;&#12290;PEs&#30340;&#36825;&#31181;&#29305;&#27530;&#29305;&#24615;&#32473;&#29616;&#26377;&#30340;NEE&#26041;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;PEs&#30340;&#21452;&#37325;&#36523;&#20221;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#31216;&#20026;PerNee&#65292;&#20027;&#35201;&#22522;&#20110;&#35782;&#21035;PEs&#26469;&#25552;&#21462;&#23884;&#22871;&#20107;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PerNee&#39318;&#20808;&#35782;&#21035;&#20869;&#37096;&#21644;&#22806;&#37096;&#20107;&#20214;&#30340;&#35302;&#21457;&#22120;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#31867;&#35302;&#21457;&#22120;&#23545;&#20043;&#38388;&#20851;&#31995;&#31867;&#22411;&#26469;&#35782;&#21035;PEs&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#34920;&#31034;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;NEE&#24615;&#33021;&#65292;PerNee&#36890;&#36807;&#25552;&#31034;&#23398;&#20064;&#23558;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#35282;&#33394;&#30340;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#30001;&#20110;&#29616;&#26377;&#30340;NEE&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;Gen&#65289;
&lt;/p&gt;
&lt;p&gt;
Nested Event Extraction (NEE) aims to extract complex event structures where an event contains other events as its arguments recursively. Nested events involve a kind of Pivot Elements (PEs) that simultaneously act as arguments of outer events and as triggers of inner events, and thus connect them into nested structures. This special characteristic of PEs brings challenges to existing NEE methods, as they cannot well cope with the dual identities of PEs. Therefore, this paper proposes a new model, called PerNee, which extracts nested events mainly based on recognizing PEs. Specifically, PerNee first recognizes the triggers of both inner and outer events and further recognizes the PEs via classifying the relation type between trigger pairs. In order to obtain better representations of triggers and arguments to further improve NEE performance, it incorporates the information of both event types and argument roles into PerNee through prompt learning. Since existing NEE datasets (e.g., Gen
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.10182</link><description>&lt;p&gt;
&#38899;&#20048;&#20135;&#21697;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive and Risky Message Assessment for Music Products. (arXiv:2309.10182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#38382;&#39064;&#65306;&#22914;&#20309;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#35780;&#20272;&#38899;&#20048;&#20135;&#21697;&#20013;&#30340;&#27491;&#38754;&#21644;&#39118;&#38505;&#20449;&#24687;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#35282;&#24230;&#22810;&#32423;&#38899;&#20048;&#20869;&#23481;&#35780;&#20272;&#30340;&#22522;&#20934;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#24207;&#25968;&#32422;&#26463;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19981;&#20165;&#26126;&#26174;&#20248;&#20110;&#24378;&#22823;&#30340;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#23545;&#24212;&#26041;&#27861;&#65292;&#32780;&#19988;&#21487;&#20197;&#21516;&#26102;&#35780;&#20272;&#22810;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel research problem: assessing positive and risky messages from music products. We first establish a benchmark for multi-angle multi-level music content assessment and then present an effective multi-task prediction model with ordinality-enforcement to solve this problem. Our result shows the proposed method not only significantly outperforms strong task-specific counterparts but can concurrently evaluate multiple aspects.
&lt;/p&gt;</description></item><item><title>Echotune&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;Echo-MSA&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20174;&#24103;&#21040;&#35805;&#35821;&#30340;&#21508;&#31181;&#39063;&#31890;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#38271;&#24230;&#27880;&#24847;&#21147;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.07765</link><description>&lt;p&gt;
Echotune: &#21033;&#29992;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24615;&#30340;&#27169;&#22359;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#22312;ASR&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Echotune: A Modular Extractor Leveraging the Variable-Length Nature of Speech in ASR Tasks. (arXiv:2309.07765v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07765
&lt;/p&gt;
&lt;p&gt;
Echotune&#26159;&#19968;&#20010;&#27169;&#22359;&#21270;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;Echo-MSA&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#20174;&#24103;&#21040;&#35805;&#35821;&#30340;&#21508;&#31181;&#39063;&#31890;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#38271;&#24230;&#27880;&#24847;&#21147;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#24050;&#34987;&#35777;&#26126;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#65292;&#25104;&#20026;&#35813;&#39046;&#22495;&#20247;&#22810;&#30740;&#31350;&#30340;&#22522;&#30784;&#32452;&#20214;&#12290;&#21382;&#21490;&#19978;&#65292;&#35768;&#22810;&#26041;&#27861;&#20381;&#36182;&#20110;&#22266;&#23450;&#38271;&#24230;&#30340;&#27880;&#24847;&#21147;&#31383;&#21475;&#65292;&#36825;&#23545;&#20110;&#25345;&#32493;&#26102;&#38388;&#21644;&#22797;&#26434;&#24615;&#19981;&#21516;&#30340;&#35821;&#38899;&#26679;&#26412;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#23548;&#33268;&#25968;&#25454;&#36807;&#24230;&#24179;&#28369;&#21270;&#21644;&#24573;&#35270;&#20102;&#38271;&#26399;&#36830;&#36890;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Echo-MSA&#65292;&#19968;&#20010;&#20855;&#26377;&#21487;&#21464;&#38271;&#24230;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28789;&#27963;&#27169;&#22359;&#65292;&#21487;&#20197;&#36866;&#24212;&#19981;&#21516;&#22797;&#26434;&#24615;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#35821;&#38899;&#26679;&#26412;&#12290;&#35813;&#27169;&#22359;&#25552;&#20379;&#20102;&#20174;&#24103;&#21644;&#38899;&#32032;&#21040;&#21333;&#35789;&#21644;&#35805;&#35821;&#30340;&#21508;&#31181;&#39063;&#31890;&#24230;&#30340;&#35821;&#38899;&#29305;&#24449;&#25552;&#21462;&#30340;&#28789;&#27963;&#24615;&#12290;&#25552;&#20986;&#30340;&#35774;&#35745;&#25429;&#25417;&#21040;&#20102;&#35821;&#38899;&#30340;&#21487;&#21464;&#38271;&#24230;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#38271;&#24230;&#27880;&#24847;&#21147;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#21033;&#29992;&#20102;&#19968;&#20010;&#24179;&#34892;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20102;&#19968;&#20010;&#21160;&#24577;&#38376;&#25511;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture has proven to be highly effective for Automatic Speech Recognition (ASR) tasks, becoming a foundational component for a plethora of research in the domain. Historically, many approaches have leaned on fixed-length attention windows, which becomes problematic for varied speech samples in duration and complexity, leading to data over-smoothing and neglect of essential long-term connectivity. Addressing this limitation, we introduce Echo-MSA, a nimble module equipped with a variable-length attention mechanism that accommodates a range of speech sample complexities and durations. This module offers the flexibility to extract speech features across various granularities, spanning from frames and phonemes to words and discourse. The proposed design captures the variable length feature of speech and addresses the limitations of fixed-length attention. Our evaluation leverages a parallel attention architecture complemented by a dynamic gating mechanism that amalgam
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.12032</link><description>&lt;p&gt;
&#20174;&#25968;&#37327;&#21040;&#36136;&#37327;&#65306;&#21033;&#29992;&#33258;&#25105;&#24341;&#23548;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#25552;&#21319;LLM&#24615;&#33021;&#20197;&#36827;&#34892;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning. (arXiv:2308.12032v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#36890;&#36807;&#24341;&#20837;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#25351;&#26631;&#65288;IFD&#65289;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#22312;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#65292;&#23637;&#31034;&#20102;&#20248;&#20110;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#65292;&#25351;&#20196;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#24050;&#25104;&#20026;&#19968;&#20010;&#28966;&#28857;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#25105;&#24341;&#23548;&#30340;&#26041;&#27861;&#65292;&#35753;LLM&#33021;&#22815;&#33258;&#20027;&#22320;&#35782;&#21035;&#21644;&#36873;&#25321;&#22823;&#35268;&#27169;&#24320;&#28304;&#25968;&#25454;&#38598;&#20013;&#30340;&#31934;&#36873;&#26679;&#26412;&#65292;&#26377;&#25928;&#20943;&#23569;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#25163;&#21160;&#31579;&#36873;&#21644;&#28508;&#22312;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#25351;&#20196;&#36981;&#24490;&#38590;&#24230;&#65288;IFD&#65289;&#25351;&#26631;&#65292;&#23427;&#25104;&#20026;&#20102;&#19968;&#20010;&#20915;&#23450;&#24615;&#24037;&#20855;&#65292;&#29992;&#20110;&#35782;&#21035;&#27169;&#22411;&#26399;&#26395;&#21709;&#24212;&#21644;&#33258;&#20027;&#29983;&#25104;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#28789;&#27963;&#24212;&#29992;IFD&#65292;&#25105;&#20204;&#33021;&#22815;&#25214;&#21040;&#31934;&#36873;&#26679;&#26412;&#65292;&#20174;&#32780;&#22823;&#24133;&#25552;&#21319;&#27169;&#22411;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;Alpaca&#21644;WizardLM&#31561;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#39564;&#35777;&#25903;&#25345;&#25105;&#20204;&#30340;&#21457;&#29616;&#65307;&#20165;&#20351;&#29992;&#20256;&#32479;&#25968;&#25454;&#36755;&#20837;&#30340;10%&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33258;&#25105;&#24341;&#23548;&#25361;&#36873;&#21644;IFD&#25351;&#26631;&#30340;&#32508;&#21512;&#24847;&#21619;&#30528;LLM&#20248;&#21270;&#30340;&#19968;&#20010;&#21464;&#38761;&#24615;&#39134;&#36291;&#65292;&#26377;&#26395;&#21516;&#26102;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#38477;&#20302;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of Large Language Models, the balance between instruction data quality and quantity has become a focal point. Recognizing this, we introduce a self-guided methodology for LLMs to autonomously discern and select cherry samples from vast open-source datasets, effectively minimizing manual curation and potential cost for instruction tuning an LLM. Our key innovation, the Instruction-Following Difficulty (IFD) metric, emerges as a pivotal tool to identify discrepancies between a model's expected responses and its autonomous generation prowess. Through the adept application of IFD, cherry samples are pinpointed, leading to a marked uptick in model training efficiency. Empirical validations on renowned datasets like Alpaca and WizardLM underpin our findings; with a mere 10% of conventional data input, our strategy showcases improved results. This synthesis of self-guided cherry-picking and the IFD metric signifies a transformative leap in the optimization of LLMs, promising both
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#19982;ASR&#27169;&#22359;&#65292;&#25104;&#21151;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#20174;80%&#38477;&#20302;&#21040;26.4%&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#23558;&#20854;&#36827;&#19968;&#27493;&#38477;&#20302;&#21040;14.5%&#12290;</title><link>http://arxiv.org/abs/2308.11380</link><description>&lt;p&gt;
Convoifilter: &#40481;&#23614;&#37202;&#20250;&#35821;&#38899;&#35782;&#21035;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#19982;ASR&#27169;&#22359;&#65292;&#25104;&#21151;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#20174;80%&#38477;&#20302;&#21040;26.4%&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#23558;&#20854;&#36827;&#19968;&#27493;&#38477;&#20302;&#21040;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#25317;&#25380;&#12289;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#35828;&#35805;&#32773;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#23558;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#19982;&#32972;&#26223;&#22122;&#22768;&#20998;&#31163;&#65292;&#32467;&#21512;ASR&#27169;&#22359;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20174;80%&#38477;&#20302;&#21040;26.4%&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#30340;&#21464;&#21270;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#20250;&#29420;&#31435;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#22686;&#24378;&#21487;&#33021;&#20250;&#23548;&#33268;ASR&#25928;&#29575;&#19979;&#38477;&#12290;&#36890;&#36807;&#23454;&#26045;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#21035;&#35843;&#25972;&#30340;WER&#20174;26.4%&#38477;&#20302;&#21040;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
&lt;/p&gt;</description></item><item><title>WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.05361</link><description>&lt;p&gt;
WeaverBird: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#37329;&#34701;&#20915;&#31574;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine. (arXiv:2308.05361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05361
&lt;/p&gt;
&lt;p&gt;
WeaverBird&#26159;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#26412;&#22320;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#65292;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WeaverBird&#65292;&#19968;&#20010;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;&#26234;&#33021;&#23545;&#35805;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;GPT&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#21033;&#29992;&#37329;&#34701;&#30456;&#20851;&#25991;&#26412;&#30340;&#24191;&#27867;&#35821;&#26009;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#33021;&#22815;&#29702;&#35299;&#22797;&#26434;&#30340;&#37329;&#34701;&#26597;&#35810;&#65292;&#20363;&#22914;&#8220;&#22312;&#36890;&#36135;&#33192;&#32960;&#26399;&#38388;&#22914;&#20309;&#31649;&#29702;&#25105;&#30340;&#25237;&#36164;&#65311;&#8221;&#24182;&#25552;&#20379;&#26126;&#26234;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#36824;&#38598;&#25104;&#20102;&#26412;&#22320;&#30340;&#30693;&#35782;&#24211;&#21644;&#25628;&#32034;&#24341;&#25806;&#20197;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#12290;&#26368;&#32456;&#30340;&#22238;&#31572;&#26159;&#22522;&#20110;&#25628;&#32034;&#32467;&#26524;&#36827;&#34892;&#26465;&#20214;&#32422;&#26463;&#30340;&#65292;&#24182;&#21253;&#21547;&#36866;&#24403;&#30340;&#24341;&#29992;&#26469;&#28304;&#65292;&#20174;&#32780;&#20855;&#26377;&#22686;&#24378;&#30340;&#21487;&#20449;&#24230;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#37329;&#34701;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#24050;&#32463;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#30456;&#27604;&#20854;&#20182;&#27169;&#22411;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#29992;&#25143;&#21487;&#20197;&#22312;&#25105;&#20204;&#30340;&#22312;&#32447;&#28436;&#31034;&#32593;&#31449;https://weaverbird.ttic.edu&#19982;&#25105;&#20204;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#24182;&#35266;&#30475;&#25105;&#20204;&#30340;2&#20998;&#38047;&#28436;&#31034;&#35270;&#39057;https://www.youtube.com/watch?v=yofgeq&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WeaverBird, an intelligent dialogue system designed specifically for the finance domain. Our system harnesses a large language model of GPT architecture that has been tuned using extensive corpora of finance-related text. As a result, our system possesses the capability to understand complex financial queries, such as "How should I manage my investments during inflation?", and provide informed responses. Furthermore, our system incorporates a local knowledge base and a search engine to retrieve relevant information. The final responses are conditioned on the search results and include proper citations to the sources, thus enjoying an enhanced credibility. Through a range of finance-related questions, we have demonstrated the superior performance of our system compared to other models. To experience our system firsthand, users can interact with our live demo at https://weaverbird.ttic.edu, as well as watch our 2-min video illustration at https://www.youtube.com/watch?v=yofgeq
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#25910;&#38598;&#20102;&#32422;1.5&#30334;&#19975;&#26465;&#28085;&#30422;60&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25512;&#25991;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#38395;&#23186;&#20307;&#25253;&#36947;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#19982;&#35813;&#35805;&#39064;&#30456;&#20851;&#30340;&#20027;&#20307;&#12289;&#31435;&#22330;&#12289;&#27010;&#24565;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2306.12886</link><description>&lt;p&gt;
&#20840;&#29699;&#21465;&#20107;&#30340;&#25581;&#31034;&#65306;&#19968;&#20221;&#22810;&#35821;&#31181;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#25506;&#35752;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#38395;&#23186;&#20307;&#25253;&#36947;
&lt;/p&gt;
&lt;p&gt;
Unveiling Global Narratives: A Multilingual Twitter Dataset of News Media on the Russo-Ukrainian Conflict. (arXiv:2306.12886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12886
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25910;&#38598;&#20102;&#32422;1.5&#30334;&#19975;&#26465;&#28085;&#30422;60&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25512;&#25991;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25512;&#29305;&#25968;&#25454;&#38598;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;&#20420;&#20044;&#20914;&#31361;&#30340;&#26032;&#38395;&#23186;&#20307;&#25253;&#36947;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#26631;&#31614;&#21487;&#20197;&#35782;&#21035;&#19982;&#35813;&#35805;&#39064;&#30456;&#20851;&#30340;&#20027;&#20307;&#12289;&#31435;&#22330;&#12289;&#27010;&#24565;&#21644;&#24773;&#24863;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20420;&#20044;&#20914;&#31361;&#19968;&#30452;&#37117;&#26159;&#20840;&#29699;&#23186;&#20307;&#23494;&#38598;&#25253;&#36947;&#30340;&#20027;&#39064;&#12290;&#20102;&#35299;&#36825;&#20010;&#35805;&#39064;&#32972;&#21518;&#30340;&#20840;&#29699;&#21465;&#20107;&#23545;&#20110;&#26088;&#22312;&#20174;&#22810;&#20010;&#23618;&#38754;&#33719;&#21462;&#27934;&#35265;&#30340;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20221;&#29420;&#29305;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25910;&#38598;&#24182;&#22788;&#29702;&#19990;&#30028;&#21508;&#22320;&#26032;&#38395;&#25110;&#23186;&#20307;&#20844;&#21496;&#21457;&#24067;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#25512;&#25991;&#65292;&#37325;&#28857;&#20851;&#27880;&#36825;&#20010;&#35805;&#39064;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;2022&#24180;2&#26376;&#33267;2023&#24180;5&#26376;&#30340;&#25512;&#25991;&#65292;&#20197;&#25910;&#38598;&#32422;1.5&#30334;&#19975;&#26465;&#20351;&#29992;60&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#25512;&#25991;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#25512;&#25991;&#37117;&#38468;&#24102;&#26377;&#22788;&#29702;&#36807;&#30340;&#26631;&#31614;&#65292;&#20801;&#35768;&#23545;&#25552;&#21040;&#30340;&#20027;&#20307;&#12289;&#31435;&#22330;&#12289;&#27010;&#24565;&#21644;&#34920;&#36798;&#30340;&#24773;&#24863;&#36827;&#34892;&#35782;&#21035;&#12290;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#20026;&#24076;&#26395;&#20174;&#19981;&#21516;&#26041;&#38754;&#35843;&#26597;&#20420;&#20044;&#20914;&#31361;&#30340;&#20840;&#29699;&#21465;&#20107;&#65292;&#20363;&#22914;&#35841;&#26159;&#20027;&#35201;&#30340;&#30456;&#20851;&#26041;&#12289;&#25345;&#20160;&#20040;&#24577;&#24230;&#12289;&#36825;&#20123;&#24577;&#24230;&#30340;&#26469;&#28304;&#22312;&#21738;&#37324;&#20197;&#21450;&#19981;&#21516;&#30340;&#27010;&#24565;&#22914;&#20309;&#65292;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing Russo-Ukrainian conflict has been a subject of intense media coverage worldwide. Understanding the global narrative surrounding this topic is crucial for researchers that aim to gain insights into its multifaceted dimensions. In this paper, we present a novel dataset that focuses on this topic by collecting and processing tweets posted by news or media companies on social media across the globe. We collected tweets from February 2022 to May 2023 to acquire approximately 1.5 million tweets in 60 different languages. Each tweet in the dataset is accompanied by processed tags, allowing for the identification of entities, stances, concepts, and sentiments expressed. The availability of the dataset serves as a valuable resource for researchers aiming to investigate the global narrative surrounding the ongoing conflict from various aspects such as who are the prominent entities involved, what stances are taken, where do these stances originate, and how are the different concepts 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#31216;&#20026;NeuroComparatives&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#36827;&#34892;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#65292;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#30693;&#35782;&#36827;&#34892;&#20005;&#26684;&#36807;&#28388;&#12290;</title><link>http://arxiv.org/abs/2305.04978</link><description>&lt;p&gt;
NeuroComparatives&#65306;&#27604;&#36739;&#30693;&#35782;&#30340;&#31070;&#32463;&#31526;&#21495;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#31216;&#20026;NeuroComparatives&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#36827;&#34892;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#65292;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#30693;&#35782;&#36827;&#34892;&#20005;&#26684;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#30693;&#35782;&#26159;&#25105;&#20204;&#19990;&#30028;&#30693;&#35782;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#30740;&#31350;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#27604;&#36739;&#30693;&#35782;&#33719;&#21462;&#20219;&#21153;&#65292;&#21463;&#21040;&#20687;GPT-3&#36825;&#26679;&#26497;&#31471;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26174;&#30528;&#25552;&#39640;&#30340;&#25512;&#21160;&#65292;&#25512;&#21160;&#20102;&#23558;&#20182;&#20204;&#30340;&#30693;&#35782;&#25910;&#38598;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#21162;&#21147;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;API&#35775;&#38382;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30693;&#35782;&#33719;&#21462;&#30340;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30475;&#20284;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#65306;&#26356;&#26131;&#20110;&#35775;&#38382;&#12289;&#35268;&#27169;&#26356;&#23567;&#12289;&#24615;&#33021;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33719;&#21462;&#27604;&#36739;&#30693;&#35782;&#65292;&#20174;&#32780;&#36798;&#21040;&#19982;&#22823;&#35268;&#27169;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;NeuroComparatives&#65292;&#19968;&#31181;&#20351;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#30340;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#26032;&#26694;&#26550;&#65292;&#20854;&#21518;&#32039;&#23494;&#36807;&#28388;&#29983;&#25104;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#21487;&#20197;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#32500;&#24230;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2305.02869</link><description>&lt;p&gt;
&#36890;&#36807;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#23454;&#29616;2&#20493;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
2x Faster Language Model Pre-training via Masked Structural Growth. (arXiv:2305.02869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#21487;&#20197;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#25324;&#20840;&#32500;&#24230;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#20174;&#23567;&#22411;Transformer&#32467;&#26500;&#36880;&#27493;&#25193;&#23637;&#21040;&#22823;&#22411;&#32467;&#26500;&#65292;&#21152;&#24555;&#39044;&#35757;&#32451;&#36827;&#31243;&#12290;&#36825;&#31181;&#28176;&#36827;&#24335;&#25104;&#38271;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#26377;&#20004;&#20010;&#65292;&#21363;&#25104;&#38271;&#36827;&#31243;&#21644;&#25104;&#38271;&#25805;&#20316;&#12290;&#23545;&#20110;&#25104;&#38271;&#36827;&#31243;&#65292;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#28145;&#24230;&#21644;&#21069;&#39304;&#23618;&#30340;&#22810;&#38454;&#27573;&#25193;&#23637;&#65292;&#20294;&#27599;&#20010;&#32500;&#24230;&#23545;&#36827;&#31243;&#25928;&#29575;&#30340;&#24433;&#21709;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#32780;&#23545;&#20110;&#25104;&#38271;&#25805;&#20316;&#65292;&#29616;&#26377;&#30740;&#31350;&#20381;&#36182;&#20110;&#26032;&#26435;&#37325;&#30340;&#21021;&#22987;&#21270;&#26469;&#32487;&#25215;&#21407;&#26377;&#30340;&#30693;&#35782;&#65292;&#21482;&#23454;&#29616;&#20102;&#38750;&#20005;&#26684;&#30340;&#20989;&#25968;&#20445;&#30041;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#36827;&#19968;&#27493;&#30340;&#35757;&#32451;&#21160;&#24577;&#20248;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#25513;&#30721;&#32467;&#26500;&#25104;&#38271;&#65288;MSG&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#28041;&#21450;&#25152;&#26377;&#21487;&#33021;&#32500;&#24230;&#30340;&#25104;&#38271;&#36827;&#31243;&#21644;&#29420;&#31435;&#20110;&#26032;&#26435;&#37325;&#21021;&#22987;&#21270;&#30340;&#20989;&#25968;&#20005;&#26684;&#20445;&#30041;&#25104;&#38271;&#25805;&#20316;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MSG&#21487;&#26174;&#33879;&#21152;&#36895;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acceleration of large language model pre-training is a critical issue in present NLP research. In this paper, we focus on speeding up pre-training by progressively growing from a small Transformer structure to a large one. There are two main research problems related to progressive growth: growth schedule and growth operator. For growth schedule, existing work has explored multi-stage expansion of depth and feedforward layers. However, the impact of each dimension on the schedule's efficiency is still an open question. For growth operator, existing work relies on the initialization of new weights to inherit knowledge, and achieve only non-strict function preservation, limiting further optimization of training dynamics. To address these issues, we propose Masked Structural Growth (MSG), including growth schedules involving all possible dimensions and strictly function-preserving growth operators that is independent of the initialization of new weights. Experiments show that MSG is signi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;</title><link>http://arxiv.org/abs/2304.13567</link><description>&lt;p&gt;
&#20301;&#32622;&#20559;&#24046;&#23545;token&#20998;&#31867;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Position Bias on Language Models in Token Classification. (arXiv:2304.13567v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13567
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;token&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#22312;&#20855;&#20307;&#20219;&#21153;&#20013;&#65292;BERT&#12289;ERNIE&#12289;ELECTRA&#31561;&#32534;&#30721;&#22120;&#20197;&#21450;GPT2&#21644;BLOOM&#31561;&#35299;&#30721;&#22120;&#30340;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#25110;&#35789;&#24615;&#26631;&#27880;&#31561;&#19979;&#28216;&#20219;&#21153;&#24050;&#30693;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#36127;&#31034;&#20363;&#30340;&#27604;&#20363;&#21644;&#31867;&#19981;&#24179;&#34913;&#26041;&#38754;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#21478;&#19968;&#20010;&#29305;&#23450;&#38382;&#39064;&#65292;&#21363;token&#20998;&#31867;&#20219;&#21153;&#20013;&#27491;&#31034;&#20363;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Token&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#20301;&#32622;&#20559;&#24046;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;CoNLL03&#21644;OntoNote5.0&#29992;&#20110;NER&#65292;English Tree Bank UD_en&#21644;TweeBank&#29992;&#20110;POS&#26631;&#35760;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#30740;&#31350;Transformer&#27169;&#22411;&#20013;&#30340;&#20301;&#32622;&#20559;&#24046;&#12290;&#25105;&#20204;&#21457;&#29616;&#20687;BERT&#12289;ERNIE&#12289;ELECTRA&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#21644;&#20687;GPT2 &#21644;BLOOM&#36825;&#26679;&#30340;&#35299;&#30721;&#22120;&#24179;&#22343;&#24615;&#33021;&#19979;&#38477;&#20102;3%&#21644;9%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have shown state-of-the-art performance in Natural Language Processing (NLP) tasks. Downstream tasks such as Named Entity Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data imbalance issues, specifically in terms of the ratio of positive to negative examples, and class imbalance. In this paper, we investigate an additional specific issue for language models, namely the position bias of positive examples in token classification tasks. Therefore, we conduct an in-depth evaluation of the impact of position bias on the performance of LMs when fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We propose an evaluation approach to investigate position bias in Transformer models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in their performan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;</title><link>http://arxiv.org/abs/2304.07633</link><description>&lt;p&gt;
&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#21270;&#31070;&#32463;&#27169;&#22411;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#22810;&#27169;&#24577;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#28436;&#21270;&#25345;&#32493;&#22686;&#38271;&#65292;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#35875;&#35328;&#25110;&#34394;&#20551;&#26032;&#38395;&#32534;&#36753;&#20027;&#35201;&#20381;&#36182;&#20110;&#29983;&#25104;&#21644;/&#25110;&#20266;&#36896;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#35270;&#39057;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;&#34394;&#20551;&#20449;&#24687;&#21019;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#19981;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65289;&#26469;&#27450;&#39575;&#20844;&#20247;&#21644;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#31181;&#26032;&#22411;&#30340;&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#38590;&#24230;&#65292;&#20063;&#22686;&#21152;&#20102;&#28548;&#28165;&#30340;&#38590;&#24230;&#65292;&#22240;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#27169;&#24577;&#37117;&#36275;&#22815;&#25509;&#36817;&#30495;&#23454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#21435;&#19978;&#19979;&#25991;&#26816;&#27979;&#65292;&#21516;&#26102;&#35782;&#21035;&#19981;&#21305;&#37197;&#30340;&#23545;&#21644;&#36328;&#27169;&#24577;&#30683;&#30462;&#65292;&#36825;&#23545;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#30340;&#35760;&#24405;&#28548;&#28165;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22522;&#20110;Abstract M&#36827;&#34892;&#31526;&#21495;&#21270;&#20998;&#35299;&#65292;&#24471;&#21040;&#19968;&#32452;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the sustained evolution of misinformation that aims at manipulating public opinions. Unlike traditional rumors or fake news editors who mainly rely on generated and/or counterfeited images, text and videos, current misinformation creators now more tend to use out-of-context multimedia contents (e.g. mismatched images and captions) to deceive the public and fake news detection systems. This new type of misinformation increases the difficulty of not only detection but also clarification, because every individual modality is close enough to true information. To address this challenge, in this paper we explore how to achieve interpretable cross-modal de-contextualization detection that simultaneously identifies the mismatched pairs and the cross-modal contradictions, which is helpful for fact-check websites to document clarifications. The proposed model first symbolically disassembles the text-modality information to a set of fact queries based on the Abstract M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item></channel></rss>