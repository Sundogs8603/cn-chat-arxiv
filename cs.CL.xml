<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02491</link><description>&lt;p&gt;
&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Norms of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02491
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25361;&#25112;&#65292;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#28085;&#30422;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#38382;&#39064;&#65292;&#36890;&#36807;&#22810;&#20195;&#29702;&#26694;&#26550;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#31038;&#20250;&#35268;&#33539;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#65292;&#20197;&#26816;&#39564;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#12290;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35201;&#27714;&#20855;&#26377;&#35299;&#20915;&#31038;&#20250;&#35268;&#33539;&#30340;&#22522;&#26412;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#26368;&#22823;&#30340;&#31038;&#20250;&#35268;&#33539;&#25216;&#33021;&#38598;&#65292;&#21253;&#25324;402&#39033;&#25216;&#33021;&#21644;12,383&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20174;&#35266;&#28857;&#21644;&#35770;&#28857;&#21040;&#25991;&#21270;&#21644;&#27861;&#24459;&#31561;&#24191;&#27867;&#30340;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#26681;&#25454;K-12&#35838;&#31243;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#30452;&#25509;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31038;&#20250;&#29702;&#35299;&#33021;&#21147;&#19982;&#20154;&#31867;&#36827;&#34892;&#27604;&#36739;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#19982;&#23567;&#23398;&#29983;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20960;&#20046;&#38543;&#26426;&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT3.5-Turbo&#21644;LLaMA2-Chat&#65289;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20165;&#30053;&#20302;&#20110;&#20154;&#31867;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20195;&#29702;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#29702;&#35299;&#31038;&#20250;&#35268;&#33539;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02491v1 Announce Type: cross  Abstract: We present a new challenge to examine whether large language models understand social norms. In contrast to existing datasets, our dataset requires a fundamental understanding of social norms to solve. Our dataset features the largest set of social norm skills, consisting of 402 skills and 12,383 questions covering a wide set of social norms ranging from opinions and arguments to culture and laws. We design our dataset according to the K-12 curriculum. This enables the direct comparison of the social understanding of large language models to humans, more specifically, elementary students. While prior work generates nearly random accuracy on our benchmark, recent large language models such as GPT3.5-Turbo and LLaMA2-Chat are able to improve the performance significantly, only slightly below human performance. We then propose a multi-agent framework based on large language models to improve the models' ability to understand social norms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#65292;&#21457;&#29616;&#34429;&#28982;LLMs&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;</title><link>https://arxiv.org/abs/2404.02124</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;
&lt;/p&gt;
&lt;p&gt;
Exploring Automated Distractor Generation for Math Multiple-choice Questions via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02124
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#65292;&#21457;&#29616;&#34429;&#28982;LLMs&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#39064;&#22312;&#20960;&#20046;&#25152;&#26377;&#25945;&#32946;&#23618;&#27425;&#20013;&#37117;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#26131;&#20110;&#31649;&#29702;&#12289;&#35780;&#20998;&#65292;&#24182;&#19988;&#26159;&#35780;&#20272;&#21644;&#23454;&#36341;&#20013;&#21487;&#38752;&#30340;&#26684;&#24335;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#20043;&#19968;&#26159;&#24178;&#25200;&#39033;&#65292;&#21363;&#38024;&#23545;&#30495;&#23454;&#23398;&#29983;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#32780;&#35774;&#35745;&#30340;&#19981;&#27491;&#30830;&#36873;&#39033;&#12290;&#30446;&#21069;&#65292;&#21046;&#20316;&#39640;&#36136;&#37327;&#24178;&#25200;&#39033;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#28982;&#26159;&#25945;&#24072;&#21644;&#23398;&#20064;&#20869;&#23481;&#35774;&#35745;&#32773;&#30340;&#21171;&#21160;&#21644;&#32791;&#26102;&#24037;&#20316;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#39046;&#22495;&#20013;&#33258;&#21160;&#29983;&#25104;&#24178;&#25200;&#39033;&#30340;&#20219;&#21153;&#65292;&#24182;&#25506;&#32034;&#20102;&#21508;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#21040;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;&#30495;&#23454;&#25968;&#23398;&#22810;&#39033;&#36873;&#25321;&#39064;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#21457;&#29616;&#34429;&#28982;LLM&#21487;&#20197;&#29983;&#25104;&#19968;&#20123;&#25968;&#23398;&#19978;&#26377;&#25928;&#30340;&#24178;&#25200;&#39033;&#65292;&#20294;&#23427;&#20204;&#22312;&#39044;&#27979;&#24120;&#35265;&#38169;&#35823;&#25110;&#35823;&#35299;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02124v1 Announce Type: new  Abstract: Multiple-choice questions (MCQs) are ubiquitous in almost all levels of education since they are easy to administer, grade, and are a reliable format in assessments and practices. One of the most important aspects of MCQs is the distractors, i.e., incorrect options that are designed to target common errors or misconceptions among real students. To date, the task of crafting high-quality distractors largely remains a labor and time-intensive process for teachers and learning content designers, which has limited scalability. In this work, we study the task of automated distractor generation in the domain of math MCQs and explore a wide variety of large language model (LLM)-based approaches, from in-context learning to fine-tuning. We conduct extensive experiments using a real-world math MCQ dataset and find that although LLMs can generate some mathematically valid distractors, they are less adept at anticipating common errors or misconcept
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18933</link><description>&lt;p&gt;
SemEval&#20219;&#21153;1&#65306;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#32780;&#20808;&#21069;&#30340;&#20849;&#20139;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21017;&#35843;&#26597;&#20102;&#36328;&#36234;14&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#23612;&#35821;&#12289;&#22522;&#23612;&#20122;&#40065;&#23433;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#30340;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#24182;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#22320;&#21306;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#29305;&#28857;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#30456;&#23545;&#26377;&#38480;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19968;&#20010;&#19982;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#21477;&#23545;&#65292;&#35813;&#20998;&#25968;&#34920;&#31034;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#31243;&#24230;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#22312;&#19977;&#20010;&#20027;&#35201;&#36712;&#36947;&#20013;&#30340;14&#31181;&#35821;&#35328;&#20013;&#25353;&#23427;&#20204;&#22312;&#24847;&#20041;&#19978;&#30340;&#25509;&#36817;&#31243;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#31243;&#24230;&#65289;&#23545;&#21477;&#23545;&#36827;&#34892;&#25490;&#21517;&#65306;(a) &#30417;&#30563;&#65292;(b) &#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#38598;&#25104;&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#20026;&#20302;&#22256;&#24785;&#24230;&#30340;&#29256;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.16038</link><description>&lt;p&gt;
&#21333;&#35843;&#37322;&#20041;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Monotonic Paraphrasing Improves Generalization of Language Model Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16038
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#38598;&#25104;&#35299;&#30721;&#36807;&#31243;&#65292;&#23558;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#20026;&#20302;&#22256;&#24785;&#24230;&#30340;&#29256;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#29616;&#21487;&#33021;&#20250;&#38543;&#30528;&#21516;&#19968;&#20219;&#21153;&#30340;&#19981;&#21516;&#25552;&#31034;&#25110;&#25351;&#20196;&#32780;&#21464;&#21270;&#12290;&#36825;&#31181;&#29616;&#35937;&#30340;&#19968;&#20010;&#20844;&#35748;&#22240;&#32032;&#26159;&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#30340;&#29087;&#24713;&#31243;&#24230;&#65292;&#36890;&#24120;&#36890;&#36807;&#20854;&#22256;&#24785;&#24230;&#26469;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#21487;&#33021;&#25552;&#31034;&#30701;&#35821;&#30340;&#24040;&#22823;&#31354;&#38388;&#65292;&#25214;&#21040;&#22256;&#24785;&#24230;&#26368;&#20302;&#30340;&#25552;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21333;&#35843;&#37322;&#20041;&#65288;MonoPara&#65289;&#65292;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#30721;&#31574;&#30053;&#65292;&#26681;&#25454;&#37322;&#20041;LM&#21644;&#30446;&#26631;LM&#65288;&#21363;&#25552;&#31034;&#25110;&#25351;&#20196;&#25191;&#34892;&#22120;&#65289;&#30340;&#38598;&#21512;&#26469;&#23558;&#32473;&#23450;&#25552;&#31034;&#25110;&#25351;&#20196;&#37322;&#20041;&#21270;&#20026;&#20854;&#20302;&#22256;&#24785;&#24230;&#30340;&#23545;&#24212;&#29289;&#12290;&#38598;&#21512;&#35299;&#30721;&#36807;&#31243;&#21487;&#20197;&#26377;&#25928;&#22320;&#37322;&#20041;&#21407;&#22987;&#25552;&#31034;&#32780;&#19981;&#25913;&#21464;&#20854;&#35821;&#20041;&#21547;&#20041;&#65292;&#21516;&#26102;&#21333;&#35843;&#22320;&#38477;&#20302;&#27599;&#20010;&#29983;&#25104;&#29289;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16038v1 Announce Type: new  Abstract: Performance of large language models (LLMs) may vary with different prompts or instructions of even the same task. One commonly recognized factor for this phenomenon is the model's familiarity with the given prompt or instruction, which is typically estimated by its perplexity. However, finding the prompt with the lowest perplexity is challenging, given the enormous space of possible prompting phrases. In this paper, we propose monotonic paraphrasing (MonoPara), an end-to-end decoding strategy that paraphrases given prompts or instructions into their lower perplexity counterparts based on an ensemble of a paraphrase LM for prompt (or instruction) rewriting, and a target LM (i.e. the prompt or instruction executor) that constrains the generation for lower perplexity. The ensemble decoding process can efficiently paraphrase the original prompt without altering its semantic meaning, while monotonically decreasing the perplexity of each gene
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00199</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Socratic Question Generation using Data Augmentation and Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00199
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#65292;&#25913;&#36827;&#20102;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#26041;&#27861;&#65292;&#20943;&#36731;&#25945;&#24072;&#32321;&#37325;&#30340;&#24037;&#20316;&#37327;&#65292;&#38450;&#27490;&#29983;&#25104;&#26080;&#25928;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33487;&#26684;&#25289;&#24213;&#26041;&#27861;&#26159;&#19968;&#31181;&#24341;&#23548;&#23398;&#29983;&#29420;&#31435;&#35299;&#20915;&#38382;&#39064;&#32780;&#19981;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20559;&#22909;&#20248;&#21270;&#25913;&#36827;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#33487;&#26684;&#25289;&#24213;&#38382;&#39064;&#65292;&#20197;&#20943;&#36731;&#25945;&#24072;&#30340;&#32321;&#37325;&#24037;&#20316;&#37327;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#28041;&#21450;&#25552;&#31034;&#36825;&#20123;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26377;&#26102;&#20250;&#20135;&#29983;&#26080;&#25928;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#30452;&#25509;&#25581;&#31034;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#25110;&#25552;&#20379;&#26080;&#20851;&#25110;&#36807;&#26089;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#39318;&#20808;&#25552;&#20986;&#19968;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#29616;&#26377;&#30340;&#33487;&#26684;&#25289;&#24213;&#25552;&#38382;&#25968;&#25454;&#38598;&#65307;&#20854;&#27425;&#65292;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#26469;&#20248;&#21270;&#24320;&#28304;&#24040;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;LLama 2&#65292;&#20197;&#26356;&#20542;&#21521;&#20110;&#22320;&#38754;&#30495;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00199v1 Announce Type: new  Abstract: The Socratic method is a way of guiding students toward solving a problem independently without directly revealing the solution to the problem. Although this method has been shown to significantly improve student learning outcomes, it remains a complex labor-intensive task for instructors. Large language models (LLMs) can be used to augment human effort by automatically generating Socratic questions for students. However, existing methods that involve prompting these LLMs sometimes produce invalid outputs, e.g., those that directly reveal the solution to the problem or provide irrelevant or premature questions. To alleviate this problem, inspired by reinforcement learning with AI feedback (RLAIF), we first propose a data augmentation method to enrich existing Socratic questioning datasets with questions that are invalid in specific ways. Next, we propose a method to optimize open-source LLMs such as LLama 2 to prefer ground-truth questio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.15758</link><description>&lt;p&gt;
Chimera: &#34701;&#21512;&#25152;&#26377;&#20196;&#29260;&#30340;&#26080;&#25439;&#35299;&#30721;&#26041;&#27861;&#65292;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Chimera: A Lossless Decoding Method for Accelerating Large Language Models Inference by Fusing all Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Chimera&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#65292;&#36890;&#36807;&#24341;&#20837;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#21644;&#20004;&#31181;&#31574;&#30053;&#65292;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#65292;&#20197;&#35299;&#20915;&#35299;&#30721;&#36807;&#31243;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#34987;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#25152;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#30446;&#21069;&#30340;&#26041;&#27861;&#24050;&#32463;&#21512;&#24182;&#20102;&#39069;&#22806;&#30340;&#35299;&#30721;&#22836;&#65292;&#20197;&#23454;&#29616;&#23545;&#22810;&#20010;&#21518;&#32493;&#20196;&#29260;&#30340;&#24182;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#30721;&#22836;&#30340;&#20934;&#30830;&#24615;&#36828;&#19981;&#21450;&#33258;&#22238;&#24402;&#35299;&#30721;&#26041;&#27861;&#12290;&#37492;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chimera&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#25512;&#27979;&#37319;&#26679;&#35774;&#35745;&#30340;&#26032;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#33609;&#31295;&#27169;&#22411;&#65292;&#33021;&#22815;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#29983;&#25104;&#30340;&#20196;&#29260;&#26469;&#39044;&#27979;&#21518;&#32493;&#21333;&#35789;&#12290;&#20026;&#20102;&#30830;&#20445;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#22312;&#36731;&#37327;&#32423;&#33609;&#31295;&#27169;&#22411;&#20013;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#24213;&#23618;&#25429;&#33719;&#30701;&#31243;&#20381;&#36182;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15758v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their widespread application is hindered by the resource-intensive decoding process. To address this challenge, current approaches have incorporated additional decoding heads to enable parallel prediction of multiple subsequent tokens, thereby achieving inference acceleration. Nevertheless, the accuracy of these decoding heads falls short of the auto-regressive decoding approach.   In light of these limitations, we propose Chimera, a novel framework specifically designed for speculative sampling. Within this framework, we introduce a lightweight draft model that effectively utilizes previously generated tokens to predict subsequent words. To ensure both accuracy and efficiency, we present two strategies within the lightweight draft model. Firstly, we focus on capturing short-range dependencies at the bottom layer. Secondly, we leverage
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.15473</link><description>&lt;p&gt;
&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#22312;RLHF&#20013;&#39640;&#25928;&#24314;&#27169;&#22870;&#21169;&#65306;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Domain Knowledge for Efficient Reward Modelling in RLHF: A Case-Study in E-Commerce Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15473
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;RLHF&#20013;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#26469;&#38477;&#20302;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#25152;&#38656;&#30340;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#24050;&#25104;&#20026;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26397;&#21521;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#30340;&#20027;&#23548;&#31574;&#30053;&#12290;&#35813;&#31574;&#30053;&#30340;&#20851;&#38190;&#22312;&#20110;&#20351;&#29992;&#19968;&#20010;&#33021;&#22815;&#21453;&#26144;&#19982;&#20154;&#31867;&#30456;&#20851;&#30340;&#28508;&#22312;&#22870;&#21169;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#65288;{$\varphi$}&#65289;&#12290;&#34429;&#28982;&#36825;&#19968;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#35757;&#32451;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#20559;&#22909;&#27880;&#37322;&#65288;&#36890;&#24120;&#25968;&#37327;&#32423;&#20026;&#25968;&#19975;&#65289;&#26469;&#35757;&#32451;{$\varphi$}&#12290;&#22914;&#26524;&#22870;&#21169;&#27169;&#22411;&#21487;&#20197;&#34987;&#26222;&#36941;&#20351;&#29992;&#65292;&#36825;&#31181;&#22823;&#35268;&#27169;&#20559;&#22909;&#27880;&#37322;&#26159;&#21487;&#20197;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#20215;&#20540;/&#30446;&#26631;&#26159;&#20027;&#35266;&#30340;&#65292;&#24182;&#19988;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#24615;&#36136;&#12290;&#36825;&#23545;&#20110;&#25910;&#38598;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#30340;&#22810;&#26679;&#21270;&#20559;&#22909;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#39046;&#22495;&#30693;&#35782;&#34701;&#20837;{$\varphi$}&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#25152;&#38656;&#27880;&#37322;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#22312;&#30005;&#23376;&#21830;&#21153;&#24847;&#35265;&#25688;&#35201;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15473v1 Announce Type: new  Abstract: Reinforcement Learning from Human Feedback (RLHF) has become a dominating strategy in steering Language Models (LMs) towards human values/goals. The key to the strategy is employing a reward model ({$\varphi$}) which can reflect a latent reward model with humans. While this strategy has proven to be effective, the training methodology requires a lot of human preference annotation (usually of the order of tens of thousands) to train {$\varphi$}. Such large-scale preference annotations can be achievable if the reward model can be ubiquitously used. However, human values/goals are subjective and depend on the nature of the task. This poses a challenge in collecting diverse preferences for downstream applications. To address this, we propose a novel methodology to infuse domain knowledge into {$\varphi$}, which reduces the size of preference annotation required. We validate our approach in E-Commerce Opinion Summarization, with a significant
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13919</link><description>&lt;p&gt;
SYNFAC-EDIT: &#29992;&#20110;&#20020;&#24202;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#23545;&#40784;&#30340;&#21512;&#25104;&#27169;&#20223;&#32534;&#36753;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
SYNFAC-EDIT: Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#20197;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#65292;&#24357;&#34917;&#20102;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#21487;&#29992;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT&#21644;Llama&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#20107;&#23454;&#19981;&#20934;&#30830;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#26159;&#20020;&#24202;NLP&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#38169;&#35823;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#20107;&#23454;&#23545;&#40784;&#30340;&#19987;&#23478;&#27880;&#37322;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#19988;&#26377;&#38480;&#30340;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#27969;&#31243;&#65292;&#21033;&#29992;GPT-3.5&#21644;GPT-4&#29983;&#25104;&#39640;&#36136;&#37327;&#21453;&#39304;&#65292;&#26088;&#22312;&#22686;&#24378;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#30340;&#20107;&#23454;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#32534;&#36753;&#21453;&#39304;&#65292;&#22312;&#27809;&#26377;&#39069;&#22806;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#25311;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#25913;&#21892;AI&#31995;&#32479;&#36755;&#20986;&#30340;&#23454;&#38469;&#22330;&#26223;&#12290;&#23613;&#31649;GPT&#22312;&#21508;&#31181;&#20020;&#24202;NLP&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#20102;&#19987;&#19994;&#27700;&#24179;&#65292;&#27604;&#22914;&#21307;&#23398;&#25191;&#29031;&#32771;&#35797;&#65292;&#20294;&#23545;&#20854;&#25552;&#20379;&#25913;&#21892;&#36739;&#24369;LM&#25110;LLM&#29983;&#25104;&#36136;&#37327;&#30340;&#19987;&#19994;&#32423;&#32534;&#36753;&#21453;&#39304;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13919v1 Announce Type: cross  Abstract: Large Language Models (LLMs) such as GPT and Llama have demonstrated significant achievements in summarization tasks but struggle with factual inaccuracies, a critical issue in clinical NLP applications where errors could lead to serious consequences. To counter the high costs and limited availability of expert-annotated data for factual alignment, this study introduces an innovative pipeline that utilizes GPT-3.5 and GPT-4 to generate high-quality feedback aimed at enhancing factual consistency in clinical note summarization. Our research primarily focuses on edit feedback, mirroring the practical scenario in which medical professionals refine AI system outputs without the need for additional annotations. Despite GPT's proven expertise in various clinical NLP tasks, such as the Medical Licensing Examination, there is scant research on its capacity to deliver expert-level edit feedback for improving weaker LMs or LLMs generation qualit
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01858</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explaining latent representations of generative models with large multimodal models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01858
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#23545;&#20110;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#27169;&#22411;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#65292;&#19988;&#23450;&#24615;&#22320;&#23637;&#31034;&#20102;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#29983;&#25104;&#28508;&#22312;&#22240;&#32032;&#34920;&#31034;&#26159;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#38543;&#30528;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#23427;&#21487;&#20197;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#23545;&#40784;&#20197;&#29983;&#25104;&#31572;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20840;&#38754;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#37327;&#21270;&#35780;&#20272;&#20102;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#22312;&#22810;&#20010;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20043;&#38388;&#35780;&#20272;&#20102;&#35299;&#37322;&#29983;&#25104;&#30340;&#24615;&#33021;&#65292;&#24182;&#23450;&#24615;&#22320;&#21487;&#35270;&#21270;&#20102;&#27599;&#20010;&#28508;&#22312;&#22240;&#32032;&#30340;&#21464;&#21270;&#65292;&#20197;&#23398;&#20064;&#19981;&#21516;&#29983;&#25104;&#27169;&#22411;&#23545;&#35299;&#37322;&#30340;&#35299;&#32544;&#25928;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning interpretable representations of data generative latent factors is an important topic for the development of artificial intelligence. With the rise of the large multimodal model, it can align images with text to generate answers. In this work, we propose a framework to comprehensively explain each latent factor in the generative models using a large multimodal model. We further measure the uncertainty of our generated explanations, quantitatively evaluate the performance of explanation generation among multiple large multimodal models, and qualitatively visualize the variations of each latent factor to learn the disentanglement effects of different generative models on explanations. Finally, we discuss the explanatory capabilities and limitations of state-of-the-art large multimodal models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2402.01805</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#22270;&#25512;&#29702;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Limitations of Graph Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25512;&#29702;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#28145;&#24230;&#65292;&#24182;&#21457;&#29616;&#20102;LLMs&#30340;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#23545;&#20110;&#33410;&#28857;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#26377;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#19988;LLMs&#23384;&#22312;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#65292;&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#25512;&#29702;&#25552;&#31034;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20165;&#36890;&#36807;&#22522;&#20110;&#35821;&#35328;&#30340;&#25552;&#31034;&#23601;&#23637;&#31034;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22270;&#25512;&#29702;&#38382;&#39064;&#27979;&#35797;&#20102;5&#31181;&#19981;&#21516;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65292;GPT-3.5&#65292;Claude-2&#65292;Llama-2&#21644;Palm-2&#65289;&#30340;&#25512;&#29702;&#28145;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;10&#20010;&#19981;&#21516;&#30340;&#22270;&#36941;&#21382;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#20195;&#34920;&#30528;&#36880;&#27493;&#22686;&#21152;&#30340;&#22797;&#26434;&#24615;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#22270;&#22823;&#23567;&#20197;&#21450;&#19981;&#21516;&#24418;&#24335;&#30340;k-shot&#25552;&#31034;&#30340;&#35774;&#32622;&#20998;&#26512;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#36825;&#20010;&#22522;&#20934;&#27979;&#35797;&#36807;&#31243;&#65292;&#25105;&#20204;&#20984;&#26174;&#20102;LLMs&#30340;&#21508;&#31181;&#23616;&#38480;&#24615;&#12289;&#20559;&#35265;&#21644;&#23646;&#24615;&#65292;&#27604;&#22914;&#19982;&#27599;&#20010;&#33410;&#28857;&#30340;&#36941;&#21382;&#33258;&#30001;&#24230;&#30340;&#24179;&#22343;&#24230;&#25968;&#21576;&#21453;&#21521;&#20851;&#31995;&#65292;k-shot&#25552;&#31034;&#23545;&#22270;&#25512;&#29702;&#20219;&#21153;&#30340;&#25972;&#20307;&#36127;&#38754;&#24433;&#21709;&#65292;&#20197;&#21450;&#31215;&#26497;&#30340;&#22238;&#24212;&#20559;&#24046;&#23548;&#33268;LLMs&#26080;&#27861;&#35782;&#21035;&#26377;&#25928;&#35299;&#30340;&#32570;&#22833;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#19987;&#38376;&#29992;&#20110;&#22270;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Large Language Models have demonstrated various types of reasoning capabilities through language-based prompts alone. However, in this paper, we test the depth of graph reasoning for 5 different LLMs (GPT-4, GPT-3.5, Claude-2, Llama-2 and Palm-2) through the problems of graph reasoning. In particular, we design 10 distinct problems of graph traversal, each representing increasing levels of complexity. Further, we analyze the performance of models across various settings such as varying sizes of graphs as well as different forms of k-shot prompting. We highlight various limitations, biases, and properties of LLMs through this benchmarking process, such as an inverse relation to the average degrees of freedom of traversal per node in graphs, the overall negative impact of k-shot prompting on graph reasoning tasks, and a positive response bias which prevents LLMs from identifying the absence of a valid solution. Finally, we propose a new prompting technique specially designed f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;HICL&#21033;&#29992;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26356;&#26126;&#30830;&#30340;&#25552;&#31034;&#26041;&#24335;&#26469;&#22686;&#24378;LLM&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#31034;&#20363;&#26816;&#32034;&#22120;&#65288;HER&#65289;&#65292;&#25105;&#20204;&#36824;&#33021;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#31034;&#33539;&#12290;&#23545;&#20110;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;HICL&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.01949</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Hint-enhanced In-Context Learning wakes Large Language Models up for knowledge-intensive tasks. (arXiv:2311.01949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01949
&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;HICL&#21033;&#29992;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#26356;&#26126;&#30830;&#30340;&#25552;&#31034;&#26041;&#24335;&#26469;&#22686;&#24378;LLM&#30340;&#23398;&#20064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#31034;&#20363;&#26816;&#32034;&#22120;&#65288;HER&#65289;&#65292;&#25105;&#20204;&#36824;&#33021;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#26469;&#22686;&#24378;&#31034;&#33539;&#12290;&#23545;&#20110;&#24320;&#25918;&#22495;&#38382;&#31572;&#65292;HICL&#22312;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#24050;&#32463;&#20986;&#29616;&#65292;&#20351;&#24471;&#23427;&#20204;&#33021;&#22815;&#20174;&#31034;&#33539;&#20013;&#23398;&#20064;&#36755;&#20837;-&#26631;&#31614;&#26144;&#23556;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#26631;&#20934;ICL&#35774;&#32622;&#19979;&#65292;LLM&#26377;&#26102;&#20250;&#24573;&#30053;&#31034;&#33539;&#20013;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#20363;&#31216;&#20026;&#25552;&#31034;&#22686;&#24378;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;HICL&#65289;&#65292;&#26469;&#25506;&#32034;ICL&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#22495;&#38382;&#31572;&#20013;&#12290;HICL&#21033;&#29992;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20174;&#31034;&#33539;&#20013;&#25552;&#21462;&#19982;&#26597;&#35810;&#30456;&#20851;&#30340;&#30693;&#35782;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#30693;&#35782;&#19982;LLM&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36319;&#36394;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#20197;&#35782;&#21035;&#29305;&#23450;&#30340;&#31034;&#20363;&#65292;&#24182;&#24341;&#20837;&#19968;&#20010;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#31034;&#20363;&#26816;&#32034;&#22120;&#65288;HER&#65289;&#26469;&#36873;&#25321;&#26377;&#20449;&#24687;&#37327;&#30340;&#31034;&#20363;&#36827;&#34892;&#22686;&#24378;&#31034;&#33539;&#12290;&#25105;&#20204;&#20351;&#29992;HER&#35780;&#20272;&#20102;HICL&#22312;3&#20010;&#24320;&#25918;&#22495;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#24182;&#35266;&#23519;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) ability has emerged with the increasing scale of large language models (LLMs), enabling them to learn input-label mappings from demonstrations and perform well on downstream tasks. However, under the standard ICL setting, LLMs may sometimes neglect query-related information in demonstrations, leading to incorrect predictions. To address this limitation, we propose a new paradigm called Hint-enhanced In-Context Learning (HICL) to explore the power of ICL in open-domain question answering, an important form in knowledge-intensive tasks. HICL leverages LLMs' reasoning ability to extract query-related knowledge from demonstrations, then concatenates the knowledge to prompt LLMs in a more explicit way. Furthermore, we track the source of this knowledge to identify specific examples, and introduce a Hint-related Example Retriever (HER) to select informative examples for enhanced demonstrations. We evaluate HICL with HER on 3 open-domain QA benchmarks, and observe av
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.10688</link><description>&lt;p&gt;
&#19968;&#31181;&#20165;&#35299;&#30721;&#22120;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A decoder-only foundation model for time-series forecasting. (arXiv:2310.10688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22522;&#30784;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#25509;&#36817;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#22522;&#30784;&#27169;&#22411;&#65292;&#20854;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#24320;&#31665;&#21363;&#29992;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#25509;&#36817;&#27599;&#20010;&#20010;&#21035;&#25968;&#25454;&#38598;&#19978;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;&#22312;&#22823;&#22411;&#26102;&#38388;&#24207;&#21015;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#34917;&#19969;&#35299;&#30721;&#22120;&#24335;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#39044;&#27979;&#21382;&#21490;&#38271;&#24230;&#12289;&#39044;&#27979;&#38271;&#24230;&#21644;&#26102;&#38388;&#31890;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by recent advances in large language models for Natural Language Processing (NLP), we design a time-series foundation model for forecasting whose out-of-the-box zero-shot performance on a variety of public datasets comes close to the accuracy of state-of-the-art supervised forecasting models for each individual dataset. Our model is based on pretraining a patched-decoder style attention model on a large time-series corpus, and can work well across different forecasting history lengths, prediction lengths and temporal granularities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.08475</link><description>&lt;p&gt;
&#25105;&#20204;&#33021;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#32534;&#36753;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#20294;&#36825;&#39033;&#24037;&#20316;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#32534;&#36753;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#19982;&#32534;&#36753;&#21333;&#27169;&#24335;LLMs&#30456;&#27604;&#65292;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#32534;&#36753;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#26356;&#39640;&#32423;&#21035;&#30340;&#23457;&#26597;&#21644;&#24910;&#37325;&#32771;&#34385;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#31216;&#20026;MMEdit&#65292;&#29992;&#20110;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21253;&#25324;&#21508;&#31181;&#27169;&#22411;&#32534;&#36753;&#22522;&#32447;&#30340;&#32508;&#21512;&#23454;&#39564;&#65292;&#24182;&#20998;&#26512;&#20102;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#30340;&#19981;&#21516;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#26681;&#25454;&#32463;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20043;&#21069;&#30340;&#22522;&#32447;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#21487;&#20197;&#23454;&#29616;&#32534;&#36753;&#22810;&#27169;&#24335;LLMs&#65292;&#20294;&#25928;&#26524;&#20173;&#28982;&#19981;&#29702;&#24819;&#65292;&#34920;&#26126;&#36825;&#20010;&#20219;&#21153;&#21487;&#33021;&#23384;&#22312;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#35265;&#35299;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/zjunlp/EasyEdit&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we focus on editing Multimodal Large Language Models (MLLMs). Compared to editing single-modal LLMs, multimodal model editing is more challenging, which demands a higher level of scrutiny and careful consideration in the editing process. To facilitate research in this area, we construct a new benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite of innovative metrics for evaluation. We conduct comprehensive experiments involving various model editing baselines and analyze the impact of editing different components for multimodal LLMs. Empirically, we notice that previous baselines can implement editing multimodal LLMs to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We hope that our work can provide the NLP community with insights. Code and dataset are available in https://github.com/zjunlp/EasyEdit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24076;&#26395;&#33021;&#22815;&#25512;&#21160;&#20420;&#35821;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2309.10931</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;
&lt;/p&gt;
&lt;p&gt;
A Family of Pretrained Transformer Language Models for Russian. (arXiv:2309.10931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10931
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#39044;&#35757;&#32451;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#32534;&#30721;&#22120;&#12289;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20102;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24076;&#26395;&#33021;&#22815;&#25512;&#21160;&#20420;&#35821;&#39046;&#22495;&#30340;NLP&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;Transformer&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#26041;&#27861;&#21644;&#24212;&#29992;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#19987;&#38376;&#38024;&#23545;&#20420;&#35821;&#30340;&#36825;&#31181;&#27169;&#22411;&#30340;&#21457;&#23637;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#32452;&#22522;&#20110;&#32534;&#30721;&#22120;&#65288;ruBERT, ruRoBERTa, ruELECTRA&#65289;&#12289;&#35299;&#30721;&#22120;&#65288;ruGPT-3&#65289;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65288;ruT5, FRED-T5&#65289;&#27169;&#22411;&#30340;13&#20010;&#20420;&#35821;Transformer LMs&#65292;&#20855;&#26377;&#22810;&#31181;&#23610;&#23544;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#36890;&#36807;HuggingFace&#24179;&#21488;&#36731;&#26494;&#33719;&#21462;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#21644;&#39044;&#35757;&#32451;&#30340;&#25253;&#21578;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20420;&#35821;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#21457;&#24067;&#36825;&#20123;&#19987;&#38376;&#30340;Transformer LMs&#65292;&#25105;&#20204;&#24076;&#26395;&#25299;&#23485;NLP&#30740;&#31350;&#26041;&#21521;&#30340;&#33539;&#22260;&#65292;&#24182;&#20419;&#36827;&#38024;&#23545;&#20420;&#35821;&#30340;&#24037;&#19994;&#35299;&#20915;&#26041;&#26696;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, Transformer language models (LMs) represent a fundamental component of the NLP research methodologies and applications. However, the development of such models specifically for the Russian language has received little attention. This paper presents a collection of 13 Russian Transformer LMs based on the encoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder (ruT5, FRED-T5) models in multiple sizes. Access to these models is readily available via the HuggingFace platform. We provide a report of the model architecture design and pretraining, and the results of evaluating their generalization abilities on Russian natural language understanding and generation datasets and benchmarks. By pretraining and releasing these specialized Transformer LMs, we hope to broaden the scope of the NLP research directions and enable the development of industrial solutions for the Russian language.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LibriSQA&#65292;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;ASR&#20219;&#21153;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;LLMs&#19978;&#25191;&#34892;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#30340;&#26174;&#33879;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10390</link><description>&lt;p&gt;
LibriSQA&#65306;&#36890;&#36807;&#26032;&#22411;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#25512;&#36827;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LibriSQA&#65292;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;ASR&#20219;&#21153;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;LLMs&#19978;&#25191;&#34892;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#30340;&#26174;&#33879;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21487;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#27169;&#24577;&#21151;&#33021;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38656;&#35201;&#35821;&#38899;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#23545;&#40784;&#21644;&#28145;&#24230;&#20132;&#20114;&#30340;&#21475;&#35821;&#38382;&#31572;&#65288;SQA&#65289;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;LLM&#19978;&#30340;SQA&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;Librispeech&#21019;&#36896;&#20102;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;LibriSQA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33258;&#28982;&#23545;&#35805;&#26684;&#24335;&#30340;&#31532;&#19968;&#37096;&#20998;&#21644;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#31572;&#26696;&#20197;&#21450;&#20998;&#26512;&#29255;&#27573;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;&#36825;&#20004;&#37096;&#20998;&#20849;&#21253;&#21547;107k&#20010;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#30340;SQA&#23545;&#12290;&#37492;&#20110;&#29616;&#26377;&#35821;&#38899;-&#25991;&#26412;LLM&#30340;&#26126;&#26174;&#21294;&#20047;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#22312;LibriSQA&#19978;&#25191;&#34892;SQA&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;ASR&#25913;&#20026;SQA&#26684;&#24335;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26694;&#26550;&#22788;&#29702;ASR&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated commendable performance across a myriad of domains and tasks, existing LLMs still exhibit a palpable deficit in handling multimodal functionalities, especially for the Spoken Question Answering (SQA) task which necessitates precise alignment and deep interaction between speech and text features. To address the SQA challenge on LLMs, we initially curated the free-form and open-ended LibriSQA dataset from Librispeech, comprising Part I with natural conversational formats and Part II encompassing multiple-choice questions followed by answers and analytical segments. Both parts collectively include 107k SQA pairs that cover various topics. Given the evident paucity of existing speech-text LLMs, we propose a lightweight, end-to-end framework to execute the SQA task on the LibriSQA, witnessing significant results. By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks. Our empirical f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#25991;&#26723;&#32423;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#36164;&#28304;&#25104;&#26412;&#21644;&#29615;&#22659;&#24847;&#35782;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#30340;&#37197;&#32622;&#19979;&#65292;&#20934;&#30830;&#24615;&#25439;&#22833;&#36739;&#23567;&#12290;&#36825;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#30340;&#27169;&#22411;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.02022</link><description>&lt;p&gt;
&#39640;&#25928;&#24773;&#24863;&#20998;&#26512;&#65306;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#38598;&#25104;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36164;&#28304;&#21487;&#34892;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Efficient Sentiment Analysis: A Resource-Aware Evaluation of Feature Extraction Techniques, Ensembling, and Deep Learning Models. (arXiv:2308.02022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#25991;&#26723;&#32423;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#37325;&#28857;&#32771;&#34385;&#20102;&#36164;&#28304;&#25104;&#26412;&#21644;&#29615;&#22659;&#24847;&#35782;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#36164;&#28304;&#28040;&#32791;&#36739;&#20302;&#30340;&#37197;&#32622;&#19979;&#65292;&#20934;&#30830;&#24615;&#25439;&#22833;&#36739;&#23567;&#12290;&#36825;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#30340;&#27169;&#22411;&#37096;&#32626;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#26368;&#22823;&#21270;&#20934;&#30830;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#26102;&#65292;&#24120;&#24120;&#24573;&#35270;&#20854;&#20182;&#37325;&#35201;&#30340;&#31995;&#32479;&#24615;&#33021;&#25351;&#26631;&#12290;&#20808;&#21069;&#30340;&#27169;&#22411;&#24456;&#23481;&#26131;&#34987;&#36951;&#24536;&#65292;&#23613;&#31649;&#23427;&#20204;&#21487;&#33021;&#22312;&#35745;&#31639;&#36164;&#28304;&#26377;&#38480;&#25110;&#30456;&#23545;&#26356;&#26114;&#36149;&#30340;&#35774;&#32622;&#20013;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#25991;&#26723;&#32423;&#24773;&#24863;&#20998;&#26512;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#20027;&#35201;&#20851;&#27880;&#23545;&#20110;&#27169;&#22411;&#37096;&#32626;&#21487;&#34892;&#24615;&#21644;&#29615;&#22659;&#24847;&#35782;&#30340;&#36164;&#28304;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#12289;&#38598;&#25104;&#25928;&#26524;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#28145;&#24230;&#23398;&#20064;&#24314;&#27169;&#20197;&#21450;&#39046;&#22495;&#26080;&#20851;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#26576;&#20123;&#26367;&#20195;&#37197;&#32622;&#22312;&#36164;&#28304;&#28040;&#32791;&#26041;&#38754;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#65288;&#26368;&#39640;&#36798;24,283*&#65289;&#33410;&#30465;&#65292;&#32780;&#20934;&#30830;&#24615;&#25439;&#22833;&#21482;&#26377;&#36739;&#23567;&#30340;(&lt;1%)&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#65292;&#20934;&#30830;&#24615;&#30340;&#24046;&#24322;&#20250;&#32553;&#23567;&#65292;&#32780;&#36164;&#28304;&#28040;&#32791;&#30340;&#24046;&#24322;&#20250;&#22686;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reaching for NLP systems that maximize accuracy, other important metrics of system performance are often overlooked. Prior models are easily forgotten despite their possible suitability in settings where large computing resources are unavailable or relatively more costly. In this paper, we perform a broad comparative evaluation of document-level sentiment analysis models with a focus on resource costs that are important for the feasibility of model deployment and general climate consciousness. Our experiments consider different feature extraction techniques, the effect of ensembling, task-specific deep learning modeling, and domain-independent large language models (LLMs). We find that while a fine-tuned LLM achieves the best accuracy, some alternate configurations provide huge (up to 24, 283 *) resource savings for a marginal (&lt;1%) loss in accuracy. Furthermore, we find that for smaller datasets, the differences in accuracy shrink while the difference in resource consumption gro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32039;&#20945;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;Vesper&#65292;&#36890;&#36807;&#20248;&#21270;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#32771;&#34385;&#24773;&#24863;&#29305;&#24449;&#24182;&#37319;&#29992;&#24773;&#24863;&#24341;&#23548;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#22686;&#24378;&#23545;&#24773;&#24863;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10757</link><description>&lt;p&gt;
Vesper&#65306;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32039;&#20945;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Vesper: A Compact and Effective Pretrained Model for Speech Emotion Recognition. (arXiv:2307.10757v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#32039;&#20945;&#39640;&#25928;&#39044;&#35757;&#32451;&#27169;&#22411;Vesper&#65292;&#36890;&#36807;&#20248;&#21270;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#32771;&#34385;&#24773;&#24863;&#29305;&#24449;&#24182;&#37319;&#29992;&#24773;&#24863;&#24341;&#23548;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#22686;&#24378;&#23545;&#24773;&#24863;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36890;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#36866;&#24212;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;PTMs&#20026;&#20154;&#24037;&#26234;&#33021;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#65292;&#20294;&#23427;&#20204;&#26159;&#20026;&#36890;&#29992;&#20219;&#21153;&#26500;&#24314;&#30340;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#20173;&#26377;&#25552;&#21319;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;PTMs&#20307;&#31215;&#36739;&#22823;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#21487;&#33021;&#38754;&#20020;&#25361;&#25112;&#12290;&#38024;&#23545;&#20197;&#19978;&#38480;&#21046;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21478;&#19968;&#20010;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#20248;&#21270;&#22823;&#35268;&#27169;PTMs&#20197;&#29983;&#25104;&#32039;&#20945;&#19988;&#39640;&#25928;&#30340;&#20219;&#21153;&#29305;&#23450;PTMs&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#24773;&#24863;&#29305;&#23450;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;Vesper&#12290;Vesper&#22312;&#22522;&#20110;WavLM&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32771;&#34385;&#20102;&#24773;&#24863;&#29305;&#24449;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#24773;&#24863;&#20449;&#24687;&#30340;&#25935;&#24863;&#24615;&#65292;Vesper&#37319;&#29992;&#24773;&#24863;&#24341;&#23548;&#30340;&#25513;&#34109;&#31574;&#30053;&#26469;&#35782;&#21035;&#38656;&#35201;&#25513;&#34109;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a paradigm that adapts general large-scale pretrained models (PTMs) to speech emotion recognition task. Although PTMs shed new light on artificial general intelligence, they are constructed with general tasks in mind, and thus, their efficacy for specific tasks can be further improved. Additionally, employing PTMs in practical applications can be challenging due to their considerable size. Above limitations spawn another research direction, namely, optimizing large-scale PTMs for specific tasks to generate task-specific PTMs that are both compact and effective. In this paper, we focus on the speech emotion recognition task and propose an improved emotion-specific pretrained encoder called Vesper. Vesper is pretrained on a speech dataset based on WavLM and takes into account emotional characteristics. To enhance sensitivity to emotional information, Vesper employs an emotion-guided masking strategy to identify the regions that need masking. Subsequently, Vesper emplo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21482;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#30340;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#24212;&#29992;&#20110;Yoruba&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2306.11371</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#22522;&#30784;&#23569;&#37327;&#31034;&#20363;&#35789;&#27719;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Visually grounded few-shot word learning in low-resource settings. (arXiv:2306.11371v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21482;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#30340;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#24212;&#29992;&#20110;Yoruba&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22810;&#27169;&#24577;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35270;&#35273;&#35821;&#38899;&#27169;&#22411;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#35789;-&#22270;&#20687;&#23454;&#20363;&#23545;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#21333;&#35789;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#33258;&#28982;&#35789;-&#22270;&#20687;&#23545;&#19978;&#24037;&#20316;&#65292;&#20294;&#20351;&#29992;&#26356;&#23569;&#30340;&#31034;&#20363;&#65292;&#24182;&#19988;&#21487;&#20197;&#24212;&#29992;&#20110;&#23454;&#38469;&#20302;&#36164;&#28304;&#35821;&#35328;Yoruba&#30340;&#22810;&#27169;&#24577;&#23569;&#37327;&#31034;&#20363;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a visually grounded speech model that learns new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this few-shot learning problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. Moreover, all previous studies were performed using English speech-image data. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots, and then illustrate how this approach can be applied for multimodal few-shot learning in a real low-resource language, Yoruba. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelledspeech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new
&lt;/p&gt;</description></item><item><title>Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.14497</link><description>&lt;p&gt;
&#33258;&#25105;&#31934;&#30952;&#65306;&#36890;&#36807;&#38382;&#39064;&#31934;&#21270;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement. (arXiv:2305.14497v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14497
&lt;/p&gt;
&lt;p&gt;
Self-Polish&#26159;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992; Chain-of-Thought&#65288;CoT&#65289;&#31561;&#25552;&#31034;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#25506;&#32034;&#20102;&#21512;&#29702;&#21270;&#21644;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#24573;&#30053;&#20102;&#20302;&#36136;&#37327;&#25512;&#29702;&#38382;&#39064;&#21487;&#33021;&#20250;&#26174;&#30528;&#24433;&#21709;&#25512;&#29702;&#24615;&#33021;&#30340;&#28508;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Self-Polish&#65288;SP&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20419;&#20351;&#27169;&#22411;&#36880;&#27493;&#23436;&#21892;&#32473;&#23450;&#30340;&#38382;&#39064;&#20197;&#20351;&#20854;&#26356;&#26131;&#29702;&#35299;&#21644;&#21487;&#35299;&#20915;&#65292;&#20197;&#20419;&#36827;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#25945;&#25480;&#27169;&#22411;&#28040;&#38500;&#26080;&#20851;&#20449;&#24687;&#65292;&#37325;&#26032;&#25490;&#21015;&#36923;&#36753;&#32467;&#26500;&#65292;&#24182;&#23558;&#23616;&#37096;&#26465;&#20214;&#24182;&#34892;&#32452;&#32455;&#25104;&#26032;&#30340;&#26465;&#20214;&#12290; SP&#19982;&#25152;&#26377;&#20854;&#20182;&#25552;&#31034;&#26041;&#27861;&#27491;&#20132;&#65292;&#26041;&#20415;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#38598;&#25104;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the pr
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.11527</link><description>&lt;p&gt;
InstructIE: &#19968;&#20221;&#22522;&#20110;&#25351;&#20196;&#30340;&#20013;&#25991;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
InstructIE: A Chinese Instruction-based Information Extraction Dataset. (arXiv:2305.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11527
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20221;&#20013;&#25991;&#30340;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;270,000&#20010;&#24369;&#30417;&#30563;&#30340;&#25968;&#25454;&#21644;1,000&#20010;&#39640;&#36136;&#37327;&#27880;&#37322;&#23454;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#26377;&#24453;&#25913;&#36827;&#65292;&#35813;&#20219;&#21153;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#65292;&#31216;&#20026;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462; (Instruction-based IE)&#65292;&#23427;&#26088;&#22312;&#35201;&#27714;&#31995;&#32479;&#36981;&#24490;&#29305;&#23450;&#30340;&#25351;&#20196;&#25110;&#25351;&#21335;&#26469;&#25552;&#21462;&#20449;&#24687;&#12290;&#20026;&#20102;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;InstructIE&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#20013;&#25991;&#32500;&#22522;&#30334;&#31185;&#30340; 270,000 &#20010;&#24369;&#30417;&#30563;&#25968;&#25454;&#21644; 1,000 &#20010;&#39640;&#36136;&#37327;&#20247;&#21253;&#27880;&#37322;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#22312;InstructIE&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34920;&#29616;&#24456;&#26377;&#24076;&#26395;&#65292;&#20294;&#20173;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#26696;&#20363;&#30740;&#31350;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#22522;&#20110;&#25351;&#20196;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#22266;&#26377;&#30340;&#25361;&#25112;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#22312; https://github.com/zjunlp/DeepKE/tree/main/example/llm &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new Information Extraction (IE) task dubbed Instruction-based IE, which aims to ask the system to follow specific instructions or guidelines to extract information. To facilitate research in this area, we construct a dataset called InstructIE, consisting of 270,000 weakly supervised data from Chinese Wikipedia and 1,000 high-quality crowdsourced annotated instances. We further evaluate the performance of various baseline models on the InstructIE dataset. The results reveal that although current models exhibit promising performance, there is still room for improvement. Furthermore, we conduct a comprehensive case study analysis, underlining the challenges inherent in the Instruction-based IE task. Code and dataset are available at https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;</description></item></channel></rss>