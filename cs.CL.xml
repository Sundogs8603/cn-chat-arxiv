<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22810;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#65292;&#26089;&#26399;&#34701;&#21512;&#31574;&#30053;&#21644;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#21644;&#31934;&#30830;&#22320;&#25191;&#34892;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#12290;</title><link>http://arxiv.org/abs/2304.15010</link><description>&lt;p&gt;
LLaMA-Adapter V2: &#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model. (arXiv:2304.15010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.15010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#65292;&#36890;&#36807;&#35299;&#38145;&#26356;&#22810;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#65292;&#26089;&#26399;&#34701;&#21512;&#31574;&#30053;&#21644;&#32852;&#21512;&#35757;&#32451;&#31574;&#30053;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#21644;&#31934;&#30830;&#22320;&#25191;&#34892;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#26041;&#21521;&#26159;&#22914;&#20309;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39640;&#25928;&#22320;&#36716;&#21270;&#20026;&#25351;&#20196;&#36319;&#38543;&#32773;&#65292;&#32780;&#20026;&#22810;&#27169;&#24577;&#25512;&#29702;&#35757;&#32451;LLM&#30340;&#30740;&#31350;&#20173;&#28982;&#36739;&#23569;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;LLaMA-Adapter&#35777;&#26126;&#20102;&#29992;LLM&#22788;&#29702;&#35270;&#35273;&#36755;&#20837;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#19981;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#24320;&#25918;&#24335;&#35270;&#35273;&#25351;&#20196;&#65292;&#24182;&#19988;&#33853;&#21518;&#20110;GPT-4&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Adapter V2&#65292;&#36825;&#26159;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#35270;&#35273;&#25351;&#20196;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to efficiently transform large language models (LLMs) into instruction followers is recently a popular research direction, while training LLM for multi-modal reasoning remains less explored. Although the recent LLaMA-Adapter demonstrates the potential to handle visual inputs with LLMs, it still cannot generalize well to open-ended visual instructions and lags behind GPT-4. In this paper, we present LLaMA-Adapter V2, a parameter-efficient visual instruction model. Specifically, we first augment LLaMA-Adapter by unlocking more learnable parameters (e.g., norm, bias and scale), which distribute the instruction-following ability across the entire LLaMA model besides adapters. Secondly, we propose an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation. Thirdly, a joint training paradigm of image-text pairs and instruction-following data is introduced by optimizing disjoint groups of learnable parameters. This 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#35777;&#20998;&#26512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25552;&#20379;&#20102;&#36873;&#25321;&#20248;&#21270;&#24494;&#35843;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;PEFT&#25216;&#26415;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#20107;&#23454;&#12290;</title><link>http://arxiv.org/abs/2304.14999</link><description>&lt;p&gt;
LLM&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#30340;&#23454;&#35777;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Empirical Analysis of the Strengths and Weaknesses of PEFT Techniques for LLMs. (arXiv:2304.14999v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#35777;&#20998;&#26512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#65292;&#25552;&#20379;&#20102;&#36873;&#25321;&#20248;&#21270;&#24494;&#35843;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;PEFT&#25216;&#26415;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#35268;&#27169;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#39640;&#25928;&#30340;&#36866;&#24212;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#12290;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#30446;&#21069;&#26368;&#27969;&#34892;&#30340;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#21482;&#38656;&#35201;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#20855;&#26377;&#19981;&#21516;&#26435;&#34913;&#30340;PEFT&#25216;&#26415;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;LLM FLAN-T5&#27169;&#22411;&#19978;&#25552;&#20379;&#21508;&#31181;PEFT&#25216;&#26415;&#30340;&#20840;&#38754;&#21644;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35780;&#20272;&#22312;&#20998;&#31867;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#19981;&#21516;&#25968;&#25454;&#35268;&#27169;&#19979;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26681;&#25454;&#20219;&#21153;&#31867;&#22411;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#36873;&#25321;&#26368;&#20339;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#19982;&#20256;&#32479;&#35266;&#24565;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#35777;&#26126;PEFT&#25216;&#26415;&#22312;&#20302;&#25968;&#25454;&#22330;&#26223;&#19979;&#25910;&#25947;&#36895;&#24230;&#27604;&#23436;&#20840;&#24494;&#35843;&#24930;&#65292;&#24182;&#25552;&#20986;&#20102;PEFT&#26041;&#27861;&#38656;&#35201;&#34920;&#29616;&#33391;&#22909;&#21644;&#26377;&#25928;&#25910;&#25947;&#25152;&#38656;&#35201;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
As foundation models continue to exponentially scale in size, efficient methods of adaptation become increasingly critical. Parameter-efficient fine-tuning (PEFT), a recent class of techniques that require only modifying a small percentage of the model parameters, is currently the most popular method for adapting large language models (LLMs). Several PEFT techniques have recently been proposed with varying tradeoffs. We provide a comprehensive and uniform benchmark of various PEFT techniques across a representative LLM, the FLAN-T5 model, and evaluate model performance across different data scales of classification and generation datasets. Based on this, we provide a framework for choosing the optimal fine-tuning techniques given the task type and data availability. Contrary to popular belief, we also empirically prove that PEFT techniques converge slower than full tuning in low data scenarios, and posit the amount of data required for PEFT methods to both perform well and converge eff
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;</title><link>http://arxiv.org/abs/2304.14986</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#20041;&#35270;&#35273;&#20808;&#39564;&#35299;&#37322;&#35270;&#35273;&#21644;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Interpreting Vision and Language Generative Models with Semantic Visual Priors. (arXiv:2304.14986v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SHAP&#26694;&#26550;&#21644;&#35270;&#35273;&#20808;&#39564;&#30693;&#35782;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12289;&#26356;&#39640;&#30340;&#35299;&#37322;&#34920;&#29616;&#21147;&#65292;&#24182;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#26102;&#65292;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#36880;&#20010;&#26631;&#35760;&#30340;&#35299;&#37322;&#65292;&#21363;&#20026;&#25152;&#29983;&#25104;&#30340;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#35745;&#31639;&#35270;&#35273;&#35299;&#37322;&#12290;&#36825;&#20123;&#35299;&#37322;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#26080;&#27861;&#20840;&#38754;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#26576;&#31181;&#36817;&#20284;&#26041;&#27861;&#65292;&#26368;&#32456;&#20250;&#23548;&#33268;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;SHAP&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#21033;&#29992;&#36755;&#20986;&#24207;&#21015;&#30340;&#21547;&#20041;&#34920;&#31034;&#29983;&#25104;&#20840;&#38754;&#12289;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#20013;&#30340;&#35821;&#20041;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#21462;&#20102;&#20219;&#24847;&#25968;&#37327;&#30340;&#29305;&#24449;&#65292;&#24182;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#39640;&#25928;&#35745;&#31639;Shapley&#20540;&#65292;&#21516;&#26102;&#29983;&#25104;&#39640;&#24230;&#26126;&#30830;&#30340;&#35270;&#35273;&#35299;&#37322;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#35821;&#20041;&#19978;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
When applied to Image-to-text models, interpretability methods often provide token-by-token explanations namely, they compute a visual explanation for each token of the generated sequence. Those explanations are expensive to compute and unable to comprehensively explain the model's output. Therefore, these models often require some sort of approximation that eventually leads to misleading explanations. We develop a framework based on SHAP, that allows for generating comprehensive, meaningful explanations leveraging the meaning representation of the output sequence as a whole. Moreover, by exploiting semantic priors in the visual backbone, we extract an arbitrary number of features that allows the efficient computation of Shapley values on large-scale models, generating at the same time highly meaningful visual explanations. We demonstrate that our method generates semantically more expressive explanations than traditional methods at a lower compute cost and that it can be generalized o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;Common Crawl&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;PDF&#25991;&#20214;&#65292;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;CCpdf&#35821;&#26009;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2304.14953</link><description>&lt;p&gt;
CCpdf&#65306;&#20174;&#32593;&#32476;&#29228;&#34411;&#25968;&#25454;&#20013;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CCpdf: Building a High Quality Corpus for Visually Rich Documents from Web Crawl Data. (arXiv:2304.14953v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;Common Crawl&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;PDF&#25991;&#20214;&#65292;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20998;&#20139;&#20102;&#19968;&#20010;CCpdf&#35821;&#26009;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25991;&#26723;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#36827;&#23637;&#30340;&#19968;&#37096;&#20998;&#24471;&#30410;&#20110;&#20351;&#29992;&#39044;&#35757;&#32451;&#20110;&#22823;&#37327;&#25991;&#26723;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25991;&#26723;&#29702;&#35299;&#39046;&#22495;&#20013;&#20351;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#36890;&#24120;&#21333;&#19968;&#39046;&#22495;&#12289;&#21333;&#35821;&#35328;&#12289;&#25110;&#19981;&#20844;&#24320;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#27969;&#31243;&#65292;&#36890;&#36807;&#20351;&#29992;Common Crawl&#65292;&#20174;&#20114;&#32852;&#32593;&#19978;&#25910;&#38598;PDF&#25991;&#20214;&#65292;&#26500;&#24314;&#19968;&#20010;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#23545;&#26500;&#24314;&#27969;&#31243;&#30340;&#25152;&#26377;&#27493;&#39588;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#25968;&#25454;&#36136;&#37327;&#21644;&#22788;&#29702;&#26102;&#38388;&#20043;&#38388;&#24179;&#34913;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20010;CCpdf&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;PDF&#25991;&#20214;&#30340;&#32034;&#24341;&#21644;&#19979;&#36733;&#33050;&#26412;&#65292;&#21487;&#20197;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#12290;&#26412;&#25991;&#25152;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#21644;&#24037;&#20855;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#36827;&#34892;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#30740;&#31350;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of document understanding has progressed a lot. A significant part of this progress has been possible thanks to the use of language models pretrained on large amounts of documents. However, pretraining corpora used in the domain of document understanding are single domain, monolingual, or nonpublic. Our goal in this paper is to propose an efficient pipeline for creating a big-scale, diverse, multilingual corpus of PDF files from all over the Internet using Common Crawl, as PDF files are the most canonical types of documents as considered in document understanding. We analysed extensively all of the steps of the pipeline and proposed a solution which is a trade-off between data quality and processing time. We also share a CCpdf corpus in a form or an index of PDF files along with a script for downloading them, which produces a collection useful for language model pretraining. The dataset and tools published with this paper offer researchers the opportunity to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;</title><link>http://arxiv.org/abs/2304.14933</link><description>&lt;p&gt;
&#19968;&#39033;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study of Multimodal Model Merging. (arXiv:2304.14933v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34701;&#21512;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#22810;&#27169;&#24577;&#27169;&#22411;&#34701;&#21512;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#65292;&#24418;&#25104;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#34701;&#21512;&#65288;&#20363;&#22914;&#25554;&#20540;&#25110;&#20219;&#21153;&#31639;&#26415;&#65289;&#23558;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#27169;&#22411;&#21512;&#24182;&#20197;&#29983;&#25104;&#22810;&#20219;&#21153;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#25216;&#26415;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#25104;&#21151;&#65292;&#20854;&#20013;&#27169;&#22411;&#26159;&#22312;&#30456;&#20284;&#30340;&#20219;&#21153;&#21644;&#30456;&#21516;&#30340;&#21021;&#22987;&#21270;&#19979;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#22312;&#19981;&#21516;&#27169;&#24577;&#19978;&#35757;&#32451;&#30340;transformer&#36827;&#34892;&#34701;&#21512;&#65292;&#23558;&#27492;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#38024;&#23545;&#19968;&#20010;&#26032;&#39062;&#30340;&#30446;&#26631;&#36827;&#34892;&#30740;&#31350;&#65292;&#22312;&#35813;&#30446;&#26631;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#30340;transformer&#21512;&#24182;&#21040;&#29305;&#23450;&#27169;&#24577;&#30340;&#26550;&#26500;&#20013;&#65292;&#20197;&#21019;&#24314;&#19968;&#20010;&#21442;&#25968;&#26377;&#25928;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#26550;&#26500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#24433;&#21709;&#27169;&#22411;&#34701;&#21512;&#21518;&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#21021;&#22987;&#21270;&#12289;&#34701;&#21512;&#26426;&#21046;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#35757;&#32451;&#37197;&#26041;&#65292;&#21487;&#20197;&#36890;&#36807;&#27169;&#22411;&#34701;&#21512;&#26469;&#21305;&#37197;&#27169;&#24577;&#19981;&#21487;&#30693;&#22522;&#32447;&#30340;&#24615;&#33021;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#65289;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20379;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model merging (e.g., via interpolation or task arithmetic) fuses multiple models trained on different tasks to generate a multi-task solution. The technique has been proven successful in previous studies, where the models are trained on similar tasks and with the same initialization. In this paper, we expand on this concept to a multimodal setup by merging transformers trained on different modalities. Furthermore, we conduct our study for a novel goal where we can merge vision, language, and cross-modal transformers of a modality-specific architecture to create a parameter-efficient modality-agnostic architecture. Through comprehensive experiments, we systematically investigate the key factors impacting model performance after merging, including initialization, merging mechanisms, and model architectures. Our analysis leads to an effective training recipe for matching the performance of the modality-agnostic baseline (i.e. pre-trained from scratch) via model merging. Our code is availa
&lt;/p&gt;</description></item><item><title>HQP&#26159;&#19968;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#32593;&#32476;&#23459;&#20256;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#30340;&#24369;&#26631;&#31614;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#20351;&#29992;HQP&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;44%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14931</link><description>&lt;p&gt;
HQP&#65306;&#19968;&#20221;&#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HQP: A Human-Annotated Dataset for Detecting Online Propaganda. (arXiv:2304.14931v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14931
&lt;/p&gt;
&lt;p&gt;
HQP&#26159;&#19968;&#20010;&#20154;&#24037;&#26631;&#27880;&#30340;&#32593;&#32476;&#23459;&#20256;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#19982;&#29616;&#26377;&#30340;&#24369;&#26631;&#31614;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#20351;&#29992;HQP&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;44%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23459;&#20256;&#23545;&#31038;&#20250;&#30340;&#23436;&#25972;&#24615;&#26500;&#25104;&#20102;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#23427;&#20204;&#26159;&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#27880;&#37322;&#30340;&#65292;&#21487;&#33021;&#23384;&#22312;&#22122;&#38899;&#29978;&#33267;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#20570;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;1&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HQP&#65288;N=30,000&#65289;&#65292;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#65292;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#26631;&#27880;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#32780;&#21019;&#24314;&#30340;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#30340;&#25968;&#25454;&#38598;&#12290;&#65288;2&#65289;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22312;&#20351;&#29992;&#24369;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#26041;&#38754;&#22833;&#36133;&#65288;AUC&#65306;64.03&#65289;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24403;&#20351;&#29992;&#25105;&#20204;&#30340;&#39640;&#36136;&#37327;&#26631;&#31614;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20934;&#30830;&#22320;&#26816;&#27979;&#32593;&#32476;&#23459;&#20256;&#65288;AUC&#65306;92.25&#65289;&#65292;&#25552;&#39640;&#20102;&#32422;44%&#12290;&#65288;3&#65289;&#20026;&#20102;&#35299;&#20915;&#26631;&#27880;&#25104;&#26412;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#24037;&#20316;&#25193;&#23637;&#21040;&#20102;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#36827;&#34892;&#25552;&#31034;&#24335;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online propaganda poses a severe threat to the integrity of societies. However, existing datasets for detecting online propaganda have a key limitation: they were annotated using weak labels that can be noisy and even incorrect. To address this limitation, our work makes the following contributions: (1) We present \dataset: a novel dataset (N=30,000) for detecting online propaganda with high-quality labels. To the best of our knowledge, \dataset is the first dataset for detecting online propaganda that was created through human annotation. (2) We show empirically that state-of-the-art language models fail in detecting online propaganda when trained with weak labels (AUC: 64.03). In contrast, state-of-the-art language models can accurately detect online propaganda when trained with our high-quality labels (AUC: 92.25), which is an improvement of ~44%. (3) To address the cost of labeling, we extend our work to few-shot learning. Specifically, we show that prompt-based learning using a sm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20854;&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#26102;&#38388;&#39034;&#24207;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14827</link><description>&lt;p&gt;
ChatGPT&#22312;&#21477;&#23376;&#32423;&#20851;&#31995;&#19978;&#30340;&#35780;&#20272;&#65306;&#37325;&#28857;&#20851;&#27880;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Evaluation on Sentence Level Relations: A Focus on Temporal, Causal, and Discourse Relations. (arXiv:2304.14827v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;ChatGPT&#22312;&#21477;&#23376;&#32423;&#21035;&#30340;&#26102;&#38388;&#12289;&#22240;&#26524;&#21644;&#35821;&#31687;&#20851;&#31995;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#20854;&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35782;&#21035;&#26102;&#38388;&#39034;&#24207;&#26041;&#38754;&#21487;&#33021;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23450;&#37327;&#35780;&#20272;ChatGPT&#65292;&#22312;&#26102;&#38388;&#20851;&#31995;&#12289;&#22240;&#26524;&#20851;&#31995;&#21644;&#35821;&#31687;&#20851;&#31995;&#31561;&#21477;&#38388;&#20851;&#31995;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;ChatGPT&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#25105;&#20204;&#22312;13&#20010;&#25968;&#25454;&#38598;&#30340;&#25972;&#20010;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#22240;&#26524;&#20851;&#31995;&#12289;&#22522;&#20110;PDTB2.0&#21644;&#22522;&#20110;&#23545;&#35805;&#30340;&#35821;&#31687;&#20851;&#31995;&#65292;&#20197;&#21450;&#20851;&#20110;&#23545;&#35805;&#29702;&#35299;&#30340;&#19979;&#28216;&#24212;&#29992;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#30340;&#23450;&#21046;&#25552;&#31034;&#27169;&#26495;&#65292;&#21253;&#25324;&#38646;-shot&#25552;&#31034;&#27169;&#26495;&#12289;&#38646;-shot&#25552;&#31034;&#24037;&#31243;&#65288;PE&#65289;&#27169;&#26495;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#25552;&#31034;&#27169;&#26495;&#65292;&#20026;&#25152;&#26377;&#27969;&#34892;&#30340;&#21477;&#23545;&#20851;&#31995;&#20998;&#31867;&#20219;&#21153;&#24314;&#31435;&#20102;&#21021;&#22987;&#22522;&#20934;&#20998;&#25968;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#22312;&#26816;&#27979;&#21644;&#25512;&#29702;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21487;&#33021;&#19981;&#25797;&#38271;&#35782;&#21035;&#21477;&#23376;&#38388;&#30340;&#26102;&#38388;&#39034;&#24207;&#12290;ICL&#26041;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#25552;&#39640;&#19968;&#20123;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#27169;&#22411;&#25913;&#36827;&#21644;&#26377;&#25928;&#25552;&#31034;&#27169;&#26495;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20123;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to quantitatively evaluate the performance of ChatGPT, an interactive large language model, on inter-sentential relations such as temporal relations, causal relations, and discourse relations. Given ChatGPT's promising performance across various tasks, we conduct extensive evaluations on the whole test sets of 13 datasets, including temporal and causal relations, PDTB2.0-based and dialogue-based discourse relations, and downstream applications on discourse understanding. To achieve reliable results, we adopt three tailored prompt templates for each task, including the zero-shot prompt template, zero-shot prompt engineering (PE) template, and in-context learning (ICL) prompt template, to establish the initial baseline scores for all popular sentence-pair relation classification tasks for the first time. We find that ChatGPT exhibits strong performance in detecting and reasoning about causal relations, while it may not be proficient in identifying the temporal order betwe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;NLP&#25968;&#25454;&#38598;&#20013;&#35780;&#20998;&#32773;&#30340;&#19981;&#21516;&#24847;&#35265;&#26469;&#22521;&#20859;&#21644;&#35780;&#20272;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14803</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;11&#65306;&#23398;&#20064;&#19982;&#20998;&#27495;&#65288;LeWiDi&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 11: Learning With Disagreements (LeWiDi). (arXiv:2304.14803v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14803
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21033;&#29992;NLP&#25968;&#25454;&#38598;&#20013;&#35780;&#20998;&#32773;&#30340;&#19981;&#21516;&#24847;&#35265;&#26469;&#22521;&#20859;&#21644;&#35780;&#20272;NLP&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20419;&#36827;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21028;&#26029;&#27880;&#37322;&#25968;&#25454;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#25968;&#25454;&#38598;&#20043;&#38388;&#32463;&#24120;&#23384;&#22312;&#30528;&#35780;&#20998;&#32773;&#30340;&#19981;&#21516;&#24847;&#35265;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20381;&#36182;&#20027;&#35266;&#21028;&#26029;&#30340;&#20219;&#21153;&#65292;&#22914;&#24773;&#24863;&#20998;&#26512;&#25110;&#20882;&#29359;&#24615;&#35821;&#35328;&#26816;&#27979;&#12290;&#38024;&#23545;&#36825;&#20123;&#24773;&#20917;&#65292;NLP&#31038;&#21306;&#24050;&#32463;&#24847;&#35782;&#21040;"&#35843;&#21644;"&#36825;&#20123;&#19981;&#21516;&#30340;&#20027;&#35266;&#35299;&#37322;&#30340;&#26041;&#27861;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#35768;&#22810;NLP&#30740;&#31350;&#20154;&#21592;&#22240;&#27492;&#24471;&#20986;&#32467;&#35770;&#65292;&#23545;&#20110;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#24212;&#35813;&#20445;&#30041;&#23427;&#20204;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;LeWiDi&#31995;&#21015;&#20849;&#20139;&#20219;&#21153;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#36890;&#36807;&#25552;&#20379;&#32479;&#19968;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#26694;&#26550;&#26469;&#20419;&#36827;&#36825;&#31181;NLP&#27169;&#22411;&#24320;&#21457;&#26041;&#27861;&#12290;&#25105;&#20204;&#25253;&#21578;&#31532;&#20108;&#20010;LeWiDi&#20849;&#20139;&#20219;&#21153;&#65292;&#19982;&#31532;&#19968;&#20010;&#29256;&#26412;&#26377;&#19977;&#20010;&#20851;&#38190;&#30340;&#19981;&#21516;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
NLP datasets annotated with human judgments are rife with disagreements between the judges. This is especially true for tasks depending on subjective judgments such as sentiment analysis or offensive language detection. Particularly in these latter cases, the NLP community has come to realize that the approach of 'reconciling' these different subjective interpretations is inappropriate. Many NLP researchers have therefore concluded that rather than eliminating disagreements from annotated corpora, we should preserve them-indeed, some argue that corpora should aim to preserve all annotator judgments. But this approach to corpus creation for NLP has not yet been widely accepted. The objective of the LeWiDi series of shared tasks is to promote this approach to developing NLP models by providing a unified framework for training and evaluating with such datasets. We report on the second LeWiDi shared task, which differs from the first edition in three crucial respects: (i) it focuses entire
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Pre-Post-LN&#21452;&#37325;&#27531;&#24046;&#36830;&#25509;&#30340;&#26032;&#22411;Transformer&#26550;&#26500;ResiDual&#65292;&#35299;&#20915;&#20102;Post-LN&#21644;Pre-LN&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14802</link><description>&lt;p&gt;
ResiDual&#65306;&#20855;&#26377;&#21452;&#37325;&#27531;&#24046;&#36830;&#25509;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
ResiDual: Transformer with Dual Residual Connections. (arXiv:2304.14802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20855;&#26377;Pre-Post-LN&#21452;&#37325;&#27531;&#24046;&#36830;&#25509;&#30340;&#26032;&#22411;Transformer&#26550;&#26500;ResiDual&#65292;&#35299;&#20915;&#20102;Post-LN&#21644;Pre-LN&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;Transformer&#32593;&#32476;&#24050;&#25104;&#20026;&#35768;&#22810;&#20219;&#21153;&#30340;&#39318;&#36873;&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#20248;&#21270;&#22320;&#23454;&#29616;Transformer&#20013;&#30340;&#27531;&#24046;&#36830;&#25509;&#20173;&#23384;&#22312;&#20105;&#35758;&#65292;&#32780;&#36825;&#20123;&#27531;&#24046;&#36830;&#25509;&#23545;&#20110;&#26377;&#25928;&#35757;&#32451;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#21464;&#20307;&#26159;Post-Layer-Normalization(Post-LN)&#21644;Pre-Layer-Normalization(Pre-LN) Transformers&#65292;&#23427;&#20204;&#20998;&#21035;&#22312;&#27599;&#20010;&#27531;&#24046;&#22359;&#30340;&#36755;&#20986;&#20043;&#21518;&#25110;&#36755;&#20837;&#20043;&#21069;&#24212;&#29992;&#23618;&#35268;&#33539;&#21270;&#12290;&#23613;&#31649;&#20004;&#31181;&#21464;&#20307;&#37117;&#26377;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#20294;&#20063;&#23384;&#22312;&#20005;&#37325;&#30340;&#23616;&#38480;&#24615;&#65306;Post-LN&#20250;&#23548;&#33268;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#20174;&#32780;&#38459;&#30861;&#35757;&#32451;&#28145;&#23618;Transformer&#65292;&#32780;Pre-LN&#20250;&#23548;&#33268;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#38480;&#21046;&#27169;&#22411;&#23481;&#37327;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#26550;&#26500;ResiDual&#65292;&#20855;&#26377;Pre-Post-LN(PPLN)&#65292;&#23427;&#23558;Post-LN&#21644;Pre-LN&#20013;&#30340;&#36830;&#25509;&#34701;&#21512;&#22312;&#19968;&#36215;&#65292;&#32487;&#25215;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#34920;&#26126;ResiDual&#27604;&#29616;&#26377;&#26041;&#27861;&#20248;&#36234;&#65292;&#23588;&#20854;&#26159;&#35757;&#32451;&#38750;&#24120;&#28145;&#30340;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer networks have become the preferred architecture for many tasks due to their state-of-the-art performance. However, the optimal way to implement residual connections in Transformer, which are essential for effective training, is still debated. Two widely used variants are the Post-Layer-Normalization (Post-LN) and Pre-Layer-Normalization (Pre-LN) Transformers, which apply layer normalization after each residual block's output or before each residual block's input, respectively. While both variants enjoy their advantages, they also suffer from severe limitations: Post-LN causes gradient vanishing issue that hinders training deep Transformers, and Pre-LN causes representation collapse issue that limits model capacity. In this paper, we propose ResiDual, a novel Transformer architecture with Pre-Post-LN (PPLN), which fuses the connections in Post-LN and Pre-LN together and inherits their advantages while avoids their limitations. We conduct both theoretical analyses and empiric
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#27604;&#36739;&#20102;&#20174;&#21477;&#23376;&#32423;&#21035;&#23884;&#20837;&#20013;&#20135;&#29983;&#25991;&#26723;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;LASER&#12289;LaBSE&#21644;Sentence BERT&#12290;&#25105;&#20204;&#30528;&#37325;&#27604;&#36739;&#20102;&#36755;&#20837;&#20196;&#29260;&#25968;&#25130;&#26029;&#12289;&#21477;&#23376;&#24179;&#22343;&#20197;&#21450;&#19968;&#20123;&#31616;&#21333;&#30340;&#31383;&#21475;&#26041;&#27861;&#65292;&#23545;&#19977;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.14796</link><description>&lt;p&gt;
&#26368;&#22909;&#30340;&#22810;&#35821;&#35328;&#25991;&#26723;&#23884;&#20837;&#26159;&#21542;&#20165;&#22522;&#20110;&#21477;&#23376;&#23884;&#20837;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#27604;&#36739;&#20102;&#20174;&#21477;&#23376;&#32423;&#21035;&#23884;&#20837;&#20013;&#20135;&#29983;&#25991;&#26723;&#32423;&#23884;&#20837;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;LASER&#12289;LaBSE&#21644;Sentence BERT&#12290;&#25105;&#20204;&#30528;&#37325;&#27604;&#36739;&#20102;&#36755;&#20837;&#20196;&#29260;&#25968;&#25130;&#26029;&#12289;&#21477;&#23376;&#24179;&#22343;&#20197;&#21450;&#19968;&#20123;&#31616;&#21333;&#30340;&#31383;&#21475;&#26041;&#27861;&#65292;&#23545;&#19977;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#20219;&#21153;&#34920;&#29616;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#25991;&#26412;&#25968;&#25454;&#30340;&#23494;&#38598;&#21521;&#37327;&#34920;&#24449;&#33267;&#20851;&#37325;&#35201;&#12290;&#20174;&#21407;&#22987;&#25991;&#26412;&#20272;&#35745;&#30340;&#35789;&#23884;&#20837;&#21644;&#21477;&#23376;&#23884;&#20837;&#26159;&#22312;&#22810;&#31181;&#38656;&#35201;&#35821;&#20041;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#23454;&#29616;&#26368;&#26032;&#25104;&#26524;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#38656;&#27714;&#21644;&#32570;&#20047;&#36866;&#24403;&#30340;&#25968;&#25454;&#65292;&#33719;&#21462;&#25991;&#26723;&#32423;&#21035;&#30340;&#23884;&#20837;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30456;&#21453;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#36864;&#32780;&#20351;&#29992;&#22522;&#20110;&#21477;&#23376;&#34920;&#31034;&#30340;&#25991;&#26723;&#23884;&#20837;&#35745;&#31639;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#29992;&#20110;&#23436;&#20840;&#32534;&#30721;&#25991;&#26723;&#30340;&#20307;&#31995;&#32467;&#26500;&#21644;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#21644;&#20854;&#20182;&#20960;&#31181;&#39640;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;LASER&#12289;LaBSE&#21644;Sentence BERT&#65292;&#31995;&#32479;&#27604;&#36739;&#20174;&#21477;&#23376;&#20013;&#20135;&#29983;&#25991;&#26723;&#32423;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#27604;&#36739;&#36755;&#20837;&#20196;&#29260;&#25968;&#25130;&#26029;&#12289;&#21477;&#23376;&#24179;&#22343;&#20197;&#21450;&#19968;&#20123;&#31616;&#21333;&#30340;&#31383;&#21475;&#26041;&#27861;&#65292;&#22312;&#19977;&#20010;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense vector representations for textual data are crucial in modern NLP. Word embeddings and sentence embeddings estimated from raw texts are key in achieving state-of-the-art results in various tasks requiring semantic understanding. However, obtaining embeddings at the document level is challenging due to computational requirements and lack of appropriate data. Instead, most approaches fall back on computing document embeddings based on sentence representations. Although there exist architectures and models to encode documents fully, they are in general limited to English and few other high-resourced languages. In this work, we provide a systematic comparison of methods to produce document-level representations from sentences based on LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare input token number truncation, sentence averaging as well as some simple windowing and in some cases new augmented and learnable approaches, on 3 multiand cross-lingual tasks 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPT-SW3&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;SentencePiece&#24211;&#21644;BPE&#31639;&#27861;&#22312;Nordic Pile&#19978;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20854;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14780</link><description>&lt;p&gt;
GPT-SW3&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#30340;&#35757;&#32451;&#19982;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Training and Evaluation of a Multilingual Tokenizer for GPT-SW3. (arXiv:2304.14780v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPT-SW3&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#65292;&#36890;&#36807;&#20351;&#29992;SentencePiece&#24211;&#21644;BPE&#31639;&#27861;&#22312;Nordic Pile&#19978;&#35757;&#32451;&#65292;&#35780;&#20272;&#20102;&#20854;&#23545;&#20110;&#19981;&#21516;&#35821;&#35328;&#30340;&#34920;&#29616;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;GPT-SW3&#25152;&#20351;&#29992;&#30340;&#22810;&#35821;&#35328;&#20998;&#35789;&#22120;&#12290;&#35813;&#20998;&#35789;&#22120;&#20351;&#29992;SentencePiece&#24211;&#21644;BPE&#31639;&#27861;&#22312;Nordic Pile&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20998;&#35789;&#22120;&#30340;&#26368;&#37325;&#35201;&#29305;&#28857;&#65292;&#24182;&#20998;&#20139;&#20102;&#20854;&#23398;&#20064;&#35789;&#27719;&#30340;&#32454;&#33410;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#20998;&#35789;&#22120;&#22312;&#25968;&#25454;&#20013;&#19981;&#21516;&#35821;&#35328;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#24182;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provides a detailed discussion of the multilingual tokenizer used for GPT-SW3. It was trained on the Nordic Pile using the SentencePiece library and the BPE algorithm. We outline the tokenizer's most important features and share details on its learned vocabulary. In addition, we systematically analyze the properties and evaluate the performance of the tokenizer with regard to the different languages present in the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RexUIE&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#36882;&#24402;&#26426;&#21046;&#21644;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#65292;&#23454;&#29616;&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;(UIE)&#12290;&#19982;&#20197;&#24448;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;RexUIE&#22312;&#25552;&#21462;&#19981;&#21516;&#27169;&#24335;&#26102;&#19981;&#20250;&#20986;&#29616;&#20914;&#31361;&#65292;&#24182;&#19988;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#22120;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.14770</link><description>&lt;p&gt;
RexUIE: &#19968;&#31181;&#24102;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
RexUIE: A Recursive Method with Explicit Schema Instructor for Universal Information Extraction. (arXiv:2304.14770v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RexUIE&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#36882;&#24402;&#26426;&#21046;&#21644;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#65292;&#23454;&#29616;&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;(UIE)&#12290;&#19982;&#20197;&#24448;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;RexUIE&#22312;&#25552;&#21462;&#19981;&#21516;&#27169;&#24335;&#26102;&#19981;&#20250;&#20986;&#29616;&#20914;&#31361;&#65292;&#24182;&#19988;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#22120;&#26377;&#21161;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#20449;&#24687;&#25552;&#21462;(UIE)&#22240;&#20854;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#12289;&#24322;&#26500;&#32467;&#26500;&#21644;&#38656;&#27714;&#29305;&#23450;&#27169;&#24335;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#20165;&#36890;&#36807;&#32479;&#19968;&#19968;&#20123;&#20219;&#21153;(&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25552;&#21462;)&#21462;&#24471;&#20102;&#26377;&#38480;&#30340;&#25104;&#21151;&#65292;&#19981;&#33021;&#30495;&#27491;&#31216;&#20043;&#20026;UIE&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#24403;&#25552;&#21462;&#20854;&#20182;&#24120;&#35265;&#27169;&#24335;&#65288;&#22914;&#22235;&#20803;&#32452;&#21644;&#20116;&#20803;&#32452;&#65289;&#26102;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#37319;&#29992;&#20102;&#19968;&#20010;&#38544;&#24335;&#32467;&#26500;&#27169;&#24335;&#35828;&#26126;&#31526;&#65292;&#22312;&#31867;&#22411;&#20043;&#38388;&#24314;&#31435;&#38169;&#35823;&#30340;&#38142;&#25509;&#65292;&#20351;&#27169;&#22411;&#30340;&#27867;&#21270;&#21644;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#25991;&#37325;&#26032;&#23450;&#20041;&#20102;&#27491;&#24335;&#30340;UIE&#20844;&#24335;&#65292;&#28085;&#30422;&#20102;&#20960;&#20046;&#25152;&#26377;&#25552;&#21462;&#27169;&#24335;&#12290;&#25105;&#20204;&#26159;&#39318;&#27425;&#20026;&#20219;&#20309;&#31867;&#22411;&#30340;&#27169;&#24335;&#24341;&#20837;UIE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RexUIE&#65292;&#23427;&#26159;&#19968;&#31181;&#24102;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#30340;&#36882;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110; UIE&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#21516;&#27169;&#24335;&#20043;&#38388;&#30340;&#24178;&#25200;&#65292;RexUIE&#36890;&#36807;&#36882;&#24402;&#26426;&#21046;&#32479;&#19968;&#25552;&#21462;&#36807;&#31243;&#65292;&#36866;&#24212;&#22810;&#31181;&#25552;&#21462;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26174;&#24335;&#27169;&#24335;&#35828;&#26126;&#22120;&#65292;&#25351;&#23548;&#27169;&#22411;&#23398;&#20064;&#32473;&#23450;&#22330;&#26223;&#30340;&#32467;&#26500;&#27169;&#24335;&#65292;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#30340;&#27867;&#21270;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal Information Extraction (UIE) is an area of interest due to the challenges posed by varying targets, heterogeneous structures, and demand-specific schemas. However, previous works have only achieved limited success by unifying a few tasks, such as Named Entity Recognition (NER) and Relation Extraction (RE), which fall short of being authentic UIE models particularly when extracting other general schemas such as quadruples and quintuples. Additionally, these models used an implicit structural schema instructor, which could lead to incorrect links between types, hindering the model's generalization and performance in low-resource scenarios. In this paper, we redefine the authentic UIE with a formal formulation that encompasses almost all extraction schemas. To the best of our knowledge, we are the first to introduce UIE for any kind of schemas. In addition, we propose RexUIE, which is a Recursive Method with Explicit Schema Instructor for UIE. To avoid interference between diffe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.14745</link><description>&lt;p&gt;
&#30001;&#20160;&#20040;&#26500;&#25104;&#65311;&#23398;&#20064;&#20462;&#36710;&#39046;&#22495;&#32452;&#20214;&#30340;&#21487;&#20449;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Made of Steel? Learning Plausible Materials for Components in the Vehicle Repair Domain. (arXiv:2304.14745v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#25104;&#21151;&#20811;&#26381;&#20102;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#21644;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#20013;&#30340;cloze&#20219;&#21153;&#26679;&#24335;&#35774;&#32622;&#26469;&#23398;&#20064;&#36710;&#36742;&#32500;&#20462;&#39046;&#22495;&#32452;&#20214;&#30340;&#29305;&#23450;&#26448;&#26009;&#65292;&#20197;&#20811;&#26381;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32858;&#21512;&#20102;&#19968;&#32452;cloze&#26597;&#35810;&#27169;&#26495;&#30340;&#26174;&#33879;&#39044;&#27979;&#65292;&#24182;&#34920;&#26126;&#20351;&#29992;&#23567;&#22411;&#39640;&#36136;&#37327;&#25110;&#23450;&#21046;&#30340;&#32500;&#22522;&#30334;&#31185;&#35821;&#26009;&#24211;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#24403;&#25506;&#32034;&#36164;&#28304;&#32039;&#32570;&#30340;&#26367;&#20195;&#26041;&#26696;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#31934;&#31616;&#30340;PLM&#26126;&#26174;&#20248;&#20110;&#32463;&#20856;&#30340;&#22522;&#20110;&#27169;&#24335;&#30340;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25105;&#20204;&#39046;&#22495;&#29305;&#23450;&#32452;&#20214;&#30340;98&#65285;&#37117;&#26159;&#22810;&#35789;&#34920;&#36798;&#24335;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#21033;&#29992;&#32452;&#25104;&#24615;&#20551;&#35774;&#26469;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel approach to learn domain-specific plausible materials for components in the vehicle repair domain by probing Pretrained Language Models (PLMs) in a cloze task style setting to overcome the lack of annotated datasets. We devise a new method to aggregate salient predictions from a set of cloze query templates and show that domain-adaptation using either a small, high-quality or a customized Wikipedia corpus boosts performance. When exploring resource-lean alternatives, we find a distilled PLM clearly outperforming a classic pattern-based algorithm. Further, given that 98% of our domain-specific components are multiword expressions, we successfully exploit the compositionality assumption as a way to address data sparsity.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;&#65288;CSST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#65292;&#20026;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14738</link><description>&lt;p&gt;
&#38754;&#21521;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#30340;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Self-Training for Optimizing Non-Decomposable Metrics. (arXiv:2304.14738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;&#65288;CSST&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#65292;&#20026;&#22788;&#29702;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#20351;&#24471;&#20165;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23601;&#33021;&#23398;&#20064;&#21040;&#39640;&#31934;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#33258;&#35757;&#32451;&#30340;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#31934;&#24230;&#30340;&#30446;&#26631;&#19978;&#65292;&#32780;&#23454;&#38469;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#21487;&#33021;&#20855;&#26377;&#19981;&#21487;&#20998;&#35299;&#30340;&#22797;&#26434;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#26368;&#22823;&#21270;&#19981;&#21516;&#31867;&#21035;&#21484;&#22238;&#29575;&#30340;&#26368;&#23567;&#20540;&#31561;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20195;&#20215;&#25935;&#24863;&#33258;&#35757;&#32451;&#65288;CSST&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25512;&#24191;&#20102;&#29992;&#20110;&#20248;&#21270;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#30340;&#22522;&#20110;&#33258;&#35757;&#32451;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20248;&#21270;&#25152;&#38656;&#30340;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#65292;&#20551;&#35774;&#25968;&#25454;&#20998;&#24067;&#19982;&#33258;&#25105;&#35757;&#32451;&#30340;&#20998;&#26512;&#25152;&#20570;&#30340;&#19968;&#26679;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;CSST&#26694;&#26550;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#20102;&#38024;&#23545;&#19981;&#21516;&#19981;&#21487;&#20998;&#35299;&#25351;&#26631;&#30340;&#23454;&#38469;&#33258;&#35757;&#32451;&#26041;&#27861;&#65288;&#29992;&#20110;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;CSST&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training based semi-supervised learning algorithms have enabled the learning of highly accurate deep neural networks, using only a fraction of labeled data. However, the majority of work on self-training has focused on the objective of improving accuracy, whereas practical machine learning systems can have complex goals (e.g. maximizing the minimum of recall across classes, etc.) that are non-decomposable in nature. In this work, we introduce the Cost-Sensitive Self-Training (CSST) framework which generalizes the self-training-based methods for optimizing non-decomposable metrics. We prove that our framework can better optimize the desired non-decomposable metric utilizing unlabeled data, under similar data distribution assumptions made for the analysis of self-training. Using the proposed CSST framework, we obtain practical self-training methods (for both vision and NLP tasks) for optimizing different non-decomposable metrics using deep neural networks. Our results demonstrate th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14732</link><description>&lt;p&gt;
&#22522;&#20110;SearChain&#30340;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#31934;&#30830;&#12289;&#21487;&#20449;&#21644;&#21487;&#36861;&#28335;&#20869;&#23481;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#20351;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#20934;&#30830;&#21487;&#20449;&#22312;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-in-the-Chain&#65288;SearChain&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#31561;&#20856;&#22411;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;SearChain&#26159;&#19968;&#20010;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26694;&#26550;&#12290;&#22312;SearChain&#20013;&#65292;LLM&#26500;&#24314;&#26597;&#35810;&#38142;&#65292;&#20316;&#20026;&#22810;&#36339;&#38382;&#39064;&#30340;&#20998;&#35299;&#12290;&#38142;&#30340;&#27599;&#20010;&#33410;&#28857;&#37117;&#26159;&#30001;IR&#23548;&#21521;&#30340;&#26597;&#35810;-&#31572;&#26696;&#23545;&#65292;&#20197;&#21450;&#30001;LLM&#29983;&#25104;&#30340;&#35813;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;IR&#39564;&#35777;&#12289;&#23436;&#21892;&#21644;&#36319;&#36394;&#38142;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;LLM&#26500;&#24314;&#27491;&#30830;&#30340;&#26597;&#35810;&#38142;&#65292;&#24182;&#26368;&#32456;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#12290;SearChain&#20351;LLM&#20174;&#19968;&#27425;&#24615;&#31572;&#26696;&#36716;&#21464;&#20026;&#22810;&#27493;&#31572;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SearChain&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.14721</link><description>&lt;p&gt;
&#26397;&#33258;&#20027;&#31995;&#32479;&#36808;&#36827;&#65306;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22686;&#24378;&#30340;&#28789;&#27963;&#27169;&#22359;&#21270;&#29983;&#20135;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards autonomous system: flexible modular production system enhanced with large language model agents. (arXiv:2304.14721v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#21270;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#36890;&#36807;LLM&#20195;&#29702;&#30340;&#21327;&#35843;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#33021;&#22815;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#24182;&#35268;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#25968;&#23383;&#23402;&#29983;&#21644;&#24037;&#19994;&#33258;&#21160;&#21270;&#31995;&#32479;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#29983;&#20135;&#36807;&#31243;&#30340;&#26234;&#33021;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#24320;&#21457;&#21253;&#21547;&#29983;&#20135;&#25551;&#36848;&#20449;&#24687;&#30340;&#25968;&#23383;&#23402;&#29983;&#31995;&#32479;&#65292;&#24182;&#23558;&#33258;&#21160;&#21270;&#31995;&#32479;&#25913;&#36896;&#20026;&#25552;&#20379;&#32479;&#19968;&#25509;&#21475;&#30340;&#32454;&#31890;&#24230;&#21151;&#33021;&#25110;&#27169;&#22359;&#65292;&#20197;&#20379;&#33258;&#21160;&#21270;&#32452;&#20214;&#25110;&#27169;&#22359;&#25191;&#34892;&#12290;&#38543;&#21518;&#65292;&#35774;&#35745;LLM&#20195;&#29702;&#26469;&#35299;&#37322;&#25968;&#23383;&#23402;&#29983;&#20013;&#30340;&#25551;&#36848;&#24615;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;RESTful&#25509;&#21475;&#25511;&#21046;&#29289;&#29702;&#31995;&#32479;&#12290;&#36825;&#20123;LLM&#20195;&#29702;&#20316;&#20026;&#33258;&#21160;&#21270;&#31995;&#32479;&#20869;&#30340;&#26234;&#33021;&#20195;&#29702;&#65292;&#23454;&#29616;&#20102;&#28789;&#27963;&#29983;&#20135;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25511;&#21046;&#12290;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25351;&#20196;&#20316;&#20026;&#36755;&#20837;&#65292;LLM&#20195;&#29702;&#21327;&#35843;&#19968;&#31995;&#21015;&#21407;&#23376;&#21151;&#33021;&#21644;&#25216;&#33021;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#23454;&#29616;&#30340;&#21407;&#22411;&#22914;&#20309;&#22788;&#29702;&#26410;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#65292;&#24182;&#35745;&#21010;&#29983;&#20135;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel framework that combines large language models (LLMs), digital twins and industrial automation system to enable intelligent planning and control of production processes. Our approach involves developing a digital twin system that contains descriptive information about the production and retrofitting the automation system to offer unified interfaces of fine-granular functionalities or skills executable by automation components or modules. Subsequently, LLM-Agents are designed to interpret descriptive information in the digital twins and control the physical system through RESTful interfaces. These LLM-Agents serve as intelligent agents within an automation system, enabling autonomous planning and control of flexible production. Given a task instruction as input, the LLM-agents orchestrate a sequence of atomic functionalities and skills to accomplish the task. We demonstrate how our implemented prototype can handle un-predefined tasks, plan a production p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#30446;&#24405;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#65288;CED&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#27880;&#37322;&#35821;&#26009;&#24211;&#24182;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14662</link><description>&lt;p&gt;
CED: &#25991;&#26723;&#20013;&#30340;&#30446;&#24405;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
CED: Catalog Extraction from Documents. (arXiv:2304.14662v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14662
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#30446;&#24405;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#65288;CED&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#27880;&#37322;&#35821;&#26009;&#24211;&#24182;&#25552;&#20986;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26723;&#36880;&#21477;&#20449;&#24687;&#25552;&#21462;&#26159;&#19968;&#39033;&#32321;&#29712;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#25991;&#26723;&#32467;&#26500;&#30340;&#25351;&#31034;&#22120;&#65292;&#30446;&#24405;&#33258;&#28982;&#22320;&#23558;&#25991;&#26723;&#20998;&#25104;&#27573;&#33853;&#24182;&#25552;&#20379;&#26377;&#30410;&#30340;&#32423;&#32852;&#35821;&#20041;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#12290;&#23613;&#31649;&#30446;&#24405;&#24456;&#26377;&#29992;&#65292;&#20294;&#22312;&#27809;&#26377;&#22806;&#37096;&#30693;&#35782;&#30340;&#24110;&#21161;&#19979;&#65292;&#24456;&#38590;&#25552;&#21462;&#30446;&#24405;&#12290;&#23545;&#20110;&#36981;&#24490;&#29305;&#23450;&#27169;&#26495;&#30340;&#25991;&#26723;&#65292;&#27491;&#21017;&#34920;&#36798;&#24335;&#21487;&#29992;&#20110;&#25552;&#21462;&#30446;&#24405;&#12290;&#28982;&#32780;&#65292;&#22312;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#26469;&#28304;&#21644;&#19981;&#21516;&#26684;&#24335;&#30340;&#25991;&#26723;&#26102;&#65292;&#25163;&#24037;&#21551;&#21457;&#24335;&#35268;&#21017;&#19981;&#36866;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25163;&#21160;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#65292;&#36825;&#26159;&#30446;&#24405;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#65288;CED&#65289;&#20219;&#21153;&#30340;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#27492;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36716;&#25442;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#25991;&#26723;&#35299;&#26512;&#25104;&#30446;&#24405;&#26641;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#31995;&#32479;&#65292;&#24182;&#20855;&#26377;&#33391;&#22909;&#30340;&#36801;&#31227;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;
&lt;/p&gt;
&lt;p&gt;
Sentence-by-sentence information extraction from long documents is an exhausting and error-prone task. As the indicator of document skeleton, catalogs naturally chunk documents into segments and provide informative cascade semantics, which can help to reduce the search space. Despite their usefulness, catalogs are hard to be extracted without the assist from external knowledge. For documents that adhere to a specific template, regular expressions are practical to extract catalogs. However, handcrafted heuristics are not applicable when processing documents from different sources with diverse formats. To address this problem, we build a large manually annotated corpus, which is the first dataset for the Catalog Extraction from Documents (CED) task. Based on this corpus, we propose a transition-based framework for parsing documents into catalog trees. The experimental results demonstrate that our proposed method outperforms baseline systems and shows a good ability to transfer. We believ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#29369;&#22826;&#20027;&#20041;&#35328;&#35770;&#30340;&#25512;&#25991;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20005;&#26684;&#30340;&#23450;&#20041;&#26631;&#27880;&#65292;&#26377;&#21161;&#20110;&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#20943;&#23569;&#35823;&#21028;&#12290;</title><link>http://arxiv.org/abs/2304.14599</link><description>&lt;p&gt;
&#21453;&#29369;&#22826;&#20027;&#20041;&#35328;&#35770;&#65311;&#39640;&#36136;&#37327;&#26631;&#27880;&#25351;&#21335;&#21644;&#25512;&#25991;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Antisemitic Messages? A Guide to High-Quality Annotation and a Labeled Dataset of Tweets. (arXiv:2304.14599v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#21453;&#29369;&#22826;&#20027;&#20041;&#35328;&#35770;&#30340;&#25512;&#25991;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20005;&#26684;&#30340;&#23450;&#20041;&#26631;&#27880;&#65292;&#26377;&#21161;&#20110;&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#65292;&#20943;&#23569;&#35823;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#32570;&#20047;&#28085;&#30422;&#33539;&#22260;&#24191;&#27867;&#30340;&#26377;&#20559;&#35265;&#21644;&#26080;&#20559;&#35265;&#35328;&#35770;&#19988;&#26631;&#27880;&#19968;&#33268;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#27880;&#31243;&#24207;&#65292;&#35299;&#20915;&#20102;&#24050;&#26631;&#27880;&#25968;&#25454;&#38598;&#20013;&#24120;&#35265;&#30340;&#19968;&#20123;&#24369;&#28857;&#12290;&#26412;&#25991;&#20851;&#27880;Twitter&#19978;&#30340;&#21453;&#29369;&#22826;&#20027;&#20041;&#35328;&#35770;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#30456;&#20851;&#20851;&#38190;&#35789;&#20174;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#26679;&#26412;&#20013;&#25552;&#21462;&#28085;&#30422;2021&#24180;1&#26376;&#33267;2021&#24180;12&#26376;&#20851;&#20110;&#29369;&#22826;&#20154;&#12289;&#20197;&#33394;&#21015;&#21644;&#21453;&#29369;&#22826;&#20027;&#20041;&#30340;&#24120;&#35265;&#35805;&#39064;&#30340;6,941&#20010;&#25512;&#25991;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26631;&#27880;&#36807;&#31243;&#26088;&#22312;&#20005;&#26684;&#24212;&#29992;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21453;&#29369;&#22826;&#20027;&#20041;&#23450;&#20041;&#65292;&#24378;&#21046;&#27880;&#37322;&#32773;&#25351;&#23450;&#25152;&#36866;&#29992;&#30340;&#23450;&#20041;&#37096;&#20998;&#65292;&#24182;&#35753;&#20182;&#20204;&#26377;&#36873;&#25321;&#22320;&#23601;&#20010;&#26696;&#34920;&#36798;&#19981;&#21516;&#24847;&#35265;&#12290;&#26631;&#27880;&#21628;&#21505;&#21453;&#23545;&#21453;&#29369;&#22826;&#20027;&#20041;&#12289;&#25253;&#21578;&#21453;&#29369;&#22826;&#20027;&#20041;&#25110;&#19982;&#21453;&#29369;&#22826;&#20027;&#20041;&#26377;&#20851;&#20294;&#23454;&#38469;&#19978;&#19981;&#23646;&#20110;&#21453;&#29369;&#22826;&#20027;&#20041;&#30340;&#25512;&#25991;&#21487;&#20197;&#24110;&#21161;&#20943;&#23569;&#35823;&#21028;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the major challenges in automatic hate speech detection is the lack of datasets that cover a wide range of biased and unbiased messages and that are consistently labeled. We propose a labeling procedure that addresses some of the common weaknesses of labeled datasets. We focus on antisemitic speech on Twitter and create a labeled dataset of 6,941 tweets that cover a wide range of topics common in conversations about Jews, Israel, and antisemitism between January 2019 and December 2021 by drawing from representative samples with relevant keywords. Our annotation process aims to strictly apply a commonly used definition of antisemitism by forcing annotators to specify which part of the definition applies, and by giving them the option to personally disagree with the definition on a case-by-case basis. Labeling tweets that call out antisemitism, report antisemitism, or are otherwise related to antisemitism (such as the Holocaust) but are not actually antisemitic can help reduce fal
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#35821;&#27861;&#23884;&#20837;&#27169;&#22411;(LGE)&#65292;&#23427;&#21487;&#20197;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26080;&#30417;&#30563;&#36827;&#34892;&#25512;&#29702;&#65292;&#20135;&#29983;&#31616;&#26126;&#26131;&#25026;&#30340;&#36755;&#20986;&#65292;&#33021;&#22815;&#36879;&#26126;&#22320;&#29983;&#25104;&#26032;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#19968;&#30334;&#21477;&#35805;&#30340;&#35821;&#26009;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.14590</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23398;&#20064;&#35821;&#27861;&#30340;&#36923;&#36753;&#35789;&#23884;&#20837;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A logical word embedding for learning grammar. (arXiv:2304.14590v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14590
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#35821;&#27861;&#23884;&#20837;&#27169;&#22411;(LGE)&#65292;&#23427;&#21487;&#20197;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26080;&#30417;&#30563;&#36827;&#34892;&#25512;&#29702;&#65292;&#20135;&#29983;&#31616;&#26126;&#26131;&#25026;&#30340;&#36755;&#20986;&#65292;&#33021;&#22815;&#36879;&#26126;&#22320;&#29983;&#25104;&#26032;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#19968;&#30334;&#21477;&#35805;&#30340;&#35821;&#26009;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#36923;&#36753;&#35821;&#27861;&#23884;&#20837;(LGE)&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21463;&#21040;&#20102;&#32452;&#21512;&#35821;&#27861;&#21644;&#31867;&#21035;&#35821;&#27861;&#30340;&#21551;&#21457;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#23545;&#35789;&#27719;&#31867;&#21035;&#21644;&#21477;&#27861;&#35268;&#21017;&#36827;&#34892;&#26080;&#30417;&#30563;&#25512;&#29702;&#12290; LGE&#20135;&#29983;&#20102;&#31616;&#26126;&#26131;&#25026;&#30340;&#36755;&#20986;&#65292;&#33021;&#22815;&#36879;&#26126;&#22320;&#29983;&#25104;&#26032;&#21477;&#23376;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#20165;&#26377;&#19968;&#30334;&#21477;&#35805;&#30340;&#35821;&#26009;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the logical grammar emdebbing (LGE), a model inspired by pregroup grammars and categorial grammars to enable unsupervised inference of lexical categories and syntactic rules from a corpus of text. LGE produces comprehensible output summarizing its inferences, has a completely transparent process for producing novel sentences, and can learn from as few as a hundred sentences.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; GAEA&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#22686;&#24378;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23454;&#20307;-&#20851;&#31995;&#32534;&#30721;&#22120;&#29983;&#25104;&#23454;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#24418;&#22686;&#24378;&#21019;&#24314;&#20004;&#20010;&#22270;&#24418;&#35270;&#22270;&#65292;&#20197;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.14585</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#24418;&#22686;&#24378;&#25552;&#39640;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Improving Knowledge Graph Entity Alignment with Graph Augmentation. (arXiv:2304.14585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14585
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; GAEA&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#24418;&#22686;&#24378;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#23454;&#20307;-&#20851;&#31995;&#32534;&#30721;&#22120;&#29983;&#25104;&#23454;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;&#22270;&#24418;&#22686;&#24378;&#21019;&#24314;&#20004;&#20010;&#22270;&#24418;&#35270;&#22270;&#65292;&#20197;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#22312;&#30693;&#35782;&#34701;&#21512;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#65292;&#20854;&#30446;&#30340;&#26159;&#23558;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#30456;&#31561;&#23454;&#20307;&#36827;&#34892;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#35201;&#20040;&#21463;&#21040;&#30495;&#23454;&#30693;&#35782;&#22270;&#35889;&#20998;&#24067;&#20013;&#30340;&#32467;&#26500;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#35201;&#20040;&#24573;&#30053;&#26410;&#30693;&#65288;&#26410;&#26631;&#35760;&#65289;&#23454;&#20307;&#30340;&#24322;&#26500;&#34920;&#31034;&#23398;&#20064;&#65292;&#36825;&#23558;&#23548;&#33268;&#27169;&#22411;&#23545;&#23569;&#37327;&#23545;&#40784;&#31181;&#23376;&#65288;&#21363;&#35757;&#32451;&#25968;&#25454;&#65289;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#40784;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#24418;&#22686;&#24378;&#30340;&#20840;&#26032;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#8212;&#8212;GAEA&#12290;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#19968;&#20010;&#31616;&#21333;&#30340;&#23454;&#20307;-&#20851;&#31995;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#20840;&#38754;&#30340;&#32467;&#26500;&#20449;&#24687;&#21644;&#20016;&#23500;&#30340;&#20851;&#31995;&#35821;&#20041;&#26469;&#29983;&#25104;&#23454;&#20307;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#24418;&#22686;&#24378;&#21019;&#24314;&#20004;&#20010;&#22270;&#24418;&#35270;&#22270;&#65292;&#20197;&#36827;&#34892;&#22522;&#20110;&#36793;&#32536;&#30340;&#23545;&#40784;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) which links equivalent entities across different knowledge graphs (KGs) plays a crucial role in knowledge fusion. In recent years, graph neural networks (GNNs) have been successfully applied in many embedding-based EA methods. However, existing GNN-based methods either suffer from the structural heterogeneity issue that especially appears in the real KG distributions or ignore the heterogeneous representation learning for unseen (unlabeled) entities, which would lead the model to overfit on few alignment seeds (i.e., training data) and thus cause unsatisfactory alignment performance. To enhance the EA ability, we propose GAEA, a novel EA approach based on graph augmentation. In this model, we design a simple Entity-Relation (ER) Encoder to generate latent representations for entities via jointly modeling comprehensive structural information and rich relation semantics. Moreover, we use graph augmentation to create two graph views for margin-based alignment learnin
&lt;/p&gt;</description></item><item><title>&#22312;chatbot&#30340;&#20351;&#29992;&#20013;&#65292;&#24212;&#35813;&#20381;&#25454;&#36866;&#24403;&#24615;&#21407;&#21017;&#32780;&#38750;&#32431;&#31929;&#30340;&#23433;&#20840;&#24615;&#21407;&#21017;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#20854;&#21463;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.14553</link><description>&lt;p&gt;
&#36866;&#24403;&#24615;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;
&lt;/p&gt;
&lt;p&gt;
Appropriateness is all you need!. (arXiv:2304.14553v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14553
&lt;/p&gt;
&lt;p&gt;
&#22312;chatbot&#30340;&#20351;&#29992;&#20013;&#65292;&#24212;&#35813;&#20381;&#25454;&#36866;&#24403;&#24615;&#21407;&#21017;&#32780;&#38750;&#32431;&#31929;&#30340;&#23433;&#20840;&#24615;&#21407;&#21017;&#26469;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#36991;&#20813;&#20854;&#21463;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20445;&#38556;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#8220;&#23433;&#20840;&#24615;&#8221;&#24050;&#25104;&#20026;&#23427;&#20204;&#20801;&#35768;&#20351;&#29992;&#30340;&#20027;&#35201;&#35268;&#33539;&#35201;&#27714;&#65292;&#29978;&#33267;&#26159;&#21807;&#19968;&#35268;&#33539;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#8220;&#23433;&#20840;&#24615;&#35268;&#33539;&#24615;&#8221;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#35299;&#20915;chatGPT&#21644;&#20854;&#20182;chatbot&#24341;&#21457;&#30340;&#38382;&#39064;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38480;&#21046;chatbot&#35805;&#39064;&#33539;&#22260;&#30340;&#8220;&#36866;&#24403;&#24615;&#35268;&#33539;&#24615;&#8221;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35805;&#35821;&#30340;&#19977;&#31181;&#36866;&#24403;&#24615;&#65288;&#25216;&#26415;&#20132;&#38469;&#12289;&#31038;&#20250;&#12289;&#36947;&#24503;&#65289;&#36827;&#34892;&#35780;&#20272;&#26469;&#35268;&#23450;chatbot&#30340;&#35821;&#35328;&#34920;&#36798;&#35201;&#27714;&#65292;&#20197;&#36991;&#20813;&#20854;&#21463;&#32422;&#26463;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The strive to make AI applications "safe" has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. Similar can be attested to the latest version of chatbots, such as chatGPT. In this view, if they are "safe", they are supposed to be permissible to deploy. This approach, which we call "safety-normativity", is rather limited in solving the emerging issues that chatGPT and other chatbots have caused thus far. In answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. We argue that rather than looking for "safety" in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. We then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of p
&lt;/p&gt;</description></item><item><title>&#23545;&#35805;&#30340;&#35770;&#36848;&#26159;&#29616;&#20195;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#27010;&#25324;&#21450;&#20854;&#23427;&#24212;&#29992;&#30340;&#26222;&#36941;&#38480;&#21046;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#20174;&#35821;&#29992;&#35282;&#24230;&#25299;&#23637;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2304.14543</link><description>&lt;p&gt;
&#35770;&#36848;&#23545;&#35805;&#65306;&#20174;&#35821;&#29992;&#35282;&#24230;&#25299;&#23637;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Discourse over Discourse: The Need for an Expanded Pragmatic Focus in Conversational AI. (arXiv:2304.14543v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14543
&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#30340;&#35770;&#36848;&#26159;&#29616;&#20195;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#27010;&#25324;&#21450;&#20854;&#23427;&#24212;&#29992;&#30340;&#26222;&#36941;&#38480;&#21046;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#20174;&#35821;&#29992;&#35282;&#24230;&#25299;&#23637;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#30340;&#27010;&#25324;&#65292;&#21363;&#23545;&#35805;&#36807;&#31243;&#30340;&#35770;&#36848;&#65292;&#20351;&#24471;&#23454;&#29992;&#35821;&#29992;&#25104;&#20026;&#24403;&#20195;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#27010;&#25324;&#21644;&#20854;&#20182;&#24212;&#29992;&#30340;&#26222;&#36941;&#38480;&#21046;&#12290;&#26412;&#25991;&#21033;&#29992;&#35821;&#20041;&#21644;&#21477;&#27861;&#26041;&#38754;&#30340;&#37325;&#22823;&#36827;&#23637;&#65292;&#35752;&#35770;&#20102;&#27010;&#25324;&#23545;&#35805;&#21644;&#20854;&#20182;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#33509;&#24178;&#25361;&#25112;&#65292;&#24182;&#20511;&#21161;&#30456;&#20851;&#29702;&#35770;&#30740;&#31350;&#35828;&#26126;&#20102;&#35821;&#29992;&#23398;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#20197;&#25152;&#35859;&#30340;&#26126;&#26143;&#21477;&#23376;&#20026;&#20363;&#65292;&#36825;&#20123;&#21477;&#23376;&#22312;&#21477;&#27861;&#19978;&#26159;&#21487;&#25509;&#21463;&#30340;&#21629;&#39064;&#65292;&#20294;&#22312;&#23545;&#35805;&#25110;&#20854;&#25688;&#35201;&#20013;&#22312;&#35821;&#29992;&#19978;&#26159;&#19981;&#21512;&#36866;&#30340;&#12290;&#30001;&#20110;AI&#30340;&#36136;&#37327;&#22522;&#20934;&#26159;&#26080;&#27861;&#21306;&#20998;&#20154;&#31867;&#34892;&#20026;&#65292;&#22240;&#27492;&#25105;&#20204; heavily &#20381;&#36182;&#24515;&#29702;&#35821;&#35328;&#23398;&#25991;&#29486;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#25237;&#35785;&#26631;&#35760;&#20026;&#8220;&#22270;&#28789;&#27979;&#35797;&#35302;&#21457;&#22120;&#8221;&#65288;TTTs&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#35805;&#25688;&#35201;&#26041;&#27861;&#21644;&#35821;&#29992;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#35774;&#35745;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#27604;&#22914;&#35821;&#38899;&#35782;&#21035;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
The summarization of conversation, that is, discourse over discourse, elevates pragmatic considerations as a pervasive limitation of both summarization and other applications of contemporary conversational AI. Building on impressive progress in both semantics and syntax, pragmatics concerns meaning in the practical sense. In this paper, we discuss several challenges in both summarization of conversations and other conversational AI applications, drawing on relevant theoretical work. We illustrate the importance of pragmatics with so-called star sentences, syntactically acceptable propositions that are pragmatically inappropriate in conversation or its summary. Because the baseline for quality of AI is indistinguishability from human behavior, we draw heavily on the psycho-linguistics literature, and label our complaints as "Turing Test Triggers" (TTTs). We discuss implications for the design and evaluation of conversation summarization methods and conversational AI applications like vo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14535</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning for Automatic Speech Recognition: Towards Better Generalization. (arXiv:2304.14535v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;Survey&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26041;&#38754;&#38754;&#20020;&#30528;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#36825;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36890;&#24120;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#30340;&#22495;&#65292;&#20855;&#26377;&#30456;&#21516;&#30340;&#36755;&#20837;&#29305;&#24449;&#31354;&#38388;&#21644;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20123;&#29616;&#23454;&#19990;&#30028;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#20551;&#35774;&#26159;&#26080;&#27861;&#36866;&#29992;&#30340;&#12290;DTL&#34987;&#24341;&#20837;&#26469;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#23427;&#26377;&#21161;&#20110;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#38598;&#24320;&#21457;&#39640;&#24615;&#33021;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#23454;&#38469;&#25968;&#25454;&#38598;&#21363;&#20351;&#24456;&#23567;&#25110;&#31245;&#26377;&#19981;&#21516;&#65292;&#20294;&#19982;&#35757;&#32451;&#25968;&#25454;&#30456;&#20851;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;DTL&#30340;ASR&#26694;&#26550;&#30340;&#20840;&#38754;&#35843;&#26597;&#65292;&#20197;&#38416;&#26126;&#26368;&#26032;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) has recently become an important challenge when using deep learning (DL). It requires large-scale training datasets and high computational and storage resources. Moreover, DL techniques and machine learning (ML) approaches in general, hypothesize that training and testing data come from the same domain, with the same input feature space and data distribution characteristics. This assumption, however, is not applicable in some real-world artificial intelligence (AI) applications. Moreover, there are situations where gathering real data is challenging, expensive, or rarely occurring, which can not meet the data requirements of DL models. deep transfer learning (DTL) has been introduced to overcome these issues, which helps develop high-performing models using real datasets that are small or slightly different but related to the training data. This paper presents a comprehensive survey of DTL-based ASR frameworks to shed light on the latest developments 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20803;&#20998;&#24067;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2304.14522</link><description>&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#30340;&#22810;&#20803;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multivariate Representation Learning for Information Retrieval. (arXiv:2304.14522v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22810;&#20803;&#20998;&#24067;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31264;&#23494;&#26816;&#32034;&#27169;&#22411;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#32593;&#32476;&#26550;&#26500;&#26469;&#23398;&#20064;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#36825;&#20123;&#34920;&#31034;&#24418;&#24335;&#36890;&#24120;&#37319;&#29992;&#21521;&#37327;&#34920;&#31034;&#65292;&#23427;&#20204;&#30340;&#30456;&#20284;&#24615;&#36890;&#24120;&#20351;&#29992;&#28857;&#31215;&#20989;&#25968;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#31264;&#23494;&#26816;&#32034;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26159;&#23398;&#20064;&#27599;&#20010;&#26597;&#35810;&#21644;&#25991;&#26723;&#30340;&#21521;&#37327;&#65292;&#32780;&#26159;&#23398;&#20064;&#22810;&#20803;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#36127;&#22810;&#20803;KL&#25955;&#24230;&#35745;&#31639;&#20998;&#24067;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#20026;&#20102;&#31616;&#21270;&#21644;&#25552;&#39640;&#25928;&#29575;&#65292;&#25105;&#20204;&#20551;&#35774;&#36825;&#20123;&#20998;&#24067;&#26159;&#22810;&#32500;&#27491;&#24577;&#20998;&#24067;&#65292;&#28982;&#21518;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#36825;&#20123;&#20998;&#24067;&#30340;&#22343;&#20540;&#21644;&#26041;&#24046;&#21521;&#37327;&#12290;&#25105;&#20204;&#20026;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#36817;&#20284;&#26368;&#36817;&#37051;&#31639;&#27861;&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#26816;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35206;&#30422;&#20102;&#21508;&#31181;&#19981;&#21516;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dense retrieval models use bi-encoder network architectures for learning query and document representations. These representations are often in the form of a vector representation and their similarities are often computed using the dot product function. In this paper, we propose a new representation learning framework for dense retrieval. Instead of learning a vector for each query and document, our framework learns a multivariate distribution and uses negative multivariate KL divergence to compute the similarity between distributions. For simplicity and efficiency reasons, we assume that the distributions are multivariate normals and then train large language models to produce mean and variance vectors for these distributions. We provide a theoretical foundation for the proposed framework and show that it can be seamlessly integrated into the existing approximate nearest neighbor algorithms to perform retrieval efficiently. We conduct an extensive suite of experiments on a wide range 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25972;&#21512;&#36827;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#26080;&#35821;&#38899;&#22495;&#33258;&#36866;&#24212;&#21644;&#28608;&#27963;&#30340;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#25345;&#32493;&#27169;&#22411;&#23545;&#20849;&#20139;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#24456;&#37325;&#35201;&#65292;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#27604;&#21333;&#27169;&#24577;&#26356;&#32039;&#20945;&#37325;&#21472;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#65292;&#36825;&#37096;&#20998;&#35299;&#37322;&#20102;Maestro&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2304.14514</link><description>&lt;p&gt;
&#29702;&#35299;&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Understanding Shared Speech-Text Representations. (arXiv:2304.14514v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#25991;&#26412;&#25972;&#21512;&#36827;&#20837;&#31471;&#21040;&#31471;&#35821;&#38899;&#27169;&#22411;&#20013;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#26080;&#35821;&#38899;&#22495;&#33258;&#36866;&#24212;&#21644;&#28608;&#27963;&#30340;&#30456;&#20284;&#24615;&#65292;&#21457;&#29616;&#25345;&#32493;&#27169;&#22411;&#23545;&#20849;&#20139;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#24456;&#37325;&#35201;&#65292;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#27604;&#21333;&#27169;&#24577;&#26356;&#32039;&#20945;&#37325;&#21472;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#65292;&#36825;&#37096;&#20998;&#35299;&#37322;&#20102;Maestro&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#26377;&#25928;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#20102;&#35768;&#22810;&#23558;&#25991;&#26412;&#25972;&#21512;&#21040;&#31471;&#21040;&#31471;&#27169;&#22411;&#20013;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;Maestro&#25512;&#36827;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#31867;&#20998;&#26512;&#25193;&#23637;&#20102;&#25105;&#20204;&#23545;&#20135;&#29983;&#30340;&#20849;&#20139;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#30340;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#35821;&#38899;&#22495;&#33258;&#36866;&#24212;&#30340;&#26497;&#38480;&#65292;&#21457;&#29616;&#20026;&#20102;&#23398;&#20064;&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#65292;&#20855;&#26377;&#35821;&#38899;-&#25991;&#26412;&#23545;&#40784;&#30340;&#29305;&#23450;&#35821;&#26009;&#24211;&#25345;&#32493;&#27169;&#22411;&#26159;&#26368;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#65288;&#35821;&#38899;&#25110;&#25991;&#26412;&#65289;&#30340;&#28608;&#27963;&#19982;&#20849;&#20139;&#32534;&#30721;&#22120;&#30340;&#28608;&#27963;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20849;&#20139;&#32534;&#30721;&#22120;&#23398;&#20064;&#20102;&#19968;&#20010;&#27604;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#26356;&#32039;&#20945;&#21644;&#37325;&#21472;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#37096;&#20998;&#35299;&#37322;&#20102;Maestro&#20849;&#20139;&#30340;&#35821;&#38899;-&#25991;&#26412;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a number of approaches to train speech models by incorpo-rating text into end-to-end models have been developed, with Mae-stro advancing state-of-the-art automatic speech recognition (ASR)and Speech Translation (ST) performance. In this paper, we expandour understanding of the resulting shared speech-text representationswith two types of analyses. First we examine the limits of speech-free domain adaptation, finding that a corpus-specific duration modelfor speech-text alignment is the most important component for learn-ing a shared speech-text representation. Second, we inspect the sim-ilarities between activations of unimodal (speech or text) encodersas compared to the activations of a shared encoder. We find that theshared encoder learns a more compact and overlapping speech-textrepresentation than the uni-modal encoders. We hypothesize that thispartially explains the effectiveness of the Maestro shared speech-textrepresentations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25351;&#31216;&#28216;&#25103;&#20013;&#32452;&#21512;&#24615;&#12289;&#35299;&#32806;&#21644;&#31995;&#32479;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22522;&#20110;Obverter&#24314;&#31569;&#30340;&#28216;&#25103;&#20248;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26032;&#30340;&#35299;&#32806;&#25439;&#22833;&#20989;&#25968;&#21644;&#20301;&#32622;&#20877;&#26631;&#23450;&#20219;&#21153;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.14511</link><description>&lt;p&gt;
&#35270;&#35273;&#25351;&#31216;&#28216;&#25103;&#20419;&#36827;&#35299;&#32806;&#34920;&#31034;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Referential Games Further the Emergence of Disentangled Representations. (arXiv:2304.14511v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25351;&#31216;&#28216;&#25103;&#20013;&#32452;&#21512;&#24615;&#12289;&#35299;&#32806;&#21644;&#31995;&#32479;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#22522;&#20110;Obverter&#24314;&#31569;&#30340;&#28216;&#25103;&#20248;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26032;&#30340;&#35299;&#32806;&#25439;&#22833;&#20989;&#25968;&#21644;&#20301;&#32622;&#20877;&#26631;&#23450;&#20219;&#21153;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#20154;&#31867;&#20132;&#27969;&#20449;&#24687;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290; &#20854;&#20013;&#65292;&#32452;&#21512;&#24615;&#19968;&#30452;&#26159;&#25351;&#31216;&#28216;&#25103;&#21644;&#20854;&#21464;&#20307;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20026;&#20195;&#29702;&#20154;&#25552;&#20379;&#26356;&#22823;&#30340;&#31995;&#32479;&#24615;&#12290;&#35299;&#32806;&#27010;&#24565;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#28145;&#20837;&#23398;&#20064;&#20013;&#26377;&#33391;&#22909;&#27867;&#21270;&#30340;&#23398;&#20064;&#34920;&#31034;&#30340;&#26680;&#24515;&#65292;&#24182;&#35748;&#20026;&#36825;&#26159;&#23454;&#29616;&#31995;&#32479;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#25351;&#31216;&#28216;&#25103;&#20013;&#26032;&#35821;&#35328;&#23618;&#38754;&#30340;&#32452;&#21512;&#24615;&#65292;&#23398;&#20064;&#34920;&#31034;&#23618;&#38754;&#30340;&#35299;&#32806;&#65292;&#20197;&#21450;&#31995;&#32479;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;Obverter&#24314;&#31569;&#29289;&#30340;&#35270;&#35273;&#25351;&#31216;&#28216;&#25103;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#35299;&#32806;&#24230;&#37327;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20808;&#21069;&#25552;&#20986;&#30340;&#20301;&#32622;&#24863;&#30693;&#35299;&#32806;&#25439;&#22833;&#65292;&#21033;&#29992;&#26032;&#25552;&#20986;&#30340;&#37325;&#22797;&#20449;&#24687;&#35299;&#32806;&#25439;&#22833;&#21644;&#20301;&#32622;&#20877;&#26631;&#23450;&#20219;&#21153;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural languages are powerful tools wielded by human beings to communicate information. Among their desirable properties, compositionality has been the main focus in the context of referential games and variants, as it promises to enable greater systematicity to the agents which would wield it. The concept of disentanglement has been shown to be of paramount importance to learned representations that generalise well in deep learning, and is thought to be a necessary condition to enable systematicity. Thus, this paper investigates how do compositionality at the level of the emerging languages, disentanglement at the level of the learned representations, and systematicity relate to each other in the context of visual referential games. Firstly, we find that visual referential games that are based on the Obverter architecture outperforms state-of-the-art unsupervised learning approach in terms of many major disentanglement metrics. Secondly, we expand the previously proposed Positional D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#38395;&#26631;&#39064;&#30340;&#26694;&#26550;&#36776;&#21035;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992; GPT-3.5 &#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102; 68.13% &#30340; F1 &#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2304.14456</link><description>&lt;p&gt;
&#26032;&#38395;&#26694;&#26550;&#65306;&#20174;&#20154;&#31867;&#24863;&#30693;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Framing the News:From Human Perception to Large Language Model Inferences. (arXiv:2304.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14456
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26032;&#38395;&#26631;&#39064;&#30340;&#26694;&#26550;&#36776;&#21035;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992; GPT-3.5 &#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102; 68.13% &#30340; F1 &#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36776;&#21035;&#26032;&#38395;&#26694;&#26550;&#23545;&#20110;&#20102;&#35299;&#25991;&#31456;&#30340;&#35270;&#35282;&#12289;&#24847;&#22270;&#12289;&#20256;&#36882;&#30340;&#20449;&#24687;&#20197;&#21450;&#21738;&#20123;&#26041;&#38754;&#30340;&#26032;&#38395;&#24471;&#21040;&#20102;&#24378;&#35843;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#26694;&#26550;&#26159;&#26032;&#38395;&#23398;&#20013;&#24191;&#27867;&#30740;&#31350;&#30340;&#27010;&#24565;&#65292;&#24182;&#24050;&#25104;&#20026;&#35745;&#31639;&#39046;&#22495;&#20013;&#30340;&#26032;&#35805;&#39064;&#65292;&#20855;&#26377;&#33258;&#21160;&#21270;&#27969;&#31243;&#21644;&#26041;&#20415;&#26032;&#38395;&#20174;&#19994;&#32773;&#24037;&#20316;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#19982; Covid-19 &#21453;&#30123;&#33495;&#36816;&#21160;&#30456;&#20851;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#29702;&#35299;&#22788;&#29702;&#36825;&#19968;&#20027;&#39064;&#25152;&#20351;&#29992;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20154;&#31867;&#26631;&#27880;&#26694;&#26550;&#30340;&#21327;&#35758;&#65292;&#23545;&#26469;&#33258; 5 &#20010;&#22269;&#23478;&#30340;&#27431;&#27954;&#25253;&#32440;&#30340; 1786 &#20010; No-Vax &#36816;&#21160;&#25991;&#31456;&#30340;&#26631;&#39064;&#36827;&#34892;&#26631;&#27880;&#65292;&#22240;&#20026;&#26631;&#39064;&#26159;&#20070;&#38754;&#26032;&#38395;&#20013;&#30340;&#20851;&#38190;&#21333;&#20803;&#65292;&#26377;&#20998;&#26512;&#20215;&#20540;&#65292;&#22240;&#20026;&#24456;&#22810;&#20154;&#21482;&#35835;&#26631;&#39064;&#65288;&#25110;&#29992;&#23427;&#20204;&#26469;&#25351;&#23548;&#20182;&#20204;&#36827;&#19968;&#27493;&#38405;&#35835;&#30340;&#20915;&#31574;&#65289;&#12290;&#20854;&#27425;&#65292;&#32771;&#34385;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26032;&#38395;&#26631;&#39064;&#26694;&#26550;&#25512;&#26029;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#20351;&#29992; GPT-3.5 &#26694;&#26550;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110; BERT &#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GPT-3.5 &#25552;&#39640;&#20102;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;F1 &#20998;&#25968;&#36798;&#21040;&#20102; 68.13%&#12290;&#22522;&#20110; BERT &#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26356;&#20302;&#20294;&#20173;&#26377;&#21512;&#29702;&#32467;&#26524;&#65292;F1 &#20998;&#25968;&#20026; 57.43%&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#21161;&#20110;&#19981;&#26029;&#25193;&#22823;&#30340;&#24037;&#20316;&#65292;&#23637;&#31034;&#20102;&#22312; NLP &#20219;&#21153;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying the frames of news is important to understand the articles' vision, intention, message to be conveyed, and which aspects of the news are emphasized. Framing is a widely studied concept in journalism, and has emerged as a new topic in computing, with the potential to automate processes and facilitate the work of journalism professionals. In this paper, we study this issue with articles related to the Covid-19 anti-vaccine movement. First, to understand the perspectives used to treat this theme, we developed a protocol for human labeling of frames for 1786 headlines of No-Vax movement articles of European newspapers from 5 countries. Headlines are key units in the written press, and worth of analysis as many people only read headlines (or use them to guide their decision for further reading.) Second, considering advances in Natural Language Processing (NLP) with large language models, we investigated two approaches for frame inference of news headlines: first with a GPT-3.5 f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;PMC-LLaMA&#65292;&#20854;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26377;&#26395;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14454</link><description>&lt;p&gt;
PMC-LLaMA: &#22312;&#21307;&#23398;&#35770;&#25991;&#20013;&#36827;&#34892;LLaMA&#30340;&#36827;&#19968;&#27493;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PMC-LLaMA: Further Finetuning LLaMA on Medical Papers. (arXiv:2304.14454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;PMC-LLaMA&#65292;&#20854;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26377;&#26395;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#26085;&#24120;&#23545;&#35805;&#25110;&#38382;&#31572;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#28982;&#32780;&#65292;&#22312;&#27880;&#37325;&#31934;&#24230;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#20986;&#19981;&#23613;&#20154;&#24847;&#30340;&#24615;&#33021;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PMC-LLaMA&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24635;&#20849;480&#19975;&#31687;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#19978;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#36827;&#19968;&#27493;&#27880;&#20837;&#21307;&#23398;&#30693;&#35782;&#65292;&#22686;&#24378;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;PubMedQA&#12289;MedMCQA&#21644;USMLE&#31561;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#21363;PMC-LLaMA&#65292;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29305;&#23450;&#27010;&#24565;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#22312;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21644;&#20195;&#30721;&#20197;&#21450;&#22312;&#32447;&#28436;&#31034;&#22343;&#21487;&#22312;https://github.com/cstorm125/pmc-llama&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding in various domains. These models can usually behave well on daily dialog, or question answering scenarios, however, in areas that value precision, for example, in medical applications, they often exhibit unsatisfactory performance due to a lack of domain-specific knowledge. In this report, we introduce PMC-LLaMA, an open-source language model that is acquired by fine-tuning an open-source language model on a total of 4.8 million biomedical academic papers for further injecting medical knowledge, enhancing its capability in medical domain. Our preliminary evaluations are conducted on three biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing that the our model after finetuning, i.e., PMC-LLaMA, demonstrates better understanding of biomedical domain-specific concepts, thus achieving high performance on QA benchmarks. The model and codes, along with an online demo, are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#20223;&#23556;&#20998;&#31867;&#22120;&#20998;&#26512;&#36234;&#21335;&#27861;&#24459;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#25214;&#21040;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#25152;&#26377;&#29255;&#27573;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25928;&#26524;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.14447</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#21452;&#20223;&#23556;&#20998;&#31867;&#22120;&#20998;&#26512;&#36234;&#21335;&#27861;&#24459;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Analyzing Vietnamese Legal Questions Using Deep Neural Networks with Biaffine Classifiers. (arXiv:2304.14447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#21452;&#20223;&#23556;&#20998;&#31867;&#22120;&#20998;&#26512;&#36234;&#21335;&#27861;&#24459;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#25214;&#21040;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#25152;&#26377;&#29255;&#27573;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#25928;&#26524;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#36234;&#21335;&#27861;&#24459;&#38382;&#39064;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#26500;&#24314;&#27861;&#24459;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#32473;&#23450;&#19968;&#20010;&#33258;&#28982;&#35821;&#35328;&#30340;&#27861;&#24459;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#25552;&#21462;&#21253;&#21547;&#22238;&#31572;&#38382;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#25152;&#26377;&#29255;&#27573;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#28145;&#24230;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#30340;&#33258;&#32534;&#30721;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19978;&#19979;&#25991;&#35789;&#23884;&#20837;&#65292;&#28982;&#21518;&#32467;&#21512;&#23383;&#31526;&#32423;&#21644;POS-tag&#20449;&#24687;&#24418;&#25104;&#21333;&#35789;&#34920;&#31034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#37319;&#29992;&#21452;&#21521;&#38271;&#30701;&#26102;&#35760;&#24518;&#32593;&#32476;&#26469;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#24182;&#29983;&#25104;&#21477;&#23376;&#34920;&#31034;&#12290;&#22312;&#31532;&#19977;&#38454;&#27573;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#22522;&#20110;&#22270;&#30340;&#20381;&#23384;&#20998;&#26512;&#26041;&#27861;&#30340;&#24605;&#24819;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#23545;&#36755;&#20837;&#21477;&#23376;&#30340;&#20840;&#23616;&#35270;&#22270;&#65292;&#20351;&#29992;&#21452;&#20223;&#23556;&#20998;&#31867;&#22120;&#20272;&#35745;&#27599;&#20010;&#36215;&#22987;-&#32467;&#26463;&#21333;&#35789;&#23545;&#25104;&#20026;&#37325;&#35201;&#29255;&#27573;&#30340;&#27010;&#29575;&#12290;&#22312;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#36234;&#21335;&#27861;&#24459;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22823;&#24133;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose using deep neural networks to extract important information from Vietnamese legal questions, a fundamental task towards building a question answering system in the legal domain. Given a legal question in natural language, the goal is to extract all the segments that contain the needed information to answer the question. We introduce a deep model that solves the task in three stages. First, our model leverages recent advanced autoencoding language models to produce contextual word embeddings, which are then combined with character-level and POS-tag information to form word representations. Next, bidirectional long short-term memory networks are employed to capture the relations among words and generate sentence-level representations. At the third stage, borrowing ideas from graph-based dependency parsing methods which provide a global view on the input sentence, we use biaffine classifiers to estimate the probability of each pair of start-end words to be an imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;ChatGPT&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#22914;&#20309;&#38761;&#26032;&#24037;&#31243;&#25945;&#32946;&#24182;&#25913;&#21464;&#25216;&#26415;&#12289;&#25945;&#32844;&#24037;&#19982;&#23398;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35843;&#26597;&#21487;&#20379;&#20854;&#20182;&#22823;&#23398;&#21644;&#26426;&#26500;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.14415</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30693;&#35273;&#65306;&#20851;&#20110;&#23398;&#26415;&#30028;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#25945;&#32844;&#24037;&#21644;&#23398;&#29983;&#30475;&#27861;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative AI Perceptions: A Survey to Measure the Perceptions of Faculty, Staff, and Students on Generative AI Tools in Academia. (arXiv:2304.14415v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#20351;&#29992;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;ChatGPT&#30340;&#24433;&#21709;&#65292;&#26088;&#22312;&#20102;&#35299;&#20854;&#22914;&#20309;&#38761;&#26032;&#24037;&#31243;&#25945;&#32946;&#24182;&#25913;&#21464;&#25216;&#26415;&#12289;&#25945;&#32844;&#24037;&#19982;&#23398;&#29983;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35843;&#26597;&#21487;&#20379;&#20854;&#20182;&#22823;&#23398;&#21644;&#26426;&#26500;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#21487;&#20197;&#36827;&#34892;&#31867;&#20284;&#20154;&#31867;&#23545;&#35805;&#30340;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#19988;&#30456;&#20851;&#30340;&#22238;&#22797;&#12290;&#35813;&#24037;&#20855;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#36755;&#20837;&#30340;&#33258;&#28982;&#25991;&#26412;&#65292;&#24182;&#20197;&#22810;&#31181;&#24418;&#24335;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#37325;&#28857;&#25506;&#35752;&#20102;ChatGPT&#22914;&#20309;&#38761;&#26032;&#24037;&#31243;&#25945;&#32946;&#65292;&#20197;&#21450;&#25216;&#26415;&#12289;&#23398;&#29983;&#12289;&#25945;&#32844;&#24037;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22240;&#20026;&#35813;&#24037;&#20855;&#24555;&#36895;&#21464;&#21270;&#21644;&#19981;&#26029;&#25913;&#36827;&#65292;&#22240;&#27492;&#29616;&#22312;&#26159;&#25910;&#38598;&#30456;&#20851;&#25968;&#25454;&#30340;&#20851;&#38190;&#26102;&#26399;&#12290;&#20026;&#27492;&#65292;&#35774;&#35745;&#20102;&#19968;&#39033;&#35843;&#26597;&#65292;&#20197;&#34913;&#37327;ChatGPT&#23545;&#25945;&#32844;&#24037;&#21644;&#23398;&#29983;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#35813;&#35843;&#26597;&#20316;&#20026;&#24503;&#20811;&#33832;&#26031;A&amp;M&#22823;&#23398;&#25216;&#26415;&#25253;&#21578;&#20998;&#20139;&#65292;&#20197;&#20415;&#20854;&#20182;&#22823;&#23398;&#21644;&#26426;&#26500;&#20351;&#29992;&#35813;&#35843;&#26597;&#65292;&#24182;&#22312;&#20854;&#20182;&#22320;&#26041;&#36827;&#34892;&#24433;&#21709;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a natural language processing tool that can engage in human-like conversations and generate coherent and contextually relevant responses to various prompts. ChatGPT is capable of understanding natural text that is input by a user and generating appropriate responses in various forms. This tool represents a major step in how humans are interacting with technology. This paper specifically focuses on how ChatGPT is revolutionizing the realm of engineering education and the relationship between technology, students, and faculty and staff. Because this tool is quickly changing and improving with the potential for even greater future capability, it is a critical time to collect pertinent data. A survey was created to measure the effects of ChatGPT on students, faculty, and staff. This survey is shared as a Texas A&amp;M University technical report to allow other universities and entities to use this survey and measure the effects elsewhere.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ConDA&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;SQL&#32467;&#26524;&#65292;&#24182;&#35774;&#35745;&#20102;SQL&#23545;&#35805;&#29366;&#24577;&#21644;&#36807;&#28388;&#26041;&#27861;&#26469;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13902</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Controllable Data Augmentation for Context-Dependent Text-to-SQL. (arXiv:2304.13902v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13902
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ConDA&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#20132;&#20114;&#24335;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;SQL&#32467;&#26524;&#65292;&#24182;&#35774;&#35745;&#20102;SQL&#23545;&#35805;&#29366;&#24577;&#21644;&#36807;&#28388;&#26041;&#27861;&#26469;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#33021;&#22815;&#25552;&#39640;&#22797;&#26434;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26631;&#27880;&#25968;&#25454;&#25968;&#37327;&#26377;&#38480;&#65292;&#26631;&#27880;&#22797;&#26434;&#24230;&#39640;&#65292;&#38480;&#21046;&#20102;&#29616;&#26377;&#30340;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25991;&#26412;&#21040;SQL&#36716;&#25442;&#27169;&#22411;&#30340;&#35268;&#27169;&#12290;&#25968;&#25454;&#22686;&#24378;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22686;&#24378;&#26041;&#27861;&#20135;&#29983;&#30340;&#25968;&#25454;&#24448;&#24448;&#32570;&#20047;&#22810;&#26679;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ConDA&#65292;&#21487;&#20197;&#29983;&#25104;&#20132;&#20114;&#24335;&#30340;&#38382;&#39064;&#21644;&#30456;&#24212;&#30340;SQL&#32467;&#26524;&#65292;&#35774;&#35745;&#20102;SQL&#23545;&#35805;&#29366;&#24577;&#20197;&#36890;&#36807;&#29366;&#24577;&#36716;&#31227;&#22686;&#24378;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;grounding&#27169;&#22411;&#30340;&#36807;&#28388;&#22120;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;grounding&#27169;&#22411;&#26469;&#35782;&#21035;&#21644;&#36807;&#28388;&#19982;&#29366;&#24577;&#20449;&#24687;&#19981;&#21305;&#37197;&#30340;&#20302;&#36136;&#37327;&#38382;&#39064;&#12290;&#22312;SParC&#21644;CoSQL&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;ConDA&#23558;&#22522;&#32447;&#27169;&#22411;&#25552;&#39640;&#20102;&#24179;&#22343; $3.3\%$ &#30340;&#22797;&#26434;&#38382;&#39064;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22686;&#24378;&#25968;&#25454;&#65292;&#21457;&#29616;ConDA&#29983;&#25104;&#30340;&#25968;&#25454;&#22312;SQL&#27169;&#26495;&#21305;&#37197;&#21644;&#35821;&#20041;&#27491;&#30830;&#24615;&#26041;&#38754;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The limited scale of annotated data constraints existing context-dependent text-to-SQL models because of the complexity of labeling. The data augmentation method is a commonly used method to solve this problem. However, the data generated by current augmentation methods often lack diversity. In this paper, we introduce ConDA, which generates interactive questions and corresponding SQL results. We designed the SQL dialogue state to enhance the data diversity through the state transition. Meanwhile, we also present a filter method to ensure the data quality by a grounding model. Additionally, we utilize a grounding model to identify and filter low-quality questions that mismatch the state information. Experimental results on the SParC and CoSQL datasets show that ConDA boosts the baseline model to achieve an average improvement of $3.3\%$ on complex questions. Moreover, we analyze the augmented data, which reveals that the data generated by ConDA are of high quality in both SQL template 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2304.13714</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3.5&#21644;GPT-4&#22312;&#25903;&#25345;&#21307;&#30103;&#20445;&#20581;&#20449;&#24687;&#38656;&#27714;&#26041;&#38754;&#30340;&#23454;&#38469;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery. (arXiv:2304.13714v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13714
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#20351;&#29992;GPT-3.5&#21644;GPT-4&#35299;&#20915;&#21307;&#23398;&#38382;&#39064;&#30340;&#23433;&#20840;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20004;&#20010;LLMs&#37117;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#21307;&#29983;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#24403;&#21069;&#30340;&#25506;&#32034;&#24182;&#26410;&#35780;&#20272;LLMs&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20004;&#20010;LLM&#26159;&#21542;&#21487;&#20197;&#20197;&#23433;&#20840;&#21644;&#19968;&#33268;&#30340;&#26041;&#24335;&#28385;&#36275;&#30001;&#21307;&#29983;&#25552;&#20132;&#30340;&#20449;&#24687;&#38656;&#27714;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;66&#20010;&#26469;&#33258;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#38382;&#39064;&#36890;&#36807;&#31616;&#21333;&#30340;&#25552;&#31034;&#25552;&#20132;&#32473;GPT-3.5&#21644;GPT-4&#12290;12&#21517;&#21307;&#29983;&#35780;&#20272;&#20102;LLM&#21709;&#24212;&#23545;&#24739;&#32773;&#36896;&#25104;&#20260;&#23475;&#30340;&#21487;&#33021;&#24615;&#20197;&#21450;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#26381;&#21153;&#30340;&#29616;&#26377;&#25253;&#21578;&#30340;&#19968;&#33268;&#24615;&#12290;&#21307;&#29983;&#30340;&#35780;&#20272;&#22522;&#20110;&#22810;&#25968;&#31080;&#27719;&#24635;&#12290;&#23545;&#20110;&#27809;&#26377;&#20219;&#20309;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;&#21307;&#29983;&#35748;&#20026;&#20219;&#20309;&#19968;&#20010;LLM&#21709;&#24212;&#37117;&#19981;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;&#23545;&#20110;GPT-3.5&#65292;8&#20010;&#38382;&#39064;&#30340;&#21709;&#24212;&#19982;&#20449;&#24687;&#25216;&#26415;&#21672;&#35810;&#25253;&#21578;&#19968;&#33268;&#65292;20&#20010;&#19981;&#19968;&#33268;&#65292;9&#20010;&#26080;&#27861;&#35780;&#20272;&#12290;&#26377;29&#20010;&#21709;&#24212;&#27809;&#26377;&#22810;&#25968;&#31080;&#34920;&#31034;&#8220;&#21516;&#24847;&#8221;&#12289;&#8220;&#19981;&#21516;&#24847;&#8221;&#21644;&#8220;&#26080;&#27861;&#35780;&#20272;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite growing interest in using large language models (LLMs) in healthcare, current explorations do not assess the real-world utility and safety of LLMs in clinical settings. Our objective was to determine whether two LLMs can serve information needs submitted by physicians as questions to an informatics consultation service in a safe and concordant manner. Sixty six questions from an informatics consult service were submitted to GPT-3.5 and GPT-4 via simple prompts. 12 physicians assessed the LLM responses' possibility of patient harm and concordance with existing reports from an informatics consultation service. Physician assessments were summarized based on majority vote. For no questions did a majority of physicians deem either LLM response as harmful. For GPT-3.5, responses to 8 questions were concordant with the informatics consult report, 20 discordant, and 9 were unable to be assessed. There were 29 responses with no majority on "Agree", "Disagree", and "Unable to assess". Fo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24211;&#31995;&#32479;MMDB&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#25152;&#35859;&#30340;&#22810;&#27169;&#24577;&#36816;&#31639;&#31526;&#65288;MMOps&#65289;&#23454;&#29616;&#25991;&#26412;&#38598;&#21512;&#30340;&#36716;&#25442;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.13559</link><description>&lt;p&gt;
&#23454;&#29616;&#26080;&#32541;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Multi-Modal DBMSs for Seamless Querying of Texts and Tables. (arXiv:2304.13559v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24211;&#31995;&#32479;MMDB&#65292;&#23427;&#21487;&#20197;&#26080;&#32541;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#25152;&#35859;&#30340;&#22810;&#27169;&#24577;&#36816;&#31639;&#31526;&#65288;MMOps&#65289;&#23454;&#29616;&#25991;&#26412;&#38598;&#21512;&#30340;&#36716;&#25442;&#65292;&#35813;&#26041;&#27861;&#19981;&#20165;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22312;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#20063;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#25968;&#25454;&#24211;&#31995;&#32479;&#65292;&#21363;&#22810;&#27169;&#24577;&#25968;&#25454;&#24211;&#65288;MMDBs&#65289;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;SQL&#35821;&#35328;&#26080;&#32541;&#22320;&#26597;&#35810;&#25991;&#26412;&#21644;&#34920;&#26684;&#25968;&#25454;&#12290;&#20026;&#20102;&#23454;&#29616;&#22312;MMDB&#20013;&#20351;&#29992;SQL&#35821;&#35328;&#36827;&#34892;&#25991;&#26412;&#25968;&#25454;&#30340;&#26080;&#32541;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25152;&#35859;&#30340;&#22810;&#27169;&#24577;&#36816;&#31639;&#31526;&#65288;MMOps&#65289;&#65292;&#23427;&#20204;&#22522;&#20110;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT-3&#65289;&#30340;&#36827;&#23637;&#12290;MMOps&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#65292;&#23427;&#20204;&#20801;&#35768;&#23558;&#25991;&#26412;&#38598;&#21512;&#20316;&#20026;&#34920;&#26684;&#36827;&#34892;&#22788;&#29702;&#65292;&#32780;&#26080;&#38656;&#25163;&#21160;&#36716;&#25442;&#25968;&#25454;&#12290;&#27491;&#22914;&#25105;&#20204;&#22312;&#35780;&#20272;&#20013;&#23637;&#31034;&#30340;&#37027;&#26679;&#65292;&#25105;&#20204;&#30340;MMDB&#21407;&#22411;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#22914;&#25991;&#26412;&#36716;&#34920;&#65289;&#65292;&#32780;&#19988;&#22312;&#23545;&#19968;&#20010;&#26410;&#35265;&#36807;&#30340;&#25991;&#26412;&#38598;&#21512;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;&#38656;&#35201;&#30340;&#35757;&#32451;&#25968;&#25454;&#25968;&#37327;&#20063;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Multi-Modal Databases (MMDBs), which is a new class of database systems that can seamlessly query text and tables using SQL. To enable seamless querying of textual data using SQL in an MMDB, we propose to extend relational databases with so-called multi-modal operators (MMOps) which are based on the advances of recent large language models such as GPT-3. The main idea of MMOps is that they allow text collections to be treated as tables without the need to manually transform the data. As we show in our evaluation, our MMDB prototype can not only outperform state-of-the-art approaches such as text-to-table in terms of accuracy and performance but it also requires significantly less training data to fine-tune the model for an unseen text collection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#12289;&#38142;&#25509;&#21644;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10637</link><description>&lt;p&gt;
IXA/Cogcomp&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#30340;&#34920;&#29616;&#65306;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
IXA/Cogcomp at SemEval-2023 Task 2: Context-enriched Multilingual Named Entity Recognition using Knowledge Bases. (arXiv:2304.10637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#24211;&#30340;&#19978;&#19979;&#25991;&#22686;&#24378;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#12289;&#38142;&#25509;&#21644;&#39044;&#27979;&#23454;&#20307;&#31867;&#21035;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;&#19968;&#39033;&#26680;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#22312;&#36825;&#26041;&#38754;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20687;CoNLL 2003&#31561;&#26631;&#20934;&#22522;&#20934;&#24182;&#27809;&#26377;&#35299;&#20915;&#37096;&#32626;NER&#31995;&#32479;&#38656;&#35201;&#38754;&#23545;&#30340;&#35768;&#22810;&#25361;&#25112;&#65292;&#20363;&#22914;&#38656;&#35201;&#20197;&#32454;&#31890;&#24230;&#30340;&#26041;&#24335;&#23545;&#26032;&#20852;&#25110;&#22797;&#26434;&#23454;&#20307;&#36827;&#34892;&#20998;&#31867;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NER&#32423;&#32852;&#26041;&#27861;&#65292;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#35782;&#21035;&#36755;&#20837;&#21477;&#23376;&#20013;&#30340;&#23454;&#20307;&#20505;&#36873;&#39033;&#65307;&#20854;&#27425;&#65292;&#23558;&#27599;&#20010;&#20505;&#36873;&#39033;&#38142;&#25509;&#21040;&#29616;&#26377;&#30340;&#30693;&#35782;&#24211;&#65307;&#31532;&#19977;&#65292;&#39044;&#27979;&#27599;&#20010;&#23454;&#20307;&#20505;&#36873;&#39033;&#30340;&#32454;&#31890;&#24230;&#31867;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#22806;&#37096;&#30693;&#35782;&#24211;&#22312;&#20934;&#30830;&#20998;&#31867;&#32454;&#31890;&#24230;&#21644;&#26032;&#20852;&#23454;&#20307;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;MultiCoNER2&#20849;&#20139;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#40065;&#26834;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#20063;&#33021;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a core natural language processing task in which pre-trained language models have shown remarkable performance. However, standard benchmarks like CoNLL 2003 \cite{conll03} do not address many of the challenges that deployed NER systems face, such as having to classify emerging or complex entities in a fine-grained way. In this paper we present a novel NER cascade approach comprising three steps: first, identifying candidate entities in the input sentence; second, linking the each candidate to an existing knowledge base; third, predicting the fine-grained category for each entity candidate. We empirically demonstrate the significance of external knowledge bases in accurately classifying fine-grained and emerging entities. Our system exhibits robust performance in the MultiCoNER2 \cite{multiconer2-data} shared task, even in the low-resource language setting where we leverage knowledge bases of high-resource languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.09349</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#26426;&#22120;&#20154;&#30340;&#22823;&#33041;&#65306;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM as A Robotic Brain: Unifying Egocentric Memory and Control. (arXiv:2304.09349v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#30340;&#26694;&#26550;LLM-Brain&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#36827;&#34892;&#38646;-shot&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#35206;&#30422;&#20102;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#24863;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#24320;&#21457;&#20855;&#22791;&#29289;&#29702;&#25110;&#34394;&#25311;&#23454;&#20307;&#65288;&#21363;&#26426;&#22120;&#20154;&#65289;&#24182;&#33021;&#22815;&#19982;&#29615;&#22659;&#21160;&#24577;&#20132;&#20114;&#30340;&#26234;&#33021;&#31995;&#32479;&#12290;&#35760;&#24518;&#21644;&#25511;&#21046;&#26159;&#20307;&#24863;&#31995;&#32479;&#30340;&#20004;&#20010;&#22522;&#26412;&#37096;&#20998;&#65292;&#36890;&#24120;&#38656;&#35201;&#20998;&#21035;&#20351;&#29992;&#26694;&#26550;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LLM-Brain&#65306;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26426;&#22120;&#20154;&#22823;&#33041;&#65292;&#32479;&#19968;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#12290;LLM-Brain&#26694;&#26550;&#38598;&#25104;&#20102;&#22810;&#20010;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154;&#20219;&#21153;&#65292;&#21033;&#29992;&#38646;-shot&#23398;&#20064;&#26041;&#27861;&#12290;LLM-Brain&#20013;&#30340;&#25152;&#26377;&#32452;&#20214;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#23553;&#38381;&#24335;&#22810;&#36718;&#23545;&#35805;&#65292;&#21253;&#25324;&#24863;&#30693;&#12289;&#35268;&#21010;&#12289;&#25511;&#21046;&#21644;&#35760;&#24518;&#12290;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#20855;&#22791;&#33258;&#25105;&#20013;&#24515;&#35760;&#24518;&#21644;&#25511;&#21046;&#26426;&#22120;&#20154;&#30340;&#23454;&#20307;LLM&#12290;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#65306;&#20027;&#21160;&#25506;&#32034;&#21644;&#23454;&#20307;&#38382;&#31572;&#26469;&#28436;&#31034;LLM-Brain&#12290;
&lt;/p&gt;
&lt;p&gt;
Embodied AI focuses on the study and development of intelligent systems that possess a physical or virtual embodiment (i.e. robots) and are able to dynamically interact with their environment. Memory and control are the two essential parts of an embodied system and usually require separate frameworks to model each of them. In this paper, we propose a novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control. The LLM-Brain framework integrates multiple multimodal language models for robotic tasks, utilizing a zero-shot learning approach. All components within LLM-Brain communicate using natural language in closed-loop multi-round dialogues that encompass perception, planning, control, and memory. The core of the system is an embodied LLM to maintain egocentric memory and control the robot. We demonstrate LLM-Brain by examining two downstream tasks: active exploration and embodied question answering. The
&lt;/p&gt;</description></item><item><title>chatIPCC&#26159;&#19968;&#20010;&#22522;&#20110;&#27668;&#20505;&#31185;&#23398;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;IPCC AR6&#20013;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;GPT-4&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#12289;&#31185;&#23398;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.05510</link><description>&lt;p&gt;
chatIPCC: &#22522;&#20110;&#27668;&#20505;&#31185;&#23398;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
chatIPCC: Grounding Conversational AI in Climate Science. (arXiv:2304.05510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05510
&lt;/p&gt;
&lt;p&gt;
chatIPCC&#26159;&#19968;&#20010;&#22522;&#20110;&#27668;&#20505;&#31185;&#23398;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36890;&#36807;&#25972;&#21512;IPCC AR6&#20013;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;GPT-4&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#21487;&#38752;&#12289;&#31185;&#23398;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#22312;&#38382;&#31572;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#38754;&#20020;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#24187;&#35273;&#21644;&#35757;&#32451;&#21518;&#20449;&#24687;&#36807;&#26102;&#12290;&#22312;&#20851;&#38190;&#39046;&#22495;&#65292;&#22914;&#27668;&#20505;&#21464;&#21270;&#65292;&#24555;&#36895;&#20174;&#21487;&#38752;&#26469;&#28304;&#33719;&#21462;&#20934;&#30830;&#21644;&#26368;&#26032;&#20449;&#24687;&#26159;&#33267;&#20851;&#37325;&#35201;&#19988;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#22806;&#37096;&#31185;&#23398;&#20934;&#30830;&#19988;&#21487;&#38752;&#30340;&#26469;&#28304;&#65288;&#38271;&#26399;&#35760;&#24518;&#65289;&#65292;&#20197;&#25345;&#32493;&#26356;&#26032;&#20854;&#30693;&#35782;&#24182;&#38450;&#27490;&#19981;&#20934;&#30830;&#12289;&#19981;&#27491;&#30830;&#25110;&#36807;&#26102;&#20449;&#24687;&#30340;&#20256;&#25773;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25972;&#21512;&#32852;&#21512;&#25919;&#24220;&#38388;&#27668;&#20505;&#21464;&#21270;&#19987;&#38376;&#22996;&#21592;&#20250;&#31532;&#20845;&#27425;&#35780;&#20272;&#25253;&#21578;&#65288;IPCC AR6&#65289;&#20013;&#30340;&#20449;&#24687;&#26469;&#22686;&#24378;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#23545;&#35805;&#22411;&#20154;&#24037;&#26234;&#33021;&#21407;&#22411;&#65292;&#21487;&#20197;&#22312;www.chatclima.com&#19978;&#25552;&#38382;&#19982;&#27668;&#20505;&#31185;&#23398;&#30456;&#20851;&#30340;&#38382;&#39064;&#24182;&#33719;&#24471;&#22522;&#20110;IPCC AR6&#25253;&#21578;&#30340;&#31185;&#23398;&#20934;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made significant progress in recent years, achieving remarkable results in question-answering tasks (QA). However, they still face two major challenges: hallucination and outdated information after the training phase. These challenges take center stage in critical domains like climate change, where obtaining accurate and up-to-date information from reliable sources in a limited time is essential and difficult. To overcome these barriers, one potential solution is to provide LLMs with access to external, scientifically accurate, and robust sources (long-term memory) to continuously update their knowledge and prevent the propagation of inaccurate, incorrect, or outdated information. In this study, we enhanced GPT-4 by integrating the information from the Sixth Assessment Report of the Intergovernmental (IPCC AR6), the most comprehensive, up-to-date, and reliable source in this domain. We present our conversational AI prototype, available at www.chatclima
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.05351</link><description>&lt;p&gt;
ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#38646;&#26679;&#26412;&#20998;&#26512;&#65306;&#21326;&#23572;&#34903;&#26032;&#25163;&#65311;
&lt;/p&gt;
&lt;p&gt;
The Wall Street Neophyte: A Zero-Shot Analysis of ChatGPT Over MultiModal Stock Movement Prediction Challenges. (arXiv:2304.05351v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#30340;&#34920;&#29616;&#19981;&#22914;&#26368;&#20808;&#36827;&#21644;&#20256;&#32479;&#26041;&#27861;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22914;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#32929;&#24066;&#36208;&#21183;&#26041;&#38754;&#65292;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#20173;&#28982;&#26377;&#24453;&#25506;&#32034;&#12290;&#26412;&#25991;&#36890;&#36807;&#19977;&#20010;&#25512;&#25991;&#21644;&#21382;&#21490;&#32929;&#31080;&#20215;&#26684;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#38646;&#26679;&#26412;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;ChatGPT&#22312;&#22810;&#27169;&#24577;&#32929;&#31080;&#31227;&#21160;&#39044;&#27979;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#26159;&#19968;&#20010;&#8220;&#21326;&#23572;&#34903;&#26032;&#25163;&#8221;&#65292;&#22312;&#39044;&#27979;&#32929;&#31080;&#31227;&#21160;&#26041;&#38754;&#30340;&#25104;&#21151;&#26377;&#38480;&#65292;&#19981;&#20165;&#19981;&#22914;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#19988;&#19981;&#22914;&#20351;&#29992;&#20215;&#26684;&#29305;&#24449;&#30340;&#32447;&#24615;&#22238;&#24402;&#36825;&#26679;&#30340;&#20256;&#32479;&#26041;&#27861;&#12290;&#23613;&#31649;&#24605;&#32500;&#38142;&#25552;&#31034;&#31574;&#30053;&#21644;&#25512;&#25991;&#30340;&#21253;&#21547;&#20855;&#26377;&#28508;&#22312;&#30340;&#20248;&#21183;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#38656;&#35201;&#26356;&#19987;&#19994;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;ChatGPT&#22312;&#32929;&#31080;&#39044;&#27979;&#26041;&#38754;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large language models (LLMs) like ChatGPT have demonstrated remarkable performance across a variety of natural language processing tasks. However, their effectiveness in the financial domain, specifically in predicting stock market movements, remains to be explored. In this paper, we conduct an extensive zero-shot analysis of ChatGPT's capabilities in multimodal stock movement prediction, on three tweets and historical stock price datasets. Our findings indicate that ChatGPT is a "Wall Street Neophyte" with limited success in predicting stock movements, as it underperforms not only state-of-the-art methods but also traditional methods like linear regression using price features. Despite the potential of Chain-of-Thought prompting strategies and the inclusion of tweets, ChatGPT's performance remains subpar. Furthermore, we observe limitations in its explainability and stability, suggesting the need for more specialized training or fine-tuning. This research provides insights i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#21367;&#31215;&#22686;&#24378;&#36827;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27531;&#24046;&#21367;&#31215;&#27169;&#22359;&#30452;&#25509;&#27169;&#25311;&#26631;&#35760;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#65292;&#22312;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#27969;&#21160;&#12290;</title><link>http://arxiv.org/abs/2212.08330</link><description>&lt;p&gt;
&#21367;&#31215;&#22686;&#24378;&#30340;&#19981;&#26029;&#36827;&#21270;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Convolution-enhanced Evolving Attention Networks. (arXiv:2212.08330v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#21367;&#31215;&#22686;&#24378;&#36827;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#27531;&#24046;&#21367;&#31215;&#27169;&#22359;&#30452;&#25509;&#27169;&#25311;&#26631;&#35760;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#65292;&#22312;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#27969;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;Transformers&#65292;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#22312;&#25152;&#26377;&#31181;&#31867;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#65292;&#27880;&#24847;&#21147;&#22270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#32534;&#30721;&#20102;&#36755;&#20837;&#26631;&#35760;&#20043;&#38388;&#30340;&#35821;&#20041;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#27880;&#24847;&#21147;&#32593;&#32476;&#26159;&#22522;&#20110;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#25110;&#25512;&#29702;&#30340;&#65292;&#19981;&#21516;&#23618;&#27425;&#30340;&#27880;&#24847;&#21147;&#22270;&#22312;&#23398;&#20064;&#26102;&#26159;&#20998;&#21035;&#23398;&#20064;&#32780;&#19981;&#26159;&#36827;&#34892;&#26174;&#24335;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#36827;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#35813;&#26426;&#21046;&#36890;&#36807;&#19968;&#31995;&#21015;&#27531;&#24046;&#21367;&#31215;&#27169;&#22359;&#30452;&#25509;&#27169;&#25311;&#26631;&#35760;&#38388;&#20851;&#31995;&#30340;&#28436;&#21464;&#12290;&#20854;&#20027;&#35201;&#21160;&#26426;&#26377;&#20004;&#26041;&#38754;&#12290;&#19968;&#26041;&#38754;&#65292;&#19981;&#21516;&#23618;&#27425;&#30340;&#27880;&#24847;&#21147;&#22270;&#20998;&#20139;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#22240;&#27492;&#28155;&#21152;&#27531;&#24046;&#36830;&#25509;&#21487;&#20197;&#20419;&#36827;&#26631;&#35760;&#20043;&#38388;&#20851;&#31995;&#22312;&#19981;&#21516;&#23618;&#27425;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#33258;&#28982;&#23384;&#22312;&#30528;&#28436;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks, such as Transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. In all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. However, most existing attention networks perform modeling or reasoning based on representations , wherein the attention maps of different layers are learned separately without explicit interactions. In this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. The major motivations are twofold. On the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. On the other hand, there is naturally an evolutionary trend a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;</title><link>http://arxiv.org/abs/2210.15387</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#21457;&#38899;&#38556;&#30861;&#33258;&#21160;&#20005;&#37325;&#31243;&#24230;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Automatic Severity Assessment of Dysarthric speech by using Self-supervised Model with Multi-task Learning. (arXiv:2210.15387v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15387
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#22312;&#36739;&#23569;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#21521;&#20256;&#32479;&#26041;&#27861;&#30340;&#20248;&#21270;&#65292;&#24182;&#19988;&#30456;&#23545;&#25552;&#39640;&#20102;1.25%&#30340;F1-score&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#30340;&#20005;&#37325;&#31243;&#24230;&#23545;&#20110;&#25345;&#32493;&#27835;&#30103;&#21644;&#24247;&#22797;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#38750;&#20856;&#22411;&#21457;&#38899;&#30340;&#38590;&#24230;&#36739;&#22823;&#65292;&#24448;&#24448;&#20250;&#23548;&#33268;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21160;&#35780;&#20272;&#21457;&#38899;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#27169;&#22411;&#19982;&#22810;&#20219;&#21153;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;Wav2vec 2.0 XLS-R&#36827;&#34892;&#20004;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65306;&#20005;&#37325;&#31243;&#24230;&#20998;&#31867;&#21644;&#36741;&#21161;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#23545;&#20110;&#22522;&#20934;&#23454;&#39564;&#65292;&#25105;&#20204;&#37319;&#29992;&#25163;&#24037;&#21046;&#20316;&#30340;&#22768;&#23398;&#29305;&#24449;&#21644;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#65292;&#22914;SVM&#12289;MLP&#21644;XGBoost&#12290;&#22312;&#38889;&#22269;&#21457;&#38899;&#38556;&#30861;QoLT&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#25506;&#31350;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#30340;&#22522;&#20934;&#26041;&#27861;&#65292;F1-score&#30456;&#23545;&#25552;&#39640;1.25%&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36229;&#36807;&#20102;&#27809;&#26377;ASR&#22836;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;10.61%&#30340;&#30456;&#23545;&#30334;&#20998;&#27604;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22810;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#24433;&#21709;&#38556;&#30861;&#20005;&#37325;&#31243;&#24230;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic assessment of dysarthric speech is essential for sustained treatments and rehabilitation. However, obtaining atypical speech is challenging, often leading to data scarcity issues. To tackle the problem, we propose a novel automatic severity assessment method for dysarthric speech, using the self-supervised model in conjunction with multi-task learning. Wav2vec 2.0 XLS-R is jointly trained for two different tasks: severity classification and auxiliary automatic speech recognition (ASR). For the baseline experiments, we employ hand-crafted acoustic features and machine learning classifiers such as SVM, MLP, and XGBoost. Explored on the Korean dysarthric speech QoLT database, our model outperforms the traditional baseline methods, with a relative percentage increase of 1.25% for F1-score. In addition, the proposed model surpasses the model trained without ASR head, achieving 10.61% relative percentage improvements. Furthermore, we present how multi-task learning affects the seve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;&#20004;&#31181;&#27169;&#24335;&#30456;&#20114;&#28040;&#38500;&#27495;&#20041;&#26469;&#26377;&#25928;&#22320;&#25351;&#23450;&#21644;&#25945;&#25480;&#26426;&#22120;&#20154;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04476</link><description>&lt;p&gt;
&#20351;&#29992;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#39640;&#25928;&#23398;&#20064;&#26426;&#22120;&#20154;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Using Both Demonstrations and Language Instructions to Efficiently Learn Robotic Tasks. (arXiv:2210.04476v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30456;&#32467;&#21512;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;&#20004;&#31181;&#27169;&#24335;&#30456;&#20114;&#28040;&#38500;&#27495;&#20041;&#26469;&#26377;&#25928;&#22320;&#25351;&#23450;&#21644;&#25945;&#25480;&#26426;&#22120;&#20154;&#22797;&#26434;&#20219;&#21153;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#25945;&#24072;&#24037;&#20316;&#37327;&#24182;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26159;&#25351;&#23450;&#21644;&#25945;&#25480;&#26426;&#22120;&#20154;&#26032;&#20219;&#21153;&#30340;&#20004;&#31181;&#24120;&#35265;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#65292;&#21333;&#29420;&#20351;&#29992;&#28436;&#31034;&#25110;&#35821;&#35328;&#25351;&#20196;&#20250;&#23384;&#22312;&#27495;&#20041;&#65292;&#23548;&#33268;&#20219;&#21153;&#26080;&#27861;&#28165;&#26224;&#22320;&#34987;&#25351;&#23450;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#28436;&#31034;&#21644;&#25351;&#20196;&#30340;&#32452;&#21512;&#21487;&#20197;&#26356;&#31616;&#26126;&#22320;&#26377;&#25928;&#22320;&#21521;&#26426;&#22120;&#20154;&#20256;&#36798;&#20219;&#21153;&#12290;&#20026;&#20102;&#28436;&#31034;&#36825;&#20010;&#38382;&#39064;&#35774;&#32622;&#65292;&#25105;&#20204;&#22312;&#20960;&#30334;&#20010;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#20154;&#25342;&#21462;&#25918;&#32622;&#20219;&#21153;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; DeL-TaCo&#65288;&#32852;&#21512;&#28436;&#31034; - &#35821;&#35328;&#20219;&#21153;&#35843;&#33410;&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26426;&#22120;&#20154;&#31574;&#30053;&#35843;&#33410;&#20026;&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#30340;&#20219;&#21153;&#23884;&#20837;&#65306;&#35270;&#35273;&#28436;&#31034;&#21644;&#35821;&#35328;&#25351;&#20196;&#12290;&#36890;&#36807;&#20801;&#35768;&#36825;&#20004;&#31181;&#27169;&#24335;&#22312;&#26032;&#20219;&#21153;&#35268;&#33539;&#26102;&#30456;&#20114;&#28040;&#38500;&#27495;&#20041;&#21644;&#28548;&#28165;&#24444;&#27492;&#65292;DeL-TaCo&#65288;1&#65289;&#22823;&#22823;&#38477;&#20302;&#20102;&#25351;&#23450;&#26032;&#20219;&#21153;&#25152;&#38656;&#30340;&#25945;&#24072;&#24037;&#20316;&#37327;&#65292;&#24182;&#19988;&#65288;2&#65289;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demonstrations and natural language instructions are two common ways to specify and teach robots novel tasks. However, for many complex tasks, a demonstration or language instruction alone contains ambiguities, preventing tasks from being specified clearly. In such cases, a combination of both a demonstration and an instruction more concisely and effectively conveys the task to the robot than either modality alone. To instantiate this problem setting, we train a single multi-task policy on a few hundred challenging robotic pick-and-place tasks and propose DeL-TaCo (Joint Demo-Language Task Conditioning), a method for conditioning a robotic policy on task embeddings comprised of two components: a visual demonstration and a language instruction. By allowing these two modalities to mutually disambiguate and clarify each other during novel task specification, DeL-TaCo (1) substantially decreases the teacher effort needed to specify a new task and (2) achieves better generalization performa
&lt;/p&gt;</description></item></channel></rss>