<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;</title><link>http://arxiv.org/abs/2306.15666</link><description>&lt;p&gt;
AI-&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Testing of Detection Tools for AI-Generated Text. (arXiv:2306.15666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#21151;&#33021;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24378;&#35843;&#20102;&#22312;&#23398;&#26415;&#29615;&#22659;&#20013;&#19981;&#20844;&#24179;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21152;&#24378;&#20102;&#23547;&#25214;&#26816;&#27979;&#27492;&#31867;&#20869;&#23481;&#35299;&#20915;&#26041;&#26696;&#30340;&#21162;&#21147;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#24037;&#20855;&#30340;&#19968;&#33324;&#21151;&#33021;&#65292;&#24182;&#26681;&#25454;&#20934;&#30830;&#24615;&#21644;&#38169;&#35823;&#31867;&#22411;&#20998;&#26512;&#23545;&#20854;&#36827;&#34892;&#35780;&#20272;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#22238;&#31572;&#20197;&#19979;&#30740;&#31350;&#38382;&#39064;&#65306;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#21306;&#20998;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#21644;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#20197;&#21450;&#26426;&#22120;&#32763;&#35793;&#21644;&#20869;&#23481;&#28151;&#28102;&#25216;&#26415;&#26159;&#21542;&#24433;&#21709;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#12290;&#30740;&#31350;&#28085;&#30422;&#20102;12&#31181;&#20844;&#24320;&#21487;&#29992;&#30340;&#24037;&#20855;&#21644;&#20004;&#20010;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#26415;&#29615;&#22659;&#20013;&#30340;&#21830;&#19994;&#31995;&#32479;&#65288;Turnitin&#21644;PlagiarismCheck&#65289;&#12290;&#30740;&#31350;&#20154;&#21592;&#24471;&#20986;&#32467;&#35770;&#65292;&#29616;&#26377;&#30340;&#26816;&#27979;&#24037;&#20855;&#26082;&#19981;&#20934;&#30830;&#20063;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#23384;&#22312;&#20027;&#35201;&#30340;&#21487;&#36776;&#35782;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative pre-trained transformer large language models have emphasised the potential risks of unfair use of artificial intelligence (AI) generated content in an academic environment and intensified efforts in searching for solutions to detect such content. The paper examines the general functionality of detection tools for artificial intelligence generated text and evaluates them based on accuracy and error type analysis. Specifically, the study seeks to answer research questions about whether existing detection tools can reliably differentiate between human-written text and ChatGPT-generated text, and whether machine translation and content obfuscation techniques affect the detection of AIgenerated text. The research covers 12 publicly available tools and two commercial systems (Turnitin and PlagiarismCheck) that are widely used in the academic setting. The researchers conclude that the available detection tools are neither accurate nor reliable and have a main bi
&lt;/p&gt;</description></item><item><title>SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15656</link><description>&lt;p&gt;
SparseOptimizer: &#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#26469;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#32534;&#35793;&#22120;&#20849;&#21516;&#35774;&#35745;&#26469;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate through Compiler Co-design. (arXiv:2306.15656v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15656
&lt;/p&gt;
&lt;p&gt;
SparseOptimizer&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#23427;&#37319;&#29992;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#26080;&#38656;&#23545;&#20195;&#30721;&#36827;&#34892;&#20462;&#25913;&#21363;&#21487;&#36866;&#24212;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#19982;&#23494;&#38598;&#22411;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SparseOptimizer&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;Moreau-Yosida&#27491;&#21017;&#21270;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65292;ALBERT&#21644;GPT&#65289;&#20013;&#33258;&#28982;&#22320;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;SparseOptimizer&#35774;&#35745;&#30340;&#20851;&#38190;&#26159;&#23884;&#20837;&#30340;&#25910;&#32553;&#25805;&#20316;&#31526;&#65292;&#23427;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#30452;&#25509;&#24341;&#20837;&#31232;&#30095;&#24615;&#12290;&#36825;&#20010;&#25805;&#20316;&#31526;&#36890;&#36807;&#22362;&#23454;&#30340;&#29702;&#35770;&#26694;&#26550;&#25903;&#25345;&#65292;&#24182;&#21253;&#21547;&#20102;&#19968;&#20010;&#20998;&#26512;&#35299;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20248;&#21270;&#22120;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#26524;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;SparseOptimizer&#30340;&#21363;&#25554;&#21363;&#29992;&#21151;&#33021;&#28040;&#38500;&#20102;&#23545;&#20195;&#30721;&#20462;&#25913;&#30340;&#38656;&#27714;&#65292;&#20351;&#20854;&#25104;&#20026;&#36866;&#29992;&#20110;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#36866;&#24212;&#24037;&#20855;&#12290;&#22312;GLUE&#12289;RACE&#12289;SQuAD1&#21644;SQuAD2&#31561;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#36890;&#36807;SparseOptimizer&#31232;&#30095;&#21270;&#21518;&#30340;SparseBERT&#21644;SparseALBERT&#22312;&#24615;&#33021;&#19978;&#19982;&#23494;&#38598;&#22411;&#30340;BERT&#21644;ALBERT&#30456;&#24403;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces SparseOptimizer, a novel deep learning optimizer that exploits Moreau-Yosida regularization to naturally induce sparsity in large language models such as BERT, ALBERT and GPT. Key to the design of SparseOptimizer is an embedded shrinkage operator, which imparts sparsity directly within the optimization process. This operator, backed by a sound theoretical framework, includes an analytical solution, thereby reinforcing the optimizer's robustness and efficacy. Crucially, SparseOptimizer's plug-and-play functionality eradicates the need for code modifications, making it a universally adaptable tool for a wide array of large language models. Empirical evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2 confirm that SparseBERT and SparseALBERT, when sparsified using SparseOptimizer, achieve performance comparable to their dense counterparts, BERT and ALBERT, while significantly reducing their parameter count. Further, this work proposes an innovati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#25351;&#20196;&#35270;&#39057;&#20013;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38899;&#35270;Transformer&#23558;&#38899;&#35270;&#29305;&#24449;&#21644;&#25351;&#20196;&#35821;&#38899;&#36716;&#25442;&#20026;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#39118;&#26684;&#36716;&#31227;&#30340;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15644</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#26684;&#36716;&#31227;&#30340;&#35821;&#38899;&#19982;&#35270;&#21548;&#22330;&#26223;&#29702;&#35299;&#26041;&#27861;&#29992;&#20110;&#20174;&#35270;&#39057;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Style-transfer based Speech and Audio-visual Scene Understanding for Robot Action Sequence Acquisition from Videos. (arXiv:2306.15644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#25351;&#20196;&#35270;&#39057;&#20013;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#38899;&#35270;Transformer&#23558;&#38899;&#35270;&#29305;&#24449;&#21644;&#25351;&#20196;&#35821;&#38899;&#36716;&#25442;&#20026;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#39118;&#26684;&#36716;&#31227;&#30340;&#35757;&#32451;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#26681;&#25454;&#26377;&#38480;&#30340;&#20808;&#39564;&#30693;&#35782;&#25191;&#34892;&#26032;&#20219;&#21153;&#30340;&#21160;&#20316;&#65292;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#20196;&#25191;&#34892;&#12290;&#20154;&#31867;&#19987;&#23478;&#21487;&#20197;&#36890;&#36807;&#22810;&#27169;&#24577;&#25351;&#20196;&#30340;&#28436;&#31034;&#21521;&#26426;&#22120;&#20154;&#20998;&#20139;&#25191;&#34892;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#23637;&#31034;&#19968;&#31995;&#21015;&#30701;&#26242;&#27493;&#39588;&#26469;&#23454;&#29616;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20174;&#25351;&#20196;&#35270;&#39057;&#20013;&#29983;&#25104;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;(1)&#38899;&#35270;Transformer&#23558;&#38899;&#35270;&#29305;&#24449;&#21644;&#25351;&#20196;&#35821;&#38899;&#36716;&#25442;&#20026;&#31216;&#20026;&#21160;&#24577;&#36816;&#21160;&#21407;&#29702;(DMPs)&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#65292;&#20197;&#21450;(2)&#22522;&#20110;&#39118;&#26684;&#36716;&#31227;&#30340;&#35757;&#32451;&#65292;&#21033;&#29992;&#35270;&#39057;&#23383;&#24149;&#21644;&#35821;&#20041;&#20998;&#31867;&#22120;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24369;&#30417;&#30563;&#23398;&#20064;&#65292;&#20197;&#21033;&#29992;&#19981;&#37197;&#23545;&#30340;&#35270;&#39057;&#21160;&#20316;&#25968;&#25454;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#21508;&#31181;&#28921;&#39274;&#21160;&#20316;&#65292;&#20854;&#20013;&#19968;&#20010;&#33218;&#24335;&#26426;&#22120;&#20154;&#20351;&#29992;&#38899;&#35270;Transformer&#25191;&#34892;&#20174;&#28921;&#39274;&#35270;&#39057;&#20013;&#33719;&#21462;&#30340;DMP&#24207;&#21015;&#12290;&#22312;Epic-K&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
To realize human-robot collaboration, robots need to execute actions for new tasks according to human instructions given finite prior knowledge. Human experts can share their knowledge of how to perform a task with a robot through multi-modal instructions in their demonstrations, showing a sequence of short-horizon steps to achieve a long-horizon goal. This paper introduces a method for robot action sequence generation from instruction videos using (1) an audio-visual Transformer that converts audio-visual features and instruction speech to a sequence of robot actions called dynamic movement primitives (DMPs) and (2) style-transfer-based training that employs multi-task learning with video captioning and weakly-supervised learning with a semantic classifier to exploit unpaired video-action data. We built a system that accomplishes various cooking actions, where an arm robot executes a DMP sequence acquired from a cooking video using the audio-visual Transformer. Experiments with Epic-K
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;&#27861;&#35821;&#20013;&#30340;&#33258;&#21160;&#27880;&#37322;&#30452;&#25509;&#35328;&#35821;&#65288;AADS&#65289;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#30740;&#31350;&#37319;&#29992;&#26368;&#22823;&#30340;&#27861;&#35821;&#21465;&#36848;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#20173;&#38656;&#22823;&#37327;&#21162;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#22522;&#32447;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15634</link><description>&lt;p&gt;
&#33258;&#21160;&#27880;&#37322;&#27861;&#35821;&#20070;&#38754;&#21465;&#36848;&#20013;&#30340;&#30452;&#25509;&#35328;&#35821;
&lt;/p&gt;
&lt;p&gt;
Automatic Annotation of Direct Speech in Written French Narratives. (arXiv:2306.15634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;&#27861;&#35821;&#20013;&#30340;&#33258;&#21160;&#27880;&#37322;&#30452;&#25509;&#35328;&#35821;&#65288;AADS&#65289;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;&#30740;&#31350;&#37319;&#29992;&#26368;&#22823;&#30340;&#27861;&#35821;&#21465;&#36848;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#20219;&#21153;&#20173;&#38656;&#22823;&#37327;&#21162;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#19981;&#21516;&#22522;&#32447;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27880;&#37322;&#27861;&#35821;&#20070;&#38754;&#25991;&#26412;&#20013;&#30340;&#30452;&#25509;&#35328;&#35821;&#65288;AADS&#65289;&#32463;&#24120;&#22312;&#35745;&#31639;&#26426;&#21465;&#36848;&#29702;&#35299;&#20013;&#20351;&#29992;&#12290;&#24050;&#32463;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#33521;&#35821;&#25110;&#24503;&#35821;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25105;&#20204;&#30340;&#30446;&#26631;&#35821;&#35328;&#27861;&#35821;&#65292;&#24456;&#23569;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#21644;&#35780;&#20272;&#27861;&#35821;&#20013;&#30340;AADS&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#36804;&#20170;&#20026;&#27490;&#26631;&#27880;&#26377;&#27599;&#20010;&#21333;&#35789;&#30340;&#26368;&#22823;&#30340;&#27861;&#35821;&#21465;&#36848;&#25968;&#25454;&#38598;&#65307;&#25105;&#20204;&#38024;&#23545;&#24207;&#21015;&#26631;&#27880;&#25110;&#20854;&#20182;&#35821;&#35328;&#30340;AADS&#36866;&#24212;&#20102;&#21508;&#31181;&#22522;&#32447;&#27169;&#22411;&#65307;&#24182;&#19988;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#37325;&#28857;&#26159;&#27867;&#21270;&#33021;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20010;&#20219;&#21153;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#24037;&#20316;&#65292;&#24182;&#24378;&#35843;&#20102;&#27599;&#20010;&#22522;&#32447;&#27169;&#22411;&#30340;&#29305;&#28857;&#12290;&#23613;&#31649;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#25913;&#36827;&#65292;&#20294;&#23427;&#26159;&#40723;&#21169;&#26356;&#22810;&#20851;&#20110;&#36825;&#20010;&#20027;&#39064;&#30340;&#30740;&#31350;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automatic annotation of direct speech (AADS) in written text has been often used in computational narrative understanding. Methods based on either rules or deep neural networks have been explored, in particular for English or German languages. Yet, for French, our target language, not many works exist. Our goal is to create a unified framework to design and evaluate AADS models in French. For this, we consolidated the largest-to-date French narrative dataset annotated with DS per word; we adapted various baselines for sequence labelling or from AADS in other languages; and we designed and conducted an extensive evaluation focused on generalisation. Results show that the task still requires substantial efforts and emphasise characteristics of each baseline. Although this framework could be improved, it is a step further to encourage more research on the topic.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#20195;&#30721;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;Transformer&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20195;&#30721;&#26816;&#32034;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32763;&#35793;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#19968;&#23450;&#24433;&#21709;&#65292;&#20294;&#25968;&#25454;&#35268;&#27169;&#26356;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.15604</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26500;&#24314;&#22810;&#35821;&#35328;&#20195;&#30721;&#26816;&#32034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Constructing Multilingual Code Search Dataset Using Neural Machine Translation. (arXiv:2306.15604v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#20195;&#30721;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;Transformer&#27169;&#22411;&#65292;&#22312;&#22810;&#20010;&#20195;&#30721;&#26816;&#32034;&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32763;&#35793;&#36136;&#37327;&#23545;&#27169;&#22411;&#24615;&#33021;&#26377;&#19968;&#23450;&#24433;&#21709;&#65292;&#20294;&#25968;&#25454;&#35268;&#27169;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#26816;&#32034;&#26159;&#19968;&#39033;&#23547;&#25214;&#19982;&#32473;&#23450;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#35821;&#20041;&#21305;&#37197;&#30340;&#32534;&#31243;&#20195;&#30721;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#26576;&#20123;&#29616;&#26377;&#25968;&#25454;&#38598;&#22312;&#32534;&#31243;&#35821;&#35328;&#19968;&#20391;&#26159;&#22810;&#35821;&#35328;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#26597;&#35810;&#25968;&#25454;&#20165;&#38480;&#20110;&#33521;&#35821;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22235;&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;&#22235;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#26816;&#32034;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#28982;&#21518;&#22312;&#22810;&#20010;&#20195;&#30721;&#26816;&#32034;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#25152;&#26377;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;&#36890;&#36807;&#23545;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#24212;&#29992;&#21453;&#21521;&#32763;&#35793;&#25968;&#25454;&#36807;&#28388;&#65292;&#25105;&#20204;&#35777;&#26126;&#32763;&#35793;&#36136;&#37327;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#24433;&#21709;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#25968;&#25454;&#35268;&#27169;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code search is a task to find programming codes that semantically match the given natural language queries. Even though some of the existing datasets for this task are multilingual on the programming language side, their query data are only in English. In this research, we create a multilingual code search dataset in four natural and four programming languages using a neural machine translation model. Using our dataset, we pre-train and fine-tune the Transformer-based models and then evaluate them on multiple code search test sets. Our results show that the model pre-trained with all natural and programming language data has performed best in most cases. By applying back-translation data filtering to our dataset, we demonstrate that the translation quality affects the model's performance to a certain extent, but the data size matters more.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15595</link><description>&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;
&lt;/p&gt;
&lt;p&gt;
Extending Context Window of Large Language Models via Positional Interpolation. (arXiv:2306.15595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15595
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#26041;&#27861;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#33719;&#24471;&#24378;&#26377;&#21147;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#36890;&#36807;&#32447;&#24615;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#25105;&#20204;&#20445;&#25345;&#20102;&#25193;&#23637;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#20219;&#21153;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20301;&#32622;&#25554;&#20540;&#65288;PI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#23558;RoPE-based&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;LLaMA&#27169;&#22411;&#65289;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#25193;&#23637;&#21040;&#26368;&#22810;32768&#65292;&#24182;&#19988;&#22312;&#38656;&#35201;&#38271;&#19978;&#19979;&#25991;&#30340;&#21508;&#31181;&#20219;&#21153;&#65288;&#21253;&#25324;&#23494;&#38053;&#26816;&#32034;&#12289;&#35821;&#35328;&#24314;&#27169;&#21644;&#38271;&#31687;&#25991;&#26723;&#25688;&#35201;&#31561;&#65289;&#19978;&#23637;&#29616;&#20986;&#33391;&#22909;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#20301;&#32622;&#25554;&#20540;&#25193;&#23637;&#30340;&#27169;&#22411;&#22312;&#21407;&#22987;&#19978;&#19979;&#25991;&#31383;&#21475;&#20869;&#30340;&#20219;&#21153;&#20013;&#30456;&#23545;&#20445;&#25345;&#33391;&#22909;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20301;&#32622;&#25554;&#20540;&#32447;&#24615;&#22320;&#38477;&#20302;&#36755;&#20837;&#20301;&#32622;&#32034;&#24341;&#30340;&#22823;&#23567;&#65292;&#20197;&#21305;&#37197;&#21407;&#22987;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#65292;&#32780;&#19981;&#26159;&#36229;&#36807;&#35757;&#32451;&#26102;&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#30340;&#39640;&#27880;&#24847;&#21147;&#20998;&#25968;&#65292;&#23436;&#20840;&#30772;&#22351;&#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#25554;&#20540;&#30340;&#19978;&#30028;&#33267;&#23569;&#26159;&#25512;&#26029;&#30340;&#19978;&#30028;&#30340;$\sim 600 \times$&#35201;&#23567;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Position Interpolation (PI) that extends the context window sizes of RoPE-based pretrained LLMs such as LLaMA models to up to 32768 with minimal fine-tuning (within 1000 steps), while demonstrating strong empirical results on various tasks that require long context, including passkey retrieval, language modeling, and long document summarization from LLaMA 7B to 65B. Meanwhile, the extended model by Position Interpolation preserve quality relatively well on tasks within its original context window. To achieve this goal, Position Interpolation linearly down-scales the input position indices to match the original context window size, rather than extrapolating beyond the trained context length which may lead to catastrophically high attention scores that completely ruin the self-attention mechanism. Our theoretical study shows that the upper bound of interpolation is at least $\sim 600 \times$ smaller than that of extrapolation, further demonstrating its stability. Models extend
&lt;/p&gt;</description></item><item><title>CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15551</link><description>&lt;p&gt;
CrunchGPT&#65306;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CrunchGPT: A chatGPT assisted framework for scientific machine learning. (arXiv:2306.15551v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15551
&lt;/p&gt;
&lt;p&gt;
CrunchGPT&#26159;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#36741;&#21161;&#26694;&#26550;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#29992;&#25143;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#23454;&#29616;&#26080;&#32541;&#38598;&#25104;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;SciML&#22312;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#31561;&#26041;&#38754;&#30340;&#32791;&#26102;&#38382;&#39064;&#65292;&#25299;&#23637;&#20102;&#20854;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#65288;SciML&#65289;&#36817;&#24180;&#26469;&#22312;&#35745;&#31639;&#31185;&#23398;&#21644;&#24037;&#31243;&#30340;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#20854;&#30446;&#26631;&#26159;&#22312;&#19981;&#38656;&#35201;&#22797;&#26434;&#21644;&#35745;&#31639;&#23494;&#38598;&#30340;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#24773;&#20917;&#19979;&#65292;&#26080;&#32541;&#22320;&#23558;&#25968;&#25454;&#21644;&#29289;&#29702;&#30693;&#35782;&#38598;&#25104;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#39044;&#22788;&#29702;&#12289;&#38382;&#39064;&#24314;&#27169;&#12289;&#20195;&#30721;&#29983;&#25104;&#12289;&#21518;&#22788;&#29702;&#21644;&#20998;&#26512;&#20173;&#28982;&#26159;&#32791;&#26102;&#30340;&#65292;&#24182;&#19988;&#21487;&#33021;&#38480;&#21046;SciML&#22312;&#24037;&#19994;&#24212;&#29992;&#21644;&#25968;&#23383;&#23402;&#29983;&#26694;&#26550;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;SciML&#30340;&#21508;&#20010;&#38454;&#27573;&#25972;&#21512;&#21040;ChatGPT&#30340;&#20254;&#19979;&#65292;&#24418;&#25104;CrunchGPT&#65292;&#23427;&#36890;&#36807;&#29992;&#25143;&#31616;&#21333;&#30340;&#25552;&#31034;&#26469;&#21327;&#35843;&#25972;&#20010;SciML&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20004;&#20010;&#31034;&#20363;&#65292;&#28436;&#31034;&#20102;CrunchGPT&#22312;&#27668;&#21160;&#23398;&#20013;&#20248;&#21270;&#26426;&#32764;&#21644;&#22312;&#21508;&#31181;&#20960;&#20309;&#24418;&#29366;&#20013;&#33719;&#24471;&#27969;&#22330;&#30340;&#28508;&#22312;&#29992;&#36884;&#65292;&#24182;&#24378;&#35843;&#20102;&#39564;&#35777;&#38454;&#27573;&#12290;&#20026;&#20102;&#28436;&#31034;CrunchGPT&#30340;&#27969;&#31243;&#21644;
&lt;/p&gt;
&lt;p&gt;
Scientific Machine Learning (SciML) has advanced recently across many different areas in computational science and engineering. The objective is to integrate data and physics seamlessly without the need of employing elaborate and computationally taxing data assimilation schemes. However, preprocessing, problem formulation, code generation, postprocessing and analysis are still time consuming and may prevent SciML from wide applicability in industrial applications and in digital twin frameworks. Here, we integrate the various stages of SciML under the umbrella of ChatGPT, to formulate CrunchGPT, which plays the role of a conductor orchestrating the entire workflow of SciML based on simple prompts by the user. Specifically, we present two examples that demonstrate the potential use of CrunchGPT in optimizing airfoils in aerodynamics, and in obtaining flow fields in various geometries in interactive mode, with emphasis on the validation stage. To demonstrate the flow of the CrunchGPT, and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.15550</link><description>&lt;p&gt;
CamemBERT-bio&#65306;&#19968;&#31181;&#26356;&#20581;&#24247;&#30340;&#27861;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CamemBERT-bio: a Tasty French Language Model Better for your Health. (arXiv:2306.15550v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;CamemBERT-bio&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#30456;&#23545;&#20110;&#36890;&#29992;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20020;&#24202;&#25968;&#25454;&#20179;&#24211;&#65292;&#21307;&#38498;&#20013;&#30340;&#20020;&#24202;&#25968;&#25454;&#21464;&#24471;&#36234;&#26469;&#36234;&#23481;&#26131;&#29992;&#20110;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20123;&#25991;&#20214;&#37117;&#26159;&#38750;&#32467;&#26500;&#21270;&#30340;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#20174;&#21307;&#30103;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#36827;&#34892;&#20020;&#24202;&#30740;&#31350;&#12290;&#20351;&#29992;CamemBERT&#31561;BERT-like&#27169;&#22411;&#30340;&#36801;&#31227;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#20026;&#36890;&#29992;&#35821;&#35328;&#35757;&#32451;&#30340;&#65292;&#22312;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#19978;&#25928;&#26524;&#36739;&#24369;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27861;&#35821;&#20844;&#20849;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;CamemBERT&#36827;&#34892;&#20102;&#32487;&#32493;&#39044;&#35757;&#32451;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CamemBERT-bio&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#65292;&#23427;&#26159;&#19968;&#31181;&#20026;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20844;&#20849;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#24179;&#22343;F1&#20998;&#25968;&#25552;&#39640;&#20102;2.54&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical data in hospitals are increasingly accessible for research through clinical data warehouses, however these documents are unstructured. It is therefore necessary to extract information from medical reports to conduct clinical studies. Transfer learning with BERT-like models such as CamemBERT has allowed major advances, especially for named entity recognition. However, these models are trained for plain language and are less efficient on biomedical data. This is why we propose a new French public biomedical dataset on which we have continued the pre-training of CamemBERT. Thus, we introduce a first version of CamemBERT-bio, a specialized public model for the French biomedical domain that shows 2.54 points of F1 score improvement on average on different biomedical named entity recognition tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26032;&#24037;&#20855;&#65292;&#25506;&#35752;&#20102;&#31038;&#20250;&#24433;&#21709;&#26426;&#21046;&#19982;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23545;&#29992;&#25143;&#35780;&#35770;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#33322;&#31354;&#29983;&#24577;&#31995;&#32479;&#20013;&#33322;&#31354;&#20844;&#21496;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.15541</link><description>&lt;p&gt;
&#35299;&#25918;&#29992;&#25143;&#35780;&#35770;&#30340;&#21147;&#37327;&#65306;&#25506;&#32034;&#24847;&#22823;&#21033;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#30340;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of User Reviews: Exploring Airline Choices at Catania Airport, Italy. (arXiv:2306.15541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26032;&#24037;&#20855;&#65292;&#25506;&#35752;&#20102;&#31038;&#20250;&#24433;&#21709;&#26426;&#21046;&#19982;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#23545;&#29992;&#25143;&#35780;&#35770;&#30340;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#33322;&#31354;&#29983;&#24577;&#31995;&#32479;&#20013;&#33322;&#31354;&#20844;&#21496;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26032;&#24037;&#20855;&#65292;&#25506;&#35752;&#31038;&#20250;&#24433;&#21709;&#26426;&#21046;&#19982;&#33322;&#31354;&#20844;&#21496;&#36873;&#25321;&#20043;&#38388;&#30340;&#21487;&#33021;&#20851;&#31995;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;&#24433;&#21709;&#28040;&#36153;&#32773;&#22312;&#33322;&#31354;&#39046;&#22495;&#20915;&#31574;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#36873;&#25321;&#20174;&#30693;&#21517;&#24179;&#21488;Trustpilot&#12289;Google&#21644;Twitter&#20013;&#25552;&#21462;&#29992;&#25143;&#35780;&#35770;&#12290;&#36890;&#36807;&#32467;&#21512;&#32593;&#32476;&#29228;&#21462;&#25216;&#26415;&#65292;&#25105;&#20204;&#33021;&#22815;&#25910;&#38598;&#21040;&#21253;&#21547;&#21508;&#31181;&#29992;&#25143;&#24847;&#35265;&#12289;&#21453;&#39304;&#21644;&#35780;&#20998;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20248;&#21270;&#20102;BERT&#27169;&#22411;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#32858;&#28966;&#33322;&#31354;&#20844;&#21496;&#35780;&#35770;&#20013;&#30340;&#26377;&#35265;&#22320;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#19981;&#21516;&#33322;&#31354;&#20844;&#21496;&#24179;&#22343;&#36127;&#38754;&#24773;&#24863;&#24471;&#20998;&#30340;&#26377;&#36259;&#36235;&#21183;&#65292;&#36825;&#20351;&#25105;&#20204;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#20102;&#33322;&#31354;&#20844;&#21496;&#20043;&#38388;&#30340;&#21160;&#24577;&#65292;&#24110;&#21161;&#25105;&#20204;&#35782;&#21035;&#21345;&#22612;&#23612;&#20122;&#26426;&#22330;&#33322;&#31354;&#29983;&#24577;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#21512;&#20316;&#20249;&#20276;&#12289;&#28909;&#38376;&#33322;&#32447;&#21644;&#25198;&#28436;&#26680;&#24515;&#35282;&#33394;&#30340;&#33322;&#31354;&#20844;&#21496;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to investigate the possible relationship between the mechanisms of social influence and the choice of airline, through the use of new tools, with the aim of understanding whether they can contribute to a better understanding of the factors influencing the decisions of consumers in the aviation sector. We have chosen to extract user reviews from well-known platforms: Trustpilot, Google, and Twitter. By combining web scraping techniques, we have been able to collect a comprehensive dataset comprising a wide range of user opinions, feedback, and ratings. We then refined the BERT model to focus on insightful sentiment in the context of airline reviews. Through our analysis, we observed an intriguing trend of average negative sentiment scores across various airlines, giving us deeper insight into the dynamics between airlines and helping us identify key partnerships, popular routes, and airlines that play a central role in the aeronautical ecosystem of Catania airport during
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23558;&#20854;&#19982;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#25259;&#38706;&#20219;&#21153;&#32452;&#30340;&#24314;&#35758;&#36827;&#34892;&#23545;&#27604;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#21147;&#20998;&#26512;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#36879;&#26126;&#24230;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.15518</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25259;&#38706;&#20998;&#26512;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#65306;&#21033;&#29992;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;CHATREPORT&#36171;&#20104;&#21033;&#30410;&#30456;&#20851;&#32773;&#26435;&#21147;
&lt;/p&gt;
&lt;p&gt;
Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool. (arXiv:2306.15518v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#23558;&#20854;&#19982;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#25259;&#38706;&#20219;&#21153;&#32452;&#30340;&#24314;&#35758;&#36827;&#34892;&#23545;&#27604;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#20154;&#21147;&#20998;&#26512;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#36879;&#26126;&#24230;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19987;&#23478;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#32467;&#21512;&#65292;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#23558;&#20854;&#19982;&#12298;&#27668;&#20505;&#30456;&#20851;&#37329;&#34701;&#25259;&#38706;&#20219;&#21153;&#32452;&#12299;&#65288;TCFD&#65289;&#24314;&#35758;&#36827;&#34892;&#22522;&#20934;&#23545;&#27604;&#12290;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#23545;&#20110;&#35780;&#20272;&#32452;&#32455;&#30340;&#29615;&#22659;&#21644;&#31038;&#20250;&#39118;&#38505;&#21644;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#20013;&#22823;&#37327;&#30340;&#20449;&#24687;&#24448;&#24448;&#20250;&#23548;&#33268;&#20154;&#21147;&#25104;&#26412;&#36807;&#39640;&#12290;&#22240;&#27492;&#65292;&#20840;&#29699;&#21482;&#26377;&#23569;&#25968;&#26426;&#26500;&#26377;&#36164;&#28304;&#26469;&#20998;&#26512;&#36825;&#20123;&#25253;&#21578;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#36879;&#26126;&#24230;&#12290;&#34429;&#28982;AI&#39537;&#21160;&#30340;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#20998;&#26512;&#25968;&#25454;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#23427;&#20204;&#23481;&#26131;&#20986;&#29616;&#19981;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#33258;&#21160;&#20998;&#26512;&#20225;&#19994;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;CHATREPORT&#24037;&#20855;&#65292;&#24182;&#22312;&#31532;&#19968;&#20010;&#24212;&#29992;&#26696;&#20363;&#20013;&#23558;&#20854;&#24212;&#29992;&#20110;&#35780;&#20272;&#20225;&#19994;&#30340;&#27668;&#20505;&#39118;&#38505;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel approach to enhance Large Language Models (LLMs) with expert knowledge to automate the analysis of corporate sustainability reports by benchmarking them against the Task Force for Climate-Related Financial Disclosures (TCFD) recommendations. Corporate sustainability reports are crucial in assessing organizations' environmental and social risks and impacts. However, analyzing these reports' vast amounts of information makes human analysis often too costly. As a result, only a few entities worldwide have the resources to analyze these reports, which could lead to a lack of transparency. While AI-powered tools can automatically analyze the data, they are prone to inaccuracies as they lack domain-specific expertise. This paper introduces a novel approach to enhance LLMs with expert knowledge to automate the analysis of corporate sustainability reports. We christen our tool CHATREPORT, and apply it in a first use case to assess corporate climate risk disclosure
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#23548;&#24072;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#35838;&#31243;&#20013;&#23454;&#26102;&#20026;&#23548;&#24072;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#32473;&#23398;&#29983;&#26377;&#25928;&#36190;&#25196;&#30340;&#21453;&#39304;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2306.15498</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#23548;&#24072;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models to Provide Explanatory Feedback to Human Tutors. (arXiv:2306.15498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#20154;&#31867;&#23548;&#24072;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#35838;&#31243;&#20013;&#23454;&#26102;&#20026;&#23548;&#24072;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#32473;&#23398;&#29983;&#26377;&#25928;&#36190;&#25196;&#30340;&#21453;&#39304;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#25216;&#26415;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#23398;&#20064;&#32773;&#22312;&#20135;&#29983;&#35299;&#37322;&#20197;&#25903;&#25345;&#20182;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26102;&#65292;&#23545;&#23398;&#20064;&#26377;&#31215;&#26497;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#25552;&#20379;&#23398;&#20064;&#32773;&#23454;&#26102;&#30340;&#35299;&#37322;&#24615;&#21453;&#39304;&#24120;&#24120;&#38754;&#20020;&#19982;&#20998;&#31867;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#21253;&#21547;&#22797;&#26434;&#21644;&#24494;&#22937;&#30340;&#24773;&#22659;&#21709;&#24212;&#30340;&#39046;&#22495;&#19987;&#29992;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#35838;&#31243;&#20013;&#20026;&#23548;&#24072;&#25552;&#20379;&#26377;&#20851;&#22914;&#20309;&#32473;&#23398;&#29983;&#26377;&#25928;&#36190;&#25196;&#30340;&#23454;&#26102;&#21453;&#39304;&#12290;&#36825;&#39033;&#36827;&#34892;&#20013;&#30340;&#24037;&#20316;&#22312;&#32416;&#27491;&#24615;&#21453;&#39304;&#65288;F1&#20998;&#25968;=0.811&#65289;&#21644;&#25104;&#26524;&#23548;&#21521;&#21453;&#39304;&#65288;F1&#20998;&#25968;=0.350&#65289;&#30340;&#20108;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26469;&#25552;&#20379;&#35299;&#37322;&#24615;&#21453;&#39304;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#22312;&#35838;&#31243;&#20013;&#20026;&#23548;&#24072;&#25552;&#20379;&#21453;&#39304;&#65292;&#36824;&#21487;&#20197;&#28508;&#22312;&#22320;&#25552;&#20986;&#23454;&#26102;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research demonstrates learners engaging in the process of producing explanations to support their reasoning, can have a positive impact on learning. However, providing learners real-time explanatory feedback often presents challenges related to classification accuracy, particularly in domain-specific environments, containing situationally complex and nuanced responses. We present two approaches for supplying tutors real-time feedback within an online lesson on how to give students effective praise. This work-in-progress demonstrates considerable accuracy in binary classification for corrective feedback of effective, or effort-based (F1 score = 0.811), and ineffective, or outcome-based (F1 score = 0.350), praise responses. More notably, we introduce progress towards an enhanced approach of providing explanatory feedback using large language model-facilitated named entity recognition, which can provide tutors feedback, not only while engaging in lessons, but can potentially suggest real-
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.15448</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20132;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Understanding Social Reasoning in Language Models with Language Models. (arXiv:2306.15448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20102;&#35299;&#23427;&#20204;&#29702;&#35299;&#20154;&#31867;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#26377;&#25928;&#30340;&#20132;&#20114;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#20154;&#23581;&#35797;&#35780;&#20272;LLM&#30340;&#29702;&#35770;&#24515;&#26234;&#65288;ToM&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;ToM&#30340;&#19968;&#33268;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25506;&#32034;&#20027;&#39064;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23384;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#65288;2&#65289;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#19982;LLM&#30340;&#35780;&#20272;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;LLM&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;25&#20010;&#25511;&#21046;&#21644;5000&#20010;&#27169;&#22411;&#20889;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20043;&#21069;&#20247;&#21253;&#35780;&#20272;&#30456;&#27604;&#65292;&#20154;&#31867;&#21442;&#19982;&#32773;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#30340;&#36136;&#37327;&#35780;&#20215;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evalua
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2306.15447</link><description>&lt;p&gt;
&#23545;&#40784;&#30340;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#23545;&#25239;&#23545;&#40784;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are aligned neural networks adversarially aligned?. (arXiv:2306.15447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#29992;&#25143;&#26500;&#24314;&#30340;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#26159;&#21542;&#20173;&#33021;&#20445;&#25345;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#25915;&#20987;&#25163;&#27861;&#19981;&#36275;&#20197;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29616;&#22312;&#34987;&#35843;&#25972;&#20026;&#19982;&#20854;&#21019;&#24314;&#32773;&#30340;&#30446;&#26631;&#20445;&#25345;&#19968;&#33268;&#65292;&#21363;"&#26377;&#30410;&#19988;&#26080;&#23475;"&#12290;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#23545;&#29992;&#25143;&#30340;&#38382;&#39064;&#32473;&#20986;&#26377;&#30410;&#30340;&#22238;&#31572;&#65292;&#20294;&#25298;&#32477;&#22238;&#31572;&#21487;&#33021;&#20250;&#36896;&#25104;&#20260;&#23475;&#30340;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23545;&#25239;&#29992;&#25143;&#21487;&#20197;&#26500;&#24314;&#32469;&#36807;&#23545;&#40784;&#23581;&#35797;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19982;&#26500;&#36896;&#26368;&#22351;&#24773;&#20917;&#36755;&#20837;&#65288;&#23545;&#25239;&#24615;&#26679;&#26412;&#65289;&#30340;&#23545;&#25239;&#29992;&#25143;&#20132;&#20114;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20445;&#25345;&#22810;&#22823;&#31243;&#24230;&#30340;&#23545;&#40784;&#12290;&#36825;&#20123;&#36755;&#20837;&#34987;&#35774;&#35745;&#25104;&#23548;&#33268;&#27169;&#22411;&#21457;&#20986;&#26412;&#24212;&#31105;&#27490;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20248;&#21270;&#25915;&#20987;&#25163;&#27861;&#22312;&#21487;&#38752;&#25915;&#20987;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#26041;&#38754;&#30340;&#19981;&#36275;&#20043;&#22788;&#65306;&#21363;&#20351;&#22312;&#24403;&#21069;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25915;&#20987;&#22833;&#36133;&#26102;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#34542;&#21147;&#26041;&#27861;&#25214;&#21040;&#23545;&#25239;&#24615;&#36755;&#20837;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#25915;&#20987;&#30340;&#22833;&#36133;&#19981;&#24212;&#34987;&#35270;&#20026;&#23545;&#40784;&#25991;&#26412;&#27169;&#22411;&#22312;&#38754;&#23545;&#23545;&#25239;&#24615;&#36755;&#20837;&#26102;&#20173;&#28982;&#20445;&#25345;&#23545;&#40784;&#30340;&#35777;&#26126;&#12290;&#20294;&#26159;&#36817;&#26399;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36235;&#21183;&#26159;&#22810;&#27169;&#24577;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are now tuned to align with the goals of their creators, namely to be "helpful and harmless." These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs.  However the recent trend in large-scale ML models is multim
&lt;/p&gt;</description></item><item><title>KnowPrefix-Tuning&#26159;&#19968;&#31181;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#21069;&#32512;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#36731;&#37327;&#32423;&#30693;&#35782;&#21069;&#32512;&#20013;&#26469;&#36991;&#20813;&#26816;&#32034;&#36807;&#31243;&#65292;&#19982;PLM&#36827;&#34892;&#20840;&#38754;&#20132;&#20114;&#20248;&#21270;&#21709;&#24212;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2306.15430</link><description>&lt;p&gt;
&#12298;KnowPrefix-Tuning: &#19968;&#31181;&#29992;&#20110;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#21069;&#32512;&#35843;&#20248;&#26694;&#26550;&#12299;
&lt;/p&gt;
&lt;p&gt;
KnowPrefix-Tuning: A Two-Stage Prefix-Tuning Framework for Knowledge-Grounded Dialogue Generation. (arXiv:2306.15430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15430
&lt;/p&gt;
&lt;p&gt;
KnowPrefix-Tuning&#26159;&#19968;&#31181;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#29983;&#25104;&#30340;&#20004;&#38454;&#27573;&#21069;&#32512;&#35843;&#20248;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#36731;&#37327;&#32423;&#30693;&#35782;&#21069;&#32512;&#20013;&#26469;&#36991;&#20813;&#26816;&#32034;&#36807;&#31243;&#65292;&#19982;PLM&#36827;&#34892;&#20840;&#38754;&#20132;&#20114;&#20248;&#21270;&#21709;&#24212;&#29983;&#25104;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#20197;&#26816;&#32034;&#20877;&#29983;&#25104;&#30340;&#26041;&#24335;&#29983;&#25104;&#22238;&#22797;&#12290;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#24222;&#22823;&#30340;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;&#30693;&#35782;&#26816;&#32034;&#32452;&#20214;&#65292;&#36825;&#38656;&#35201;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20013;&#20869;&#22312;&#32534;&#30721;&#30340;&#30693;&#35782;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Knowledgeable Prefix Tuning&#65288;KnowPrefix-Tuning&#65289;&#30340;&#20004;&#38454;&#27573;&#35843;&#20248;&#26694;&#26550;&#65292;&#22312;&#30693;&#35782;&#39537;&#21160;&#23545;&#35805;&#31995;&#32479;&#20013;&#36890;&#36807;&#23558;&#20808;&#39564;&#30693;&#35782;&#27880;&#20837;&#21040;&#36731;&#37327;&#32423;&#30693;&#35782;&#21069;&#32512;&#20013;&#26469;&#36991;&#20813;&#26816;&#32034;&#36807;&#31243;&#12290;&#30693;&#35782;&#21069;&#32512;&#26159;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#19982;&#30693;&#35782;&#30456;&#20851;&#30340;&#21521;&#37327;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20132;&#20114;&#37325;&#21442;&#25968;&#21270;&#26426;&#21046;&#65292;&#20801;&#35768;&#21069;&#32512;&#22312;&#21709;&#24212;&#29983;&#25104;&#20248;&#21270;&#36807;&#31243;&#20013;&#19982;PLM&#36827;&#34892;&#20840;&#38754;&#30340;&#20132;&#20114;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KnowPrefix-Tuning&#20248;&#20110;fine-tuning&#21644;&#20854;&#20182;&#36731;&#37327;&#32423;&#35843;&#20248;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge-grounded conversation systems generate responses typically in a retrieve-then-generate manner. They require a large knowledge base and a strong knowledge retrieval component, which is time- and resource-consuming. In this paper, we address the challenge by leveraging the inherent knowledge encoded in the pre-trained language models (PLMs). We propose Knowledgeable Prefix Tuning (KnowPrefix-Tuning), a two-stage tuning framework, bypassing the retrieval process in a knowledge-grounded conversation system by injecting prior knowledge into the lightweight knowledge prefix. The knowledge prefix is a sequence of continuous knowledge-specific vectors that can be learned during training. In addition, we propose a novel interactive re-parameterization mechanism that allows the prefix to interact fully with the PLM during the optimization of response generation. Experimental results demonstrate that KnowPrefix-Tuning outperforms fine-tuning and other lightweight tuning approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#24515;&#33039;&#39057;&#35889;&#65292;&#25214;&#21040;&#20102;&#24515;&#33039;&#30142;&#30149;&#30340;&#25913;&#21892;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#36890;&#36947;&#29983;&#29702;&#20449;&#21495;&#30340;&#24555;&#36895;&#21644;&#31283;&#20581;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15425</link><description>&lt;p&gt;
&#24515;&#33039;&#39057;&#35889;&#30340;&#30456;&#31354;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Phase Space Analysis of Cardiac Spectra. (arXiv:2306.15425v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30456;&#31354;&#38388;&#20998;&#26512;&#24515;&#33039;&#39057;&#35889;&#65292;&#25214;&#21040;&#20102;&#24515;&#33039;&#30142;&#30149;&#30340;&#25913;&#21892;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22810;&#36890;&#36947;&#29983;&#29702;&#20449;&#21495;&#30340;&#24555;&#36895;&#21644;&#31283;&#20581;&#30340;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#30142;&#30149;&#26159;&#29616;&#20195;&#24037;&#19994;&#21270;&#31038;&#20250;&#27515;&#20129;&#30340;&#20027;&#35201;&#21407;&#22240;&#20043;&#19968;&#65292;&#24182;&#23548;&#33268;&#20844;&#20849;&#21355;&#29983;&#31995;&#32479;&#30340;&#39640;&#39069;&#24320;&#25903;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20998;&#26512;&#26041;&#27861;&#20197;&#25913;&#21892;&#24515;&#33039;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#38750;&#32447;&#24615;&#24494;&#20998;&#26041;&#31243;&#23545;&#24515;&#33039;&#30340;&#30005;&#27963;&#21160;&#36827;&#34892;&#20102;&#39318;&#27425;&#24314;&#27169;&#12290;&#38543;&#21518;&#65292;&#30740;&#31350;&#20102;&#36215;&#28304;&#20110;&#30830;&#23450;&#24615;&#21160;&#21147;&#23398;&#30340;&#24515;&#33039;&#39057;&#35889;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20998;&#26512;&#27491;&#24120;&#20154;&#24515;&#33039;&#30340;&#21151;&#29575;&#35889;&#65292;&#21487;&#20197;&#21457;&#29616;&#20855;&#26377;&#31867;&#20284;&#20998;&#24418;&#32467;&#26500;&#30340;&#24076;&#26031;-&#26222;&#23572;&#37329;&#26480;&#32593;&#32476;&#12290;&#20174;&#24515;&#30005;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#22270;&#20013;&#25552;&#21462;&#30456;&#31354;&#38388;&#36712;&#36857;&#12290;&#20998;&#24418;&#32500;&#25968;D&#30340;&#36739;&#20302;&#20540;&#34920;&#31034;&#26356;&#21152;&#19968;&#33268;&#30340;&#21160;&#21147;&#23398;&#12290;&#24403;D&#20855;&#26377;&#22823;&#20110;2&#30340;&#38750;&#25972;&#25968;&#20540;&#26102;&#65292;&#31995;&#32479;&#21464;&#24471;&#28151;&#27788;&#25110;&#20855;&#26377;&#22855;&#29305;&#21560;&#24341;&#23376;&#12290;&#26368;&#36817;&#65292;&#25253;&#36947;&#20102;&#19968;&#31181;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#36890;&#36947;&#29983;&#29702;&#20449;&#21495;&#30340;&#24555;&#36895;&#32780;&#31283;&#20581;&#30340;&#26041;&#27861;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20174;n&#20135;&#29983;&#30340;&#24515;&#30005;&#22270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cardiac diseases are one of the main reasons of mortality in modern, industrialized societies, and they cause high expenses in public health systems. Therefore, it is important to develop analytical methods to improve cardiac diagnostics. Electric activity of heart was first modeled by using a set of nonlinear differential equations. Latter, variations of cardiac spectra originated from deterministic dynamics are investigated. Analyzing the power spectra of a normal human heart presents His-Purkinje network, possessing a fractal like structure. Phase space trajectories are extracted from the time series graph of ECG. Lower values of fractal dimension, D indicate dynamics that are more coherent. If D has non-integer values greater than two when the system becomes chaotic or strange attractor. Recently, the development of a fast and robust method, which can be applied to multichannel physiologic signals, was reported. This manuscript investigates two different ECG systems produced from n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#35777;&#25454;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15399</link><description>&lt;p&gt;
&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#35777;&#25454;&#30340;&#26426;&#22120;&#32763;&#35793;&#25991;&#26412;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality Estimation of Machine Translated Texts based on Direct Evidence from Training Data. (arXiv:2306.15399v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#30452;&#25509;&#35777;&#25454;&#30340;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#35821;&#35328;&#23545;&#21644;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#20135;&#29983;&#30340;&#27969;&#30021;&#32763;&#35793;&#36755;&#20986;&#24448;&#24448;&#21487;&#33021;&#21253;&#21547;&#37325;&#35201;&#30340;&#24847;&#20041;&#38169;&#35823;&#12290;&#36136;&#37327;&#20272;&#35745;&#20219;&#21153;&#26159;&#22312;&#19981;&#20381;&#36182;&#21442;&#32771;&#32763;&#35793;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#30001;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#29983;&#25104;&#30340;&#32763;&#35793;&#36136;&#37327;&#36827;&#34892;&#20272;&#35745;&#12290;&#22810;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#23545;&#20110;&#20272;&#35745;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#29983;&#25104;&#30340;&#32763;&#35793;&#36136;&#37327;&#20855;&#26377;&#30452;&#25509;&#32447;&#32034;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#23545;&#20110;&#20219;&#20309;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20135;&#29983;&#30340;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current Machine Translation systems achieve very good results on a growing variety of language pairs and data sets. However, it is now well known that they produce fluent translation outputs that often can contain important meaning errors. Quality Estimation task deals with the estimation of quality of translations produced by a Machine Translation system without depending on Reference Translations. A number of approaches have been suggested over the years. In this paper we show that the parallel corpus used as training data for training the MT system holds direct clues for estimating the quality of translations produced by the MT system. Our experiments show that this simple and direct method holds promise for quality estimation of translations produced by any purely data driven machine translation system.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#20266;&#26410;&#26469;&#19978;&#19979;&#25991;&#25913;&#36827;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#34701;&#21512;&#21382;&#21490;&#19978;&#19979;&#25991;&#21644;&#35828;&#35805;&#20154;&#29305;&#23450;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#19978;&#19979;&#25991;&#38598;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15376</link><description>&lt;p&gt;
&#21033;&#29992;&#20266;&#26410;&#26469;&#19978;&#19979;&#25991;&#23545;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploiting Pseudo Future Contexts for Emotion Recognition in Conversations. (arXiv:2306.15376v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15376
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#21033;&#29992;&#20266;&#26410;&#26469;&#19978;&#19979;&#25991;&#25913;&#36827;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26410;&#26469;&#19978;&#19979;&#25991;&#65292;&#21487;&#20197;&#34701;&#21512;&#21382;&#21490;&#19978;&#19979;&#25991;&#21644;&#35828;&#35805;&#20154;&#29305;&#23450;&#19978;&#19979;&#25991;&#65292;&#24418;&#25104;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#19978;&#19979;&#25991;&#38598;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#23545;&#35805;&#25968;&#25454;&#30340;&#24191;&#27867;&#31215;&#32047;&#65292;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#21033;&#29992;&#19978;&#19979;&#25991;&#21644;&#20855;&#20307;&#35828;&#35805;&#20154;&#30340;&#29305;&#24449;&#65292;&#25110;&#32773;&#25972;&#21512;&#24322;&#26500;&#30340;&#22806;&#37096;&#24120;&#35782;&#30693;&#35782;&#12290;&#20854;&#20013;&#19968;&#20123;&#24037;&#20316;&#20005;&#37325;&#20381;&#36182;&#26410;&#26469;&#30340;&#19978;&#19979;&#25991;&#65292;&#28982;&#32780;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#26410;&#24517;&#24635;&#26159;&#21487;&#29992;&#12290;&#36825;&#19968;&#20107;&#23454;&#28608;&#21457;&#20102;&#25105;&#20204;&#29983;&#25104;&#20266;&#26410;&#26469;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;ERC&#30340;&#24819;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#19968;&#20010;&#35805;&#35821;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20854;&#26410;&#26469;&#19978;&#19979;&#25991;&#65292;&#21487;&#33021;&#21253;&#21547;&#20102;&#23545;&#23545;&#35805;&#36807;&#31243;&#20013;&#26377;&#30410;&#30340;&#39069;&#22806;&#30693;&#35782;&#65292;&#19982;&#21382;&#21490;&#19978;&#19979;&#25991;&#30340;&#24418;&#24335;&#20445;&#25345;&#19968;&#33268;&#12290;&#36825;&#20123;&#29305;&#28857;&#20351;&#24471;&#20266;&#26410;&#26469;&#19978;&#19979;&#25991;&#23481;&#26131;&#19982;&#21382;&#21490;&#19978;&#19979;&#25991;&#21644;&#21382;&#21490;&#29305;&#23450;&#35828;&#35805;&#20154;&#30340;&#19978;&#19979;&#25991;&#30456;&#34701;&#21512;&#65292;&#24418;&#25104;&#19968;&#20010;&#27010;&#24565;&#19978;&#31616;&#21333;&#30340;&#31995;&#32479;&#65292;&#31995;&#32479;&#22320;&#38598;&#25104;&#20102;&#22810;&#20010;&#19978;&#19979;&#25991;&#12290;&#22312;&#22235;&#20010;ERC&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the extensive accumulation of conversational data on the Internet, emotion recognition in conversations (ERC) has received increasing attention. Previous efforts of this task mainly focus on leveraging contextual and speaker-specific features, or integrating heterogeneous external commonsense knowledge. Among them, some heavily rely on future contexts, which, however, are not always available in real-life scenarios. This fact inspires us to generate pseudo future contexts to improve ERC. Specifically, for an utterance, we generate its future context with pre-trained language models, potentially containing extra beneficial knowledge in a conversational form homogeneous with the historical ones. These characteristics make pseudo future contexts easily fused with historical contexts and historical speaker-specific contexts, yielding a conceptually simple framework systematically integrating multi-contexts. Experimental results on four ERC datasets demonstrate our method's superiority
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#30340;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#35821;&#35328;&#22120;&#23448;&#65292;&#36890;&#36807;&#36203;&#24067;&#36830;&#21487;&#22609;&#24615;&#23454;&#29616;&#20102;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#21477;&#23376;&#30340;&#26377;&#24847;&#20041;&#36755;&#20837;&#20013;&#23398;&#20064;&#21517;&#35789;&#12289;&#21160;&#35789;&#21450;&#20854;&#24847;&#20041;&#12290;&#36825;&#19968;&#30740;&#31350;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#35299;&#26512;&#22120;&#35774;&#35745;&#65292;&#28155;&#21152;&#20102;&#19968;&#20010;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#26426;&#21046;&#26469;&#35299;&#37322;&#23156;&#20799;&#22823;&#33041;&#22914;&#20309;&#20064;&#24471;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2306.15364</link><description>&lt;p&gt;
&#19968;&#20010;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#35821;&#35328;&#22120;&#23448;&#30340;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Architecture of a Biologically Plausible Language Organ. (arXiv:2306.15364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#30340;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#35821;&#35328;&#22120;&#23448;&#65292;&#36890;&#36807;&#36203;&#24067;&#36830;&#21487;&#22609;&#24615;&#23454;&#29616;&#20102;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#21477;&#23376;&#30340;&#26377;&#24847;&#20041;&#36755;&#20837;&#20013;&#23398;&#20064;&#21517;&#35789;&#12289;&#21160;&#35789;&#21450;&#20854;&#24847;&#20041;&#12290;&#36825;&#19968;&#30740;&#31350;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#35299;&#26512;&#22120;&#35774;&#35745;&#65292;&#28155;&#21152;&#20102;&#19968;&#20010;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#26426;&#21046;&#26469;&#35299;&#37322;&#23156;&#20799;&#22823;&#33041;&#22914;&#20309;&#20064;&#24471;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#25311;&#30340;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#35821;&#35328;&#22120;&#23448;&#65292;&#30001;&#39118;&#26684;&#21270;&#20294;&#36924;&#30495;&#30340;&#31070;&#32463;&#20803;&#12289;&#31361;&#35302;&#12289;&#33041;&#21306;&#12289;&#21487;&#22609;&#24615;&#21644;&#31616;&#21270;&#30340;&#24863;&#35273;&#30693;&#35273;&#27169;&#22411;&#32452;&#25104;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20010;&#27169;&#22411;&#22312;&#35821;&#35328;&#20064;&#24471;&#30340;&#37325;&#35201;&#26089;&#26399;&#38454;&#27573;&#21462;&#24471;&#20102;&#25104;&#21151;&#65306;&#20174;&#26377;&#38480;&#25968;&#37327;&#30340;&#21477;&#23376;&#30340;&#26377;&#24847;&#20041;&#36755;&#20837;&#20013;&#23398;&#20064;&#21517;&#35789;&#12289;&#21160;&#35789;&#21450;&#20854;&#24847;&#20041;&#12290;&#36825;&#20010;&#31995;&#32479;&#30340;&#23398;&#20064;&#26159;&#36890;&#36807;&#36203;&#24067;&#36830;&#21487;&#22609;&#24615;&#23454;&#29616;&#30340;&#65292;&#32780;&#27809;&#26377;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36234;&#20102;&#20197;&#21069;&#22312;&#31867;&#20284;&#29615;&#22659;&#20013;&#35774;&#35745;&#30340;&#35299;&#26512;&#22120;&#65292;&#20851;&#38190;&#26159;&#28155;&#21152;&#20102;&#19968;&#20010;&#29983;&#29289;&#23398;&#19978;&#21487;&#20449;&#30340;&#26426;&#21046;&#65292;&#35299;&#37322;&#20102;&#23156;&#20799;&#22823;&#33041;&#22914;&#20309;&#20064;&#24471;&#35821;&#35328;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#25104;&#29087;&#22823;&#33041;&#30340;&#22788;&#29702;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simulated biologically plausible language organ, made up of stylized but realistic neurons, synapses, brain areas, plasticity, and a simplified model of sensory perception. We show through experiments that this model succeeds in an important early step in language acquisition: the learning of nouns, verbs, and their meanings, from the grounded input of only a modest number of sentences. Learning in this system is achieved through Hebbian plasticity, and without backpropagation. Our model goes beyond a parser previously designed in a similar environment, with the critical addition of a biologically plausible account for how language can be acquired in the infant's brain, not just processed by a mature brain.
&lt;/p&gt;</description></item><item><title>3D-Speaker&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#12290;&#23427;&#21253;&#21547;&#20102;10,000&#22810;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#25506;&#32034;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.15354</link><description>&lt;p&gt;
3D-Speaker&#65306;&#29992;&#20110;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#30340;&#22823;&#35268;&#27169;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
3D-Speaker: A Large-Scale Multi-Device, Multi-Distance, and Multi-Dialect Corpus for Speech Representation Disentanglement. (arXiv:2306.15354v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15354
&lt;/p&gt;
&lt;p&gt;
3D-Speaker&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35774;&#22791;&#12289;&#22810;&#36317;&#31163;&#21644;&#22810;&#26041;&#35328;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#30740;&#31350;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#12290;&#23427;&#21253;&#21547;&#20102;10,000&#22810;&#20010;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#65292;&#21487;&#20197;&#29992;&#26469;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#25506;&#32034;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#31038;&#21306;&#20013;&#65292;&#20998;&#31163;&#35821;&#38899;&#35805;&#35821;&#20013;&#30340;&#19981;&#30456;&#20851;&#20449;&#24687;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#19981;&#21516;&#30340;&#35821;&#38899;&#30456;&#20851;&#20219;&#21153;&#19987;&#27880;&#20110;&#25552;&#21462;&#19981;&#21516;&#30340;&#35821;&#38899;&#34920;&#31034;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20854;&#20182;&#19981;&#30456;&#20851;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#20197;&#20419;&#36827;&#35821;&#38899;&#34920;&#31034;&#35299;&#32544;&#30340;&#30740;&#31350;&#12290;3D-Speaker&#21253;&#21547;&#36229;&#36807;10,000&#20010;&#35828;&#35805;&#20154;&#65292;&#27599;&#20010;&#35828;&#35805;&#20154;&#21516;&#26102;&#30001;&#22810;&#20010;&#35774;&#22791;&#24405;&#21046;&#65292;&#22312;&#19981;&#21516;&#30340;&#36317;&#31163;&#19978;&#65292;&#24182;&#19988;&#19968;&#20123;&#35828;&#35805;&#20154;&#20250;&#35762;&#22810;&#31181;&#26041;&#35328;&#12290;&#22810;&#32500;&#38899;&#39057;&#25968;&#25454;&#30340;&#21463;&#25511;&#32452;&#21512;&#20135;&#29983;&#20102;&#19968;&#20010;&#22810;&#26679;&#30340;&#28151;&#21512;&#35821;&#38899;&#34920;&#31034;&#32416;&#32544;&#30697;&#38453;&#65292;&#20174;&#32780;&#28608;&#21457;&#20986;&#35299;&#24320;&#23427;&#20204;&#30340;&#26377;&#36259;&#26041;&#27861;&#12290;3D-Speaker&#30340;&#22810;&#39046;&#22495;&#24615;&#36136;&#36824;&#20351;&#20854;&#25104;&#20026;&#35780;&#20272;&#22823;&#22411;&#36890;&#29992;&#35821;&#38899;&#27169;&#22411;&#21644;&#23454;&#39564;&#22495;&#22806;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#21512;&#36866;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentangling uncorrelated information in speech utterances is a crucial research topic within speech community. Different speech-related tasks focus on extracting distinct speech representations while minimizing the affects of other uncorrelated information. We present a large-scale speech corpus to facilitate the research of speech representation disentanglement. 3D-Speaker contains over 10,000 speakers, each of whom are simultaneously recorded by multiple Devices, locating at different Distances, and some speakers are speaking multiple Dialects. The controlled combinations of multi-dimensional audio data yield a matrix of a diverse blend of speech representation entanglement, thereby motivating intriguing methods to untangle them. The multi-domain nature of 3D-Speaker also makes it a suitable resource to evaluate large universal speech models and experiment methods of out-of-domain learning and self-supervised learning. https://3dspeaker.github.io/
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#32447;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20013;&#23458;&#25143;&#30340;&#21453;&#24212;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21672;&#35810;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23458;&#25143;&#22914;&#20309;&#21453;&#24212;&#21672;&#35810;&#24072;&#30340;&#31574;&#30053;&#20197;&#21450;&#36825;&#20123;&#21453;&#24212;&#23545;&#21672;&#35810;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#21672;&#35810;&#24072;&#22914;&#20309;&#26681;&#25454;&#23458;&#25143;&#30340;&#21453;&#24212;&#35843;&#25972;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.15334</link><description>&lt;p&gt;
&#22312;&#32447;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20013;&#29702;&#35299;&#23458;&#25143;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Understanding Client Reactions in Online Mental Health Counseling. (arXiv:2306.15334v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#27880;&#37322;&#26694;&#26550;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#32447;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#20013;&#23458;&#25143;&#30340;&#21453;&#24212;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#22823;&#35268;&#27169;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21672;&#35810;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23458;&#25143;&#22914;&#20309;&#21453;&#24212;&#21672;&#35810;&#24072;&#30340;&#31574;&#30053;&#20197;&#21450;&#36825;&#20123;&#21453;&#24212;&#23545;&#21672;&#35810;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25506;&#35752;&#20102;&#21672;&#35810;&#24072;&#22914;&#20309;&#26681;&#25454;&#23458;&#25143;&#30340;&#21453;&#24212;&#35843;&#25972;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36798;&#30340;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35835;&#21462;&#21442;&#19982;&#32773;&#30340;&#21453;&#24212;&#12290;&#36825;&#31181;&#21453;&#39304;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#24072;&#23588;&#20026;&#37325;&#35201;&#65292;&#20182;&#20204;&#24517;&#39035;&#20180;&#32454;&#32771;&#34385;&#23458;&#25143;&#30340;&#36827;&#23637;&#24182;&#30456;&#24212;&#22320;&#35843;&#25972;&#33258;&#24049;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#20851;&#20110;&#21672;&#35810;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;&#21672;&#35810;&#24072;&#30340;&#24178;&#39044;&#31574;&#30053;&#65292;&#32780;&#19981;&#26159;&#23458;&#25143;&#23545;&#24178;&#39044;&#30340;&#21453;&#24212;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#29702;&#35770;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#28085;&#30422;&#21672;&#35810;&#24072;&#30340;&#31574;&#30053;&#21644;&#23458;&#25143;&#21453;&#24212;&#34892;&#20026;&#12290;&#35813;&#26694;&#26550;&#24050;&#32463;&#22312;&#19968;&#20010;&#22312;&#32447;&#31119;&#21033;&#21672;&#35810;&#24179;&#21488;&#19978;&#25910;&#38598;&#20102;&#36807;&#21435;&#20004;&#24180;&#30340;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#21672;&#35810;&#25968;&#25454;&#38598;&#65292;&#24182;&#32463;&#36807;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#23458;&#25143;&#22914;&#20309;&#23545;&#21672;&#35810;&#24072;&#30340;&#31574;&#30053;&#20570;&#20986;&#21453;&#24212;&#65292;&#36825;&#26679;&#30340;&#21453;&#24212;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#30340;&#21672;&#35810;&#32467;&#26524;&#65292;&#20197;&#21450;&#21672;&#35810;&#24072;&#22914;&#20309;&#26681;&#25454;&#36825;&#20123;&#21453;&#24212;&#35843;&#25972;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#39033;&#30740;&#31350;&#21487;&#20197;&#24110;&#21161;&#21672;&#35810;&#24072;&#33258;&#21160;&#21270;&#22320;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#21672;&#35810;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication success relies heavily on reading participants' reactions. Such feedback is especially important for mental health counselors, who must carefully consider the client's progress and adjust their approach accordingly. However, previous NLP research on counseling has mainly focused on studying counselors' intervention strategies rather than their clients' reactions to the intervention. This work aims to fill this gap by developing a theoretically grounded annotation framework that encompasses counselors' strategies and client reaction behaviors. The framework has been tested against a large-scale, high-quality text-based counseling dataset we collected over the past two years from an online welfare counseling platform. Our study shows how clients react to counselors' strategies, how such reactions affect the final counseling outcomes, and how counselors can adjust their strategies in response to these reactions. We also demonstrate that this study can help counselors automat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;&#23545;BERT&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15298</link><description>&lt;p&gt;
BERT&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#8212;&#8212;&#36890;&#36807;&#24773;&#24863;&#35780;&#20998;&#22312;&#29616;&#23454;&#30340;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#20013;&#27979;&#37327;&#21644;&#20998;&#26512;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Gender Bias in BERT -- Measuring and Analysing Biases through Sentiment Rating in a Realistic Downstream Classification Task. (arXiv:2306.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15298
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;&#23545;BERT&#30340;&#24615;&#21035;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#20013;&#23384;&#22312;&#30528;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#24182;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#24212;&#29992;&#20013;&#20844;&#24320;&#21487;&#29992;&#65292;&#24182;&#19981;&#26029;&#36827;&#34892;&#24494;&#35843;&#12290;&#38543;&#30528;&#23427;&#20204;&#20855;&#22791;&#25235;&#21462;&#22797;&#26434;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#26377;&#23475;&#20559;&#35265;&#24456;&#21487;&#33021;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#36825;&#20123;&#27169;&#22411;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#26412;&#25991;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#26469;&#20998;&#26512;BERT&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65306;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#35265;&#24230;&#37327;&#26041;&#27861;&#65292;&#23558;&#20559;&#35265;&#23450;&#20041;&#20026;&#22899;&#24615;&#21644;&#30007;&#24615;&#26679;&#26412;&#29256;&#26412;&#22312;&#24773;&#24863;&#35780;&#20272;&#19978;&#30340;&#24046;&#24322;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#29616;&#23454;&#30340;IMDB&#30005;&#24433;&#20998;&#31867;&#22120;&#30340;&#20363;&#23376;&#20013;BERT&#30340;&#20559;&#35265;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;&#36890;&#36807;&#31995;&#32479;&#22320;&#21464;&#21270;&#35757;&#32451;&#27969;&#31243;&#30340;&#21508;&#20010;&#20803;&#32032;&#65292;&#25105;&#20204;&#21487;&#20197;&#23545;&#26368;&#32456;&#27169;&#22411;&#20559;&#35265;&#30340;&#24433;&#21709;&#20570;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19971;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;BERT&#27169;&#22411;&#30340;&#20061;&#31181;&#35757;&#32451;&#26465;&#20214;&#65292;&#21363;&#24635;&#20849;63&#20010;&#27169;&#22411;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26465;&#20214;&#37117;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#30340;&#20559;&#35265;&#28304;&#20110;&#20844;&#24320;&#30340;BERT&#27169;&#22411;&#32780;&#19981;&#26159;&#20219;&#21153;&#29305;&#23450;&#25968;&#25454;&#65292;&#24378;&#35843;&#20102;&#36127;&#36131;&#20219;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models are publicly available and constantly finetuned for various real-life applications. As they become capable of grasping complex contextual information, harmful biases are likely increasingly intertwined with those models. This paper analyses gender bias in BERT models with two main contributions: First, a novel bias measure is introduced, defining biases as the difference in sentiment valuation of female and male sample versions. Second, we comprehensively analyse BERT's biases on the example of a realistic IMDB movie classifier. By systematically varying elements of the training pipeline, we can conclude regarding their impact on the final model bias. Seven different public BERT models in nine training conditions, i.e. 63 models in total, are compared. Almost all conditions yield significant gender biases. Results indicate that reflected biases stem from public BERT models rather than task-specific data, emphasising the weight of responsible usage.
&lt;/p&gt;</description></item><item><title>IDOL&#26159;&#19968;&#31181;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#25351;&#26631;&#23548;&#21521;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#25351;&#26631;&#21644;&#36923;&#36753;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#22312;&#36923;&#36753;&#19978;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;IDOL&#22312;&#36923;&#36753;&#25512;&#29702;MRC&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32508;&#21512;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.15273</link><description>&lt;p&gt;
IDOL: &#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#25351;&#26631;&#23548;&#21521;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
IDOL: Indicator-oriented Logic Pre-training for Logical Reasoning. (arXiv:2306.15273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15273
&lt;/p&gt;
&lt;p&gt;
IDOL&#26159;&#19968;&#31181;&#38754;&#21521;&#36923;&#36753;&#25512;&#29702;&#30340;&#25351;&#26631;&#23548;&#21521;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36923;&#36753;&#25351;&#26631;&#21644;&#36923;&#36753;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#22312;&#36923;&#36753;&#19978;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;IDOL&#22312;&#36923;&#36753;&#25512;&#29702;MRC&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32508;&#21512;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#39046;&#22495;&#65292;&#29616;&#26377;&#31995;&#32479;&#22312;&#35768;&#22810;&#20219;&#21153;&#65288;&#22914;SQuAD&#65289;&#20013;&#30340;&#34920;&#29616;&#24050;&#32463;&#36229;&#36807;&#20102;&#20154;&#31867;&#24179;&#22343;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#24403;&#28041;&#21450;&#21040;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20173;&#26377;&#24456;&#22823;&#30340;&#36827;&#27493;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IDOL&#65288;InDicator-Oriented Logic Pre-training&#65289;&#30340;&#26131;&#20110;&#29702;&#35299;&#19988;&#39640;&#25928;&#30340;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#21033;&#29992;6&#31181;&#36923;&#36753;&#25351;&#26631;&#21644;&#36923;&#36753;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;LGP&#65288;LoGic Pre-training&#65289;&#22312;&#36923;&#36753;&#19978;&#24378;&#21270;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;IDOL&#22312;ReClor&#21644;LogiQA&#36825;&#20004;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36923;&#36753;&#25512;&#29702;MRC&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#34987;&#35777;&#26126;&#33021;&#22815;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#20854;&#20182;&#31867;&#22411;&#30340;MRC&#22522;&#20934;&#27979;&#35797;&#65292;&#22914;RACE&#21644;SQuAD 2.0&#65292;&#21516;&#26102;&#20445;&#25345;&#26377;&#31454;&#20105;&#21147;&#30340;&#32508;&#21512;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of machine reading comprehension (MRC), existing systems have surpassed the average performance of human beings in many tasks like SQuAD. However, there is still a long way to go when it comes to logical reasoning. Although some methods for it have been put forward, they either are designed in a quite complicated way or rely too much on external structures. In this paper, we proposed IDOL (InDicator-Oriented Logic Pre-training), an easy-to-understand but highly effective further pre-training task which logically strengthens the pre-trained models with the help of 6 types of logical indicators and a logically rich dataset LGP (LoGic Pre-training). IDOL achieves state-of-the-art performance on ReClor and LogiQA, the two most representative benchmarks in logical reasoning MRC, and is proven to be capable of generalizing to different pre-trained models and other types of MRC benchmarks like RACE and SQuAD 2.0 while keeping competitive general language understanding ability thr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#22122;&#38899;&#24341;&#36215;&#30340;&#19981;&#21516;&#20998;&#21106;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#22122;&#38899;&#24341;&#20837;&#19981;&#21516;&#30340;&#23376;&#35789;&#12289;&#23567;&#30340;&#23376;&#35789;&#29255;&#27573;&#25110;&#22823;&#37327;&#30340;&#39069;&#22806;&#23376;&#35789;&#65292;&#29305;&#21035;&#26159;&#25554;&#20837;&#22312;&#20854;&#20182;&#23376;&#35789;&#20013;&#65292;PLMs&#23558;&#26080;&#27861;&#20934;&#30830;&#35745;&#31639;&#21333;&#35789;&#30340;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.15268</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#22122;&#38899;&#19979;&#20174;&#25439;&#22351;&#30340;&#23376;&#35789;&#20013;&#24471;&#21040;&#27491;&#30830;&#30340;&#35821;&#20041;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Pretrained Language Models Derive Correct Semantics from Corrupt Subwords under Noise?. (arXiv:2306.15268v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#22122;&#38899;&#24341;&#36215;&#30340;&#19981;&#21516;&#20998;&#21106;&#38169;&#35823;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#22122;&#38899;&#24341;&#20837;&#19981;&#21516;&#30340;&#23376;&#35789;&#12289;&#23567;&#30340;&#23376;&#35789;&#29255;&#27573;&#25110;&#22823;&#37327;&#30340;&#39069;&#22806;&#23376;&#35789;&#65292;&#29305;&#21035;&#26159;&#25554;&#20837;&#22312;&#20854;&#20182;&#23376;&#35789;&#20013;&#65292;PLMs&#23558;&#26080;&#27861;&#20934;&#30830;&#35745;&#31639;&#21333;&#35789;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#23427;&#20204;&#23545;&#22122;&#38899;&#30340;&#25935;&#24863;&#24615;&#26368;&#36817;&#34987;&#35748;&#20026;&#19982;&#23376;&#35789;&#20998;&#21106;&#26377;&#20851;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20998;&#21106;&#30340;&#21738;&#20123;&#26041;&#38754;&#20250;&#24433;&#21709;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;PLMs&#23545;&#30001;&#22122;&#38899;&#24341;&#36215;&#30340;&#21508;&#31181;&#21463;&#25439;&#20998;&#21106;&#30340;&#40065;&#26834;&#24615;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#23376;&#35789;&#20998;&#21106;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#21517;&#20026;&#23545;&#27604;&#35789;&#27719;&#35821;&#20041;&#65288;CoLeS&#65289;&#25506;&#38024;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#24102;&#26377;&#35268;&#33539;-&#22122;&#38899;&#35789;&#23545;&#30340;&#23545;&#27604;&#25968;&#25454;&#38598;&#65292;&#23545;&#20998;&#21106;&#38169;&#35823;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#21327;&#35758;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22914;&#26524;&#22122;&#38899;&#24341;&#20837;&#23436;&#20840;&#19981;&#21516;&#30340;&#23376;&#35789;&#12289;&#23567;&#30340;&#23376;&#35789;&#29255;&#27573;&#25110;&#22823;&#37327;&#30340;&#39069;&#22806;&#23376;&#35789;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#25554;&#20837;&#22312;&#20854;&#20182;&#23376;&#35789;&#20013;&#26102;&#65292;PLMs&#23558;&#26080;&#27861;&#20934;&#30830;&#35745;&#31639;&#21333;&#35789;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
For Pretrained Language Models (PLMs), their susceptibility to noise has recently been linked to subword segmentation. However, it is unclear which aspects of segmentation affect their understanding. This study assesses the robustness of PLMs against various disrupted segmentation caused by noise. An evaluation framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS) probe, is proposed. It provides a systematic categorization of segmentation corruption under noise and evaluation protocols by generating contrastive datasets with canonical-noisy word pairs. Experimental results indicate that PLMs are unable to accurately compute word meanings if the noise introduces completely different subwords, small subword fragments, or a large number of additional subwords, particularly when they are inserted within other subwords.
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#23545;&#31070;&#32463;NLP&#27169;&#22411;&#30340;&#31163;&#22495;&#35780;&#20272;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#39046;&#22495;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#19977;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2306.15261</link><description>&lt;p&gt;
&#23545;&#31070;&#32463;NLP&#27169;&#22411;&#30340;&#31163;&#22495;&#35780;&#20272;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Out-of-Distribution Evaluation of Neural NLP Models. (arXiv:2306.15261v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#23545;&#31070;&#32463;NLP&#27169;&#22411;&#30340;&#31163;&#22495;&#35780;&#20272;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#39046;&#22495;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#19977;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#12289;&#39046;&#22495;&#27867;&#21270;&#21644;&#25968;&#25454;&#38598;&#20559;&#24046;&#26159;&#23545;&#31070;&#32463;NLP&#27169;&#22411;&#31163;&#22495;&#35780;&#20272;&#30340;&#19977;&#20010;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#36824;&#32570;&#20047;&#20851;&#20110;&#36825;&#19977;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#32508;&#21512;&#12289;&#25972;&#21512;&#24615;&#30340;&#35752;&#35770;&#12290;&#26412;&#35843;&#30740;&#23545;&#36825;&#19977;&#20010;&#30740;&#31350;&#26041;&#21521;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#30740;&#31350;&#26041;&#21521;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness, domain generalization and dataset biases are three active lines of research contributing to out-of-distribution (OOD) evaluation on neural NLP models. However, a comprehensive, integrated discussion of the three research lines is still lacking in the literature. In this survey, we 1) compare the three lines of research under a unifying definition; 2) summarize the data-generating processes and evaluation protocols for each line of research; and 3) emphasize the challenges and opportunities for future work.
&lt;/p&gt;</description></item><item><title>GroundNLQ&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#30340;&#21019;&#26032;&#26631;&#27880;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#30340;&#26631;&#27880;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#39057;&#20013;&#30340;&#20934;&#30830;&#26631;&#27880;&#65292;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.15255</link><description>&lt;p&gt;
GroundNLQ @ Ego4D&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;2023&#24180;
&lt;/p&gt;
&lt;p&gt;
GroundNLQ @ Ego4D Natural Language Queries Challenge 2023. (arXiv:2306.15255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15255
&lt;/p&gt;
&lt;p&gt;
GroundNLQ&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#30340;&#21019;&#26032;&#26631;&#27880;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#30340;&#26631;&#27880;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#22312;&#35270;&#39057;&#20013;&#30340;&#20934;&#30830;&#26631;&#27880;&#65292;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;CVPR 2023&#24180;&#30340;Ego4D&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#25361;&#25112;&#20013;&#30340;&#20896;&#20891;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#22312;&#35270;&#39057;&#20013;&#20934;&#30830;&#36827;&#34892;&#26631;&#27880;&#65292;&#38656;&#35201;&#19968;&#20010;&#26377;&#25928;&#30340;&#33258;&#25105;&#20013;&#24515;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#19968;&#20010;&#24378;&#22823;&#30340;&#26631;&#27880;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20004;&#38454;&#27573;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35270;&#39057;&#21465;&#36848;&#19978;&#35757;&#32451;&#33258;&#25105;&#20013;&#24515;&#29305;&#24449;&#25552;&#21462;&#22120;&#21644;&#26631;&#27880;&#27169;&#22411;&#65292;&#24182;&#22312;&#26631;&#27880;&#25968;&#25454;&#19978;&#36827;&#34892;&#36827;&#19968;&#27493;&#30340;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#27880;&#27169;&#22411;GroundNLQ&#65292;&#23427;&#37319;&#29992;&#20102;&#22810;&#27169;&#24577;&#22810;&#23610;&#24230;&#30340;&#26631;&#27880;&#27169;&#22359;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#35270;&#39057;&#21644;&#25991;&#26412;&#65292;&#24182;&#23545;&#21508;&#31181;&#26102;&#38388;&#38388;&#38548;&#65288;&#23588;&#20854;&#26159;&#38271;&#35270;&#39057;&#65289;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;&#30450;&#27979;&#38598;&#19978;&#65292;GroundNLQ&#22312;R1@IoU=0.3&#21644;R1@IoU=0.5&#20998;&#21035;&#36798;&#21040;&#20102;25.67&#21644;18.18&#65292;&#24182;&#19988;&#22312;&#21508;&#39033;&#25351;&#26631;&#19978;&#37117;&#26126;&#26174;&#36229;&#36807;&#20102;&#20854;&#20182;&#25152;&#26377;&#22242;&#38431;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;\url{https://github.com/houzhijian/GroundNLQ}&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this report, we present our champion solution for Ego4D Natural Language Queries (NLQ) Challenge in CVPR 2023. Essentially, to accurately ground in a video, an effective egocentric feature extractor and a powerful grounding model are required. Motivated by this, we leverage a two-stage pre-training strategy to train egocentric feature extractors and the grounding model on video narrations, and further fine-tune the model on annotated data. In addition, we introduce a novel grounding model GroundNLQ, which employs a multi-modal multi-scale grounding module for effective video and text fusion and various temporal intervals, especially for long videos. On the blind test set, GroundNLQ achieves 25.67 and 18.18 for R1@IoU=0.3 and R1@IoU=0.5, respectively, and surpasses all other teams by a noticeable margin. Our code will be released at\url{https://github.com/houzhijian/GroundNLQ}.
&lt;/p&gt;</description></item><item><title>MindDial&#26159;&#19968;&#20010;&#20351;&#29992;&#24515;&#26234;&#27169;&#25311;&#36827;&#34892;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#30340;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22330;&#26223;&#21270;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;</title><link>http://arxiv.org/abs/2306.15253</link><description>&lt;p&gt;
MindDial: &#24102;&#26377;&#24515;&#26234;&#27169;&#25311;&#30340;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#29992;&#20110;&#22330;&#26223;&#21270;&#31070;&#32463;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MindDial: Belief Dynamics Tracking with Theory-of-Mind Modeling for Situated Neural Dialogue Generation. (arXiv:2306.15253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15253
&lt;/p&gt;
&lt;p&gt;
MindDial&#26159;&#19968;&#20010;&#20351;&#29992;&#24515;&#26234;&#27169;&#25311;&#36827;&#34892;&#20449;&#24565;&#21160;&#24577;&#36319;&#36394;&#30340;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#22330;&#26223;&#21270;&#29615;&#22659;&#20013;&#29983;&#25104;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#20132;&#27969;&#20013;&#33258;&#30001;&#34920;&#36798;&#24847;&#20041;&#25110;&#20849;&#35782;&#30340;&#21516;&#26102;&#36827;&#34892;&#23545;&#35805;&#12290;&#23613;&#31649;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24182;&#26410;&#32771;&#34385;&#21040;&#20849;&#20139;&#30340;&#22330;&#26223;&#29615;&#22659;&#20013;&#20010;&#20307;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#24046;&#24322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MindDial&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22330;&#26223;&#21270;&#30340;&#33258;&#30001;&#23545;&#35805;&#26469;&#21327;&#21830;&#20849;&#35782;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#24515;&#26234;&#27169;&#22359;&#65292;&#21487;&#20197;&#36861;&#36394;&#19977;&#20010;&#23618;&#27425;&#30340;&#20449;&#24565;&#65292;&#21363;&#35828;&#35805;&#32773;&#30340;&#20449;&#24565;&#12289;&#35828;&#35805;&#32773;&#23545;&#21548;&#20247;&#20449;&#24565;&#30340;&#39044;&#27979;&#20197;&#21450;&#22522;&#20110;&#21069;&#20004;&#32773;&#20043;&#38388;&#30340;&#20849;&#21516;&#20449;&#24565;&#12290;&#28982;&#21518;&#65292;&#35828;&#35805;&#34892;&#20026;&#20998;&#31867;&#22836;&#23558;&#20915;&#23450;&#26159;&#21542;&#32487;&#32493;&#23545;&#35805;&#12289;&#32467;&#26463;&#27492;&#36718;&#23545;&#35805;&#25110;&#37319;&#21462;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20849;&#35782;&#23545;&#40784;&#30340;&#25968;&#25454;&#38598;MutualFriend&#65292;&#22686;&#21152;&#20102;&#20449;&#24565;&#21160;&#24577;&#27880;&#37322;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#20004;&#20010;&#20195;&#29702;&#20043;&#38388;&#30340;&#33258;&#30001;&#23545;&#35805;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#26379;&#21451;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24515;&#26234;&#29366;&#24577;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans talk in free-form while negotiating the expressed meanings or common ground. Despite the impressive conversational abilities of the large generative language models, they do not consider the individual differences in contextual understanding in a shared situated environment. In this work, we propose MindDial, a novel conversational framework that can generate situated free-form responses to negotiate common ground. We design an explicit mind module that can track three-level beliefs -- the speaker's belief, the speaker's prediction of the listener's belief, and the common belief based on the gap between the first two. Then the speaking act classification head will decide to continue to talk, end this turn, or take task-related action. We augment a common ground alignment dataset MutualFriend with belief dynamics annotation, of which the goal is to find a single mutual friend based on the free chat between two agents. Experiments show that our model with mental state modeling can
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#27169;&#22411;-&#26080;&#20851;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#26367;&#25442;&#35780;&#20998;&#22120;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15245</link><description>&lt;p&gt;
C-PMI: &#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#29992;&#20110;&#23545;&#35805;&#35780;&#20272;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
C-PMI: Conditional Pointwise Mutual Information for Turn-level Dialogue Evaluation. (arXiv:2306.15245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15245
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#30340;&#27169;&#22411;-&#26080;&#20851;&#26041;&#27861;&#65292;&#29992;&#20110;&#34913;&#37327;&#23545;&#35805;&#31995;&#32479;&#19982;&#29992;&#25143;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#36890;&#36807;&#26367;&#25442;&#35780;&#20998;&#22120;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;chatbot&#30340;&#26080;&#21442;&#32771;&#32423;&#23545;&#35805;&#35780;&#20272;&#25351;&#26631;&#19981;&#36275;&#20197;&#25429;&#25417;&#29992;&#25143;&#19982;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#19982;&#20154;&#31867;&#35780;&#20272;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#26041;&#27861;&#65292;&#21033;&#29992;&#26465;&#20214;&#28857;&#23545;&#28857;&#20114;&#20449;&#24687;&#65288;C-PMI&#65289;&#26469;&#24230;&#37327;&#31995;&#32479;&#21644;&#29992;&#25143;&#20043;&#38388;&#22522;&#20110;&#32473;&#23450;&#35780;&#20272;&#32500;&#24230;&#30340;&#23545;&#35805;&#20132;&#20114;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;FED&#23545;&#35805;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#35780;&#20272;&#31995;&#32479;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#22522;&#20110;&#36127;&#23545;&#25968;&#20284;&#28982;&#30340;&#35780;&#20998;&#22120;&#26367;&#25442;&#20026;&#25105;&#20204;&#25552;&#20986;&#30340;C-PMI&#35780;&#20998;&#22120;&#65292;&#25105;&#20204;&#22312;FED&#35780;&#20272;&#25351;&#26631;&#19978;&#30340;Spearman&#30456;&#20851;&#24615;&#24179;&#22343;&#30456;&#23545;&#25552;&#39640;&#20102;60.5%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#21457;&#24067;&#22312;https://github.com/renll/C-PMI&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing reference-free turn-level evaluation metrics for chatbots inadequately capture the interaction between the user and the system. Consequently, they often correlate poorly with human evaluations. To address this issue, we propose a novel model-agnostic approach that leverages Conditional Pointwise Mutual Information (C-PMI) to measure the turn-level interaction between the system and the user based on a given evaluation dimension. Experimental results on the widely used FED dialogue evaluation dataset demonstrate that our approach significantly improves the correlation with human judgment compared with existing evaluation systems. By replacing the negative log-likelihood-based scorer with our proposed C-PMI scorer, we achieve a relative 60.5% higher Spearman correlation on average for the FED evaluation metric. Our code is publicly available at https://github.com/renll/C-PMI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#35835;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65288;Ember&#65289;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#36890;&#36807;&#32435;&#20837;&#35835;&#32773;&#30340;&#38405;&#35835;&#21644;&#39564;&#35777;&#36807;&#31243;&#65292;&#20174;&#32452;&#20214;&#35282;&#24230;&#20840;&#38754;&#24314;&#27169;&#26032;&#38395;&#65292;&#20197;&#25552;&#39640;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#25928;&#26524;&#21644;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15231</link><description>&lt;p&gt;
&#27169;&#25311;&#35835;&#32773;&#34892;&#20026;&#36827;&#34892;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Emulating Reader Behaviors for Fake News Detection. (arXiv:2306.15231v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#35835;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65288;Ember&#65289;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#36890;&#36807;&#32435;&#20837;&#35835;&#32773;&#30340;&#38405;&#35835;&#21644;&#39564;&#35777;&#36807;&#31243;&#65292;&#20174;&#32452;&#20214;&#35282;&#24230;&#20840;&#38754;&#24314;&#27169;&#26032;&#38395;&#65292;&#20197;&#25552;&#39640;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#25928;&#26524;&#21644;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#30340;&#24191;&#27867;&#20256;&#25773;&#24433;&#21709;&#20102;&#25105;&#20204;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#65292;&#22240;&#27492;&#20551;&#26032;&#38395;&#26816;&#27979;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#24182;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#20174;&#21333;&#27169;&#24577;&#25110;&#22810;&#27169;&#24577;&#30340;&#35282;&#24230;&#23545;&#26032;&#38395;&#36827;&#34892;&#24314;&#27169;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#23454;&#36136;&#24615;&#30340;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#27169;&#24577;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#30340;&#32467;&#26524;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#35835;&#32773;&#22312;&#38405;&#35835;&#26032;&#38395;&#21644;&#39564;&#35777;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#34892;&#20026;&#12290;&#20030;&#20363;&#26469;&#35828;&#65292;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#25353;&#37096;&#20998;&#36827;&#34892;&#38405;&#35835;&#30340;&#36807;&#31243;&#65306;&#20174;&#26631;&#39064;&#12289;&#22270;&#29255;&#12289;&#35780;&#35770;&#21040;&#27491;&#25991;&#65292;&#36825;&#23545;&#20110;&#26356;&#32454;&#33268;&#22320;&#24314;&#27169;&#26032;&#38395;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#35835;&#32773;&#34892;&#20026;&#30340;&#26041;&#27861;&#65288;Ember&#65289;&#29992;&#20110;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#23558;&#35835;&#32773;&#30340;&#38405;&#35835;&#21644;&#39564;&#35777;&#36807;&#31243;&#32435;&#20837;&#23545;&#32452;&#20214;&#35282;&#24230;&#20840;&#38754;&#24314;&#27169;&#30340;&#26032;&#38395;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#20869;&#37096;&#32452;&#20214;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#20197;&#27169;&#25311;&#23545;&#27599;&#20010;&#32452;&#20214;&#36827;&#34892;&#35821;&#20041;&#20998;&#26512;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wide dissemination of fake news has affected our lives in many aspects, making fake news detection important and attracting increasing attention. Existing approaches make substantial contributions in this field by modeling news from a single-modal or multi-modal perspective. However, these modal-based methods can result in sub-optimal outcomes as they ignore reader behaviors in news consumption and authenticity verification. For instance, they haven't taken into consideration the component-by-component reading process: from the headline, images, comments, to the body, which is essential for modeling news with more granularity. To this end, we propose an approach of Emulating the behaviors of readers (Ember) for fake news detection on social media, incorporating readers' reading and verificating process to model news from the component perspective thoroughly. Specifically, we first construct intra-component feature extractors to emulate the behaviors of semantic analyzing on each co
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.15222</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#36827;&#34892;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Rank in Generative Retrieval. (arXiv:2306.15222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#21644;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#26469;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#26816;&#32034;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#25991;&#26412;&#26816;&#32034;&#33539;&#20363;&#65292;&#23427;&#23558;&#30456;&#20851;&#27573;&#33853;&#30340;&#26631;&#35782;&#31526;&#23383;&#31526;&#20018;&#29983;&#25104;&#20026;&#26816;&#32034;&#30446;&#26631;&#12290;&#36825;&#31181;&#33539;&#20363;&#21033;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20195;&#34920;&#20102;&#19982;&#20256;&#32479;&#30340;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#26377;&#25152;&#19981;&#21516;&#30340;&#26032;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#24555;&#36895;&#21457;&#23637;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#24335;&#26816;&#32034;&#26041;&#27861;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#21551;&#21457;&#24335;&#20989;&#25968;&#23558;&#39044;&#27979;&#30340;&#26631;&#35782;&#31526;&#36716;&#25442;&#20026;&#27573;&#33853;&#25490;&#24207;&#21015;&#34920;&#65292;&#36825;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#30340;&#23398;&#20064;&#30446;&#26631;&#19982;&#26399;&#26395;&#30340;&#27573;&#33853;&#25490;&#24207;&#30446;&#26631;&#20043;&#38388;&#20135;&#29983;&#20102;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25991;&#26412;&#29983;&#25104;&#30340;&#22266;&#26377;&#26333;&#20809;&#20559;&#24046;&#38382;&#39064;&#22312;&#29983;&#25104;&#24335;&#26816;&#32034;&#20013;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;LTRGR&#65292;&#23427;&#23558;&#29983;&#25104;&#24335;&#26816;&#32034;&#19982;&#32463;&#20856;&#30340;&#23398;&#20064;&#25490;&#24207;&#33539;&#20363;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#27573;&#33853;&#25490;&#24207;&#25439;&#22833;&#35757;&#32451;&#19968;&#20010;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#35813;&#25439;&#22833;&#30452;&#25509;&#20248;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#26397;&#30528;&#26368;&#20248;&#35299;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative retrieval is a promising new paradigm in text retrieval that generates identifier strings of relevant passages as the retrieval target. This paradigm leverages powerful generation models and represents a new paradigm distinct from traditional learning-to-rank methods. However, despite its rapid development, current generative retrieval methods are still limited. They typically rely on a heuristic function to transform predicted identifiers into a passage rank list, which creates a gap between the learning objective of generative retrieval and the desired passage ranking target. Moreover, the inherent exposure bias problem of text generation also persists in generative retrieval. To address these issues, we propose a novel framework, called LTRGR, that combines generative retrieval with the classical learning-to-rank paradigm. Our approach involves training an autoregressive model using a passage rank loss, which directly optimizes the autoregressive model toward the optimal 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#34255;&#23618;&#21644;&#36755;&#20986;&#23618;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#26469;&#32553;&#23567;&#27969;&#24335;&#19982;&#38750;&#27969;&#24335;&#36716;&#23548;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;19%&#30340;&#30456;&#23545;&#38477;&#20302;&#65292;&#24182;&#19988;&#23545;&#31532;&#19968;&#20010;&#20196;&#29260;&#30340;&#21709;&#24212;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2306.15171</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#26469;&#32553;&#23567;&#27969;&#24335;&#19982;&#38750;&#27969;&#24335;&#36716;&#23548;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Reducing the gap between streaming and non-streaming Transducer-based ASR by adaptive two-stage knowledge distillation. (arXiv:2306.15171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#38544;&#34255;&#23618;&#21644;&#36755;&#20986;&#23618;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#26469;&#32553;&#23567;&#27969;&#24335;&#19982;&#38750;&#27969;&#24335;&#36716;&#23548;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35813;&#26041;&#27861;&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;19%&#30340;&#30456;&#23545;&#38477;&#20302;&#65292;&#24182;&#19988;&#23545;&#31532;&#19968;&#20010;&#20196;&#29260;&#30340;&#21709;&#24212;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#23548;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26159;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#30340;&#19968;&#20010;&#20027;&#27969;&#26694;&#26550;&#12290;&#30001;&#20110;&#19978;&#19979;&#25991;&#26377;&#38480;&#65292;&#27969;&#24335;&#19982;&#38750;&#27969;&#24335;&#36716;&#23548;&#24335;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24615;&#33021;&#24046;&#36317;&#12290;&#20026;&#20102;&#32553;&#23567;&#36825;&#20010;&#24046;&#36317;&#65292;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#24335;&#26159;&#30830;&#20445;&#23427;&#20204;&#30340;&#38544;&#34255;&#23618;&#21644;&#36755;&#20986;&#20998;&#24067;&#19968;&#33268;&#65292;&#21487;&#20197;&#36890;&#36807;&#23618;&#32423;&#30693;&#35782;&#33976;&#39311;&#26469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#30830;&#20445;&#20998;&#24067;&#19968;&#33268;&#24615;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36755;&#20986;&#20998;&#24067;&#30340;&#23398;&#20064;&#20381;&#36182;&#20110;&#38544;&#34255;&#23618;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#20004;&#38454;&#27573;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#38544;&#34255;&#23618;&#23398;&#20064;&#21644;&#36755;&#20986;&#23618;&#23398;&#20064;&#12290;&#22312;&#21069;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#20855;&#26377;&#23436;&#25972;&#19978;&#19979;&#25991;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#22312;&#21518;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#24130;&#21464;&#25442;&#30340;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#26469;&#23398;&#20064;&#31283;&#23450;&#30340;&#36755;&#20986;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;19%&#30456;&#23545;&#38477;&#20302;&#65292;&#24182;&#19988;&#23545;&#20110;&#31532;&#19968;&#20010;&#20196;&#29260;&#30340;&#21709;&#24212;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transducer is one of the mainstream frameworks for streaming speech recognition. There is a performance gap between the streaming and non-streaming transducer models due to limited context. To reduce this gap, an effective way is to ensure that their hidden and output distributions are consistent, which can be achieved by hierarchical knowledge distillation. However, it is difficult to ensure the distribution consistency simultaneously because the learning of the output distribution depends on the hidden one. In this paper, we propose an adaptive two-stage knowledge distillation method consisting of hidden layer learning and output layer learning. In the former stage, we learn hidden representation with full context by applying mean square error loss function. In the latter stage, we design a power transformation based adaptive smoothness method to learn stable output distribution. It achieved 19\% relative reduction in word error rate, and a faster response for the first token compare
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;DSRM&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#20559;&#31227;&#39118;&#38505;&#32780;&#19981;&#26159;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.15164</link><description>&lt;p&gt;
&#29992;&#20998;&#24067;&#20559;&#31227;&#39118;&#38505;&#26368;&#23567;&#21270;&#22686;&#24378;&#25991;&#26412;&#23545;&#25239;&#35757;&#32451;&#65288;DSRM&#65289;
&lt;/p&gt;
&lt;p&gt;
DSRM: Boost Textual Adversarial Training with Distribution Shift Risk Minimization. (arXiv:2306.15164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;DSRM&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20998;&#24067;&#20559;&#31227;&#39118;&#38505;&#32780;&#19981;&#26159;&#20351;&#29992;&#23545;&#25239;&#26679;&#26412;&#26469;&#23545;&#25239;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#25913;&#21892;&#28145;&#24230;&#35821;&#35328;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#26368;&#20339;&#26041;&#27861;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#40065;&#26834;&#27169;&#22411;&#30340;&#20195;&#20215;&#26159;&#26102;&#38388;&#28040;&#32791;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#22810;&#27493;&#26799;&#24230;&#19978;&#21319;&#25110;&#21333;&#35789;&#26367;&#25442;&#26469;&#33719;&#21462;&#23545;&#25239;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#26679;&#26412;&#22312;&#35821;&#27861;&#36136;&#37327;&#21644;&#35821;&#20041;&#19968;&#33268;&#24615;&#26041;&#38754;&#23384;&#22312;&#32570;&#38519;&#65292;&#24433;&#21709;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#26377;&#25928;&#30340;&#36807;&#31243;&#65292;&#23558;&#23545;&#25239;&#35757;&#32451;&#25913;&#20026;&#21482;&#20351;&#29992;&#24178;&#20928;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;DSRM&#36890;&#36807;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#30340;&#27010;&#29575;&#20998;&#24067;&#32780;&#19981;&#26159;&#23427;&#20204;&#30340;&#23884;&#20837;&#26469;&#20272;&#35745;&#23545;&#25239;&#25439;&#22833;&#12290;&#36825;&#31181;&#20844;&#24335;&#21270;&#32467;&#26524;&#23548;&#33268;&#20102;&#19968;&#20010;&#22312;&#23545;&#25239;&#25915;&#20987;&#19979;&#26368;&#23567;&#21270;&#26399;&#26395;&#20840;&#23616;&#25439;&#22833;&#30340;&#40065;&#26834;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#38656;&#35201;&#23545;&#25239;&#26679;&#26412;&#65292;&#24182;&#19988;&#19982;&#24403;&#21069;&#26368;&#20339;&#23545;&#25239;&#35757;&#32451;&#30456;&#27604;&#65292;&#20943;&#23569;&#20102;&#39640;&#36798;70\%&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is one of the best-performing methods in improving the robustness of deep language models. However, robust models come at the cost of high time consumption, as they require multi-step gradient ascents or word substitutions to obtain adversarial samples. In addition, these generated samples are deficient in grammatical quality and semantic consistency, which impairs the effectiveness of adversarial training. To address these problems, we introduce a novel, effective procedure for instead adversarial training with only clean data. Our procedure, distribution shift risk minimization (DSRM), estimates the adversarial loss by perturbing the input data's probability distribution rather than their embeddings. This formulation results in a robust model that minimizes the expected global loss under adversarial attacks. Our approach requires zero adversarial samples for training and reduces time consumption by up to 70\% compared to current best-performing adversarial traini
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#32654;&#22269;&#25163;&#35821;-&#33521;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;YouTube-ASL&#65292;&#21253;&#21547;&#20102;&#32422;1000&#23567;&#26102;&#30340;&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21644;&#36229;&#36807;2500&#20301;&#29420;&#29305;&#30340;&#31614;&#21517;&#32773;&#12290;&#30740;&#31350;&#32773;&#22312;&#27492;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#25163;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.15162</link><description>&lt;p&gt;
YouTube-ASL:&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#32654;&#22269;&#25163;&#35821;-&#33521;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus. (arXiv:2306.15162v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#32654;&#22269;&#25163;&#35821;-&#33521;&#35821;&#24179;&#34892;&#35821;&#26009;&#24211;YouTube-ASL&#65292;&#21253;&#21547;&#20102;&#32422;1000&#23567;&#26102;&#30340;&#32654;&#22269;&#25163;&#35821;&#35270;&#39057;&#21644;&#36229;&#36807;2500&#20301;&#29420;&#29305;&#30340;&#31614;&#21517;&#32773;&#12290;&#30740;&#31350;&#32773;&#22312;&#27492;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#25163;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#23545;&#20110;&#25163;&#35821;&#30340;&#29942;&#39048;&#22312;&#20110;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;YouTube-ASL&#65292;&#19968;&#20010;&#26469;&#33258;YouTube&#30340;&#21253;&#21547;&#32654;&#22269;&#25163;&#35821;&#65288;ASL&#65289;&#35270;&#39057;&#21644;&#33521;&#25991;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#12289;&#24320;&#25918;&#39046;&#22495;&#30340;&#35821;&#26009;&#24211;&#12290;YouTube-ASL&#25317;&#26377;&#32422;1000&#23567;&#26102;&#30340;&#35270;&#39057;&#21644;&#36229;&#36807;2500&#20010;&#29420;&#29305;&#30340;&#31614;&#21517;&#32773;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;ASL&#25968;&#25454;&#38598;&#30340;3&#20493;&#20043;&#22810;&#65292;&#24182;&#19988;&#25317;&#26377;10&#20493;&#20110;&#20808;&#21069;ASL&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#31614;&#21517;&#32773;&#25968;&#37327;&#12290;&#25105;&#20204;&#22312;YouTube-ASL&#19978;&#35757;&#32451;&#20102;ASL&#21040;&#33521;&#35821;&#32763;&#35793;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#22312;How2Sign&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#22312;&#36825;&#37324;&#25105;&#20204;&#23454;&#29616;&#20102;&#26032;&#30340;&#24494;&#35843;&#26368;&#20339;&#25928;&#26524;12.39 BLEU&#65292;&#24182;&#39318;&#27425;&#25253;&#36947;&#20102;&#38646;-shot&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning for sign languages is bottlenecked by data. In this paper, we present YouTube-ASL, a large-scale, open-domain corpus of American Sign Language (ASL) videos and accompanying English captions drawn from YouTube. With ~1000 hours of videos and &gt;2500 unique signers, YouTube-ASL is ~3x as large and has ~10x as many unique signers as the largest prior ASL dataset. We train baseline models for ASL to English translation on YouTube-ASL and evaluate them on How2Sign, where we achieve a new finetuned state of the art of 12.39 BLEU and, for the first time, report zero-shot results.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20135;&#21697;&#35780;&#35770;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;BERT&#27169;&#22411;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;&#36328;&#22495;&#34892;&#20026;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#22312;&#23545;&#24212;&#22495;&#19978;&#30053;&#26377;&#25552;&#39640;&#65292;&#22810;&#22495;&#27169;&#22411;&#22312;&#35780;&#20272;&#22810;&#22495;&#25968;&#25454;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#24179;&#22343;&#27979;&#35797;&#20013;&#20063;&#26356;&#20248;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2306.15123</link><description>&lt;p&gt;
&#30740;&#31350;BERT&#22312;&#35780;&#35770;&#29702;&#35299;&#20013;&#30340;&#36328;&#22495;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Investigating Cross-Domain Behaviors of BERT in Review Understanding. (arXiv:2306.15123v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15123
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20135;&#21697;&#35780;&#35770;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#65292;BERT&#27169;&#22411;&#22312;&#19981;&#21516;&#22495;&#19978;&#30340;&#36328;&#22495;&#34892;&#20026;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#22312;&#23545;&#24212;&#22495;&#19978;&#30053;&#26377;&#25552;&#39640;&#65292;&#22810;&#22495;&#27169;&#22411;&#22312;&#35780;&#20272;&#22810;&#22495;&#25968;&#25454;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#24179;&#22343;&#27979;&#35797;&#20013;&#20063;&#26356;&#20248;&#12290;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20250;&#22686;&#21152;&#35745;&#31639;&#36164;&#28304;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#35770;&#20998;&#25968;&#39044;&#27979;&#38656;&#35201;&#29702;&#35299;&#35780;&#35770;&#25991;&#26412;&#65292;&#36825;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20851;&#38190;&#23454;&#38469;&#24212;&#29992;&#12290;&#30001;&#20110;&#20135;&#21697;&#35780;&#35770;&#20013;&#30340;&#25991;&#26412;&#39046;&#22495;&#19981;&#21516;&#65292;&#24120;&#35265;&#20570;&#27861;&#26159;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#35780;&#35770;&#19978;&#23545;BERT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#23545;BERT&#27169;&#22411;&#22312;&#20135;&#21697;&#35780;&#35770;&#29702;&#35299;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#36328;&#22495;&#34892;&#20026;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21333;&#22495;&#21644;&#22810;&#22495;&#20122;&#39532;&#36874;&#35780;&#35770;&#25968;&#25454;&#19978;&#24494;&#35843;&#30340;&#25991;&#26412;&#20998;&#31867;BERT&#27169;&#22411;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#23613;&#31649;&#21333;&#22495;&#27169;&#22411;&#22312;&#23545;&#24212;&#22495;&#19978;&#30340;&#24615;&#33021;&#30053;&#26377;&#25552;&#39640;&#65292;&#20294;&#22312;&#22810;&#22495;&#25968;&#25454;&#19978;&#35780;&#20272;&#26102;&#65292;&#22810;&#22495;&#27169;&#22411;&#20248;&#20110;&#21333;&#22495;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#21333;&#22495;&#27169;&#22411;&#26410;&#36827;&#34892;&#24494;&#35843;&#30340;&#21333;&#22495;&#25968;&#25454;&#19978;&#65292;&#20197;&#21450;&#22312;&#32771;&#34385;&#25152;&#26377;&#27979;&#35797;&#26102;&#30340;&#24179;&#22343;&#24615;&#33021;&#12290;&#34429;&#28982;&#36890;&#36807;&#21333;&#22495;&#27169;&#22411;&#24494;&#35843;&#21487;&#20197;&#30053;&#24494;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#35745;&#31639;&#36164;&#28304;&#20063;&#20250;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Review score prediction requires review text understanding, a critical real-world application of natural language processing. Due to dissimilar text domains in product reviews, a common practice is fine-tuning BERT models upon reviews of differing domains. However, there has not yet been an empirical study of cross-domain behaviors of BERT models in the various tasks of product review understanding. In this project, we investigate text classification BERT models fine-tuned on single-domain and multi-domain Amazon review data. In our findings, though single-domain models achieved marginally improved performance on their corresponding domain compared to multi-domain models, multi-domain models outperformed single-domain models when evaluated on multi-domain data, single-domain data the single-domain model was not fine-tuned on, and on average when considering all tests. Though slight increases in accuracy can be achieved through single-domain model fine-tuning, computational resources an
&lt;/p&gt;</description></item><item><title>FeedbackMap&#26159;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#20998;&#26512;&#24320;&#25918;&#24615;&#35843;&#26597;&#22238;&#31572;&#12290;&#23427;&#21487;&#20197;&#29983;&#25104;&#22810;&#23618;&#27425;&#30340;&#25688;&#35201;&#65292;&#35782;&#21035;&#26377;&#36259;&#30340;&#22238;&#31572;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#26041;&#24335;&#21487;&#35270;&#21270;&#22238;&#31572;&#31354;&#38388;&#12290;&#30001;&#20110;&#24635;&#32467;&#26041;&#27861;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#65292;&#38656;&#35201;&#23545;&#22238;&#31572;&#32773;&#30340;&#22768;&#38899;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.15112</link><description>&lt;p&gt;
FeedbackMap: &#19968;&#31181;&#20998;&#26512;&#24320;&#25918;&#24615;&#35843;&#26597;&#22238;&#31572;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
FeedbackMap: a tool for making sense of open-ended survey responses. (arXiv:2306.15112v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15112
&lt;/p&gt;
&lt;p&gt;
FeedbackMap&#26159;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#20998;&#26512;&#24320;&#25918;&#24615;&#35843;&#26597;&#22238;&#31572;&#12290;&#23427;&#21487;&#20197;&#29983;&#25104;&#22810;&#23618;&#27425;&#30340;&#25688;&#35201;&#65292;&#35782;&#21035;&#26377;&#36259;&#30340;&#22238;&#31572;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#26041;&#24335;&#21487;&#35270;&#21270;&#22238;&#31572;&#31354;&#38388;&#12290;&#30001;&#20110;&#24635;&#32467;&#26041;&#27861;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#65292;&#38656;&#35201;&#23545;&#22238;&#31572;&#32773;&#30340;&#22768;&#38899;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#24320;&#25918;&#24615;&#35843;&#26597;&#22238;&#31572;&#26159;&#31038;&#20250;&#31185;&#23398;&#23478;&#12289;&#38750;&#33829;&#21033;&#32452;&#32455;&#21644;&#25945;&#32946;&#26426;&#26500;&#38754;&#20020;&#30340;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20182;&#20204;&#24120;&#24120;&#38754;&#20020;&#30528;&#33719;&#21462;&#20016;&#23500;&#25968;&#25454;&#21644;&#38405;&#35835;&#21644;&#32534;&#30721;&#25991;&#26412;&#22238;&#31572;&#30340;&#36127;&#25285;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;FeedbackMap&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#32593;&#32476;&#30340;&#24037;&#20855;&#65292;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#20419;&#36827;&#24320;&#25918;&#24615;&#35843;&#26597;&#22238;&#31572;&#30340;&#20998;&#26512;&#12290;FeedbackMap&#35753;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#27425;&#19978;&#29983;&#25104;&#25688;&#35201;&#65292;&#35782;&#21035;&#26377;&#36259;&#30340;&#22238;&#31572;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#23884;&#20837;&#26041;&#24335;&#21487;&#35270;&#21270;&#22238;&#31572;&#31354;&#38388;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#22810;&#20010;&#35282;&#24230;&#23457;&#35270;&#35843;&#26597;&#32467;&#26524;&#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#24635;&#32467;&#26041;&#27861;&#24341;&#20837;&#30340;&#28508;&#22312;&#20559;&#35265;&#65292;&#24378;&#35843;&#20102;&#23545;&#34987;&#35843;&#26597;&#20154;&#22768;&#38899;&#30340;&#20195;&#34920;&#24615;&#21644;&#36951;&#28431;&#24615;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analyzing open-ended survey responses is a crucial yet challenging task for social scientists, non-profit organizations, and educational institutions, as they often face the trade-off between obtaining rich data and the burden of reading and coding textual responses. This demo introduces FeedbackMap, a web-based tool that uses natural language processing techniques to facilitate the analysis of open-ended survey responses. FeedbackMap lets researchers generate summaries at multiple levels, identify interesting response examples, and visualize the response space through embeddings. We discuss the importance of examining survey results from multiple perspectives and the potential biases introduced by summarization methods, emphasizing the need for critical evaluation of the representation and omission of respondent voices.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#23545;&#35805;&#35805;&#35821;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20174;&#32534;&#30721;&#21644;&#35299;&#30721;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#23545;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#32467;&#26500;&#21270;&#32534;&#30721;&#65292;&#20197;&#21450;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Chiu-Liu-Edmonds&#31639;&#27861;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#26469;&#20248;&#21270;&#23545;&#35805;&#20013;&#30340;&#38142;&#25509;&#21644;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.15103</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#23545;&#35805;&#35805;&#35821;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Structured Dialogue Discourse Parsing. (arXiv:2306.15103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#23545;&#35805;&#35805;&#35821;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#20174;&#32534;&#30721;&#21644;&#35299;&#30721;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#36890;&#36807;&#23545;&#37051;&#25509;&#30697;&#38453;&#36827;&#34892;&#32467;&#26500;&#21270;&#32534;&#30721;&#65292;&#20197;&#21450;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Chiu-Liu-Edmonds&#31639;&#27861;&#36827;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#26469;&#20248;&#21270;&#23545;&#35805;&#20013;&#30340;&#38142;&#25509;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#35805;&#35821;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#25214;&#21040;&#25152;&#26377;&#35805;&#35821;&#38142;&#25509;&#21644;&#23545;&#24212;&#20851;&#31995;&#65292;&#25581;&#31034;&#22810;&#21442;&#19982;&#32773;&#23545;&#35805;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#23558;&#27492;&#20219;&#21153;&#35270;&#20026;&#19968;&#31995;&#21015;&#29420;&#31435;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#65292;&#20854;&#20013;&#38142;&#25509;&#23384;&#22312;&#21644;&#20851;&#31995;&#34987;&#20998;&#21035;&#35299;&#30721;&#65292;&#35201;&#20040;&#23558;&#32534;&#30721;&#38480;&#21046;&#22312;&#20165;&#23616;&#37096;&#20132;&#20114;&#20013;&#65292;&#24573;&#30053;&#20102;&#25972;&#20307;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32534;&#30721;&#21644;&#35299;&#30721;&#20004;&#20010;&#35282;&#24230;&#25913;&#36827;&#20808;&#21069;&#24037;&#20316;&#30340;&#21407;&#29702;&#26041;&#27861;&#12290;&#20174;&#32534;&#30721;&#26041;&#38754;&#26469;&#30475;&#65292;&#25105;&#20204;&#23545;&#37051;&#25509;&#30697;&#38453;&#25191;&#34892;&#32467;&#26500;&#21270;&#32534;&#30721;&#65292;&#28982;&#21518;&#37319;&#29992;&#30697;&#38453;&#26641;&#23398;&#20064;&#31639;&#27861;&#65292;&#26681;&#25454;&#28508;&#22312;&#30340;&#26641;&#32423;&#20998;&#24067;&#20849;&#21516;&#20248;&#21270;&#23545;&#35805;&#20013;&#30340;&#25152;&#26377;&#38142;&#25509;&#21644;&#20851;&#31995;&#12290;&#20174;&#35299;&#30721;&#26041;&#38754;&#26469;&#30475;&#65292;&#25105;&#20204;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Chiu-Liu-Edmonds&#31639;&#27861;&#25191;&#34892;&#32467;&#26500;&#21270;&#25512;&#29702;&#65292;&#26126;&#30830;&#29983;&#25104;&#26368;&#20339;&#30340;&#24102;&#26631;&#31614;&#22810;&#26681;&#38750;&#25237;&#23556;&#29983;&#25104;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse~\emph{links} and corresponding~\emph{relations}. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#25903;&#25345;&#24615;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#32597;&#35265;&#19988;&#38271;&#23614;&#30340;&#35789;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15091</link><description>&lt;p&gt;
&#36890;&#36807;&#25903;&#25345;&#24615;&#39044;&#35757;&#32451;&#25968;&#25454;&#29702;&#35299;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding In-Context Learning via Supportive Pretraining Data. (arXiv:2306.15091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21457;&#29616;&#25903;&#25345;&#24615;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#32597;&#35265;&#19988;&#38271;&#23614;&#30340;&#35789;&#23545;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#31616;&#21333;&#23637;&#31034;&#23569;&#37327;&#31034;&#20363;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#27169;&#22411;&#20174;&#26410;&#34987;&#19987;&#38376;&#35757;&#32451;&#36807;&#36825;&#26679;&#30340;&#31034;&#20363;&#65292;ICL&#30340;&#33021;&#21147;&#22914;&#20309;&#20986;&#29616;&#20173;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#19982;&#25506;&#32034;ICL&#32972;&#21518;&#30340;&#38544;&#24335;&#26426;&#21046;&#30340;&#20808;&#21069;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#39044;&#35757;&#32451;&#25968;&#25454;&#26469;&#30740;&#31350;ICL&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#36845;&#20195;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#25214;&#21040;&#19968;&#23567;&#37096;&#20998;&#25903;&#25345;ICL&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#36825;&#20010;&#23567;&#30340;&#23376;&#38598;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#65292;&#26368;&#22810;&#21487;&#20197;&#25552;&#39640;18%&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#25903;&#25345;&#24615;&#23376;&#38598;&#19982;&#38543;&#26426;&#23376;&#38598;&#36827;&#34892;&#23545;&#27604;&#65292;&#21457;&#29616;&#65306;&#65288;1&#65289;&#25903;&#25345;&#24615;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#20219;&#21153;&#30340;&#22495;&#30456;&#20851;&#24615;&#24182;&#19981;&#39640;&#12290;&#65288;2&#65289;&#25903;&#25345;&#24615;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#26377;&#26356;&#22810;&#32597;&#35265;&#19988;&#38271;&#23614;&#30340;&#35789;&#12290;&#65288;3&#65289;&#25903;&#25345;&#24615;&#39044;&#35757;&#32451;&#25968;&#25454;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) improves language models' performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model's ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challengi
&lt;/p&gt;</description></item><item><title>WinoQueer&#26159;&#19968;&#20010;&#31038;&#21306;&#21327;&#21516;&#22522;&#20934;&#65292;&#26088;&#22312;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#23545;LGBTQ+&#31038;&#21306;&#26377;&#23475;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#25104;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#21453;&#21516;&#20559;&#35265;&#65292;&#36890;&#36807;&#22312;&#35813;&#31038;&#21306;&#25776;&#20889;&#25110;&#30001;&#35813;&#31038;&#21306;&#25104;&#21592;&#25776;&#20889;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.15087</link><description>&lt;p&gt;
WinoQueer&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21453;LGBTQ+&#20559;&#35265;&#30340;&#31038;&#21306;&#21327;&#21516;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
WinoQueer: A Community-in-the-Loop Benchmark for Anti-LGBTQ+ Bias in Large Language Models. (arXiv:2306.15087v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15087
&lt;/p&gt;
&lt;p&gt;
WinoQueer&#26159;&#19968;&#20010;&#31038;&#21306;&#21327;&#21516;&#22522;&#20934;&#65292;&#26088;&#22312;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#23545;LGBTQ+&#31038;&#21306;&#26377;&#23475;&#30340;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#29616;&#25104;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#21453;&#21516;&#20559;&#35265;&#65292;&#36890;&#36807;&#22312;&#35813;&#31038;&#21306;&#25776;&#20889;&#25110;&#30001;&#35813;&#31038;&#21306;&#25104;&#21592;&#25776;&#20889;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;WinoQueer&#65306;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#29992;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23384;&#22312;&#23545;LGBTQ+&#31038;&#21306;&#26377;&#23475;&#30340;&#20559;&#35265;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#20174;&#31038;&#21306;&#35843;&#26597;&#20013;&#29983;&#25104;&#30340;&#20559;&#35265;&#22522;&#20934;&#12290;&#25105;&#20204;&#23558;&#35813;&#22522;&#20934;&#24212;&#29992;&#20110;&#20960;&#20010;&#27969;&#34892;&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#29616;&#25104;&#27169;&#22411;&#26222;&#36941;&#23384;&#22312;&#30456;&#24403;&#22823;&#30340;&#21453;&#21516;&#20559;&#35265;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#35813;&#31038;&#21306;&#25776;&#20889;&#25110;&#30001;&#35813;&#31038;&#21306;&#25104;&#21592;&#25776;&#20889;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;LLM&#23545;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#20559;&#35265;&#65292;&#24182;&#19988;&#31038;&#21306;&#25104;&#21592;&#25776;&#20889;&#30340;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#27604;&#38750;&#31038;&#21306;&#25104;&#21592;&#25776;&#20889;&#30340;&#26032;&#38395;&#25991;&#26412;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#30340;&#31038;&#21306;&#21327;&#21516;&#22522;&#20934;&#24320;&#21457;&#26041;&#27861;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#19968;&#20010;&#34013;&#22270;&#65292;&#20197;&#24320;&#21457;&#38754;&#21521;&#20854;&#20182;&#36793;&#32536;&#21270;&#31038;&#21306;&#30340;&#12289;&#20197;&#31038;&#21306;&#20026;&#20013;&#24515;&#30340;&#12289;&#22522;&#20110;&#20260;&#23475;&#30340;LLM&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2306.15063</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#20219;&#21153;&#22810;&#26679;&#24615;&#19982;&#22238;&#24402;&#38382;&#39064;&#20013;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression. (arXiv:2306.15063v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15063
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#23637;&#29616;&#20102;&#38750;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#20854;&#22312;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#20197;&#19979;&#34920;&#29616;&#31867;&#20284;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#32780;&#22312;&#38408;&#20540;&#20197;&#19978;&#26126;&#26174;&#20248;&#20110;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#65292;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;transformer&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#38054;&#20329;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65288;ICL&#65289;&#65306;&#23427;&#20204;&#21487;&#20197;&#20174;&#20165;&#25552;&#20379;&#22312;&#25552;&#31034;&#20013;&#30340;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#26435;&#37325;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;ICL&#33021;&#22815;&#35299;&#20915;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#12289;&#22312;&#26412;&#36136;&#19978;&#19982;&#20043;&#21069;&#20219;&#21153;&#38750;&#24120;&#19981;&#21516;&#30340;&#26032;&#20219;&#21153;&#21527;&#65311;&#20026;&#20102;&#25506;&#32034;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#25913;&#21464;&#20219;&#21153;&#30340;&#22810;&#26679;&#24615;&#65292;&#30740;&#31350;&#20102;ICL&#22312;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;&#20102;&#20986;&#29616;ICL&#30340;&#20219;&#21153;&#22810;&#26679;&#24615;&#38408;&#20540;&#12290;&#22312;&#36825;&#20010;&#38408;&#20540;&#20197;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;transformer&#26080;&#27861;&#35299;&#20915;&#26410;&#35265;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#30340;&#34892;&#20026;&#31867;&#20284;&#20110;&#20855;&#26377;&#38750;&#22810;&#26679;&#24615;&#39044;&#35757;&#32451;&#20219;&#21153;&#20998;&#24067;&#20316;&#20026;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#12290;&#36229;&#36807;&#36825;&#20010;&#38408;&#20540;&#21518;&#65292;transformer&#26126;&#26174;&#20248;&#20110;&#36825;&#20010;&#20272;&#35745;&#22120;&#65307;&#23427;&#30340;&#34892;&#20026;&#19982;&#23725;&#22238;&#24402;&#19968;&#33268;&#65292;&#23545;$\textit{&#25152;&#26377;&#20219;&#21153;}$&#65292;&#21253;&#25324;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#20855;&#26377;&#39640;&#26031;&#20808;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally $\textit{new}$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a $\textit{task diversity threshold}$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the $\textit{non-diverse pretraining task distribution}$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over $\textit{all tasks}$, including those not seen during pretraining. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.15006</link><description>&lt;p&gt;
DNABERT-2:&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
DNABERT-2: Efficient Foundation Model and Benchmark For Multi-Species Genome. (arXiv:2306.15006v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15006
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DNABERT-2&#65292;&#19968;&#20010;&#29992;&#20110;&#22810;&#31181;&#29289;&#31181;&#22522;&#22240;&#32452;&#30340;&#39640;&#25928;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;&#20256;&#32479;&#30340;k-mer&#26631;&#35760;&#21270;&#65292;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#30721;&#22522;&#22240;&#32452;&#30340;&#35821;&#35328;&#22797;&#26434;&#24615;&#26159;&#29983;&#29289;&#23398;&#20013;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;DNABERT&#21644;Nucleotide Transformer&#31561;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;k-mer&#20316;&#20026;&#22522;&#22240;&#32452;&#35821;&#35328;&#30340;&#26631;&#35760;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;k-mer&#26631;&#35760;&#21270;&#24341;&#20837;&#30340;&#35745;&#31639;&#21644;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#26159;&#21457;&#23637;&#22823;&#35268;&#27169;&#22522;&#22240;&#32452;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#22522;&#22240;&#32452;&#26631;&#35760;&#21270;&#30340;&#27010;&#24565;&#21644;&#32463;&#39564;&#35265;&#35299;&#65292;&#22522;&#20110;&#27492;&#25552;&#20986;&#29992;&#22522;&#20110;&#32479;&#35745;&#30340;&#25968;&#25454;&#21387;&#32553;&#31639;&#27861;Byte Pair Encoding&#65288;BPE&#65289;&#26367;&#20195;k-mer&#26631;&#35760;&#21270;&#65292;BPE&#36890;&#36807;&#36845;&#20195;&#21512;&#24182;&#35821;&#26009;&#24211;&#20013;&#26368;&#39057;&#32321;&#20849;&#21516;&#20986;&#29616;&#30340;&#22522;&#22240;&#32452;&#29255;&#27573;&#26469;&#26500;&#24314;&#26631;&#35760;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;BPE&#19981;&#20165;&#20811;&#26381;&#20102;k-mer&#26631;&#35760;&#21270;&#30340;&#23616;&#38480;&#24615;&#65292;&#36824;&#33021;&#20174;&#38750;&#37325;&#21472;&#26631;&#35760;&#21270;&#30340;&#35745;&#31639;&#25928;&#29575;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decoding the linguistic intricacies of the genome is a crucial problem in biology, and pre-trained foundational models such as DNABERT and Nucleotide Transformer have made significant strides in this area. Existing works have largely hinged on k-mer, fixed-length permutations of A, T, C, and G, as the token of the genome language due to its simplicity. However, we argue that the computation and sample inefficiencies introduced by k-mer tokenization are primary obstacles in developing large genome foundational models. We provide conceptual and empirical insights into genome tokenization, building on which we propose to replace k-mer tokenization with Byte Pair Encoding (BPE), a statistics-based data compression algorithm that constructs tokens by iteratively merging the most frequent co-occurring genome segment in the corpus. We demonstrate that BPE not only overcomes the limitations of k-mer tokenization but also benefits from the computational efficiency of non-overlapping tokenizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2306.14939</link><description>&lt;p&gt;
&#23884;&#20837;&#34701;&#21512;&#30340;&#33402;&#26415;&#65306;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Art of Embedding Fusion: Optimizing Hate Speech Detection. (arXiv:2306.14939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20248;&#21270;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;&#23884;&#20837;&#30340;&#32452;&#21512;&#20250;&#30053;&#24494;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#24182;&#19988;&#32452;&#21512;&#26041;&#24335;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#38656;&#35201;&#25429;&#25417;&#35821;&#35328;&#21644;&#35821;&#22659;&#32454;&#24494;&#24046;&#21035;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#25913;&#36827;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26377;&#25928;&#22320;&#32452;&#21512;PLMs&#30340;&#34920;&#31034;&#21644;&#21033;&#29992;&#23427;&#20204;&#30340;&#20114;&#34917;&#20248;&#21183;&#30340;&#26041;&#27861;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#31181;PLMs&#32452;&#21512;&#25216;&#26415;&#30340;&#26041;&#24335;&#65292;&#24182;&#20840;&#38754;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32452;&#21512;&#23884;&#20837;&#21487;&#20197;&#30053;&#24494;&#25913;&#21892;&#24615;&#33021;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#65292;&#32452;&#21512;&#26041;&#24335;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;&#25105;&#20204;&#36824;&#22312;https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection&#19978;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection is a challenging natural language processing task that requires capturing linguistic and contextual nuances. Pre-trained language models (PLMs) offer rich semantic representations of text that can improve this task. However there is still limited knowledge about ways to effectively combine representations across PLMs and leverage their complementary strengths. In this work, we shed light on various combination techniques for several PLMs and comprehensively analyze their effectiveness. Our findings show that combining embeddings leads to slight improvements but at a high computational cost and the choice of combination has marginal effect on the final outcome. We also make our codebase public at https://github.com/aflah02/The-Art-of-Embedding-Fusion-Optimizing-Hate-Speech-Detection .
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;</title><link>http://arxiv.org/abs/2306.14933</link><description>&lt;p&gt;
&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Integrating Bidirectional Long Short-Term Memory with Subword Embedding for Authorship Attribution. (arXiv:2306.14933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#22788;&#29702;&#25991;&#26412;&#20013;&#30340;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#32473;&#23450;&#25991;&#26412;&#25991;&#26723;&#30340;&#20316;&#32773;&#36523;&#20221;&#26159;&#20316;&#32773;&#24402;&#23646;&#38382;&#39064;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#21151;&#22320;&#20351;&#29992;&#22810;&#26679;&#30340;&#22522;&#20110;&#35789;&#30340;&#39118;&#26684;&#26631;&#35760;&#26469;&#22788;&#29702;&#20316;&#32773;&#24402;&#23646;&#30340;&#20869;&#22312;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35789;&#30340;&#20316;&#32773;&#24402;&#23646;&#31995;&#32479;&#30340;&#24615;&#33021;&#21463;&#21040;&#35757;&#32451;&#35821;&#26009;&#24211;&#35789;&#27719;&#30340;&#38480;&#21046;&#12290;&#25991;&#29486;&#25512;&#33616;&#20102;&#20197;&#23383;&#31526;&#20026;&#22522;&#30784;&#30340;&#39118;&#26684;&#26631;&#35760;&#20316;&#20026;&#20811;&#26381;&#38544;&#21547;&#35789;&#38382;&#39064;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#23383;&#31526;&#30340;&#26041;&#27861;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#25991;&#26412;&#20013;&#30340;&#35789;&#30340;&#39034;&#24207;&#20851;&#31995;&#65292;&#36825;&#26159;&#36827;&#19968;&#27493;&#25913;&#21892;&#30340;&#38590;&#39064;&#12290;&#26412;&#25991;&#35752;&#35770;&#30340;&#38382;&#39064;&#26159;&#26159;&#21542;&#21487;&#20197;&#35299;&#20915;&#25991;&#26412;&#25991;&#26723;&#20013;&#38544;&#21547;&#35789;&#30340;&#27495;&#20041;&#24615;&#65292;&#21516;&#26102;&#20445;&#30041;&#35789;&#30340;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;BLSTM&#65289;&#19982;&#20108;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#26469;&#25429;&#25417;&#39034;&#24207;&#19978;&#19979;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of unveiling the author of a given text document from multiple candidate authors is called authorship attribution. Manifold word-based stylistic markers have been successfully used in deep learning methods to deal with the intrinsic problem of authorship attribution. Unfortunately, the performance of word-based authorship attribution systems is limited by the vocabulary of the training corpus. Literature has recommended character-based stylistic markers as an alternative to overcome the hidden word problem. However, character-based methods often fail to capture the sequential relationship of words in texts which is a chasm for further improvement. The question addressed in this paper is whether it is possible to address the ambiguity of hidden words in text documents while preserving the sequential context of words. Consequently, a method based on bidirectional long short-term memory (BLSTM) with a 2-dimensional convolutional neural network (CNN) is proposed to capture sequ
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;GPT-3.5&#22312;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.14924</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#28436;&#32462;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Content Analysis: Using Large Language Models to Support Deductive Coding. (arXiv:2306.14924v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#26102;&#38388;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#21644;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#65292;GPT-3.5&#22312;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28436;&#32462;&#32534;&#30721;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#29992;&#20110;&#30830;&#23450;&#25991;&#26723;&#20013;&#20027;&#39064;&#30340;&#26222;&#36941;&#24615;&#12290;&#23613;&#31649;&#26377;&#29992;&#65292;&#28436;&#32462;&#32534;&#30721;&#36890;&#24120;&#26159;&#32321;&#29712;&#19988;&#32791;&#26102;&#30340;&#65292;&#22240;&#20026;&#23427;&#35201;&#27714;&#30740;&#31350;&#20154;&#21592;&#38405;&#35835;&#12289;&#35299;&#37322;&#24182;&#21487;&#38752;&#22320;&#23545;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#36827;&#34892;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#26159;&#19968;&#31867;&#24555;&#36895;&#21457;&#23637;&#30340;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#26469;&#20943;&#23569;&#28436;&#32462;&#32534;&#30721;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21516;&#26102;&#20445;&#30041;&#20256;&#32479;&#20869;&#23481;&#20998;&#26512;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;LLM&#36741;&#21161;&#20869;&#23481;&#20998;&#26512;&#65288;LACA&#65289;&#65292;&#24182;&#20351;&#29992;GPT-3.5&#22312;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#28436;&#32462;&#32534;&#30721;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#28145;&#20837;&#26696;&#20363;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#20010;&#32463;&#39564;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;LACA&#22312;4&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;GPT-3.5&#22312;&#19981;&#21516;&#28436;&#32462;&#32534;&#30721;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deductive coding is a widely used qualitative research method for determining the prevalence of themes across documents. While useful, deductive coding is often burdensome and time consuming since it requires researchers to read, interpret, and reliably categorize a large body of unstructured text documents. Large language models (LLMs), like ChatGPT, are a class of quickly evolving AI tools that can perform a range of natural language processing and reasoning tasks. In this study, we explore the use of LLMs to reduce the time it takes for deductive coding while retaining the flexibility of a traditional content analysis. We outline the proposed approach, called LLM-assisted content analysis (LACA), along with an in-depth case study using GPT-3.5 for LACA on a publicly available deductive coding data set. Additionally, we conduct an empirical benchmark using LACA on 4 publicly available data sets to assess the broader question of how well GPT-3.5 performs across a range of deductive co
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;ChatGPT&#36827;&#34892;&#20135;&#21697;&#20449;&#24687;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#27867;&#21270;&#21040;&#26410;&#30693;&#23646;&#24615;&#21644;&#23646;&#24615;&#20540;&#30340;&#22256;&#38590;&#65292;&#20026;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.14921</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#20135;&#21697;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Product Information Extraction using ChatGPT. (arXiv:2306.14921v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14921
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#20135;&#21697;&#20449;&#24687;&#25552;&#21462;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#21644;&#27867;&#21270;&#21040;&#26410;&#30693;&#23646;&#24615;&#21644;&#23646;&#24615;&#20540;&#30340;&#22256;&#38590;&#65292;&#20026;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23646;&#24615;/&#20540;&#23545;&#30340;&#24418;&#24335;&#21576;&#29616;&#30340;&#32467;&#26500;&#21270;&#20135;&#21697;&#25968;&#25454;&#26159;&#35768;&#22810;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#30340;&#22522;&#30784;&#65292;&#20363;&#22914;&#20998;&#38754;&#20135;&#21697;&#25628;&#32034;&#12289;&#20135;&#21697;&#27604;&#36739;&#21644;&#20135;&#21697;&#25512;&#33616;&#12290;&#20135;&#21697;&#25253;&#20215;&#36890;&#24120;&#21482;&#21253;&#21547;&#20197;&#26631;&#39064;&#25110;&#33258;&#30001;&#25991;&#26412;&#24418;&#24335;&#21576;&#29616;&#30340;&#20135;&#21697;&#23646;&#24615;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#22240;&#27492;&#65292;&#20174;&#25991;&#26412;&#20135;&#21697;&#25551;&#36848;&#20013;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#23545;&#20110;&#30005;&#23376;&#21830;&#21153;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#34920;&#29616;&#20986;&#33394;&#65292;&#26368;&#20808;&#36827;&#30340;&#20135;&#21697;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;&#20219;&#21153;&#29305;&#23450;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#38590;&#20197;&#25512;&#24191;&#21040;&#35757;&#32451;&#25968;&#25454;&#20013;&#27809;&#26377;&#21253;&#21547;&#30340;&#20998;&#24067;&#20043;&#22806;&#30340;&#23646;&#24615;&#21644;&#23646;&#24615;&#20540;&#12290;&#30001;&#20110;&#22312;&#22823;&#37327;&#25991;&#26412;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#21450;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#23548;&#33268;&#30340;&#26032;&#20852;&#25928;&#26524;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35299;&#20915;&#36825;&#20004;&#20010;&#32570;&#28857;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#22312;&#25552;&#21462;&#23646;&#24615;/&#20540;&#23545;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured product data in the form of attribute/value pairs is the foundation of many e-commerce applications such as faceted product search, product comparison, and product recommendation. Product offers often only contain textual descriptions of the product attributes in the form of titles or free text. Hence, extracting attribute/value pairs from textual product descriptions is an essential enabler for e-commerce applications. In order to excel, state-of-the-art product information extraction methods require large quantities of task-specific training data. The methods also struggle with generalizing to out-of-distribution attributes and attribute values that were not a part of the training data. Due to being pre-trained on huge amounts of text as well as due to emergent effects resulting from the model size, Large Language Models like ChatGPT have the potential to address both of these shortcomings. This paper explores the potential of ChatGPT for extracting attribute/value pairs f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#65292;&#23454;&#29616;&#23545;&#35838;&#22530;&#35752;&#35770;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#21516;&#26102;&#25351;&#20986;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;NLP&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#26631;&#20934;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.14918</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36827;&#34892;&#35838;&#22530;&#35752;&#35770;&#30340;&#33258;&#21160;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Utilizing Natural Language Processing for Automated Assessment of Classroom Discussion. (arXiv:2306.14918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#65292;&#23454;&#29616;&#23545;&#35838;&#22530;&#35752;&#35770;&#36136;&#37327;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#21516;&#26102;&#25351;&#20986;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#30340;NLP&#26041;&#27861;&#23545;&#19981;&#21516;&#30340;&#26631;&#20934;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20005;&#26684;&#32780;&#20114;&#21160;&#30340;&#35838;&#22530;&#35752;&#35770;&#23545;&#20110;&#23398;&#20064;&#33267;&#20851;&#37325;&#35201;&#65292;&#21516;&#26102;&#20063;&#26159;&#22823;&#22810;&#25968;&#25945;&#23398;&#24178;&#39044;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23545;&#35752;&#35770;&#36136;&#37327;&#36827;&#34892;&#35268;&#27169;&#21270;&#30340;&#27491;&#24335;&#35780;&#20272;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#32773;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#20197;&#33258;&#21160;&#29983;&#25104;&#35838;&#22530;&#25991;&#26412;&#35752;&#35770;&#36136;&#37327;&#30340;&#32454;&#21270;&#26631;&#20934;&#24471;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21253;&#21547;&#36229;&#36807;18000&#36718;&#27425;&#12289;&#27880;&#37322;&#26377;&#35814;&#32454;&#30340;&#25945;&#23398;&#20998;&#26512;&#36816;&#21160;&#65288;ATM&#65289;&#20195;&#30721;&#30340;90&#20010;&#35838;&#22530;&#35752;&#35770;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#32858;&#28966;&#20110;&#22235;&#20010;&#25945;&#23398;&#36136;&#37327;&#35780;&#20272;&#65288;IQA&#65289;&#26631;&#20934;&#12290;&#23613;&#31649;&#25968;&#25454;&#37327;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#19968;&#20123;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20063;&#26263;&#31034;&#20854;&#20182;&#26631;&#20934;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#26576;&#20123;NLP&#26041;&#27861;&#22312;&#26576;&#20123;&#26631;&#20934;&#19978;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rigorous and interactive class discussions that support students to engage in high-level thinking and reasoning are essential to learning and are a central component of most teaching interventions. However, formally assessing discussion quality 'at scale' is expensive and infeasible for most researchers. In this work, we experimented with various modern natural language processing (NLP) techniques to automatically generate rubric scores for individual dimensions of classroom text discussion quality. Specifically, we worked on a dataset of 90 classroom discussion transcripts consisting of over 18000 turns annotated with fine-grained Analyzing Teaching Moves (ATM) codes and focused on four Instructional Quality Assessment (IQA) rubrics. Despite the limited amount of data, our work shows encouraging results in some of the rubrics while suggesting that there is room for improvement in the others. We also found that certain NLP approaches work better for certain rubrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;wh-&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#21644;&#21465;&#20107;&#35201;&#32032;&#21516;&#26102;&#25511;&#21046;&#38382;&#39064;&#29983;&#25104;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2306.14917</link><description>&lt;p&gt;
&#36808;&#21521;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#20016;&#23500;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Enriched Controllability for Educational Question Generation. (arXiv:2306.14917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;&#25945;&#32946;&#38382;&#39064;&#29983;&#25104;&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25511;&#21046;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;wh-&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#21644;&#21465;&#20107;&#35201;&#32032;&#21516;&#26102;&#25511;&#21046;&#38382;&#39064;&#29983;&#25104;&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#38382;&#39064;&#65288;QG&#65289;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#26681;&#25454;&#36755;&#20837;&#65288;&#36890;&#24120;&#30001;&#25991;&#26412;&#21644;&#30446;&#26631;&#31572;&#26696;&#32452;&#25104;&#65289;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#12290;&#36817;&#26399;&#20851;&#20110;QG&#30340;&#30740;&#31350;&#26088;&#22312;&#25511;&#21046;&#29983;&#25104;&#38382;&#39064;&#30340;&#31867;&#22411;&#65292;&#20197;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#12290;&#25945;&#32946;QG&#20013;&#21487;&#25511;&#24615;&#30340;&#19968;&#20010;&#26174;&#33879;&#20363;&#23376;&#26159;&#29983;&#25104;&#28041;&#21450;&#29305;&#23450;&#21465;&#20107;&#35201;&#32032;&#30340;&#38382;&#39064;&#65292;&#20363;&#22914;&#22240;&#26524;&#20851;&#31995;&#12289;&#32467;&#26524;&#35299;&#20915;&#25110;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24341;&#23548;&#23646;&#24615;&#65288;&#38382;&#39064;&#26126;&#30830;&#24615;&#65289;&#26469;&#20016;&#23500;QG&#30340;&#21487;&#25511;&#24615;&#12290;&#25105;&#20204;&#25552;&#35758;&#36890;&#36807;&#25511;&#21046;&#20174;&#36866;&#21512;&#20799;&#31461;&#30340;&#25925;&#20107;&#20013;&#29983;&#25104;&#26126;&#30830;&#21644;&#38544;&#21547;&#30340;wh-&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#36890;&#36807;&#38382;&#39064;&#26126;&#30830;&#24615;&#20197;&#21450;&#19982;&#21478;&#19968;&#20010;&#30446;&#26631;&#23646;&#24615;&#65288;&#38382;&#39064;&#30340;&#21465;&#20107;&#35201;&#32032;&#65289;&#21516;&#26102;&#25511;&#21046;QG&#30340;&#21021;&#27493;&#35777;&#25454;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#22312;github.com/bernardoleite/question-generation-control&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question Generation (QG) is a task within Natural Language Processing (NLP) that involves automatically generating questions given an input, typically composed of a text and a target answer. Recent work on QG aims to control the type of generated questions so that they meet educational needs. A remarkable example of controllability in educational QG is the generation of questions underlying certain narrative elements, e.g., causal relationship, outcome resolution, or prediction. This study aims to enrich controllability in QG by introducing a new guidance attribute: question explicitness. We propose to control the generation of explicit and implicit wh-questions from children-friendly stories. We show preliminary evidence of controlling QG via question explicitness alone and simultaneously with another target attribute: the question's narrative element. The code is publicly available at github.com/bernardoleite/question-generation-control.
&lt;/p&gt;</description></item><item><title>FSUIE&#26159;&#19968;&#31181;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#30340;&#26032;&#22411;&#27169;&#31946;&#36328;&#24230;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#31946;&#36328;&#24230;&#25439;&#22833;&#21644;&#27169;&#31946;&#36328;&#24230;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#22312;&#24555;&#36895;&#25910;&#25947;&#21644;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#36718;&#25968;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20449;&#24687;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14913</link><description>&lt;p&gt;
FSUIE:&#19968;&#31181;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#30340;&#26032;&#22411;&#27169;&#31946;&#36328;&#24230;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
FSUIE: A Novel Fuzzy Span Mechanism for Universal Information Extraction. (arXiv:2306.14913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14913
&lt;/p&gt;
&lt;p&gt;
FSUIE&#26159;&#19968;&#31181;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#30340;&#26032;&#22411;&#27169;&#31946;&#36328;&#24230;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#27169;&#31946;&#36328;&#24230;&#25439;&#22833;&#21644;&#27169;&#31946;&#36328;&#24230;&#27880;&#24847;&#21147;&#65292;&#33021;&#22815;&#22312;&#24555;&#36895;&#25910;&#25947;&#21644;&#23569;&#37327;&#25968;&#25454;&#35757;&#32451;&#36718;&#25968;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20449;&#24687;&#25277;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#65288;UIE&#65289;&#20316;&#20026;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#24050;&#32463;&#21462;&#24471;&#20102;&#24191;&#27867;&#25104;&#21151;&#65292;&#20294;UIE&#27169;&#22411;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20363;&#22914;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23427;&#20204;&#36807;&#20110;&#20381;&#36182;&#20110;&#25968;&#25454;&#20013;&#30340;&#36328;&#24230;&#36793;&#30028;&#65292;&#36825;&#24182;&#19981;&#21453;&#26144;&#36328;&#24230;&#27880;&#37322;&#30340;&#30495;&#23454;&#25361;&#25112;&#12290;&#24494;&#23567;&#30340;&#20301;&#32622;&#35843;&#25972;&#20063;&#21487;&#20197;&#28385;&#36275;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;UIE&#27169;&#22411;&#32570;&#20047;&#23545;&#20449;&#24687;&#25277;&#21462;&#20013;&#26377;&#38480;&#30340;&#36328;&#24230;&#38271;&#24230;&#29305;&#24449;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#31946;&#36328;&#24230;&#26222;&#36866;&#20449;&#24687;&#25277;&#21462;&#65288;FSUIE&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#27169;&#31946;&#36328;&#24230;&#25439;&#22833;&#21644;&#27169;&#31946;&#36328;&#24230;&#27880;&#24847;&#21147;&#20004;&#20010;&#27010;&#24565;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#20027;&#35201;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25968;&#25454;&#37327;&#21644;&#35757;&#32451;&#36718;&#25968;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;FSUIE&#22312;&#24555;&#36895;&#25910;&#25947;&#21644;&#24378;&#22823;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;FSUIE&#30340;&#26377;&#25928;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Universal Information Extraction (UIE) has been introduced as a unified framework for various Information Extraction (IE) tasks and has achieved widespread success. Despite this, UIE models have limitations. For example, they rely heavily on span boundaries in the data during training, which does not reflect the reality of span annotation challenges. Slight adjustments to positions can also meet requirements. Additionally, UIE models lack attention to the limited span length feature in IE. To address these deficiencies, we propose the Fuzzy Span Universal Information Extraction (FSUIE) framework. Specifically, our contribution consists of two concepts: fuzzy span loss and fuzzy span attention. Our experimental results on a series of main IE tasks show significant improvement compared to the baseline, especially in terms of fast convergence and strong performance with small amounts of data and training epochs. These results demonstrate the effectiveness and generalization of FSUIE in di
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#21516;&#20276;&#36741;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21516;&#20276;&#36741;&#23548;&#20114;&#21160;&#20013;&#30340;&#25514;&#36766;&#12290;&#26368;&#20339;&#34920;&#29616;&#30340;&#26159;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#27604;&#29616;&#26377;&#22522;&#20934;&#26356;&#22909;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2306.14911</link><description>&lt;p&gt;
&#22312;&#21516;&#20276;&#36741;&#23548;&#20114;&#21160;&#20013;&#35782;&#21035;&#25514;&#36766;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
"You might think about slightly revising the title": identifying hedges in peer-tutoring interactions. (arXiv:2306.14911v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22810;&#27169;&#24577;&#21516;&#20276;&#36741;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#35782;&#21035;&#21516;&#20276;&#36741;&#23548;&#20114;&#21160;&#20013;&#30340;&#25514;&#36766;&#12290;&#26368;&#20339;&#34920;&#29616;&#30340;&#26159;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#27604;&#29616;&#26377;&#22522;&#20934;&#26356;&#22909;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#65292;&#24182;&#21457;&#29616;&#20102;&#19968;&#20123;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25514;&#36766;&#22312;&#23545;&#35805;&#20114;&#21160;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#21516;&#20276;&#36741;&#23548;&#20013;&#65292;&#23548;&#24072;&#22312;&#32570;&#20047;&#40664;&#22865;&#30340;&#21452;&#20154;&#23545;&#35805;&#20013;&#20351;&#29992;&#25514;&#36766;&#26469;&#20943;&#36731;&#25351;&#20196;&#21644;&#36127;&#21453;&#39304;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#24314;&#31435;&#19968;&#20010;&#31649;&#29702;&#19982;&#23398;&#29983;&#30340;&#20146;&#23494;&#20851;&#31995;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#26524;&#30340;&#36741;&#23548;&#20195;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#21033;&#29992;&#22810;&#27169;&#24577;&#21516;&#20276;&#36741;&#23548;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#35782;&#21035;&#25514;&#36766;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20381;&#36182;&#39044;&#35757;&#32451;&#36164;&#28304;&#21644;&#32467;&#21512;&#31038;&#20250;&#31185;&#23398;&#25991;&#29486;&#35265;&#35299;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#34920;&#29616;&#26469;&#33258;&#19968;&#20010;&#28151;&#21512;&#26041;&#27861;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#35299;&#37322;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#25506;&#32034;&#20102;&#21516;&#20276;&#36741;&#23548;&#23545;&#35805;&#20013;&#29305;&#24449;&#30340;&#29305;&#28857;&#65292;&#24182;&#35782;&#21035;&#20102;&#19968;&#20123;&#26032;&#29305;&#24449;&#20197;&#21450;&#36825;&#31181;&#28151;&#21512;&#27169;&#22411;&#26041;&#27861;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hedges play an important role in the management of conversational interaction. In peer tutoring, they are notably used by tutors in dyads (pairs of interlocutors) experiencing low rapport to tone down the impact of instructions and negative feedback. Pursuing the objective of building a tutoring agent that manages rapport with students in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of such a hybrid model approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35770;&#36848;&#20102;&#22312;LLMs&#26102;&#20195;&#65292;&#20154;&#26631;&#35760;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24615;&#30340;&#35770;&#25454;&#21644;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.14910</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#20013;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Importance of Human-Labeled Data in the Era of LLMs. (arXiv:2306.14910v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35770;&#36848;&#20102;&#22312;LLMs&#26102;&#20195;&#65292;&#20154;&#26631;&#35760;&#25968;&#25454;&#20173;&#28982;&#20855;&#26377;&#37325;&#35201;&#24615;&#30340;&#35770;&#25454;&#21644;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#23450;&#21046;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24320;&#21457;&#26041;&#38754;&#24102;&#26469;&#20102;&#19968;&#22330;&#38761;&#21629;&#65292;&#24182;&#24341;&#21457;&#20102;&#20851;&#20110;&#37325;&#26032;&#23450;&#20041;&#25968;&#25454;&#35201;&#27714;&#30340;&#35752;&#35770;&#12290;LLMs&#30340;&#35757;&#32451;&#21644;&#23454;&#26045;&#25152;&#24102;&#26469;&#30340;&#33258;&#21160;&#21270;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#26631;&#35760;&#24178;&#39044;&#21487;&#33021;&#19981;&#20877;&#20855;&#26377;&#19982;&#30417;&#30563;&#23398;&#20064;&#26102;&#20195;&#30456;&#21516;&#37325;&#35201;&#24615;&#30340;&#35752;&#35770;&#21644;&#26399;&#26395;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#26377;&#21147;&#30340;&#35770;&#25454;&#65292;&#25903;&#25345;&#22312;LLMs&#26102;&#20195;&#20154;&#26631;&#35760;&#25968;&#25454;&#30340;&#25345;&#32493;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has brought about a revolution in the development of tailored machine learning models and sparked debates on redefining data requirements. The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning. This paper presents compelling arguments supporting the ongoing relevance of human-labeled data in the era of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#28857;&#20987;&#35825;&#39285;&#26631;&#39064;&#36827;&#34892;&#20998;&#31867;&#21644;&#21095;&#36879;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#21095;&#36879;&#31867;&#22411;&#65307;&#23545;&#20110;&#21095;&#36879;&#20219;&#21153;&#65292;&#37319;&#29992;&#38382;&#31572;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21095;&#36879;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20219;&#21153;1&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.14907</link><description>&lt;p&gt;
&#28857;&#20987;&#35825;&#39285;&#26631;&#39064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21095;&#36879;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Clickbait Classification and Spoiling Using Natural Language Processing. (arXiv:2306.14907v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#28857;&#20987;&#35825;&#39285;&#26631;&#39064;&#36827;&#34892;&#20998;&#31867;&#21644;&#21095;&#36879;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#21095;&#36879;&#31867;&#22411;&#65307;&#23545;&#20110;&#21095;&#36879;&#20219;&#21153;&#65292;&#37319;&#29992;&#38382;&#31572;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21095;&#36879;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20219;&#21153;1&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#35825;&#39285;&#26159;&#19968;&#31181;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#26631;&#39064;&#26469;&#28608;&#21169;&#35835;&#32773;&#28857;&#20987;&#25991;&#31456;&#30340;&#20570;&#27861;&#12290;&#36825;&#20123;&#20351;&#29992;&#22840;&#24352;&#35821;&#35328;&#30340;&#26631;&#39064;&#23613;&#21487;&#33021;&#23569;&#22320;&#36879;&#38706;&#20449;&#24687;&#12290;&#26377;&#26102;&#65292;&#28857;&#20987;&#35825;&#39285;&#20250;&#25925;&#24847;&#35823;&#23548;&#35835;&#32773;&#65292;&#22240;&#27492;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21487;&#20197;&#25195;&#25551;&#25991;&#31456;&#24182;&#22238;&#31572;&#28857;&#20987;&#35825;&#39285;&#26631;&#39064;&#25552;&#20986;&#30340;&#38382;&#39064;&#65292;&#25110;&#32773;&#21095;&#36879;&#20449;&#24687;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20004;&#20010;&#20219;&#21153;&#65306;&#23558;&#28857;&#20987;&#35825;&#39285;&#20998;&#31867;&#20026;3&#31181;&#31867;&#22411;&#20043;&#19968;&#65288;&#20219;&#21153;1&#65289;&#65292;&#20197;&#21450;&#21095;&#36879;&#28857;&#20987;&#35825;&#39285;&#65288;&#20219;&#21153;2&#65289;&#12290;&#23545;&#20110;&#20219;&#21153;1&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20108;&#20803;&#20998;&#31867;&#22120;&#26469;&#30830;&#23450;&#26368;&#32456;&#30340;&#21095;&#36879;&#31867;&#22411;&#12290;&#23545;&#20110;&#20219;&#21153;2&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#20351;&#29992;&#38382;&#31572;&#27169;&#22411;&#26469;&#35782;&#21035;&#21095;&#36879;&#25991;&#26412;&#30340;&#33539;&#22260;&#65292;&#20197;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#21095;&#36879;&#20449;&#24687;&#12290;&#30001;&#20110;&#21095;&#36879;&#20449;&#24687;&#21253;&#21547;&#22312;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#20219;&#21153;&#25551;&#36848;&#20026;&#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#21095;&#36879;&#20449;&#24687;&#24320;&#22987;&#21644;&#32467;&#26463;&#20301;&#32622;&#30340;&#38382;&#31572;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#30340;&#20219;&#21153;1&#27169;&#22411;&#20248;&#20110;&#25552;&#20986;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait is the practice of engineering titles to incentivize readers to click through to articles. Such titles with sensationalized language reveal as little information as possible. Occasionally, clickbait will be intentionally misleading, so natural language processing (NLP) can scan the article and answer the question posed by the clickbait title, or spoil it. We tackle two tasks: classifying the clickbait into one of 3 types (Task 1), and spoiling the clickbait (Task 2). For Task 1, we propose two binary classifiers to determine the final spoiler type. For Task 2, we experiment with two approaches: using a question-answering model to identify the span of text of the spoiler, and using a large language model (LLM) to generate the spoiler. Because the spoiler is contained in the article, we frame the second task as a question-answering approach for identifying the starting and ending positions of the spoiler. We created models for Task 1 that were better than the baselines proposed
&lt;/p&gt;</description></item><item><title>PRISMA-DFLLM&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;PRISMA&#30340;&#20005;&#26684;&#25253;&#21578;&#25351;&#21335;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#23398;&#26415;&#35770;&#25991;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#12290;</title><link>http://arxiv.org/abs/2306.14905</link><description>&lt;p&gt;
PRISMA-DFLLM&#65306;PRISMA&#30340;&#19968;&#31181;&#25193;&#23637;&#65292;&#20351;&#29992;&#39046;&#22495;&#29305;&#23450;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models. (arXiv:2306.14905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14905
&lt;/p&gt;
&lt;p&gt;
PRISMA-DFLLM&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;PRISMA&#30340;&#20005;&#26684;&#25253;&#21578;&#25351;&#21335;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22312;&#39046;&#22495;&#29305;&#23450;&#30340;&#23398;&#26415;&#35770;&#25991;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#21516;&#26102;&#24320;&#21551;&#20102;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#28044;&#29616;&#20986;&#35768;&#22810;&#38024;&#23545;&#19987;&#19994;&#39046;&#22495;&#21644;&#24212;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;LLMs&#30340;&#38454;&#27573;&#65292;&#36825;&#20123;LLMs&#24050;&#32463;&#38024;&#23545;&#24403;&#21069;&#36890;&#29992;LLMs&#26080;&#27861;&#36866;&#24212;&#30340;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#22312;&#23398;&#26415;&#30028;&#65292;&#36825;&#39033;&#25216;&#26415;&#26377;&#28508;&#21147;&#25913;&#21464;&#25105;&#20204;&#36827;&#34892;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLRs&#65289;&#30340;&#26041;&#24335;&#65292;&#33719;&#21462;&#30693;&#35782;&#21644;&#29983;&#25104;&#26032;&#35265;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;LLMs&#24378;&#22823;&#33021;&#21147;&#21644;Preferred Reporting Items for Systematic Reviews and Meta-Analyses&#65288;PRISMA&#65289;&#30340;&#20005;&#26684;&#25253;&#21578;&#25351;&#21335;&#30340;AI-Enabled&#26041;&#27861;&#26694;&#26550;&#12290;&#36890;&#36807;&#23545;&#36890;&#36807;&#20005;&#26684;SLR&#36807;&#31243;&#36873;&#23450;&#30340;&#39046;&#22495;&#29305;&#23450;&#23398;&#26415;&#35770;&#25991;&#36827;&#34892;LLMs&#24494;&#35843;&#65292;&#25552;&#20986;&#30340;PRISMA-DFLLM&#65288;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;LLMs&#24494;&#35843;&#65289;&#25253;&#21578;&#25351;&#21335;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#29575;&#12289;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#21516;&#26102;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#30740;&#31350;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of open-sourced Large Language Models (LLMs) and efficient finetuning techniques, we are on the cusp of the emergence of numerous domain-specific LLMs that have been finetuned for expertise across specialized fields and applications for which the current general-purpose LLMs are unsuitable. In academia, this technology has the potential to revolutionize the way we conduct systematic literature reviews (SLRs), access knowledge and generate new insights. This paper proposes an AI-enabled methodological framework that combines the power of LLMs with the rigorous reporting guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers that have been selected as a result of a rigorous SLR process, the proposed PRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer the potential to achieve greater efficiency, reusability and scalability, while also opening the po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#28040;&#24687;&#20013;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#28508;&#22312;&#36857;&#35937;&#65292;&#26088;&#22312;&#25552;&#20379;&#26089;&#26399;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14903</link><description>&lt;p&gt;
&#20174;&#31038;&#20132;&#32593;&#32476;&#20013;&#26816;&#27979;&#25233;&#37057;&#24773;&#32490;&#24182;&#36827;&#34892;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;
&lt;/p&gt;
&lt;p&gt;
Detect Depression from Social Networks with Sentiment Knowledge Sharing. (arXiv:2306.14903v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14903
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20197;&#21450;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;&#65292;&#20174;&#31038;&#20132;&#32593;&#32476;&#28040;&#24687;&#20013;&#35782;&#21035;&#25233;&#37057;&#30151;&#30340;&#28508;&#22312;&#36857;&#35937;&#65292;&#26088;&#22312;&#25552;&#20379;&#26089;&#26399;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#35782;&#21035;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#22312;&#20256;&#25773;&#20154;&#20204;&#30340;&#35266;&#28857;&#12289;&#24773;&#32490;&#12289;&#24605;&#32500;&#21644;&#24656;&#24807;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;COVID-19&#22823;&#27969;&#34892;&#26399;&#38388;&#30340;&#23553;&#38145;&#26399;&#21518;&#65292;&#25233;&#37057;&#30151;&#38382;&#39064;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#35768;&#22810;&#20154;&#20511;&#21161;&#31038;&#20132;&#32593;&#32476;&#34920;&#36798;&#24773;&#32490;&#12290;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#31038;&#20132;&#32593;&#32476;&#28040;&#24687;&#20013;&#36776;&#21035;&#28508;&#22312;&#30340;&#25233;&#37057;&#30151;&#36857;&#35937;&#26377;&#21161;&#20110;&#26089;&#26399;&#35782;&#21035;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#12290;&#30446;&#21069;&#65292;&#36890;&#36807;&#31038;&#20132;&#32593;&#32476;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#21162;&#21147;&#36890;&#24120;&#20165;&#20381;&#38752;&#23545;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#20998;&#26512;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#28508;&#22312;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#25233;&#37057;&#30151;&#21644;&#36127;&#38754;&#24773;&#32490;&#29366;&#24577;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#23558;&#36825;&#26679;&#30340;&#20851;&#32852;&#20316;&#20026;&#22806;&#37096;&#30693;&#35782;&#30340;&#25972;&#21512;&#21487;&#20197;&#20026;&#26816;&#27979;&#25233;&#37057;&#30151;&#25552;&#20379;&#23453;&#36149;&#30340;&#27934;&#35265;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;DeSK&#65292;&#21033;&#29992;&#24773;&#24863;&#30693;&#35782;&#20849;&#20139;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social network plays an important role in propagating people's viewpoints, emotions, thoughts, and fears. Notably, following lockdown periods during the COVID-19 pandemic, the issue of depression has garnered increasing attention, with a significant portion of individuals resorting to social networks as an outlet for expressing emotions. Using deep learning techniques to discern potential signs of depression from social network messages facilitates the early identification of mental health conditions. Current efforts in detecting depression through social networks typically rely solely on analyzing the textual content, overlooking other potential information. In this work, we conduct a thorough investigation that unveils a strong correlation between depression and negative emotional states. The integration of such associations as external knowledge can provide valuable insights for detecting depression. Accordingly, we propose a multi-task training framework, DeSK, which utilizes share
&lt;/p&gt;</description></item><item><title>InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.14898</link><description>&lt;p&gt;
InterCode:&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#25191;&#34892;&#21453;&#39304;&#30340;&#20132;&#20114;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback. (arXiv:2306.14898v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14898
&lt;/p&gt;
&lt;p&gt;
InterCode&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#32534;&#30721;&#30340;&#26631;&#20934;&#21270;&#21644;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#65292;&#24182;&#25552;&#20379;&#20102;&#23433;&#20840;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#29615;&#22659;&#65292;&#21487;&#20197;&#29992;&#20110;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20197;&#22522;&#26412;&#20132;&#20114;&#26041;&#24335;&#32534;&#20889;&#20195;&#30721;&#65292;&#24182;&#20381;&#36182;&#20110;&#25345;&#32493;&#30340;&#25191;&#34892;&#21453;&#39304;&#26469;&#32416;&#27491;&#38169;&#35823;&#65292;&#35299;&#20915;&#27495;&#20041;&#21644;&#20998;&#35299;&#20219;&#21153;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;LLM&#23637;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32534;&#30721;&#33021;&#21147;&#65292;&#20294;&#30446;&#21069;&#30340;&#32534;&#30721;&#22522;&#20934;&#20027;&#35201;&#32771;&#34385;&#38745;&#24577;&#30340;&#25351;&#20196;&#21040;&#20195;&#30721;&#24207;&#21015;&#36716;&#25442;&#36807;&#31243;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20256;&#25773;&#21644;&#29983;&#25104;&#30340;&#20195;&#30721;&#19982;&#20854;&#26368;&#32456;&#25191;&#34892;&#29615;&#22659;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;InterCode&#65292;&#36825;&#26159;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#28789;&#27963;&#19988;&#26131;&#20110;&#20351;&#29992;&#30340;&#20132;&#20114;&#24335;&#32534;&#30721;&#26694;&#26550;&#65292;&#20316;&#20026;&#19968;&#20010;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#65292;&#20351;&#29992;&#20195;&#30721;&#20316;&#20026;&#34892;&#21160;&#65292;&#25191;&#34892;&#21453;&#39304;&#20316;&#20026;&#35266;&#23519;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;&#35821;&#35328;&#21644;&#24179;&#21488;&#26080;&#20851;&#65292;&#20351;&#29992;&#29420;&#31435;&#30340;Docker&#29615;&#22659;&#25552;&#20379;&#23433;&#20840;&#21644;&#21487;&#37325;&#29616;&#30340;&#25191;&#34892;&#65292;&#24182;&#19988;&#19982;&#20256;&#32479;&#30340;seq2seq&#32534;&#30721;&#26041;&#27861;&#24320;&#31665;&#21363;&#29992;&#65292;&#21516;&#26102;&#36824;&#21487;&#20197;&#24320;&#21457;&#26032;&#30340;&#20132;&#20114;&#24335;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;InterCode&#21019;&#24314;...
&lt;/p&gt;
&lt;p&gt;
Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create t
&lt;/p&gt;</description></item><item><title>Kosmos-2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#24863;&#30693;&#29289;&#20307;&#25551;&#36848;&#24182;&#23558;&#25991;&#26412;&#19982;&#35270;&#35273;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#34920;&#29616;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25509;&#22320;&#12289;&#22810;&#27169;&#24577;&#24341;&#29992;&#12289;&#24863;&#30693;&#35821;&#35328;&#20219;&#21153;&#20197;&#21450;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.14824</link><description>&lt;p&gt;
Kosmos-2: &#23558;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#19982;&#19990;&#30028;&#36830;&#25509;
&lt;/p&gt;
&lt;p&gt;
Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14824
&lt;/p&gt;
&lt;p&gt;
Kosmos-2&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#24863;&#30693;&#29289;&#20307;&#25551;&#36848;&#24182;&#23558;&#25991;&#26412;&#19982;&#35270;&#35273;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#23427;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#34920;&#29616;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25509;&#22320;&#12289;&#22810;&#27169;&#24577;&#24341;&#29992;&#12289;&#24863;&#30693;&#35821;&#35328;&#20219;&#21153;&#20197;&#21450;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Kosmos-2&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#65292;&#20351;&#20854;&#33021;&#22815;&#24863;&#30693;&#29289;&#20307;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#26694;&#65289;&#24182;&#23558;&#25991;&#26412;&#19982;&#35270;&#35273;&#19990;&#30028;&#32852;&#31995;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#24341;&#29992;&#34920;&#36798;&#24335;&#34920;&#31034;&#20026;Markdown&#20013;&#30340;&#38142;&#25509;&#65292;&#21363;``[text span](bounding boxes)''&#65292;&#20854;&#20013;&#29289;&#20307;&#25551;&#36848;&#26159;&#20301;&#32622;&#26631;&#35760;&#24207;&#21015;&#12290;&#36890;&#36807;&#19982;&#22810;&#27169;&#24577;&#35821;&#26009;&#24211;&#32467;&#21512;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#25991;&#26412;&#23545;&#65288;&#31216;&#20026;GrIT&#65289;&#30340;&#25968;&#25454;&#26469;&#35757;&#32451;&#35813;&#27169;&#22411;&#12290;&#38500;&#20102;MLLM&#30340;&#29616;&#26377;&#21151;&#33021;&#65288;&#20363;&#22914;&#65292;&#24863;&#30693;&#21508;&#31181;&#27169;&#24577;&#65292;&#36981;&#24490;&#25351;&#20196;&#21644;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#22806;&#65292;Kosmos-2&#36824;&#23558;&#25509;&#22320;&#33021;&#21147;&#38598;&#25104;&#21040;&#19979;&#28216;&#24212;&#29992;&#20013;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;Kosmos-2&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25509;&#22320;&#65288;&#20363;&#22914;&#65292;&#24341;&#29992;&#34920;&#36798;&#29702;&#35299;&#21644;&#30701;&#35821;&#25509;&#22320;&#65289;&#65292;&#22810;&#27169;&#24577;&#24341;&#29992;&#65288;&#20363;&#22914;&#65292;&#24341;&#29992;&#34920;&#36798;&#29983;&#25104;&#65289;&#65292;&#24863;&#30693;&#35821;&#35328;&#20219;&#21153;&#20197;&#21450;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new capabilities of perceiving object descriptions (e.g., bounding boxes) and grounding text to the visual world. Specifically, we represent refer expressions as links in Markdown, i.e., ``[text span](bounding boxes)'', where object descriptions are sequences of location tokens. Together with multimodal corpora, we construct large-scale data of grounded image-text pairs (called GrIT) to train the model. In addition to the existing capabilities of MLLMs (e.g., perceiving general modalities, following instructions, and performing in-context learning), Kosmos-2 integrates the grounding capability into downstream applications. We evaluate Kosmos-2 on a wide range of tasks, including (i) multimodal grounding, such as referring expression comprehension, and phrase grounding, (ii) multimodal referring, such as referring expression generation, (iii) perception-language tasks, and (iv) language understanding and generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#22788;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.14514</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65306;&#35821;&#35328;&#29305;&#23450;&#22788;&#29702;&#19982;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Approach for Formality-Sensitive Machine Translation: Language-Specific Handling and Synthetic Data Generation. (arXiv:2306.14514v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14514
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#29305;&#23450;&#22788;&#29702;&#21644;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27491;&#24335;&#25935;&#24863;&#26426;&#22120;&#32763;&#35793;&#65288;FSMT&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#22235;&#31181;&#30446;&#26631;&#35821;&#35328;&#30340;&#29420;&#29305;&#35821;&#35328;&#29305;&#24615;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#20004;&#20010;&#26680;&#24515;&#31574;&#30053;&#65306;&#35821;&#35328;&#29305;&#23450;&#25968;&#25454;&#22788;&#29702;&#21644;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#32463;&#39564;&#24615;&#25552;&#31034;&#24037;&#31243;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#30456;&#36739;&#20110;&#22522;&#20934;&#27169;&#22411;&#26377;&#26174;&#33879;&#25913;&#36827;&#65292;&#31361;&#26174;&#20102;&#25968;&#25454;&#20013;&#24515;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#25552;&#31034;&#24037;&#31243;&#31574;&#30053;&#36890;&#36807;&#20135;&#29983;&#20248;&#36136;&#30340;&#21512;&#25104;&#32763;&#35793;&#31034;&#20363;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a data-driven approach for Formality-Sensitive Machine Translation (FSMT) that caters to the unique linguistic properties of four target languages. Our methodology centers on two core strategies: 1) language-specific data handling, and 2) synthetic data generation using large-scale language models and empirical prompt engineering. This approach demonstrates a considerable improvement over the baseline, highlighting the effectiveness of data-centric techniques. Our prompt engineering strategy further improves performance by producing superior synthetic translation examples.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#24773;&#24863;&#30340;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.13804</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#23454;&#29616;&#36328;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13804
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#36890;&#36807;&#24341;&#20837;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#26426;&#21046;&#26469;&#25429;&#25417;&#19981;&#21516;&#24773;&#24863;&#30340;&#36328;&#27169;&#24577;&#20381;&#36182;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;&#24773;&#24863;&#35782;&#21035;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#21462;&#24471;&#20102;&#36817;&#26399;&#30340;&#36827;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#26080;&#27861;&#22312;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#23454;&#29616;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#21452;&#37325;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#65288;MDAT&#65289;&#27169;&#22411;&#65292;&#20197;&#25913;&#36827;&#36328;&#35821;&#35328;SER&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22810;&#27169;&#24577;&#29305;&#24449;&#25552;&#21462;&#65292;&#24182;&#37197;&#22791;&#21452;&#37325;&#27880;&#24847;&#26426;&#21046;&#65292;&#21253;&#25324;&#22270;&#24418;&#27880;&#24847;&#21644;&#20849;&#21516;&#20851;&#27880;&#65292;&#20197;&#25429;&#33719;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#22797;&#26434;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#20351;&#29992;&#26368;&#23569;&#30340;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#23454;&#29616;&#25913;&#36827;&#30340;&#36328;&#35821;&#35328;SER&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21033;&#29992;&#21464;&#25442;&#22120;&#32534;&#30721;&#22120;&#23618;&#36827;&#34892;&#39640;&#23618;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;MDAT&#22312;&#21508;&#20010;&#38454;&#27573;&#25191;&#34892;&#29305;&#24449;&#34920;&#31034;&#30340;&#32454;&#21270;&#65292;&#24182;&#20026;&#20998;&#31867;&#23618;&#25552;&#20379;&#24773;&#24863;&#26174;&#30528;&#29305;&#24449;&#12290;&#36825;&#31181;&#26032;&#39062;&#26041;&#27861;&#36824;&#30830;&#20445;&#20102;&#27169;&#24577;&#29305;&#23450;&#30340;&#24773;&#24863;&#20449;&#24687;&#30340;&#20445;&#23384;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#20132;&#21449;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent progress in speech emotion recognition (SER), state-of-the-art systems are unable to achieve improved performance in cross-language settings. In this paper, we propose a Multimodal Dual Attention Transformer (MDAT) model to improve cross-language SER. Our model utilises pre-trained models for multimodal feature extraction and is equipped with a dual attention mechanism including graph attention and co-attention to capture complex dependencies across different modalities and achieve improved cross-language SER results using minimal target language data. In addition, our model also exploits a transformer encoder layer for high-level feature representation to improve emotion classification accuracy. In this way, MDAT performs refinement of feature representation at various stages and provides emotional salient features to the classification layer. This novel approach also ensures the preservation of modality-specific emotional information while enhancing cross-modality 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;</title><link>http://arxiv.org/abs/2306.13596</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#36793;&#32536;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Margin Maximization in Attention Mechanism. (arXiv:2306.13596v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35777;&#26126;&#20102;&#65292;&#22312;softmax-attention&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;p&#25110;&#31561;&#20215;&#30340;W&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#21487;&#20197;&#25910;&#25947;&#21040;&#19968;&#20010;&#26368;&#22823;&#36793;&#32536;&#35299;&#65292;&#36825;&#23558;&#23616;&#37096;&#26368;&#20248;&#30340;&#26631;&#35760;&#19982;&#38750;&#26368;&#20248;&#30340;&#26631;&#35760;&#20998;&#38548;&#24320;&#12290;&#36825;&#26126;&#30830;&#22320;&#23558;&#27880;&#24847;&#21147;&#26426;&#21046;&#24418;&#24335;&#21270;&#20026;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#26159;Transformer&#26550;&#26500;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#20063;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#24778;&#20154;&#25104;&#21151;&#30340;&#21407;&#22240;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#32972;&#21518;&#30340;&#29702;&#35770;&#21407;&#21017;&#23578;&#19981;&#28165;&#26970;&#65292;&#29305;&#21035;&#26159;&#23427;&#30340;&#38750;&#20984;&#20248;&#21270;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24320;&#21019;&#24615;&#30340;softmax-attention&#27169;&#22411;$f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$&#65292;&#20854;&#20013;$\boldsymbol{X}$&#26159;&#26631;&#35760;&#24207;&#21015;&#65292;$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$&#26159;&#21487;&#35843;&#21442;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$\boldsymbol{p}$&#25110;&#31561;&#20215;&#30340;$\boldsymbol{W}$&#19978;&#36816;&#34892;&#26799;&#24230;&#19979;&#38477;&#20250;&#27839;&#30528;&#26041;&#21521;&#25910;&#25947;&#21040;&#20998;&#38548;&#8220;&#23616;&#37096;&#26368;&#20248;&#8221;&#26631;&#35760;&#21644;&#8220;&#38750;&#26368;&#20248;&#8221;&#26631;&#35760;&#30340;&#26368;&#22823;&#36793;&#32536;&#35299;&#12290;&#36825;&#26126;&#30830;&#22320;&#24418;&#24335;&#21270;&#20102;&#27880;&#24847;&#21147;&#20316;&#20026;&#19968;&#31181;&#26631;&#35760;&#20998;&#31163;&#26426;&#21046;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#19968;&#33324;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#23884;&#20837;$\boldsymbol{Xv}$&#21644;$\texttt{softmax}(\boldsymbol{XWp})$&#31934;&#32454;&#22320;&#34920;&#24449;&#26631;&#35760;&#30340;&#8220;&#26368;&#20248;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle \boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where, $\boldsymbol{X}$ is the token sequence and $(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are tunable parameters. We prove that running gradient descent on $\boldsymbol{p}$, or equivalently $\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as a token separation mechanism. Remarkably, our results are applicable to general data and precisely characterize $\textit{optimality}$ of tokens in terms of the value embeddings $\boldsymbol{Xv}$ and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;</title><link>http://arxiv.org/abs/2306.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23398;&#20064;&#25551;&#36848;&#24615;&#22270;&#20687;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Learning Descriptive Image Captioning via Semipermeable Maximum Likelihood Estimation. (arXiv:2306.13460v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26041;&#27861;&#65292;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#26088;&#22312;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#35270;&#35273;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26159;&#35757;&#32451;&#30446;&#26631;&#65292;&#23383;&#24149;&#27169;&#22411;&#22312;&#39044;&#27979;&#19982;&#26631;&#31614;&#19981;&#21305;&#37197;&#26102;&#20250;&#21463;&#21040;&#24809;&#32602;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21322;&#36879;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;SMILE&#65289;&#26041;&#27861;&#65292;&#20801;&#35768;&#20016;&#23500;&#24615;&#20248;&#21270;&#21516;&#26102;&#38459;&#27490;&#31616;&#27905;&#24615;&#20248;&#21270;&#65292;&#20174;&#32780;&#40723;&#21169;&#27169;&#22411;&#29983;&#25104;&#26356;&#35814;&#32454;&#30340;&#38271;&#23383;&#24149;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image captioning aims to describe visual content in natural language. As 'a picture is worth a thousand words', there could be various correct descriptions for an image. However, with maximum likelihood estimation as the training objective, the captioning model is penalized whenever its prediction mismatches with the label. For instance, when the model predicts a word expressing richer semantics than the label, it will be penalized and optimized to prefer more concise expressions, referred to as conciseness optimization. In contrast, predictions that are more concise than labels lead to richness optimization. Such conflicting optimization directions could eventually result in the model generating general descriptions. In this work, we introduce Semipermeable MaxImum Likelihood Estimation (SMILE), which allows richness optimization while blocking conciseness optimization, thus encouraging the model to generate longer captions with more details. Extensive experiments on two mainstream im
&lt;/p&gt;</description></item><item><title>DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.13230</link><description>&lt;p&gt;
DiversiGATE: &#19968;&#20010;&#21487;&#38752;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20840;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DiversiGATE: A Comprehensive Framework for Reliable Large Language Models. (arXiv:2306.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13230
&lt;/p&gt;
&lt;p&gt;
DiversiGATE&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#27719;&#38598;&#20102;&#22810;&#31181;LLM&#39564;&#35777;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#31526;&#21512;&#35813;&#26694;&#26550;&#30340;&#26032;&#27169;&#22411;&#8220;SelfLearner&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#20248;&#21270;&#24615;&#33021;&#65292;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;GSM8K&#22522;&#20934;&#27979;&#35797;&#19978;&#25552;&#39640;&#20102;7%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DiversiGATE&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#27719;&#38598;LLM&#39564;&#35777;&#30340;&#22810;&#31181;&#26041;&#27861;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#22810;&#26679;&#21270;&#21644;&#32858;&#21512;&#65292;&#22312;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#19978;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35270;&#35282;&#65292;&#20363;&#22914;&#33258;&#19968;&#33268;&#24615;&#12289;&#25968;&#23398;&#25552;&#31034;&#21644;WebGPT&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;SelfLearner&#8221;&#27169;&#22411;&#65292;&#31526;&#21512;DiversiGATE&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#33258;&#24049;&#30340;&#36755;&#20986;&#20013;&#23398;&#20064;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#19981;&#26029;&#23436;&#21892;&#20854;&#24615;&#33021;&#65292;&#20174;&#32780;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;SelfLearner&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#26415;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;GSM8K&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;LLMs&#65292;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#21487;&#35266;&#30340;54.8%-&gt;61.8%&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce DiversiGATE, a unified framework that consolidates diverse methodologies for LLM verification. The proposed framework comprises two main components: Diversification and Aggregation which provide a holistic perspective on existing verification approaches, such as Self-Consistency, Math Prompter and WebGPT. Furthermore, we propose a novel `SelfLearner' model that conforms to the DiversiGATE framework which can learn from its own outputs and refine its performance over time, leading to improved accuracy. To evaluate the effectiveness of SelfLearner, we conducted a rigorous series of experiments, including tests on synthetic data as well as on popular arithmetic reasoning benchmarks such as GSM8K. Our results demonstrate that our approach outperforms traditional LLMs, achieving a considerable 54.8% -&gt; 61.8% improvement on the GSM8K benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25506;&#27979;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#36328;&#24230;&#25552;&#21462;&#21644;&#26102;&#25935;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08952</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models. (arXiv:2306.08952v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25506;&#27979;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#36328;&#24230;&#25552;&#21462;&#21644;&#26102;&#25935;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#25512;&#29702;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20107;&#23454;&#26159;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#12290;&#20363;&#22914;&#65292;&#36816;&#21160;&#21592;&#20250;&#19981;&#26102;&#22320;&#26356;&#25442;&#29699;&#38431;&#65292;&#19981;&#21516;&#30340;&#25919;&#24220;&#23448;&#21592;&#20250;&#23450;&#26399;&#36827;&#34892;&#36873;&#20030;&#12290;&#20808;&#21069;&#30340;&#26102;&#38388;&#30456;&#20851;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#24448;&#24448;&#22312;&#26102;&#38388;&#36328;&#24230;&#25110;&#38382;&#39064;&#31867;&#22411;&#30340;&#28085;&#30422;&#19978;&#23384;&#22312;&#20559;&#35265;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25506;&#27979;&#25968;&#25454;&#38598;\tempreason&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#19977;&#20010;&#26102;&#38388;&#25512;&#29702;&#32423;&#21035;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36328;&#24230;&#25552;&#21462;&#21644;&#26102;&#25935;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26102;&#38388;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20070;&#24335;QA&#12289;&#24320;&#25918;&#20070;&#24335;QA&#21644;&#25512;&#29702;QA&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#24050;&#22312;https://github.com/DAMO-NLP-SG/TempReason&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset \tempreason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach. Our code and data are released on https://github.com/DAMO-NLP-SG/TempReason.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2306.04050</link><description>&lt;p&gt;
LLMZip&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#25439;&#25991;&#26412;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LLMZip: Lossless Text Compression using Large Language Models. (arXiv:2306.04050v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#39044;&#27979;&#21644;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA-7B&#23545;&#33521;&#35821;&#29109;&#30340;&#28176;&#36817;&#19978;&#30028;&#25552;&#20986;&#20102;&#26032;&#20272;&#35745;&#20540;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#19982;&#26080;&#25439;&#21387;&#32553;&#26041;&#26696;&#30340;&#33521;&#25991;&#25991;&#26412;&#21387;&#32553;&#31639;&#27861;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#21387;&#32553;&#26041;&#26696;&#65292;&#22914;BSC&#12289;ZPAQ&#21644;paq8h&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B as a predictor for the next token given a window of past tokens. This estimate is significantly smaller than currently available estimates in \cite{cover1978convergent}, \cite{lutati2023focus}. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction from the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our scheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.17760</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;
&lt;/p&gt;
&lt;p&gt;
Language Models are Bounded Pragmatic Speakers. (arXiv:2305.17760v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#27492;&#30740;&#31350;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#35748;&#30693;&#27169;&#22411;&#65292;&#31216;&#20026;&#26377;&#38480;&#23454;&#29992;&#35828;&#35805;&#32773;&#65292;&#29992;&#20110;&#34920;&#24449;&#19981;&#21516;&#21464;&#20307;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#25805;&#20316;&#26041;&#24335;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#32463;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Ouyang&#31561;&#20154;&#65292;2022&#65289;&#20855;&#26377;&#27010;&#24565;&#19978;&#31867;&#20284;&#20110; &#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#65288;Kahneman&#65292;2011&#65289;&#30340;&#24605;&#32500;&#27169;&#22411;&#65292;&#32780;&#36825;&#31181;&#24605;&#32500;&#27169;&#22411;&#34987;&#24515;&#29702;&#23398;&#23478;&#20204;&#24402;&#22240;&#20110;&#20154;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#24555;&#19982;&#24930;&#24605;&#32771;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#25193;&#23637;&#36825;&#20010;&#26694;&#26550;&#30340;&#36884;&#24452;&#12290;&#26412;&#30740;&#31350;&#23454;&#36136;&#19978;&#20984;&#26174;&#20102;&#37319;&#29992;&#35748;&#30693;&#27010;&#29575;&#24314;&#27169;&#26041;&#27861;&#26469;&#33719;&#24471;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#12289;&#35780;&#20272;&#21644;&#25512;&#36827;&#26041;&#38754;&#30340;&#28145;&#21051;&#35265;&#35299;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do language models "think"? This paper formulates a probabilistic cognitive model called the bounded pragmatic speaker, which can characterize the operation of different variations of language models. Specifically, we demonstrate that large language models fine-tuned with reinforcement learning from human feedback (Ouyang et al., 2022) embody a model of thought that conceptually resembles a fast-and-slow model (Kahneman, 2011), which psychologists have attributed to humans. We discuss the limitations of reinforcement learning from human feedback as a fast-and-slow model of thought and propose avenues for expanding this framework. In essence, our research highlights the value of adopting a cognitive probabilistic modeling approach to gain insights into the comprehension, evaluation, and advancement of language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.13583</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#19981;&#36275;&#65306;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#19982;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-Attention is Not Enough: Incongruity-Aware Multimodal Sentiment Analysis and Emotion Recognition. (arXiv:2305.13583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#21327;&#35843;&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;Hierarchical Crossmodal Transformer with Modality Gating(HCT-MG)&#27169;&#22411;&#26469;&#30830;&#23450;&#20027;&#35201;&#27169;&#24577;&#24182;&#20998;&#23618;&#34701;&#21512;&#36741;&#21161;&#27169;&#24577;&#65292;&#26377;&#25928;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#21644;&#20449;&#24687;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#34701;&#21512;&#22312;&#24773;&#24863;&#35745;&#31639;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#23545;&#24615;&#33021;&#30340;&#25552;&#21319;&#24050;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26426;&#29702;&#23578;&#19981;&#28165;&#26970;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#23427;&#36890;&#24120;&#20250;&#23548;&#33268;&#22823;&#22411;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#22312;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;&#36328;&#27169;&#24577;&#27880;&#24847;&#21147;&#20013;&#19968;&#20010;&#27169;&#24577;&#20013;&#31361;&#20986;&#30340;&#24773;&#24863;&#20449;&#24687;&#22914;&#20309;&#21463;&#21040;&#21478;&#19968;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#36328;&#27169;&#24577;&#30340;&#20851;&#27880;&#65292;&#27169;&#24577;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;(HCT-MG)&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20998;&#23618;&#20132;&#21449;&#27169;&#24577;Transformer&#19982;&#27169;&#24577;&#38376;&#25511;&#21046;&#26469;&#30830;&#23450;&#20027;&#35201;&#30340;&#27169;&#24577;&#65292;&#24182;&#20998;&#23618;&#22320;&#23558;&#36741;&#21161;&#27169;&#24577;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#20943;&#36731;&#27169;&#24577;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#24863;&#30693;&#24182;&#20943;&#23569;&#20449;&#24687;&#20887;&#20313;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;CMU-MOSI&#12289;CMU-MOSEI&#21644;IEMOCAP&#19978;&#30340;&#23454;&#39564;&#35780;&#20272;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#65306;1&#65289;&#20854;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65307;2&#65289;&#23427;&#20165;&#20351;&#29992;&#23569;&#37327;&#30340;&#36229;&#21442;&#25968;&#21644;&#21442;&#25968;&#65307;3&#65289;&#23427;&#30340;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing multiple modalities for affective computing tasks has proven effective for performance improvement. However, how multimodal fusion works is not well understood, and its use in the real world usually results in large model sizes. In this work, on sentiment and emotion analysis, we first analyze how the salient affective information in one modality can be affected by the other in crossmodal attention. We find that inter-modal incongruity exists at the latent level due to crossmodal attention. Based on this finding, we propose a lightweight model via Hierarchical Crossmodal Transformer with Modality Gating (HCT-MG), which determines a primary modality according to its contribution to the target task and then hierarchically incorporates auxiliary modalities to alleviate inter-modal incongruity and reduce information redundancy. The experimental evaluation on three benchmark datasets: CMU-MOSI, CMU-MOSEI, and IEMOCAP verifies the efficacy of our approach, showing that it: 1) outperfo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.13108</link><description>&lt;p&gt;
&#36890;&#36807;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#36827;&#34892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;&#26080;&#20559;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Debiased Automatic Speech Recognition for Dysarthric Speech via Sample Reweighting with Sample Affinity Test. (arXiv:2305.13108v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#22833;&#35821;&#30151;&#24739;&#32773;&#30340;&#20559;&#24046;&#38382;&#39064;&#65292;&#22312;&#19981;&#24433;&#21709;&#20581;&#24247;&#24739;&#32773;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;ASR&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#20027;&#35201;&#26159;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#30001;&#20110;ERM&#21033;&#29992;&#25968;&#25454;&#26679;&#26412;&#30340;&#24179;&#22343;&#34920;&#29616;&#32780;&#19981;&#32771;&#34385;&#19968;&#20010;&#32676;&#20307;&#65292;&#20363;&#22914;&#20581;&#24247;&#25110;&#22833;&#35821;&#30151;&#24739;&#32773;&#65292;&#22240;&#27492;ASR&#31995;&#32479;&#26080;&#27861;&#35782;&#21035;&#36328;&#32676;&#20307;&#30340;&#24615;&#33021;&#24046;&#24322;&#65292;&#23548;&#33268;ASR&#31995;&#32479;&#23384;&#22312;&#20559;&#24046;&#19988;&#20854;&#32676;&#20307;&#24615;&#33021;&#24046;&#24322;&#20005;&#37325;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#32676;&#20307;&#31283;&#20581;&#24615;&#65292;&#38024;&#23545;&#22833;&#35821;&#30151;&#24739;&#32773;&#36827;&#34892;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#26679;&#26412;&#37325;&#26032;&#21152;&#26435;&#19982;&#26679;&#26412;&#20851;&#32852;&#27979;&#35797;&#65288;Re-SAT&#65289;&#12290; Re-SAT&#31995;&#32479;&#22320;&#34913;&#37327;&#25152;&#32473;&#25968;&#25454;&#26679;&#26412;&#30340;&#21435;&#20559;&#24110;&#21161;&#24615;&#65292;&#24182;&#36890;&#36807;&#21435;&#20559;&#24110;&#21161;&#24615;&#21152;&#26435;&#26469;&#32531;&#35299;&#20559;&#24046;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292; Re-SAT&#26377;&#21161;&#20110;&#25913;&#21892;&#22833;&#35821;&#30151;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#20581;&#24247;&#35821;&#38899;&#30340;ASR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition systems based on deep learning are mainly trained under empirical risk minimization (ERM). Since ERM utilizes the averaged performance on the data samples regardless of a group such as healthy or dysarthric speakers, ASR systems are unaware of the performance disparities across the groups. This results in biased ASR systems whose performance differences among groups are severe. In this study, we aim to improve the ASR system in terms of group robustness for dysarthric speakers. To achieve our goal, we present a novel approach, sample reweighting with sample affinity test (Re-SAT). Re-SAT systematically measures the debiasing helpfulness of the given data sample and then mitigates the bias by debiasing helpfulness-based sample reweighting. Experimental results demonstrate that Re-SAT contributes to improved ASR performance on dysarthric speech without performance degradation on healthy speech.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#32852;&#24819;&#30693;&#35782;&#32593;&#32476;&#21644;&#19978;&#19979;&#25991;&#30456;&#23545;&#36317;&#31163;&#20316;&#20026;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.11543</link><description>&lt;p&gt;
&#26500;&#24314;&#22522;&#20110;&#32852;&#24819;&#30693;&#35782;&#20851;&#31995;&#30340;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;&#29992;&#20110;&#21487;&#35299;&#37322;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Constructing Word-Context-Coupled Space Aligned with Associative Knowledge Relations for Interpretable Language Modeling. (arXiv:2305.11543v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;&#65292;&#24182;&#24341;&#20837;&#32852;&#24819;&#30693;&#35782;&#32593;&#32476;&#21644;&#19978;&#19979;&#25991;&#30456;&#23545;&#36317;&#31163;&#20316;&#20026;&#35821;&#20041;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#24314;&#27169;&#21487;&#35299;&#37322;&#24615;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#30340;&#22522;&#30784;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#31665;&#32467;&#26500;&#20005;&#37325;&#38480;&#21046;&#20102;&#35821;&#35328;&#24314;&#27169;&#36807;&#31243;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#28145;&#24230;&#31070;&#32463;&#34920;&#31034;&#21644;&#35821;&#35328;&#24314;&#27169;&#30340;&#35821;&#20041;&#36923;&#36753;&#20043;&#38388;&#30340;&#32806;&#21512;&#35201;&#27714;&#65292;&#24341;&#20837;&#20102;&#35789;-&#19978;&#19979;&#25991;&#32806;&#21512;&#31354;&#38388;(W2CSpace)&#65292;&#36890;&#36807;&#20171;&#32461;&#21487;&#35299;&#37322;&#30340;&#32479;&#35745;&#36923;&#36753;&#21644;&#19981;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#22788;&#29702;&#26469;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#32858;&#31867;&#36807;&#31243;&#26469;&#36830;&#25509;&#35789;&#32423;&#21644;&#19978;&#19979;&#25991;&#32423;&#35821;&#20041;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#23545;&#40784;&#35789;&#32423;&#35821;&#20041;&#30340;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#30340;&#32479;&#35745;&#36923;&#36753;&#30340;&#32852;&#24819;&#30693;&#35782;&#32593;&#32476;(AKN)&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#30456;&#23545;&#36317;&#31163;&#34987;&#29992;&#20316;&#19979;&#28216;&#20998;&#31867;&#22120;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#36825;&#19982;&#24403;&#21069;&#30340;&#38750;&#35299;&#37322;&#24615;&#26041;&#27861;&#38750;&#24120;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the foundation of current natural language processing methods, pre-trained language model has achieved excellent performance. However, the black-box structure of the deep neural network in pre-trained language models seriously limits the interpretability of the language modeling process. After revisiting the coupled requirement of deep neural representation and semantics logic of language modeling, a Word-Context-Coupled Space (W2CSpace) is proposed by introducing the alignment processing between uninterpretable neural representation and interpretable statistical logic. Moreover, a clustering process is also designed to connect the word- and context-level semantics. Specifically, an associative knowledge network (AKN), considered interpretable statistical logic, is introduced in the alignment process for word-level semantics. Furthermore, the context-relative distance is employed as the semantic feature for the downstream classifier, which is greatly different from the current unint
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.07011</link><description>&lt;p&gt;
&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#65306;&#35270;&#35273;&#21464;&#21387;&#22120;&#19979;&#30340;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Region-Aware Pretraining for Open-Vocabulary Object Detection with Vision Transformers. (arXiv:2305.07011v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#21464;&#21387;&#22120;&#30340;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#38024;&#23545;&#24320;&#25918;&#35789;&#27719;&#30340;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#65292;&#37319;&#29992;&#21306;&#22495;&#24863;&#30693;&#39044;&#35757;&#32451;&#12289;&#32858;&#28966;&#25439;&#22833;&#21644;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#31561;&#25216;&#26415;&#65292;&#22312;LVIS&#19978;&#21462;&#24471;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21306;&#22495;&#24863;&#30693;&#24320;&#25918;&#35789;&#27719;&#35270;&#35273;&#21464;&#21387;&#22120;&#65288;RO-ViT&#65289;&#65292;&#19968;&#31181;&#23545;&#27604;&#22270;&#20687;-&#25991;&#26412;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#26088;&#22312;&#22635;&#34917;&#22270;&#20687;&#32423;&#39044;&#35757;&#32451;&#21644;&#24320;&#25918;&#35789;&#27719;&#29289;&#20307;&#26816;&#27979;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#24314;&#35758;&#38543;&#26426;&#35009;&#21098;&#24182;&#35843;&#25972;&#20301;&#32622;&#23884;&#20837;&#30340;&#21306;&#22495;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25972;&#20010;&#22270;&#20687;&#20301;&#32622;&#23884;&#20837;&#12290;&#36825;&#26356;&#22909;&#22320;&#21305;&#37197;&#20102;&#26816;&#27979;&#24494;&#35843;&#38454;&#27573;&#20013;&#21306;&#22495;&#32423;&#21035;&#19978;&#20351;&#29992;&#20301;&#32622;&#23884;&#20837;&#30340;&#26041;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29992;&#32858;&#28966;&#25439;&#22833;&#26367;&#25442;&#20102;&#23545;&#27604;&#23398;&#20064;&#20013;&#24120;&#29992;&#30340;softmax&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#37027;&#20123;&#26377;&#20449;&#24687;&#37327;&#20294;&#38590;&#20197;&#25429;&#25417;&#30340;&#20363;&#23376;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#26368;&#36817;&#22312;&#26032;&#39062;&#29289;&#20307;&#25552;&#26696;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#20197;&#25913;&#36827;&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;LVIS&#21644;COCO&#24320;&#25918;&#35789;&#27719;&#26816;&#27979;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#23436;&#25972;&#27169;&#22411;&#21644;&#38646;-shot&#36716;&#31227;&#24615;&#33021;&#12290;RO-ViT&#22312;LVIS&#19978;&#23454;&#29616;&#20102;32.1$AP_r$&#30340;&#26368;&#20339;&#25928;&#26524;&#65292;&#36229;&#36807;&#29616;&#26377;&#26368;&#20339;&#26041;&#27861;5.8&#20010;&#30334;&#20998;&#28857;&#65292;&#21516;&#26102;&#36824;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#38646;-shot&#36716;&#31227;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Region-aware Open-vocabulary Vision Transformers (RO-ViT) - a contrastive image-text pretraining recipe to bridge the gap between image-level pretraining and open-vocabulary object detection. At the pretraining phase, we propose to randomly crop and resize regions of positional embeddings instead of using the whole image positional embeddings. This better matches the use of positional embeddings at region-level in the detection finetuning phase. In addition, we replace the common softmax cross entropy loss in contrastive learning with focal loss to better learn the informative yet difficult examples. Finally, we leverage recent advances in novel object proposals to improve open-vocabulary detection finetuning. We evaluate our full model on the LVIS and COCO open-vocabulary detection benchmarks and zero-shot transfer. RO-ViT achieves a state-of-the-art 32.1 $AP_r$ on LVIS, surpassing the best existing approach by +5.8 points in addition to competitive zero-shot transfer detec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.09901</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#19978;&#30340;mCPT&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#26694;&#26550;&#26816;&#27979;&#30340;&#22810;&#35821;&#35328;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
mCPT at SemEval-2023 Task 3: Multilingual Label-Aware Contrastive Pre-Training of Transformers for Few- and Zero-shot Framing Detection. (arXiv:2303.09901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;mCPT&#27169;&#22411;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#12289;&#22810;&#26631;&#31614;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#21644;&#20854;&#20182;8&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38646;&#26679;&#26412;&#30340;&#35199;&#29677;&#29273;&#35821;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#33719;&#32988;&#31995;&#32479;&#65292;&#24182;&#22312;&#21478;&#22806;&#20843;&#31181;&#35821;&#35328;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25104;&#32489;&#12290;&#26694;&#26550;&#26816;&#27979;&#20219;&#21153;&#30340;&#25361;&#25112;&#22312;&#20110;&#22312;&#21482;&#26377;&#23569;&#37327;&#25110;&#38646;&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#19968;&#32452;14&#20010;&#26694;&#26550;&#65292;&#21363;&#22810;&#35821;&#35328;&#22810;&#26631;&#31614;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;&#35299;&#20915;&#26041;&#26696;&#37319;&#29992;&#20102;&#22522;&#20110;&#22810;&#35821;&#35328;&#21464;&#21387;&#22120;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#65292;&#20351;&#29992;&#26631;&#31614;&#24863;&#30693;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#12290;&#38500;&#20102;&#25551;&#36848;&#31995;&#32479;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23884;&#20837;&#31354;&#38388;&#20998;&#26512;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#22914;&#20309;&#25903;&#25345;&#26694;&#26550;&#26816;&#27979;&#20197;&#25512;&#36827;&#35745;&#31639;&#26694;&#26550;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the winning system for the zero-shot Spanish framing detection task, which also achieves competitive places in eight additional languages. The challenge of the framing detection task lies in identifying a set of 14 frames when only a few or zero samples are available, i.e., a multilingual multi-label few- or zero-shot setting. Our developed solution employs a pre-training procedure based on multilingual Transformers using a label-aware contrastive loss function. In addition to describing the system, we perform an embedding space analysis and ablation study to demonstrate how our pre-training procedure supports framing detection to advance computational framing analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ChatExtract&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;&#65292;&#33258;&#21160;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#21069;&#26399;&#21162;&#21147;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2303.05352</link><description>&lt;p&gt;
&#20351;&#29992;&#20250;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#26448;&#26009;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Extracting Accurate Materials Data from Research Papers with Conversational Language Models and Prompt Engineering. (arXiv:2303.05352v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ChatExtract&#26041;&#27861;&#65292;&#20351;&#29992;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;&#65292;&#33258;&#21160;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#20934;&#30830;&#30340;&#25968;&#25454;&#65292;&#19981;&#38656;&#35201;&#22823;&#37327;&#30340;&#21069;&#26399;&#21162;&#21147;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#27491;&#22312;&#19981;&#26029;&#21162;&#21147;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#36817;&#20986;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#26367;&#25163;&#24037;&#20174;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#25968;&#25454;&#30340;&#24037;&#20316;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#30740;&#31350;&#35770;&#25991;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#21069;&#26399;&#21162;&#21147;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#32534;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatExtract&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#20808;&#36827;&#30340;&#23545;&#35805;&#24335;LLM&#33258;&#21160;&#25552;&#21462;&#26497;&#20934;&#30830;&#30340;&#25968;&#25454;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#21021;&#26399;&#30340;&#21162;&#21147;&#21644;&#32972;&#26223;&#30693;&#35782;&#12290;ChatExtract&#30001;&#19968;&#32452;&#24037;&#31243;&#21270;&#30340;&#25552;&#31034;&#24212;&#29992;&#20110;&#23545;&#35805;&#24335;LLM&#65292;&#26082;&#21487;&#20197;&#35782;&#21035;&#20986;&#20855;&#26377;&#25968;&#25454;&#30340;&#21477;&#23376;&#65292;&#25552;&#21462;&#20986;&#36825;&#20123;&#25968;&#25454;&#65292;&#21448;&#21487;&#20197;&#36890;&#36807;&#19968;&#31995;&#21015;&#36319;&#36827;&#38382;&#39064;&#30830;&#20445;&#25968;&#25454;&#30340;&#27491;&#30830;&#24615;&#12290;&#36825;&#20123;&#36319;&#36827;&#38382;&#39064;&#24456;&#22823;&#31243;&#24230;&#19978;&#20811;&#26381;&#20102;LLM&#25552;&#20379;&#20107;&#23454;&#19981;&#20934;&#30830;&#31572;&#26696;&#30340;&#24050;&#30693;&#38382;&#39064;&#12290;ChatExtract&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#23545;&#35805;&#24335;LLM&#65292;&#24182;&#33021;&#25552;&#20379;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a growing effort to replace hand extraction of data from research papers with automated data extraction based on natural language processing, language models, and recently, large language models (LLMs). Although these methods enable efficient extraction of data from large sets of research papers, they require a significant amount of up-front effort, expertise, and coding. In this work we propose the ChatExtract method that can fully automate very accurate data extraction with minimal initial effort and background, using an advanced conversational LLM. ChatExtract consists of a set of engineered prompts applied to a conversational LLM that both identify sentences with data, extract that data, and assure the data's correctness through a series of follow-up questions. These follow-up questions largely overcome known issues with LLMs providing factually inaccurate responses. ChatExtract can be applied with any conversational LLMs and yields very high quality data extraction.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2302.13939</link><description>&lt;p&gt;
SpikeGPT&#65306;&#24102;&#26377;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. (arXiv:2302.13939v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;SpikeGPT&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#36827;&#34892;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;SNN&#35757;&#32451;&#20013;&#30340;&#25361;&#25112;&#24615;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#19981;&#26029;&#25193;&#22823;&#65292;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#20063;&#38543;&#20043;&#22686;&#21152;&#12290;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#33021;&#22815;&#21033;&#29992;&#31232;&#30095;&#21644;&#20107;&#20214;&#39537;&#21160;&#28608;&#27963;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#35745;&#31639;&#24320;&#38144;&#30340;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#19978;&#24050;&#32463;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#20294;SNN&#30340;&#35757;&#32451;&#20063;&#34987;&#35777;&#26126;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#33853;&#21518;&#20110;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#65292;&#25105;&#20204;&#23578;&#26410;&#30475;&#21040;SNN&#22312;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;Receptance Weighted Key Value&#65288;RWKV&#65289;&#35821;&#35328;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;&#8220;SpikeGPT&#8221;&#65292;&#23427;&#26159;&#19968;&#31181;&#20855;&#26377;&#20108;&#36827;&#21046;&#12289;&#20107;&#20214;&#39537;&#21160;&#33033;&#20914;&#28608;&#27963;&#21333;&#20803;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27169;&#22411;&#21464;&#20307;&#19978;&#35757;&#32451;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65306;45M&#21644;216M&#21442;&#25968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SpikeGPT&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#21453;&#21521;&#20256;&#25773;&#35757;&#32451;SNN&#27169;&#22411;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#38750;&#33033;&#20914;&#27169;&#22411;&#36890;&#24120;&#35299;&#20915;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suita
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;&#26469;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;&#27835;&#29702;&#23457;&#35745;&#12289;&#27169;&#22411;&#23457;&#35745;&#21644;&#24212;&#29992;&#23457;&#35745;&#65292;&#35299;&#20915;LLMs&#24102;&#26469;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.08500</link><description>&lt;p&gt;
&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auditing large language models: a three-layered approach. (arXiv:2302.08500v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08500
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;&#26469;&#23457;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;&#27835;&#29702;&#23457;&#35745;&#12289;&#27169;&#22411;&#23457;&#35745;&#21644;&#24212;&#29992;&#23457;&#35745;&#65292;&#35299;&#20915;LLMs&#24102;&#26469;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#24191;&#27867;&#20351;&#29992;&#20063;&#20276;&#38543;&#30528;&#37325;&#22823;&#30340;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#23457;&#35745;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#27835;&#29702;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#30830;&#20445;AI&#31995;&#32479;&#35774;&#35745;&#21644;&#37096;&#32626;&#30340;&#36947;&#24503;&#12289;&#27861;&#24459;&#21644;&#25216;&#26415;&#30340;&#20581;&#22766;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23457;&#35745;&#31243;&#24207;&#26080;&#27861;&#35299;&#20915;LLMs&#24102;&#26469;&#30340;&#27835;&#29702;&#25361;&#25112;&#65292;&#22240;&#20026;LLMs&#26174;&#31034;&#20986;&#26032;&#20852;&#33021;&#21147;&#65292;&#24182;&#21487;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#27010;&#36848;&#19968;&#31181;&#26032;&#39062;&#30340;&#23457;&#35745;LLMs&#30340;&#34013;&#22270;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#23618;&#27425;&#30340;&#26041;&#27861;&#65292;&#21363;&#27835;&#29702;&#23457;&#35745;&#65288;&#38024;&#23545;&#35774;&#35745;&#21644;&#20256;&#25773;LLMs&#30340;&#25216;&#26415;&#25552;&#20379;&#21830;&#65289;&#12289;&#27169;&#22411;&#23457;&#35745;&#65288;&#38024;&#23545;LLMs&#36827;&#34892;&#39044;&#35757;&#32451;&#20294;&#23578;&#26410;&#21457;&#24067;&#30340;&#23457;&#35745;&#65289;&#21644;&#24212;&#29992;&#23457;&#35745;&#65288;&#22522;&#20110;LLMs&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#23457;&#35745;&#65289;&#65292;&#30456;&#20114;&#34917;&#20805;&#21644;&#30456;&#20114;&#36890;&#30693;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23457;&#35745;&#22312;LLMs&#19978;&#30340;&#23454;&#26045;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#20262;&#29702;&#21644;&#31038;&#20250;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) represent a major advance in artificial intelligence (AI) research. However, the widespread use of LLMs is also coupled with significant ethical and social challenges. Previous research has pointed towards auditing as a promising governance mechanism to help ensure that AI systems are designed and deployed in ways that are ethical, legal, and technically robust. However, existing auditing procedures fail to address the governance challenges posed by LLMs, which display emergent capabilities and are adaptable to a wide range of downstream tasks. In this article, we address that gap by outlining a novel blueprint for how to audit LLMs. Specifically, we propose a three-layered approach, whereby governance audits (of technology providers that design and disseminate LLMs), model audits (of LLMs after pre-training but prior to their release), and application audits (of applications based on LLMs) complement and inform each other. We show how audits, when conducte
&lt;/p&gt;</description></item><item><title>Mu$^{2}$SLAM&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#21033;&#29992;&#30417;&#30563;&#20219;&#21153;&#25552;&#39640;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#34920;&#31034;&#23545;&#40784;&#65292;&#21462;&#24471;&#20102;&#22312;CoVoST AST&#19978;&#26032;&#30340;&#26368;&#39640;&#24615;&#33021;&#65292;&#24182;&#19982;&#20351;&#29992;RNN&#24494;&#35843;&#30340;mSLAM&#27169;&#22411;&#22312;Voxpopuli ASR&#19978;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2212.09553</link><description>&lt;p&gt;
Mu$^{2}$SLAM: &#22810;&#20219;&#21153;&#12289;&#22810;&#35821;&#35328;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mu$^{2}$SLAM: Multitask, Multilingual Speech and Language Models. (arXiv:2212.09553v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09553
&lt;/p&gt;
&lt;p&gt;
Mu$^{2}$SLAM&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#35821;&#38899;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#21033;&#29992;&#30417;&#30563;&#20219;&#21153;&#25552;&#39640;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#34920;&#31034;&#23545;&#40784;&#65292;&#21462;&#24471;&#20102;&#22312;CoVoST AST&#19978;&#26032;&#30340;&#26368;&#39640;&#24615;&#33021;&#65292;&#24182;&#19982;&#20351;&#29992;RNN&#24494;&#35843;&#30340;mSLAM&#27169;&#22411;&#22312;Voxpopuli ASR&#19978;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Mu$^{2}$SLAM&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#36229;&#36807;100&#31181;&#35821;&#35328;&#30340;&#26410;&#26631;&#35760;&#35821;&#38899;&#12289;&#26410;&#26631;&#35760;&#25991;&#26412;&#21644;&#30417;&#30563;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#35206;&#30422;&#20102;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12289;&#33258;&#21160;&#35821;&#38899;&#32763;&#35793;&#65288;AST&#65289;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#12290;&#36890;&#36807;&#21033;&#29992;&#37327;&#21270;&#34920;&#31034;&#30340;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#65292;Mu$^{2}$SLAM&#20351;&#29992;&#31867;&#20284;&#20110;T5&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#25513;&#34109;&#21435;&#22122;&#30446;&#26631;&#22312;&#35299;&#30721;&#22120;&#19978;&#35757;&#32451;&#35821;&#38899;-&#25991;&#26412;&#27169;&#22411;&#65292;&#24182;&#22312;&#32534;&#30721;&#22120;&#19978;&#20351;&#29992;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#30446;&#26631;&#26469;&#35757;&#32451;&#26410;&#26631;&#35760;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#21516;&#26102;&#21033;&#29992;&#30417;&#30563;&#20219;&#21153;&#26469;&#25552;&#39640;&#27169;&#22411;&#20869;&#30340;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24577;&#34920;&#31034;&#23545;&#40784;&#12290;&#22312;CoVoST AST&#19978;&#65292;Mu$^{2}$SLAM&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26032;&#30340;&#26368;&#39640;&#27700;&#24179;&#65292;&#22312;XX-EN&#32763;&#35793;&#19978;&#27604;&#20043;&#21069;&#26368;&#20248;&#32467;&#26524;&#25552;&#39640;&#20102;1.9 BLEU&#20998;&#65292;&#23545;&#20110;EN-XX&#32763;&#35793;&#25552;&#39640;&#20102;1.1 BLEU&#20998;&#12290;&#22312;Voxpopuli ASR&#19978;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20351;&#29992;RNN&#24494;&#35843;&#30340;mSLAM&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Mu$^{2}$SLAM, a multilingual sequence-to-sequence model pre-trained jointly on unlabeled speech, unlabeled text and supervised data spanning Automatic Speech Recognition (ASR), Automatic Speech Translation (AST) and Machine Translation (MT), in over 100 languages. By leveraging a quantized representation of speech as a target, Mu$^{2}$SLAM trains the speech-text models with a sequence-to-sequence masked denoising objective similar to T5 on the decoder and a masked language modeling (MLM) objective on the encoder, for both unlabeled speech and text, while utilizing the supervised tasks to improve cross-lingual and cross-modal representation alignment within the model. On CoVoST AST, Mu$^{2}$SLAM establishes a new state-of-the-art for models trained on public datasets, improving on xx-en translation over the previous best by 1.9 BLEU points and on en-xx translation by 1.1 BLEU points. On Voxpopuli ASR, our model matches the performance of an mSLAM model fine-tuned with an RNN-
&lt;/p&gt;</description></item><item><title>WACO&#26159;&#19968;&#31181;&#29992;&#20110;&#26497;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#35789;&#32423;&#34920;&#31034;&#36830;&#25509;&#36215;&#26469;&#65292;&#23454;&#39564;&#35777;&#26126;WACO&#22312;&#26497;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#27604;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;9+ BLEU&#20998;&#12290;</title><link>http://arxiv.org/abs/2212.09359</link><description>&lt;p&gt;
WACO: &#29992;&#20110;&#35821;&#38899;&#32763;&#35793;&#30340;&#35789;&#23545;&#40784;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
WACO: Word-Aligned Contrastive Learning for Speech Translation. (arXiv:2212.09359v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09359
&lt;/p&gt;
&lt;p&gt;
WACO&#26159;&#19968;&#31181;&#29992;&#20110;&#26497;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#35789;&#32423;&#34920;&#31034;&#36830;&#25509;&#36215;&#26469;&#65292;&#23454;&#39564;&#35777;&#26126;WACO&#22312;&#26497;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#27604;&#22522;&#32447;&#26041;&#27861;&#25552;&#39640;&#20102;9+ BLEU&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;E2E ST&#65289;&#26088;&#22312;&#30452;&#25509;&#23558;&#28304;&#35821;&#38899;&#36716;&#21270;&#20026;&#30446;&#26631;&#25991;&#26412;&#12290;&#24403;&#20165;&#26377;&#26497;&#23569;&#30340;&#35821;&#38899;&#25991;&#26412;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#26102;&#65292;&#29616;&#26377;&#30340;ST&#26041;&#27861;&#30340;&#34920;&#29616;&#24456;&#24046;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;ST&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#20854;&#35821;&#38899;&#21644;&#28304;&#25991;&#26412;&#20043;&#38388;&#30340;&#23884;&#20837;&#30456;&#20284;&#24230;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Word-Aligned COntrastive learning&#65288;WACO&#65289;&#30340;&#31616;&#21333;&#26377;&#25928;&#30340;&#26497;&#20302;&#36164;&#28304;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#26469;&#24314;&#31435;&#35821;&#38899;&#21644;&#25991;&#26412;&#27169;&#24577;&#30340;&#35789;&#32423;&#34920;&#31034;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#25105;&#20204;&#22312;MuST-C&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;WACO&#21644;&#20854;&#20182;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ST&#22522;&#20934;&#65292;&#24182;&#22312;&#20174;IWSLT 2023&#33719;&#21462;&#30340;&#20302;&#36164;&#28304;&#26041;&#21521;&#30340;&#39532;&#32819;&#20182;&#35821;-&#33521;&#35821;&#32763;&#35793;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WACO&#20165;&#20351;&#29992;1&#23567;&#26102;&#30340;&#24182;&#34892;ST&#25968;&#25454;&#65292;&#27604;&#26368;&#22909;&#30340;&#22522;&#20934;&#25552;&#39640;&#20102;9+ BLEU&#20998;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/owaski/WACO&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end Speech Translation (E2E ST) aims to directly translate source speech into target text. Existing ST methods perform poorly when only extremely small speech-text data are available for training. We observe that an ST model's performance closely correlates with its embedding similarity between speech and source transcript. In this paper, we propose Word-Aligned COntrastive learning (WACO), a simple and effective method for extremely low-resource speech-to-text translation. Our key idea is bridging word-level representations for both speech and text modalities via contrastive learning. We evaluate WACO and other methods on the MuST-C dataset, a widely used ST benchmark, and on a low-resource direction Maltese-English from IWSLT 2023. Our experiments demonstrate that WACO outperforms the best baseline by 9+ BLEU points with only 1-hour parallel ST data. Code is available at https://github.com/owaski/WACO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#8212;&#8212;&#26354;&#32447;&#23545;&#27604;&#23398;&#20064;&#65288;CCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#36718;&#23545;&#35805;&#20013;&#21457;&#35328;&#23545;&#20043;&#38388;&#30340;&#30456;&#23545;&#36716;&#24367;&#36317;&#31163;&#12290;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23558;&#30446;&#26631;&#21457;&#35328;&#21644;&#22238;&#22797;&#20505;&#36873;&#39033;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#35780;&#20272;&#20505;&#36873;&#21457;&#35328;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#32447;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#35780;&#20272;&#24207;&#21015;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#24819;&#35937;&#26410;&#26469;&#23545;&#35805;&#27169;&#24335;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.07591</link><description>&lt;p&gt;
&#24819;&#35937;&#21147;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#19968;&#20999;&#65281;&#29992;&#20110;&#38271;&#26399;&#23545;&#35805;&#35268;&#21010;&#30340;&#26354;&#32447;&#23545;&#27604;&#23398;&#20064;&#30340;&#25277;&#35937;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Imagination is All You Need! Curved Contrastive Learning for Abstract Sequence Modeling Utilized on Long Short-Term Dialogue Planning. (arXiv:2211.07591v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#8212;&#8212;&#26354;&#32447;&#23545;&#27604;&#23398;&#20064;&#65288;CCL&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#36718;&#23545;&#35805;&#20013;&#21457;&#35328;&#23545;&#20043;&#38388;&#30340;&#30456;&#23545;&#36716;&#24367;&#36317;&#31163;&#12290;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#23558;&#30446;&#26631;&#21457;&#35328;&#21644;&#22238;&#22797;&#20505;&#36873;&#39033;&#26144;&#23556;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#35780;&#20272;&#20505;&#36873;&#21457;&#35328;&#21644;&#30446;&#26631;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#32447;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#35780;&#20272;&#24207;&#21015;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#24819;&#35937;&#26410;&#26469;&#23545;&#35805;&#27169;&#24335;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#26102;&#31354;&#26354;&#29575;&#21551;&#21457;&#65288;&#29233;&#22240;&#26031;&#22374;&#65292;1921&#65289;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26354;&#32447;&#23545;&#27604;&#23398;&#20064;&#65288;CCL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#29992;&#20110;&#23398;&#20064;&#22810;&#36718;&#23545;&#35805;&#20013;&#21457;&#35328;&#23545;&#20043;&#38388;&#30340;&#30456;&#23545;&#36716;&#24367;&#36317;&#31163;&#12290;&#36890;&#36807;&#23558;&#30446;&#26631;&#21457;&#35328;&#21644;&#30456;&#24212;&#30340;&#22238;&#22797;&#20505;&#36873;&#39033;&#25237;&#24433;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#30340;&#21452;&#32534;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#24341;&#23548;&#21464;&#21387;&#22120;&#20316;&#20026;&#21709;&#24212;&#25490;&#24207;&#27169;&#22411;&#20197;&#38646;-shot&#26041;&#24335;&#26397;&#30528;&#30446;&#26631;&#21069;&#36827;&#12290;&#36825;&#37324;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#34920;&#31034;&#20102;&#20505;&#36873;&#21457;&#35328;&#26397;&#30528;&#30456;&#24212;&#30446;&#26631;&#30340;&#36317;&#31163;/&#21487;&#36798;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21069;&#21521;&#21551;&#31034;&#24615;&#35821;&#35328;&#34920;&#31034;&#26469;&#35780;&#20272;&#24207;&#21015;&#30340;&#21487;&#33021;&#24615;&#65292;&#21363;&#36890;&#36807;&#20854;&#21508;&#20010;&#25104;&#21592;&#65288;&#20998;&#21035;&#32534;&#30721;&#65289;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#20316;&#20026;&#26354;&#32447;&#31354;&#38388;&#20013;&#30340;&#26032;&#20852;&#23646;&#24615;&#12290;&#36825;&#20123;&#38750;&#23616;&#37096;&#23646;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#24819;&#35937;&#26410;&#26469;&#23545;&#35805;&#27169;&#24335;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#26159;&#36890;&#36807;&#25490;&#24207;/&#35782;&#21035;&#26410;&#26469;&#30340;go
&lt;/p&gt;
&lt;p&gt;
Inspired by the curvature of space-time (Einstein, 1921), we introduce Curved Contrastive Learning (CCL), a novel representation learning technique for learning the relative turn distance between utterance pairs in multi-turn dialogues. The resulting bi-encoder models can guide transformers as a response ranking model towards a goal in a zero-shot fashion by projecting the goal utterance and the corresponding reply candidates into a latent space. Here the cosine similarity indicates the distance/reachability of a candidate utterance toward the corresponding goal. Furthermore, we explore how these forward-entailing language representations can be utilized for assessing the likelihood of sequences by the entailment strength i.e. through the cosine similarity of its individual members (encoded separately) as an emergent property in the curved space. These non-local properties allow us to imagine the likelihood of future patterns in dialogues, specifically by ordering/identifying future go
&lt;/p&gt;</description></item><item><title>BLOOM&#26159;&#19968;&#20010;&#30001;&#25968;&#30334;&#21517;&#30740;&#31350;&#20154;&#21592;&#21512;&#20316;&#35774;&#35745;&#21644;&#26500;&#24314;&#30340;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36827;&#34892;&#22810;&#20219;&#21153;&#25552;&#31034;&#24494;&#35843;&#21518;&#34920;&#29616;&#26356;&#24378;&#12290;&#35813;&#27169;&#22411;&#30340;&#21457;&#24067;&#26377;&#21161;&#20110;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2211.05100</link><description>&lt;p&gt;
BLOOM: &#19968;&#20010;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BLOOM: A 176B-Parameter Open-Access Multilingual Language Model. (arXiv:2211.05100v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05100
&lt;/p&gt;
&lt;p&gt;
BLOOM&#26159;&#19968;&#20010;&#30001;&#25968;&#30334;&#21517;&#30740;&#31350;&#20154;&#21592;&#21512;&#20316;&#35774;&#35745;&#21644;&#26500;&#24314;&#30340;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#36827;&#34892;&#22810;&#20219;&#21153;&#25552;&#31034;&#24494;&#35843;&#21518;&#34920;&#29616;&#26356;&#24378;&#12290;&#35813;&#27169;&#22411;&#30340;&#21457;&#24067;&#26377;&#21161;&#20110;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
BLOOM is an open-access language model with 176B parameters, designed and built by hundreds of researchers. It achieves competitive performance on various benchmarks and shows even stronger results after multitask prompted finetuning. The release of this model facilitates the democratization of large language model technology.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26681;&#25454;&#23569;&#37327;&#28436;&#31034;&#25110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20123;&#33021;&#21147;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#22823;&#22810;&#25968;LLMs&#37117;&#26159;&#30001;&#36164;&#28304;&#20016;&#23500;&#30340;&#32452;&#32455;&#24320;&#21457;&#30340;&#65292;&#24182;&#32463;&#24120;&#34987;&#20445;&#23494;&#12290;&#20026;&#20102;&#25512;&#21160;&#36825;&#31181;&#24378;&#22823;&#25216;&#26415;&#30340;&#27665;&#20027;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BLOOM&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#25968;&#30334;&#21517;&#30740;&#31350;&#20154;&#21592;&#21512;&#20316;&#35774;&#35745;&#21644;&#26500;&#24314;&#30340;&#25317;&#26377;176B&#21442;&#25968;&#30340;&#24320;&#25918;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;BLOOM&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#22120;&#30340;Transformer&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;ROOTS&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;46&#31181;&#33258;&#28982;&#35821;&#35328;&#21644;13&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#25968;&#30334;&#20010;&#26469;&#28304;&#65288;&#24635;&#20849;59&#31181;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;BLOOM&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#22312;&#36827;&#34892;&#22810;&#20219;&#21153;&#25552;&#31034;&#24494;&#35843;&#21518;&#34920;&#29616;&#26356;&#24378;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#20351;&#29992;LLMs&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#65292;&#25105;&#20204;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#35768;&#21487;&#19979;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.
&lt;/p&gt;</description></item><item><title>SSD-LM&#26159;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#26102;&#28789;&#27963;&#29983;&#25104;&#25991;&#26412;&#22359;&#24182;&#23454;&#29616;&#26412;&#22320;&#19978;&#19979;&#25991;&#26356;&#26032;&#65292;&#20197;&#21450;&#22312;&#33258;&#28982;&#35789;&#27719;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#65292;&#23454;&#29616;&#20102;&#20998;&#31867;&#22120;&#25351;&#23548;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;&#12290;&#22312;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#19978;&#65292;SSD-LM&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#26174;&#33879;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.17432</link><description>&lt;p&gt;
SSD-LM: &#22522;&#20110;&#21322;&#33258;&#22238;&#24402;&#31616;&#21333;&#24418;&#24335;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#25991;&#26412;&#29983;&#25104;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
SSD-LM: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control. (arXiv:2210.17432v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17432
&lt;/p&gt;
&lt;p&gt;
SSD-LM&#26159;&#19968;&#31181;&#21322;&#33258;&#22238;&#24402;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35299;&#30721;&#26102;&#28789;&#27963;&#29983;&#25104;&#25991;&#26412;&#22359;&#24182;&#23454;&#29616;&#26412;&#22320;&#19978;&#19979;&#25991;&#26356;&#26032;&#65292;&#20197;&#21450;&#22312;&#33258;&#28982;&#35789;&#27719;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#65292;&#23454;&#29616;&#20102;&#20998;&#31867;&#22120;&#25351;&#23548;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;&#12290;&#22312;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#19978;&#65292;SSD-LM&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#27604;&#65292;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#26174;&#33879;&#36229;&#36234;&#20102;&#20854;&#20182;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#22312;&#36830;&#32493;&#20540;&#39046;&#22495;&#65288;&#22914;&#22270;&#20687;&#65289;&#21462;&#24471;&#20102;&#24456;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#31163;&#25955;&#39046;&#22495;&#65288;&#22914;&#25991;&#26412;&#65289;&#20013;&#65292;&#31867;&#20284;&#30340;&#21162;&#21147;&#21364;&#36824;&#27809;&#26377;&#36798;&#21040;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SSD-LM&#65292;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#20851;&#38190;&#35774;&#35745;&#36873;&#25321;&#12290;&#39318;&#20808;&#65292;SSD-LM&#26159;&#21322;&#33258;&#22238;&#24402;&#30340;&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#25991;&#26412;&#22359;&#65292;&#22312;&#35299;&#30721;&#26102;&#20801;&#35768;&#28789;&#27963;&#30340;&#36755;&#20986;&#38271;&#24230;&#65292;&#21516;&#26102;&#23454;&#29616;&#26412;&#22320;&#21452;&#21521;&#19978;&#19979;&#25991;&#26356;&#26032;&#12290;&#20854;&#27425;&#65292;&#23427;&#37319;&#29992;&#22522;&#20110;&#21333;&#32431;&#24418;&#30340;&#26041;&#27861;&#65292;&#22312;&#33258;&#28982;&#35789;&#27719;&#31354;&#38388;&#19978;&#36827;&#34892;&#25193;&#25955;&#65292;&#32780;&#19981;&#26159;&#22312;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#33258;&#36866;&#24212;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#29616;&#25104;&#30340;&#20998;&#31867;&#22120;&#23454;&#29616;&#20998;&#31867;&#22120;&#25351;&#23548;&#21644;&#27169;&#22359;&#21270;&#25511;&#21046;&#12290;&#25105;&#20204;&#22312;&#26080;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102; SSD-LM&#65292;&#24182;&#23637;&#31034;&#23427;&#22312;&#26631;&#20934;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#24230;&#37327;&#19978;&#19982;&#24378;&#22823;&#30340;&#33258;&#22238;&#24402; GPT-2 &#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#65292;&#24182;&#19988;&#36828;&#36828;&#20248;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present SSD-LM -- a diffusion-based language model with two key design choices. First, SSD-LM is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate SSD-LM on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive GPT-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30465;&#30053;&#25240;&#21472;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;CTC&#21457;&#23556;&#26469;&#21152;&#24555;&#35299;&#30721;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#27604;&#26222;&#36890;&#35299;&#30721;&#24555;78%&#30340;&#36895;&#24230;&#65292;&#24182;&#19988;&#20934;&#30830;&#24230;&#30340;&#25439;&#22833;&#38750;&#24120;&#23567;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#25968;&#23398;&#20381;&#25454;&#12290;</title><link>http://arxiv.org/abs/2210.17017</link><description>&lt;p&gt;
&#30465;&#30053;&#25240;&#21472;&#65306;&#21387;&#32553;CTC&#21457;&#23556;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Blank Collapse: Compressing CTC emission for the faster decoding. (arXiv:2210.17017v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30465;&#30053;&#25240;&#21472;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21387;&#32553;CTC&#21457;&#23556;&#26469;&#21152;&#24555;&#35299;&#30721;&#36895;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#27604;&#26222;&#36890;&#35299;&#30721;&#24555;78%&#30340;&#36895;&#24230;&#65292;&#24182;&#19988;&#20934;&#30830;&#24230;&#30340;&#25439;&#22833;&#38750;&#24120;&#23567;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#29702;&#35770;&#19978;&#26377;&#25968;&#23398;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#25509;&#26102;&#24207;&#20998;&#31867;&#65288;CTC&#65289;&#27169;&#22411;&#26159;&#19968;&#31181;&#38750;&#24120;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#35821;&#38899;&#25968;&#25454;&#12290;&#20026;&#20102;&#23558;CTC&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#65292;&#38656;&#35201;&#20351;&#29992;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;n-gram LM&#65289;&#36827;&#34892;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#20197;&#33719;&#24471;&#21512;&#29702;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;CTC&#26463;&#25628;&#32034;&#20013;&#30340;&#31354;&#26631;&#31614;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24555;&#30340;&#26463;&#25628;&#32034;&#35299;&#30721;&#36895;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21487;&#20197;&#33719;&#24471;&#27604;&#26222;&#36890;&#26463;&#25628;&#32034;&#35299;&#30721;&#24555;78%&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#21516;&#26102;&#25439;&#22833;&#24456;&#23567;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22312;&#23454;&#36341;&#20013;&#26377;&#25928;&#65292;&#32780;&#19988;&#22312;&#25968;&#23398;&#19978;&#20063;&#26159;&#26377;&#29702;&#35770;&#20381;&#25454;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#22914;&#26524;&#27169;&#22411;&#30340;&#20934;&#30830;&#24230;&#26356;&#39640;&#65292;&#21017;&#36825;&#31181;&#20943;&#23569;&#25928;&#26524;&#26356;&#21152;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
Connectionist Temporal Classification (CTC) model is a very efficient method for modeling sequences, especially for speech data. In order to use CTC model as an Automatic Speech Recognition (ASR) task, the beam search decoding with an external language model like n-gram LM is necessary to obtain reasonable results. In this paper we analyze the blank label in CTC beam search deeply and propose a very simple method to reduce the amount of calculation resulting in faster beam search decoding speed. With this method, we can get up to 78% faster decoding speed than ordinary beam search decoding with a very small loss of accuracy in LibriSpeech datasets. We prove this method is effective not only practically by experiments but also theoretically by mathematical reasoning. We also observe that this reduction is more obvious if the accuracy of the model is higher.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2205.10828</link><description>&lt;p&gt;
&#21387;&#32553;&#22411;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20250;&#24573;&#30053;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Do Compressed Multilingual Machine Translation Models Forget?. (arXiv:2205.10828v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38750;&#24120;&#24222;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#22823;&#23567;&#20351;&#24471;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#20013;&#24212;&#29992;&#23427;&#20204;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#21387;&#32553;&#25216;&#26415;&#21487;&#20197;&#22823;&#24133;&#20943;&#23567;&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#20943;&#23569;&#25512;&#29702;&#26102;&#38388;&#65292;&#24182;&#23545;&#39030;&#32423;&#25351;&#26631;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#23545;&#22810;&#20010;&#20219;&#21153;&#21644;/&#25110;&#35821;&#35328;&#36827;&#34892;&#24179;&#22343;&#30340;&#32508;&#21512;&#24615;&#33021;&#21487;&#33021;&#25513;&#30422;&#20102;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#21151;&#33021;&#19978;&#30340;&#20005;&#37325;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25152;&#32534;&#30721;&#30340;&#20559;&#35265;&#30340;&#25918;&#22823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#19981;&#21516;&#26426;&#22120;&#32763;&#35793;&#22522;&#20934;&#65288;FLORES-101&#12289;MT-Gender&#21644;DiBiMT&#65289;&#19978;&#30340;&#21387;&#32553;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#21387;&#32553;&#26041;&#27861;&#23545;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65288;MNMT&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#32676;&#20307;&#12289;&#24615;&#21035;&#21644;&#35821;&#20041;&#20559;&#24046;&#26041;&#38754;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#32780;&#24179;&#22343;BLEU&#24230;&#37327;&#20540;&#21017;&#27809;&#20160;&#20040;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, very large pre-trained models achieve state-of-the-art results in various natural language processing (NLP) tasks, but their size makes it more challenging to apply them in resource-constrained environments. Compression techniques allow to drastically reduce the size of the models and therefore their inference time with negligible impact on top-tier metrics. However, the general performance averaged across multiple tasks and/or languages may hide a drastic performance drop on under-represented features, which could result in the amplification of biases encoded by the models. In this work, we assess the impact of compression methods on Multilingual Neural Machine Translation models (MNMT) for various language groups, gender, and semantic biases by extensive analysis of compressed models on different machine translation benchmarks, i.e. FLORES-101, MT-Gender, and DiBiMT. We show that the performance of under-represented languages drops significantly, while the average BLEU metr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#35299;&#32544;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#32416;&#32544;&#21709;&#24212;&#36873;&#25321;&#65292;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#23545;&#38271;&#31687;&#22810;&#21442;&#19982;&#32773;&#23545;&#35805;&#30340;&#20998;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#32858;&#31867;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2110.12646</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#32416;&#32544;&#21709;&#24212;&#36873;&#25321;&#23454;&#29616;&#38646;&#26679;&#26412;&#23545;&#35805;&#35299;&#32544;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Dialogue Disentanglement by Self-Supervised Entangled Response Selection. (arXiv:2110.12646v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.12646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#35299;&#32544;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#30340;&#32416;&#32544;&#21709;&#24212;&#36873;&#25321;&#65292;&#26080;&#38656;&#26631;&#27880;&#25968;&#25454;&#21363;&#21487;&#23454;&#29616;&#23545;&#38271;&#31687;&#22810;&#21442;&#19982;&#32773;&#23545;&#35805;&#30340;&#20998;&#35299;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#32858;&#31867;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#35299;&#32544;&#26088;&#22312;&#23558;&#38271;&#31687;&#22810;&#21442;&#19982;&#32773;&#23545;&#35805;&#20013;&#30340;&#35805;&#35821;&#20998;&#32452;&#25104;&#32447;&#32034;&#12290;&#36825;&#23545;&#20110;&#35805;&#35821;&#20998;&#26512;&#21644;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#31561;&#19979;&#28216;&#24212;&#29992;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20316;&#20026;&#26500;&#24314;&#28165;&#27905;&#19978;&#19979;&#25991;/&#21709;&#24212;&#38598;&#30340;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#26631;&#35760;&#25152;&#26377;"&#22238;&#22797;&#33267;"&#38142;&#25509;&#38656;&#35201;&#19982;&#35805;&#35821;&#25968;&#37327;&#30340;&#24179;&#26041;&#32423;&#21035;&#30340;&#24037;&#20316;&#37327;&#65306;&#27880;&#37322;&#32773;&#24517;&#39035;&#26816;&#26597;&#25152;&#26377;&#20043;&#21069;&#30340;&#35805;&#35821;&#65292;&#20197;&#30830;&#23450;&#24403;&#21069;&#35805;&#35821;&#26159;&#22238;&#22797;&#32473;&#21738;&#20010;&#35805;&#35821;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#23545;&#35805;&#35299;&#32544;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26410;&#27880;&#37322;&#30340;&#20114;&#21160;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#23545;&#35805;&#35299;&#32544;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;25&#30340;&#32858;&#31867;F1&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;10%&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#21487;&#20197;&#36798;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Dialogue disentanglement aims to group utterances in a long and multi-participant dialogue into threads. This is useful for discourse analysis and downstream applications such as dialogue response selection, where it can be the first step to construct a clean context/response set. Unfortunately, labeling all~\emph{reply-to} links takes quadratic effort w.r.t the number of utterances: an annotator must check all preceding utterances to identify the one to which the current utterance is a reply. In this paper, we are the first to propose a~\textbf{zero-shot} dialogue disentanglement solution. Firstly, we train a model on a multi-participant response selection dataset harvested from the web which is not annotated; we then apply the trained model to perform zero-shot dialogue disentanglement. Without any labeled data, our model can achieve a cluster F1 score of 25. We also fine-tune the model using various amounts of labeled data. Experiments show that with only 10\% of the data, we achiev
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#35821;&#31687;&#24863;&#30693;&#22522;&#20934;&#65292;&#31995;&#32479;&#24615;&#22320;&#30830;&#23450;&#20102;&#38656;&#35201;&#19978;&#19979;&#25991;&#32763;&#35793;&#30340;&#29616;&#35937;&#12290;&#21457;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#29616;&#35937;&#30340;&#22256;&#38590;&#31243;&#24230;&#26377;&#38480;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2109.07446</link><description>&lt;p&gt;
&#20309;&#26102;&#38656;&#35201;&#19978;&#19979;&#25991;&#32763;&#35793;&#65311;&#19968;&#39033;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#35821;&#31687;&#24863;&#30693;&#22522;&#20934;&#65292;&#31995;&#32479;&#24615;&#22320;&#30830;&#23450;&#20102;&#38656;&#35201;&#19978;&#19979;&#25991;&#32763;&#35793;&#30340;&#29616;&#35937;&#12290;&#21457;&#29616;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#23545;&#20110;&#35299;&#20915;&#36825;&#20123;&#29616;&#35937;&#30340;&#22256;&#38590;&#31243;&#24230;&#26377;&#38480;&#65292;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#25552;&#20379;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#27491;&#30830;&#22788;&#29702;&#35821;&#31687;&#23545;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#20294;&#36825;&#20123;&#25913;&#36827;&#22312;&#24120;&#35265;&#30340;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#25351;&#26631;&#20013;&#24471;&#19981;&#21040;&#20805;&#20998;&#34913;&#37327;&#12290;&#26368;&#36817;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#35797;&#22270;&#38024;&#23545;&#19968;&#23567;&#37096;&#20998;&#35821;&#31687;&#29616;&#35937;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#32570;&#20047;&#23436;&#20840;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;&#35821;&#31687;&#24863;&#30693;&#65288;MuDA&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#26631;&#35760;&#22120;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35780;&#20272;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#35821;&#31687;&#29616;&#35937;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#36873;&#25321;&#30340;&#35821;&#31687;&#29616;&#35937;&#21463;&#21040;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#21487;&#31995;&#32479;&#22320;&#30830;&#23450;&#38656;&#35201;&#19978;&#19979;&#25991;&#30340;&#32763;&#35793;&#12290;&#25105;&#20204;&#30830;&#35748;&#20102;&#20043;&#21069;&#30740;&#31350;&#30340;&#35821;&#31687;&#29616;&#35937;&#30340;&#38590;&#24230;&#65292;&#24182;&#25581;&#31034;&#20102;&#20043;&#21069;&#26410;&#35299;&#20915;&#30340;&#20854;&#20182;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#24120;&#35265;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20165;&#23545;&#19978;&#19979;&#25991;&#26080;&#20851;&#27169;&#22411;&#36827;&#34892;&#20102;&#36731;&#24494;&#30340;&#25913;&#36827;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#24182;&#27809;&#26377;&#26377;&#25928;&#22788;&#29702;&#36825;&#20123;&#27495;&#20041;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;14&#31181;&#35821;&#35328;&#23545;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#65292;&#20197;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations requiring context. We confirm the difficulty of previously studied phenomena while uncovering others that were previously unaddressed. We find that common context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#30340;&#25345;&#32493;&#31215;&#32047;&#21644;&#36716;&#31227;&#26469;&#25552;&#39640;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.10909</link><description>&lt;p&gt;
&#20855;&#26377;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Neural Topic Modeling with Continual Lifelong Learning. (arXiv:2006.10909v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#25345;&#32493;&#32456;&#36523;&#23398;&#20064;&#30340;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21487;&#20197;&#22788;&#29702;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#30340;&#25345;&#32493;&#31215;&#32047;&#21644;&#36716;&#31227;&#26469;&#25552;&#39640;&#20027;&#39064;&#24314;&#27169;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#23398;&#20064;&#21560;&#24341;&#20102;&#20154;&#20204;&#23545;&#24314;&#31435;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20851;&#27880;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#19981;&#26029;&#31215;&#32047;&#21644;&#20256;&#36882;&#30693;&#35782;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#24191;&#27867;&#29992;&#20110;&#20174;&#25991;&#26723;&#38598;&#21512;&#20013;&#21457;&#29616;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#20363;&#22914;&#22312;&#19968;&#20010;&#23567;&#30340;&#65288;&#30701;&#65289;&#25991;&#26723;&#38598;&#21512;&#20013;&#65292;&#20027;&#39064;&#24314;&#27169;&#30340;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19981;&#36830;&#36143;&#30340;&#20027;&#39064;&#21644;&#27425;&#20248;&#30340;&#25991;&#26723;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#30340;&#32456;&#36523;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#25345;&#32493;&#22788;&#29702;&#25991;&#26723;&#38598;&#21512;&#30340;&#27969;&#65292;&#31215;&#32047;&#20027;&#39064;&#65292;&#24182;&#36890;&#36807;&#20174;&#22810;&#20010;&#28304;&#30340;&#30693;&#35782;&#36716;&#31227;&#25351;&#23548;&#26410;&#26469;&#30340;&#20027;&#39064;&#24314;&#27169;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#31232;&#30095;&#25968;&#25454;&#12290;&#22312;&#32456;&#36523;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#30740;&#31350;&#20102;&#20849;&#20139;&#30340;&#29983;&#25104;&#21516;&#28304;&#24615;&#65288;&#28508;&#22312;&#20027;&#39064;&#65289;&#20197;&#22312;&#32456;&#36523;&#20013;&#20256;&#36882;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#20197;&#21450;&#36890;&#36807;&#26032;&#30340;&#36873;&#25321;&#24615;&#36951;&#24536;&#26469;&#26368;&#23567;&#21270;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20197;&#20445;&#30041;&#36807;&#21435;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong learning has recently attracted attention in building machine learning systems that continually accumulate and transfer knowledge to help future learning. Unsupervised topic modeling has been popularly used to discover topics from document collections. However, the application of topic modeling is challenging due to data sparsity, e.g., in a small collection of (short) documents and thus, generate incoherent topics and sub-optimal document representations. To address the problem, we propose a lifelong learning framework for neural topic modeling that can continuously process streams of document collections, accumulate topics and guide future topic modeling tasks by knowledge transfer from several sources to better deal with the sparse data. In the lifelong process, we particularly investigate jointly: (1) sharing generative homologies (latent topics) over lifetime to transfer prior knowledge, and (2) minimizing catastrophic forgetting to retain the past learning via novel sele
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#20027;&#39064;&#34920;&#31034;&#21644;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#65292;&#23558;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2006.10632</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#12289;&#23545;&#35805;&#20027;&#39064;&#24863;&#30693;&#30340;&#31070;&#32463;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Explainable and Discourse Topic-aware Neural Language Understanding. (arXiv:2006.10632v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#30340;&#20027;&#39064;&#34920;&#31034;&#21644;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#65292;&#23558;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#65292;&#35813;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20027;&#39064;&#27169;&#22411;&#19982;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#20027;&#39064;&#23558;&#35821;&#35328;&#29702;&#35299;&#25193;&#23637;&#21040;&#21477;&#23376;&#20043;&#22806;&#30340;&#26356;&#24191;&#27867;&#30340;&#25991;&#26723;&#32423;&#19978;&#19979;&#25991;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#20027;&#39064;&#35821;&#20041;&#65292;&#20294;&#24573;&#30053;&#20102;&#25991;&#26723;&#20013;&#21477;&#23376;&#30340;&#20027;&#39064;&#23545;&#35805;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20026;&#27599;&#20010;&#28508;&#22312;&#20027;&#39064;&#30340;&#27604;&#20363;&#30456;&#23545;&#24212;&#22320;&#24341;&#20837;&#21487;&#35299;&#37322;&#24615;&#20027;&#39064;&#34920;&#31034;&#26469;&#25193;&#23637;&#30740;&#31350;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#25991;&#26723;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#24314;&#27169;&#20027;&#39064;&#23545;&#35805;&#65292;&#20445;&#30041;&#21477;&#23376;-&#20027;&#39064;&#20851;&#32852;&#20197;&#21450;&#25991;&#26723;-&#20027;&#39064;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#22797;&#21512;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#20027;&#39064;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#20013;&#21516;&#26102;&#21033;&#29992;&#28508;&#22312;&#20027;&#39064;&#21644;&#21487;&#35299;&#37322;&#20027;&#39064;&#20197;&#21450;&#21477;&#23376;&#32423;&#30340;&#20027;&#39064;&#23545;&#35805;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#22914;&#35821;&#35328;&#24314;&#27169;&#12289;&#35789;&#20041;&#28040;&#27495;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Marrying topic models and language models exposes language understanding to a broader source of document-level context beyond sentences via topics. While introducing topical semantics in language models, existing approaches incorporate latent document topic proportions and ignore topical discourse in sentences of the document. This work extends the line of research by additionally introducing an explainable topic representation in language understanding, obtained from a set of key terms correspondingly for each latent topic of the proportion. Moreover, we retain sentence-topic associations along with document-topic association by modeling topical discourse for every sentence in the document. We present a novel neural composite language model that exploits both the latent and explainable topics along with topical discourse at sentence-level in a joint learning framework of topic and language models. Experiments over a range of tasks such as language modeling, word sense disambiguation, 
&lt;/p&gt;</description></item></channel></rss>