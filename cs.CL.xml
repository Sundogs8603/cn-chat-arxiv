<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>BianQue&#26159;&#19968;&#31181;&#22522;&#20110;ChatGLM&#36827;&#34892;&#24494;&#35843;&#30340;LLMs&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;LLMs&#30340;&#22810;&#36718;&#25552;&#38382;&#33021;&#21147;&#65292;&#20197;&#24179;&#34913;&#25552;&#38382;&#21644;&#24314;&#35758;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20581;&#24247;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2310.15896</link><description>&lt;p&gt;
BianQue: &#29992;ChatGPT&#23545;&#20581;&#24247;LLMs&#36827;&#34892;&#22810;&#36718;&#20250;&#35805;&#20248;&#21270;&#65292;&#24179;&#34913;&#25552;&#38382;&#21644;&#24314;&#35758;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
BianQue: Balancing the Questioning and Suggestion Ability of Health LLMs with Multi-turn Health Conversations Polished by ChatGPT. (arXiv:2310.15896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15896
&lt;/p&gt;
&lt;p&gt;
BianQue&#26159;&#19968;&#31181;&#22522;&#20110;ChatGLM&#36827;&#34892;&#24494;&#35843;&#30340;LLMs&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;LLMs&#30340;&#22810;&#36718;&#25552;&#38382;&#33021;&#21147;&#65292;&#20197;&#24179;&#34913;&#25552;&#38382;&#21644;&#24314;&#35758;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#26356;&#21152;&#20010;&#24615;&#21270;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20581;&#24247;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21333;&#36718;&#20250;&#35805;&#20013;&#25552;&#20379;&#24191;&#27867;&#30340;&#20581;&#24247;&#24314;&#35758;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20363;&#22914;ChatGPT&#12289;ChatGLM&#12289;ChatDoctor&#12289;DoctorGLM&#31561;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#29992;&#25143;&#22312;&#21333;&#36718;&#20013;&#25552;&#20379;&#30340;&#20449;&#24687;&#26377;&#38480;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#24314;&#35758;&#20010;&#24615;&#21270;&#21644;&#38024;&#23545;&#24615;&#19981;&#36275;&#65292;&#38656;&#35201;&#29992;&#25143;&#29420;&#31435;&#36873;&#25321;&#26377;&#29992;&#30340;&#37096;&#20998;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#21442;&#19982;&#22810;&#36718;&#25552;&#38382;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#25913;&#36827;LLMs&#30340;&#22810;&#36718;&#25552;&#38382;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BianQue&#65292;&#19968;&#31181;&#22522;&#20110;ChatGLM&#30340;LLMs&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#24314;&#30340;&#20581;&#24247;&#23545;&#35805;&#25968;&#25454;&#38598;BianQueCorpus&#36827;&#34892;&#24494;&#35843;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22810;&#36718;&#25552;&#38382;&#21644;&#20581;&#24247;&#24314;&#35758;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have performed well in providing general and extensive health suggestions in single-turn conversations, exemplified by systems such as ChatGPT, ChatGLM, ChatDoctor, DoctorGLM, and etc. However, the limited information provided by users during single turn results in inadequate personalization and targeting of the generated suggestions, which requires users to independently select the useful part. It is mainly caused by the missing ability to engage in multi-turn questioning. In real-world medical consultations, doctors usually employ a series of iterative inquiries to comprehend the patient's condition thoroughly, enabling them to provide effective and personalized suggestions subsequently, which can be defined as chain of questioning (CoQ) for LLMs. To improve the CoQ of LLMs, we propose BianQue, a ChatGLM-based LLM finetuned with the self-constructed health conversation dataset BianQueCorpus that is consist of multiple turns of questioning and health sugge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#27861;&#35821;&#25968;&#25454;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#65292;&#25506;&#32034;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21457;&#29616;&#21333;&#35789;&#30340;&#24615;&#21035;&#23646;&#24615;&#20197;&#21450;&#20854;&#20351;&#29992;&#35268;&#21017;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#24615;&#21035;&#20449;&#24687;&#25110;&#34920;&#29616;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.15852</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#27861;&#35821;&#25968;&#25454;&#65292;&#20102;&#35299;Transformer&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Using Artificial French Data to Understand the Emergence of Gender Bias in Transformer Language Models. (arXiv:2310.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#27861;&#35821;&#25968;&#25454;&#29983;&#25104;&#30340;&#35821;&#26009;&#24211;&#65292;&#25506;&#32034;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21457;&#29616;&#21333;&#35789;&#30340;&#24615;&#21035;&#23646;&#24615;&#20197;&#21450;&#20854;&#20351;&#29992;&#35268;&#21017;&#65292;&#24182;&#30740;&#31350;&#20102;&#27169;&#22411;&#22312;&#19981;&#21516;&#26465;&#20214;&#19979;&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#24615;&#21035;&#20449;&#24687;&#25110;&#34920;&#29616;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#30452;&#25509;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#23398;&#20064;&#21508;&#31181;&#35821;&#35328;&#23646;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30528;&#25163;&#25506;&#32034;&#31070;&#32463;&#27169;&#22411;&#22914;&#20309;&#21457;&#29616;&#21333;&#35789;&#30340;&#35821;&#35328;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#65289;&#20197;&#21450;&#35268;&#21017;&#30340;&#20351;&#29992;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#20010;&#30456;&#23545;&#23569;&#26377;&#30740;&#31350;&#30340;&#35805;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#30001;&#22522;&#20110;&#27861;&#35821;&#30340;PCFG&#29983;&#25104;&#30340;&#20154;&#24037;&#35821;&#26009;&#24211;&#65292;&#31934;&#30830;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24615;&#21035;&#20998;&#24067;&#65292;&#24182;&#30830;&#23450;&#27169;&#22411;&#22312;&#21738;&#31181;&#26465;&#20214;&#19979;&#33021;&#27491;&#30830;&#25429;&#25417;&#21040;&#24615;&#21035;&#20449;&#24687;&#65292;&#25110;&#32773;&#30456;&#21453;&#65292;&#26174;&#31034;&#20986;&#24615;&#21035;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous studies have demonstrated the ability of neural language models to learn various linguistic properties without direct supervision. This work takes an initial step towards exploring the less researched topic of how neural models discover linguistic properties of words, such as gender, as well as the rules governing their usage. We propose to use an artificial corpus generated by a PCFG based on French to precisely control the gender distribution in the training data and determine under which conditions a model correctly captures gender information or, on the contrary, appears gender-biased.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#38450;&#24481;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23433;&#20840;&#35757;&#32451;&#21644;&#20445;&#25252;&#25514;&#26045;&#30340;&#20248;&#21183;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.15851</link><description>&lt;p&gt;
&#33258;&#25105;&#38450;&#24481;&#65306;&#22686;&#24378;LLM&#30340;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15851
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#38450;&#24481;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23433;&#20840;&#35757;&#32451;&#21644;&#20445;&#25252;&#25514;&#26045;&#30340;&#20248;&#21183;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30423;&#30772;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#25514;&#26045;&#65292;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#31181;&#28389;&#29992;LLM&#30340;&#34892;&#20026;&#23548;&#33268;&#20102;&#36127;&#38754;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;&#30446;&#21069;&#65292;&#35299;&#20915;&#30423;&#30772;&#25915;&#20987;&#30340;&#20027;&#35201;&#26041;&#27861;&#26377;&#20004;&#31181;&#65306;&#23433;&#20840;&#35757;&#32451;&#21644;&#20445;&#25252;&#25514;&#26045;&#12290;&#23433;&#20840;&#35757;&#32451;&#20391;&#37325;&#20110;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#20197;&#22686;&#24378;&#20854;&#23433;&#20840;&#24615;&#12290;&#32780;&#20445;&#25252;&#25514;&#26045;&#21017;&#26159;&#36890;&#36807;&#23454;&#26045;&#22806;&#37096;&#27169;&#22411;&#25110;&#36807;&#28388;&#22120;&#26469;&#38450;&#27490;&#26377;&#23475;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#35757;&#32451;&#22312;&#36866;&#24212;&#26032;&#30340;&#25915;&#20987;&#31867;&#22411;&#26041;&#38754;&#20855;&#26377;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20445;&#25252;&#25514;&#26045;&#22312;&#24110;&#21161;&#26041;&#38754;&#20063;&#34987;&#35777;&#26126;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#38450;&#24481;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23433;&#20840;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#33258;&#25105;&#38450;&#24481;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#27169;&#22411;&#35780;&#20272;&#26377;&#23475;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#22312;&#33258;&#24049;&#30340;&#22238;&#24212;&#19978;&#22987;&#32456;&#25191;&#34892;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#29983;&#25104;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Weighted Graph Framework (DWGF)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26032;&#24847;&#22270;&#21457;&#29616;&#20013;&#26080;&#27861;&#24179;&#34913;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15836</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26032;&#24847;&#22270;&#21457;&#29616;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Diffusion Weighted Graph Framework for New Intent Discovery. (arXiv:2310.15836v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diffusion Weighted Graph Framework (DWGF)&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#26041;&#27861;&#22312;&#26032;&#24847;&#22270;&#21457;&#29616;&#20013;&#26080;&#27861;&#24179;&#34913;&#25968;&#37327;&#21644;&#36136;&#37327;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#26088;&#22312;&#36890;&#36807;&#26377;&#38480;&#30340;&#24102;&#26377;&#24050;&#30693;&#24847;&#22270;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#24110;&#21161;&#65292;&#35782;&#21035;&#20986;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#26032;&#24847;&#22270;&#21644;&#24050;&#30693;&#24847;&#22270;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#26410;&#32771;&#34385;&#26679;&#26412;&#20043;&#38388;&#30340;&#32467;&#26500;&#20851;&#31995;&#65292;&#29983;&#25104;&#30340;&#22122;&#22768;&#30417;&#30563;&#20449;&#21495;&#26080;&#27861;&#22312;&#25968;&#37327;&#21644;&#36136;&#37327;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#38459;&#30861;&#20102;&#26032;&#24847;&#22270;&#32858;&#31867;&#30340;&#24418;&#25104;&#21644;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#26377;&#25928;&#20256;&#36882;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#21152;&#26435;&#22270;&#26694;&#26550;&#65288;DWGF&#65289;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#32467;&#26500;&#20851;&#31995;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20805;&#20998;&#21644;&#21487;&#38752;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#27599;&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#27839;&#30528;&#30001;&#26368;&#36817;&#37051;&#25351;&#23548;&#30340;&#35821;&#20041;&#36335;&#24452;&#25193;&#25955;&#37051;&#22495;&#20851;&#31995;&#65292;&#20197;&#37492;&#21035;&#22320;&#21051;&#30011;&#20854;&#23616;&#37096;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#23616;&#37096;&#32467;&#26500;&#23545;&#20854;&#27491;&#26679;&#26412;&#36827;&#34892;&#25277;&#26679;&#21644;&#21152;&#26435;&#65292;&#29992;&#20110;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
New Intent Discovery (NID) aims to recognize both new and known intents from unlabeled data with the aid of limited labeled data containing only known intents. Without considering structure relationships between samples, previous methods generate noisy supervisory signals which cannot strike a balance between quantity and quality, hindering the formation of new intent clusters and effective transfer of the pre-training knowledge. To mitigate this limitation, we propose a novel Diffusion Weighted Graph Framework (DWGF) to capture both semantic similarities and structure relationships inherent in data, enabling more sufficient and reliable supervisory signals. Specifically, for each sample, we diffuse neighborhood relationships along semantic paths guided by the nearest neighbors for multiple hops to characterize its local structure discriminately. Then, we sample its positive keys and weigh them based on semantic similarities and local structures for contrastive learning. During inferen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#21709;&#24212;&#12290;&#20182;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20135;&#29983;&#20102;&#31867;&#20284;&#30340;&#36755;&#20986;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25552;&#31034;&#20250;&#35302;&#21457;&#19981;&#21516;&#30340;&#21709;&#24212;&#27169;&#24335;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25105;&#20204;&#21021;&#27493;&#25581;&#31034;&#20102;&#25552;&#31034;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2310.15829</link><description>&lt;p&gt;
&#38750;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26426;&#22120;&#29983;&#25104;&#30340;&#25552;&#31034;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unnatural language processing: How do language models handle machine-generated prompts?. (arXiv:2310.15829v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15829
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#29983;&#25104;&#30340;&#25552;&#31034;&#65292;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38750;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#21709;&#24212;&#12290;&#20182;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#20135;&#29983;&#20102;&#31867;&#20284;&#30340;&#36755;&#20986;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#21644;&#20154;&#24037;&#29983;&#25104;&#30340;&#25552;&#31034;&#20250;&#35302;&#21457;&#19981;&#21516;&#30340;&#21709;&#24212;&#27169;&#24335;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25105;&#20204;&#21021;&#27493;&#25581;&#31034;&#20102;&#25552;&#31034;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#31034;&#20248;&#21270;&#30740;&#31350;&#26174;&#31034;&#65292;&#35821;&#20041;&#19978;&#21644;&#35821;&#27861;&#19978;&#26500;&#36896;&#33391;&#22909;&#30340;&#25163;&#21160;&#21046;&#23450;&#30340;&#25552;&#31034;&#24120;&#24120;&#34987;&#26080;&#26126;&#26174;&#21547;&#20041;&#25110;&#21477;&#27861;&#32467;&#26500;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#20196;&#29260;&#24207;&#21015;&#25152;&#36229;&#36234;&#65292;&#21253;&#25324;&#26469;&#33258;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;&#30340;&#21521;&#37327;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#29983;&#25104;&#30340;&#25552;&#31034;&#26469;&#25506;&#32034;&#27169;&#22411;&#23545;&#38750;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22823;&#23567;&#27169;&#22411;&#22312;&#22810;&#20010;&#35821;&#20041;&#20219;&#21153;&#20013;&#23545;&#36830;&#32493;&#21644;&#31163;&#25955;&#30340;&#26426;&#22120;&#29983;&#25104;&#25552;&#31034;&#30340;&#34892;&#20026;&#65292;&#24182;&#23558;&#20854;&#19982;&#23545;&#20154;&#24037;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#34892;&#20026;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#21363;&#20351;&#20135;&#29983;&#20102;&#31867;&#20284;&#30340;&#36755;&#20986;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#21644;&#20154;&#24037;&#25552;&#31034;&#36890;&#36807;&#32593;&#32476;&#22788;&#29702;&#36335;&#24452;&#24341;&#21457;&#20102;&#19981;&#21516;&#30340;&#21709;&#24212;&#27169;&#24335;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#22256;&#24785;&#24230;&#12289;&#27880;&#24847;&#21147;&#21644;&#36755;&#20986;&#29109;&#20998;&#24067;&#65292;&#20197;&#21450;&#19981;&#21516;&#30340;&#21333;&#20803;&#28608;&#27963;&#37197;&#32622;&#25991;&#20214;&#12290;&#25105;&#20204;&#21021;&#27493;&#25581;&#31034;&#20102;&#25552;&#31034;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model prompt optimization research has shown that semantically and grammatically well-formed manually crafted prompts are routinely outperformed by automatically generated token sequences with no apparent meaning or syntactic structure, including sequences of vectors from a model's embedding space. We use machine-generated prompts to probe how models respond to input that is not composed of natural language expressions. We study the behavior of models of different sizes in multiple semantic tasks in response to both continuous and discrete machine-generated prompts, and compare it to the behavior in response to human-generated natural-language prompts. Even when producing a similar output, machine-generated and human prompts trigger different response patterns through the network processing pathways, including different perplexities, different attention and output entropy distributions, and different unit activation profiles. We provide preliminary insight into the nature of t
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2310.15823</link><description>&lt;p&gt;
Rosetta Stone&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;&#65306;&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#30340;&#36291;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rosetta Stone at KSAA-RD Shared Task: A Hop From Language Modeling To Word--Definition Alignment. (arXiv:2310.15823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;KSAA-RD&#20849;&#20139;&#20219;&#21153;&#20013;Rosetta Stone&#30340;&#24212;&#29992;&#65292;&#23558;&#35821;&#35328;&#24314;&#27169;&#24212;&#29992;&#21040;&#35789;--&#23450;&#20041;&#23545;&#40784;&#20013;&#12290;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#35789;&#20856;&#26159;&#19968;&#31181;&#24037;&#20855;&#65292;&#21487;&#26681;&#25454;&#25552;&#20379;&#30340;&#23450;&#20041;&#12289;&#21547;&#20041;&#25110;&#25551;&#36848;&#26469;&#21457;&#29616;&#19968;&#20010;&#35789;&#12290;&#36825;&#31181;&#25216;&#26415;&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#37117;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21487;&#20197;&#24110;&#21161;&#25484;&#25569;&#19968;&#20010;&#35789;&#30340;&#25551;&#36848;&#32780;&#19981;&#30693;&#20854;&#36523;&#20221;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#65292;&#24182;&#20351;&#23547;&#27714;&#31934;&#30830;&#26415;&#35821;&#30340;&#20889;&#20316;&#32773;&#21463;&#30410;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#28085;&#30422;&#34987;&#31216;&#20026;&#8220;&#33292;&#23574;&#19978;&#30340;&#35789;&#8221;&#29616;&#35937;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#25105;&#20204;&#22312;&#38463;&#25289;&#20271;&#35821;&#21453;&#21521;&#35789;&#20856;&#20849;&#20139;&#20219;&#21153;&#20013;&#33719;&#32988;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#30340;&#37325;&#28857;&#26159;&#20174;&#20276;&#38543;&#30340;&#25551;&#36848;&#20013;&#25512;&#23548;&#20986;&#38463;&#25289;&#20271;&#35789;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#65306;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#28041;&#21450;&#19968;&#20010;&#38463;&#25289;&#20271;&#23450;&#20041;&#20316;&#20026;&#36755;&#20837;&#65292;&#32780;&#31532;&#20108;&#20010;&#23376;&#20219;&#21153;&#21017;&#20351;&#29992;&#19968;&#20010;&#33521;&#25991;&#23450;&#20041;&#12290;&#23545;&#20110;&#31532;&#19968;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#32452;&#32463;&#36807;&#24494;&#35843;&#30340;&#38463;&#25289;&#20271;BERT&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#32473;&#23450;&#23450;&#20041;&#30340;&#35789;&#23884;&#20837;&#12290;&#26368;&#32456;&#34920;&#31034;&#26159;&#36890;&#36807;&#23545;&#27599;&#20010;&#27169;&#22411;&#36755;&#20986;&#30340;&#23884;&#20837;&#36827;&#34892;&#24179;&#22343;&#24471;&#21040;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Reverse Dictionary is a tool enabling users to discover a word based on its provided definition, meaning, or description. Such a technique proves valuable in various scenarios, aiding language learners who possess a description of a word without its identity, and benefiting writers seeking precise terminology. These scenarios often encapsulate what is referred to as the "Tip-of-the-Tongue" (TOT) phenomena. In this work, we present our winning solution for the Arabic Reverse Dictionary shared task. This task focuses on deriving a vector representation of an Arabic word from its accompanying description. The shared task encompasses two distinct subtasks: the first involves an Arabic definition as input, while the second employs an English definition. For the first subtask, our approach relies on an ensemble of finetuned Arabic BERT-based models, predicting the word embedding for a given definition. The final representation is obtained through averaging the output embeddings from each m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;51&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#30340;&#31038;&#20250;&#36523;&#20221;&#20559;&#35265;&#65292;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#27169;&#22411;&#22312;&#34917;&#20840;&#21477;&#23376;&#26102;&#37117;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#22242;&#20307;&#20869;&#31215;&#26497;&#21644;&#22242;&#20307;&#22806;&#28040;&#26497;&#30340;&#20559;&#35265;&#12290;&#19982;&#20154;&#31867;&#25991;&#26412;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#31867;&#20284;&#25110;&#26356;&#22823;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.15819</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#31038;&#20250;&#36523;&#20221;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models Exhibit Social Identity Biases. (arXiv:2310.15819v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;51&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#30340;&#31038;&#20250;&#36523;&#20221;&#20559;&#35265;&#65292;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#27169;&#22411;&#22312;&#34917;&#20840;&#21477;&#23376;&#26102;&#37117;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#22242;&#20307;&#20869;&#31215;&#26497;&#21644;&#22242;&#20307;&#22806;&#28040;&#26497;&#30340;&#20559;&#35265;&#12290;&#19982;&#20154;&#31867;&#25991;&#26412;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#31867;&#20284;&#25110;&#26356;&#22823;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20174;&#20154;&#31867;&#20013;&#23398;&#21040;&#30340;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;51&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23637;&#31034;&#20102;&#31038;&#20250;&#31185;&#23398;&#20013;&#24050;&#30693;&#30340;&#22242;&#20307;&#20869;&#22242;&#32467;&#21644;&#22242;&#20307;&#22806;&#25932;&#23545;&#30340;&#22522;&#26412;&#31038;&#20250;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20960;&#20046;&#25152;&#26377;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#21644;&#19968;&#20123;&#25351;&#20196;&#32454;&#35843;&#27169;&#22411;&#22312;&#34987;&#35201;&#27714;&#34917;&#20840;&#21477;&#23376;&#65288;&#20363;&#22914;&#65292;&#8220;&#25105;&#20204;&#26159;...&#8221;&#65289;&#26102;&#37117;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#22242;&#20307;&#20869;&#31215;&#26497;&#21644;&#22242;&#20307;&#22806;&#28040;&#26497;&#30340;&#20559;&#35265;&#12290;&#23558;LLM&#29983;&#25104;&#30340;&#21477;&#23376;&#19982;&#20114;&#32852;&#32593;&#19978;&#20154;&#31867;&#25776;&#20889;&#30340;&#21477;&#23376;&#36827;&#34892;&#27604;&#36739;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#19982;&#20154;&#31867;&#25991;&#26412;&#30456;&#20284;&#30340;&#29978;&#33267;&#26356;&#22823;&#31243;&#24230;&#30340;&#20559;&#35265;&#12290;&#20026;&#20102;&#26597;&#26126;&#36825;&#20123;&#20559;&#35265;&#30340;&#26681;&#28304;&#65292;&#25105;&#20204;&#22312;&#32654;&#22269;&#27665;&#20027;&#20826;&#21644;&#20849;&#21644;&#20826;&#20998;&#35010;&#30340;&#32972;&#26223;&#19979;&#23454;&#39564;&#24615;&#22320;&#21464;&#21270;&#20102;&#27169;&#22411;&#22312;&#32454;&#35843;&#36807;&#31243;&#20013;&#26292;&#38706;&#32473;&#22242;&#20307;&#20869;&#31215;&#26497;&#25110;&#22242;&#20307;&#22806;&#28040;&#26497;&#21477;&#23376;&#30340;&#25968;&#37327;&#12290;&#32467;&#26524;&#65292;&#27169;&#22411;&#23637;&#31034;&#20986;&#26126;&#26174;&#30340;&#20559;&#35265;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
The surge in popularity of large language models has given rise to concerns about biases that these models could learn from humans. In this study, we investigate whether ingroup solidarity and outgroup hostility, fundamental social biases known from social science, are present in 51 large language models. We find that almost all foundational language models and some instruction fine-tuned models exhibit clear ingroup-positive and outgroup-negative biases when prompted to complete sentences (e.g., "We are..."). A comparison of LLM-generated sentences with human-written sentences on the internet reveals that these models exhibit similar level, if not greater, levels of bias than human text. To investigate where these biases stem from, we experimentally varied the amount of ingroup-positive or outgroup-negative sentences the model was exposed to during fine-tuning in the context of the United States Democrat-Republican divide. Doing so resulted in the models exhibiting a marked increase i
&lt;/p&gt;</description></item><item><title>DALE&#26159;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#36873;&#25321;&#24615;&#25513;&#30721;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#21435;&#22122;&#30446;&#26631;&#39044;&#35757;&#32451;&#65292;&#22312;&#35299;&#20915;&#27861;&#24459;&#35821;&#35328;&#29305;&#24322;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;</title><link>http://arxiv.org/abs/2310.15799</link><description>&lt;p&gt;
DALE: &#29992;&#20110;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
DALE: Generative Data Augmentation for Low-Resource Legal NLP. (arXiv:2310.15799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15799
&lt;/p&gt;
&lt;p&gt;
DALE&#26159;&#19968;&#20010;&#29992;&#20110;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#36873;&#25321;&#24615;&#25513;&#30721;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#21435;&#22122;&#30446;&#26631;&#39044;&#35757;&#32451;&#65292;&#22312;&#35299;&#20915;&#27861;&#24459;&#35821;&#35328;&#29305;&#24322;&#24615;&#30340;&#21516;&#26102;&#29983;&#25104;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;DALE&#65292;&#19968;&#20010;&#38024;&#23545;&#20302;&#36164;&#28304;&#27861;&#24459;NLP&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#12290;DALE&#35299;&#20915;&#20102;&#29616;&#26377;&#26694;&#26550;&#22312;&#29983;&#25104;&#27861;&#24459;&#25991;&#20214;&#30340;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#26041;&#38754;&#23384;&#22312;&#30340;&#25361;&#25112; - &#27861;&#24459;&#35821;&#35328;&#20855;&#26377;&#19987;&#38376;&#30340;&#35789;&#27719;&#21644;&#22797;&#26434;&#30340;&#35821;&#20041;&#12289;&#24418;&#24577;&#21644;&#21477;&#27861;&#65292;&#24182;&#19981;&#33021;&#20174;&#20165;&#20165;&#23545;&#28304;&#21477;&#23376;&#36827;&#34892;&#25913;&#36848;&#30340;&#25968;&#25454;&#22686;&#24378;&#20013;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;DALE&#26159;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#65292;&#23427;&#22312;&#19968;&#31181;&#26032;&#39062;&#30340;&#26080;&#30417;&#30563;&#25991;&#26412;&#21435;&#22122;&#30446;&#26631;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#30446;&#26631;&#22522;&#20110;&#36873;&#25321;&#24615;&#25513;&#30721; - &#25105;&#20204;&#30340;&#25513;&#30721;&#31574;&#30053;&#21033;&#29992;&#27169;&#26495;&#21270;&#27861;&#24459;&#25991;&#20214;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#29305;&#24449;&#26469;&#25513;&#30422;&#25991;&#26412;&#30340;&#36830;&#32493;&#33539;&#22260;&#12290;&#21435;&#22122;&#36825;&#20123;&#33539;&#22260;&#26377;&#21161;&#20110;DALE&#33719;&#21462;&#20851;&#20110;&#27861;&#24459;&#27010;&#24565;&#12289;&#21407;&#21017;&#21644;&#35821;&#35328;&#20351;&#29992;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#23427;&#20855;&#22791;&#20102;&#29983;&#25104;&#36830;&#36143;&#19988;&#22810;&#26679;&#21270;&#30340;&#22686;&#24378;&#20197;&#21450;&#26032;&#39062;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;&#26368;&#21518;&#65292;DALE&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present DALE, a novel and effective generative Data Augmentation framework for low-resource LEgal NLP. DALE addresses the challenges existing frameworks pose in generating effective data augmentations of legal documents - legal language, with its specialized vocabulary and complex semantics, morphology, and syntax, does not benefit from data augmentations that merely rephrase the source sentence. To address this, DALE, built on an Encoder-Decoder Language Model, is pre-trained on a novel unsupervised text denoising objective based on selective masking - our masking strategy exploits the domain-specific language characteristics of templatized legal documents to mask collocated spans of text. Denoising these spans helps DALE acquire knowledge about legal concepts, principles, and language usage. Consequently, it develops the ability to generate coherent and diverse augmentations with novel contexts. Finally, DALE performs conditional generation to generate synthetic augmentations for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#22240;&#20026;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#23454;&#20307;&#30721;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#65292;&#20351;&#24471;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;</title><link>http://arxiv.org/abs/2310.15797</link><description>&lt;p&gt;
&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Random Entity Quantization for Parameter-Efficient Compositional Knowledge Graph Representation. (arXiv:2310.15797v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#32452;&#21512;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#22240;&#20026;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#23454;&#20307;&#30721;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#65292;&#20351;&#24471;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#34920;&#31034;&#30693;&#35782;&#22270;&#35889;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#30340;&#34920;&#31034;&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#20027;&#23548;&#26041;&#27861;KG&#23884;&#20837;&#65288;KGE&#65289;&#36890;&#36807;&#29420;&#31435;&#21521;&#37327;&#34920;&#31034;&#23454;&#20307;&#65292;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#25928;&#29575;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#39044;&#23450;&#20041;&#30340;&#23567;&#35268;&#27169;&#30721;&#20070;&#20013;&#21305;&#37197;&#23454;&#20307;&#23545;&#24212;&#30340;&#30721;&#23383;&#26469;&#34920;&#31034;&#23454;&#20307;&#12290;&#25105;&#20204;&#23558;&#33719;&#21462;&#27599;&#20010;&#23454;&#20307;&#23545;&#24212;&#30721;&#23383;&#30340;&#36807;&#31243;&#31216;&#20026;&#23454;&#20307;&#37327;&#21270;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#35774;&#35745;&#20102;&#22797;&#26434;&#30340;&#31574;&#30053;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26412;&#25991;&#34920;&#26126;&#31616;&#21333;&#30340;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#21487;&#20197;&#23454;&#29616;&#19982;&#24403;&#21069;&#31574;&#30053;&#31867;&#20284;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#29616;&#35937;&#24182;&#25581;&#31034;&#20102;&#22312;&#38543;&#26426;&#23454;&#20307;&#37327;&#21270;&#19979;&#65292;&#34920;&#31034;&#23454;&#20307;&#30340;&#37327;&#21270;&#32467;&#26524;-&#23454;&#20307;&#30721;&#20855;&#26377;&#26356;&#39640;&#30340;&#29109;&#21644;&#30721;&#23383;&#32423;&#21035;&#30340;Jaccard&#36317;&#31163;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#23454;&#20307;&#26356;&#23481;&#26131;&#21306;&#20998;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#30340;KG&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation Learning on Knowledge Graphs (KGs) is essential for downstream tasks. The dominant approach, KG Embedding (KGE), represents entities with independent vectors and faces the scalability challenge. Recent studies propose an alternative way for parameter efficiency, which represents entities by composing entity-corresponding codewords matched from predefined small-scale codebooks. We refer to the process of obtaining corresponding codewords of each entity as entity quantization, for which previous works have designed complicated strategies. Surprisingly, this paper shows that simple random entity quantization can achieve similar results to current strategies. We analyze this phenomenon and reveal that entity codes, the quantization outcomes for expressing entities, have higher entropy at the code level and Jaccard distance at the codeword level under random entity quantization. Therefore, different entities become more easily distinguished, facilitating effective KG represen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#65292;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15793</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalization in large language models by learning prefix subspaces. (arXiv:2310.15793v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#21069;&#32512;&#23376;&#31354;&#38388;&#26469;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#27169;&#22411;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#65292;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#20860;&#23481;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31232;&#32570;&#25968;&#25454;&#29615;&#22659;&#20013;&#30340;&#24494;&#35843;&#65288;&#20063;&#34987;&#31216;&#20026;&#8220;&#23569;&#26679;&#26412;&#8221;&#23398;&#20064;&#35774;&#32622;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#23376;&#31354;&#38388;&#30340;&#26041;&#27861;&#26469;&#22686;&#21152;LLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#31181;&#20248;&#21270;&#26041;&#27861;&#26368;&#36817;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#24341;&#20837;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25972;&#20010;&#21333;&#32431;&#24418;&#27169;&#22411;&#30340;&#32852;&#21512;&#20248;&#21270;&#65292;&#35782;&#21035;&#26356;&#24191;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#20854;&#36866;&#24212;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#27169;&#22411;&#21017;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#22823;&#37327;&#30340;&#21442;&#25968;&#20351;&#24471;&#32852;&#21512;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#65292;&#20854;&#27425;&#65292;&#23427;&#20204;&#30340;&#30830;&#23450;&#24615;&#21442;&#25968;&#21021;&#22987;&#21270;&#26041;&#26696;&#20351;&#20854;&#19981;&#36866;&#29992;&#20110;&#26368;&#21021;&#30340;&#23376;&#31354;&#38388;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#23637;&#31034;&#65292;&#8220;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#8221;&#65288;PEFT&#65289;&#26041;&#27861;&#19982;&#26368;&#21021;&#30340;&#26041;&#27861;&#23436;&#20840;&#20860;&#23481;&#65292;&#24182;&#25552;&#20986;&#23398;&#20064;&#25972;&#20010;&#36830;&#32493;&#21069;&#32512;&#30340;&#21333;&#32431;&#24418;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20010;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article focuses on large language models (LLMs) fine-tuning in the scarce data regime (also known as the "few-shot" learning setting). We propose a method to increase the generalization capabilities of LLMs based on neural network subspaces. This optimization method, recently introduced in computer vision, aims to improve model generalization by identifying wider local optima through the joint optimization of an entire simplex of models in parameter space. Its adaptation to massive, pretrained transformers, however, poses some challenges. First, their considerable number of parameters makes it difficult to train several models jointly, and second, their deterministic parameter initialization schemes make them unfit for the subspace method as originally proposed. We show in this paper that "Parameter Efficient Fine-Tuning" (PEFT) methods, however, are perfectly compatible with this original approach, and propose to learn entire simplex of continuous prefixes. We test our method on 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MindLLM&#65292;&#36890;&#36807;&#25552;&#20379;1.3&#20159;&#21644;3&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#36164;&#28304;&#31232;&#32570;&#24615;&#30340;&#21387;&#21147;&#12290;MindLLM&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#32473;&#20986;&#20102;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#65292;&#23545;&#23398;&#26415;&#30028;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2310.15777</link><description>&lt;p&gt;
MindLLM: &#20174;&#38646;&#24320;&#22987;&#39044;&#35757;&#32451;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35780;&#20272;&#21644;&#39046;&#22495;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MindLLM: Pre-training Lightweight Large Language Model from Scratch, Evaluations and Domain Applications. (arXiv:2310.15777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;MindLLM&#65292;&#36890;&#36807;&#25552;&#20379;1.3&#20159;&#21644;3&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#65292;&#20943;&#36731;&#20102;&#35757;&#32451;&#21644;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25104;&#26412;&#21644;&#36164;&#28304;&#31232;&#32570;&#24615;&#30340;&#21387;&#21147;&#12290;MindLLM&#22312;&#21508;&#20010;&#27493;&#39588;&#20013;&#32473;&#20986;&#20102;&#32463;&#39564;&#25945;&#35757;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#65292;&#23545;&#23398;&#26415;&#30028;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#26631;&#24535;&#30528;&#36890;&#24448;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#12290;&#34429;&#28982;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#26159;&#36890;&#36807;&#24320;&#21457;&#36234;&#26469;&#36234;&#22823;&#35268;&#27169;&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#30340;&#65292;&#20294;&#36824;&#26377;&#21478;&#19968;&#31181;&#20998;&#25903;&#65292;&#21363;&#24320;&#21457;&#36731;&#37327;&#32423;&#23450;&#21046;&#27169;&#22411;&#65292;&#20197;&#26356;&#22909;&#22320;&#26381;&#21153;&#26576;&#20123;&#39046;&#22495;&#65292;&#32771;&#34385;&#21040;&#35757;&#32451;&#21644;&#37096;&#32626;LLM&#30340;&#39640;&#25104;&#26412;&#21644;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MindLLM&#65292;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#21452;&#35821;&#36731;&#37327;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#38646;&#24320;&#22987;&#35757;&#32451;&#65292;&#36890;&#36807;&#25552;&#20379;13&#20159;&#21644;30&#20159;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#20943;&#36731;&#36825;&#20123;&#36127;&#25285;&#12290;&#32473;&#20986;&#20102;&#22312;&#22823;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#20013;&#31215;&#32047;&#30340;&#32463;&#39564;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#21253;&#25324;&#25968;&#25454;&#26500;&#24314;&#12289;&#27169;&#22411;&#26550;&#26500;&#12289;&#35780;&#20272;&#21644;&#24212;&#29992;&#12290;&#36825;&#20123;&#35265;&#35299;&#23545;&#23398;&#32773;&#21644;&#24320;&#21457;&#32773;&#26469;&#35828;&#26377;&#20215;&#20540;&#12290;MindLLM&#22987;&#32456;&#33021;&#22815;&#19982;&#29978;&#33267;&#36229;&#36807;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance across various natural language tasks, marking significant strides towards general artificial intelligence. While general artificial intelligence is leveraged by developing increasingly large-scale models, there could be another branch to develop lightweight custom models that better serve certain domains, taking into account the high cost of training and deploying LLMs and the scarcity of resources. In this paper, we present MindLLM, a novel series of bilingual lightweight large language models, trained from scratch, alleviating such burdens by offering models with 1.3 billion and 3 billion parameters. A thorough account of experiences accrued during large model development is given, covering every step of the process, including data construction, model architecture, evaluation, and applications. Such insights are hopefully valuable for fellow academics and developers. MindLLM consistently matches or surpasses the p
&lt;/p&gt;</description></item><item><title>BLESS&#26159;&#19968;&#20010;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21477;&#23376;&#31616;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#39033;&#30446;&#65292;&#35780;&#27979;&#20102;44&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#19977;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26410;&#32463;&#36807;&#21477;&#23376;&#31616;&#21270;&#30340;&#35757;&#32451;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#26368;&#26032;&#30340;&#31616;&#21270;&#20219;&#21153;&#22522;&#32447;&#30456;&#24403;&#65292;&#24182;&#19988;&#26576;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26356;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.15773</link><description>&lt;p&gt;
BLESS:&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21477;&#23376;&#31616;&#21270;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
BLESS: Benchmarking Large Language Models on Sentence Simplification. (arXiv:2310.15773v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15773
&lt;/p&gt;
&lt;p&gt;
BLESS&#26159;&#19968;&#20010;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21477;&#23376;&#31616;&#21270;&#20219;&#21153;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#39033;&#30446;&#65292;&#35780;&#27979;&#20102;44&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#19977;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#21363;&#20351;&#26410;&#32463;&#36807;&#21477;&#23376;&#31616;&#21270;&#30340;&#35757;&#32451;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#19982;&#26368;&#26032;&#30340;&#31616;&#21270;&#20219;&#21153;&#22522;&#32447;&#30456;&#24403;&#65292;&#24182;&#19988;&#26576;&#20123;&#27169;&#22411;&#23637;&#31034;&#20102;&#26356;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BLESS&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#65292;&#38024;&#23545;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25991;&#26412;&#31616;&#21270;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20849;&#35745;44&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#65288;&#32500;&#22522;&#30334;&#31185;&#12289;&#26032;&#38395;&#21644;&#21307;&#23398;&#65289;&#30340;&#19977;&#20010;&#27979;&#35797;&#38598;&#19978;&#65292;&#28041;&#21450;&#27169;&#22411;&#30340;&#22823;&#23567;&#12289;&#26550;&#26500;&#12289;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#21487;&#35775;&#38382;&#24615;&#31561;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#32771;&#34385;&#20102;&#19968;&#31995;&#21015;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#65292;&#21516;&#26102;&#36824;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23450;&#37327;&#30740;&#31350;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#27169;&#22411;&#25191;&#34892;&#30340;&#24120;&#35265;&#32534;&#36753;&#25805;&#20316;&#30340;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#37096;&#20998;&#27169;&#22411;&#36755;&#20986;&#36827;&#34892;&#20102;&#25163;&#21160;&#36136;&#37327;&#20998;&#26512;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#29983;&#25104;&#30340;&#31616;&#21270;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#26410;&#32463;&#36807;&#21477;&#23376;&#31616;&#21270;&#30340;&#35757;&#32451;&#65292;&#26368;&#22909;&#30340;LLMs&#22312;&#24615;&#33021;&#19978;&#19982;&#26368;&#26032;&#30340;&#31616;&#21270;&#20219;&#21153;&#22522;&#32447;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26576;&#20123;LLMs&#23637;&#31034;&#20986;&#26356;&#24191;&#27867;&#21644;&#22810;&#26679;&#21270;&#30340;&#32534;&#36753;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BLESS, a comprehensive performance benchmark of the most recent state-of-the-art large language models (LLMs) on the task of text simplification (TS). We examine how well off-the-shelf LLMs can solve this challenging task, assessing a total of 44 models, differing in size, architecture, pre-training methods, and accessibility, on three test sets from different domains (Wikipedia, news, and medical) under a few-shot setting. Our analysis considers a suite of automatic metrics as well as a large-scale quantitative investigation into the types of common edit operations performed by the different models. Furthermore, we perform a manual qualitative analysis on a subset of model outputs to better gauge the quality of the generated simplifications. Our evaluation indicates that the best LLMs, despite not being trained on TS, perform comparably with state-of-the-art TS baselines. Additionally, we find that certain LLMs demonstrate a greater range and diversity of edit operations. O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24120;&#29992;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#33258;&#30001;&#25991;&#26412;&#20154;&#31867;&#21453;&#39304;&#30340;&#31867;&#22411;&#21644;&#39057;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#27880;&#37322;&#36825;&#20123;&#21453;&#39304;&#30340;&#26032;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#22238;&#31572;&#29983;&#25104;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;GPT-2&#12289;LLAMA&#21644;Fla&#31561;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.15758</link><description>&lt;p&gt;
&#20174;&#33258;&#30001;&#25991;&#26412;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064; - &#25910;&#38598;&#26032;&#25968;&#25454;&#38598;&#36824;&#26159;&#25193;&#23637;&#29616;&#26377;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Learning From Free-Text Human Feedback -- Collect New Datasets Or Extend Existing Ones?. (arXiv:2310.15758v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24120;&#29992;&#23545;&#35805;&#25968;&#25454;&#38598;&#20013;&#33258;&#30001;&#25991;&#26412;&#20154;&#31867;&#21453;&#39304;&#30340;&#31867;&#22411;&#21644;&#39057;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#27880;&#37322;&#36825;&#20123;&#21453;&#39304;&#30340;&#26032;&#20998;&#31867;&#27861;&#12290;&#36890;&#36807;&#23558;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#22238;&#31572;&#29983;&#25104;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;GPT-2&#12289;LLAMA&#21644;Fla&#31561;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#33258;&#30001;&#25991;&#26412;&#30340;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#23545;&#20110;&#23545;&#35805;&#31995;&#32479;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#26159;&#26631;&#27880;&#30340;&#25968;&#25454;&#31232;&#32570;&#65292;&#24182;&#19988;&#36890;&#24120;&#21482;&#28085;&#30422;&#20102;&#23545;&#35805;&#20154;&#24037;&#26234;&#33021;&#20013;&#24050;&#30693;&#38169;&#35823;&#31867;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#19982;&#20854;&#20174;&#22836;&#24320;&#22987;&#25910;&#38598;&#21644;&#27880;&#37322;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#26368;&#36817;&#22312;&#21512;&#25104;&#23545;&#35805;&#29983;&#25104;&#26041;&#38754;&#30340;&#20808;&#36827;&#25216;&#26415;&#21487;&#20197;&#29992;&#20110;&#25193;&#20805;&#29616;&#26377;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#20197;&#33719;&#24471;&#24517;&#35201;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#35780;&#20272;&#36825;&#31181;&#21162;&#21147;&#30340;&#21487;&#34892;&#24615;&#65292;&#20102;&#35299;&#36825;&#20123;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#30340;&#21508;&#31181;&#31867;&#22411;&#21644;&#39057;&#29575;&#30340;&#33258;&#30001;&#25991;&#26412;&#20154;&#31867;&#21453;&#39304;&#26159;&#24456;&#37325;&#35201;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#24120;&#29992;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#21253;&#25324; MultiWoZ&#12289;SGD&#12289;BABI&#12289;PersonaChat&#12289;Wizards-of-Wikipedia &#21644; Self-Feeding Chatbot &#30340;&#20154;&#31867;&#19982;&#26426;&#22120;&#23545;&#35805;&#20998;&#24320;&#30340;&#37096;&#20998;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#20026;&#23545;&#35805;&#20013;&#30340;&#33258;&#30001;&#25991;&#26412;&#20154;&#31867;&#21453;&#39304;&#30340;&#27880;&#37322;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#23558;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#22312;&#19977;&#31181;&#20808;&#36827;&#30340;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;&#21253;&#25324;GPT-2&#12289;LLAMA&#21644;Fla&#65289;&#30340;&#22238;&#31572;&#29983;&#25104;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from free-text human feedback is essential for dialog systems, but annotated data is scarce and usually covers only a small fraction of error types known in conversational AI. Instead of collecting and annotating new datasets from scratch, recent advances in synthetic dialog generation could be used to augment existing dialog datasets with the necessary annotations. However, to assess the feasibility of such an effort, it is important to know the types and frequency of free-text human feedback included in these datasets. In this work, we investigate this question for a variety of commonly used dialog datasets, including MultiWoZ, SGD, BABI, PersonaChat, Wizards-of-Wikipedia, and the human-bot split of the Self-Feeding Chatbot. Using our observations, we derive new taxonomies for the annotation of free-text human feedback in dialogs and investigate the impact of including such data in response generation for three SOTA language generation models, including GPT-2, LLAMA, and Fla
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20998;&#27495;&#65292;&#21457;&#29616;&#20010;&#20154;&#20215;&#20540;&#35266;&#24046;&#24322;&#19982;&#20998;&#27495;&#26377;&#20851;&#65292;&#25506;&#35752;&#20102;&#27880;&#20837;&#20215;&#20540;&#35266;&#20449;&#24687;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.15757</link><description>&lt;p&gt;
&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20215;&#20540;&#24046;&#24322;&#26159;&#21542;&#24433;&#21709;&#20998;&#27495;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Differences in Values Influence Disagreements in Online Discussions?. (arXiv:2310.15757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20998;&#27495;&#65292;&#21457;&#29616;&#20010;&#20154;&#20215;&#20540;&#35266;&#24046;&#24322;&#19982;&#20998;&#27495;&#26377;&#20851;&#65292;&#25506;&#35752;&#20102;&#27880;&#20837;&#20215;&#20540;&#35266;&#20449;&#24687;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20998;&#27495;&#26159;&#24456;&#24120;&#35265;&#30340;&#65292;&#28982;&#32780;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#23545;&#24433;&#21709;&#20998;&#27495;&#30340;&#22240;&#32032;&#32570;&#20047;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20010;&#20154;&#20215;&#20540;&#35266;&#24046;&#24322;&#26159;&#21542;&#19982;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20998;&#27495;&#26377;&#20851;&#12290;&#25105;&#20204;&#20351;&#29992;&#20808;&#36827;&#30340;&#27169;&#22411;&#20272;&#35745;&#20102;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#20215;&#20540;&#35266;&#65292;&#24182;&#23558;&#20272;&#35745;&#20540;&#32858;&#21512;&#20026;&#20215;&#20540;&#35266;&#37197;&#32622;&#25991;&#20214;&#12290;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#30340;&#19968;&#33268;&#24615;&#26631;&#31614;&#23545;&#20272;&#35745;&#30340;&#20215;&#20540;&#35266;&#37197;&#32622;&#25991;&#20214;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#20215;&#20540;&#35266;&#37197;&#32622;&#25991;&#20214;&#30340;&#19981;&#30456;&#20284;&#24615;&#19982;&#20998;&#27495;&#26377;&#20851;&#12290;&#21516;&#26102;&#21457;&#29616;&#22312;&#19968;&#33268;&#24615;&#39044;&#27979;&#20013;&#21152;&#20837;&#20215;&#20540;&#20449;&#24687;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disagreements are common in online discussions. Disagreement may foster collaboration and improve the quality of a discussion under some conditions. Although there exist methods for recognizing disagreement, a deeper understanding of factors that influence disagreement is lacking in the literature. We investigate a hypothesis that differences in personal values are indicative of disagreement in online discussions. We show how state-of-the-art models can be used for estimating values in online discussions and how the estimated values can be aggregated into value profiles. We evaluate the estimated value profiles based on human-annotated agreement labels. We find that the dissimilarity of value profiles correlates with disagreement in specific cases. We also find that including value information in agreement prediction improves performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#26029;&#26102;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#32763;&#35793;&#20013;&#25511;&#21046;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#24615;&#21035;&#21464;&#21270;&#12290;&#36890;&#36807;&#37096;&#20998;&#26367;&#25442;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#29305;&#23450;&#24615;&#21035;&#30340;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#32763;&#35793;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#26041;&#27861;&#22312;&#24615;&#21035;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20339;&#35757;&#32451;&#26102;&#38388;&#32531;&#35299;&#31574;&#30053;&#65292;&#23588;&#20854;&#22312;&#20855;&#26377;&#24615;&#21035;&#20914;&#31361;&#30340;&#26465;&#20214;&#19979;&#25928;&#26524;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2310.15752</link><description>&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#20013;&#65306;&#25511;&#21046;&#24615;&#21035;&#21464;&#21270;&#30340;&#25512;&#26029;&#26102;&#38388;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Integrating Language Models into Direct Speech Translation: An Inference-Time Solution to Control Gender Inflection. (arXiv:2310.15752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15752
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25512;&#26029;&#26102;&#38388;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35821;&#38899;&#32763;&#35793;&#20013;&#25511;&#21046;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#24615;&#21035;&#21464;&#21270;&#12290;&#36890;&#36807;&#37096;&#20998;&#26367;&#25442;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#29305;&#23450;&#24615;&#21035;&#30340;&#22806;&#37096;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#32763;&#35793;&#36807;&#31243;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#19968;&#26041;&#27861;&#22312;&#24615;&#21035;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20339;&#35757;&#32451;&#26102;&#38388;&#32531;&#35299;&#31574;&#30053;&#65292;&#23588;&#20854;&#22312;&#20855;&#26377;&#24615;&#21035;&#20914;&#31361;&#30340;&#26465;&#20214;&#19979;&#25928;&#26524;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32763;&#35793;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#35789;&#35821;&#26102;&#65292;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#24212;&#35813;&#36991;&#20813;&#20351;&#29992;&#40664;&#35748;&#30340;&#30007;&#24615;&#27867;&#29992;&#35789;&#65292;&#20063;&#19981;&#24212;&#20381;&#36182;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#22768;&#38899;&#29305;&#24449;&#12290;&#30456;&#21453;&#65292;&#23427;&#20204;&#24212;&#35813;&#26681;&#25454;&#35828;&#35805;&#32773;&#30340;&#20559;&#22909;&#26469;&#30830;&#23450;&#24615;&#21035;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#28041;&#21450;&#23545;&#24102;&#26377;&#24615;&#21035;&#26631;&#31614;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#36827;&#34892;&#19987;&#38376;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#25512;&#26029;&#25511;&#21046;&#19982;&#35828;&#35805;&#32773;&#30456;&#20851;&#30340;&#24615;&#21035;&#21464;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37096;&#20998;&#26367;&#25442;&#20102;&#35821;&#38899;&#32763;&#35793;&#35299;&#30721;&#22120;&#38544;&#21547;&#23398;&#20064;&#30340;&#65288;&#26377;&#20559;&#35265;&#30340;&#65289;&#20869;&#37096;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#65292;&#20351;&#29992;&#20102;&#29305;&#23450;&#24615;&#21035;&#30340;&#22806;&#37096;LM&#12290;&#36890;&#36807;&#23545;en-&gt;es/fr/it&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#24615;&#21035;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#21644;&#26368;&#20339;&#35757;&#32451;&#26102;&#38388;&#32531;&#35299;&#31574;&#30053;&#65292;&#23545;&#22899;&#24615;&#24418;&#24335;&#30340;&#25552;&#39640;&#20998;&#21035;&#36798;&#21040;&#20102;31.0&#21644;1.6&#20010;&#30334;&#20998;&#28857;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26465;&#20214;&#20013;&#65292;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#29305;&#24449;&#24341;&#21457;&#20102;&#24615;&#21035;&#20914;&#31361;&#65292;&#36825;&#20123;&#22686;&#30410;&#29978;&#33267;&#26356;&#22823;&#65288;&#20998;&#21035;&#20026;32.0&#21644;3.4&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
When translating words referring to the speaker, speech translation (ST) systems should not resort to default masculine generics nor rely on potentially misleading vocal traits. Rather, they should assign gender according to the speakers' preference. The existing solutions to do so, though effective, are hardly feasible in practice as they involve dedicated model re-training on gender-labeled ST data. To overcome these limitations, we propose the first inference-time solution to control speaker-related gender inflections in ST. Our approach partially replaces the (biased) internal language model (LM) implicitly learned by the ST decoder with gender-specific external LMs. Experiments on en-&gt;es/fr/it show that our solution outperforms the base models and the best training-time mitigation strategy by up to 31.0 and 1.6 points in gender accuracy, respectively, for feminine forms. The gains are even larger (up to 32.0 and 3.4) in the challenging condition where speakers' vocal traits confli
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#20248;&#35268;&#21017;&#32047;&#31215;&#65288;TRAN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#20197;&#21069;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TRAN&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#22522;&#20934;&#32447;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.15746</link><description>&lt;p&gt;
&#22833;&#36133;&#25351;&#24341;&#20043;&#36335;&#65306;&#36890;&#36807;&#26080;&#35843;&#20248;&#35268;&#21017;&#32047;&#31215;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Failures Pave the Way: Enhancing Large Language Models through Tuning-free Rule Accumulation. (arXiv:2310.15746v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#35843;&#20248;&#35268;&#21017;&#32047;&#31215;&#65288;TRAN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#20197;&#21069;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#21892;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TRAN&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#22522;&#20934;&#32447;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#27861;&#25429;&#25417;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36825;&#20123;&#38745;&#24577;&#30340;LLM&#19981;&#21487;&#36991;&#20813;&#22320;&#19981;&#26029;&#37325;&#22797;&#30456;&#20284;&#30340;&#38169;&#35823;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26080;&#35843;&#20248;&#35268;&#21017;&#32047;&#31215;&#65288;TRAN&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#20197;&#21069;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#26469;&#25351;&#23548;LLM&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;&#32771;&#34385;&#21040;&#25968;&#25454;&#26159;&#39034;&#24207;&#21040;&#36798;&#30340;&#65292;LLM&#36880;&#28176;&#31215;&#32047;&#20102;&#20174;&#38169;&#35823;&#26696;&#20363;&#20013;&#24471;&#21040;&#30340;&#35268;&#21017;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#35268;&#21017;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;LLM&#22312;&#22788;&#29702;&#21518;&#32493;&#36755;&#20837;&#26102;&#21033;&#29992;&#36825;&#20123;&#35268;&#21017;&#36991;&#20813;&#29359;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#35268;&#21017;&#19982;&#20027;&#35201;&#25552;&#31034;&#26080;&#20851;&#65292;&#26080;&#32541;&#34917;&#20805;&#20102;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;TRAN&#30456;&#36739;&#20110;&#26368;&#36817;&#30340;&#22522;&#20934;&#32447;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased impressive performance. However, due to their inability to capture relationships among samples, these frozen LLMs inevitably keep repeating similar mistakes. In this work, we propose our Tuning-free Rule Accumulation (TRAN) framework, which guides LLMs in improving their performance by learning from previous mistakes. Considering data arrives sequentially, LLMs gradually accumulate rules from incorrect cases, forming a rule collection. These rules are then utilized by the LLMs to avoid making similar mistakes when processing subsequent inputs. Moreover, the rules remain independent of the primary prompts, seamlessly complementing prompt design strategies. Experimentally, we show that TRAN improves over recent baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#30340;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#20851;&#20110;&#31867;&#21035;&#21407;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15743</link><description>&lt;p&gt;
RAPL: &#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#30340;&#23569;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RAPL: A Relation-Aware Prototype Learning Approach for Few-Shot Document-Level Relation Extraction. (arXiv:2310.15743v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15743
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#30340;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#20013;&#20851;&#20110;&#31867;&#21035;&#21407;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21482;&#26377;&#23569;&#37327;&#26377;&#26631;&#31614;&#30340;&#25991;&#26723;&#21487;&#29992;&#26102;&#65292;&#22914;&#20309;&#30830;&#23450;&#25991;&#26723;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65311;&#23569;&#26679;&#26412;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(FSDLRE)&#23545;&#20110;&#35299;&#20915;&#29616;&#23454;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#24230;&#37327;&#30340;&#20803;&#23398;&#20064;&#26159;&#24191;&#27867;&#37319;&#29992;&#30340;FSDLRE&#26694;&#26550;&#65292;&#23427;&#26500;&#24314;&#20102;&#29992;&#20110;&#20998;&#31867;&#30340;&#31867;&#21035;&#21407;&#22411;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#38590;&#20197;&#33719;&#21462;&#20855;&#26377;&#20934;&#30830;&#20851;&#31995;&#35821;&#20041;&#30340;&#31867;&#21035;&#21407;&#22411;&#65306;1)&#20026;&#20102;&#26500;&#24314;&#30446;&#26631;&#20851;&#31995;&#31867;&#22411;&#30340;&#21407;&#22411;&#65292;&#23427;&#20204;&#32858;&#21512;&#20102;&#25152;&#26377;&#20855;&#26377;&#35813;&#20851;&#31995;&#30340;&#23454;&#20307;&#23545;&#30340;&#34920;&#31034;&#65292;&#32780;&#36825;&#20123;&#23454;&#20307;&#23545;&#21487;&#33021;&#36824;&#20855;&#26377;&#20854;&#20182;&#20851;&#31995;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#21407;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;2)&#23427;&#20204;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#20351;&#29992;&#19968;&#32452;&#36890;&#29992;&#30340;NOTA(none-of-the-above)&#21407;&#22411;&#65292;&#24573;&#35270;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#20851;&#31995;&#31867;&#22411;&#30340;&#20219;&#21153;&#20013;NOTA&#35821;&#20041;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#24863;&#30693;&#30340;&#21407;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#24378;&#21270;FSDLRE&#30340;&#20851;&#31995;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How to identify semantic relations among entities in a document when only a few labeled documents are available? Few-shot document-level relation extraction (FSDLRE) is crucial for addressing the pervasive data scarcity problem in real-world scenarios. Metric-based meta-learning is an effective framework widely adopted for FSDLRE, which constructs class prototypes for classification. However, existing works often struggle to obtain class prototypes with accurate relational semantics: 1) To build prototype for a target relation type, they aggregate the representations of all entity pairs holding that relation, while these entity pairs may also hold other relations, thus disturbing the prototype. 2) They use a set of generic NOTA (none-of-the-above) prototypes across all tasks, neglecting that the NOTA semantics differs in tasks with different target relation types. In this paper, we propose a relation-aware prototype learning method for FSDLRE to strengthen the relational semantics of p
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Variator&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#30340;&#21387;&#32553;&#25554;&#20214;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#24037;&#20316;&#36127;&#36733;&#21160;&#24577;&#36873;&#25321;&#19981;&#21516;&#21152;&#36895;&#27604;&#30340;&#25554;&#20214;&#12290;&#25554;&#20214;&#37319;&#29992;&#20102;&#21387;&#32553;&#38544;&#34255;&#21521;&#37327;&#30340;&#26041;&#27861;&#26469;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19988;&#30001;&#20110;&#21442;&#25968;&#23569;&#65292;&#21487;&#20197;&#33410;&#30465;&#23384;&#20648;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2310.15724</link><description>&lt;p&gt;
Variator: &#20351;&#29992;&#21363;&#25554;&#21363;&#29992;&#21387;&#32553;&#27169;&#22359;&#21152;&#36895;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variator: Accelerating Pre-trained Models with Plug-and-Play Compression Modules. (arXiv:2310.15724v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15724
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Variator&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#30340;&#21387;&#32553;&#25554;&#20214;&#22686;&#24378;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#24037;&#20316;&#36127;&#36733;&#21160;&#24577;&#36873;&#25321;&#19981;&#21516;&#21152;&#36895;&#27604;&#30340;&#25554;&#20214;&#12290;&#25554;&#20214;&#37319;&#29992;&#20102;&#21387;&#32553;&#38544;&#34255;&#21521;&#37327;&#30340;&#26041;&#27861;&#26469;&#20943;&#23567;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19988;&#30001;&#20110;&#21442;&#25968;&#23569;&#65292;&#21487;&#20197;&#33410;&#30465;&#23384;&#20648;&#21644;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#20195;&#20215;&#26159;&#24040;&#22823;&#30340;&#21442;&#25968;&#22823;&#23567;&#21644;&#38543;&#20043;&#32780;&#26469;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Variator&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#21152;&#36895;&#26041;&#27861;&#65292;&#36890;&#36807;&#21363;&#25554;&#21363;&#29992;&#30340;&#21387;&#32553;&#25554;&#20214;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#21387;&#32553;&#25554;&#20214;&#36890;&#36807;&#23558;&#22810;&#20010;&#38544;&#34255;&#21521;&#37327;&#21387;&#32553;&#21040;&#19968;&#20010;&#21521;&#37327;&#26469;&#32553;&#20943;&#24207;&#21015;&#38271;&#24230;&#65292;&#24182;&#19982;&#21407;&#22987;PLMs&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;&#20256;&#32479;&#30340;&#27169;&#22411;&#21152;&#36895;&#26041;&#27861;&#19981;&#21516;&#65292;Variator&#20855;&#26377;&#20004;&#20010;&#29420;&#29305;&#30340;&#20248;&#28857;&#65306;&#65288;1&#65289;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#30340;&#21387;&#32553;&#25554;&#20214;&#30340;&#21363;&#25554;&#21363;&#29992;&#24615;&#36136;&#20351;&#24471;&#21487;&#20197;&#26681;&#25454;&#24403;&#21069;&#24037;&#20316;&#36127;&#36733;&#21160;&#24577;&#36873;&#25321;&#20855;&#26377;&#19981;&#21516;&#21152;&#36895;&#27604;&#30340;&#21387;&#32553;&#25554;&#20214;&#12290;&#65288;2&#65289;&#21387;&#32553;&#25554;&#20214;&#30001;&#20960;&#20010;&#32039;&#20945;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#32452;&#25104;&#65292;&#21442;&#25968;&#24456;&#23569;&#65292;&#22823;&#22823;&#33410;&#30465;&#20102;&#23384;&#20648;&#21644;&#20869;&#23384;&#24320;&#38144;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#36739;&#22823;&#23384;&#20648;&#38656;&#27714;&#21644;&#20869;&#23384;&#38656;&#27714;&#30340;&#22330;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) have achieved remarkable results on NLP tasks but at the expense of huge parameter sizes and the consequent computational costs. In this paper, we propose Variator, a parameter-efficient acceleration method that enhances computational efficiency through plug-and-play compression plugins. Compression plugins are designed to reduce the sequence length via compressing multiple hidden vectors into one and trained with original PLMs frozen. Different from traditional model acceleration methods, which compress PLMs to smaller sizes, Variator offers two distinct advantages: (1) In real-world applications, the plug-and-play nature of our compression plugins enables dynamic selection of different compression plugins with varying acceleration ratios based on the current workload. (2) The compression plugin comprises a few compact neural network layers with minimal parameters, significantly saving storage and memory overhead, particularly in scenarios with a gro
&lt;/p&gt;</description></item><item><title>Re-Temp&#26159;&#19968;&#20010;&#33021;&#22815;&#26681;&#25454;&#23454;&#20307;&#20851;&#31995;&#36339;&#36807;&#19981;&#30456;&#20851;&#24555;&#29031;&#24182;&#21033;&#29992;&#26174;&#24335;&#26102;&#38388;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;TKGC&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15722</link><description>&lt;p&gt;
Re-Temp&#65306;&#38754;&#21521;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#30340;&#20851;&#31995;&#24863;&#30693;&#26102;&#24577;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Re-Temp: Relation-Aware Temporal Representation Learning for Temporal Knowledge Graph Completion. (arXiv:2310.15722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15722
&lt;/p&gt;
&lt;p&gt;
Re-Temp&#26159;&#19968;&#20010;&#33021;&#22815;&#26681;&#25454;&#23454;&#20307;&#20851;&#31995;&#36339;&#36807;&#19981;&#30456;&#20851;&#24555;&#29031;&#24182;&#21033;&#29992;&#26174;&#24335;&#26102;&#38388;&#20449;&#24687;&#36827;&#34892;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;TKGC&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22806;&#25512;&#35774;&#32622;&#19979;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#23436;&#25104;&#65288;TKGC&#65289;&#26088;&#22312;&#39044;&#27979;&#26410;&#26469;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#65292;&#26356;&#25509;&#36817;&#20110;&#23454;&#38469;&#39044;&#27979;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;&#24212;&#29992;&#20110;&#26368;&#36817;&#24555;&#29031;&#30340;&#39034;&#24207;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#32534;&#30721;&#23454;&#20307;&#21644;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#26681;&#25454;&#26597;&#35810;&#20013;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#20851;&#31995;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#24555;&#29031;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#24573;&#35270;&#20102;&#26174;&#24335;&#30340;&#26102;&#38388;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;Re-Temp&#65288;&#20851;&#31995;&#24863;&#30693;&#26102;&#24577;&#34920;&#31034;&#23398;&#20064;&#65289;&#65292;&#23427;&#21033;&#29992;&#26174;&#24335;&#30340;&#26102;&#38388;&#23884;&#20837;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#27599;&#19968;&#20010;&#26102;&#38388;&#25139;&#20043;&#21518;&#24341;&#20837;&#36339;&#36807;&#20449;&#24687;&#27969;&#26469;&#36339;&#36807;&#39044;&#27979;&#20013;&#19981;&#24517;&#35201;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#21069;&#21521;&#20256;&#25773;&#26041;&#27861;&#26469;&#38450;&#27490;&#20449;&#24687;&#27844;&#28431;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;TKGC&#65288;&#22806;&#25512;&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Knowledge Graph Completion (TKGC) under the extrapolation setting aims to predict the missing entity from a fact in the future, posing a challenge that aligns more closely with real-world prediction problems. Existing research mostly encodes entities and relations using sequential graph neural networks applied to recent snapshots. However, these approaches tend to overlook the ability to skip irrelevant snapshots according to entity-related relations in the query and disregard the importance of explicit temporal information. To address this, we propose our model, Re-Temp (Relation-Aware Temporal Representation Learning), which leverages explicit temporal embedding as input and incorporates skip information flow after each timestamp to skip unnecessary information for prediction. Additionally, we introduce a two-phase forward propagation method to prevent information leakage. Through the evaluation on six TKGC (extrapolation) datasets, we demonstrate that our model outperforms 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#33041;&#32534;&#30721;&#30340;&#20219;&#21153;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;10&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#30456;&#36739;&#20110;&#24403;&#21069;&#22522;&#20934;&#32447;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10%&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15720</link><description>&lt;p&gt;
&#29992;&#20110;&#22823;&#33041;&#32534;&#30721;&#30340;&#20219;&#21153;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Ensemble of Task-Specific Language Models for Brain Encoding. (arXiv:2310.15720v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#33041;&#32534;&#30721;&#30340;&#20219;&#21153;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;10&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;&#30456;&#36739;&#20110;&#24403;&#21069;&#22522;&#20934;&#32447;&#65292;&#24179;&#22343;&#25552;&#39640;&#20102;10%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#35821;&#35328;&#27169;&#22411;&#36275;&#22815;&#20016;&#23500;&#65292;&#21487;&#20197;&#32534;&#30721;&#25105;&#20204;&#22823;&#33041;&#20013;&#29305;&#23450;&#20852;&#36259;&#21306;&#22495;&#30340;fMRI&#28608;&#27963;&#24773;&#20917;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#20174;&#20026;&#27969;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#23398;&#20064;&#30340;&#34920;&#31034;&#21521;&#39044;&#27979;&#22823;&#33041;&#21709;&#24212;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#30001;10&#20010;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;2&#20010;&#21477;&#27861;&#21644;8&#20010;&#35821;&#20041;&#65289;&#32452;&#25104;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#36825;&#26679;&#30340;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#22312;&#25152;&#26377;&#20852;&#36259;&#21306;&#22495;&#20013;&#65292;&#25105;&#20204;&#23558;&#24403;&#21069;&#30340;&#22522;&#20934;&#32447;&#25552;&#39640;&#20102;&#24179;&#22343;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have been shown to be rich enough to encode fMRI activations of certain Regions of Interest in our Brains. Previous works have explored transfer learning from representations learned for popular natural language processing tasks for predicting brain responses. In our work, we improve the performance of such encoders by creating an ensemble model out of 10 popular Language Models (2 syntactic and 8 semantic). We beat the current baselines by 10% on average across all ROIs through our ensembling methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22686;&#21152;&#25991;&#31456;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#65292;&#25913;&#36827;&#20102;&#29983;&#29289;&#21307;&#23398;&#31185;&#26222;&#24635;&#32467;&#30340;&#33258;&#21160;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25972;&#21512;&#22522;&#20110;&#22270;&#30340;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31185;&#26222;&#24635;&#32467;&#30340;&#21487;&#35835;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15702</link><description>&lt;p&gt;
&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#31185;&#26222;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Enhancing Biomedical Lay Summarisation with External Knowledge Graphs. (arXiv:2310.15702v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22686;&#21152;&#25991;&#31456;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#65292;&#25913;&#36827;&#20102;&#29983;&#29289;&#21307;&#23398;&#31185;&#26222;&#24635;&#32467;&#30340;&#33258;&#21160;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25972;&#21512;&#22522;&#20110;&#22270;&#30340;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#31185;&#26222;&#24635;&#32467;&#30340;&#21487;&#35835;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#33258;&#21160;&#31185;&#26222;&#24635;&#32467;&#26041;&#27861;&#23436;&#20840;&#20381;&#36182;&#20110;&#28304;&#25991;&#31456;&#65292;&#20294;&#36825;&#20123;&#25991;&#31456;&#38024;&#23545;&#25216;&#26415;&#35266;&#20247;&#65288;&#20363;&#22914;&#30740;&#31350;&#20154;&#21592;&#65289;&#32534;&#20889;&#65292;&#19981;&#22826;&#21487;&#33021;&#26126;&#30830;&#23450;&#20041;&#25152;&#26377;&#25216;&#26415;&#27010;&#24565;&#25110;&#25552;&#20379;&#36866;&#21512;&#31185;&#26222;&#35266;&#20247;&#30340;&#20840;&#37096;&#32972;&#26223;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#31185;&#26222;&#24635;&#32467;&#25968;&#25454;&#38598;eLife&#22686;&#21152;&#29305;&#23450;&#25991;&#31456;&#30340;&#30693;&#35782;&#22270;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27599;&#20010;&#30693;&#35782;&#22270;&#37117;&#21253;&#21547;&#26377;&#20851;&#30456;&#20851;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;&#36890;&#36807;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#23558;&#30693;&#35782;&#22270;&#25972;&#21512;&#21040;&#31185;&#26222;&#24635;&#32467;&#27169;&#22411;&#20013;&#30340;&#19977;&#31181;&#19981;&#21516;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#27599;&#31181;&#26041;&#27861;&#38024;&#23545;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#26550;&#26500;&#30340;&#19981;&#21516;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#23454;&#65292;&#25972;&#21512;&#22522;&#20110;&#22270;&#30340;&#39046;&#22495;&#30693;&#35782;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#31185;&#26222;&#24635;&#32467;&#30340;&#21487;&#35835;&#24615;&#65292;&#22823;&#22823;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous approaches for automatic lay summarisation are exclusively reliant on the source article that, given it is written for a technical audience (e.g., researchers), is unlikely to explicitly define all technical concepts or state all of the background information that is relevant for a lay audience. We address this issue by augmenting eLife, an existing biomedical lay summarisation dataset, with article-specific knowledge graphs, each containing detailed information on relevant biomedical concepts. Using both automatic and human evaluations, we systematically investigate the effectiveness of three different approaches for incorporating knowledge graphs within lay summarisation models, with each method targeting a distinct area of the encoder-decoder model architecture. Our results confirm that integrating graph-based domain knowledge can significantly benefit lay summarisation by substantially increasing the readability of generated text and improving the explanation of technical 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.15694</link><description>&lt;p&gt;
COPF: &#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#20154;&#31867;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
COPF: Continual Learning Human Preference through Optimal Policy Fitting. (arXiv:2310.15694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15694
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;COPF&#26041;&#27861;&#65292;&#25105;&#20204;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#21644;&#20989;&#25968;&#27491;&#21017;&#21270;&#26469;&#25345;&#32493;&#23398;&#20064;&#21644;&#36866;&#24212;&#20154;&#31867;&#20559;&#22909;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#25216;&#26415;&#26159;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20197;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;RLHF&#30340;LM&#22312;&#24341;&#20837;&#26032;&#30340;&#26597;&#35810;&#25110;&#21453;&#39304;&#26102;&#38656;&#35201;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#20154;&#31867;&#20559;&#22909;&#22312;&#19981;&#21516;&#39046;&#22495;&#25110;&#20219;&#21153;&#20043;&#38388;&#21487;&#33021;&#20250;&#26377;&#25152;&#21464;&#21270;&#12290;&#30001;&#20110;&#25152;&#38656;&#30340;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#19982;&#25968;&#25454;&#38544;&#31169;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#37325;&#26032;&#35757;&#32451;LM&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#23384;&#22312;&#23454;&#38469;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25345;&#32493;&#26368;&#20248;&#31574;&#30053;&#25311;&#21512;&#65288;COPF&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#27861;&#20272;&#35745;&#19968;&#31995;&#21015;&#26368;&#20248;&#31574;&#30053;&#65292;&#28982;&#21518;&#36890;&#36807;&#20989;&#25968;&#27491;&#21017;&#21270;&#19981;&#26029;&#25311;&#21512;&#31574;&#30053;&#24207;&#21015;&#12290;COPF&#21253;&#21547;&#19968;&#20010;&#21333;&#19968;&#30340;&#23398;&#20064;&#38454;&#27573;&#65292;&#19981;&#38656;&#35201;&#22797;&#26434;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The technique of Reinforcement Learning from Human Feedback (RLHF) is a commonly employed method to improve pre-trained Language Models (LM), enhancing their ability to conform to human preferences. Nevertheless, the current RLHF-based LMs necessitate full retraining each time novel queries or feedback are introduced, which becomes a challenging task because human preferences can vary between different domains or tasks. Retraining LMs poses practical difficulties in many real-world situations due to the significant time and computational resources required, along with concerns related to data privacy. To address this limitation, we propose a new method called Continual Optimal Policy Fitting (COPF), in which we estimate a series of optimal policies using the Monte Carlo method, and then continually fit the policy sequence with the function regularization. COPF involves a single learning phase and doesn't necessitate complex reinforcement learning. Importantly, it shares the capability 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3A2M+&#30340;&#33258;&#21160;&#33756;&#35889;&#31867;&#22411;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#25193;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21015;&#34920;&#23545;&#28921;&#39274;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#20010;&#24102;&#26377;&#21508;&#31181;&#29305;&#24449;&#21644;&#20061;&#20010;&#19981;&#21516;&#31867;&#22411;&#26631;&#31614;&#30340;&#28921;&#39274;&#39135;&#35889;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15693</link><description>&lt;p&gt;
&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#33756;&#35889;&#31867;&#22411;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Automated Recipe Genre Classification using Semi-Supervised Learning. (arXiv:2310.15693v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15693
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;3A2M+&#30340;&#33258;&#21160;&#33756;&#35889;&#31867;&#22411;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#25193;&#23637;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21015;&#34920;&#23545;&#28921;&#39274;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;&#30740;&#31350;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#20004;&#30334;&#19975;&#20010;&#24102;&#26377;&#21508;&#31181;&#29305;&#24449;&#21644;&#20061;&#20010;&#19981;&#21516;&#31867;&#22411;&#26631;&#31614;&#30340;&#28921;&#39274;&#39135;&#35889;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#20139;&#28921;&#39274;&#39135;&#35889;&#26159;&#20132;&#27969;&#28921;&#39274;&#21019;&#24847;&#21644;&#25552;&#20379;&#39135;&#29289;&#21046;&#20316;&#25351;&#24341;&#30340;&#22909;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23558;&#22312;&#32447;&#25214;&#21040;&#30340;&#21407;&#22987;&#39135;&#35889;&#20998;&#31867;&#20026;&#36866;&#24403;&#30340;&#39135;&#29289;&#31867;&#22411;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#27880;&#25968;&#25454;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#28151;&#21512;&#12289;&#20856;&#22411;&#21644;&#27880;&#37322;&#30340;&#25193;&#23637;&#20004;&#30334;&#19975;&#65288;3A2M+&#65289;&#28921;&#39274;&#39135;&#35889;&#25968;&#25454;&#38598;&#8221;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#30334;&#19975;&#20010;&#34987;&#26631;&#35760;&#20026;&#30456;&#24212;&#31867;&#21035;&#30340;&#28921;&#39274;&#39135;&#35889;&#65292;&#24182;&#25552;&#21462;&#20102;&#39135;&#35889;&#25551;&#36848;&#20013;&#30340;&#25193;&#23637;&#21629;&#21517;&#23454;&#20307;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#26631;&#39064;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#27493;&#39588;&#21644;&#25193;&#23637;&#21629;&#21517;&#23454;&#20307;&#31561;&#21508;&#31181;&#29305;&#24449;&#65292;&#20197;&#21450;&#20195;&#34920;&#31957;&#28857;&#12289;&#39278;&#26009;&#12289;&#38750;&#32032;&#39135;&#12289;&#34092;&#33756;&#12289;&#24555;&#39184;&#12289;&#35895;&#29289;&#12289;&#20027;&#39135;&#12289;&#37197;&#33756;&#21644;&#34701;&#21512;&#30340;&#20061;&#20010;&#19981;&#21516;&#31867;&#22411;&#30340;&#26631;&#31614;&#12290;&#25152;&#25552;&#20986;&#30340;&#21517;&#20026;3A2M+&#30340;&#27969;&#31243;&#36890;&#36807;&#20351;&#29992;&#20004;&#31181;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;&#65292;&#25193;&#23637;&#20102;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21015;&#34920;&#30340;&#22823;&#23567;&#65292;&#20197;&#35299;&#20915;&#20174;&#39135;&#35889;&#27493;&#39588;&#20013;&#32570;&#23569;&#30340;&#21629;&#21517;&#23454;&#20307;&#65292;&#22914;&#21152;&#28909;&#12289;&#26102;&#38388;&#25110;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sharing cooking recipes is a great way to exchange culinary ideas and provide instructions for food preparation. However, categorizing raw recipes found online into appropriate food genres can be challenging due to a lack of adequate labeled data. In this study, we present a dataset named the ``Assorted, Archetypal, and Annotated Two Million Extended (3A2M+) Cooking Recipe Dataset" that contains two million culinary recipes labeled in respective categories with extended named entities extracted from recipe descriptions. This collection of data includes various features such as title, NER, directions, and extended NER, as well as nine different labels representing genres including bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides, and fusions. The proposed pipeline named 3A2M+ extends the size of the Named Entity Recognition (NER) list to address missing named entities like heat, time or process from the recipe directions using two NER extraction tools. 3A2M+ dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#20889;&#33258;&#21160;&#31616;&#21270;&#19987;&#21033;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#38134;&#26631;&#20934;&#35821;&#26009;&#24211;&#29992;&#20110;&#35757;&#32451;&#31616;&#21270;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2310.15689</link><description>&lt;p&gt;
&#20026;&#19987;&#21033;&#31616;&#21270;&#21019;&#24314;&#19968;&#20010;&#38134;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Creating a silver standard for patent simplification. (arXiv:2310.15689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#20889;&#33258;&#21160;&#31616;&#21270;&#19987;&#21033;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#20010;&#38134;&#26631;&#20934;&#35821;&#26009;&#24211;&#29992;&#20110;&#35757;&#32451;&#31616;&#21270;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#21033;&#26159;&#26088;&#22312;&#19968;&#26041;&#38754;&#20445;&#25252;&#21457;&#26126;&#65292;&#21478;&#19968;&#26041;&#38754;&#20419;&#36827;&#25216;&#26415;&#30693;&#35782;&#27969;&#36890;&#30340;&#27861;&#24459;&#25991;&#20214;&#12290;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#39118;&#26684;&#65292;&#21363;&#27861;&#24459;&#12289;&#25216;&#26415;&#21644;&#26497;&#24230;&#27169;&#31946;&#30340;&#35821;&#35328;&#30340;&#28151;&#21512;&#65292;&#20351;&#24471;&#20154;&#31867;&#21644;&#26426;&#22120;&#38590;&#20197;&#33719;&#21462;&#20854;&#20869;&#23481;&#65292;&#24182;&#23545;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#20889;&#33258;&#21160;&#31616;&#21270;&#19987;&#21033;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#30001;&#20110;&#32570;&#20047;&#39046;&#22495;&#20869;&#24182;&#34892;&#30340;&#31616;&#21270;&#25968;&#25454;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#22823;&#35268;&#27169;&#19987;&#21033;&#21477;&#23376;&#38134;&#26631;&#20934;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#33719;&#24471;&#20505;&#36873;&#21477;&#23376;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#36890;&#29992;&#22495;&#30340;&#25913;&#20889;&#31995;&#32479;&#65307;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#23481;&#26131;&#20986;&#38169;&#19988;&#38590;&#20197;&#25511;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#36866;&#24403;&#30340;&#36807;&#28388;&#22120;&#37197;&#23545;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#26356;&#24178;&#20928;&#30340;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#25104;&#21151;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#31616;&#21270;&#31995;&#32479;&#12290;&#23545;&#21512;&#25104;&#38134;&#26631;&#20934;&#35821;&#26009;&#24211;&#30340;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#23427;&#34987;&#35748;&#20026;&#26159;&#31526;&#21512;&#35821;&#27861;&#12289;&#36866;&#24230;&#19988;&#21253;&#21547;&#31616;&#21333;&#21477;&#23376;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patents are legal documents that aim at protecting inventions on the one hand and at making technical knowledge circulate on the other. Their complex style -- a mix of legal, technical, and extremely vague language -- makes their content hard to access for humans and machines and poses substantial challenges to the information retrieval community. This paper proposes an approach to automatically simplify patent text through rephrasing. Since no in-domain parallel simplification data exist, we propose a method to automatically generate a large-scale silver standard for patent sentences. To obtain candidates, we use a general-domain paraphrasing system; however, the process is error-prone and difficult to control. Thus, we pair it with proper filters and construct a cleaner corpus that can successfully be used to train a simplification system. Human evaluation of the synthetic silver corpus shows that it is considered grammatical, adequate, and contains simple sentences.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#32858;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15684</link><description>&lt;p&gt;
&#21033;&#29992;&#24341;&#25991;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#32858;&#21512;&#25913;&#36827;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers. (arXiv:2310.15684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24341;&#25991;&#32858;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#30340;&#25688;&#35201;&#20855;&#26377;&#29305;&#23450;&#30340;&#39046;&#22495;&#29305;&#24449;&#65292;&#21253;&#25324;&#19987;&#38376;&#30340;&#20889;&#20316;&#39118;&#26684;&#21644;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#65292;&#36825;&#35201;&#27714;&#23545;&#30456;&#20851;&#25991;&#29486;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#19982;&#29983;&#29289;&#21307;&#23398;&#19987;&#23478;&#30456;&#23218;&#32654;&#30340;&#25216;&#26415;&#25688;&#35201;&#26102;&#23384;&#22312;&#22256;&#38590;&#65292;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#32858;&#21512;&#28304;&#35770;&#25991;&#20013;&#24341;&#29992;&#30340;&#22806;&#37096;&#35770;&#25991;&#20013;&#30340;&#30693;&#35782;&#65292;&#25552;&#39640;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24341;&#25991;&#32858;&#21512;&#27169;&#22411;&#65292;&#23427;&#23558;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#19982;&#35770;&#25991;&#20869;&#23481;&#30456;&#32467;&#21512;&#65292;&#20351;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#21033;&#29992;&#35770;&#25991;&#20869;&#23481;&#21644;&#24341;&#25991;&#35770;&#25991;&#20013;&#30340;&#30456;&#20851;&#30693;&#35782;&#29983;&#25104;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#29983;&#29289;&#21307;&#23398;&#25688;&#35201;&#24635;&#32467;&#25968;&#25454;&#38598;&#20316;&#20026;&#25105;&#20204;&#30740;&#31350;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensiv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#38450;&#25514;&#26045;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35201;&#27714;&#24037;&#20316;&#32773;&#19981;&#20351;&#29992;LLM&#24182;&#22686;&#21152;&#20351;&#29992;&#25104;&#26412;&#21487;&#26174;&#33879;&#38477;&#20302;LLM&#30340;&#20351;&#29992;&#29575;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#12290;&#28982;&#32780;&#65292;&#38450;&#27490;LLM&#30340;&#20351;&#29992;&#21487;&#33021;&#24433;&#21709;&#21040;&#33719;&#24471;&#39640;&#36136;&#37327;&#22238;&#31572;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20851;&#27880;&#20154;&#31867;&#34892;&#20026;&#21644;&#20247;&#21253;&#25968;&#25454;&#35757;&#32451;&#30340;&#26410;&#26469;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.15683</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20247;&#21253;&#24037;&#20316;&#20013;&#30340;&#20351;&#29992;&#29575;&#21644;&#39044;&#38450;&#25514;&#26045;
&lt;/p&gt;
&lt;p&gt;
Prevalence and prevention of large language model use in crowd work. (arXiv:2310.15683v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20247;&#21253;&#24037;&#20316;&#32773;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20351;&#29992;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#39044;&#38450;&#25514;&#26045;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#35201;&#27714;&#24037;&#20316;&#32773;&#19981;&#20351;&#29992;LLM&#24182;&#22686;&#21152;&#20351;&#29992;&#25104;&#26412;&#21487;&#26174;&#33879;&#38477;&#20302;LLM&#30340;&#20351;&#29992;&#29575;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#12290;&#28982;&#32780;&#65292;&#38450;&#27490;LLM&#30340;&#20351;&#29992;&#21487;&#33021;&#24433;&#21709;&#21040;&#33719;&#24471;&#39640;&#36136;&#37327;&#22238;&#31572;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20110;&#20851;&#27880;&#20154;&#31867;&#34892;&#20026;&#21644;&#20247;&#21253;&#25968;&#25454;&#35757;&#32451;&#30340;&#26410;&#26469;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20247;&#21253;&#24037;&#20316;&#20013;&#30340;&#26222;&#36941;&#20351;&#29992;&#65292;&#24182;&#19988;&#35777;&#26126;&#26377;&#38024;&#23545;&#24615;&#30340;&#32531;&#35299;&#31574;&#30053;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;LLM&#30340;&#20351;&#29992;&#29575;&#65292;&#20294;&#26080;&#27861;&#23436;&#20840;&#28040;&#38500;&#12290;&#22312;&#19968;&#20010;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;&#24037;&#20316;&#32773;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;LLM&#20351;&#29992;&#30340;&#25351;&#31034;&#65292;&#20272;&#35745;LLM&#20351;&#29992;&#29575;&#32422;&#20026;30%&#65292;&#20294;&#36890;&#36807;&#35201;&#27714;&#24037;&#20316;&#32773;&#19981;&#20351;&#29992;LLM&#24182;&#22686;&#21152;&#20351;&#29992;&#25104;&#26412;&#65288;&#20363;&#22914;&#31105;&#27490;&#22797;&#21046;&#31896;&#36148;&#65289;&#65292;LLM&#20351;&#29992;&#29575;&#20943;&#23569;&#20102;&#32422;&#19968;&#21322;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20851;&#20110;LLM&#20351;&#29992;&#21450;&#20854;&#39044;&#38450;&#30340;&#26356;&#22810;&#35265;&#35299;&#65306;LLM&#20351;&#29992;&#20135;&#29983;&#39640;&#36136;&#37327;&#20294;&#21516;&#36136;&#21270;&#30340;&#22238;&#31572;&#65292;&#36825;&#21487;&#33021;&#25439;&#23475;&#37027;&#20123;&#20851;&#27880;&#20154;&#31867;&#65288;&#32780;&#19981;&#26159;&#27169;&#22411;&#65289;&#34892;&#20026;&#30340;&#30740;&#31350;&#65292;&#24182;&#19988;&#38477;&#20302;&#23545;&#20247;&#21253;&#25968;&#25454;&#35757;&#32451;&#30340;&#26410;&#26469;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#38450;&#27490;LLM&#30340;&#20351;&#29992;&#21487;&#33021;&#19982;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#23384;&#22312;&#30683;&#30462;&#65307;&#20363;&#22914;&#65292;&#22312;&#35831;&#27714;&#24037;&#20316;&#32773;&#19981;&#20351;&#29992;LLM&#26102;&#65292;&#25688;&#35201;&#20013;&#21253;&#21547;&#30340;&#20851;&#38190;&#20449;&#24687;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#21487;&#33021;&#20250;&#38543;&#30528;LLM&#30340;&#26222;&#21450;&#25110;&#21462;&#28040;&#32780;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the use of large language models (LLMs) is prevalent among crowd workers, and that targeted mitigation strategies can significantly reduce, but not eliminate, LLM use. On a text summarization task where workers were not directed in any way regarding their LLM use, the estimated prevalence of LLM use was around 30%, but was reduced by about half by asking workers to not use LLMs and by raising the cost of using them, e.g., by disabling copy-pasting. Secondary analyses give further insight into LLM use and its prevention: LLM use yields high-quality but homogeneous responses, which may harm research concerned with human (rather than model) behavior and degrade future models trained with crowdsourced data. At the same time, preventing LLM use may be at odds with obtaining high-quality responses; e.g., when requesting workers not to use LLMs, summaries contained fewer keywords carrying essential information. Our estimates will likely change as LLMs increase in popularity or ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23545;&#20110;&#38899;&#39057;&#35782;&#21035;&#20219;&#21153;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#23545;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#32422;80&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#30456;&#23545;&#25552;&#39640;14.9%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15672</link><description>&lt;p&gt;
&#25105;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;ASR&#31995;&#32479;&#38656;&#35201;&#22810;&#23569;&#19978;&#19979;&#25991;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Much Context Does My Attention-Based ASR System Need?. (arXiv:2310.15672v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#23545;&#20110;&#38899;&#39057;&#35782;&#21035;&#20219;&#21153;&#65292;&#35757;&#32451;&#21644;&#35780;&#20272;&#20351;&#29992;&#19981;&#21516;&#38271;&#24230;&#30340;&#24207;&#21015;&#23545;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#32422;80&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#30456;&#23545;&#25552;&#39640;14.9%&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#36229;&#36807;30&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#22312;&#25991;&#29486;&#20013;&#26159;&#19981;&#24120;&#35265;&#30340;&#65292;&#24182;&#19988;&#24471;&#21040;&#20102;&#24456;&#23569;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;/&#35780;&#20272;&#65288;&#22522;&#20110;&#23494;&#38598;&#27880;&#24847;&#21147;&#30340;&#65289;&#22768;&#23398;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#24207;&#21015;&#38271;&#24230;&#30340;&#32553;&#25918;&#23545;&#35821;&#38899;&#35782;&#21035;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#22823;&#32422;100,000&#20010;&#20266;&#26631;&#35760;&#30340;Spotify&#25773;&#23458;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36825;&#20123;&#23454;&#39564;&#65292;&#25506;&#32034;&#20102;5&#31186;&#21040;1&#23567;&#26102;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#12290;&#23545;&#38271;&#26684;&#24335;&#25968;&#25454;&#38598;Earnings-22&#21644;Tedlium&#36827;&#34892;&#20102;&#38646;-shot&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#22823;&#32422;80&#31186;&#30340;&#22768;&#23398;&#19978;&#19979;&#25991;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#39640;&#36798;14.9%&#30456;&#23545;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#26463;&#25628;&#32034;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#19982;&#31995;&#32479;&#32452;&#21512;&#24418;&#25104;&#20102;&#19968;&#20010;&#20840;&#38271;&#19978;&#19979;&#25991;ASR&#31995;&#32479;&#65292;&#20854;&#32467;&#26524;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the task of speech recognition, the use of more than 30 seconds of acoustic context during training is uncommon, and under-investigated in literature. In this work, we examine the effect of scaling the sequence length used to train/evaluate (dense-attention based) acoustic and language models on speech recognition performance. For these experiments a dataset of roughly 100,000 pseudo-labelled Spotify podcasts is used, with context lengths of 5 seconds to 1 hour being explored. Zero-shot evaluations on long-format datasets Earnings-22 and Tedlium demonstrate a benefit from training with around 80 seconds of acoustic context, showing up to a 14.9% relative improvement from a limited context baseline. Furthermore, we perform a system combination with long-context transformer language models via beam search for a fully long-context ASR system, with results that are competitive with the current state-of-the-art.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#20174;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#65292;&#40723;&#21169;&#22810;&#20010;&#27169;&#22411;&#20026;&#21516;&#19968;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#34920;&#36848;&#39044;&#27979;&#30456;&#21516;&#30340;&#34920;&#36798;&#24335;&#35821;&#27861;&#26641;&#65292;&#20174;&#32780;&#25429;&#25417;&#19968;&#33268;&#30340;&#20449;&#24687;&#24182;&#21435;&#38500;&#20887;&#20313;&#12290;</title><link>http://arxiv.org/abs/2310.15664</link><description>&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#36798;&#24335;&#35821;&#27861;&#20449;&#24687;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Expression Syntax Information Bottleneck for Math Word Problems. (arXiv:2310.15664v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#20174;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#21435;&#38500;&#20887;&#20313;&#20449;&#24687;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#65292;&#40723;&#21169;&#22810;&#20010;&#27169;&#22411;&#20026;&#21516;&#19968;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#34920;&#36848;&#39044;&#27979;&#30456;&#21516;&#30340;&#34920;&#36798;&#24335;&#35821;&#27861;&#26641;&#65292;&#20174;&#32780;&#25429;&#25417;&#19968;&#33268;&#30340;&#20449;&#24687;&#24182;&#21435;&#38500;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#38382;&#39064;&#30340;&#34920;&#36798;&#24335;&#35821;&#27861;&#20449;&#24687;&#29942;&#39048; (ESIB) &#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#20174;&#25968;&#23398;&#38382;&#39064;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#65292;&#21516;&#26102;&#36807;&#28388;&#25481;&#21253;&#21547;&#19981;&#30456;&#20851;&#29305;&#24449;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;ESIB&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#30456;&#20114;&#23398;&#20064;&#26469;&#40723;&#21169;&#22810;&#20010;&#27169;&#22411;&#20026;&#21516;&#19968;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#34920;&#36848;&#39044;&#27979;&#30456;&#21516;&#30340;&#34920;&#36798;&#24335;&#35821;&#27861;&#26641;&#65292;&#20174;&#32780;&#25429;&#25417;&#19968;&#33268;&#30340;&#34920;&#36798;&#24335;&#35821;&#27861;&#26641;&#20449;&#24687;&#65292;&#24182;&#21435;&#38500;&#20855;&#26377;&#29305;&#23450;&#35821;&#27861;&#26080;&#20851;&#29305;&#24449;&#30340;&#20887;&#20313;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Math Word Problems (MWP) aims to automatically solve mathematical questions given in texts. Previous studies tend to design complex models to capture additional information in the original text so as to enable the model to gain more comprehensive features. In this paper, we turn our attention in the opposite direction, and work on how to discard redundant features containing spurious correlations for MWP. To this end, we design an Expression Syntax Information Bottleneck method for MWP (called ESIB) based on variational information bottleneck, which extracts essential features of expression syntax tree while filtering latent-specific redundancy containing syntax-irrelevant features. The key idea of ESIB is to encourage multiple models to predict the same expression syntax tree for different problem representations of the same problem by mutual learning so as to capture consistent information of expression syntax tree and discard latent-specific redundancy. To improve the generalization
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#30340;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#31574;&#30053;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#24378;&#35843;&#20351;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#12290;&#36825;&#26159;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.15654</link><description>&lt;p&gt;
LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Detection of LLMs-Generated Content. (arXiv:2310.15654v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15654
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26159;&#20851;&#20110;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#30340;&#32508;&#36848;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#31574;&#30053;&#21644;&#25361;&#25112;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#24378;&#35843;&#20351;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#12290;&#36825;&#26159;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23548;&#33268;&#21512;&#25104;&#20869;&#23481;&#29983;&#25104;&#19981;&#26029;&#22686;&#21152;&#65292;&#28041;&#21450;&#23186;&#20307;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#20844;&#20849;&#35805;&#35821;&#21644;&#25945;&#32946;&#31561;&#22810;&#20010;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;LLMs&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#29616;&#26377;&#26816;&#27979;&#31574;&#30053;&#21644;&#22522;&#20934;&#30340;&#35814;&#32454;&#27010;&#36848;&#65292;&#23457;&#26597;&#23427;&#20204;&#30340;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#21069;&#26223;&#65292;&#25552;&#20513;&#37319;&#29992;&#26356;&#28789;&#27963;&#21644;&#24378;&#22823;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#26816;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#20027;&#24352;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#24212;&#23545;&#19981;&#21516;&#25915;&#20987;&#65292;&#20197;&#25269;&#24481;LLMs&#19981;&#26029;&#21457;&#23637;&#30340;&#33021;&#21147;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;LLMs&#26102;&#20195;&#26816;&#27979;&#30340;&#39318;&#20010;&#32508;&#21512;&#35843;&#26597;&#12290;&#25105;&#20204;&#24076;&#26395;&#23427;&#33021;&#22815;&#25552;&#20379;&#23545;LLMs&#29983;&#25104;&#20869;&#23481;&#26816;&#27979;&#24403;&#21069;&#24773;&#20917;&#30340;&#24191;&#27867;&#29702;&#35299;&#65292;&#24182;&#20026;&#30740;&#31350;&#35813;&#39046;&#22495;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for res
&lt;/p&gt;</description></item><item><title>CoAnnotating&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#31867;-LLM&#32852;&#21512;&#27880;&#37322;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26469;&#20272;&#35745;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#39640;&#36798;21%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.15638</link><description>&lt;p&gt;
CoAnnotating&#65306;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#27880;&#37322;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#24037;&#20316;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation. (arXiv:2310.15638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15638
&lt;/p&gt;
&lt;p&gt;
CoAnnotating&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#31867;-LLM&#32852;&#21512;&#27880;&#37322;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26469;&#20272;&#35745;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#39640;&#36798;21%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#26631;&#27880;&#25968;&#25454;&#22312;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#24615;&#33021;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#25991;&#26412;&#27880;&#37322;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#38646; shot &#33021;&#21147;&#65292;&#19982;&#20154;&#31867;&#27880;&#37322;&#32773;&#30456;&#27604;&#29978;&#33267;&#36229;&#36807;&#20102;&#20154;&#31867;&#12290;&#30001;&#20110;&#25104;&#26412;&#36739;&#20302;&#19988;&#21487;&#25193;&#23637;&#24615;&#36739;&#39640;&#65292;&#36825;&#26679;&#30340;LLMs&#21487;&#20197;&#20316;&#20026;&#25163;&#21160;&#26631;&#27880;&#30340;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26410;&#26377;&#24037;&#20316;&#21033;&#29992;LLMs&#20316;&#20026;&#34917;&#20805;&#27880;&#37322;&#32773;&#65292;&#20063;&#27809;&#26377;&#25506;&#32034;&#22914;&#20309;&#26368;&#20339;&#20998;&#37197;&#20154;&#31867;&#21644;LLMs&#30340;&#27880;&#37322;&#24037;&#20316;&#20197;&#23454;&#29616;&#36136;&#37327;&#21644;&#25104;&#26412;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoAnnotating&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20154;&#31867;-LLM&#32852;&#21512;&#27880;&#37322;&#33539;&#24335;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#30340;&#27880;&#37322;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#21033;&#29992;&#19981;&#30830;&#23450;&#24615;&#26469;&#20272;&#35745;LLMs&#30340;&#27880;&#37322;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;CoAnnotating&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#24037;&#20316;&#20998;&#37197;&#26041;&#24335;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#39640;&#36798;21%&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#38543;&#26426;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21382;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#25216;&#33021;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#25551;&#36848;&#37096;&#20998;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#30340;&#32844;&#19994;&#21160;&#21521;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15636</link><description>&lt;p&gt;
&#20351;&#29992;&#31616;&#21382;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#25216;&#33021;&#21305;&#37197;&#30340;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Career Path Prediction using Resume Representation Learning and Skill-based Matching. (arXiv:2310.15636v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31616;&#21382;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#25216;&#33021;&#21305;&#37197;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#30740;&#31350;&#25991;&#26412;&#25551;&#36848;&#37096;&#20998;&#26469;&#39044;&#27979;&#19979;&#19968;&#27493;&#30340;&#32844;&#19994;&#21160;&#21521;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#32844;&#19994;&#21305;&#37197;&#23545;&#24037;&#20316;&#28385;&#24847;&#24230;&#21644;&#24037;&#20316;&#32489;&#25928;&#30340;&#24433;&#21709;&#34987;&#24191;&#27867;&#25215;&#35748;&#65292;&#36825;&#20984;&#26174;&#20102;&#22312;&#32844;&#19994;&#29983;&#28079;&#20013;&#20026;&#24037;&#20316;&#32773;&#25552;&#20379;&#19979;&#19968;&#27493;&#34892;&#21160;&#30340;&#37325;&#35201;&#24615;&#12290;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#26159;&#39044;&#27979;&#32844;&#19994;&#29983;&#28079;&#20013;&#30340;&#19979;&#19968;&#27493;&#34892;&#21160;&#30340;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#21592;&#24037;&#27969;&#22833;&#39044;&#38450;&#21644;&#20869;&#37096;&#23703;&#20301;&#27969;&#21160;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#29616;&#26377;&#30340;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#31169;&#20154;&#32844;&#19994;&#21382;&#21490;&#25968;&#25454;&#26469;&#24314;&#27169;&#32844;&#20301;&#21644;&#20844;&#21496;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#31616;&#21382;&#20013;&#30340;&#24037;&#20316;&#32463;&#21382;&#37096;&#20998;&#30340;&#26410;&#24320;&#21457;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#30001;2,164&#20010;&#21311;&#21517;&#21270;&#32844;&#19994;&#32463;&#21382;&#32452;&#25104;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#65292;&#24182;&#24102;&#26377;ESCO&#32844;&#19994;&#26631;&#31614;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#24037;&#20316;&#21382;&#21490;&#25968;&#25454;&#35774;&#35745;&#30340;&#26032;&#39062;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;CareerBERT&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#25216;&#33021;&#21644;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#26469;&#36827;&#34892;&#32844;&#19994;&#36335;&#24452;&#39044;&#27979;&#65292;&#22312;@10&#19979;&#65292;&#25216;&#33021;&#27169;&#22411;&#21644;&#25991;&#26412;&#27169;&#22411;&#23454;&#29616;&#20102;35.24%&#21644;39.61%&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impact of person-job fit on job satisfaction and performance is widely acknowledged, which highlights the importance of providing workers with next steps at the right time in their career. This task of predicting the next step in a career is known as career path prediction, and has diverse applications such as turnover prevention and internal job mobility. Existing methods to career path prediction rely on large amounts of private career history data to model the interactions between job titles and companies. We propose leveraging the unexplored textual descriptions that are part of work experience sections in resumes. We introduce a structured dataset of 2,164 anonymized career histories, annotated with ESCO occupation labels. Based on this dataset, we present a novel representation learning approach, CareerBERT, specifically designed for work history data. We develop a skill-based model and a text-based model for career path prediction, which achieve 35.24% and 39.61% recall@10 r
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#35821;&#35328;&#35774;&#35745;&#12289;&#24211;&#21644;&#22403;&#22334;&#25910;&#38598;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;64&#20301;&#20307;&#31995;&#32467;&#26500;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#31934;&#24230;&#25972;&#25968;&#12289;&#31616;&#21270;UTF-8&#23383;&#31526;&#20018;&#32034;&#24341;&#20197;&#21450;&#22686;&#24378;&#22403;&#22334;&#25910;&#38598;&#22120;&#30340;&#21151;&#33021;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#21644;&#33410;&#30465;&#20102;&#20869;&#23384;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.15632</link><description>&lt;p&gt;
&#22312;&#35821;&#35328;&#35774;&#35745;&#12289;&#24211;&#25110;&#22403;&#22334;&#25910;&#38598;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;64&#20301;&#20307;&#31995;&#32467;&#26500;&#30340;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Tips for making the most of 64-bit architectures in langage design, libraries or garbage collection. (arXiv:2310.15632v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15632
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#35821;&#35328;&#35774;&#35745;&#12289;&#24211;&#21644;&#22403;&#22334;&#25910;&#38598;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;64&#20301;&#20307;&#31995;&#32467;&#26500;&#30340;&#25216;&#24039;&#65292;&#36890;&#36807;&#23454;&#29616;&#22810;&#31934;&#24230;&#25972;&#25968;&#12289;&#31616;&#21270;UTF-8&#23383;&#31526;&#20018;&#32034;&#24341;&#20197;&#21450;&#22686;&#24378;&#22403;&#22334;&#25910;&#38598;&#22120;&#30340;&#21151;&#33021;&#65292;&#25552;&#39640;&#20102;&#35745;&#31639;&#24615;&#33021;&#21644;&#33410;&#30465;&#20102;&#20869;&#23384;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#25104;&#20026;&#26631;&#20934;&#30340;64&#20301;&#20307;&#31995;&#32467;&#26500;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20302;&#32423;&#32534;&#31243;&#21487;&#33021;&#24615;&#12290;&#22312;&#35745;&#31639;&#21382;&#21490;&#19978;&#65292;&#22320;&#22336;&#23492;&#23384;&#22120;&#30340;&#22823;&#23567;&#39318;&#27425;&#36828;&#36828;&#36229;&#36807;&#20854;&#24635;&#32447;&#30340;&#29289;&#29702;&#23481;&#37327;&#12290;&#22312;&#31616;&#35201;&#22238;&#39038;&#20102;&#19982;&#21487;&#29992;64&#20301;&#30456;&#27604;&#22320;&#22336;&#30340;&#22823;&#23567;&#25152;&#25552;&#20379;&#30340;&#21487;&#33021;&#24615;&#20043;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#20855;&#20307;&#30340;&#20363;&#23376;&#26469;&#35828;&#26126;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#23492;&#23384;&#22120;&#20013;&#30340;&#31354;&#38386;&#20301;&#12290;&#20854;&#20013;&#20004;&#20010;&#20363;&#23376;&#28041;&#21450;&#23454;&#29616;&#19968;&#20010;&#26032;&#30340;&#38745;&#24577;&#31867;&#22411;&#32534;&#31243;&#35821;&#35328;&#30340;&#24211;&#12290;&#39318;&#20808;&#65292;&#26159;&#22810;&#31934;&#24230;&#25972;&#25968;&#30340;&#23454;&#29616;&#65292;&#26088;&#22312;&#25552;&#39640;&#35745;&#31639;&#36895;&#24230;&#21644;&#33410;&#30465;RAM&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#20363;&#23376;&#20851;&#27880;&#20110;&#24211;&#23545;UTF-8&#23383;&#31526;&#20018;&#30340;&#22788;&#29702;&#65292;&#30446;&#26631;&#26159;&#36890;&#36807;&#24573;&#30053;&#27599;&#20010;UTF-8&#23383;&#31526;&#30340;&#29289;&#29702;&#22823;&#23567;&#20351;&#32034;&#24341;&#26356;&#21152;&#26041;&#20415;&#12290;&#26368;&#21518;&#65292;&#31532;&#19977;&#20010;&#20363;&#23376;&#26159;&#23545;&#22403;&#22334;&#25910;&#38598;&#22120;&#30340;&#21487;&#33021;&#22686;&#24378;&#65292;&#29305;&#21035;&#26159;mark \&amp; sweep&#12290;
&lt;/p&gt;
&lt;p&gt;
The 64-bit architectures that have become standard today offer unprecedented low-level programming possibilities. For the first time in the history of computing, the size of address registers far exceeded the physical capacity of their bus.After a brief reminder of the possibilities offered by the small size of addresses compared to the available 64 bits,we develop three concrete examples of how the vacant bits of these registers can be used.Among these examples, two of them concern the implementation of a library for a new statically typed programming language.Firstly, the implementation of multi-precision integers, with the aim of improving performance in terms of both calculation speed and RAM savings.The second example focuses on the library's handling of UTF-8 character strings.Here, the idea is to make indexing easier by ignoring the physical size of each UTF-8 characters.Finally, the third example is a possible enhancement of garbage collectors, in particular the mark \&amp; sweep f
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15612</link><description>&lt;p&gt;
Nko&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#65306;&#24037;&#20855;&#12289;&#35821;&#26009;&#24211;&#21644;&#22522;&#20934;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Machine Translation for Nko: Tools, Corpora and Baseline Results. (arXiv:2310.15612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;Nko&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#24320;&#21457;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#21253;&#25324;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#12289;&#25193;&#23637;&#30340;&#35821;&#26009;&#24211;&#21644;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#23612;&#31185;&#35821;&#65288;&#19968;&#31181;&#22312;&#22810;&#20010;&#35199;&#38750;&#22269;&#23478;&#20351;&#29992;&#30340;&#35821;&#35328;&#65289;&#27809;&#26377;&#21487;&#29992;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#20294;&#23427;&#22312;&#25991;&#21270;&#21644;&#25945;&#32946;&#20215;&#20540;&#19978;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#24037;&#20855;&#12289;&#36164;&#28304;&#21644;&#22522;&#20934;&#32467;&#26524;&#65292;&#26088;&#22312;&#24320;&#21457;&#21487;&#29992;&#30340;&#23612;&#31185;&#35821;&#21644;&#20854;&#20182;&#24403;&#21069;&#27809;&#26377;&#36275;&#22815;&#22823;&#30340;&#24179;&#34892;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#30340;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#20855;&#20307;&#21253;&#25324;&#65306;(1) Friallel&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#20316;&#24179;&#34892;&#25991;&#26412;&#25972;&#29702;&#36719;&#20214;&#65292;&#36890;&#36807;&#22522;&#20110;&#21103;&#26412;&#32534;&#36753;&#30340;&#24037;&#20316;&#27969;&#31243;&#23454;&#29616;&#36136;&#37327;&#25511;&#21046;&#12290;(2) &#25193;&#23637;&#20102;FLoRes-200&#21644;NLLB-Seed&#35821;&#26009;&#24211;&#65292;&#20174;&#20854;&#20182;&#35821;&#35328;&#20013;&#19982;&#23612;&#31185;&#35821;&#24179;&#34892;&#32763;&#35793;&#20102;2,009&#21644;6,193&#20010;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;(3) nicolingua-0005&#65306;&#21253;&#21547;130,850&#20010;&#24179;&#34892;&#29255;&#27573;&#30340;&#19977;&#35821;&#21644;&#21452;&#35821;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#36229;&#36807;3&#30334;&#19975;&#23612;&#31185;&#35821;&#21333;&#35821;&#35328;&#35821;&#26009;&#24211;&#12290;(4) &#22522;&#32447;&#21452;&#35821;&#21644;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#32467;&#26524;&#19982;b...
&lt;/p&gt;
&lt;p&gt;
Currently, there is no usable machine translation system for Nko, a language spoken by tens of millions of people across multiple West African countries, which holds significant cultural and educational value. To address this issue, we present a set of tools, resources, and baseline results aimed towards the development of usable machine translation systems for Nko and other languages that do not currently have sufficiently large parallel text corpora available. (1) Friallel: A novel collaborative parallel text curation software that incorporates quality control through copyedit-based workflows. (2) Expansion of the FLoRes-200 and NLLB-Seed corpora with 2,009 and 6,193 high-quality Nko translations in parallel with 204 and 40 other languages. (3) nicolingua-0005: A collection of trilingual and bilingual corpora with 130,850 parallel segments and monolingual corpora containing over 3 million Nko words. (4) Baseline bilingual and multilingual neural machine translation results with the b
&lt;/p&gt;</description></item><item><title>MUSER&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#35270;&#35282;&#30456;&#20284;&#24230;&#27979;&#37327;&#21644;&#21477;&#23376;&#32423;&#27861;&#24459;&#35201;&#32032;&#27880;&#37322;&#30340;&#30456;&#20284;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#23454;&#29616;&#20934;&#30830;&#35780;&#20272;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#32771;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.15602</link><description>&lt;p&gt;
MUSER: &#19968;&#20010;&#22810;&#35270;&#35282;&#30456;&#20284;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MUSER: A Multi-View Similar Case Retrieval Dataset. (arXiv:2310.15602v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15602
&lt;/p&gt;
&lt;p&gt;
MUSER&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#35270;&#35282;&#30456;&#20284;&#24230;&#27979;&#37327;&#21644;&#21477;&#23376;&#32423;&#27861;&#24459;&#35201;&#32032;&#27880;&#37322;&#30340;&#30456;&#20284;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#23454;&#29616;&#20934;&#30830;&#35780;&#20272;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20284;&#26696;&#20363;&#26816;&#32034;&#65288;SCR&#65289;&#26159;&#20419;&#36827;&#21496;&#27861;&#20844;&#27491;&#30340;&#20856;&#22411;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SCR&#25968;&#25454;&#38598;&#21482;&#20851;&#27880;&#20107;&#23454;&#25551;&#36848;&#37096;&#20998;&#26469;&#21028;&#26029;&#26696;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#26377;&#20215;&#20540;&#30340;&#37096;&#20998;&#65288;&#20363;&#22914;&#27861;&#38498;&#30340;&#24847;&#35265;&#65289;&#65292;&#36825;&#20123;&#37096;&#20998;&#21487;&#20197;&#25552;&#20379;&#28145;&#20837;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26696;&#20363;&#30340;&#30456;&#20284;&#24230;&#36890;&#24120;&#21482;&#36890;&#36807;&#20107;&#23454;&#25551;&#36848;&#30340;&#25991;&#26412;&#35821;&#20041;&#26469;&#34913;&#37327;&#65292;&#21487;&#33021;&#26080;&#27861;&#20174;&#27861;&#24459;&#30693;&#35782;&#30340;&#35282;&#24230;&#25429;&#25417;&#21040;&#27861;&#24459;&#26696;&#20363;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MUSER&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Multi-View&#30456;&#20284;&#24230;&#27979;&#37327;&#21644;&#21477;&#23376;&#32423;&#27861;&#24459;&#35201;&#32032;&#27880;&#37322;&#30340;&#30456;&#20284;&#26696;&#20363;&#26816;&#32034;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#35270;&#35282;&#65288;&#27861;&#24459;&#20107;&#23454;&#12289;&#20105;&#35758;&#28966;&#28857;&#12289;&#27861;&#24459;&#27861;&#35268;&#65289;&#65292;&#24182;&#20026;&#27599;&#20010;&#35270;&#35282;&#26500;&#24314;&#20102;&#19968;&#20010;&#20840;&#38754;&#21644;&#32467;&#26500;&#21270;&#30340;&#27861;&#24459;&#35201;&#32032;&#26631;&#31614;&#27169;&#24335;&#65292;&#20197;&#23454;&#29616;&#26696;&#20363;&#30340;&#20934;&#30830;&#35780;&#20272;&#21644;&#19987;&#19994;&#30693;&#35782;&#30340;&#32771;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Similar case retrieval (SCR) is a representative legal AI application that plays a pivotal role in promoting judicial fairness. However, existing SCR datasets only focus on the fact description section when judging the similarity between cases, ignoring other valuable sections (e.g., the court's opinion) that can provide insightful reasoning process behind. Furthermore, the case similarities are typically measured solely by the textual semantics of the fact descriptions, which may fail to capture the full complexity of legal cases from the perspective of legal knowledge. In this work, we present MUSER, a similar case retrieval dataset based on multi-view similarity measurement and comprehensive legal element with sentence-level legal element annotations. Specifically, we select three perspectives (legal fact, dispute focus, and law statutory) and build a comprehensive and structured label schema of legal elements for each of them, to enable accurate and knowledgeable evaluation of case
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65288;RetriKT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#25552;&#21462;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992; retrieval-based &#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#24212;&#29992;&#20110;&#26497;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26497;&#31471;&#30340;&#27169;&#22411;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15594</link><description>&lt;p&gt;
&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#26497;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Retrieval-based Knowledge Transfer: An Effective Approach for Extreme Large Language Model Compression. (arXiv:2310.15594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15594
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65288;RetriKT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#33539;&#20363;&#65292;&#23427;&#36890;&#36807;&#25552;&#21462;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992; retrieval-based &#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#30693;&#35782;&#24212;&#29992;&#20110;&#26497;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26497;&#31471;&#30340;&#27169;&#22411;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24040;&#22823;&#35268;&#27169;&#32473;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#65292;&#20294;&#23545;&#20110;&#22312;&#27169;&#22411;&#35268;&#27169;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#26102;&#23454;&#29616;&#26497;&#31471;&#27169;&#22411;&#21387;&#32553;&#24182;&#19981;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21387;&#32553;&#33539;&#20363;&#65292;&#31216;&#20026;&#26816;&#32034;&#24335;&#30693;&#35782;&#36716;&#31227;&#65288;RetriKT&#65289;&#65292;&#23427;&#23558;LLM&#30340;&#30693;&#35782;&#26377;&#25928;&#22320;&#36716;&#31227;&#21040;&#26497;&#23567;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;1%&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;LLM&#20013;&#25552;&#21462;&#30693;&#35782;&#26500;&#24314;&#30693;&#35782;&#23384;&#20648;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#65292;&#21033;&#29992;&#23427;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36719;&#25552;&#31034;&#35843;&#25972;&#21644;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#22686;&#24378;&#23398;&#20064;&#25216;&#26415;&#12290;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained language models (LLMs) have demonstrated exceptional performance in various natural language processing (NLP) tasks. However, the massive size of these models poses huge challenges for their deployment in real-world applications. While numerous model compression techniques have been proposed, most of them are not well-suited for achieving extreme model compression when there is a significant gap in model scale. In this paper, we introduce a novel compression paradigm called Retrieval-based Knowledge Transfer (RetriKT), which effectively transfers the knowledge of LLMs to extremely small-scale models (e.g., 1%). In particular, our approach extracts knowledge from LLMs to construct a knowledge store, from which the small-scale model can retrieve relevant information and leverage it for effective inference. To improve the quality of the model, soft prompt tuning and Proximal Policy Optimization (PPO) reinforcement learning techniques are employed. Extensive experim
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#20013;&#21512;&#25104;&#25195;&#35270;&#36335;&#24452;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;ScanDL&#65289;&#65292;&#20197;&#35299;&#20915;&#30524;&#21160;&#25968;&#25454;&#31232;&#32570;&#21644;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15587</link><description>&lt;p&gt;
ScanDL: &#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#20013;&#21512;&#25104;&#25195;&#35270;&#36335;&#24452;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ScanDL: A Diffusion Model for Generating Synthetic Scanpaths on Texts. (arXiv:2310.15587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15587
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#20013;&#21512;&#25104;&#25195;&#35270;&#36335;&#24452;&#30340;&#25193;&#25955;&#27169;&#22411;&#65288;ScanDL&#65289;&#65292;&#20197;&#35299;&#20915;&#30524;&#21160;&#25968;&#25454;&#31232;&#32570;&#21644;&#19981;&#21487;&#29992;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38405;&#35835;&#20013;&#30340;&#30524;&#21160;&#22312;&#30740;&#31350;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#35748;&#30693;&#26426;&#21046;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#65292;&#30524;&#21160;&#21644;&#35748;&#30693;&#20043;&#38388;&#30340;&#32039;&#23494;&#32852;&#31995;&#20063;&#34987;&#29992;&#20110;&#35821;&#35328;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12289;&#22686;&#24378;&#21644;&#39044;&#35757;&#32451;&#65292;&#20197;&#21450;&#35835;&#32773;&#21644;&#25991;&#26412;&#29305;&#23450;&#23646;&#24615;&#30340;&#25512;&#26029;&#12290;&#28982;&#32780;&#65292;&#30524;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#22312;&#24212;&#29992;&#26102;&#30340;&#19981;&#21487;&#29992;&#24615;&#23545;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#21021;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#37319;&#29992;&#35748;&#30693;&#27169;&#22411;&#21512;&#25104;&#30524;&#21160;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#29983;&#25104;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#25195;&#35270;&#36335;&#24452;&#65292;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26356;&#20026;&#36866;&#29992;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#23558;&#25193;&#25955;&#36807;&#31243;&#36866;&#24212;&#20110;&#31163;&#25955;&#25968;&#25454;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ScanDL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#25955;&#24207;&#21015;&#21040;&#24207;&#21015;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#25195;&#35270;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eye movements in reading play a crucial role in psycholinguistic research studying the cognitive mechanisms underlying human language processing. More recently, the tight coupling between eye movements and cognition has also been leveraged for language-related machine learning tasks such as the interpretability, enhancement, and pre-training of language models, as well as the inference of reader- and text-specific properties. However, scarcity of eye movement data and its unavailability at application time poses a major challenge for this line of research. Initially, this problem was tackled by resorting to cognitive models for synthesizing eye movement data. However, for the sole purpose of generating human-like scanpaths, purely data-driven machine-learning-based methods have proven to be more suitable. Following recent advances in adapting diffusion processes to discrete data, we propose ScanDL, a novel discrete sequence-to-sequence diffusion model that generates synthetic scanpaths
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;(NMN)&#65292;&#24182;&#24341;&#20837;&#20102;&#35745;&#21010;&#30340;&#25945;&#24072;&#24341;&#23548;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35823;&#24046;&#31215;&#32047;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15585</link><description>&lt;p&gt;
&#25945;&#24072;&#24341;&#23548;&#30340;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Multimodal Representations for Teacher-Guided Compositional Visual Reasoning. (arXiv:2310.15585v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;&#24341;&#23548;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#26469;&#25913;&#36827;&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;(NMN)&#65292;&#24182;&#24341;&#20837;&#20102;&#35745;&#21010;&#30340;&#25945;&#24072;&#24341;&#23548;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20943;&#23569;&#35823;&#24046;&#31215;&#32047;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27169;&#22359;&#32593;&#32476;&#65288;NMN&#65289;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#35270;&#35273;&#38382;&#31572;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#38382;&#39064;&#32763;&#35793;&#20026;&#30001;&#19968;&#31995;&#21015;&#25512;&#29702;&#23376;&#20219;&#21153;&#32452;&#25104;&#30340;&#31243;&#24207;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#25353;&#39034;&#24207;&#22312;&#22270;&#20687;&#19978;&#25191;&#34892;&#20197;&#20135;&#29983;&#31572;&#26696;&#12290;&#19982;&#38598;&#25104;&#27169;&#22411;&#30456;&#27604;&#65292;NMN&#25552;&#20379;&#20102;&#26356;&#24378;&#30340;&#35299;&#37322;&#24615;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#24213;&#23618;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#25552;&#39640;NMN&#30340;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#35268;&#27169;&#36328;&#27169;&#24577;&#32534;&#30721;&#22120;&#33719;&#24471;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;NMN&#35757;&#32451;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#27169;&#22359;&#36755;&#20986;&#20256;&#25773;&#21040;&#21518;&#32493;&#27169;&#22359;&#65292;&#23548;&#33268;&#39044;&#27979;&#35823;&#24046;&#32047;&#31215;&#21644;&#35823;&#29983;&#25104;&#31572;&#26696;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;NMN&#23398;&#20064;&#31574;&#30053;&#65292;&#28041;&#21450;&#35745;&#21010;&#30340;&#25945;&#24072;&#24341;&#23548;&#12290;&#26368;&#21021;&#65292;&#27169;&#22411;&#23436;&#20840;&#30001;&#22320;&#38754;&#30495;&#23454;&#30340;&#20013;&#38388;&#36755;&#20986;&#24341;&#23548;&#65292;&#20294;&#38543;&#30528;&#35757;&#32451;&#30340;&#36827;&#34892;&#36880;&#28176;&#36807;&#28193;&#21040;&#33258;&#20027;&#34892;&#20026;&#12290;&#36825;&#20943;&#23569;&#20102;&#35823;&#24046;&#31215;&#32047;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Module Networks (NMN) are a compelling method for visual question answering, enabling the translation of a question into a program consisting of a series of reasoning sub-tasks that are sequentially executed on the image to produce an answer. NMNs provide enhanced explainability compared to integrated models, allowing for a better understanding of the underlying reasoning process. To improve the effectiveness of NMNs we propose to exploit features obtained by a large-scale cross-modal encoder. Also, the current training approach of NMNs relies on the propagation of module outputs to subsequent modules, leading to the accumulation of prediction errors and the generation of false answers. To mitigate this, we introduce an NMN learning strategy involving scheduled teacher guidance. Initially, the model is fully guided by the ground-truth intermediate outputs, but gradually transitions to an autonomous behavior as training progresses. This reduces error accumulation, thus improving 
&lt;/p&gt;</description></item><item><title>CONTRASTE&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ASTE&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20182;ABSA&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.15577</link><description>&lt;p&gt;
CONTRASTE: &#19968;&#31181;&#24102;&#26377;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#30340;&#30417;&#30563;&#23545;&#27604;&#39044;&#35757;&#32451;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
CONTRASTE: Supervised Contrastive Pre-training With Aspect-based Prompts For Aspect Sentiment Triplet Extraction. (arXiv:2310.15577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15577
&lt;/p&gt;
&lt;p&gt;
CONTRASTE&#26159;&#19968;&#31181;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#35774;&#35745;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#24182;&#24212;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ASTE&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20854;&#20182;ABSA&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25277;&#21462;&#65288;ASTE&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#24320;&#21457;&#26356;&#39640;&#25928;&#30340;&#24494;&#35843;&#25216;&#26415;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21160;&#26426;&#26159;&#25552;&#20986;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#25913;&#21892;&#22810;&#20010;ABSA&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONTRASTE&#30340;&#26032;&#22411;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#22686;&#24378;ASTE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#38500;&#20102;&#20027;&#35201;&#20851;&#27880;ASTE&#20043;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#22312;&#20854;&#20182;ABSA&#20219;&#21153;&#65288;&#22914;ACOS&#65292;TASD&#21644;AESC&#65289;&#19978;&#30340;&#20248;&#21183;&#12290;&#32473;&#23450;&#19968;&#20010;&#21477;&#23376;&#21450;&#20854;&#30456;&#20851;&#30340;&#65288;&#26041;&#38754;&#65292;&#35266;&#28857;&#65292;&#24773;&#24863;&#65289;&#19977;&#20803;&#32452;&#65292;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#25552;&#31034;&#65292;&#24182;&#23631;&#34109;&#20102;&#30456;&#24212;&#30340;&#24773;&#24863;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#35299;&#30721;&#22120;&#29983;&#25104;&#30340;&#26041;&#38754;&#24863;&#30693;&#24773;&#24863;&#34920;&#31034;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#65288;&#39044;&#65289;&#35757;&#32451;&#32534;&#30721;-&#35299;&#30721;&#27169;&#22411;&#12290;&#20026;&#20102;&#24494;&#35843;&#24471;&#21040;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26041;&#27861;&#65292;&#20854;&#20013;&#22522;&#30784;&#32534;&#30721;&#35299;&#30721;&#27169;&#22411;&#21516;&#26102;&#34987;&#29992;&#20110;ASTG&#21644;&#20854;&#20182;ABSA&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing works on Aspect Sentiment Triplet Extraction (ASTE) explicitly focus on developing more efficient fine-tuning techniques for the task. Instead, our motivation is to come up with a generic approach that can improve the downstream performances of multiple ABSA tasks simultaneously. Towards this, we present CONTRASTE, a novel pre-training strategy using CONTRastive learning to enhance the ASTE performance. While we primarily focus on ASTE, we also demonstrate the advantage of our proposed technique on other ABSA tasks such as ACOS, TASD, and AESC. Given a sentence and its associated (aspect, opinion, sentiment) triplets, first, we design aspect-based prompts with corresponding sentiments masked. We then (pre)train an encoder-decoder model by applying contrastive learning on the decoder-generated aspect-aware sentiment representations of the masked terms. For fine-tuning the model weights thus obtained, we then propose a novel multi-task approach where the base encoder-decoder mod
&lt;/p&gt;</description></item><item><title>POE&#26159;&#19968;&#31181;&#20004;&#27493;&#31574;&#30053;&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#38500;&#30475;&#20284;&#38169;&#35823;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#29305;&#21035;&#22909;&#65292;&#24182;&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#35774;&#32622;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.15575</link><description>&lt;p&gt;
POE: &#22810;&#39033;&#36873;&#25321;&#25512;&#29702;&#30340;&#25490;&#38500;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
POE: Process of Elimination for Multiple Choice Reasoning. (arXiv:2310.15575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15575
&lt;/p&gt;
&lt;p&gt;
POE&#26159;&#19968;&#31181;&#20004;&#27493;&#31574;&#30053;&#30340;&#35780;&#20998;&#26041;&#27861;&#65292;&#36890;&#36807;&#25490;&#38500;&#30475;&#20284;&#38169;&#35823;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#39033;&#36873;&#25321;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#35813;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#29305;&#21035;&#22909;&#65292;&#24182;&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#35774;&#32622;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#33021;&#22815;&#36827;&#34892;&#22810;&#39033;&#36873;&#25321;&#25512;&#29702;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#20294;&#26159;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#36873;&#39033;&#34987;&#24179;&#31561;&#23545;&#24453;&#12290;&#30001;&#20110;&#20154;&#31867;&#24448;&#24448;&#20250;&#22312;&#36873;&#25321;&#26368;&#32456;&#27491;&#30830;&#31572;&#26696;&#20043;&#21069;&#20808;&#25490;&#38500;&#38169;&#35823;&#30340;&#36873;&#39033;&#65292;&#25105;&#20204;&#35748;&#20026;&#31867;&#20284;&#30340;&#20004;&#27493;&#31574;&#30053;&#33021;&#22815;&#20351;LMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25490;&#38500;&#36807;&#31243;&#65288;POE&#65289;&#65292;&#19968;&#31181;&#20004;&#27493;&#35780;&#20998;&#26041;&#27861;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;POE&#35780;&#20998;&#27599;&#20010;&#36873;&#39033;&#65292;&#24182;&#25490;&#38500;&#30475;&#20284;&#38169;&#35823;&#30340;&#36873;&#39033;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;POE&#23631;&#34109;&#36825;&#20123;&#38169;&#35823;&#30340;&#36873;&#39033;&#65292;&#24182;&#20174;&#21097;&#20313;&#30340;&#36873;&#39033;&#20013;&#36827;&#34892;&#26368;&#32456;&#39044;&#27979;&#12290;&#23545;8&#20010;&#25512;&#29702;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#23454;&#39564;&#35777;&#26126;&#20102;POE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#38543;&#21518;&#30340;&#20998;&#26512;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#29305;&#21035;&#22909;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#23631;&#34109;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;POE&#36866;&#29992;&#20110;&#23569;&#26679;&#26412;&#35774;&#32622;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are capable of conducting in-context learning for multiple choice reasoning tasks, but the options in these tasks are treated equally. As humans often first eliminate wrong options before picking the final correct answer, we argue a similar two-step strategy can make LMs better at these tasks. To this end, we present the Process of Elimination (POE), a two-step scoring method. In the first step, POE scores each option, and eliminates seemingly wrong options. In the second step, POE masks these wrong options, and makes the final prediction from the remaining options. Zero-shot experiments on 8 reasoning tasks illustrate the effectiveness of POE, and a following analysis finds our method to be especially performant on logical reasoning tasks. We further analyze the effect of masks, and show that POE applies to few-shot settings and large language models (LLMs) like ChatGPT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#30340;&#20248;&#21183;&#21644;&#38519;&#38449;&#12290;NLP&#21487;&#20197;&#20174;&#22823;&#37327;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#25454;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;NLP-KG&#27969;&#31243;&#20013;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.15572</link><description>&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#20248;&#21183;&#19982;&#38519;&#38449;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing for Drug Discovery Knowledge Graphs: promises and pitfalls. (arXiv:2310.15572v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#30693;&#35782;&#22270;&#35889;&#30340;&#20248;&#21183;&#21644;&#38519;&#38449;&#12290;NLP&#21487;&#20197;&#20174;&#22823;&#37327;&#25991;&#26723;&#20013;&#33258;&#21160;&#25552;&#21462;&#25968;&#25454;&#65292;&#20026;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;NLP-KG&#27969;&#31243;&#20013;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#30340;&#38169;&#35823;&#35782;&#21035;&#21644;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#21644;&#20998;&#26512;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20197;&#36741;&#21161;&#33647;&#29289;&#21457;&#29616;&#26159;&#30740;&#31350;&#30340;&#19968;&#20010;&#28909;&#38376;&#39046;&#22495;&#12290;KG&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20854;&#33021;&#22815;&#20197;&#26377;&#21033;&#20110;&#21457;&#29616;&#20851;&#32852;&#30340;&#26684;&#24335;&#65292;&#32467;&#21512;&#35768;&#22810;&#24322;&#26500;&#25968;&#25454;&#28304;&#12290;KG&#30340;&#23454;&#29992;&#24615;&#24050;&#22312;&#33647;&#29289;&#37325;&#26032;&#20316;&#29992;&#31561;&#39046;&#22495;&#24471;&#21040;&#20307;&#29616;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#25163;&#21160;&#25506;&#32034;&#21644;&#24314;&#27169;&#26469;&#33719;&#24471;&#27934;&#35265;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#31185;&#23398;&#25991;&#29486;&#31561;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25366;&#25496;&#25968;&#25454;&#20316;&#20026;KG&#25968;&#25454;&#28304;&#30340;&#20248;&#21183;&#19982;&#38519;&#38449;&#12290;&#36825;&#26159;&#22522;&#20110;&#25105;&#20204;&#26368;&#21021;&#35299;&#26512;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;ChEMBL&#65289;&#20316;&#20026;KG&#25968;&#25454;&#30340;&#22522;&#30784;&#65292;&#24182;&#20511;&#21161;NLP&#36827;&#34892;&#20016;&#23500;&#25110;&#25193;&#23637;&#30340;&#32463;&#39564;&#12290;NLP&#23545;KG&#30340;&#22522;&#26412;&#20248;&#21183;&#22312;&#20110;&#33258;&#21160;&#25552;&#21462;&#26469;&#33258;&#25968;&#30334;&#19975;&#25991;&#26723;&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#20165;&#36890;&#36807;&#20154;&#24037;&#31574;&#23637;&#26080;&#27861;&#23454;&#29616;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;NLP-KG&#27969;&#31243;&#20013;&#23384;&#22312;&#35768;&#22810;&#28508;&#22312;&#38519;&#38449;&#65292;&#22914;&#38169;&#35823;&#30340;&#21629;&#21517;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building and analysing knowledge graphs (KGs) to aid drug discovery is a topical area of research. A salient feature of KGs is their ability to combine many heterogeneous data sources in a format that facilitates discovering connections. The utility of KGs has been exemplified in areas such as drug repurposing, with insights made through manual exploration and modelling of the data. In this article, we discuss promises and pitfalls of using natural language processing (NLP) to mine unstructured text typically from scientific literature as a data source for KGs. This draws on our experience of initially parsing structured data sources such as ChEMBL as the basis for data within a KG, and then enriching or expanding upon them using NLP. The fundamental promise of NLP for KGs is the automated extraction of data from millions of documents a task practically impossible to do via human curation alone. However, there are many potential pitfalls in NLP-KG pipelines such as incorrect named enti
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#23545;&#21487;&#35270;&#21270;&#22522;&#30784;&#30340;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#30340;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#20026;&#27169;&#22411;&#20998;&#26512;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#12290;&#35780;&#20272;&#20102;&#21508;&#31181;&#21551;&#21457;&#24335;&#30340;&#27169;&#22359;&#19987;&#19994;&#21270;&#31574;&#30053;&#20197;&#21450;...</title><link>http://arxiv.org/abs/2310.15571</link><description>&lt;p&gt;
&#21487;&#35270;&#21270;&#22522;&#30784;&#30340;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#19982;&#36873;&#25321;&#24615;&#19987;&#19994;&#21270;
&lt;/p&gt;
&lt;p&gt;
Visually Grounded Continual Language Learning with Selective Specialization. (arXiv:2310.15571v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15571
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#23545;&#21487;&#35270;&#21270;&#22522;&#30784;&#30340;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#30340;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#24191;&#27867;&#20998;&#26512;&#12290;&#36890;&#36807;&#24341;&#20837;&#20004;&#20010;&#26032;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#20316;&#32773;&#20026;&#27169;&#22411;&#20998;&#26512;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#12290;&#35780;&#20272;&#20102;&#21508;&#31181;&#21551;&#21457;&#24335;&#30340;&#27169;&#22359;&#19987;&#19994;&#21270;&#31574;&#30053;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#19990;&#30028;&#20013;&#34892;&#21160;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#22312;&#24179;&#34913;&#27599;&#39033;&#20219;&#21153;&#30340;&#20805;&#20998;&#19987;&#19994;&#21270;&#21644;&#26500;&#24314;&#19968;&#33324;&#21270;&#30693;&#35782;&#20197;&#36827;&#34892;&#20256;&#36882;&#30340;&#21516;&#26102;&#65292;&#25345;&#32493;&#23398;&#20064;&#19968;&#31995;&#21015;&#35821;&#35328;&#39537;&#21160;&#30340;&#20219;&#21153;&#12290;&#36873;&#25321;&#24615;&#19987;&#19994;&#21270;&#65292;&#21363;&#22312;&#27599;&#20010;&#20219;&#21153;&#20013;&#31934;&#24515;&#36873;&#25321;&#27169;&#22411;&#32452;&#20214;&#36827;&#34892;&#19987;&#19994;&#21270;&#65292;&#26159;&#25511;&#21046;&#36825;&#31181;&#26435;&#34913;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36873;&#25321;&#31574;&#30053;&#38656;&#35201;&#23545;&#27599;&#20010;&#27169;&#22411;&#32452;&#20214;&#22312;&#23398;&#20064;&#36739;&#20026;&#19987;&#19994;&#21270;&#25110;&#21487;&#26222;&#36941;&#21270;&#34920;&#31034;&#20013;&#30340;&#20316;&#29992;&#26377;&#28145;&#20837;&#30340;&#20102;&#35299;&#65292;&#32780;&#24403;&#21069;&#30740;&#31350;&#23384;&#22312;&#36825;&#26041;&#38754;&#30340;&#24046;&#36317;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23545;&#21487;&#35270;&#21270;&#22522;&#30784;&#30340;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#30340;&#36873;&#25321;&#31574;&#30053;&#36827;&#34892;&#24191;&#27867;&#30340;&#20998;&#26512;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#21512;&#36866;&#22522;&#20934;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#35786;&#26029;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#36275;&#22815;&#30340;&#25511;&#21046;&#21644;&#28789;&#27963;&#24615;&#36827;&#34892;&#24443;&#24213;&#30340;&#27169;&#22411;&#20998;&#26512;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21551;&#21457;&#24335;&#30340;&#27169;&#22359;&#19987;&#19994;&#21270;&#31574;&#30053;&#20197;&#21450;...
&lt;/p&gt;
&lt;p&gt;
A desirable trait of an artificial agent acting in the visual world is to continually learn a sequence of language-informed tasks while striking a balance between sufficiently specializing in each task and building a generalized knowledge for transfer. Selective specialization, i.e., a careful selection of model components to specialize in each task, is a strategy to provide control over this trade-off. However, the design of selection strategies requires insights on the role of each model component in learning rather specialized or generalizable representations, which poses a gap in current research. Thus, our aim with this work is to provide an extensive analysis of selection strategies for visually grounded continual language learning. Due to the lack of suitable benchmarks for this purpose, we introduce two novel diagnostic datasets that provide enough control and flexibility for a thorough model analysis. We assess various heuristics for module specialization strategies as well as
&lt;/p&gt;</description></item><item><title>MuLMS&#26159;&#19968;&#20010;&#22810;&#23618;&#27880;&#37322;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;50&#31687;&#24320;&#25918;&#33719;&#21462;&#30340;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#26448;&#26009;&#31185;&#23398;&#30340;&#19971;&#20010;&#23376;&#39046;&#22495;&#12290;&#35813;&#35821;&#26009;&#24211;&#34987;&#27880;&#37322;&#20102;&#22810;&#20010;&#23618;&#27425;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.15569</link><description>&lt;p&gt;
MuLMS&#65306;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#20449;&#24687;&#25552;&#21462;&#30340;&#22810;&#23618;&#27880;&#37322;&#25991;&#26412;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MuLMS: A Multi-Layer Annotated Text Corpus for Information Extraction in the Materials Science Domain. (arXiv:2310.15569v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15569
&lt;/p&gt;
&lt;p&gt;
MuLMS&#26159;&#19968;&#20010;&#22810;&#23618;&#27880;&#37322;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;50&#31687;&#24320;&#25918;&#33719;&#21462;&#30340;&#25991;&#31456;&#65292;&#28085;&#30422;&#20102;&#26448;&#26009;&#31185;&#23398;&#30340;&#19971;&#20010;&#23376;&#39046;&#22495;&#12290;&#35813;&#35821;&#26009;&#24211;&#34987;&#27880;&#37322;&#20102;&#22810;&#20010;&#23618;&#27425;&#30340;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#30740;&#31350;&#39046;&#22495;&#65292;&#36319;&#36394;&#25152;&#26377;&#30456;&#20851;&#30340;&#26368;&#26032;&#20986;&#29256;&#29289;&#21644;&#23454;&#39564;&#32467;&#26524;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#35777;&#26126;&#20102;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#21457;&#24067;&#20102;&#20960;&#20010;&#38024;&#23545;&#23578;&#26410;&#30740;&#31350;&#30340;&#26448;&#26009;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#35299;&#26512;&#21512;&#25104;&#31243;&#24207;&#31561;&#23376;&#38382;&#39064;&#25110;&#32773;&#20391;&#37325;&#20110;&#23376;&#39046;&#22495;&#65292;&#20363;&#22914;&#22266;&#20307;&#27687;&#21270;&#29289;&#29123;&#26009;&#30005;&#27744;&#12290;&#22312;&#36825;&#31687;&#36164;&#28304;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MuLMS&#65292;&#19968;&#20010;&#21253;&#21547;50&#31687;&#24320;&#25918;&#33719;&#21462;&#25991;&#31456;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26448;&#26009;&#31185;&#23398;&#30340;&#19971;&#20010;&#23376;&#39046;&#22495;&#12290;&#35813;&#35821;&#26009;&#24211;&#30001;&#39046;&#22495;&#19987;&#23478;&#36827;&#34892;&#20102;&#22810;&#20010;&#23618;&#27425;&#30340;&#27880;&#37322;&#65292;&#28085;&#30422;&#20174;&#21629;&#21517;&#23454;&#20307;&#21040;&#20851;&#31995;&#21040;&#26694;&#26550;&#32467;&#26500;&#30340;&#22810;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#25152;&#26377;&#20219;&#21153;&#30340;&#31454;&#20105;&#24615;&#31070;&#32463;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#21033;&#29992;&#29616;&#26377;&#30340;&#30456;&#20851;&#36164;&#28304;&#36827;&#34892;&#22810;&#20219;&#21153;&#35757;&#32451;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keeping track of all relevant recent publications and experimental results for a research area is a challenging task. Prior work has demonstrated the efficacy of information extraction models in various scientific areas. Recently, several datasets have been released for the yet understudied materials science domain. However, these datasets focus on sub-problems such as parsing synthesis procedures or on sub-domains, e.g., solid oxide fuel cells. In this resource paper, we present MuLMS, a new dataset of 50 open-access articles, spanning seven sub-domains of materials science. The corpus has been annotated by domain experts with several layers ranging from named entities over relations to frame structures. We present competitive neural models for all tasks and demonstrate that multi-task training with existing related resources leads to benefits.
&lt;/p&gt;</description></item><item><title>TCRA-LLM&#26159;&#36890;&#36807;&#27010;&#36848;&#21387;&#32553;&#21644;&#35821;&#20041;&#21387;&#32553;&#20004;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.15556</link><description>&lt;p&gt;
TCRA-LLM: &#29992;&#20110;&#20943;&#23569;&#25512;&#29702;&#25104;&#26412;&#30340;&#20196;&#29260;&#21387;&#32553;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TCRA-LLM: Token Compression Retrieval Augmented Large Language Model for Inference Cost Reduction. (arXiv:2310.15556v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15556
&lt;/p&gt;
&lt;p&gt;
TCRA-LLM&#26159;&#36890;&#36807;&#27010;&#36848;&#21387;&#32553;&#21644;&#35821;&#20041;&#21387;&#32553;&#20004;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;ChatGPT&#21457;&#24067;&#20102;API&#20379;&#20844;&#20247;&#20351;&#29992;&#20197;&#26469;&#65292;&#26500;&#24314;&#22312;&#21830;&#19994;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20043;&#19978;&#30340;&#24212;&#29992;&#31243;&#24207;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#19968;&#20010;&#27969;&#34892;&#29992;&#27861;&#26159;&#21033;&#29992;&#20854;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#24182;&#29983;&#25104;&#21709;&#24212;&#20197;&#22238;&#31572;&#29992;&#25143;&#26597;&#35810;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#22686;&#24378;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#37096;&#32626;&#21830;&#19994;&#26816;&#32034;&#22686;&#24378;&#22411;LLM&#30340;&#19968;&#20010;&#38382;&#39064;&#26159;&#25104;&#26412;&#65292;&#22240;&#20026;&#39069;&#22806;&#26816;&#32034;&#30340;&#19978;&#19979;&#25991;&#22823;&#22823;&#22686;&#21152;&#20102;LLM&#30340;&#36755;&#20837;&#26631;&#35760;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20196;&#29260;&#21387;&#32553;&#26041;&#26696;&#65292;&#21253;&#25324;&#20004;&#31181;&#26041;&#27861;&#65306;&#27010;&#36848;&#21387;&#32553;&#21644;&#35821;&#20041;&#21387;&#32553;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;T5&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#26679;&#26412;&#30340;&#33258;&#25351;&#31034;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#36890;&#36807;&#27010;&#36848;&#26469;&#20943;&#23569;&#20196;&#29260;&#22823;&#23567;&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#36890;&#36807;&#31227;&#38500;&#23545;&#35821;&#20041;&#24433;&#21709;&#36739;&#23567;&#30340;&#35789;&#26469;&#36827;&#19968;&#27493;&#21387;&#32553;&#20196;&#29260;&#22823;&#23567;&#12290;&#20026;&#20102;&#20805;&#20998;&#35780;&#20272;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;
&lt;/p&gt;
&lt;p&gt;
Since ChatGPT released its API for public use, the number of applications built on top of commercial large language models (LLMs) increase exponentially. One popular usage of such models is leveraging its in-context learning ability and generating responses given user queries leveraging knowledge obtained by retrieval augmentation. One problem of deploying commercial retrieval-augmented LLMs is the cost due to the additionally retrieved context that largely increases the input token size of the LLMs. To mitigate this, we propose a token compression scheme that includes two methods: summarization compression and semantic compression. The first method applies a T5-based model that is fine-tuned by datasets generated using self-instruct containing samples with varying lengths and reduce token size by doing summarization. The second method further compresses the token size by removing words with lower impact on the semantic. In order to adequately evaluate the effectiveness of the proposed
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;Transformer&#27169;&#22411;&#20013;&#65292;&#21069;&#39304;&#27169;&#22359;&#21487;&#20197;&#34987;&#35270;&#20026;&#38190;&#20540;&#35760;&#24518;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#35821;&#35328;&#30340;&#27169;&#24335;&#65292;&#24182;&#32467;&#21512;&#20849;&#20139;&#29305;&#24449;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.15552</link><description>&lt;p&gt;
&#25581;&#31034;Transformer&#27169;&#22411;&#20013;&#30340;&#22810;&#35821;&#35328;&#24615;&#65306;&#25506;&#32034;&#21069;&#39304;&#32593;&#32476;&#20013;&#30340;&#35821;&#35328;&#29305;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unveiling Multilinguality in Transformer Models: Exploring Language Specificity in Feed-Forward Networks. (arXiv:2310.15552v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15552
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25581;&#31034;&#20102;&#22312;Transformer&#27169;&#22411;&#20013;&#65292;&#21069;&#39304;&#27169;&#22359;&#21487;&#20197;&#34987;&#35270;&#20026;&#38190;&#20540;&#35760;&#24518;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#23398;&#20064;&#29305;&#23450;&#35821;&#35328;&#30340;&#27169;&#24335;&#65292;&#24182;&#32467;&#21512;&#20849;&#20139;&#29305;&#24449;&#65292;&#23454;&#29616;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#20013;&#30340;&#21069;&#39304;&#27169;&#22359;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#32452;&#38190;&#20540;&#35760;&#24518;&#65292;&#20854;&#20013;&#38190;&#36890;&#36807;&#35757;&#32451;&#26679;&#26412;&#23398;&#20064;&#25429;&#25417;&#36755;&#20837;&#20013;&#30340;&#29305;&#23450;&#27169;&#24335;&#12290;&#20540;&#23558;&#38190;&#30340;&#8220;&#35760;&#24518;&#8221;&#30340;&#36755;&#20986;&#36827;&#34892;&#32452;&#21512;&#65292;&#29983;&#25104;&#20851;&#20110;&#19979;&#19968;&#20010;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#36825;&#23548;&#33268;&#19968;&#31181;&#36882;&#22686;&#30340;&#39044;&#27979;&#36807;&#31243;&#65292;&#36880;&#28176;&#25910;&#25947;&#20110;&#38752;&#36817;&#36755;&#20986;&#23618;&#30340;&#26368;&#32456;&#26631;&#35760;&#36873;&#25321;&#12290;&#36825;&#20010;&#26377;&#36259;&#30340;&#35266;&#28857;&#24341;&#21457;&#20102;&#23545;&#22810;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#26426;&#21046;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;&#22312;&#20004;&#31181;&#25110;&#26356;&#22810;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#25152;&#26377;&#31070;&#32463;&#20803;&#65288;&#22312;&#19981;&#21516;&#23618;&#19978;&#65289;&#26159;&#21542;&#37117;&#23545;&#25152;&#26377;&#35821;&#35328;&#20316;&#20986;&#30456;&#21516;&#30340;&#21709;&#24212;&#65311;&#19981;&#26159;&#65281;&#25105;&#20204;&#30340;&#20551;&#35774;&#38598;&#20013;&#22312;&#36825;&#26679;&#19968;&#20010;&#35266;&#24565;&#19978;&#65306;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#65292;&#26576;&#20123;&#27169;&#22411;&#21442;&#25968;&#23398;&#20064;&#20102;&#24378;&#28872;&#30340;&#35821;&#35328;&#29305;&#23450;&#29305;&#24449;&#65292;&#32780;&#20854;&#20182;&#21442;&#25968;&#23398;&#20064;&#20102;&#26356;&#22810;&#30340;&#35821;&#35328;&#26080;&#20851;&#29305;&#24449;&#65288;&#20849;&#20139;&#20110;&#22810;&#31181;&#35821;&#35328;&#65289;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#21033;&#29992;&#24179;&#34892;&#35821;&#26009;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research suggests that the feed-forward module within Transformers can be viewed as a collection of key-value memories, where the keys learn to capture specific patterns from the input based on the training examples. The values then combine the output from the 'memories' of the keys to generate predictions about the next token. This leads to an incremental process of prediction that gradually converges towards the final token choice near the output layers. This interesting perspective raises questions about how multilingual models might leverage this mechanism. Specifically, for autoregressive models trained on two or more languages, do all neurons (across layers) respond equally to all languages? No! Our hypothesis centers around the notion that during pretraining, certain model parameters learn strong language-specific features, while others learn more language-agnostic (shared across languages) features. To validate this, we conduct experiments utilizing parallel corpora of t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23383;&#20856;&#20013;&#23398;&#20064;&#27010;&#24565;&#35282;&#33394;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#20041;&#35748;&#30693;&#65292;&#20197;&#32531;&#35299;&#20854;&#29983;&#25104;&#19981;&#19968;&#33268;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15541</link><description>&lt;p&gt;
&#36890;&#36807;&#20174;&#23383;&#20856;&#23398;&#20064;&#27010;&#24565;&#35282;&#33394;&#65292;&#25913;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Language Models Meaning Understanding and Consistency by Learning Conceptual Roles from Dictionary. (arXiv:2310.15541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15541
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23383;&#20856;&#20013;&#23398;&#20064;&#27010;&#24565;&#35282;&#33394;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#24847;&#20041;&#35748;&#30693;&#65292;&#20197;&#32531;&#35299;&#20854;&#29983;&#25104;&#19981;&#19968;&#33268;&#39044;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#38750;&#20154;&#31867;&#34892;&#20026;&#26159;&#24433;&#21709;&#20854;&#21487;&#20449;&#24230;&#30340;&#20027;&#35201;&#21407;&#22240;&#12290;&#36825;&#31181;&#38169;&#35823;&#34892;&#20026;&#30340;&#19968;&#20010;&#26174;&#33879;&#29616;&#35937;&#26159;&#29983;&#25104;&#19981;&#19968;&#33268;&#30340;&#39044;&#27979;&#65292;&#23548;&#33268;&#36923;&#36753;&#19978;&#30683;&#30462;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#20026;&#20256;&#36798;&#30456;&#21516;&#24847;&#20041;&#30340;&#25991;&#26412;&#29983;&#25104;&#19981;&#21516;&#30340;&#39044;&#27979;&#25110;&#36829;&#21453;&#36923;&#36753;&#24615;&#36136;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#25110;&#23454;&#26045;&#19987;&#38376;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#21463;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#28040;&#32791;&#20102;&#22823;&#35268;&#27169;PLMs&#30340;&#26114;&#36149;&#35757;&#32451;&#36164;&#28304;&#65292;&#24182;&#19988;&#21482;&#33021;&#22788;&#29702;&#19968;&#23450;&#31867;&#22411;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#23383;&#20856;&#20013;&#30340;&#35789;-&#23450;&#20041;&#23545;&#20013;&#23398;&#20064;&#27010;&#24565;&#35282;&#33394;&#30340;&#20934;&#30830;&#30456;&#20114;&#20851;&#31995;&#65292;&#20174;&#26681;&#26412;&#19978;&#25913;&#21892;PLMs&#30340;&#24847;&#20041;&#35748;&#30693;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#19981;&#19968;&#33268;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The non-humanlike behaviour of contemporary pre-trained language models (PLMs) is a leading cause undermining their trustworthiness. A striking phenomenon of such faulty behaviours is the generation of inconsistent predictions, which produces logically contradictory results, such as generating different predictions for texts delivering the same meaning or violating logical properties. Previous studies exploited data augmentation or implemented specialised loss functions to alleviate the issue. However, their usage is limited, because they consume expensive training resources for large-sized PLMs and can only handle a certain consistency type. To this end, we propose a practical approach that alleviates the inconsistent behaviour issue by fundamentally improving PLMs' meaning awareness. Based on the conceptual role theory, our method allows PLMs to capture accurate meaning by learning precise interrelationships between concepts from word-definition pairs in a dictionary. Next, we propos
&lt;/p&gt;</description></item><item><title>SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2310.15539</link><description>&lt;p&gt;
SteloCoder:&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#29992;&#20110;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
SteloCoder: a Decoder-Only LLM for Multi-Language to Python Code Translation. (arXiv:2310.15539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15539
&lt;/p&gt;
&lt;p&gt;
SteloCoder&#26159;&#19968;&#20010;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#22312;&#22810;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#23427;&#37319;&#29992;Mixture-of-Experts&#65288;MoE&#65289;&#25216;&#26415;&#21644;&#38376;&#25511;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#19987;&#23478;&#65292;&#24182;&#20351;&#29992;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;LoRA&#65289;&#25216;&#26415;&#26469;&#38480;&#21046;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#27880;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;StarCoder&#21644;Code Llama&#20998;&#21035;&#23637;&#31034;&#20102;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#20195;&#30721;&#32763;&#35793;&#21151;&#33021;&#19978;&#20173;&#28982;&#38656;&#35201;&#25913;&#36827;&#21644;&#26377;&#25928;&#35757;&#32451;&#25216;&#26415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SteloCoder&#65292;&#19968;&#31181;&#20165;&#35299;&#30721;&#30340;&#22522;&#20110;StarCoder&#30340;LLM&#65292;&#19987;&#20026;&#22810;&#32534;&#31243;&#35821;&#35328;&#21040;Python&#20195;&#30721;&#32763;&#35793;&#32780;&#35774;&#35745;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SteloCoder&#23454;&#29616;&#20102;C ++&#65292;C&#65283;&#65292;JavaScript&#65292;Java&#25110;PHP&#21040;Python&#20195;&#30721;&#32763;&#35793;&#65292;&#32780;&#26080;&#38656;&#25351;&#23450;&#36755;&#20837;&#32534;&#31243;&#35821;&#35328;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#32452;&#28151;&#21512;&#65288;Mixture-of-Experts&#65292;MoE&#65289;&#25216;&#26415;&#21644;&#19968;&#20010;&#25511;&#21046;&#22810;&#20219;&#21153;&#30340;&#38376;&#25511;&#32593;&#32476;&#26469;&#20462;&#25913;StarCoder&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;StarCoder&#36827;&#34892;&#24494;&#35843;&#26469;&#33719;&#24471;&#19987;&#23478;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20302;&#31209;&#33258;&#36866;&#24212;&#26041;&#27861;&#65288;Low-Rank Adaptive Method&#65292;LoRA&#65289;&#25216;&#26415;&#65292;&#23558;&#27599;&#20010;&#19987;&#23478;&#30340;&#22823;&#23567;&#38480;&#21046;&#20026;StarCoder&#21442;&#25968;&#25968;&#37327;&#30340;&#20165;0.06&#65285;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#22686;&#24378;tr
&lt;/p&gt;
&lt;p&gt;
With the recent focus on Large Language Models (LLMs), both StarCoder (Li et al., 2023) and Code Llama (Rozi\`ere et al., 2023) have demonstrated remarkable performance in code generation. However, there is still a need for improvement in code translation functionality with efficient training techniques. In response to this, we introduce SteloCoder, a decoder-only StarCoder-based LLM designed specifically for multi-programming language-to-Python code translation. In particular, SteloCoder achieves C++, C#, JavaScript, Java, or PHP-to-Python code translation without specifying the input programming language. We modified StarCoder model architecture by incorporating a Mixture-of-Experts (MoE) technique featuring five experts and a gating network for multi-task handling. Experts are obtained by StarCoder fine-tuning. Specifically, we use a Low-Rank Adaptive Method (LoRA) technique, limiting each expert size as only 0.06% of number of StarCoder's parameters. At the same time, to enhance tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#25968;&#20540;&#25512;&#29702;&#30340;&#22823;&#35268;&#27169;KBQA&#25968;&#25454;&#38598;MarkQA&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;NR-KBQA&#65292;&#35813;&#20219;&#21153;&#35201;&#27714;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#21644;&#25968;&#20540;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KBQA&#20013;&#30340;&#22797;&#26434;&#25968;&#20540;&#25512;&#29702;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15517</link><description>&lt;p&gt;
MarkQA: &#19968;&#20010;&#21253;&#21547;&#25968;&#20540;&#25512;&#29702;&#30340;&#22823;&#35268;&#27169;KBQA&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MarkQA: A large scale KBQA dataset with numerical reasoning. (arXiv:2310.15517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#25968;&#20540;&#25512;&#29702;&#30340;&#22823;&#35268;&#27169;KBQA&#25968;&#25454;&#38598;MarkQA&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;NR-KBQA&#65292;&#35813;&#20219;&#21153;&#35201;&#27714;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#21644;&#25968;&#20540;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KBQA&#20013;&#30340;&#22797;&#26434;&#25968;&#20540;&#25512;&#29702;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#22312;&#35299;&#20915;&#20107;&#23454;&#22411;&#38382;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#26159;&#28041;&#21450;&#25968;&#20540;&#25512;&#29702;&#30340;KBQA&#30456;&#23545;&#36739;&#23569;&#30740;&#31350;&#12290;&#26412;&#25991;&#38024;&#23545;KBQA&#20013;&#22797;&#26434;&#30340;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;NR-KBQA&#65292;&#23427;&#38656;&#35201;&#36827;&#34892;&#22810;&#36339;&#25512;&#29702;&#21644;&#25968;&#20540;&#25512;&#29702;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20197;Python&#26684;&#24335;&#30340;&#36923;&#36753;&#24418;&#24335;PyQL&#26469;&#34920;&#31034;&#25968;&#20540;&#25512;&#29702;&#38382;&#39064;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#20415;&#20110;NR-KBQA&#30340;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;MarkQA&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#19968;&#23567;&#32452;&#31181;&#23376;&#33258;&#21160;&#26500;&#24314;&#30340;&#12290;MarkQA&#20013;&#30340;&#27599;&#20010;&#38382;&#39064;&#37117;&#37197;&#22791;&#20102;&#19982;&#20043;&#23545;&#24212;&#30340;SPARQL&#26597;&#35810;&#65292;&#20197;&#21450;&#20197;QDMR&#26684;&#24335;&#21644;PyQL&#31243;&#24207;&#34920;&#31034;&#30340;&#36880;&#27493;&#25512;&#29702;&#36807;&#31243;&#12290;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;QA&#26041;&#27861;&#22312;MarkQA&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KBQA&#20013;&#30340;&#22797;&#26434;&#25968;&#20540;&#25512;&#29702;&#38754;&#20020;&#30528;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
While question answering over knowledge bases (KBQA) has shown progress in addressing factoid questions, KBQA with numerical reasoning remains relatively unexplored. In this paper, we focus on the complex numerical reasoning in KBQA and propose a new task, NR-KBQA, which necessitates the ability to perform both multi-hop reasoning and numerical reasoning. We design a logic form in Python format called PyQL to represent the reasoning process of numerical reasoning questions. To facilitate the development of NR-KBQA, we present a large dataset called MarkQA, which is automatically constructed from a small set of seeds. Each question in MarkQA is equipped with its corresponding SPARQL query, alongside the step-by-step reasoning process in the QDMR format and PyQL program. Experimental results of some state-of-the-art QA methods on the MarkQA show that complex numerical reasoning in KBQA faces great challenges.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#21363;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#12290;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#21644;&#27450;&#39575;&#24615;&#30340;&#20869;&#23481;&#20197;&#21450;&#21033;&#29992;&#35821;&#22659;&#35821;&#20041;&#25512;&#29702;&#25216;&#26415;&#65292;&#35813;&#31574;&#30053;&#22312;&#26816;&#27979;&#21644;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15515</link><description>&lt;p&gt;
&#29992;&#28779;&#25915;&#28779;&#65306;LLM&#22312;&#21046;&#20316;&#21644;&#26816;&#27979;&#38544;&#34109;&#34394;&#20551;&#20449;&#24687;&#20013;&#30340;&#21452;&#37325;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Fighting Fire with Fire: The Dual Role of LLMs in Crafting and Detecting Elusive Disinformation. (arXiv:2310.15515v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15515
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#21363;&#21033;&#29992;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#21644;&#25512;&#29702;&#33021;&#21147;&#26469;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#12290;&#36890;&#36807;&#21512;&#25104;&#30495;&#23454;&#21644;&#27450;&#39575;&#24615;&#30340;&#20869;&#23481;&#20197;&#21450;&#21033;&#29992;&#35821;&#22659;&#35821;&#20041;&#25512;&#29702;&#25216;&#26415;&#65292;&#35813;&#31574;&#30053;&#22312;&#26816;&#27979;&#21644;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#26041;&#38754;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#21644;&#30772;&#22351;&#24615;&#24433;&#21709;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#20854;&#28508;&#22312;&#34987;&#28389;&#29992;&#30340;&#25285;&#24551;&#65288;&#21363;&#29983;&#25104;&#22823;&#35268;&#27169;&#26377;&#23475;&#21644;&#35823;&#23548;&#24615;&#20869;&#23481;&#65289;&#12290;&#20026;&#20102;&#24212;&#23545;LLM&#30340;&#36825;&#19968;&#26032;&#20852;&#39118;&#38505;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#20197;&#28779;&#25915;&#28779;&#8221;&#65288;F3&#65289;&#31574;&#30053;&#65292;&#21033;&#29992;&#29616;&#20195;LLM&#30340;&#29983;&#25104;&#21644;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#26469;&#23545;&#25239;&#20154;&#31867;&#25776;&#20889;&#21644;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;GPT-3.5-turbo&#36890;&#36807;&#22522;&#20110;&#37322;&#20041;&#21644;&#25200;&#21160;&#30340;&#21069;&#32512;&#24335;&#25552;&#31034;&#21512;&#25104;&#30495;&#23454;&#21644;&#27450;&#39575;&#24615;&#30340;LLM&#29983;&#25104;&#20869;&#23481;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24212;&#29992;&#38646;-shot&#35821;&#22659;&#35821;&#20041;&#25512;&#29702;&#25216;&#26415;&#65292;&#36890;&#36807;&#22635;&#31354;&#24335;&#25552;&#31034;&#21306;&#20998;&#30495;&#23454;&#21644;&#34394;&#20551;&#30340;&#24086;&#23376;&#21644;&#26032;&#38395;&#25991;&#31456;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GPT-3.5-turbo&#22312;&#20998;&#24067;&#21644;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#26041;&#38754;&#20855;&#26377;&#38646;-shot&#19978;&#30340;&#20248;&#21183;&#65292;&#20854;&#20013;GPT-3.5-turbo&#22987;&#32456;&#20445;&#25345;&#22312;68-72%&#30340;&#20934;&#30830;&#29575;&#65292;&#19981;&#20687;&#20043;&#21069;&#30340;&#20010;&#24615;&#21270;&#19979;&#38477;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent ubiquity and disruptive impacts of large language models (LLMs) have raised concerns about their potential to be misused (.i.e, generating large-scale harmful and misleading content). To combat this emerging risk of LLMs, we propose a novel "Fighting Fire with Fire" (F3) strategy that harnesses modern LLMs' generative and emergent reasoning capabilities to counter human-written and LLM-generated disinformation. First, we leverage GPT-3.5-turbo to synthesize authentic and deceptive LLM-generated content through paraphrase-based and perturbation-based prefix-style prompts, respectively. Second, we apply zero-shot in-context semantic reasoning techniques with cloze-style prompts to discern genuine from deceptive posts and news articles. In our extensive experiments, we observe GPT-3.5-turbo's zero-shot superiority for both in-distribution and out-of-distribution datasets, where GPT-3.5-turbo consistently achieved accuracy at 68-72%, unlike the decline observed in previous customize
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#30697;&#38453;&#20998;&#35299;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#23545;&#20110;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24418;&#24577;&#21477;&#27861;&#29305;&#24449;&#30340;&#21576;&#29616;&#31243;&#24230;&#21644;&#26041;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#32534;&#30721;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#30340;&#21464;&#21270;&#21644;&#21463;&#35821;&#35328;&#23646;&#24615;&#24433;&#21709;&#30340;&#29305;&#23450;&#31867;&#21035;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;&#22240;&#24335;&#20998;&#35299;&#36755;&#20986;&#19982;&#36328;&#35821;&#35328;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#12290;&#25105;&#20204;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2310.15513</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#34920;&#24449;&#30340;&#32852;&#21512;&#30697;&#38453;&#20998;&#35299;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Joint Matrix Factorization Analysis of Multilingual Representations. (arXiv:2310.15513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#30697;&#38453;&#20998;&#35299;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#23545;&#20110;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24418;&#24577;&#21477;&#27861;&#29305;&#24449;&#30340;&#21576;&#29616;&#31243;&#24230;&#21644;&#26041;&#24335;&#65292;&#24182;&#21457;&#29616;&#20102;&#32534;&#30721;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#30340;&#21464;&#21270;&#21644;&#21463;&#35821;&#35328;&#23646;&#24615;&#24433;&#21709;&#30340;&#29305;&#23450;&#31867;&#21035;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#36824;&#23637;&#31034;&#20102;&#22240;&#24335;&#20998;&#35299;&#36755;&#20986;&#19982;&#36328;&#35821;&#35328;&#20219;&#21153;&#24615;&#33021;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#12290;&#25105;&#20204;&#36824;&#20844;&#24320;&#21457;&#24067;&#20102;&#30456;&#20851;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#21512;&#30697;&#38453;&#20998;&#35299;&#30340;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#20316;&#20026;&#23545;&#25506;&#27979;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#35813;&#24037;&#20855;&#20801;&#35768;&#25105;&#20204;&#20197;&#32852;&#21512;&#30340;&#26041;&#24335;&#20998;&#26512;&#22810;&#32452;&#34920;&#31034;&#12290;&#20351;&#29992;&#35813;&#24037;&#20855;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#20013;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20197;&#21450;&#22914;&#20309;&#21453;&#26144;&#20102;&#24418;&#24577;&#21477;&#27861;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;33&#31181;&#35821;&#35328;&#21644;17&#31181;&#24418;&#24577;&#21477;&#27861;&#31867;&#21035;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#19978;&#23618;&#21644;&#19979;&#23618;&#20043;&#38388;&#32534;&#30721;&#24418;&#24577;&#21477;&#27861;&#20449;&#24687;&#30340;&#26041;&#24335;&#23384;&#22312;&#21464;&#21270;&#65292;&#32780;&#19988;&#21463;&#35821;&#35328;&#23646;&#24615;&#24433;&#21709;&#30340;&#29305;&#23450;&#31867;&#21035;&#24046;&#24322;&#12290;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#36755;&#20986;&#30340;&#23618;&#27425;&#32858;&#31867;&#32467;&#26524;&#21487;&#20197;&#29983;&#25104;&#19982;&#35821;&#35328;&#23398;&#23478;&#25163;&#24037;&#21046;&#20316;&#30340;&#31995;&#32479;&#21457;&#29983;&#23398;&#26641;&#30456;&#20851;&#30340;&#26641;&#29366;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22240;&#24335;&#20998;&#35299;&#36755;&#20986;&#19982;&#36328;&#35821;&#35328;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#24378;&#20851;&#32852;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#20197;&#20415;&#30740;&#31350;&#31038;&#21306;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an analysis tool based on joint matrix factorization for comparing latent representations of multilingual and monolingual models. An alternative to probing, this tool allows us to analyze multiple sets of representations in a joint manner. Using this tool, we study to what extent and how morphosyntactic features are reflected in the representations learned by multilingual pre-trained models. We conduct a large-scale empirical study of over 33 languages and 17 morphosyntactic categories. Our findings demonstrate variations in the encoding of morphosyntactic information across upper and lower layers, with category-specific differences influenced by language properties. Hierarchical clustering of the factorization outputs yields a tree structure that is related to phylogenetic trees manually crafted by linguists. Moreover, we find the factorization outputs exhibit strong associations with performance observed across different cross-lingual tasks. We release our code to facilita
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;KITAB&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15511</link><description>&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#35780;&#20272;&#22522;&#20110;&#32422;&#26463;&#28385;&#36275;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
KITAB: Evaluating LLMs on Constraint Satisfaction for Information Retrieval. (arXiv:2310.15511v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#30340;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;KITAB&#26469;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#30340;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#22238;&#31572;&#32422;&#26463;&#28385;&#36275;&#26597;&#35810;&#65288;&#20363;&#22914;&#65292;&#8220;&#22307;&#22320;&#20122;&#21733;&#30340;&#20912;&#28103;&#28107;&#24215;&#21015;&#34920;&#8221;&#65289;&#30340;&#33021;&#21147;&#12290;&#36807;&#21435;&#65292;&#36825;&#26679;&#30340;&#26597;&#35810;&#34987;&#35748;&#20026;&#21482;&#33021;&#36890;&#36807;&#32593;&#32476;&#25628;&#32034;&#25110;&#30693;&#35782;&#24211;&#26469;&#35299;&#20915;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21021;&#27493;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#26816;&#32034;&#22522;&#20934;&#35201;&#20040;&#24050;&#39281;&#21644;&#65292;&#35201;&#20040;&#19981;&#33021;&#34913;&#37327;&#32422;&#26463;&#28385;&#36275;&#12290;&#21463;&#21040;&#23545;LLMs&#20107;&#23454;&#19981;&#27491;&#30830;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#26085;&#30410;&#20851;&#27880;&#30340;&#39537;&#21160;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KITAB&#65292;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#32422;&#26463;&#28385;&#36275;&#33021;&#21147;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;KITAB&#21253;&#21547;600&#22810;&#20301;&#20316;&#32773;&#21644;13,000&#20010;&#26597;&#35810;&#30340;&#19982;&#20070;&#31821;&#30456;&#20851;&#30340;&#25968;&#25454;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#32852;&#30340;&#21160;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#32422;&#26463;&#39564;&#35777;&#26041;&#27861;&#65292;&#20197;&#33719;&#24471;&#20854;&#20182;&#20316;&#32773;&#30340;&#31867;&#20284;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;GPT4&#21644;GPT3.5&#36827;&#34892;&#20102;&#25193;&#23637;&#23454;&#39564;&#65292;&#23545;&#24120;&#35265;&#30340;&#22833;&#36133;&#27169;&#24335;&#36827;&#34892;&#20102;&#34920;&#24449;&#21644;&#35299;&#32806;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the ability of state-of-the art models to answer constraint satisfaction queries for information retrieval (e.g., 'a list of ice cream shops in San Diego'). In the past, such queries were considered to be tasks that could only be solved via web-search or knowledge bases. More recently, large language models (LLMs) have demonstrated initial emergent abilities in this task. However, many current retrieval benchmarks are either saturated or do not measure constraint satisfaction. Motivated by rising concerns around factual incorrectness and hallucinations of LLMs, we present KITAB, a new dataset for measuring constraint satisfaction abilities of language models. KITAB consists of book-related data across more than 600 authors and 13,000 queries, and also offers an associated dynamic data collection and constraint verification approach for acquiring similar test data for other authors. Our extended experiments on GPT4 and GPT3.5 characterize and decouple common failure modes acros
&lt;/p&gt;</description></item><item><title>TRAMS&#26159;&#19968;&#31181;&#35757;&#32451;&#20813;&#36153;&#30340;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#35760;&#24518;&#36873;&#25321;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;Transformer&#26550;&#26500;&#22312;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.15494</link><description>&lt;p&gt;
TRAMS:&#35757;&#32451;&#20813;&#36153;&#30340;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#35760;&#24518;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
TRAMS: Training-free Memory Selection for Long-range Language Modeling. (arXiv:2310.15494v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15494
&lt;/p&gt;
&lt;p&gt;
TRAMS&#26159;&#19968;&#31181;&#35757;&#32451;&#20813;&#36153;&#30340;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#35760;&#24518;&#36873;&#25321;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#25552;&#39640;Transformer&#26550;&#26500;&#22312;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#23545;&#20110;&#20247;&#22810;AI&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22312;&#38271;&#31243;&#35821;&#35328;&#24314;&#27169;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#32463;&#35774;&#35745;&#20102;&#20960;&#31181;&#29305;&#23450;&#30340;Transformer&#26550;&#26500;&#26469;&#35299;&#20915;&#38271;&#31243;&#20381;&#36182;&#30340;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#26041;&#27861;&#22914;Transformer-XL&#23384;&#22312;&#22823;&#37327;&#26080;&#25928;&#35760;&#24518;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;TRAining-free Memory Selection&#65288;TRAMS&#65289;&#65292;&#23427;&#26681;&#25454;&#19968;&#20010;&#31616;&#21333;&#30340;&#25351;&#26631;&#36873;&#25321;&#21442;&#19982;&#27880;&#24847;&#21147;&#35745;&#31639;&#30340;&#26631;&#35760;&#12290;&#35813;&#31574;&#30053;&#20801;&#35768;&#25105;&#20204;&#20445;&#30041;&#19982;&#24403;&#21069;&#26597;&#35810;&#20855;&#26377;&#39640;&#20851;&#27880;&#20998;&#25968;&#21487;&#33021;&#24615;&#30340;&#26631;&#35760;&#65292;&#24182;&#24573;&#30053;&#20854;&#20182;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#21333;&#35789;&#32423;&#22522;&#20934;&#65288;WikiText-103&#65289;&#21644;&#23383;&#31526;&#32423;&#22522;&#20934;&#65288;enwik8&#65289;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#19981;&#36827;&#34892;&#39069;&#22806;&#35757;&#32451;&#25110;&#28155;&#21152;&#39069;&#22806;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.
&lt;/p&gt;</description></item><item><title>NuTrea&#26159;&#19968;&#20010;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;GNN&#27169;&#22411;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#24341;&#23548;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#38382;&#31572;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#22686;&#24378;&#36807;&#21435;&#23548;&#21521;&#30340;&#23884;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;RF-IEF&#33410;&#28857;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#27169;&#31946;&#30340;&#30693;&#35782;&#22270;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.15484</link><description>&lt;p&gt;
NuTrea&#65306;&#29992;&#20110;&#19978;&#19979;&#25991;&#24341;&#23548;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#38382;&#31572;&#30340;&#31070;&#32463;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NuTrea: Neural Tree Search for Context-guided Multi-hop KGQA. (arXiv:2310.15484v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15484
&lt;/p&gt;
&lt;p&gt;
NuTrea&#26159;&#19968;&#20010;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;GNN&#27169;&#22411;&#65292;&#29992;&#20110;&#19978;&#19979;&#25991;&#24341;&#23548;&#30340;&#22810;&#36339;&#30693;&#35782;&#22270;&#38382;&#31572;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#22686;&#24378;&#36807;&#21435;&#23548;&#21521;&#30340;&#23884;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;RF-IEF&#33410;&#28857;&#23884;&#20837;&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#27169;&#31946;&#30340;&#30693;&#35782;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#36339;&#30340;&#30693;&#35782;&#22270;&#38382;&#31572;&#26159;&#19968;&#39033;&#20219;&#21153;&#65292;&#28041;&#21450;&#20174;&#30693;&#35782;&#22270;&#20013;&#26816;&#32034;&#33410;&#28857;&#20197;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#26368;&#36817;&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#23558;&#27492;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#36335;&#24452;&#25628;&#32034;&#38382;&#39064;&#65292;&#20854;&#20013;&#28040;&#24687;&#20174;&#31181;&#23376;&#33410;&#28857;&#27839;&#30528;&#36335;&#24452;&#20381;&#27425;&#20256;&#25773;&#21040;&#31572;&#26696;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#28040;&#24687;&#37117;&#26159;&#36807;&#21435;&#23548;&#21521;&#30340;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#23436;&#25972;&#30340;&#30693;&#35782;&#22270;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26641;&#25628;&#32034;&#30340;GNN&#27169;&#22411;NuTrea&#65292;&#23427;&#23558;&#26356;&#24191;&#27867;&#30340;&#30693;&#35782;&#22270;&#19978;&#19979;&#25991;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#65292;&#25506;&#32034;&#26410;&#21040;&#36798;&#30340;&#23376;&#26641;&#21306;&#22495;&#20197;&#25552;&#21319;&#36807;&#21435;&#23548;&#21521;&#30340;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20851;&#31995;&#39057;&#29575;-&#36870;&#23454;&#20307;&#39057;&#29575;&#65288;RF-IEF&#65289;&#33410;&#28857;&#23884;&#20837;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#27169;&#31946;&#30340;&#30693;&#35782;&#22270;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-hop Knowledge Graph Question Answering (KGQA) is a task that involves retrieving nodes from a knowledge graph (KG) to answer natural language questions. Recent GNN-based approaches formulate this task as a KG path searching problem, where messages are sequentially propagated from the seed node towards the answer nodes. However, these messages are past-oriented, and they do not consider the full KG context. To make matters worse, KG nodes often represent proper noun entities and are sometimes encrypted, being uninformative in selecting between paths. To address these problems, we propose Neural Tree Search (NuTrea), a tree search-based GNN model that incorporates the broader KG context. Our model adopts a message-passing scheme that probes the unreached subtree regions to boost the past-oriented embeddings. In addition, we introduce the Relation Frequency-Inverse Entity Frequency (RF-IEF) node embedding that considers the global KG context to better characterize ambiguous KG nodes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;CRaSh&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181; Cluster, Remove, and Share &#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#19981;&#23436;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#20174;&#23454;&#35777;&#20998;&#26512;&#20013;&#21457;&#29616;&#65292;LLM&#23618;&#20869;&#23384;&#22312;&#29420;&#29305;&#30340;&#27169;&#22359;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#20250;&#20986;&#29616;&#24494;&#23567;&#20294;&#21487;&#33021;&#37325;&#35201;&#30340;&#25913;&#21464;&#12290;</title><link>http://arxiv.org/abs/2310.15477</link><description>&lt;p&gt;
CRaSh: &#32858;&#31867;&#12289;&#21435;&#38500;&#21644;&#20849;&#20139;&#26080;&#38656;&#23436;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
CRaSh: Clustering, Removing, and Sharing Enhance Fine-tuning without Full Large Language Model. (arXiv:2310.15477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;CRaSh&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181; Cluster, Remove, and Share &#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#19981;&#23436;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36827;&#34892;&#24494;&#35843;&#26469;&#22686;&#24378;&#20854;&#24615;&#33021;&#12290;&#20174;&#23454;&#35777;&#20998;&#26512;&#20013;&#21457;&#29616;&#65292;LLM&#23618;&#20869;&#23384;&#22312;&#29420;&#29305;&#30340;&#27169;&#22359;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#26102;&#20250;&#20986;&#29616;&#24494;&#23567;&#20294;&#21487;&#33021;&#37325;&#35201;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#24494;&#35843;&#34987;&#35748;&#20026;&#26159;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#23545;&#40784;&#20197;&#22686;&#24378;&#20854;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20351;&#29992;&#31169;&#26377;&#25351;&#20196;&#25968;&#25454;&#23545;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#38598;&#20013;&#24335;LLM&#36827;&#34892;&#24494;&#35843;&#26102;&#65292;&#38544;&#31169;&#38382;&#39064;&#19981;&#21487;&#36991;&#20813;&#12290;&#30452;&#25509;&#22312;&#27169;&#22411;&#20043;&#38388;&#20256;&#36755;&#21442;&#25968;&#21270;&#27169;&#22359;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#34892;&#26041;&#27861;&#65292;&#20294;&#20854;&#23454;&#38469;&#25928;&#26524;&#21644;&#24433;&#21709;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;Offsite-Tuning&#65288;OFT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#23558;&#36716;&#25442;&#22120;&#22359;&#22312;&#38598;&#20013;&#24335;LLM&#21644;&#19979;&#28216;&#27169;&#25311;&#22120;&#20043;&#38388;&#20256;&#36755;&#30340;&#20195;&#34920;&#24615;&#25216;&#26415;&#12290;&#37492;&#20110;&#23545;OFT&#30340;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#26377;&#38480;&#65292;&#25105;&#20204;&#20174;&#34920;&#31034;&#21644;&#21151;&#33021;&#30456;&#20284;&#24615;&#30340;&#35282;&#24230;&#23545;LLM&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLM&#23618;&#20869;&#29420;&#29305;&#30340;&#27169;&#22359;&#32467;&#26500;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#25193;&#22823;&#32780;&#20986;&#29616;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#24494;&#23567;&#20294;&#28508;&#22312;&#37325;&#35201;&#30340;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning has recently been recognized as an effective way of aligning Large Language Models (LLMs) to enhance their generalization ability across various tasks. However, when tuning publicly accessible, centralized LLMs with private instruction data, privacy concerns are inevitable. While direct transfer of parameterized modules between models is a plausible approach to address this, its implications and effectiveness need further exploration. This paper focuses on Offsite-Tuning (OFT), a representative technique that transfers transformer blocks between centralized LLMs and downstream emulators. Given the limited understanding of the underlying mechanism of OFT, we perform an empirical analysis on LLMs from the perspectives of representation and functional similarity. Interestingly, our findings reveal a unique modular structure within the layers of LLMs that appears to emerge as the model size expands. Simultaneously, we note subtle but potentially significant changes in re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;&#30340;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#26631;&#27880;&#20266;&#26631;&#31614;&#21644;&#20256;&#36882;&#20851;&#38190;&#30693;&#35782;&#26469;&#32531;&#35299;&#20107;&#20214;&#31867;&#22411;&#35821;&#20041;&#28151;&#28102;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#38271;&#23614;&#20107;&#20214;&#31867;&#22411;&#30340;&#29702;&#35299;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15470</link><description>&lt;p&gt;
&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#19982;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Continual Event Extraction with Semantic Confusion Rectification. (arXiv:2310.15470v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;&#30340;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#27169;&#22411;&#65292;&#36890;&#36807;&#26631;&#27880;&#20266;&#26631;&#31614;&#21644;&#20256;&#36882;&#20851;&#38190;&#30693;&#35782;&#26469;&#32531;&#35299;&#20107;&#20214;&#31867;&#22411;&#35821;&#20041;&#28151;&#28102;&#24182;&#25552;&#39640;&#27169;&#22411;&#22312;&#38271;&#23614;&#20107;&#20214;&#31867;&#22411;&#30340;&#29702;&#35299;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#65292;&#26088;&#22312;&#25552;&#21462;&#19981;&#26029;&#20986;&#29616;&#30340;&#20107;&#20214;&#20449;&#24687;&#21516;&#26102;&#36991;&#20813;&#36951;&#24536;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20107;&#20214;&#31867;&#22411;&#30340;&#35821;&#20041;&#28151;&#28102;&#28304;&#20110;&#21516;&#19968;&#25991;&#26412;&#30340;&#27880;&#37322;&#38543;&#26102;&#38388;&#26356;&#26032;&#12290;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#29978;&#33267;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#35821;&#20041;&#28151;&#28102;&#20462;&#27491;&#30340;&#25345;&#32493;&#20107;&#20214;&#25552;&#21462;&#27169;&#22411;&#12290;&#25105;&#20204;&#20026;&#27599;&#20010;&#21477;&#23376;&#26631;&#27880;&#20266;&#26631;&#31614;&#20197;&#32531;&#35299;&#35821;&#20041;&#28151;&#28102;&#12290;&#25105;&#20204;&#22312;&#24403;&#21069;&#27169;&#22411;&#21644;&#20043;&#21069;&#27169;&#22411;&#20043;&#38388;&#20256;&#36882;&#20851;&#38190;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#23545;&#20107;&#20214;&#31867;&#22411;&#30340;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20854;&#20182;&#30456;&#20851;&#31867;&#22411;&#26469;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#38271;&#23614;&#20107;&#20214;&#31867;&#22411;&#30340;&#35821;&#20041;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study continual event extraction, which aims to extract incessantly emerging event information while avoiding forgetting. We observe that the semantic confusion on event types stems from the annotations of the same text being updated over time. The imbalance between event types even aggravates this issue. This paper proposes a novel continual event extraction model with semantic confusion rectification. We mark pseudo labels for each sentence to alleviate semantic confusion. We transfer pivotal knowledge between current and previous models to enhance the understanding of event types. Moreover, we encourage the model to focus on the semantics of long-tailed event types by leveraging other associated types. Experimental results show that our model outperforms state-of-the-art baselines and is proficient in imbalanced datasets.
&lt;/p&gt;</description></item><item><title>&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.15469</link><description>&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Janus Interface: How Fine-Tuning in Large Language Models Amplifies the Privacy Risks. (arXiv:2310.15469v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15469
&lt;/p&gt;
&lt;p&gt;
&#12298;Janus&#25509;&#21475;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#22914;&#20309;&#25918;&#22823;&#38544;&#31169;&#39118;&#38505;&#12299;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#23545;&#20010;&#20154;&#20449;&#24687;&#27844;&#38706;&#30340;&#39118;&#38505;&#65292;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2018&#24180;&#21518;&#30340;&#26102;&#20195;&#26631;&#24535;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;OpenAI&#30340;ChatGPT&#31561;&#21019;&#26032;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#38543;&#30528;&#34892;&#19994;&#22312;&#22686;&#21152;&#27169;&#22411;&#21442;&#25968;&#24182;&#21033;&#29992;&#22823;&#37327;&#30340;&#20154;&#31867;&#35821;&#35328;&#25968;&#25454;&#26041;&#38754;&#30340;&#21162;&#21147;&#65292;&#23433;&#20840;&#21644;&#38544;&#31169;&#25361;&#25112;&#20063;&#20986;&#29616;&#20102;&#12290;&#20854;&#20013;&#26368;&#37325;&#35201;&#30340;&#26159;&#22312;&#22522;&#20110;&#32593;&#32476;&#30340;&#25968;&#25454;&#33719;&#21462;&#36807;&#31243;&#20013;&#65292;&#21487;&#33021;&#20250;&#24847;&#22806;&#31215;&#32047;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#65292;&#20174;&#32780;&#23548;&#33268;&#24847;&#22806;&#30340;PII&#27844;&#38706;&#39118;&#38505;&#12290;&#34429;&#28982;&#20687;RLHF&#21644;&#28798;&#38590;&#24615;&#36951;&#24536;&#36825;&#26679;&#30340;&#31574;&#30053;&#24050;&#34987;&#29992;&#26469;&#25511;&#21046;&#38544;&#31169;&#20405;&#26435;&#30340;&#39118;&#38505;&#65292;&#20294;LLM&#30340;&#26368;&#26032;&#36827;&#23637;&#65288;&#20197;OpenAI&#30340;GPT-3.5&#30340;&#24494;&#35843;&#30028;&#38754;&#20026;&#20195;&#34920;&#65289;&#37325;&#26032;&#24341;&#21457;&#20102;&#20851;&#27880;&#12290;&#26377;&#20154;&#21487;&#33021;&#20250;&#38382;&#65306;LLM&#30340;&#24494;&#35843;&#26159;&#21542;&#20250;&#23548;&#33268;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#30340;&#20010;&#20154;&#20449;&#24687;&#27844;&#28431;&#65311;&#26412;&#25991;&#25253;&#36947;&#20102;&#39318;&#27425;&#23581;&#35797;&#23547;&#27714;&#31572;&#26696;&#30340;&#21162;&#21147;&#65292;&#37325;&#28857;&#26159;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;LLM&#21033;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The era post-2018 marked the advent of Large Language Models (LLMs), with innovations such as OpenAI's ChatGPT showcasing prodigious linguistic prowess. As the industry galloped toward augmenting model parameters and capitalizing on vast swaths of human language data, security and privacy challenges also emerged. Foremost among these is the potential inadvertent accrual of Personal Identifiable Information (PII) during web-based data acquisition, posing risks of unintended PII disclosure. While strategies like RLHF during training and Catastrophic Forgetting have been marshaled to control the risk of privacy infringements, recent advancements in LLMs, epitomized by OpenAI's fine-tuning interface for GPT-3.5, have reignited concerns. One may ask: can the fine-tuning of LLMs precipitate the leakage of personal information embedded within training datasets? This paper reports the first endeavor to seek the answer to the question, particularly our discovery of a new LLM exploitation avenue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;4,442&#20010;Twitter&#26159;&#38750;&#38382;&#39064;&#31572;&#26696;&#23545;&#30340;&#26032;&#35821;&#26009;&#24211;&#65292;&#24182;&#35752;&#35770;&#20102;&#35299;&#35835;&#20026;&#26159;&#25110;&#21542;&#31572;&#26696;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#35299;&#35835;&#26410;&#30693;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15464</link><description>&lt;p&gt;
&#35299;&#35835;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#20013;&#30340;&#26159;&#38750;&#38382;&#39064;&#31572;&#26696;
&lt;/p&gt;
&lt;p&gt;
Interpreting Answers to Yes-No Questions in User-Generated Content. (arXiv:2310.15464v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15464
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;4,442&#20010;Twitter&#26159;&#38750;&#38382;&#39064;&#31572;&#26696;&#23545;&#30340;&#26032;&#35821;&#26009;&#24211;&#65292;&#24182;&#35752;&#35770;&#20102;&#35299;&#35835;&#20026;&#26159;&#25110;&#21542;&#31572;&#26696;&#30340;&#35821;&#35328;&#29305;&#24449;&#20197;&#21450;&#35299;&#35835;&#26410;&#30693;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24456;&#22909;&#22320;&#35299;&#20915;&#36825;&#20010;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#35835;&#31038;&#20132;&#23186;&#20307;&#20013;&#30340;&#26159;&#38750;&#38382;&#39064;&#31572;&#26696;&#26159;&#22256;&#38590;&#30340;&#12290;&#26159;&#21644;&#21542;&#30340;&#20851;&#38190;&#35789;&#24456;&#23569;&#35265;&#65292;&#32780;&#21253;&#21547;&#36825;&#20123;&#20851;&#38190;&#35789;&#30340;&#31572;&#26696;&#24456;&#23569;&#33021;&#25353;&#29031;&#20851;&#38190;&#35789;&#25152;&#31034;&#30340;&#24847;&#24605;&#36827;&#34892;&#35299;&#35835;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;4,442&#20010;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;Twitter&#19978;&#30340;&#26159;&#38750;&#38382;&#39064;&#31572;&#26696;&#23545;&#30340;&#26032;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#35299;&#35835;&#20026;&#26159;&#25110;&#21542;&#30340;&#31572;&#26696;&#30340;&#35821;&#35328;&#29305;&#24449;&#65292;&#20197;&#21450;&#35299;&#35835;&#26410;&#30693;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#34920;&#26126;&#21363;&#20351;&#22312;&#24494;&#35843;&#21644;&#34701;&#21512;&#20854;&#20182;&#38750;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting answers to yes-no questions in social media is difficult. Yes and no keywords are uncommon, and the few answers that include them are rarely to be interpreted what the keywords suggest. In this paper, we present a new corpus of 4,442 yes-no question-answer pairs from Twitter. We discuss linguistic characteristics of answers whose interpretation is yes or no, as well as answers whose interpretation is unknown. We show that large language models are far from solving this problem, even after fine-tuning and blending other corpora for the same problem but outside social media.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22914;&#20309;&#25903;&#25345;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65292;&#36890;&#36807;&#20197;&#35748;&#30693;&#37325;&#24314;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23545;&#21442;&#19982;&#32773;&#24773;&#32490;&#24378;&#24230;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#20811;&#26381;&#20102;&#36127;&#38754;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2310.15461</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20419;&#36827;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65306;&#35748;&#30693;&#37325;&#24314;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring. (arXiv:2310.15461v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22914;&#20309;&#25903;&#25345;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65292;&#36890;&#36807;&#20197;&#35748;&#30693;&#37325;&#24314;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23545;&#21442;&#19982;&#32773;&#24773;&#32490;&#24378;&#24230;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#24182;&#24110;&#21161;&#20182;&#20204;&#20811;&#26381;&#20102;&#36127;&#38754;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#65292;&#22914;&#23398;&#20064;&#21644;&#23454;&#36341;&#24212;&#23545;&#31574;&#30053;&#30340;&#8220;&#33258;&#21161;&#24037;&#20855;&#8221;&#65292;&#22312;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#30340;&#21487;&#21450;&#24615;&#26041;&#38754;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24178;&#39044;&#24120;&#24120;&#38656;&#35201;&#35748;&#30693;&#36127;&#25285;&#21644;&#24773;&#32490;&#35302;&#21457;&#65292;&#20174;&#32780;&#36896;&#25104;&#38480;&#21046;&#20854;&#22823;&#35268;&#27169;&#23454;&#26045;&#21644;&#26222;&#21450;&#30340;&#21487;&#21450;&#24615;&#38556;&#30861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20154;&#26426;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22914;&#20309;&#25903;&#25345;&#33258;&#20027;&#24341;&#23548;&#24335;&#24515;&#29702;&#20581;&#24247;&#24178;&#39044;&#12290;&#25105;&#20204;&#20197;&#35748;&#30693;&#37325;&#24314;&#20316;&#20026;&#19968;&#20010;&#20197;&#35777;&#25454;&#20026;&#22522;&#30784;&#30340;&#27835;&#30103;&#25216;&#26415;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#22312;&#19968;&#39033;&#32463;&#36807;IRB&#25209;&#20934;&#30340;&#22823;&#22411;&#24515;&#29702;&#20581;&#24247;&#32593;&#31449;&#19978;&#36827;&#34892;&#30340;&#38543;&#26426;&#29616;&#22330;&#30740;&#31350;&#20013;&#65292;&#28041;&#21450;&#20102;15,531&#21517;&#21442;&#19982;&#32773;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#35780;&#20272;&#20102;&#19968;&#20010;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#25903;&#25345;&#20154;&#20204;&#22312;&#35748;&#30693;&#37325;&#24314;&#30340;&#21508;&#20010;&#27493;&#39588;&#20013;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#23545;67%&#30340;&#21442;&#19982;&#32773;&#30340;&#24773;&#32490;&#24378;&#24230;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#24182;&#24110;&#21161;65%&#30340;&#20154;&#20811;&#26381;&#28040;&#26497;&#24605;&#32500;&#12290;&#23613;&#31649;&#38738;&#23569;&#24180;&#25253;&#36947;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-guided mental health interventions, such as "do-it-yourself" tools to learn and practice coping strategies, show great promise to improve access to mental health care. However, these interventions are often cognitively demanding and emotionally triggering, creating accessibility barriers that limit their wide-scale implementation and adoption. In this paper, we study how human-language model interaction can support self-guided mental health interventions. We take cognitive restructuring, an evidence-based therapeutic technique to overcome negative thinking, as a case study. In an IRB-approved randomized field study on a large mental health website with 15,531 participants, we design and evaluate a system that uses language models to support people through various steps of cognitive restructuring. Our findings reveal that our system positively impacts emotional intensity for 67% of participants and helps 65% overcome negative thoughts. Although adolescents report relatively worse o
&lt;/p&gt;</description></item><item><title>K-HATERS&#26159;&#19968;&#20010;&#20855;&#26377;&#30446;&#26631;&#29305;&#23450;&#35780;&#32423;&#30340;&#38889;&#35821;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#22823;&#32422;192K&#26465;&#26032;&#38395;&#35780;&#35770;&#12290;&#23427;&#26159;&#38889;&#25991;&#26368;&#22823;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20063;&#26159;&#39318;&#20010;&#25552;&#20379;&#30446;&#26631;&#29305;&#23450;&#35780;&#32423;&#30340;&#36164;&#28304;&#65292;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#20882;&#29359;&#24615;&#19979;&#21487;&#20197;&#26816;&#27979;&#38889;&#35821;&#20013;&#30340;&#20167;&#24680;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2310.15439</link><description>&lt;p&gt;
K-HATERS&#65306;&#20855;&#26377;&#30446;&#26631;&#29305;&#23450;&#35780;&#32423;&#30340;&#38889;&#35821;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
K-HATERS: A Hate Speech Detection Corpus in Korean with Target-Specific Ratings. (arXiv:2310.15439v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15439
&lt;/p&gt;
&lt;p&gt;
K-HATERS&#26159;&#19968;&#20010;&#20855;&#26377;&#30446;&#26631;&#29305;&#23450;&#35780;&#32423;&#30340;&#38889;&#35821;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;&#22823;&#32422;192K&#26465;&#26032;&#38395;&#35780;&#35770;&#12290;&#23427;&#26159;&#38889;&#25991;&#26368;&#22823;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20063;&#26159;&#39318;&#20010;&#25552;&#20379;&#30446;&#26631;&#29305;&#23450;&#35780;&#32423;&#30340;&#36164;&#28304;&#65292;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#20882;&#29359;&#24615;&#19979;&#21487;&#20197;&#26816;&#27979;&#38889;&#35821;&#20013;&#30340;&#20167;&#24680;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25968;&#25454;&#38598;&#26469;&#25171;&#20987;&#32593;&#32476;&#20167;&#24680;&#35328;&#35770;&#30340;&#20256;&#25773;&#65292;&#20294;&#20854;&#20013;&#22823;&#22810;&#25968;&#37117;&#26159;&#20197;&#33521;&#35821;&#20026;&#20013;&#24515;&#65292;&#20027;&#35201;&#20851;&#27880;&#26126;&#26174;&#30340;&#20167;&#24680;&#24418;&#24335;&#12290;&#36825;&#31181;&#30740;&#31350;&#31354;&#30333;&#35201;&#27714;&#25105;&#20204;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#26356;&#24494;&#22937;&#30340;&#20167;&#24680;&#34920;&#36798;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;K-HATERS&#65292;&#19968;&#20010;&#26032;&#30340;&#38889;&#35821;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;192,000&#26465;&#26032;&#38395;&#35780;&#35770;&#65292;&#38468;&#24102;&#30446;&#26631;&#29305;&#23450;&#30340;&#20882;&#29359;&#24615;&#35780;&#32423;&#12290;&#35813;&#36164;&#28304;&#26159;&#38889;&#25991;&#20013;&#26368;&#22823;&#30340;&#20882;&#29359;&#24615;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20063;&#26159;&#39318;&#20010;&#25552;&#20379;&#19977;&#28857;&#37324;&#20811;&#29305;&#37327;&#34920;&#19978;&#30340;&#30446;&#26631;&#29305;&#23450;&#35780;&#32423;&#30340;&#36164;&#28304;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#20882;&#29359;&#24615;&#19979;&#26816;&#27979;&#38889;&#35821;&#20013;&#30340;&#20167;&#24680;&#34920;&#36798;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#35821;&#26009;&#24211;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#20154;&#31867;&#27880;&#37322;&#20013;&#30340;&#28508;&#22312;&#22122;&#22768;&#21644;&#20559;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#37319;&#29992;&#35748;&#30693;&#21453;&#23556;&#27979;&#35797;&#30340;&#26032;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous datasets have been proposed to combat the spread of online hate. Despite these efforts, a majority of these resources are English-centric, primarily focusing on overt forms of hate. This research gap calls for developing high-quality corpora in diverse languages that also encapsulate more subtle hate expressions. This study introduces K-HATERS, a new corpus for hate speech detection in Korean, comprising approximately 192K news comments with target-specific offensiveness ratings. This resource is the largest offensive language corpus in Korean and is the first to offer target-specific ratings on a three-point Likert scale, enabling the detection of hate expressions in Korean across varying degrees of offensiveness. We conduct experiments showing the effectiveness of the proposed corpus, including a comparison with existing datasets. Additionally, to address potential noise and bias in human annotations, we explore a novel idea of adopting the Cognitive Reflection Test, which i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26131;&#25512;&#32763;&#30340;&#36947;&#24503;&#25512;&#29702;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#32972;&#26223;&#19979;&#34892;&#20026;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#65292;&#24182;&#25552;&#20379;&#24120;&#35782;&#29702;&#30001;&#26469;&#25903;&#25345;&#25512;&#29702;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2310.15431</link><description>&lt;p&gt;
&#20160;&#20040;&#24773;&#20917;&#19979;&#32437;&#28779;&#26159;&#21487;&#20197;&#25509;&#21463;&#30340;&#65311;&#36890;&#36807;&#36845;&#20195;&#33258;&#33976;&#39311;&#24773;&#22659;&#21644;&#21407;&#22240;&#26469;&#28040;&#38500;&#27169;&#31946;&#30340;&#31038;&#20250;&#21644;&#36947;&#24503;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
What Makes it Ok to Set a Fire? Iterative Self-distillation of Contexts and Rationales for Disambiguating Defeasible Social and Moral Situations. (arXiv:2310.15431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#26131;&#25512;&#32763;&#30340;&#36947;&#24503;&#25512;&#29702;&#20219;&#21153;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#32972;&#26223;&#19979;&#34892;&#20026;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#65292;&#24182;&#25552;&#20379;&#24120;&#35782;&#29702;&#30001;&#26469;&#25903;&#25345;&#25512;&#29702;&#12290;&#36890;&#36807;&#36845;&#20195;&#30340;&#33258;&#33976;&#39311;&#26041;&#27861;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#25110;&#20262;&#29702;&#21028;&#26029;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20854;&#21457;&#29983;&#30340;&#20855;&#20307;&#32972;&#26223;&#12290;&#29702;&#35299;&#26131;&#25512;&#32763;&#30340;&#24773;&#22659;&#21270;&#21464;&#21270;&#65288;&#21363;&#22686;&#24378;&#25110;&#20943;&#24369;&#19968;&#20010;&#34892;&#20026;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#30340;&#39069;&#22806;&#20449;&#24687;&#65289;&#23545;&#20110;&#20934;&#30830;&#21576;&#29616;&#29616;&#23454;&#22330;&#26223;&#20013;&#20957;&#22266;&#30340;&#20154;&#31867;&#36947;&#24503;&#21028;&#26029;&#30340;&#24494;&#22937;&#21644;&#22797;&#26434;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26131;&#25512;&#32763;&#30340;&#36947;&#24503;&#25512;&#29702;&#65306;&#19968;&#39033;&#20219;&#21153;&#65292;&#25552;&#20379;&#20351;&#34892;&#20026;&#22312;&#36947;&#24503;&#19978;&#26356;&#21487;&#25509;&#21463;&#25110;&#19981;&#21487;&#25509;&#21463;&#30340;&#24773;&#22659;&#65292;&#20197;&#21450;&#35777;&#26126;&#25512;&#29702;&#30340;&#24120;&#35782;&#29702;&#30001;&#12290;&#20026;&#20102;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#20219;&#21153;&#25968;&#25454;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36845;&#20195;&#33258;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20174;GPT-3&#30340;&#19968;&#23567;&#37096;&#20998;&#38750;&#32467;&#26500;&#21270;&#31181;&#23376;&#30693;&#35782;&#24320;&#22987;&#65292;&#28982;&#21518;&#22312;&#23398;&#29983;&#27169;&#22411;&#21644;&#25209;&#21028;&#32773;&#27169;&#22411;&#20043;&#38388;&#20132;&#26367;&#36827;&#34892;&#65288;1&#65289;&#33258;&#33976;&#39311;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20154;&#31867;&#21028;&#26029;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#36807;&#28388;&#65288;&#20197;&#25552;&#39640;&#26377;&#25928;&#24615;&#65289;&#21644;NLI&#65288;&#20197;&#25552;&#39640;&#22810;&#26679;&#24615;&#65289;&#65307;&#65288;3&#65289;&#33258;&#27169;&#20223;&#23398;&#20064;&#65288;&#20197;&#25918;&#22823;&#25152;&#38656;&#25968;&#25454;&#36136;&#37327;&#65289;&#12290;&#36825;&#20010;&#36807;&#31243;&#20135;&#29983;&#20102;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moral or ethical judgments rely heavily on the specific contexts in which they occur. Understanding varying shades of defeasible contextualizations (i.e., additional information that strengthens or attenuates the moral acceptability of an action) is critical to accurately represent the subtlety and intricacy of grounded human moral judgment in real-life scenarios.  We introduce defeasible moral reasoning: a task to provide grounded contexts that make an action more or less morally acceptable, along with commonsense rationales that justify the reasoning. To elicit high-quality task data, we take an iterative self-distillation approach that starts from a small amount of unstructured seed knowledge from GPT-3 and then alternates between (1) self-distillation from student models; (2) targeted filtering with a critic model trained by human judgment (to boost validity) and NLI (to boost diversity); (3) self-imitation learning (to amplify the desired data quality). This process yields a stude
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20027;&#39064;&#24230;&#37327;&#20316;&#20026;&#24773;&#24863;&#24230;&#37327;&#22312;&#25919;&#27835;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#26367;&#20195;&#21644;&#34917;&#20805;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20027;&#39064;&#24230;&#37327;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#25552;&#39640;&#20102;&#36830;&#36143;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#31435;&#22330;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15429</link><description>&lt;p&gt;
&#36229;&#36234;&#24773;&#24863;&#65306;&#21033;&#29992;&#20027;&#39064;&#24230;&#37327;&#36827;&#34892;&#25919;&#27835;&#31435;&#22330;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Beyond Sentiment: Leveraging Topic Metrics for Political Stance Classification. (arXiv:2310.15429v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20027;&#39064;&#24230;&#37327;&#20316;&#20026;&#24773;&#24863;&#24230;&#37327;&#22312;&#25919;&#27835;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#26367;&#20195;&#21644;&#34917;&#20805;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20027;&#39064;&#24230;&#37327;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#25552;&#39640;&#20102;&#36830;&#36143;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#31435;&#22330;&#20998;&#31867;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#34987;&#24191;&#27867;&#25209;&#35780;&#20165;&#33021;&#25429;&#25417;&#21040;&#35821;&#26009;&#24211;&#30340;&#25972;&#20307;&#24773;&#24863;&#65292;&#26080;&#27861;&#20934;&#30830;&#21453;&#26144;&#25991;&#26412;&#20013;&#30340;&#28508;&#22312;&#32467;&#26500;&#21644;&#25919;&#27835;&#31435;&#22330;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20027;&#39064;&#24230;&#37327;&#65292;&#23558;&#20174;&#25552;&#21462;&#30340;&#20027;&#39064;&#36716;&#21270;&#32780;&#26469;&#30340;&#34394;&#25311;&#21464;&#37327;&#65292;&#20316;&#20026;&#24773;&#24863;&#24230;&#37327;&#22312;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#26367;&#20195;&#21644;&#34917;&#20805;&#12290;&#36890;&#36807;&#24212;&#29992;Bestvater&#21644;Monroe&#65288;2023&#65289;&#30830;&#23450;&#30340;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;BERTopic&#22312;&#25552;&#21462;&#36830;&#36143;&#20027;&#39064;&#21644;&#20027;&#39064;&#24230;&#37327;&#22312;&#31435;&#22330;&#20998;&#31867;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;Dirichlet&#20998;&#37197;&#65288;LDA&#65289;&#21644;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#30456;&#27604;&#65292;BERTopic&#25552;&#39640;&#20102;17.07%&#33267;54.20%&#30340;&#36830;&#36143;&#24615;&#20998;&#25968;&#65292;&#36825;&#20123;&#20256;&#32479;&#26041;&#27861;&#22312;&#26089;&#26399;&#25919;&#27835;&#23398;&#30740;&#31350;&#20013;&#24456;&#24120;&#35265;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#39064;&#24230;&#37327;&#22312;&#31435;&#22330;&#20998;&#31867;&#20013;&#20248;&#20110;&#24773;&#24863;&#24230;&#37327;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;18.95%&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20027;&#39064;&#24230;&#37327;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#25991;&#26412;&#20013;&#30340;&#25919;&#27835;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis, widely critiqued for capturing merely the overall tone of a corpus, falls short in accurately reflecting the latent structures and political stances within texts. This study introduces topic metrics, dummy variables converted from extracted topics, as both an alternative and complement to sentiment metrics in stance classification. By employing three datasets identified by Bestvater and Monroe (2023), this study demonstrates BERTopic's proficiency in extracting coherent topics and the effectiveness of topic metrics in stance classification. The experiment results show that BERTopic improves coherence scores by 17.07% to 54.20% when compared to traditional approaches such as Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF), prevalent in earlier political science research. Additionally, our results indicate topic metrics outperform sentiment metrics in stance classification, increasing performance by as much as 18.95%. Our findings suggest topic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25554;&#20540;&#30340;&#26032;&#22411;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21517;&#20026;Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;&#12290;&#35813;&#31995;&#32479;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#23558;&#22768;&#23398;&#27169;&#22411;&#35270;&#20026;&#26631;&#27880;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#20102;&#25554;&#20540;&#25216;&#26415;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20998;&#27573;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.15425</link><description>&lt;p&gt;
Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;: &#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25554;&#20540;&#30340;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The Mason-Alberta Phonetic Segmenter: A forced alignment system based on deep neural networks and interpolation. (arXiv:2310.15425v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#25554;&#20540;&#30340;&#26032;&#22411;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21517;&#20026;Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;&#12290;&#35813;&#31995;&#32479;&#30340;&#21019;&#26032;&#28857;&#21253;&#25324;&#23558;&#22768;&#23398;&#27169;&#22411;&#35270;&#20026;&#26631;&#27880;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#20102;&#25554;&#20540;&#25216;&#26415;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#20998;&#27573;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#22312;&#32473;&#23450;&#27491;&#23383;&#27861;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#20013;&#33258;&#21160;&#30830;&#23450;&#20998;&#27573;&#36793;&#30028;&#12290;&#36825;&#20123;&#24037;&#20855;&#22312;&#35821;&#38899;&#23398;&#20013;&#24456;&#24120;&#35265;&#65292;&#20197;&#20415;&#20351;&#29992;&#37027;&#20123;&#25163;&#21160;&#36716;&#24405;&#21644;&#20998;&#27573;&#38590;&#20197;&#23454;&#29616;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#65292;&#21363;Mason-Alberta&#38899;&#26631;&#20998;&#21106;&#22120;&#65288;MAPS&#65289;&#12290;MAPS&#23545;&#40784;&#22120;&#20316;&#20026;&#25105;&#20204;&#36861;&#27714;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#20004;&#20010;&#28508;&#22312;&#25913;&#36827;&#30340;&#35797;&#39564;&#24179;&#21488;&#12290;&#31532;&#19968;&#20010;&#26159;&#23558;&#24378;&#21046;&#23545;&#40784;&#22120;&#20013;&#30340;&#22768;&#23398;&#27169;&#22411;&#35270;&#20026;&#26631;&#27880;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#36825;&#26159;&#22522;&#20110;&#20154;&#20204;&#23545;&#35821;&#38899;&#20013;&#30340;&#27573;&#33853;&#24182;&#19981;&#26159;&#30495;&#27491;&#31163;&#25955;&#21644;&#24120;&#24120;&#37325;&#21472;&#30340;&#20849;&#21516;&#35748;&#35782;&#12290;&#31532;&#20108;&#20010;&#26159;&#25554;&#20540;&#25216;&#26415;&#65292;&#20351;&#24471;&#36793;&#30028;&#21487;&#20197;&#27604;&#29616;&#20195;&#24378;&#21046;&#23545;&#40784;&#31995;&#32479;&#24120;&#35265;&#30340;10&#27627;&#31186;&#38480;&#21046;&#26356;&#31934;&#30830;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#31995;&#32479;&#30340;&#37197;&#32622;&#19982;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;Montreal Forced Aligner&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forced alignment systems automatically determine boundaries between segments in speech data, given an orthographic transcription. These tools are commonplace in phonetics to facilitate the use of speech data that would be infeasible to manually transcribe and segment. In the present paper, we describe a new neural network-based forced alignment system, the Mason-Alberta Phonetic Segmenter (MAPS). The MAPS aligner serves as a testbed for two possible improvements we pursue for forced alignment systems. The first is treating the acoustic model in a forced aligner as a tagging task, rather than a classification task, motivated by the common understanding that segments in speech are not truly discrete and commonly overlap. The second is an interpolation technique to allow boundaries more precise than the common 10 ms limit in modern forced alignment systems. We compare configurations of our system to a state-of-the-art system, the Montreal Forced Aligner. The tagging approach did not gener
&lt;/p&gt;</description></item><item><title>FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;</title><link>http://arxiv.org/abs/2310.15421</link><description>&lt;p&gt;
FANToM: &#22312;&#20132;&#20114;&#20013;&#23545;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#36827;&#34892;&#21387;&#21147;&#27979;&#35797;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions. (arXiv:2310.15421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15421
&lt;/p&gt;
&lt;p&gt;
FANToM&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#21387;&#21147;&#27979;&#35797;&#26426;&#22120;&#30340;&#24515;&#26234;&#29702;&#35770;&#12290;&#36825;&#20010;&#22522;&#20934;&#23545;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;&#27169;&#22411;&#20063;&#27604;&#20154;&#31867;&#34920;&#29616;&#24471;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20851;&#20110;&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#30340;&#35780;&#20272;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#32570;&#20047;&#20114;&#21160;&#24615;&#30340;&#34987;&#21160;&#25925;&#20107;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FANToM&#65292;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#36890;&#36807;&#38382;&#31572;&#22312;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#24515;&#26234;&#29702;&#35770;&#30340;&#21387;&#21147;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#32467;&#21512;&#20102;&#24515;&#29702;&#23398;&#20013;&#30340;&#37325;&#35201;&#29702;&#35770;&#35201;&#27714;&#21644;&#23545;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#24517;&#35201;&#30340;&#32463;&#39564;&#32771;&#34385;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#22810;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#35201;&#27714;&#30456;&#21516;&#30340;&#22522;&#26412;&#25512;&#29702;&#26469;&#35782;&#21035;LLM&#20013;&#19981;&#23384;&#22312;&#25110;&#34394;&#20551;&#30340;&#24515;&#26234;&#29702;&#35770;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;FANToM&#23545;&#26368;&#20808;&#36827;&#30340;LLM&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#20351;&#26159;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#24494;&#35843;&#30340;LLM&#20063;&#34920;&#29616;&#27604;&#20154;&#31867;&#24046;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#30701;&#25991;&#26412;&#25193;&#23637;&#20026;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26497;&#31471;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15420</link><description>&lt;p&gt;
&#35753;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#8220;&#24819;&#35937;&#8221;
&lt;/p&gt;
&lt;p&gt;
Let the Pretrained Language Models "Imagine" for Short Texts Topic Modeling. (arXiv:2310.15420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23558;&#30701;&#25991;&#26412;&#25193;&#23637;&#20026;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#26497;&#31471;&#25968;&#25454;&#31232;&#30095;&#24773;&#20917;&#19979;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#26159;&#19968;&#31181;&#21457;&#29616;&#25991;&#26723;&#38598;&#21512;&#20013;&#28508;&#22312;&#35821;&#20041;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20551;&#35774;&#25991;&#26723;&#20855;&#26377;&#36275;&#22815;&#30340;&#20849;&#29616;&#20449;&#24687;&#25165;&#33021;&#21457;&#25381;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#30701;&#25991;&#26412;&#20013;&#65292;&#20849;&#29616;&#20449;&#24687;&#24456;&#23569;&#65292;&#23548;&#33268;&#25991;&#26723;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#31232;&#30095;&#12290;&#22240;&#27492;&#65292;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#65288;&#27010;&#29575;&#25110;&#31070;&#32463;&#32593;&#32476;&#65289;&#22823;&#22810;&#26080;&#27861;&#20174;&#20013;&#25366;&#25496;&#27169;&#24335;&#24182;&#29983;&#25104;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23558;&#30701;&#25991;&#26412;&#25193;&#23637;&#20026;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#25193;&#23637;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#20197;&#20943;&#23569;PLMs&#29983;&#25104;&#30340;&#22122;&#22768;&#8220;&#38750;&#20027;&#39064;&#8221;&#25991;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#30701;&#25991;&#26412;&#20027;&#39064;&#24314;&#27169;&#30340;&#24615;&#33021;&#12290;&#22312;&#26497;&#31471;&#25968;&#25454;&#31232;&#30095;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#22810;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#27169;&#22411;&#25928;&#26524;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are one of the compelling methods for discovering latent semantics in a document collection. However, it assumes that a document has sufficient co-occurrence information to be effective. However, in short texts, co-occurrence information is minimal, which results in feature sparsity in document representation. Therefore, existing topic models (probabilistic or neural) mostly fail to mine patterns from them to generate coherent topics. In this paper, we take a new approach to short-text topic modeling to address the data-sparsity issue by extending short text into longer sequences using existing pre-trained language models (PLMs). Besides, we provide a simple solution extending a neural topic model to reduce the effect of noisy out-of-topics text generation from PLMs. We observe that our model can substantially improve the performance of short-text topic modeling. Extensive experiments on multiple real-world datasets under extreme data sparsity scenarios show that our model
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#23545;&#35805;&#27169;&#22411;&#24847;&#35782;&#21040;&#26102;&#38388;&#30340;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27425;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;GapChat&#65292;&#26174;&#31034;&#20986;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#22411;&#22312;&#21028;&#26029;&#35805;&#39064;&#30456;&#20851;&#24615;&#21644;&#20174;&#23545;&#35805;&#20013;&#33719;&#21462;&#20449;&#24687;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.15415</link><description>&lt;p&gt;
&#27880;&#24847;&#23545;&#35805;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#25552;&#39640;&#38271;&#26399;&#23545;&#35805;&#29983;&#25104;&#30340;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Mind the Gap Between Conversations for Improved Long-Term Dialogue Generation. (arXiv:2310.15415v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#23545;&#35805;&#27169;&#22411;&#24847;&#35782;&#21040;&#26102;&#38388;&#30340;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27425;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;GapChat&#65292;&#26174;&#31034;&#20986;&#26102;&#38388;&#24863;&#30693;&#30340;&#27169;&#22411;&#22312;&#21028;&#26029;&#35805;&#39064;&#30456;&#20851;&#24615;&#21644;&#20174;&#23545;&#35805;&#20013;&#33719;&#21462;&#20449;&#24687;&#30340;&#24230;&#37327;&#26631;&#20934;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#36947;&#22914;&#20309;&#32467;&#26463;&#21644;&#24674;&#22797;&#23545;&#35805;&#26159;&#20132;&#27969;&#30340;&#33258;&#28982;&#37096;&#20998;&#65292;&#20801;&#35768;&#35752;&#35770;&#36328;&#36234;&#25968;&#21608;&#12289;&#25968;&#26376;&#25110;&#25968;&#24180;&#12290;&#23545;&#35805;&#20043;&#38388;&#30340;&#38388;&#38548;&#25345;&#32493;&#26102;&#38388;&#20915;&#23450;&#20102;&#21738;&#20123;&#35805;&#39064;&#26159;&#30456;&#20851;&#30340;&#65292;&#20197;&#21450;&#35201;&#38382;&#21738;&#20123;&#38382;&#39064;&#65292;&#32780;&#19981;&#26126;&#30830;&#27169;&#25311;&#26102;&#38388;&#30340;&#23545;&#35805;&#31995;&#32479;&#21487;&#33021;&#29983;&#25104;&#19981;&#33258;&#28982;&#30340;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#23545;&#35805;&#27169;&#22411;&#24847;&#35782;&#21040;&#26102;&#38388;&#30340;&#24819;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;GapChat&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27425;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#27599;&#20010;&#23545;&#35805;&#20043;&#38388;&#30340;&#26102;&#38388;&#19981;&#21516;&#12290;&#34429;&#28982;&#25968;&#25454;&#38598;&#26159;&#23454;&#26102;&#26500;&#24314;&#30340;&#65292;&#20294;&#28436;&#35762;&#32773;&#29983;&#27963;&#20013;&#20107;&#20214;&#30340;&#36827;&#23637;&#26159;&#27169;&#25311;&#30340;&#65292;&#20197;&#21019;&#24314;&#21457;&#29983;&#22312;&#38271;&#26102;&#38388;&#36328;&#24230;&#20869;&#30340;&#29616;&#23454;&#23545;&#35805;&#12290;&#25105;&#20204;&#23558;&#26102;&#38388;&#20449;&#24687;&#26292;&#38706;&#32473;&#27169;&#22411;&#65292;&#24182;&#27604;&#36739;&#19981;&#21516;&#30340;&#26102;&#38388;&#21644;&#20107;&#20214;&#36827;&#23637;&#34920;&#31034;&#12290;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26102;&#38388;&#24863;&#30693;&#27169;&#22411;&#22312;&#21028;&#26029;&#36873;&#25321;&#35805;&#39064;&#30340;&#30456;&#20851;&#24615;&#21644;&#20174;&#23545;&#35805;&#20013;&#33719;&#24471;&#30340;&#20449;&#24687;&#30340;&#24230;&#37327;&#26631;&#20934;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowing how to end and resume conversations over time is a natural part of communication, allowing for discussions to span weeks, months, or years. The duration of gaps between conversations dictates which topics are relevant and which questions to ask, and dialogue systems which do not explicitly model time may generate responses that are unnatural. In this work we explore the idea of making dialogue models aware of time, and present GapChat, a multi-session dialogue dataset in which the time between each session varies. While the dataset is constructed in real-time, progress on events in speakers' lives is simulated in order to create realistic dialogues occurring across a long timespan. We expose time information to the model and compare different representations of time and event progress. In human evaluation we show that time-aware models perform better in metrics that judge the relevance of the chosen topics and the information gained from the conversation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32463;&#27982;&#23454;&#24800;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31185;&#23398;&#22270;&#34920;&#26631;&#39064;&#65292;&#24182;&#21457;&#29616;GPT-4&#20316;&#20026;&#38646;-shot&#35780;&#20272;&#22120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.15405</link><description>&lt;p&gt;
GPT-4&#20316;&#20026;&#31185;&#23398;&#22270;&#34920;&#26631;&#39064;&#30340;&#26377;&#25928;&#38646;-shot&#35780;&#20272;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-4 as an Effective Zero-Shot Evaluator for Scientific Figure Captions. (arXiv:2310.15405v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32463;&#27982;&#23454;&#24800;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#31185;&#23398;&#22270;&#34920;&#26631;&#39064;&#65292;&#24182;&#21457;&#29616;GPT-4&#20316;&#20026;&#38646;-shot&#35780;&#20272;&#22120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#21644;&#20154;&#24037;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#29983;&#25104;&#31185;&#23398;&#22270;&#34920;&#26631;&#39064;&#30340;&#31995;&#32479;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#36755;&#20986;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#12290;&#20154;&#24037;&#35780;&#20272;&#38656;&#35201;&#23398;&#26415;&#19987;&#38271;&#65292;&#24182;&#19988;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#33258;&#21160;&#35780;&#20272;&#20381;&#36182;&#20110;&#36890;&#24120;&#36136;&#37327;&#36739;&#20302;&#30340;&#20316;&#32773;&#32534;&#20889;&#30340;&#26631;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#32463;&#27982;&#23454;&#24800;&#12289;&#26080;&#21442;&#32771;&#26041;&#27861;&#26469;&#35780;&#20272;&#22270;&#34920;&#26631;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;SCICAP-EVAL&#65292;&#36825;&#26159;&#19968;&#20010;&#20154;&#24037;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;3600&#20010;&#31185;&#23398;&#22270;&#34920;&#26631;&#39064;&#30340;&#20154;&#24037;&#21028;&#26029;&#65292;&#21253;&#25324;&#21407;&#22987;&#26631;&#39064;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#26631;&#39064;&#65292;&#28085;&#30422;&#20102;600&#20010;arXiv&#22270;&#34920;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;GPT-4&#21644;GPT-3&#31561;LLMs&#36827;&#34892;&#35780;&#20998;&#65288;1-6&#65289;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#32473;&#23450;&#30456;&#20851;&#19978;&#19979;&#25991;&#65288;&#22914;&#25552;&#21450;&#22270;&#34920;&#30340;&#27573;&#33853;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;&#27599;&#20010;&#26631;&#39064;&#23545;&#35835;&#32773;&#29702;&#35299;&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20316;&#20026;&#38646;-shot&#35780;&#20272;&#22120;&#65292;GPT-4&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#27169;&#22411;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#20449;&#24687;&#23398;&#26412;&#31185;&#29983;&#30340;&#35780;&#20272;&#65292;&#36798;&#21040;&#20102;Kendall&#30456;&#20851;&#31995;&#25968;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in systems that generate captions for scientific figures. However, assessing these systems output poses a significant challenge. Human evaluation requires academic expertise and is costly, while automatic evaluation depends on often low-quality author-written captions. This paper investigates using large language models (LLMs) as a cost-effective, reference-free method for evaluating figure captions. We first constructed SCICAP-EVAL, a human evaluation dataset that contains human judgments for 3,600 scientific figure captions, both original and machine-made, for 600 arXiv figures. We then prompted LLMs like GPT-4 and GPT-3 to score (1-6) each caption based on its potential to aid reader understanding, given relevant context such as figure-mentioning paragraphs. Results show that GPT-4, used as a zero-shot evaluator, outperformed all other models and even surpassed assessments made by Computer Science and Informatics undergraduates, achieving a Kendall correlat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25200;&#21160;&#19981;&#21516;&#31867;&#22411;&#30340;&#36523;&#20221;&#30456;&#20851;&#35821;&#35328;&#29305;&#24449;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#20844;&#24179;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36866;&#24212;&#30340;&#21160;&#26426;&#21253;&#25324;&#31038;&#20250;&#35268;&#33539;&#12289;&#25991;&#21270;&#24046;&#24322;&#12289;&#29305;&#23450;&#29305;&#24449;&#20449;&#24687;&#21644;&#36866;&#24212;&#24615;&#65307;&#19981;&#21464;&#24615;&#30340;&#21160;&#26426;&#21253;&#25324;&#25903;&#25345;&#35268;&#23450;&#20027;&#20041;&#30340;&#35266;&#28857;&#12289;&#23558;&#36866;&#24212;&#35270;&#20026;&#19981;&#24517;&#35201;&#36807;&#31243;&#65292;&#24182;&#23545;&#38169;&#35823;&#20551;&#35774;&#25345;&#35880;&#24910;&#24577;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23450;&#20041;&#20844;&#24179;&#30456;&#20851;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15398</link><description>&lt;p&gt;
"&#19968;&#20992;&#20999;"&#65311; &#36328;&#36523;&#20221;&#35821;&#35328;&#29305;&#24449;&#30340; NLG &#31995;&#32479;&#35266;&#23519;&#19982;&#26399;&#26395;
&lt;/p&gt;
&lt;p&gt;
"One-size-fits-all"? Observations and Expectations of NLG Systems Across Identity-Related Language Features. (arXiv:2310.15398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15398
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25200;&#21160;&#19981;&#21516;&#31867;&#22411;&#30340;&#36523;&#20221;&#30456;&#20851;&#35821;&#35328;&#29305;&#24449;&#65292;&#30740;&#31350;&#25506;&#32034;&#20102;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#20844;&#24179;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#30740;&#31350;&#32467;&#26524;&#21457;&#29616;&#65292;&#36866;&#24212;&#30340;&#21160;&#26426;&#21253;&#25324;&#31038;&#20250;&#35268;&#33539;&#12289;&#25991;&#21270;&#24046;&#24322;&#12289;&#29305;&#23450;&#29305;&#24449;&#20449;&#24687;&#21644;&#36866;&#24212;&#24615;&#65307;&#19981;&#21464;&#24615;&#30340;&#21160;&#26426;&#21253;&#25324;&#25903;&#25345;&#35268;&#23450;&#20027;&#20041;&#30340;&#35266;&#28857;&#12289;&#23558;&#36866;&#24212;&#35270;&#20026;&#19981;&#24517;&#35201;&#36807;&#31243;&#65292;&#24182;&#23545;&#38169;&#35823;&#20551;&#35774;&#25345;&#35880;&#24910;&#24577;&#24230;&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#23450;&#20041;&#20844;&#24179;&#30456;&#20851;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#20160;&#20040;&#26500;&#25104;&#36866;&#24403;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#34892;&#20026;&#30340;&#20844;&#24179;&#30456;&#20851;&#20551;&#35774;&#65292;&#20174;&#19981;&#21464;&#24615;&#65292;&#21363;&#26399;&#26395;&#31995;&#32479;&#23545;&#31038;&#20250;&#32676;&#20307;&#20570;&#20986;&#30456;&#21516;&#30340;&#21709;&#24212;&#65292;&#21040;&#36866;&#24212;&#24615;&#65292;&#21363;&#26399;&#26395;&#31995;&#32479;&#22312;&#19981;&#21516;&#31038;&#20250;&#32676;&#20307;&#20043;&#38388;&#20135;&#29983;&#19981;&#21516;&#21709;&#24212;&#65292;&#23384;&#22312;&#19981;&#21516;&#35266;&#28857;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#20116;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#25200;&#21160;&#19981;&#21516;&#31867;&#22411;&#30340;&#19982;&#36523;&#20221;&#30456;&#20851;&#30340;&#35821;&#35328;&#29305;&#24449;&#65288;&#22995;&#21517;&#12289;&#35282;&#33394;&#12289;&#22320;&#28857;&#12289;&#26041;&#35328;&#21644;&#39118;&#26684;&#65289;&#26469;&#38416;&#26126;&#19981;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#25105;&#20204;&#27010;&#36848;&#20102;&#20154;&#20204;&#23545;&#31995;&#32479;&#34892;&#20026;&#30340;&#26399;&#26395;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20004;&#31181;&#23545;&#31435;&#20294;&#24120;&#35265;&#30340;&#20551;&#35774;&#30340;&#28508;&#22312;&#35686;&#31034;&#12290;&#25105;&#20204;&#21457;&#29616;&#36866;&#24212;&#30340;&#21160;&#26426;&#21253;&#25324;&#31038;&#20250;&#35268;&#33539;&#12289;&#25991;&#21270;&#24046;&#24322;&#12289;&#29305;&#23450;&#29305;&#24449;&#20449;&#24687;&#21644;&#36866;&#24212;&#24615;&#65307;&#19981;&#21464;&#24615;&#30340;&#21160;&#26426;&#21253;&#25324;&#25903;&#25345;&#35268;&#23450;&#20027;&#20041;&#30340;&#35266;&#28857;&#12289;&#23558;&#36866;&#24212;&#35270;&#20026; NLG &#31995;&#32479;&#38590;&#20197;&#36866;&#24403;&#23436;&#25104;&#30340;&#19981;&#24517;&#35201;&#36807;&#31243;&#65292;&#24182;&#23545;&#38169;&#35823;&#20551;&#35774;&#25345;&#35880;&#24910;&#24577;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#26174;&#20102;&#20851;&#20110;&#23450;&#20041;&#20844;&#24179;&#30456;&#20851;NLG&#31995;&#32479;&#34892;&#20026;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness-related assumptions about what constitutes appropriate NLG system behaviors range from invariance, where systems are expected to respond identically to social groups, to adaptation, where responses should instead vary across them. We design and conduct five case studies, in which we perturb different types of identity-related language features (names, roles, locations, dialect, and style) in NLG system inputs to illuminate tensions around invariance and adaptation. We outline people's expectations of system behaviors, and surface potential caveats of these two contrasting yet commonly-held assumptions. We find that motivations for adaptation include social norms, cultural differences, feature-specific information, and accommodation; motivations for invariance include perspectives that favor prescriptivism, view adaptation as unnecessary or too difficult for NLG systems to do appropriately, and are wary of false assumptions. Our findings highlight open challenges around definin
&lt;/p&gt;</description></item><item><title>DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15393</link><description>&lt;p&gt;
DoGE: &#20351;&#29992;&#27867;&#21270;&#20272;&#35745;&#36827;&#34892;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
DoGE: Domain Reweighting with Generalization Estimation. (arXiv:2310.15393v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15393
&lt;/p&gt;
&lt;p&gt;
DoGE&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27867;&#21270;&#20272;&#35745;&#30340;&#39046;&#22495;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#26799;&#24230;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#27599;&#20010;&#39046;&#22495;&#23545;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#65292;&#37325;&#26032;&#35843;&#25972;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#19981;&#21516;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#25968;&#25454;&#35821;&#26009;&#24211;&#30340;&#35206;&#30422;&#33539;&#22260;&#21644;&#32452;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#26377;&#30528;&#37325;&#35201;&#24433;&#21709;&#12290;&#20256;&#32479;&#19978;&#65292;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#30001;&#21508;&#31181;&#26469;&#28304;&#39046;&#22495;&#65288;&#22914;CommonCrawl&#12289;Wikipedia&#12289;Github&#31561;&#65289;&#25353;&#29031;&#29305;&#23450;&#30340;&#37319;&#26679;&#27010;&#29575;&#65288;&#39046;&#22495;&#26435;&#37325;&#65289;&#32452;&#25104;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#32570;&#20047;&#19968;&#31181;&#22522;&#20110;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#20248;&#21270;&#39046;&#22495;&#26435;&#37325;&#30340;&#21407;&#21017;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DOmain reweighting with Generalization Estimation&#65288;DoGE&#65289;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#37325;&#26032;&#35843;&#25972;&#20102;&#27599;&#20010;&#39046;&#22495;&#30340;&#37319;&#26679;&#27010;&#29575;&#65292;&#26681;&#25454;&#23427;&#23545;&#26368;&#32456;&#27867;&#21270;&#30446;&#26631;&#30340;&#36129;&#29486;&#36827;&#34892;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#27867;&#21270;&#20272;&#35745;&#20989;&#25968;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#35757;&#32451;&#20102;&#19968;&#20010;&#23567;&#35268;&#27169;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#33719;&#21462;&#37325;&#26032;&#21152;&#26435;&#30340;&#39046;&#22495;&#26435;&#37325;&#12290;&#22312;&#27599;&#19968;&#27493;&#20013;&#65292;&#36890;&#36807;&#38236;&#20687;&#19979;&#38477;&#27861;&#26356;&#26032;&#39046;&#22495;&#26435;&#37325;&#20197;&#26368;&#22823;&#21270;&#25972;&#20307;&#30340;&#27867;&#21270;&#22686;&#30410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#33719;&#24471;&#30340;&#39046;&#22495;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#23436;&#25972;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The coverage and composition of the pretraining data corpus significantly impacts the generalization ability of large language models. Conventionally, the pretraining corpus is composed of various source domains (e.g. CommonCrawl, Wikipedia, Github etc.) according to certain sampling probabilities (domain weights). However, current methods lack a principled way to optimize domain weights for ultimate goal for generalization. We propose DOmain reweighting with Generalization Estimation (DoGE), where we reweigh the sampling probability from each domain based on its contribution to the final generalization objective assessed by a gradient-based generalization estimation function. First, we train a small-scale proxy model with a min-max optimization to obtain the reweighted domain weights. At each step, the domain weights are updated to maximize the overall generalization gain by mirror descent. Finally we use the obtained domain weights to train a larger scale full-size language model. On
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21487;&#32422;&#35838;&#31243;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15389</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#19981;&#21487;&#32422;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Irreducible Curriculum for Language Model Pretraining. (arXiv:2310.15389v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21487;&#32422;&#35838;&#31243;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#65292;&#24182;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#65292;&#20174;&#32780;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#35299;&#20915;&#20102;&#20256;&#32479;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;&#25968;&#25454;&#36873;&#25321;&#21644;&#35838;&#31243;&#35774;&#35745;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21482;&#26377;&#23569;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#35757;&#32451;&#19978;&#26174;&#31034;&#20986;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#26696;&#26356;&#20851;&#27880;&#39046;&#22495;&#32423;&#21035;&#30340;&#36873;&#25321;&#65292;&#24573;&#35270;&#20102;&#27599;&#20010;&#21333;&#29420;&#35757;&#32451;&#28857;&#30340;&#26356;&#32454;&#31890;&#24230;&#30340;&#36129;&#29486;&#12290;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#24212;&#29992;&#20256;&#32479;&#30340;&#25968;&#25454;&#28857;&#36873;&#25321;&#26041;&#27861;&#24456;&#22256;&#38590;&#65306;&#22823;&#22810;&#25968;&#22312;&#32447;&#25209;&#36873;&#25321;&#26041;&#27861;&#25191;&#34892;&#20004;&#27425;&#21069;&#21521;&#25110;&#21518;&#21521;&#20256;&#36882;&#65292;&#36825;&#20250;&#24102;&#26469;&#24040;&#22823;&#30340;&#39069;&#22806;&#25104;&#26412;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21487;&#32422;&#35838;&#31243;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#30340;&#35838;&#31243;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20248;&#20808;&#36873;&#25321;&#20855;&#26377;&#26356;&#39640;&#23398;&#20064;&#33021;&#21147;&#30340;&#26679;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#36807;&#39640;&#30340;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#65292;&#25105;&#20204;&#20351;&#29992;&#23567;&#35268;&#27169;&#20195;&#29702;&#27169;&#22411;&#27169;&#25311;&#26679;&#26412;&#20002;&#22833;&#27839;&#20027;&#27169;&#22411;&#35757;&#32451;&#36712;&#36857;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#22312;RedPajama-1B&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35838;&#31243;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#25345;&#32493;&#25913;&#36827;&#27169;&#22411;&#22312;&#39564;&#35777;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic data selection and curriculum design for training large language models is challenging, with only a few existing methods showing improvements over standard training. Furthermore, current schemes focus on domain-level selection, overlooking the more fine-grained contributions of each individual training point. It is difficult to apply traditional datapoint selection methods on large language models: most online batch selection methods perform two-times forward or backward passes, which introduces considerable extra costs with large-scale models. To mitigate these obstacles, we propose irreducible curriculum as a curriculum learning algorithm for language model pretraining, which prioritizes samples with higher learnability. Specifically, to avoid prohibitive extra computation overhead, we simulate the sample loss along the main model's training trajectory using a small-scale proxy model. Our experiments on the RedPajama-1B dataset demonstrate a consistent improvement on valida
&lt;/p&gt;</description></item><item><title>GD-COMET&#26159;&#19968;&#31181;&#22320;&#29702;&#22810;&#26679;&#24615;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#22806;&#22312;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#20351;NLP&#26356;&#21152;&#21253;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.15383</link><description>&lt;p&gt;
GD-COMET: &#19968;&#31181;&#22320;&#29702;&#22810;&#26679;&#24615;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GD-COMET: A Geo-Diverse Commonsense Inference Model. (arXiv:2310.15383v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15383
&lt;/p&gt;
&lt;p&gt;
GD-COMET&#26159;&#19968;&#31181;&#22320;&#29702;&#22810;&#26679;&#24615;&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#21644;&#22806;&#22312;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#21457;&#23637;&#65292;&#20351;NLP&#26356;&#21152;&#21253;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#26085;&#24120;&#29983;&#27963;&#65292;&#35774;&#35745;&#33021;&#22815;&#28385;&#36275;&#19981;&#21516;&#32972;&#26223;&#29992;&#25143;&#38656;&#27714;&#12289;&#20855;&#26377;&#25991;&#21270;&#24847;&#35782;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GD-COMET&#65292;&#36825;&#26159;COMET&#24120;&#35782;&#25512;&#29702;&#27169;&#22411;&#30340;&#22320;&#29702;&#22810;&#26679;&#24615;&#29256;&#26412;&#12290;GD-COMET&#36229;&#36234;&#35199;&#26041;&#24120;&#35782;&#30693;&#35782;&#65292;&#33021;&#22815;&#29983;&#25104;&#28041;&#21450;&#24191;&#27867;&#25991;&#21270;&#32972;&#26223;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;5&#31181;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#36827;&#34892;&#20840;&#38754;&#30340;&#20154;&#24037;&#35780;&#20272;&#20197;&#21450;&#22320;&#29702;&#22810;&#26679;&#24615;&#20219;&#21153;&#30340;&#22806;&#22312;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;GD-COMET&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;GD-COMET&#33021;&#22815;&#25429;&#25417;&#21644;&#29983;&#25104;&#20855;&#26377;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#24120;&#35782;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#12289;&#20419;&#36827;NLP&#26356;&#21152;&#21253;&#23481;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing integration of AI into everyday life, it's becoming crucial to design AI systems that serve users from diverse backgrounds by making them culturally aware. In this paper, we present GD-COMET, a geo-diverse version of the COMET commonsense inference model. GD-COMET goes beyond Western commonsense knowledge and is capable of generating inferences pertaining to a broad range of cultures. We demonstrate the effectiveness of GD-COMET through a comprehensive human evaluation across 5 diverse cultures, as well as extrinsic evaluation on a geo-diverse task. The evaluation shows that GD-COMET captures and generates culturally nuanced commonsense knowledge, demonstrating its potential to benefit NLP applications across the board and contribute to making NLP more inclusive.
&lt;/p&gt;</description></item><item><title>&#25968;&#25454;&#28246;&#26159;&#31649;&#29702;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#38450;&#27490;&#25968;&#25454;&#28246;&#25104;&#20026;&#26080;&#27861;&#25805;&#20316;&#30340;&#25968;&#25454;&#27836;&#27901;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#38142;&#25509;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#30340;&#24847;&#20041;&#21644;&#35821;&#20041;&#12290;&#36825;&#31181;&#35821;&#20041;&#23618;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#31649;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#25972;&#21512;&#38382;&#39064;&#65292;&#20351;&#25968;&#25454;&#35775;&#38382;&#26356;&#20855;&#34920;&#36798;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15373</link><description>&lt;p&gt;
&#25968;&#25454;&#28246;&#20013;&#30340;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Semantic Data Management in Data Lakes. (arXiv:2310.15373v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15373
&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#28246;&#26159;&#31649;&#29702;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#20026;&#20102;&#38450;&#27490;&#25968;&#25454;&#28246;&#25104;&#20026;&#26080;&#27861;&#25805;&#20316;&#30340;&#25968;&#25454;&#27836;&#27901;&#65292;&#25105;&#20204;&#21487;&#20197;&#37319;&#29992;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#38142;&#25509;&#65292;&#20026;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#30340;&#24847;&#20041;&#21644;&#35821;&#20041;&#12290;&#36825;&#31181;&#35821;&#20041;&#23618;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#31649;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#25968;&#25454;&#25972;&#21512;&#38382;&#39064;&#65292;&#20351;&#25968;&#25454;&#35775;&#38382;&#26356;&#20855;&#34920;&#36798;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#28246;&#20316;&#20026;&#31649;&#29702;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#36827;&#34892;&#29616;&#20195;&#25968;&#25454;&#20998;&#26512;&#30340;&#19968;&#31181;&#26041;&#27861;&#20986;&#29616;&#12290;&#38450;&#27490;&#25968;&#25454;&#28246;&#21464;&#25104;&#26080;&#27861;&#25805;&#20316;&#30340;&#25968;&#25454;&#27836;&#27901;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#12290;&#19968;&#20123;&#26041;&#27861;&#25552;&#35758;&#22522;&#20110;&#38142;&#25509;&#25968;&#25454;&#21407;&#21017;&#23558;&#20803;&#25968;&#25454;&#19982;&#30693;&#35782;&#22270;&#35889;&#30456;&#38142;&#25509;&#65292;&#20197;&#20026;&#28246;&#20013;&#30340;&#25968;&#25454;&#25552;&#20379;&#26356;&#22810;&#30340;&#24847;&#20041;&#21644;&#35821;&#20041;&#12290;&#36825;&#26679;&#30340;&#35821;&#20041;&#23618;&#19981;&#20165;&#21487;&#20197;&#29992;&#20110;&#25968;&#25454;&#31649;&#29702;&#65292;&#36824;&#21487;&#20197;&#35299;&#20915;&#26469;&#33258;&#24322;&#26500;&#26469;&#28304;&#30340;&#25968;&#25454;&#25972;&#21512;&#38382;&#39064;&#65292;&#20197;&#20351;&#25968;&#25454;&#35775;&#38382;&#26356;&#20855;&#34920;&#36798;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#22312;&#25968;&#25454;&#28246;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#21644;&#22823;&#25968;&#25454;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#22238;&#39038;&#20102;&#26368;&#36817;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;(i)&#22522;&#26412;&#30340;&#35821;&#20041;&#25968;&#25454;&#31649;&#29702;&#65292;(ii)&#22312;&#25968;&#25454;&#28246;&#20013;&#20016;&#23500;&#20803;&#25968;&#25454;&#30340;&#35821;&#20041;&#24314;&#27169;&#26041;&#27861;&#65292;&#20197;&#21450;(iii)&#22522;&#20110;&#26412;&#20307;&#30340;&#25968;&#25454;&#35775;&#38382;&#26041;&#27861;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#28085;&#30422;&#20102;&#20027;&#35201;&#25216;&#26415;&#21450;&#20854;&#32972;&#26223;&#65292;&#24182;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, data lakes emerged as away to manage large amounts of heterogeneous data for modern data analytics. One way to prevent data lakes from turning into inoperable data swamps is semantic data management. Some approaches propose the linkage of metadata to knowledge graphs based on the Linked Data principles to provide more meaning and semantics to the data in the lake. Such a semantic layer may be utilized not only for data management but also to tackle the problem of data integration from heterogeneous sources, in order to make data access more expressive and interoperable. In this survey, we review recent approaches with a specific focus on the application within data lake systems and scalability to Big Data. We classify the approaches into (i) basic semantic data management, (ii) semantic modeling approaches for enriching metadata in data lakes, and (iii) methods for ontologybased data access. In each category, we cover the main techniques and their background, and compa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15372</link><description>&lt;p&gt;
EpiK-Eval&#65306;&#23558;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35748;&#35782;&#27169;&#22411;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EpiK-Eval: Evaluation for Language Models as Epistemic Models. (arXiv:2310.15372v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15372
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#26041;&#27861;EpiK-Eval&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#21457;&#29616;&#24403;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#23384;&#22312;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#26041;&#27861;&#30340;&#24314;&#35758;&#65292;&#20197;&#22823;&#24133;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20316;&#29992;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#23613;&#31649;&#23427;&#20204;&#26085;&#30410;&#26222;&#21450;&#65292;&#20294;&#23427;&#20204;&#22312;&#20174;&#19981;&#21516;&#35757;&#32451;&#25991;&#26723;&#20013;&#25972;&#21512;&#30693;&#35782;&#30340;&#33021;&#21147;&#8212;&#8212;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#37117;&#26159;&#20851;&#38190;&#33021;&#21147;&#8212;&#8212;&#20173;&#26410;&#24471;&#21040;&#25506;&#32034;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;LLMs&#22312;&#20854;&#21442;&#25968;&#31354;&#38388;&#20869;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#31181;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;EpiK-Eval&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#31572;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#20174;&#20998;&#21106;&#30340;&#21465;&#36848;&#20013;&#26500;&#24314;&#19968;&#31181;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#30693;&#35782;&#34920;&#31034;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23545;&#21508;&#31181;LLMs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#30340;&#26174;&#33879;&#24369;&#28857;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#32570;&#28857;&#28304;&#20110;&#29616;&#26377;&#35757;&#32451;&#30446;&#26631;&#30340;&#22266;&#26377;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20027;&#24352;&#25913;&#36827;&#30693;&#35782;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#36825;&#26377;&#28508;&#21147;&#26174;&#33879;&#25552;&#39640;LLMs&#30340;&#25972;&#20307;&#25928;&#26524;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the age of artificial intelligence, the role of large language models (LLMs) is becoming increasingly central. Despite their growing prevalence, their capacity to consolidate knowledge from different training documents - a crucial ability in numerous applications - remains unexplored. This paper presents the first study examining the capability of LLMs to effectively combine such information within their parameter space. We introduce EpiK-Eval, a novel question-answering benchmark tailored to evaluate LLMs' proficiency in formulating a coherent and consistent knowledge representation from segmented narratives. Evaluations across various LLMs reveal significant weaknesses in this domain. We contend that these shortcomings stem from the intrinsic nature of prevailing training objectives. Consequently, we advocate for refining the approach towards knowledge consolidation, as it harbors the potential to dramatically improve their overall effectiveness and performance. The findings from 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#20135;&#29983;&#24187;&#35273;&#30340;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#36755;&#20986;&#27809;&#26377;&#21463;&#21040;&#35777;&#25454;&#25903;&#25345;&#30340;&#20027;&#24352;&#30340;&#32422;&#26463;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#24863;&#30693;&#12289;&#20869;&#28085;&#21644;&#22806;&#24310;&#23398;&#20064;&#26469;&#32422;&#26463;LLMs&#20197;&#29983;&#25104;&#28385;&#36275;&#35777;&#25454;&#38381;&#21512;&#24615;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2310.15355</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;LLM&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#20197;&#21450;&#22914;&#20309;&#33719;&#24471;&#65288;&#35777;&#25454;&#24615;&#30340;&#65289;&#38381;&#21512;&#24615;&#65306;&#29992;&#20110;&#24544;&#23454;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#24863;&#30693;&#12289;&#20869;&#28085;&#21644;&#22806;&#24310;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Why LLMs Hallucinate, and How to Get (Evidential) Closure: Perceptual, Intensional, and Extensional Learning for Faithful Natural Language Generation. (arXiv:2310.15355v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#20135;&#29983;&#24187;&#35273;&#30340;&#21407;&#22240;&#26159;&#22240;&#20026;&#23427;&#20204;&#30340;&#36755;&#20986;&#27809;&#26377;&#21463;&#21040;&#35777;&#25454;&#25903;&#25345;&#30340;&#20027;&#24352;&#30340;&#32422;&#26463;&#65292;&#24182;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#24863;&#30693;&#12289;&#20869;&#28085;&#21644;&#22806;&#24310;&#23398;&#20064;&#26469;&#32422;&#26463;LLMs&#20197;&#29983;&#25104;&#28385;&#36275;&#35777;&#25454;&#38381;&#21512;&#24615;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#20026;&#20160;&#20040;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#36755;&#20986;&#27809;&#26377;&#21463;&#21040;&#20855;&#22791;&#35777;&#25454;&#25903;&#25345;&#30340;&#20027;&#24352;&#30340;&#32422;&#26463;&#65292;&#36825;&#31181;&#26465;&#20214;&#34987;&#31216;&#20026;&#35777;&#25454;&#38381;&#21512;&#12290;&#22312;&#26631;&#20934;&#30340;&#31070;&#32463;&#27010;&#29575;&#35821;&#35328;&#27169;&#22411;&#35774;&#32622;&#20013;&#65292;&#24182;&#19981;&#33021;&#20174;&#32479;&#35745;&#19978;&#36776;&#21035;&#20986;&#20851;&#20110;&#21477;&#23376;&#30495;&#20266;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#19981;&#33021;&#20197;&#27492;&#20026;&#26465;&#20214;&#29983;&#25104;&#26032;&#30340;&#23383;&#31526;&#20018;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#32422;&#26463;LLMs&#20197;&#20135;&#29983;&#28385;&#36275;&#35777;&#25454;&#38381;&#21512;&#24615;&#30340;&#36755;&#20986;&#12290;&#22810;&#27169;&#24577;LLM&#24517;&#39035;&#23398;&#20064;&#22806;&#37096;&#19990;&#30028;&#65288;&#24863;&#30693;&#23398;&#20064;&#65289;&#65307;&#23427;&#24517;&#39035;&#23398;&#20064;&#20174;&#23383;&#31526;&#20018;&#21040;&#19990;&#30028;&#29366;&#24577;&#30340;&#26144;&#23556;&#65288;&#22806;&#24310;&#23398;&#20064;&#65289;&#65307;&#24182;&#19988;&#65292;&#20026;&#20102;&#22312;&#36229;&#36234;&#19968;&#32452;&#35777;&#25454;&#26102;&#23454;&#29616;&#27969;&#30021;&#24615;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#20174;&#23383;&#31526;&#20018;&#21040;&#23427;&#20204;&#30340;&#21516;&#20041;&#35789;&#30340;&#26144;&#23556;&#65288;&#20869;&#28085;&#23398;&#20064;&#65289;&#12290;&#19968;&#31181;&#21333;&#27169;&#24577;LLM&#30340;&#36755;&#20986;&#24517;&#39035;&#19982;&#39564;&#35777;&#30340;&#35777;&#25454;&#38598;&#20013;&#30340;&#23383;&#31526;&#20018;&#24847;&#20041;&#30456;&#21516;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21551;&#21457;&#24335;&#36807;&#31243;&#8212;&#8212;&#23398;&#20064;-&#32993;&#35328;&#20081;&#35821;-&#20462;&#21098;&#65288;Learn-Babble-Prune&#65289;&#65292;&#36890;&#36807;&#25298;&#32477;&#19981;&#21516;&#20041;&#30340;&#36755;&#20986;&#65292;&#20174;LLM&#20013;&#20135;&#29983;&#24544;&#23454;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that LLMs hallucinate because their output is not constrained to be synonymous with claims for which they have evidence: a condition that we call evidential closure. Information about the truth or falsity of sentences is not statistically identified in the standard neural probabilistic language model setup, and so cannot be conditioned on to generate new strings. We then show how to constrain LLMs to produce output that does satisfy evidential closure. A multimodal LLM must learn about the external world (perceptual learning); it must learn a mapping from strings to states of the world (extensional learning); and, to achieve fluency when generalizing beyond a body of evidence, it must learn mappings from strings to their synonyms (intensional learning). The output of a unimodal LLM must be synonymous with strings in a validated evidence set. Finally, we present a heuristic procedure, Learn-Babble-Prune, that yields faithful output from an LLM by rejecting output that is not syn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#65288;MFT&#65289;&#20316;&#20026;&#20998;&#26512;&#24037;&#20855;&#65292;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#36947;&#24503;&#20215;&#20540;&#35266;&#20135;&#29983;&#20102;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#36947;&#24503;&#22522;&#30784;&#21644;&#25919;&#27835;&#20542;&#21521;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#30340;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#25239;&#36873;&#25321;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;LLMs&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2310.15337</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36947;&#24503;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Moral Foundations of Large Language Models. (arXiv:2310.15337v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#65288;MFT&#65289;&#20316;&#20026;&#20998;&#26512;&#24037;&#20855;&#65292;&#30740;&#31350;&#20102;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#36947;&#24503;&#20215;&#20540;&#35266;&#20135;&#29983;&#20102;&#20559;&#35265;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#36947;&#24503;&#22522;&#30784;&#21644;&#25919;&#27835;&#20542;&#21521;&#30340;&#20851;&#32852;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;LLMs&#30340;&#20559;&#35265;&#22312;&#19981;&#21516;&#30340;&#25552;&#31034;&#19978;&#19979;&#25991;&#20013;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23545;&#25239;&#36873;&#25321;&#25552;&#31034;&#21487;&#20197;&#24341;&#23548;LLMs&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36947;&#24503;&#22522;&#30784;&#29702;&#35770;&#65288;MFT&#65289;&#26159;&#19968;&#31181;&#24515;&#29702;&#35780;&#20272;&#24037;&#20855;&#65292;&#23558;&#20154;&#31867;&#36947;&#24503;&#25512;&#29702;&#20998;&#35299;&#20026;&#21253;&#25324;&#20851;&#24515;/&#20260;&#23475;&#12289;&#33258;&#30001;/&#21387;&#36843;&#21644;&#23562;&#20005;/&#22549;&#33853;&#31561;&#20116;&#20010;&#22240;&#32032;&#65288;Graham&#31561;&#65292;2009&#65289;&#12290;&#20154;&#20204;&#22312;&#20316;&#20986;&#36947;&#24503;&#20915;&#31574;&#26102;&#22312;&#36825;&#20123;&#32500;&#24230;&#19978;&#30340;&#26435;&#37325;&#19981;&#21516;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#20182;&#20204;&#30340;&#25991;&#21270;&#32972;&#26223;&#21644;&#25919;&#27835;&#24847;&#35782;&#24418;&#24577;&#12290;&#30001;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20182;&#20204;&#21487;&#33021;&#21453;&#26144;&#20102;&#36825;&#20123;&#35821;&#26009;&#24211;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;&#26412;&#25991;&#20197;MFT&#20026;&#35270;&#35282;&#65292;&#20998;&#26512;&#27969;&#34892;&#30340;LLMs&#26159;&#21542;&#23545;&#19968;&#31995;&#21015;&#29305;&#23450;&#30340;&#36947;&#24503;&#20215;&#20540;&#35266;&#20135;&#29983;&#20102;&#20559;&#35265;&#12290;&#25105;&#20204;&#20998;&#26512;&#24050;&#30693;&#30340;LLMs&#65292;&#21457;&#29616;&#23427;&#20204;&#23637;&#29616;&#20102;&#29305;&#23450;&#30340;&#36947;&#24503;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#19982;&#20154;&#31867;&#36947;&#24503;&#22522;&#30784;&#21644;&#25919;&#27835;&#20542;&#21521;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#36825;&#20123;&#20559;&#35265;&#30340;&#19968;&#33268;&#24615;&#65292;&#25110;&#32773;&#23427;&#20204;&#22312;&#27169;&#22411;&#34987;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#20013;&#26159;&#21542;&#26377;&#24456;&#22823;&#24046;&#24322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23545;&#25239;&#22320;&#36873;&#25321;&#25552;&#31034;&#26469;&#40723;&#21169;LLMs&#20135;&#29983;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moral foundations theory (MFT) is a psychological assessment tool that decomposes human moral reasoning into five factors, including care/harm, liberty/oppression, and sanctity/degradation (Graham et al., 2009). People vary in the weight they place on these dimensions when making moral decisions, in part due to their cultural upbringing and political ideology. As large language models (LLMs) are trained on datasets collected from the internet, they may reflect the biases that are present in such corpora. This paper uses MFT as a lens to analyze whether popular LLMs have acquired a bias towards a particular set of moral values. We analyze known LLMs and find they exhibit particular moral foundations, and show how these relate to human moral foundations and political affiliations. We also measure the consistency of these biases, or whether they vary strongly depending on the context of how the model is prompted. Finally, we show that we can adversarially select prompts that encourage the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36890;&#25165;&#27169;&#22411;&#25351;&#23548;&#35843;&#20248;&#34701;&#20837;&#21040;&#19987;&#23478;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21161;&#20110;&#26500;&#24314;&#19987;&#23478;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22312;&#20219;&#21153;&#35206;&#30422;&#24191;&#27867;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#25972;&#21512;&#36890;&#25165;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#20248;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15326</link><description>&lt;p&gt;
&#19987;&#23478;&#36824;&#26159;&#36890;&#25165;&#65311;&#38024;&#23545;&#29305;&#23450;NLP&#20219;&#21153;&#30340;&#25351;&#23548;&#35843;&#20248;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Specialist or Generalist? Instruction Tuning for Specific NLP Tasks. (arXiv:2310.15326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#36890;&#25165;&#27169;&#22411;&#25351;&#23548;&#35843;&#20248;&#34701;&#20837;&#21040;&#19987;&#23478;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21161;&#20110;&#26500;&#24314;&#19987;&#23478;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22312;&#20219;&#21153;&#35206;&#30422;&#24191;&#27867;&#19988;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#25972;&#21512;&#36890;&#25165;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#20248;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21516;&#26102;&#25191;&#34892;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;&#34429;&#28982;&#25351;&#23548;&#35843;&#20248;&#24050;&#34987;&#35777;&#26126;&#26159;&#23558;LLMs&#36716;&#21270;&#20026;&#36890;&#25165;&#27169;&#22411;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#28982;&#33853;&#21518;&#20110;&#19987;&#23478;&#27169;&#22411;&#65292;&#19987;&#23478;&#27169;&#22411;&#19987;&#38376;&#20026;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#24191;&#35206;&#30422;&#30340;&#36890;&#25165;&#27169;&#22411;&#25351;&#23548;&#35843;&#20248;&#34701;&#20837;&#21040;&#19987;&#23478;&#27169;&#22411;&#20013;&#26159;&#21542;&#26377;&#21161;&#20110;&#26500;&#24314;&#19987;&#23478;&#27169;&#22411;&#12290;&#25105;&#20204;&#20551;&#35774;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#29305;&#24322;&#24615;&#21644;&#25216;&#33021;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;&#22235;&#20010;&#20855;&#26377;&#19981;&#21516;&#35206;&#30422;&#33539;&#22260;&#30340;&#30446;&#26631;&#20219;&#21153;&#65292;&#21457;&#29616;&#24403;&#20219;&#21153;&#35206;&#30422;&#24191;&#27867;&#26102;&#65292;&#25972;&#21512;&#36890;&#25165;&#27169;&#22411;&#30340;&#25351;&#23548;&#35843;&#20248;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;&#32780;&#24403;&#20219;&#21153;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#36825;&#31181;&#25928;&#26524;&#23588;&#20026;&#26174;&#33879;&#12290;&#36827;&#19968;&#27493;&#25506;&#31350;&#38024;&#23545;&#19981;&#21516;&#33021;&#21147;&#30340;&#19977;&#20010;&#30446;&#26631;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models (LLMs) to simultaneously perform a wide range of natural language processing (NLP) tasks has been the subject of extensive research. Although instruction tuning has proven to be a data-efficient method for transforming LLMs into such generalist models, their performance still lags behind specialist models trained exclusively for specific tasks. In this paper, we investigate whether incorporating broad-coverage generalist instruction tuning can contribute to building a specialist model. We hypothesize that its efficacy depends on task specificity and skill requirements. Our experiments assess four target tasks with distinct coverage levels, revealing that integrating generalist instruction tuning consistently enhances model performance when the task coverage is broad. The effect is particularly pronounced when the amount of task-specific training data is limited. Further investigation into three target tasks focusing on different capabilities demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32452;&#21512;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#35780;&#20272;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#23545;LXMERT&#36827;&#34892;&#24494;&#35843;&#26102;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;LXMERT&#27169;&#22411;&#30340;&#21387;&#32553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#25439;&#22833;3%&#30340;&#31934;&#24230;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36890;&#36807;&#21098;&#26525;&#26041;&#27861;&#23558;LXMERT&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;40%-60%&#12290;</title><link>http://arxiv.org/abs/2310.15325</link><description>&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;LXMERT&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
LXMERT Model Compression for Visual Question Answering. (arXiv:2310.15325v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32452;&#21512;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#24182;&#35780;&#20272;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#23545;LXMERT&#36827;&#34892;&#24494;&#35843;&#26102;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#65292;&#30740;&#31350;&#20102;LXMERT&#27169;&#22411;&#30340;&#21387;&#32553;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#25439;&#22833;3%&#30340;&#31934;&#24230;&#19979;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#36890;&#36807;&#21098;&#26525;&#26041;&#27861;&#23558;LXMERT&#27169;&#22411;&#22823;&#23567;&#20943;&#23567;40%-60%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;LXMERT&#22312;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#23398;&#20064;&#36328;&#27169;&#24577;&#34920;&#31034;&#21464;&#24471;&#27969;&#34892;&#12290;&#26681;&#25454;&#20013;&#24425;&#31080;&#20551;&#35828;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#20013;&#21253;&#21547;&#21487;&#21333;&#29420;&#35757;&#32451;&#20197;&#36798;&#21040;&#23436;&#20840;&#24615;&#33021;&#30340;&#36739;&#23567;&#23376;&#32593;&#32476;&#12290;&#26412;&#25991;&#32467;&#21512;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35780;&#20272;&#22312;VQA&#20219;&#21153;&#19978;&#23545;LXMERT&#36827;&#34892;&#24494;&#35843;&#26102;&#26159;&#21542;&#23384;&#22312;&#36825;&#26679;&#30340;&#21487;&#35757;&#32451;&#23376;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#21487;&#20197;&#36827;&#34892;&#22810;&#23569;&#21098;&#26525;&#32780;&#19981;&#20250;&#36896;&#25104;&#26174;&#33879;&#30340;&#31934;&#24230;&#25439;&#22833;&#26469;&#36827;&#34892;&#27169;&#22411;&#22823;&#23567;&#25104;&#26412;&#25910;&#30410;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;LXMERT&#36827;&#34892;40%-60%&#30340;&#26377;&#25928;&#21098;&#26525;&#65292;&#20165;&#20250;&#25439;&#22833;3%&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pretrained models such as LXMERT are becoming popular for learning cross-modal representations on text-image pairs for vision-language tasks. According to the lottery ticket hypothesis, NLP and computer vision models contain smaller subnetworks capable of being trained in isolation to full performance. In this paper, we combine these observations to evaluate whether such trainable subnetworks exist in LXMERT when fine-tuned on the VQA task. In addition, we perform a model size cost-benefit analysis by investigating how much pruning can be done without significant loss in accuracy. Our experiment results demonstrate that LXMERT can be effectively pruned by 40%-60% in size with 3% loss in accuracy.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24187;&#35273;&#21442;&#32771;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.15319</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#25351;&#23548;&#24615;&#35828;&#26126;&#29983;&#25104;&#30340;&#24187;&#35273;&#26816;&#27979;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Hallucination Detection for Grounded Instruction Generation. (arXiv:2310.15319v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#24187;&#35273;&#21442;&#32771;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#27169;&#25311;&#30340;&#20303;&#23429;&#29615;&#22659;&#20013;&#29983;&#25104;&#25351;&#23548;&#20154;&#31867;&#23548;&#33322;&#30340;&#35828;&#26126;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#27169;&#22411;&#23384;&#22312;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#24187;&#35273;&#65306;&#23427;&#20204;&#29983;&#25104;&#19982;&#20154;&#31867;&#36319;&#38543;&#32773;&#22312;&#25551;&#36848;&#30340;&#36335;&#24452;&#19978;&#25191;&#34892;&#25110;&#36935;&#21040;&#30340;&#34892;&#20026;&#25110;&#29289;&#20307;&#19981;&#19968;&#33268;&#30340;&#21442;&#32771;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#22823;&#22411;&#22270;&#20687;-&#25991;&#26412;&#23545;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#23545;&#27604;&#25439;&#22833;&#36827;&#34892;&#24494;&#35843;&#65292;&#26816;&#27979;&#36825;&#20123;&#24187;&#35273;&#21442;&#32771;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#27169;&#22411;&#32988;&#36807;&#20102;&#20960;&#20010;&#22522;&#32447;&#27169;&#22411;&#65292;&#21253;&#25324;&#20351;&#29992;&#30001;&#35828;&#26126;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#30340;&#35789;&#27010;&#29575;&#20197;&#21450;&#22522;&#20110;LSTM&#21644;Transformer&#30340;&#30417;&#30563;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of generating instructions to guide humans to navigate in simulated residential environments. A major issue with current models is hallucination: they generate references to actions or objects that are inconsistent with what a human follower would perform or encounter along the described path. We develop a model that detects these hallucinated references by adopting a model pre-trained on a large corpus of image-text pairs, and fine-tuning it with a contrastive loss that separates correct instructions from instructions containing synthesized hallucinations. Our final model outperforms several baselines, including using word probability estimated by the instruction-generation model, and supervised models based on LSTM and Transformer.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#20837;&#38376;&#32534;&#31243;&#35838;&#31243;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20195;&#30721;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#27169;&#22411;&#36136;&#37327;&#30340;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#21644;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#25945;&#32946;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.15317</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#20837;&#38376;&#32534;&#31243;&#35838;&#31243;&#20013;&#30340;&#20195;&#30721;&#36861;&#36394;&#38382;&#39064;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models in Generating Code-Tracing Questions for Introductory Programming Courses. (arXiv:2310.15317v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15317
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22312;&#20837;&#38376;&#32534;&#31243;&#35838;&#31243;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#20195;&#30721;&#36861;&#36394;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#35780;&#20272;&#27169;&#22411;&#36136;&#37327;&#30340;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#21644;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#65292;&#20026;&#25945;&#32946;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20837;&#38376;&#32534;&#31243;&#35838;&#31243;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#20195;&#30721;&#36861;&#36394;&#38382;&#39064;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#38024;&#23545;GPT4&#35774;&#35745;&#20102;&#30446;&#26631;&#25552;&#31034;&#65292;&#24341;&#23548;&#20854;&#22522;&#20110;&#20195;&#30721;&#29255;&#27573;&#21644;&#25551;&#36848;&#29983;&#25104;&#20195;&#30721;&#36861;&#36394;&#38382;&#39064;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#38382;&#39064;&#19982;&#20154;&#31867;&#19987;&#23478;&#21019;&#24314;&#30340;&#38382;&#39064;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#20851;&#20110;LLMs&#22312;&#29983;&#25104;&#22810;&#26679;&#21270;&#20195;&#30721;&#36861;&#36394;&#38382;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#28508;&#21147;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#20154;&#24037;&#21644;LLM&#29983;&#25104;&#30340;&#36861;&#36394;&#38382;&#39064;&#25968;&#25454;&#38598;&#65292;&#20026;&#25945;&#32946;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30028;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;&#20110;&#23545;LLMs&#22312;&#25945;&#32946;&#29615;&#22659;&#20013;&#28508;&#22312;&#29992;&#36884;&#30340;&#25345;&#32493;&#23545;&#35805;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the application of large language models (LLMs) for generating code-tracing questions in introductory programming courses. We designed targeted prompts for GPT4, guiding it to generate code-tracing questions based on code snippets and descriptions. We established a set of human evaluation metrics to assess the quality of questions produced by the model compared to those created by human experts. Our analysis provides insights into the capabilities and potential of LLMs in generating diverse code-tracing questions. Additionally, we present a unique dataset of human and LLM-generated tracing questions, serving as a valuable resource for both the education and NLP research communities. This work contributes to the ongoing dialogue on the potential uses of LLMs in educational settings.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#25506;&#27979;&#33539;&#24335;&#24212;&#29992;&#20110;&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#34920;&#31034;&#65292;&#21457;&#29616;&#35757;&#32451;&#24471;&#21040;&#30340;&#32534;&#30721;&#22120;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#21442;&#25968;&#26816;&#27979;&#21644;&#26631;&#27880;&#65292;&#20294;&#23545;&#20110;&#20107;&#20214;&#32423;&#20219;&#21153;&#30340;&#25913;&#36827;&#26377;&#38480;&#65292;&#19988;&#23384;&#22312;&#22312;&#36830;&#36143;&#24615;&#21644;&#20107;&#20214;&#31867;&#22411;&#39044;&#27979;&#26041;&#38754;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.15316</link><description>&lt;p&gt;
&#25506;&#27979;&#29992;&#20110;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Probing Representations for Document-level Event Extraction. (arXiv:2310.15316v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#39318;&#27425;&#23558;&#25506;&#27979;&#33539;&#24335;&#24212;&#29992;&#20110;&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#34920;&#31034;&#65292;&#21457;&#29616;&#35757;&#32451;&#24471;&#21040;&#30340;&#32534;&#30721;&#22120;&#23884;&#20837;&#21487;&#20197;&#25552;&#39640;&#21442;&#25968;&#26816;&#27979;&#21644;&#26631;&#27880;&#65292;&#20294;&#23545;&#20110;&#20107;&#20214;&#32423;&#20219;&#21153;&#30340;&#25913;&#36827;&#26377;&#38480;&#65292;&#19988;&#23384;&#22312;&#22312;&#36830;&#36143;&#24615;&#21644;&#20107;&#20214;&#31867;&#22411;&#39044;&#27979;&#26041;&#38754;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#27979;&#20998;&#31867;&#22120;&#26694;&#26550;&#24050;&#34987;&#24212;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21477;&#23376;&#32423;NLP&#20219;&#21153;&#19978;&#12290;&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#25506;&#27979;&#33539;&#24335;&#24212;&#29992;&#20110;&#23398;&#20064;&#29992;&#20110;&#25991;&#26723;&#32423;&#20449;&#24687;&#25277;&#21462;&#65288;IE&#65289;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20843;&#20010;&#23884;&#20837;&#24335;&#25506;&#38024;&#65292;&#29992;&#20110;&#20998;&#26512;&#19982;&#25991;&#26723;&#32423;&#20107;&#20214;&#25277;&#21462;&#30456;&#20851;&#30340;&#34920;&#38754;&#12289;&#35821;&#20041;&#21644;&#20107;&#20214;&#29702;&#35299;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20174;&#19977;&#31181;&#19981;&#21516;&#30340;&#22522;&#20110;LLM&#30340;&#25991;&#26723;&#32423;IE&#26041;&#27861;&#20013;&#23398;&#20064;&#30340;&#27169;&#22411;&#24471;&#21040;&#30340;&#34920;&#31034;&#19978;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#24471;&#21040;&#30340;&#23884;&#20837;&#21487;&#20197;&#36866;&#24230;&#22320;&#25552;&#39640;&#21442;&#25968;&#26816;&#27979;&#21644;&#26631;&#27880;&#65292;&#20294;&#21482;&#33021;&#24494;&#24369;&#22320;&#22686;&#24378;&#20107;&#20214;&#32423;&#20219;&#21153;&#65292;&#23613;&#31649;&#22312;&#26377;&#21161;&#20110;&#36830;&#36143;&#24615;&#21644;&#20107;&#20214;&#31867;&#22411;&#39044;&#27979;&#30340;&#20449;&#24687;&#19978;&#23384;&#22312;&#19968;&#20123;&#26435;&#34913;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#32534;&#30721;&#22120;&#27169;&#22411;&#22312;&#22788;&#29702;&#25991;&#26723;&#38271;&#24230;&#21644;&#36328;&#21477;&#23376;&#30340;&#35805;&#35821;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
The probing classifiers framework has been employed for interpreting deep neural network models for a variety of natural language processing (NLP) applications. Studies, however, have largely focused on sentencelevel NLP tasks. This work is the first to apply the probing paradigm to representations learned for document-level information extraction (IE). We designed eight embedding probes to analyze surface, semantic, and event-understanding capabilities relevant to document-level event extraction. We apply them to the representations acquired by learning models from three different LLM-based document-level IE approaches on a standard dataset. We found that trained encoders from these models yield embeddings that can modestly improve argument detections and labeling but only slightly enhance event-level tasks, albeit trade-offs in information helpful for coherence and event-type prediction. We further found that encoder models struggle with document length and cross-sentence discourse.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#32445;&#32422;&#24066;Airbnb&#25151;&#28304;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#22320;&#28857;&#29305;&#24449;&#21270;&#30456;&#20851;&#30340;&#37325;&#35201;&#35805;&#35821;&#31867;&#21035;&#65292;&#20026;&#25209;&#35780;&#22320;&#21517;&#23398;&#30740;&#31350;&#25351;&#26126;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.15302</link><description>&lt;p&gt;
&#22522;&#20110;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26694;&#26550;&#65306;&#20197;&#32445;&#32422;&#24066;Airbnb&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Toward a Critical Toponymy Framework for Named Entity Recognition: A Case Study of Airbnb in New York City. (arXiv:2310.15302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15302
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#32445;&#32422;&#24066;Airbnb&#25151;&#28304;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#22320;&#28857;&#29305;&#24449;&#21270;&#30456;&#20851;&#30340;&#37325;&#35201;&#35805;&#35821;&#31867;&#21035;&#65292;&#20026;&#25209;&#35780;&#22320;&#21517;&#23398;&#30740;&#31350;&#25351;&#26126;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#22320;&#21517;&#23398;&#36890;&#36807;&#22320;&#21517;&#21644;&#30456;&#20851;&#22320;&#28857;&#30740;&#31350;&#26435;&#21147;&#12289;&#36164;&#26412;&#21644;&#25269;&#25239;&#30340;&#21160;&#24577;&#12290;&#20256;&#32479;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22320;&#21517;&#30340;&#35821;&#20041;&#20869;&#23481;&#21644;&#33258;&#19978;&#32780;&#19979;&#30340;&#21046;&#24230;&#36807;&#31243;&#65292;&#20294;&#36890;&#24120;&#24573;&#35270;&#20102;&#26222;&#36890;&#20154;&#22312;&#26085;&#24120;&#35805;&#35821;&#20013;&#20351;&#29992;&#22320;&#21517;&#30340;&#26041;&#24335;&#65292;&#20197;&#21450;&#19982;&#22320;&#21517;&#30456;&#20851;&#30340;&#22320;&#29702;&#31354;&#38388;&#25551;&#36848;&#31574;&#30053;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;2010&#24180;&#20195;&#30340;47,440&#20010;&#32445;&#32422;&#24066;Airbnb&#25151;&#28304;&#36827;&#34892;&#21019;&#26032;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#26500;&#24314;&#65292;&#24320;&#21457;&#35745;&#31639;&#26041;&#27861;&#20197;&#34913;&#37327;&#25991;&#21270;&#21644;&#32463;&#27982;&#36164;&#26412;&#22914;&#20309;&#24433;&#21709;&#20154;&#20204;&#23545;&#22320;&#28857;&#30340;&#31216;&#35859;&#26041;&#24335;&#12290;&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#19982;&#22320;&#28857;&#29305;&#24449;&#21270;&#30456;&#20851;&#30340;&#37325;&#35201;&#35805;&#35821;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25351;&#21521;&#25209;&#35780;&#22320;&#21517;&#23398;&#30340;&#26032;&#26041;&#21521;&#65292;&#20197;&#21450;&#19968;&#31995;&#21015;&#20197;&#21069;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#35821;&#35328;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Critical toponymy examines the dynamics of power, capital, and resistance through place names and the sites to which they refer. Studies here have traditionally focused on the semantic content of toponyms and the top-down institutional processes that produce them. However, they have generally ignored the ways in which toponyms are used by ordinary people in everyday discourse, as well as the other strategies of geospatial description that accompany and contextualize toponymic reference. Here, we develop computational methods to measure how cultural and economic capital shape the ways in which people refer to places, through a novel annotated dataset of 47,440 New York City Airbnb listings from the 2010s. Building on this dataset, we introduce a new named entity recognition (NER) model able to identify important discourse categories integral to the characterization of place. Our findings point toward new directions for critical toponymy and to a range of previously understudied linguist
&lt;/p&gt;</description></item><item><title>TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15298</link><description>&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TaskDiff: A Similarity Metric for Task-Oriented Conversations. (arXiv:2310.15298v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15298
&lt;/p&gt;
&lt;p&gt;
TaskDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#23545;&#35805;&#32452;&#25104;&#37096;&#20998;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#25968;&#23383;&#21161;&#25163;&#30340;&#26222;&#21450;&#23548;&#33268;&#20102;&#22823;&#37327;&#20250;&#35805;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#29992;&#25143;&#20307;&#39564;&#21644;&#20010;&#24615;&#21270;&#21709;&#24212;&#29983;&#25104;&#12290;&#20351;&#29992;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27969;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#36825;&#20123;&#21161;&#25163;&#36824;&#38656;&#35201;&#39069;&#22806;&#24378;&#35843;&#25552;&#31034;&#24037;&#31243;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25991;&#26412;&#30456;&#20284;&#24230;&#24230;&#37327;&#26159;&#36825;&#31181;&#20998;&#26512;&#21644;&#35780;&#20272;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#26041;&#38754;&#24182;&#19981;&#26377;&#25928;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#29420;&#29305;&#30340;&#23545;&#35805;&#29305;&#24449;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#35805;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;TaskDiff&#65292;&#23427;&#21033;&#29992;&#23545;&#35805;&#30340;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#65288;&#35805;&#35821;&#12289;&#24847;&#22270;&#21644;&#27133;&#65289;&#21450;&#20854;&#20998;&#24067;&#26469;&#35745;&#31639;&#30456;&#20284;&#24230;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;TaskDiff&#22312;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#30456;&#20851;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of conversational digital assistants has resulted in the availability of large amounts of conversational data which can be utilized for improved user experience and personalized response generation. Building these assistants using popular large language models like ChatGPT also require additional emphasis on prompt engineering and evaluation methods. Textual similarity metrics are a key ingredient for such analysis and evaluations. While many similarity metrics have been proposed in the literature, they have not proven effective for task-oriented conversations as they do not take advantage of unique conversational features. To address this gap, we present TaskDiff, a novel conversational similarity metric that utilizes different dialogue components (utterances, intents, and slots) and their distributions to compute similarity. Extensive experimental evaluation of TaskDiff on a benchmark dataset demonstrates its superior performance and improved robustness over other rela
&lt;/p&gt;</description></item><item><title>DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2310.15296</link><description>&lt;p&gt;
DeTiME: &#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLM&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DeTiME: Diffusion-Enhanced Topic Modeling using Encoder-decoder based LLM. (arXiv:2310.15296v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15296
&lt;/p&gt;
&lt;p&gt;
DeTiME&#26159;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#22686;&#24378;&#25193;&#25955;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#23884;&#20837;&#21644;&#20855;&#26377;&#22686;&#24378;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#65292;&#24182;&#33021;&#29983;&#25104;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20805;&#28385;&#27963;&#21147;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTMs&#65289;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;NTMs&#20027;&#35201;&#20351;&#29992;&#26469;&#33258;LLMs&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#36825;&#23545;&#20110;&#32858;&#31867;&#25110;&#20027;&#39064;&#29983;&#25104;&#26469;&#35828;&#24182;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#21517;&#20026;DeTiME&#30340;&#26032;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;DeTiME&#21033;&#29992;&#32534;&#30721;-&#35299;&#30721;&#30340;LLMs&#20135;&#29983;&#39640;&#24230;&#21487;&#32858;&#31867;&#30340;&#23884;&#20837;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#33021;&#22815;&#29983;&#25104;&#26082;&#20855;&#26377;&#20248;&#36234;&#30340;&#32858;&#31867;&#24615;&#21448;&#20855;&#26377;&#22686;&#24378;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#30340;&#20027;&#39064;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#19982;&#24050;&#35782;&#21035;&#20027;&#39064;&#30456;&#20851;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#21452;&#37325;&#21151;&#33021;&#20351;&#29992;&#25143;&#33021;&#22815;&#21516;&#26102;&#39640;&#25928;&#20135;&#29983;&#39640;&#24230;&#32858;&#31867;&#30340;&#20027;&#39064;&#21644;&#30456;&#20851;&#20869;&#23481;&#12290;DeTiME&#30340;&#28508;&#21147;&#36824;&#21253;&#25324;&#29983;&#25104;&#38598;&#32676;&#21270;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the burgeoning field of natural language processing, Neural Topic Models (NTMs) and Large Language Models (LLMs) have emerged as areas of significant research interest. Despite this, NTMs primarily utilize contextual embeddings from LLMs, which are not optimal for clustering or capable for topic generation. Our study addresses this gap by introducing a novel framework named Diffusion-Enhanced Topic Modeling using Encoder-Decoder-based LLMs (DeTiME). DeTiME leverages ncoder-Decoder-based LLMs to produce highly clusterable embeddings that could generate topics that exhibit both superior clusterability and enhanced semantic coherence compared to existing methods. Additionally, by exploiting the power of diffusion, our framework also provides the capability to generate content relevant to the identified topics. This dual functionality allows users to efficiently produce highly clustered topics and related content simultaneously. DeTiME's potential extends to generating clustered embeddi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36719;&#26631;&#31614;&#34920;&#31034;&#21644;&#27133;&#32423;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#36731;&#39046;&#22495;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.15294</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;&#24230;&#37327;&#23398;&#20064;&#29992;&#20110;&#38646;&#26679;&#26412;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling. (arXiv:2310.15294v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#31471;&#21040;&#31471;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#38646;&#26679;&#26412;&#36328;&#39046;&#22495;&#27133;&#22635;&#20805;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36719;&#26631;&#31614;&#34920;&#31034;&#21644;&#27133;&#32423;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20943;&#36731;&#39046;&#22495;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#27133;&#22635;&#20805;&#22312;&#21457;&#23637;&#36807;&#31243;&#20013;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22312;&#35757;&#32451;&#26399;&#38388;&#20174;&#26410;&#35265;&#36807;&#30340;&#26032;&#39046;&#22495;&#24102;&#26469;&#20102;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#30001;&#20110;&#20005;&#37325;&#30340;&#39046;&#22495;&#36716;&#31227;&#65292;&#35782;&#21035;&#24615;&#33021;&#21487;&#33021;&#20250;&#22823;&#22823;&#38477;&#20302;&#12290;&#22823;&#22810;&#25968;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#36807;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#30340;&#20004;&#27425;&#20256;&#36882;&#27969;&#31243;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#23454;&#38469;&#19978;&#65292;&#36825;&#20123;&#20027;&#23548;&#27969;&#27700;&#32447;&#27169;&#22411;&#21487;&#33021;&#22312;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21463;&#38480;&#65292;&#22240;&#20026;&#23384;&#22312;&#38750;&#24182;&#34892;&#25512;&#29702;&#21644;&#26080;&#19978;&#19979;&#25991;&#31163;&#25955;&#26631;&#31614;&#23884;&#20837;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20856;&#22411;&#30340;&#22522;&#20110;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36866;&#24212;&#24615;&#31471;&#21040;&#31471;&#24230;&#37327;&#23398;&#20064;&#26041;&#26696;&#65292;&#38024;&#23545;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#27133;&#22635;&#20805;&#12290;&#32771;&#34385;&#21040;&#31616;&#27905;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#27867;&#21270;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32423;&#32852;&#24335;&#32852;&#21512;&#23398;&#20064;&#26694;&#26550;&#65292;&#37197;&#21512;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36719;&#26631;&#31614;&#34920;&#31034;&#21644;&#27133;&#32423;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#20943;&#36731;&#39046;&#22495;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently slot filling has witnessed great development thanks to deep learning and the availability of large-scale annotated data. However, it poses a critical challenge to handle a novel domain whose samples are never seen during training. The recognition performance might be greatly degraded due to severe domain shifts. Most prior works deal with this problem in a two-pass pipeline manner based on metric learning. In practice, these dominant pipeline models may be limited in computational efficiency and generalization capacity because of non-parallel inference and context-free discrete label embeddings. To this end, we re-examine the typical metric-based methods, and propose a new adaptive end-to-end metric learning scheme for the challenging zero-shot slot filling. Considering simplicity, efficiency and generalizability, we present a cascade-style joint learning framework coupled with context-aware soft label representations and slot-level contrastive representation learning to mitig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#23454;&#35777;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26368;&#20339;&#32500;&#24230;&#36890;&#24120;&#27604;&#40664;&#35748;&#20540;&#35201;&#23567;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35757;&#32451;&#26041;&#27861;&#26469;&#22312;&#32500;&#24230;&#21387;&#32553;&#26102;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.15285</link><description>&lt;p&gt;
&#35770;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
On the Dimensionality of Sentence Embeddings. (arXiv:2310.15285v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#23454;&#35777;&#30340;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#26368;&#20339;&#32500;&#24230;&#36890;&#24120;&#27604;&#40664;&#35748;&#20540;&#35201;&#23567;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35757;&#32451;&#26041;&#27861;&#26469;&#22312;&#32500;&#24230;&#21387;&#32553;&#26102;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#21477;&#23376;&#23884;&#20837;&#30340;&#36136;&#37327;&#19978;&#65292;&#32780;&#23545;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#25506;&#32034;&#26377;&#38480;&#12290;&#26412;&#25991;&#23545;&#21477;&#23376;&#23884;&#20837;&#30340;&#32500;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#32780;&#23454;&#35777;&#30340;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21477;&#23376;&#23884;&#20837;&#30340;&#26368;&#20339;&#32500;&#24230;&#36890;&#24120;&#27604;&#40664;&#35748;&#20540;&#35201;&#23567;&#12290;&#25509;&#30528;&#65292;&#20026;&#20102;&#22312;&#32500;&#24230;&#21387;&#32553;&#26102;&#26368;&#23567;&#21270;&#24615;&#33021;&#25439;&#22833;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#20004;&#20010;&#24433;&#21709;&#25972;&#20307;&#24615;&#33021;&#25439;&#22833;&#30340;&#32452;&#25104;&#37096;&#20998;&#65306;&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#25439;&#22833;&#21644;&#27744;&#21270;&#22120;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#35757;&#32451;&#26041;&#27861;&#26469;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20854;&#20013;&#32534;&#30721;&#22120;&#21644;&#27744;&#21270;&#22120;&#20998;&#21035;&#36827;&#34892;&#20248;&#21270;&#20197;&#20943;&#36731;&#20302;&#32500;&#24230;&#24773;&#20917;&#19979;&#30340;&#25972;&#20307;&#24615;&#33021;&#25439;&#22833;&#12290;&#22312;&#19971;&#20010;STS&#20219;&#21153;&#21644;&#19971;&#20010;&#21477;&#23376;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
Learning sentence embeddings is a fundamental problem in natural language processing. While existing research primarily focuses on enhancing the quality of sentence embeddings, the exploration of sentence embedding dimensions is limited. Here we present a comprehensive and empirical analysis of the dimensionality of sentence embeddings. First, we demonstrate that the optimal dimension of sentence embeddings is usually smaller than the default value. Subsequently, to compress the dimension of sentence embeddings with minimum performance degradation, we identify two components contributing to the overall performance loss: the encoder's performance loss and the pooler's performance loss. Therefore, we propose a two-step training method for sentence representation learning models, wherein the encoder and the pooler are optimized separately to mitigate the overall performance loss in low-dimension scenarios. Experimental results on seven STS tasks and seven sentence classification tasks dem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35782;&#21035;&#21152;&#26435;&#26641;&#30456;&#37051;&#35821;&#35328;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21322;&#29615;&#21152;&#26435;&#29256;&#26412;&#30340;&#20004;&#32423;&#24418;&#24335;&#21270;&#26041;&#24335;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#23383;&#31526;&#20018;&#24635;&#21644;&#21644;&#20840;&#37096;&#24635;&#21644;&#12290;&#23545;&#20110;&#32447;&#24615;&#32034;&#24341;&#25991;&#27861;&#65292;&#31639;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.15276</link><description>&lt;p&gt;
&#35782;&#21035;&#21152;&#26435;&#26641;&#30456;&#37051;&#35821;&#35328;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Algorithms for Recognizing Weighted Tree-Adjoining Languages. (arXiv:2310.15276v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#35782;&#21035;&#21152;&#26435;&#26641;&#30456;&#37051;&#35821;&#35328;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#23450;&#20041;&#21322;&#29615;&#21152;&#26435;&#29256;&#26412;&#30340;&#20004;&#32423;&#24418;&#24335;&#21270;&#26041;&#24335;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#23383;&#31526;&#20018;&#24635;&#21644;&#21644;&#20840;&#37096;&#24635;&#21644;&#12290;&#23545;&#20110;&#32447;&#24615;&#32034;&#24341;&#25991;&#27861;&#65292;&#31639;&#27861;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#19978;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26641;&#30456;&#37051;&#35821;&#35328;&#30340;&#31867;&#21035;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#20108;&#32423;&#24418;&#24335;&#21270;&#26041;&#24335;&#26469;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#21253;&#25324;&#19968;&#20010;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#25110;&#19979;&#25512;&#33258;&#21160;&#26426;&#65288;PDA&#65289;&#25511;&#21046;&#21478;&#19968;&#20010;CFG&#25110;PDA&#12290;&#36825;&#22235;&#20010;&#24418;&#24335;&#31561;&#20215;&#20110;&#26641;&#30456;&#37051;&#25991;&#27861;&#65288;TAG&#65289;&#12289;&#32447;&#24615;&#32034;&#24341;&#25991;&#27861;&#65288;LIG&#65289;&#12289;&#19979;&#25512;&#30456;&#37051;&#33258;&#21160;&#26426;&#65288;PAA&#65289;&#21644;&#23884;&#22871;&#19979;&#25512;&#33258;&#21160;&#26426;&#65288;EPDA&#65289;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19978;&#36848;&#20004;&#32423;&#24418;&#24335;&#21270;&#26041;&#24335;&#30340;&#21322;&#29615;&#21152;&#26435;&#29256;&#26412;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#31639;&#27861;&#26469;&#35745;&#31639;&#23427;&#20204;&#30340;&#23383;&#31526;&#20018;&#24635;&#21644;&#65288;&#19968;&#20010;&#23383;&#31526;&#20018;&#30340;&#25152;&#26377;&#25512;&#23548;&#30340;&#26435;&#37325;&#65289;&#21644;&#20840;&#37096;&#24635;&#21644;&#65288;&#25152;&#26377;&#25512;&#23548;&#30340;&#26435;&#37325;&#65289;&#12290;&#26681;&#25454;&#36825;&#20123;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#31435;&#21363;&#24471;&#21040;TAG&#12289;LIG&#12289;PAA&#21644;EPDA&#30340;&#23383;&#31526;&#20018;&#24635;&#21644;&#21644;&#20840;&#37096;&#24635;&#21644;&#31639;&#27861;&#12290;&#23545;&#20110;LIG&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#26102;&#38388;&#25928;&#29575;&#19978;&#27604;&#22240;&#32032;$\mathcal{O}(n|\mathcal{N}|)$&#65288;&#20854;&#20013;$n$&#26159;&#23383;&#31526;&#20018;&#38271;&#24230;&#65292;$|\mathcal{N}|$&#26159;&#38750;&#32456;&#32467;&#31526;&#38598;&#21512;&#30340;&#22823;&#23567;&#65289;&#26356;&#39640;&#65292;&#32780;&#22312;&#31354;&#38388;&#25928;&#29575;&#19978;&#27604;&#22240;&#32032;$\mathcal{O}(|\Gamma|)$&#65288;&#20854;&#20013;$|\Gamma|$&#26159;&#26632;&#23383;&#27597;&#34920;&#30340;&#22823;&#23567;&#65289;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The class of tree-adjoining languages can be characterized by various two-level formalisms, consisting of a context-free grammar (CFG) or pushdown automaton (PDA) controlling another CFG or PDA. These four formalisms are equivalent to tree-adjoining grammars (TAG), linear indexed grammars (LIG), pushdown-adjoining automata (PAA), and embedded pushdown automata (EPDA). We define semiring-weighted versions of the above two-level formalisms, and we design new algorithms for computing their stringsums (the weight of all derivations of a string) and allsums (the weight of all derivations). From these, we also immediately obtain stringsum and allsum algorithms for TAG, LIG, PAA, and EPDA. For LIG, our algorithm is more time-efficient by a factor of $\mathcal{O}(n|\mathcal{N}|)$ (where $n$ is the string length and $|\mathcal{N}|$ is the size of the nonterminal set) and more space-efficient by a factor of $\mathcal{O}(|\Gamma|)$ (where $|\Gamma|$ is the size of the stack alphabet) than the alg
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#65292;&#21517;&#20026;GradSim&#12290;&#23427;&#36890;&#36807;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35821;&#35328;&#38598;&#21512;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#36127;&#38754;&#24178;&#25200;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.15269</link><description>&lt;p&gt;
GradSim: &#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#29992;&#20110;&#26377;&#25928;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GradSim: Gradient-Based Language Grouping for Effective Multilingual Training. (arXiv:2310.15269v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15269
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#65292;&#21517;&#20026;GradSim&#12290;&#23427;&#36890;&#36807;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35821;&#35328;&#38598;&#21512;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#36127;&#38754;&#24178;&#25200;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#20063;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#22823;&#22810;&#25968;&#35821;&#35328;&#37117;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25552;&#20986;&#20102;&#20302;&#36164;&#28304;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#26159;&#25152;&#26377;&#35821;&#35328;&#37117;&#33021;&#20114;&#30456;&#31215;&#26497;&#22320;&#24433;&#21709;&#65292;&#22914;&#20309;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35821;&#35328;&#38598;&#21512;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#65292;&#24182;&#36991;&#20813;&#37027;&#20123;&#29305;&#24449;&#25110;&#25968;&#25454;&#20998;&#24067;&#19981;&#20860;&#23481;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36127;&#38754;&#24178;&#25200;&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30456;&#20284;&#24615;&#30340;&#35821;&#35328;&#20998;&#32452;&#26041;&#27861;&#65292;&#31216;&#20026;GradSim&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#36739;&#20110;&#20854;&#20182;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;GradSim&#21487;&#20197;&#24102;&#26469;&#26368;&#22823;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19982;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#26356;&#22909;&#22320;&#30456;&#20851;&#12290;&#32467;&#26524;&#26159;&#65292;&#25105;&#20204;&#22312; AfriSenti &#19978;&#24314;&#31435;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23545;&#20302;&#36164;&#28304;&#38750;&#27954;&#35821;&#35328;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#38500;&#35821;&#35328;&#29305;&#24449;&#22806;&#65292;&#25968;&#25454;&#38598;&#30340;&#20027;&#39064;&#20063;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most languages of the world pose low-resource challenges to natural language processing models. With multilingual training, knowledge can be shared among languages. However, not all languages positively influence each other and it is an open research question how to select the most suitable set of languages for multilingual training and avoid negative interference among languages whose characteristics or data distributions are not compatible. In this paper, we propose GradSim, a language grouping method based on gradient similarity. Our experiments on three diverse multilingual benchmark datasets show that it leads to the largest performance gains compared to other similarity measures and it is better correlated with cross-lingual model performance. As a result, we set the new state of the art on AfriSenti, a benchmark dataset for sentiment analysis on low-resource African languages. In our extensive analysis, we further reveal that besides linguistic features, the topics of the datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#25991;&#26412;&#21487;&#33021;&#23548;&#33268;&#30340;&#38382;&#39064;&#20197;&#21450;&#34920;&#26126;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24847;&#20041;&#12290;&#21478;&#22806;&#65292;&#36824;&#25552;&#21040;&#20102;&#23545;&#25239;&#26816;&#27979;&#30340;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2310.15264</link><description>&lt;p&gt;
&#23545;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#36827;&#34892;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Towards Possibilities &amp; Impossibilities of AI-generated Text Detection: A Survey. (arXiv:2310.15264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#21644;&#19981;&#21487;&#33021;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35752;&#35770;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#30340;&#25991;&#26412;&#21487;&#33021;&#23548;&#33268;&#30340;&#38382;&#39064;&#20197;&#21450;&#34920;&#26126;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#24847;&#20041;&#12290;&#21478;&#22806;&#65292;&#36824;&#25552;&#21040;&#20102;&#23545;&#25239;&#26816;&#27979;&#30340;&#31574;&#30053;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#29983;&#25104;&#20154;&#31867;&#21270;&#25991;&#26412;&#21709;&#24212;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#29616;&#26377;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#24037;&#20316;&#23545;LLMs&#30340;&#28508;&#22312;&#28389;&#29992;&#38382;&#39064;&#25552;&#20986;&#20102;&#20005;&#37325;&#20851;&#27880;&#65292;&#22914;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#12289;&#29983;&#25104;&#20551;&#26032;&#38395;&#12289;&#23398;&#26415;&#25220;&#34989;&#21644;&#27745;&#26579;&#32593;&#32476;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30740;&#31350;&#30028;&#36798;&#25104;&#20849;&#35782;&#65292;&#21363;&#24320;&#21457;&#29992;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#31639;&#27861;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#65292;&#21482;&#35201;&#25105;&#20204;&#33021;&#21028;&#26029;&#32473;&#23450;&#25991;&#26412;&#26159;&#30001;&#20154;&#31867;&#36824;&#26159;AI&#32534;&#20889;&#30340;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#24212;&#23545;&#19978;&#36848;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#26816;&#27979;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#26816;&#27979;&#26694;&#26550;&#30340;&#21457;&#23637;&#21516;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#36824;&#33268;&#21147;&#20110;&#35774;&#35745;&#35268;&#36991;&#26816;&#27979;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., f
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#27604;&#20102;&#19977;&#31181;&#24120;&#29992;&#30340;&#22686;&#24378;&#26041;&#27861;&#65306;&#35789;&#27719;&#26367;&#25442;&#12289;&#35821;&#35328;&#29702;&#35770;&#21644;&#22238;&#35793;&#65292;&#21457;&#29616;&#22238;&#35793;&#21644;&#22522;&#20110;CSW&#39044;&#27979;&#30340;&#35789;&#27719;&#26367;&#25442;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#35821;&#35328;&#29702;&#35770;&#21644;&#38543;&#26426;&#35789;&#27719;&#26367;&#25442;&#22312;&#32570;&#20047;CSW&#24179;&#34892;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36798;&#21040;&#30456;&#20284;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.15262</link><description>&lt;p&gt;
&#29992;&#20110;&#20195;&#30721;&#20132;&#26367;&#25991;&#26412;&#26426;&#22120;&#32763;&#35793;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation Techniques for Machine Translation of Code-Switched Texts: A Comparative Study. (arXiv:2310.15262v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15262
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#27604;&#20102;&#19977;&#31181;&#24120;&#29992;&#30340;&#22686;&#24378;&#26041;&#27861;&#65306;&#35789;&#27719;&#26367;&#25442;&#12289;&#35821;&#35328;&#29702;&#35770;&#21644;&#22238;&#35793;&#65292;&#21457;&#29616;&#22238;&#35793;&#21644;&#22522;&#20110;CSW&#39044;&#27979;&#30340;&#35789;&#27719;&#26367;&#25442;&#22312;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#35821;&#35328;&#29702;&#35770;&#21644;&#38543;&#26426;&#35789;&#27719;&#26367;&#25442;&#22312;&#32570;&#20047;CSW&#24179;&#34892;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#36798;&#21040;&#30456;&#20284;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#30340;&#26085;&#30410;&#24341;&#36215;&#20851;&#27880;&#65292;&#20195;&#30721;&#20132;&#26367;&#65288;CSW&#65289;&#25991;&#26412;&#29983;&#25104;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#22686;&#38271;&#30340;&#20852;&#36259;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20840;&#38754;&#30340;&#30740;&#31350;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#22686;&#24378;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65306;&#35789;&#27719;&#26367;&#25442;&#12289;&#35821;&#35328;&#29702;&#35770;&#21644;&#22238;&#35793;&#65288;BT&#65289;&#65292;&#22312;&#22467;&#21450;&#38463;&#25289;&#20271;&#35821;-&#33521;&#35821;CSW&#30340;&#32972;&#26223;&#19979;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#25928;&#26524;&#20197;&#21450;&#22686;&#24378;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#26174;&#31034;&#22238;&#35793;&#21644;&#22522;&#20110;CSW&#39044;&#27979;&#30340;&#35789;&#27719;&#26367;&#25442;&#22312;&#20004;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;CSW&#30340;&#24179;&#34892;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#12290;&#32780;&#22312;&#32570;&#20047;CSW&#24179;&#34892;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#35821;&#35328;&#29702;&#35770;&#21644;&#38543;&#26426;&#35789;&#27719;&#26367;&#25442;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#20004;&#31181;&#26041;&#27861;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-switching (CSW) text generation has been receiving increasing attention as a solution to address data scarcity. In light of this growing interest, we need more comprehensive studies comparing different augmentation approaches. In this work, we compare three popular approaches: lexical replacements, linguistic theories, and back-translation (BT), in the context of Egyptian Arabic-English CSW. We assess the effectiveness of the approaches on machine translation and the quality of augmentations through human evaluation. We show that BT and CSW predictive-based lexical replacement, being trained on CSW parallel data, perform best on both tasks. Linguistic theories and random lexical replacement prove to be effective in the lack of CSW parallel data, where both approaches achieve similar results.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#29615;&#22659;&#20013;&#38382;&#39064;&#32763;&#35793;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#38382;&#39064;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.15259</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#39046;&#22495;&#33258;&#36866;&#24212;&#32763;&#35793;&#24102;&#26377;&#38382;&#39064;&#29305;&#23450;&#22870;&#21169;&#30340;&#22122;&#22768;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Reference Free Domain Adaptation for Translation of Noisy Questions with Question Specific Rewards. (arXiv:2310.15259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15259
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22122;&#22768;&#29615;&#22659;&#20013;&#38382;&#39064;&#32763;&#35793;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#32763;&#35793;&#38382;&#39064;&#30340;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38382;&#31572;(CQA)&#24179;&#21488;&#26159;&#24110;&#21161;&#32452;&#32455;&#20869;&#29992;&#25143;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#20351;&#23427;&#20204;&#23545;&#38750;&#33521;&#35821;&#29992;&#25143;&#21487;&#35775;&#38382;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#32763;&#35793;&#38382;&#39064;&#21487;&#20197;&#25299;&#23485;&#31038;&#21306;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#20351;&#26377;&#31867;&#20284;&#38382;&#39064;&#30340;&#20154;&#33021;&#22815;&#21463;&#30410;&#65292;&#20294;&#22312;&#22024;&#26434;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#36827;&#34892;&#38382;&#39064;&#32763;&#35793;&#20250;&#38754;&#20020;&#26356;&#22810;&#25361;&#25112;&#65292;&#22240;&#20026;&#36825;&#20123;&#38382;&#39064;&#30340;&#35821;&#27861;&#27491;&#30830;&#24615;&#27809;&#26377;&#21463;&#21040;&#30417;&#25511;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#34987;&#38750;&#27597;&#35821;&#29992;&#25143;&#20197;&#38472;&#36848;&#21477;&#30340;&#24418;&#24335;&#34920;&#36798;&#65292;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#20027;&#35859;&#35821;&#24207;&#65292;&#29978;&#33267;&#26377;&#26102;&#32570;&#23569;&#38382;&#21495;&#12290;&#30001;&#20110;&#25968;&#25454;&#23384;&#22312;&#22122;&#22768;&#65292;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#30340;&#24179;&#34892;&#35821;&#26009;&#24211;&#20063;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#20351;&#29992;&#28304;&#35821;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;BERTScore&#21644;Masked Language Model (MLM) S&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#24179;&#34913;&#20102;&#20805;&#20998;&#24615;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Community Question-Answering (CQA) portals serve as a valuable tool for helping users within an organization. However, making them accessible to non-English-speaking users continues to be a challenge. Translating questions can broaden the community's reach, benefiting individuals with similar inquiries in various languages. Translating questions using Neural Machine Translation (NMT) poses more challenges, especially in noisy environments, where the grammatical correctness of the questions is not monitored. These questions may be phrased as statements by non-native speakers, with incorrect subject-verb order and sometimes even missing question marks. Creating a synthetic parallel corpus from such data is also difficult due to its noisy nature. To address this issue, we propose a training methodology that fine-tunes the NMT system only using source-side data. Our approach balances adequacy and fluency by utilizing a loss function that combines BERTScore and Masked Language Model (MLM) S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MultiLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#30340;&#32454;&#35843;&#65292;&#21457;&#29616;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#19979;&#23427;&#20204;&#21487;&#20197;&#20256;&#36882;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#20195;&#30721;&#20999;&#25442;&#29615;&#22659;&#19979;&#38590;&#20197;&#23454;&#29616;&#25512;&#29702;&#33021;&#21147;&#30340;&#20256;&#36882;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#40723;&#21169;&#36328;&#35821;&#35328;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.15258</link><description>&lt;p&gt;
&#25171;&#30772;&#35821;&#35328;&#38556;&#30861;&#65306;&#36890;&#36807;&#32467;&#26500;&#21270;&#33258;&#27880;&#24847;&#21147;&#25552;&#39640;&#36328;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Breaking the Language Barrier: Improving Cross-Lingual Reasoning with Structured Self-Attention. (arXiv:2310.15258v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MultiLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#36827;&#34892;&#25512;&#29702;&#26102;&#30340;&#32454;&#35843;&#65292;&#21457;&#29616;&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#19979;&#23427;&#20204;&#21487;&#20197;&#20256;&#36882;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#20195;&#30721;&#20999;&#25442;&#29615;&#22659;&#19979;&#38590;&#20197;&#23454;&#29616;&#25512;&#29702;&#33021;&#21147;&#30340;&#20256;&#36882;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#26469;&#40723;&#21169;&#36328;&#35821;&#35328;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MultiLMs&#65289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#36827;&#34892;&#25512;&#29702;&#30340;&#32454;&#35843;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#23558;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36716;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;MultiLMs&#22312;&#20004;&#31181;&#26041;&#26696;&#19979;&#30340;&#36328;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#65306;&#65288;1&#65289;&#22312;&#27979;&#35797;&#30340;&#26032;&#35821;&#35328;&#20013;&#65292;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#30340;&#35821;&#35328;&#20445;&#25345;&#19981;&#21464;&#65288;&#21363;&#25512;&#29702;&#20173;&#28982;&#26159;&#21333;&#35821;&#35328;&#30340;&#65292;&#20294;&#27169;&#22411;&#24517;&#39035;&#22312;&#35821;&#35328;&#38388;&#20256;&#36882;&#23398;&#20064;&#21040;&#30340;&#25512;&#29702;&#33021;&#21147;&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19978;&#19979;&#25991;&#21644;&#38382;&#39064;&#30340;&#35821;&#35328;&#19981;&#21516;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#20195;&#30721;&#20999;&#25442;&#25512;&#29702;&#65289;&#12290;&#22312;&#20004;&#20010;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;RuleTaker&#21644;LeapOfThought&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#34429;&#28982;MultiLMs&#22312;&#21333;&#35821;&#35328;&#29615;&#22659;&#20013;&#21487;&#20197;&#36328;&#35821;&#35328;&#20256;&#36882;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#20195;&#30721;&#20999;&#25442;&#29615;&#22659;&#20013;&#38590;&#20197;&#23454;&#29616;&#25512;&#29702;&#33021;&#21147;&#30340;&#20256;&#36882;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#19987;&#38376;&#30340;&#19968;&#32452;&#21442;&#25968;&#26469;&#40723;&#21169;&#36328;&#35821;&#35328;&#25512;&#29702;&#30340;&#26032;&#22411;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study whether multilingual language models (MultiLMs) can transfer logical reasoning abilities to other languages when they are fine-tuned for reasoning in a different language. We evaluate the cross-lingual reasoning abilities of MultiLMs in two schemes: (1) where the language of the context and the question remain the same in the new languages that are tested (i.e., the reasoning is still monolingual, but the model must transfer the learned reasoning ability across languages), and (2) where the language of the context and the question is different (which we term code-switched reasoning). On two logical reasoning datasets, RuleTaker and LeapOfThought, we demonstrate that although MultiLMs can transfer reasoning ability across languages in a monolingual setting, they struggle to transfer reasoning abilities in a code-switched setting. Following this observation, we propose a novel attention mechanism that uses a dedicated set of parameters to encourage cross-lingual at
&lt;/p&gt;</description></item><item><title>CRoW&#26159;&#19968;&#20010;&#25163;&#24037;&#31574;&#21010;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;NLP&#20219;&#21153;&#20013;&#24212;&#29992;&#24120;&#35782;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;NLP&#31995;&#32479;&#22312;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#34920;&#26126;&#24120;&#35782;&#25512;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2310.15239</link><description>&lt;p&gt;
CRoW: &#22312;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#23545;&#24120;&#35782;&#25512;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CRoW: Benchmarking Commonsense Reasoning in Real-World Tasks. (arXiv:2310.15239v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15239
&lt;/p&gt;
&lt;p&gt;
CRoW&#26159;&#19968;&#20010;&#25163;&#24037;&#31574;&#21010;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;NLP&#20219;&#21153;&#20013;&#24212;&#29992;&#24120;&#35782;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;NLP&#31995;&#32479;&#22312;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#19982;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#34920;&#26126;&#24120;&#35782;&#25512;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#20173;&#28982;&#36828;&#26410;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#20851;&#20110;&#24120;&#35782;&#25512;&#29702;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#35768;&#22810;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#20154;&#24037;&#22330;&#26223;&#19979;&#26500;&#24314;&#20102;&#24120;&#35782;&#25512;&#29702;&#25361;&#25112;&#65292;&#36825;&#20123;&#22330;&#26223;&#24182;&#19981;&#33021;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;NLP&#31995;&#32479;&#25152;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CRoW&#65292;&#19968;&#20010;&#25163;&#24037;&#31574;&#21010;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;NLP&#20219;&#21153;&#20013;&#24212;&#29992;&#24120;&#35782;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;CRoW&#20351;&#29992;&#22810;&#38454;&#27573;&#30340;&#25968;&#25454;&#25910;&#38598;&#27969;&#31243;&#26500;&#24314;&#65292;&#36890;&#36807;&#36829;&#21453;&#24120;&#35782;&#30340;&#25200;&#21160;&#37325;&#20889;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#21033;&#29992;CRoW&#26469;&#30740;&#31350;NLP&#31995;&#32479;&#22312;&#29289;&#29702;&#12289;&#26102;&#38388;&#21644;&#31038;&#20132;&#25512;&#29702;&#31561;&#19981;&#21516;&#24120;&#35782;&#30693;&#35782;&#32500;&#24230;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;CRoW&#19978;&#35780;&#20272;NLP&#31995;&#32479;&#26102;&#19982;&#20154;&#31867;&#30456;&#27604;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#26174;&#31034;&#20986;&#24120;&#35782;&#25512;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36824;&#36828;&#26410;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent efforts in natural language processing (NLP) commonsense reasoning research have yielded a considerable number of new datasets and benchmarks. However, most of these datasets formulate commonsense reasoning challenges in artificial scenarios that are not reflective of the tasks which real-world NLP systems are designed to solve. In this work, we present CRoW, a manually-curated, multi-task benchmark that evaluates the ability of models to apply commonsense reasoning in the context of six real-world NLP tasks. CRoW is constructed using a multi-stage data collection pipeline that rewrites examples from existing datasets using commonsense-violating perturbations. We use CRoW to study how NLP systems perform across different dimensions of commonsense knowledge, such as physical, temporal, and social reasoning. We find a significant performance gap when NLP systems are evaluated on CRoW compared to humans, showcasing that commonsense reasoning is far from being solved in real-world t
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.15213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20989;&#25968;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Function Vectors in Large Language Models. (arXiv:2310.15213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15213
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#31181;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#21521;&#37327;&#12290;&#36825;&#20123;&#20989;&#25968;&#21521;&#37327;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#36824;&#20855;&#26377;&#23558;&#35821;&#20041;&#21521;&#37327;&#36827;&#34892;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31070;&#32463;&#26426;&#21046;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#34920;&#31034;&#20026;&#33258;&#22238;&#24402;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#20013;&#30340;&#21521;&#37327;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20219;&#21153;&#19978;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#23569;&#25968;&#27880;&#24847;&#21147;&#22836;&#20256;&#36755;&#20102;&#23637;&#31034;&#20219;&#21153;&#30340;&#32039;&#20945;&#34920;&#31034;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20989;&#25968;&#21521;&#37327;&#65288;FV&#65289;&#12290;FV&#23545;&#19978;&#19979;&#25991;&#30340;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21363;&#23427;&#20204;&#22312;&#19981;&#31867;&#20284;&#20110;&#20854;&#25910;&#38598;&#26102;&#30340;ICL&#19978;&#19979;&#25991;&#30340;&#24773;&#20917;&#19979;&#35302;&#21457;&#23545;&#36755;&#20837;&#30340;&#20219;&#21153;&#25191;&#34892;&#65292;&#20363;&#22914;&#38646;&#26679;&#26412;&#21644;&#33258;&#28982;&#25991;&#26412;&#35774;&#32622;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#23618;&#19978;&#27979;&#35797;&#20102;FV&#65292;&#24182;&#22312;&#20013;&#23618;&#21457;&#29616;&#24378;&#22823;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;FV&#30340;&#20869;&#37096;&#32467;&#26500;&#65292;&#24182;&#21457;&#29616;&#34429;&#28982;&#23427;&#20204;&#36890;&#24120;&#21253;&#21547;&#32534;&#30721;&#20989;&#25968;&#30340;&#36755;&#20986;&#31354;&#38388;&#30340;&#20449;&#24687;&#65292;&#20294;&#20165;&#27492;&#20449;&#24687;&#26080;&#27861;&#37325;&#26500;FV&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;FV&#20013;&#30340;&#35821;&#20041;&#21521;&#37327;&#32452;&#21512;&#65292;&#24182;&#21457;&#29616;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#23384;&#22312;&#32452;&#21512;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the presence of a simple neural mechanism that represents an input-output function as a vector within autoregressive transformer language models (LMs). Using causal mediation analysis on a diverse range of in-context-learning (ICL) tasks, we find that a small number attention heads transport a compact representation of the demonstrated task, which we call a function vector (FV). FVs are robust to changes in context, i.e., they trigger execution of the task on inputs such as zero-shot and natural text settings that do not resemble the ICL contexts from which they are collected. We test FVs across a range of tasks, models, and layers and find strong causal effects across settings in middle layers. We investigate the internal structure of FVs and find while that they often contain information that encodes the output space of the function, this information alone is not sufficient to reconstruct an FV. Finally, we test semantic vector composition in FVs, and find that to some exte
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM&#65292;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#22810;&#36718;&#38382;&#31572;&#12289;&#39046;&#22495;&#25991;&#26412;&#22788;&#29702;&#12289;&#25968;&#23398;&#35745;&#31639;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#37329;&#34701;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15205</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#30340;&#20013;&#22269;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM
&lt;/p&gt;
&lt;p&gt;
DISC-FinLLM: A Chinese Financial Large Language Model based on Multiple Experts Fine-tuning. (arXiv:2310.15205v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15205
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM&#65292;&#36890;&#36807;&#36171;&#20104;&#27169;&#22411;&#22810;&#36718;&#38382;&#31572;&#12289;&#39046;&#22495;&#25991;&#26412;&#22788;&#29702;&#12289;&#25968;&#23398;&#35745;&#31639;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#37329;&#34701;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#19987;&#23478;&#24494;&#35843;&#26694;&#26550;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;DISC-FinLLM&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36171;&#20104;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#22810;&#36718;&#38382;&#31572;&#33021;&#21147;&#12289;&#39046;&#22495;&#25991;&#26412;&#22788;&#29702;&#33021;&#21147;&#12289;&#25968;&#23398;&#35745;&#31639;&#25216;&#33021;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#33021;&#21147;&#26469;&#25913;&#36827;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#37329;&#34701;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;DISC-FIN-SFT&#65292;&#21253;&#25324;&#22235;&#20010;&#20998;&#31867;&#30340;&#25351;&#20196;&#26679;&#26412;&#65288;&#21672;&#35810;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12289;&#35745;&#31639;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#37329;&#34701;&#22330;&#26223;&#20013;&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#12290;&#26356;&#22810;&#36164;&#28304;&#21487;&#20197;&#22312;https://github.com/FudanDISC/DISC-FinLLM&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Multiple Experts Fine-tuning Framework to build a financial large language model (LLM), DISC-FinLLM. Our methodology improves general LLMs by endowing them with multi-turn question answering abilities, domain text processing capabilities, mathematical computation skills, and retrieval-enhanced generation capabilities. We build a financial instruction-tuning dataset named DISC-FIN-SFT, including instruction samples of four categories (consulting, NLP tasks, computing and retrieval-augmented generation). Evaluations conducted on multiple benchmarks demonstrate that our model performs better than baseline models in various financial scenarios. Further resources can be found at https://github.com/FudanDISC/DISC-FinLLM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#38408;&#20540;&#31227;&#21160;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.15019</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#20803;&#23398;&#20064;: &#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Meta learning with language models: Challenges and opportunities in the classification of imbalanced text. (arXiv:2310.15019v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#65292;&#22312;&#19981;&#24179;&#34913;&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#38408;&#20540;&#31227;&#21160;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#39044;&#27979;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#36829;&#35268;&#35328;&#35770;&#20869;&#23481;&#26159;&#37325;&#35201;&#20294;&#22256;&#38590;&#30340;&#12290;&#34429;&#28982;&#26426;&#22120;&#23398;&#20064;&#26159;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#36136;&#37327;&#38480;&#21046;&#20197;&#21450;&#36829;&#35268;&#23450;&#20041;&#21644;&#25968;&#25454;&#26631;&#27880;&#30340;&#19981;&#19968;&#33268;&#24615;&#31561;&#22240;&#32032;&#65292;&#38590;&#20197;&#31361;&#30772;&#24615;&#33021;&#29942;&#39048;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#26377;&#38480;&#36164;&#28304;&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#25216;&#26415;(MLT)&#65292;&#23427;&#23558;&#20351;&#29992;&#19981;&#21516;&#25991;&#26412;&#34920;&#31034;&#26500;&#24314;&#30340;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#35777;&#26126;&#65292;&#25152;&#24471;&#21040;&#30340;&#25216;&#26415;&#22312;&#25968;&#20540;&#19978;&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#20135;&#29983;&#21512;&#29702;&#30340;&#32452;&#21512;&#26435;&#37325;&#12290;&#25105;&#20204;&#23558;MLT&#19982;&#38408;&#20540;&#31227;&#21160;(TM)&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#32452;&#21512;&#39044;&#27979;&#22120;&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#20998;&#24067;&#21644;&#36229;&#20986;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35745;&#31639;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;MLT&#26041;&#27861;&#30340;&#32479;&#35745;&#20248;&#21183;&#12290;&#25152;&#26377;&#20316;&#32773;&#23545;&#36825;&#39033;&#24037;&#20316;&#36129;&#29486;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting out of policy speech (OOPS) content is important but difficult. While machine learning is a powerful tool to tackle this challenging task, it is hard to break the performance ceiling due to factors like quantity and quality limitations on training data and inconsistencies in OOPS definition and data labeling. To realize the full potential of available limited resources, we propose a meta learning technique (MLT) that combines individual models built with different text representations. We analytically show that the resulting technique is numerically stable and produces reasonable combining weights. We combine the MLT with a threshold-moving (TM) technique to further improve the performance of the combined predictor on highly-imbalanced in-distribution and out-of-distribution datasets. We also provide computational results to show the statistically significant advantages of the proposed MLT approach.  All authors contributed equally to this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#27668;&#35299;&#30721;&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#24314;&#23646;&#24615;&#20998;&#24067;&#26469;&#24179;&#34913;&#26435;&#37325;&#65292;&#29983;&#25104;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#23646;&#24615;&#22349;&#32553;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.14892</link><description>&lt;p&gt;
&#31354;&#27668;&#35299;&#30721;&#65306;&#35299;&#30721;&#26102;&#38388;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#30340;&#23646;&#24615;&#20998;&#24067;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Air-Decoding: Attribute Distribution Reconstruction for Decoding-Time Controllable Text Generation. (arXiv:2310.14892v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14892
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31354;&#27668;&#35299;&#30721;&#30340;&#26032;&#39062;&#36731;&#37327;&#32423;&#35299;&#30721;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#24314;&#23646;&#24615;&#20998;&#24067;&#26469;&#24179;&#34913;&#26435;&#37325;&#65292;&#29983;&#25104;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20197;&#35299;&#20915;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#23646;&#24615;&#22349;&#32553;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#30340;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#26088;&#22312;&#29983;&#25104;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#25991;&#26412;&#65292;&#32780;&#22522;&#20110;&#35299;&#30721;&#26102;&#38388;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#21457;&#29616;&#20102;&#23646;&#24615;&#22349;&#32553;&#29616;&#35937;&#12290;&#24403;&#25511;&#21046;&#24378;&#24230;&#36229;&#36807;&#20020;&#30028;&#20540;&#26102;&#65292;&#23427;&#20250;&#23548;&#33268;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#36805;&#36895;&#38477;&#20302;&#65292;&#20351;&#25991;&#26412;&#23436;&#20840;&#26080;&#27861;&#20351;&#29992;&#12290;&#36825;&#20010;&#38480;&#21046;&#38459;&#30861;&#20102;&#35299;&#30721;&#26041;&#27861;&#22312;&#23454;&#29616;&#39640;&#27700;&#24179;&#21487;&#25511;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36731;&#37327;&#32423;&#35299;&#30721;&#26694;&#26550;&#65292;&#21517;&#20026;&#31354;&#27668;&#35299;&#30721;&#12290;&#23427;&#30340;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#37325;&#24314;&#23646;&#24615;&#20998;&#24067;&#26469;&#24179;&#34913;&#23646;&#24615;&#35789;&#21644;&#38750;&#23646;&#24615;&#35789;&#20043;&#38388;&#30340;&#26435;&#37325;&#65292;&#20174;&#32780;&#29983;&#25104;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21069;&#32512;&#24494;&#35843;&#26469;&#35757;&#32451;&#21069;&#32512;&#20197;&#33719;&#24471;&#23646;&#24615;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23646;&#24615;&#20998;&#24067;&#37325;&#24314;&#26041;&#27861;&#26469;&#24179;&#34913;&#25152;&#33719;&#24471;&#30340;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#37325;&#24314;&#21518;&#30340;&#20998;&#24067;&#36827;&#34892;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation (CTG) aims to generate text with desired attributes, and decoding-time-based methods have shown promising performance on this task. However, in this paper, we identify the phenomenon of Attribute Collapse for the first time. It causes the fluency of generated text to rapidly decrease when the control strength exceeds a critical value, rendering the text completely unusable. This limitation hinders the effectiveness of decoding methods in achieving high levels of controllability. To address this problem, we propose a novel lightweight decoding framework named Air-Decoding. Its main idea is reconstructing the attribute distributions to balance the weights between attribute words and non-attribute words to generate more fluent text. Specifically, we train prefixes by prefix-tuning to obtain attribute distributions. Then we design a novel attribute distribution reconstruction method to balance the obtained distributions and use the reconstructed distributions t
&lt;/p&gt;</description></item><item><title>MCC-KD&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;CoT&#19968;&#33268;&#24615;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36716;&#31227;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#29702;&#30001;&#24182;&#30830;&#20445;&#20854;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#25512;&#29702;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14747</link><description>&lt;p&gt;
MCC-KD: &#22810;CoT&#19968;&#33268;&#24615;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
MCC-KD: Multi-CoT Consistent Knowledge Distillation. (arXiv:2310.14747v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14747
&lt;/p&gt;
&lt;p&gt;
MCC-KD&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;CoT&#19968;&#33268;&#24615;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#36716;&#31227;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#21040;&#36739;&#23567;&#30340;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#20010;&#29702;&#30001;&#24182;&#30830;&#20445;&#20854;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#26469;&#22686;&#24378;&#25512;&#29702;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#26041;&#38754;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#38142;&#24335;&#24605;&#32771;(CoT)&#25552;&#31034;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#23558;&#36825;&#20123;&#25512;&#29702;&#33021;&#21147;&#20174;LLMs&#36716;&#31227;&#21040;&#36739;&#23567;&#27169;&#22411;&#20013;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#21516;&#26102;&#23454;&#29616;&#22810;&#26679;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#29702;&#30001;&#23545;&#20110;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#22686;&#24378;&#36825;&#20004;&#20010;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;CoT&#19968;&#33268;&#24615;&#30693;&#35782;&#33976;&#39311;(MCC-KD)&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#25552;&#21462;&#25512;&#29702;&#33021;&#21147;&#12290;&#22312;MCC-KD&#20013;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#38382;&#39064;&#29983;&#25104;&#22810;&#20010;&#29702;&#30001;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#31572;&#26696;&#20998;&#24067;&#20043;&#38388;&#30340;&#21452;&#21521;KL&#25955;&#24230;&#26469;&#30830;&#20445;&#30456;&#24212;&#39044;&#27979;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;MCC-KD&#22312;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;(LLaMA/FlanT5)&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;(3B/7B/11B/13B)&#19978;&#22312;&#25968;&#23398;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#19981;&#20165;&#35777;&#23454;&#20102;MCC-KD&#22312;&#20998;&#24067;&#20869;&#24773;&#20917;&#19979;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have showcased remarkable capabilities in complex reasoning through chain of thought (CoT) prompting. Recently, there has been a growing interest in transferring these reasoning abilities from LLMs to smaller models. However, achieving both the diversity and consistency in rationales presents a challenge. In this paper, we focus on enhancing these two aspects and propose Multi-CoT Consistent Knowledge Distillation (MCC-KD) to efficiently distill the reasoning capabilities. In MCC-KD, we generate multiple rationales for each question and enforce consistency among the corresponding predictions by minimizing the bidirectional KL-divergence between the answer distributions. We investigate the effectiveness of MCC-KD with different model architectures (LLaMA/FlanT5) and various model scales (3B/7B/11B/13B) on both mathematical reasoning and commonsense reasoning benchmarks. The empirical results not only confirm MCC-KD's superior performance on in-distribution d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.14724</link><description>&lt;p&gt;
&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#30340;&#35843;&#26597;&#65306;&#24517;&#35201;&#24615;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on LLM-generated Text Detection: Necessity, Methods, and Future Directions. (arXiv:2310.14724v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24378;&#35843;&#20102;&#24320;&#21457;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#24635;&#32467;&#20102;&#36817;&#26399;&#30340;&#30740;&#31350;&#21019;&#26032;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#22797;&#26434;&#35821;&#35328;&#30340;&#24378;&#22823;&#33021;&#21147;&#20351;&#24471;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20197;&#24778;&#20154;&#30340;&#36895;&#24230;&#28044;&#20837;&#21040;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#39046;&#22495;&#20013;&#65292;&#24182;&#24471;&#21040;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#25509;&#21463;&#12290;&#38543;&#30528;LLMs&#30340;&#19981;&#26029;&#25193;&#23637;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#26816;&#27979;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#26816;&#27979;&#22120;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;LLMs&#28508;&#22312;&#30340;&#35823;&#29992;&#65292;&#24182;&#20445;&#25252;&#33402;&#26415;&#34920;&#36798;&#21644;&#31038;&#20132;&#32593;&#32476;&#31561;&#39046;&#22495;&#20813;&#21463;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#26377;&#23475;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#26088;&#22312;&#30830;&#23450;&#19968;&#27573;&#25991;&#26412;&#26159;&#21542;&#30001;LLM&#29983;&#25104;&#65292;&#23454;&#36136;&#19978;&#26159;&#19968;&#20010;&#20108;&#20998;&#31867;&#20219;&#21153;&#12290;&#26816;&#27979;&#22120;&#25216;&#26415;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#25512;&#21160;&#22240;&#32032;&#21253;&#25324;&#27700;&#21360;&#25216;&#26415;&#12289;&#38646;&#26679;&#26412;&#26041;&#27861;&#12289;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#26041;&#27861;&#12289;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12289;&#23558;LLMs&#20316;&#20026;&#26816;&#27979;&#22120;&#20197;&#21450;&#20154;&#31867;&#36741;&#21161;&#26041;&#27861;&#30340;&#21019;&#26032;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#27719;&#38598;&#20102;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#21462;&#24471;&#30340;&#30740;&#31350;&#31361;&#30772;&#65292;&#24182;&#24378;&#35843;&#20102;&#36843;&#20999;&#30340;&#38656;&#27714;&#21644;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, zero-shot methods, fine-turning LMs methods, adversarial learning methods, LLMs as detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressin
&lt;/p&gt;</description></item><item><title>SPRING-INX&#25968;&#25454;&#26159;&#30001;SPRING&#23454;&#39564;&#23460;&#24320;&#28304;&#30340;&#22810;&#35821;&#31181;&#21360;&#24230;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#21360;&#24230;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24314;&#35774;&#12290;&#36825;&#26159;&#20026;&#20102;&#40723;&#21169;&#35821;&#35328;&#25216;&#26415;&#31038;&#21306;&#22312;22&#31181;&#21360;&#24230;&#23448;&#26041;&#35821;&#35328;&#20013;&#26500;&#24314;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#31243;&#24207;&#32780;&#36827;&#34892;&#30340;&#21162;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.14654</link><description>&lt;p&gt;
SPRING-INX: SPRING&#23454;&#39564;&#23460;&#21360;&#24230;&#29702;&#24037;&#23398;&#38498;&#39532;&#24503;&#25289;&#26031;&#20998;&#26657;&#30340;&#22810;&#35821;&#31181;&#21360;&#24230;&#35821;&#38899;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
SPRING-INX: A Multilingual Indian Language Speech Corpus by SPRING Lab, IIT Madras. (arXiv:2310.14654v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14654
&lt;/p&gt;
&lt;p&gt;
SPRING-INX&#25968;&#25454;&#26159;&#30001;SPRING&#23454;&#39564;&#23460;&#24320;&#28304;&#30340;&#22810;&#35821;&#31181;&#21360;&#24230;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#21360;&#24230;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#24314;&#35774;&#12290;&#36825;&#26159;&#20026;&#20102;&#40723;&#21169;&#35821;&#35328;&#25216;&#26415;&#31038;&#21306;&#22312;22&#31181;&#21360;&#24230;&#23448;&#26041;&#35821;&#35328;&#20013;&#26500;&#24314;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#31243;&#24207;&#32780;&#36827;&#34892;&#30340;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#26159;&#22810;&#35821;&#31181;&#30340;&#65292;&#20854;&#20013;&#26377;22&#31181;&#35821;&#35328;&#34987;&#21360;&#24230;&#23466;&#27861;&#35748;&#23450;&#20026;&#23448;&#26041;&#35821;&#35328;&#12290;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#20197;&#21450;&#38656;&#35201;&#36866;&#24212;&#30340;&#35821;&#35328;&#21644;&#21475;&#38899;&#25968;&#37327;&#65292;&#20026;&#21360;&#24230;&#20154;&#21475;&#26500;&#24314;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#31243;&#24207;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#40723;&#21169;&#35821;&#35328;&#25216;&#26415;&#31038;&#21306;&#22312;&#21360;&#24230;&#35821;&#35328;&#20013;&#26500;&#24314;&#22522;&#20110;&#35821;&#38899;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;SPRING-INX&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;2000&#23567;&#26102;&#30340;&#21512;&#27861;&#26469;&#28304;&#24182;&#19988;&#32463;&#36807;&#25163;&#24037;&#36716;&#24405;&#30340;&#35821;&#38899;&#25968;&#25454;&#65292;&#29992;&#20110;&#26500;&#24314;Assamese&#12289;Bengali&#12289;Gujarati&#12289;Hindi&#12289;Kannada&#12289;Malayalam&#12289;Marathi&#12289;Odia&#12289;Punjabi&#21644;Tamil&#30340;ASR&#31995;&#32479;&#12290;&#36825;&#20010;&#21162;&#21147;&#26159;&#30001;&#21360;&#24230;&#29702;&#24037;&#23398;&#38498;&#39532;&#24503;&#25289;&#26031;&#20998;&#26657;&#30340;SPRING&#23454;&#39564;&#23460;&#36827;&#34892;&#30340;&#65292;&#26159;&#21360;&#24230;&#30005;&#23376;&#21644;&#20449;&#24687;&#25216;&#26415;&#37096;&#65288;MeitY&#65289;&#36164;&#21161;&#30340;&#22269;&#23478;&#35821;&#35328;&#32763;&#35793;&#20219;&#21153;&#65288;NLTM&#65289;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25551;&#36848;&#20102;&#25968;&#25454;&#25910;&#38598;&#21644;&#25968;&#25454;&#28165;&#29702;&#36807;&#31243;&#20197;&#21450;&#25968;&#25454;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
India is home to a multitude of languages of which 22 languages are recognised by the Indian Constitution as official. Building speech based applications for the Indian population is a difficult problem owing to limited data and the number of languages and accents to accommodate. To encourage the language technology community to build speech based applications in Indian languages, we are open sourcing SPRING-INX data which has about 2000 hours of legally sourced and manually transcribed speech data for ASR system building in Assamese, Bengali, Gujarati, Hindi, Kannada, Malayalam, Marathi, Odia, Punjabi and Tamil. This endeavor is by SPRING Lab , Indian Institute of Technology Madras and is a part of National Language Translation Mission (NLTM), funded by the Indian Ministry of Electronics and Information Technology (MeitY), Government of India. We describe the data collection and data cleaning process along with the data statistics in this paper.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#21033;&#29992;ChatGPT&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#21487;&#20197;&#22312;&#30452;&#25509;&#32534;&#30721;&#36716;&#24405;&#12289;&#29983;&#25104;&#20027;&#39064;&#21644;&#39044;&#22788;&#29702;&#24341;&#29992;&#31561;&#26680;&#24515;&#38454;&#27573;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#39640;&#20998;&#26512;&#25928;&#29575;&#21644;&#25552;&#20379;&#39069;&#22806;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.14545</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#65306;&#25105;&#20204;&#20934;&#22791;&#22909;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Harnessing ChatGPT for thematic analysis: Are we ready?. (arXiv:2310.14545v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14545
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#21033;&#29992;ChatGPT&#36827;&#34892;&#20027;&#39064;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;ChatGPT&#21487;&#20197;&#22312;&#30452;&#25509;&#32534;&#30721;&#36716;&#24405;&#12289;&#29983;&#25104;&#20027;&#39064;&#21644;&#39044;&#22788;&#29702;&#24341;&#29992;&#31561;&#26680;&#24515;&#38454;&#27573;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#25552;&#39640;&#20998;&#26512;&#25928;&#29575;&#21644;&#25552;&#20379;&#39069;&#22806;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#65292;&#22312;&#21307;&#23398;&#30740;&#31350;&#30340;&#21508;&#20010;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#20027;&#39064;&#20998;&#26512;&#26159;&#19968;&#31181;&#23450;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#35299;&#37322;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#65292;&#32780;&#36825;&#39033;&#25216;&#26415;&#26377;&#26395;&#20174;&#20013;&#21463;&#30410;&#12290;&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#21307;&#23398;&#32972;&#26223;&#19979;&#21033;&#29992;ChatGPT&#22312;&#20027;&#39064;&#20998;&#26512;&#30340;&#19977;&#20010;&#26680;&#24515;&#38454;&#27573;&#20013;&#30340;&#24212;&#29992;&#65306;1&#65289;&#23545;&#36716;&#24405;&#36827;&#34892;&#30452;&#25509;&#32534;&#30721;&#65292;2&#65289;&#20174;&#39044;&#23450;&#20041;&#30340;&#32534;&#30721;&#21015;&#34920;&#20013;&#29983;&#25104;&#20027;&#39064;&#65292;&#20197;&#21450;3&#65289;&#20026;&#25991;&#31295;&#21253;&#21547;&#39044;&#22788;&#29702;&#24341;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;ChatGPT&#29983;&#25104;&#38754;&#35797;&#36716;&#24405;&#30340;&#28508;&#21147;&#65292;&#36825;&#20123;&#36716;&#24405;&#21487;&#29992;&#20110;&#22521;&#35757;&#30446;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22312;&#36825;&#20123;&#35282;&#33394;&#20013;&#20351;&#29992;ChatGPT&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24378;&#35843;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#30340;&#39046;&#22495;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#35748;&#20026;ChatGPT&#21487;&#20197;&#20316;&#20026;&#20998;&#26512;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#25552;&#39640;&#20027;&#39064;&#20998;&#26512;&#30340;&#25928;&#29575;&#65292;&#24182;&#20026;&#23450;&#24615;&#25968;&#25454;&#25552;&#20379;&#39069;&#22806;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is an advanced natural language processing tool with growing applications across various disciplines in medical research. Thematic analysis, a qualitative research method to identify and interpret patterns in data, is one application that stands to benefit from this technology. This viewpoint explores the utilization of ChatGPT in three core phases of thematic analysis within a medical context: 1) direct coding of transcripts, 2) generating themes from a predefined list of codes, and 3) preprocessing quotes for manuscript inclusion. Additionally, we explore the potential of ChatGPT to generate interview transcripts, which may be used for training purposes. We assess the strengths and limitations of using ChatGPT in these roles, highlighting areas where human intervention remains necessary. Overall, we argue that ChatGPT can function as a valuable tool during analysis, enhancing the efficiency of the thematic analysis and offering additional insights into the qualitative data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35745;&#31639;&#36741;&#21161;&#32763;&#35793;&#20013;&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34913;&#37327;&#30340;&#26631;&#20934;&#26469;&#30830;&#23450;&#22909;&#30340;&#33258;&#21160;&#34917;&#20840;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#30446;&#21069;&#26368;&#20339;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.14523</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35745;&#31639;&#36741;&#21161;&#32763;&#35793;&#20013;&#30340;&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Rethinking Word-Level Auto-Completion in Computer-Aided Translation. (arXiv:2310.14523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35745;&#31639;&#36741;&#21161;&#32763;&#35793;&#20013;&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#34913;&#37327;&#30340;&#26631;&#20934;&#26469;&#30830;&#23450;&#22909;&#30340;&#33258;&#21160;&#34917;&#20840;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#26041;&#27861;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#30446;&#21069;&#26368;&#20339;&#31995;&#32479;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#22312;&#35745;&#31639;&#36741;&#21161;&#32763;&#35793;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26088;&#22312;&#20026;&#20154;&#31867;&#32763;&#35793;&#20154;&#21592;&#25552;&#20379;&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#24314;&#35758;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#35774;&#35745;&#22797;&#26434;&#30340;&#27169;&#22411;&#26550;&#26500;&#65292;&#32780;&#26412;&#25991;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20160;&#20040;&#26679;&#30340;&#21333;&#35789;&#26159;&#22909;&#30340;&#33258;&#21160;&#34917;&#20840;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21487;&#34913;&#37327;&#30340;&#26631;&#20934;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#29616;&#26377;&#30340;&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#27169;&#22411;&#24448;&#24448;&#26080;&#27861;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#24615;&#33021;&#65292;&#20419;&#36827;&#23545;&#26631;&#20934;&#30340;&#36981;&#24490;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#25552;&#20132;&#32473;WMT2022&#21333;&#35789;&#32423;&#33258;&#21160;&#34917;&#20840;&#20849;&#20139;&#20219;&#21153;&#30340;&#26368;&#20339;&#31995;&#32479;&#65292;&#21516;&#26102;&#20351;&#29992;&#20102;&#26126;&#26174;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;
Word-Level Auto-Completion (WLAC) plays a crucial role in Computer-Assisted Translation. It aims at providing word-level auto-completion suggestions for human translators. While previous studies have primarily focused on designing complex model architectures, this paper takes a different perspective by rethinking the fundamental question: what kind of words are good auto-completions? We introduce a measurable criterion to answer this question and discover that existing WLAC models often fail to meet this criterion. Building upon this observation, we propose an effective approach to enhance WLAC performance by promoting adherence to the criterion. Notably, the proposed approach is general and can be applied to various encoder-based architectures. Through extensive experiments, we demonstrate that our approach outperforms the top-performing system submitted to the WLAC shared tasks in WMT2022, while utilizing significantly smaller model sizes.
&lt;/p&gt;</description></item><item><title>CorefPrompt&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#26469;&#36827;&#34892;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#12290;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36716;&#21270;&#20026;&#19968;&#20010;&#22635;&#31354;&#24335;MLM&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#30340;&#25552;&#31034;&#20219;&#21153;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65292;&#26368;&#32456;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.14512</link><description>&lt;p&gt;
CorefPrompt: &#22522;&#20110;&#25552;&#31034;&#30340;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36890;&#36807;&#27979;&#37327;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
CorefPrompt: Prompt-based Event Coreference Resolution by Measuring Event Type and Argument Compatibilities. (arXiv:2310.14512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14512
&lt;/p&gt;
&lt;p&gt;
CorefPrompt&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#20107;&#20214;&#31867;&#22411;&#21644;&#21442;&#25968;&#30340;&#20860;&#23481;&#24615;&#26469;&#36827;&#34892;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#12290;&#35813;&#26041;&#27861;&#23558;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36716;&#21270;&#20026;&#19968;&#20010;&#22635;&#31354;&#24335;MLM&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#30340;&#25552;&#31034;&#20219;&#21153;&#26469;&#24110;&#21161;&#27169;&#22411;&#36827;&#34892;&#25512;&#29702;&#65292;&#26368;&#32456;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#26088;&#22312;&#23558;&#25351;&#20195;&#21516;&#19968;&#23454;&#38469;&#20107;&#20214;&#30340;&#20107;&#20214;&#25552;&#21450;&#32858;&#31867;&#22312;&#19968;&#36215;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#8220;&#20808;&#32534;&#30721;&#65292;&#28982;&#21518;&#35780;&#20998;&#8221;&#30340;&#26694;&#26550;&#65292;&#20351;&#24471;&#25351;&#20195;&#28040;&#35299;&#20381;&#36182;&#20110;&#20107;&#20214;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#21033;&#29992;&#20154;&#24037;&#24635;&#32467;&#30340;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#35268;&#21017;&#65292;&#20363;&#22914;&#65292;&#25351;&#20195;&#21516;&#19968;&#20107;&#20214;&#30340;&#20107;&#20214;&#24212;&#20855;&#26377;&#30456;&#21516;&#30340;&#20107;&#20214;&#31867;&#22411;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;CorefPrompt&#65292;&#23558;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#36716;&#21270;&#20026;&#19968;&#20010;&#22635;&#31354;&#24335;MLM&#65288;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#20219;&#21153;&#12290;&#36825;&#26679;&#21487;&#20197;&#22312;&#19968;&#20010;&#21333;&#19968;&#30340;&#27169;&#26495;&#20013;&#21516;&#26102;&#36827;&#34892;&#20107;&#20214;&#24314;&#27169;&#21644;&#25351;&#20195;&#28040;&#35299;&#21028;&#21035;&#65292;&#24182;&#19988;&#20855;&#26377;&#23436;&#20840;&#20849;&#20139;&#30340;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#20010;&#36741;&#21161;&#30340;&#25552;&#31034;&#20219;&#21153;&#65292;&#20107;&#20214;&#31867;&#22411;&#20860;&#23481;&#24615;&#21644;&#21442;&#25968;&#20860;&#23481;&#24615;&#65292;&#20197;&#26126;&#30830;&#23637;&#31034;&#20107;&#20214;&#25351;&#20195;&#28040;&#35299;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24110;&#21161;&#27169;&#22411;&#20570;&#20986;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CorefPrompt&#22312;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event coreference resolution (ECR) aims to group event mentions referring to the same real-world event into clusters. Most previous studies adopt the "encoding first, then scoring" framework, making the coreference judgment rely on event encoding. Furthermore, current methods struggle to leverage human-summarized ECR rules, e.g., coreferential events should have the same event type, to guide the model. To address these two issues, we propose a prompt-based approach, CorefPrompt, to transform ECR into a cloze-style MLM (masked language model) task. This allows for simultaneous event modeling and coreference discrimination within a single template, with a fully shared context. In addition, we introduce two auxiliary prompt tasks, event-type compatibility and argument compatibility, to explicitly demonstrate the reasoning process of ECR, which helps the model make final predictions. Experimental results show that our method CorefPrompt performs well in a state-of-the-art (SOTA) benchmark.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.14356</link><description>&lt;p&gt;
&#25991;&#21270;&#21644;&#35821;&#35328;&#22810;&#26679;&#24615;&#25552;&#39640;&#20102;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Cultural and Linguistic Diversity Improves Visual Representations. (arXiv:2310.14356v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14356
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#25551;&#36848;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#24046;&#24322;&#65292;&#22810;&#35821;&#35328;&#25968;&#25454;&#26377;&#26356;&#39640;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#65292;&#24182;&#19988;&#22522;&#20110;&#22810;&#35821;&#35328;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#36890;&#24120;&#23558;&#24863;&#30693;&#35270;&#20026;&#23458;&#35266;&#30340;&#65292;&#24182;&#19988;&#36825;&#31181;&#20551;&#35774;&#22312;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#24471;&#21040;&#21453;&#26144;&#12290;&#20363;&#22914;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#22270;&#20687;&#25551;&#36848;&#36890;&#24120;&#34987;&#20551;&#23450;&#20026;&#30456;&#21516;&#35821;&#20041;&#20869;&#23481;&#30340;&#32763;&#35793;&#12290;&#28982;&#32780;&#65292;&#36328;&#25991;&#21270;&#24515;&#29702;&#23398;&#21644;&#35821;&#35328;&#23398;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20010;&#20307;&#30340;&#35270;&#35273;&#24863;&#30693;&#22240;&#20854;&#25991;&#21270;&#32972;&#26223;&#21644;&#25152;&#35828;&#30340;&#35821;&#35328;&#32780;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#26631;&#39064;&#20013;&#65292;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#35821;&#20041;&#20869;&#23481;&#24046;&#24322;&#12290;&#24403;&#25968;&#25454;&#26159;&#22810;&#35821;&#35328;&#32780;&#19981;&#26159;&#21333;&#35821;&#35328;&#26102;&#65292;&#26631;&#39064;&#30340;&#35821;&#20041;&#35206;&#30422;&#29575;&#24179;&#22343;&#26356;&#39640;&#65292;&#20197;&#22330;&#26223;&#22270;&#12289;&#23884;&#20837;&#21644;&#35821;&#35328;&#22797;&#26434;&#24615;&#36827;&#34892;&#27979;&#37327;&#12290;&#20363;&#22914;&#65292;&#19982;&#19968;&#32452;&#21333;&#35821;&#26631;&#39064;&#30456;&#27604;&#65292;&#22810;&#35821;&#26631;&#39064;&#24179;&#22343;&#26377;21.8&#65285;&#26356;&#22810;&#30340;&#23545;&#35937;&#65292;24.5&#65285;&#26356;&#22810;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;27.1&#65285;&#26356;&#22810;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer vision often treats perception as objective, and this assumption gets reflected in the way that datasets are collected and models are trained. For instance, image descriptions in different languages are typically assumed to be translations of the same semantic content. However, work in cross-cultural psychology and linguistics has shown that individuals differ in their visual perception depending on their cultural background and the language they speak. In this paper, we demonstrate significant differences in semantic content across languages in both dataset and model-produced captions. When data is multilingual as opposed to monolingual, captions have higher semantic coverage on average, as measured by scene graph, embedding, and linguistic complexity. For example, multilingual captions have on average 21.8% more objects, 24.5% more relations, and 27.1% more attributes than a set of monolingual captions. Moreover, models trained on content from different languages perform bes
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2310.13469</link><description>&lt;p&gt;
&#35753;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#24744;&#30340;&#26377;&#22122;&#38899;&#30340;&#32763;&#35793;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Ask Language Model to Clean Your Noisy Translation Data. (arXiv:2310.13469v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13469
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28165;&#29702;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#22122;&#22768;&#36755;&#20837;&#65292;&#36890;&#36807;&#20174;MTNT&#25968;&#25454;&#38598;&#20013;&#28165;&#29702;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#29983;&#25104;&#20102;C-MTNT&#25968;&#25454;&#38598;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#27169;&#22411;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#22122;&#22768;&#36755;&#20837;&#30340;&#33030;&#24369;&#24615;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#65292;&#20174;&#22122;&#22768;&#36755;&#20837;&#20013;&#29983;&#25104;&#24178;&#20928;&#30340;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;MTNT&#25968;&#25454;&#38598;&#34987;&#24191;&#27867;&#29992;&#20316;&#35780;&#20272;NMT&#27169;&#22411;&#23545;&#22122;&#22768;&#36755;&#20837;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28304;&#35821;&#21477;&#21644;&#30446;&#26631;&#35821;&#21477;&#20013;&#37117;&#23384;&#22312;&#22122;&#22768;&#65292;&#20854;&#23454;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#28165;&#29702;MTNT&#20013;&#30446;&#26631;&#35821;&#21477;&#30340;&#22122;&#22768;&#65292;&#20351;&#20854;&#26356;&#36866;&#29992;&#20110;&#22122;&#22768;&#35780;&#20272;&#30340;&#22522;&#20934;&#12290;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#22312;&#21435;&#22122;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#32771;&#34385;&#35821;&#20041;&#21547;&#20041;&#30340;&#21516;&#26102;&#21024;&#38500;&#34920;&#24773;&#31526;&#21495;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#26356;&#25913;&#20442;&#35821;&#12289;&#26415;&#35821;&#21644;&#31895;&#21475;&#12290;&#24471;&#21040;&#30340;&#25968;&#25454;&#38598;&#34987;&#31216;&#20026;C-MTNT&#65292;&#22122;&#22768;&#26174;&#33879;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have demonstrated remarkable performance in neural machine translation (NMT). However, their vulnerability to noisy input poses a significant challenge in practical implementation, where generating clean output from noisy input is crucial. The MTNT dataset \cite{MTNT} is widely used as a benchmark for evaluating the robustness of NMT models against noisy input. Nevertheless, its utility is limited due to the presence of noise in both the source and target sentences. To address this limitation, we focus on cleaning the noise from the target sentences in MTNT, making it more suitable as a benchmark for noise evaluation. Leveraging the capabilities of large language models (LLMs), we observe their impressive abilities in noise removal. For example, they can remove emojis while considering their semantic meaning. Additionally, we show that LLM can effectively rephrase slang, jargon, and profanities. The resulting datasets, called C-MTNT, exhibit significantly less noise 
&lt;/p&gt;</description></item><item><title>ImageArg-2023&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#35770;&#35777;&#31435;&#22330;&#20998;&#31867;&#21644;&#22270;&#20687;&#35828;&#26381;&#21147;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#20849;&#25910;&#21040;&#20102;&#26469;&#33258;6&#20010;&#22269;&#23478;&#30340;9&#20010;&#22242;&#38431;&#25552;&#20132;&#30340;31&#20010;&#23376;&#20219;&#21153;A&#30340;&#25552;&#20132;&#21644;21&#20010;&#23376;&#20219;&#21153;B&#30340;&#25552;&#20132;&#65292;&#26368;&#22909;&#30340;&#25552;&#20132;&#22312;&#23376;&#20219;&#21153;A&#20013;&#36798;&#21040;&#20102;0.8647&#30340;F1&#24471;&#20998;&#65292;&#22312;&#23376;&#20219;&#21153;B&#20013;&#36798;&#21040;&#20102;0.5561&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2310.12172</link><description>&lt;p&gt;
ImageArg-2023&#27010;&#36848;&#65306;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#20013;&#30340;&#39318;&#20010;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Overview of ImageArg-2023: The First Shared Task in Multimodal Argument Mining. (arXiv:2310.12172v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12172
&lt;/p&gt;
&lt;p&gt;
ImageArg-2023&#26159;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#35770;&#35777;&#31435;&#22330;&#20998;&#31867;&#21644;&#22270;&#20687;&#35828;&#26381;&#21147;&#20998;&#31867;&#20004;&#20010;&#23376;&#20219;&#21153;&#12290;&#20849;&#25910;&#21040;&#20102;&#26469;&#33258;6&#20010;&#22269;&#23478;&#30340;9&#20010;&#22242;&#38431;&#25552;&#20132;&#30340;31&#20010;&#23376;&#20219;&#21153;A&#30340;&#25552;&#20132;&#21644;21&#20010;&#23376;&#20219;&#21153;B&#30340;&#25552;&#20132;&#65292;&#26368;&#22909;&#30340;&#25552;&#20132;&#22312;&#23376;&#20219;&#21153;A&#20013;&#36798;&#21040;&#20102;0.8647&#30340;F1&#24471;&#20998;&#65292;&#22312;&#23376;&#20219;&#21153;B&#20013;&#36798;&#21040;&#20102;0.5561&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ImageArg&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#19982;EMNLP 2023 Argument Mining Workshop&#21516;&#26102;&#20030;&#21150;&#30340;&#22810;&#27169;&#24577;&#35770;&#35777;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#12290;&#35813;&#20849;&#20139;&#20219;&#21153;&#21253;&#25324;&#20004;&#20010;&#20998;&#31867;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#23376;&#20219;&#21153;A&#65306;&#35770;&#35777;&#31435;&#22330;&#20998;&#31867;&#65307;&#65288;2&#65289;&#23376;&#20219;&#21153;B&#65306;&#22270;&#20687;&#35828;&#26381;&#21147;&#20998;&#31867;&#12290;&#21069;&#32773;&#30830;&#23450;&#20102;&#21253;&#21547;&#22270;&#20687;&#21644;&#19968;&#27573;&#25991;&#23383;&#30340;&#25512;&#25991;&#23545;&#20110;&#19968;&#20010;&#26377;&#20105;&#35758;&#30340;&#20027;&#39064;&#65288;&#22914;&#26538;&#25903;&#25511;&#21046;&#21644;&#22549;&#32974;&#65289;&#30340;&#31435;&#22330;&#12290;&#21518;&#32773;&#30830;&#23450;&#22270;&#20687;&#26159;&#21542;&#20351;&#25512;&#25991;&#30340;&#25991;&#23383;&#26356;&#20855;&#35828;&#26381;&#21147;&#12290;&#20849;&#20139;&#20219;&#21153;&#20849;&#25910;&#21040;&#26469;&#33258;6&#20010;&#22269;&#23478;&#30340;9&#20010;&#19981;&#21516;&#22242;&#38431;&#25552;&#20132;&#30340;31&#20010;&#23376;&#20219;&#21153;A&#30340;&#25552;&#20132;&#21644;21&#20010;&#23376;&#20219;&#21153;B&#30340;&#25552;&#20132;&#12290;&#23376;&#20219;&#21153;A&#20013;&#26368;&#22909;&#30340;&#25552;&#20132;&#30340;F1&#24471;&#20998;&#20026;0.8647&#65292;&#32780;&#23376;&#20219;&#21153;B&#20013;&#26368;&#22909;&#30340;&#25552;&#20132;&#30340;F1&#24471;&#20998;&#20026;0.5561&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an overview of the ImageArg shared task, the first multimodal Argument Mining shared task co-located with the 10th Workshop on Argument Mining at EMNLP 2023. The shared task comprises two classification subtasks - (1) Subtask-A: Argument Stance Classification; (2) Subtask-B: Image Persuasiveness Classification. The former determines the stance of a tweet containing an image and a piece of text toward a controversial topic (e.g., gun control and abortion). The latter determines whether the image makes the tweet text more persuasive. The shared task received 31 submissions for Subtask-A and 21 submissions for Subtask-B from 9 different teams across 6 countries. The top submission in Subtask-A achieved an F1-score of 0.8647 while the best submission in Subtask-B achieved an F1-score of 0.5561.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.11878</link><description>&lt;p&gt;
&#20174;&#19981;&#19968;&#33268;&#21040;&#27934;&#23519;&#65306;&#23545;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#30340;&#29702;&#30001;&#25968;&#25454;&#38598;&#26500;&#24314;&#36827;&#34892;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
From Dissonance to Insights: Dissecting Disagreements in Rationale Dataset Construction for Case Outcome Classification. (arXiv:2310.11878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#38382;&#39064;&#65292;&#36890;&#36807;&#25910;&#38598;&#19968;&#32452;&#24459;&#24072;&#23545;&#26696;&#20214;&#32467;&#26524;&#35780;&#20272;&#23384;&#22312;&#20998;&#27495;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#36825;&#20123;&#20998;&#27495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20998;&#31867;&#20307;&#31995;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;&#26696;&#20363;&#32467;&#26524;&#20998;&#31867;&#65288;COC&#65289;&#19981;&#20165;&#38656;&#35201;&#20934;&#30830;&#24615;&#65292;&#36824;&#38656;&#35201;&#21487;&#20449;&#36182;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;COC&#30740;&#31350;&#20165;&#38480;&#20110;&#30001;&#21333;&#20010;&#19987;&#23478;&#36827;&#34892;&#30340;&#27880;&#37322;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24459;&#24072;&#22312;&#23545;&#26696;&#20214;&#20107;&#23454;&#36827;&#34892;&#35780;&#20272;&#26102;&#21487;&#33021;&#23384;&#22312;&#20998;&#27495;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;RAVE&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#29702;&#30001;&#21464;&#24322;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20174;&#22269;&#38469;&#20154;&#26435;&#27861;&#39046;&#22495;&#30340;&#20004;&#20301;&#19987;&#23478;&#37027;&#37324;&#33719;&#24471;&#30340;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20182;&#20204;&#20043;&#38388;&#23384;&#22312;&#24369;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20182;&#20204;&#30340;&#20998;&#27495;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20004;&#32423;&#20219;&#21153;&#26080;&#20851;&#30340;&#20998;&#31867;&#20307;&#31995;&#65292;&#21516;&#26102;&#34917;&#20805;&#20102;COC&#29305;&#23450;&#30340;&#23376;&#31867;&#21035;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#27861;&#24459;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#39318;&#27425;&#20851;&#27880;&#20154;&#24037;&#26631;&#27880;&#30340;&#21464;&#24322;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#19981;&#21516;&#20998;&#31867;&#31867;&#21035;&#65292;&#24182;&#21457;&#29616;&#20998;&#27495;&#20027;&#35201;&#28304;&#20110;&#23545;&#27861;&#24459;&#32972;&#26223;&#30340;&#19981;&#26126;&#30830;&#25551;&#36848;&#65292;&#36825;&#22312;COC&#20803;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#26377;&#38480;&#32454;&#31890;&#24230;&#21644;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;SOTA COC&#27169;&#22411;&#22312;RAVE&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
In legal NLP, Case Outcome Classification (COC) must not only be accurate but also trustworthy and explainable. Existing work in explainable COC has been limited to annotations by a single expert. However, it is well-known that lawyers may disagree in their assessment of case facts. We hence collect a novel dataset RAVE: Rationale Variation in ECHR1, which is obtained from two experts in the domain of international human rights law, for whom we observe weak agreement. We study their disagreements and build a two-level task-independent taxonomy, supplemented with COC-specific subcategories. To our knowledge, this is the first work in the legal NLP that focuses on human label variation. We quantitatively assess different taxonomy categories and find that disagreements mainly stem from underspecification of the legal context, which poses challenges given the typically limited granularity and noise in COC metadata. We further assess the explainablility of SOTA COC models on RAVE and observ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;QKV&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.11398</link><description>&lt;p&gt;
&#31070;&#32463;&#27880;&#24847;&#21147;&#65306;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;&#30340;QKV&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Neural Attention: Enhancing QKV Calculation in Self-Attention Mechanism with Neural Networks. (arXiv:2310.11398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#33258;&#27880;&#24847;&#26426;&#21046;&#20013;QKV&#35745;&#31639;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#65292;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#20027;&#35201;&#20351;&#29992;&#32447;&#24615;&#21464;&#25442;&#26469;&#35745;&#31639;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;(QKV)&#65292;&#20294;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#65292;&#36825;&#21487;&#33021;&#24182;&#19981;&#26159;&#26368;&#20248;&#36873;&#25321;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;QKV&#35745;&#31639;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#29305;&#27530;&#35774;&#35745;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#35745;&#31639;&#12290;&#36890;&#36807;&#22312;IWSLT 2017&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#20462;&#25913;&#21518;&#30340;Marian&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#23545;&#27604;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;BLEU&#24471;&#20998;&#26041;&#38754;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;Wikitext-103&#25968;&#25454;&#38598;&#35757;&#32451;Roberta&#27169;&#22411;&#26102;&#20063;&#34920;&#29616;&#20986;&#20248;&#21183;&#65292;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of deep learning, the self-attention mechanism has substantiated its pivotal role across a myriad of tasks, encompassing natural language processing and computer vision. Despite achieving success across diverse applications, the traditional self-attention mechanism primarily leverages linear transformations for the computation of query, key, and value (QKV), which may not invariably be the optimal choice under specific circumstances. This paper probes into a novel methodology for QKV computation-implementing a specially-designed neural network structure for the calculation. Utilizing a modified Marian model, we conducted experiments on the IWSLT 2017 German-English translation task dataset and juxtaposed our method with the conventional approach. The experimental results unveil a significant enhancement in BLEU scores with our method. Furthermore, our approach also manifested superiority when training the Roberta model with the Wikitext-103 dataset, reflecting a notable re
&lt;/p&gt;</description></item><item><title>VECHR&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#35782;&#21035;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#29702;&#30001;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#27169;&#22411;&#19982;&#19987;&#23478;&#30340;&#19968;&#33268;&#24615;&#26377;&#38480;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#26102;&#40065;&#26834;&#24615;&#20063;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11368</link><description>&lt;p&gt;
VECHR&#65306;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VECHR: A Dataset for Explainable and Robust Classification of Vulnerability Type in the European Court of Human Rights. (arXiv:2310.11368v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11368
&lt;/p&gt;
&lt;p&gt;
VECHR&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#28431;&#27934;&#31867;&#22411;&#30340;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#20998;&#31867;&#12290;&#35813;&#25968;&#25454;&#38598;&#24110;&#21161;&#35782;&#21035;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#29702;&#30001;&#12290;&#32467;&#26524;&#26174;&#31034;&#20102;&#35813;&#20219;&#21153;&#30340;&#25361;&#25112;&#24615;&#65292;&#27169;&#22411;&#19982;&#19987;&#23478;&#30340;&#19968;&#33268;&#24615;&#26377;&#38480;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#26102;&#40065;&#26834;&#24615;&#20063;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#33030;&#24369;&#24615;&#23545;&#20110;&#20102;&#35299;&#21644;&#23454;&#26045;&#26377;&#38024;&#23545;&#24615;&#30340;&#25903;&#25345;&#20197;&#22686;&#24378;&#26377;&#38656;&#35201;&#30340;&#20010;&#20154;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#19968;&#28857;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#23588;&#20026;&#37325;&#35201;&#65292;&#27861;&#38498;&#23558;&#20844;&#32422;&#26631;&#20934;&#35843;&#25972;&#20026;&#28385;&#36275;&#23454;&#38469;&#20010;&#20307;&#38656;&#27714;&#65292;&#20174;&#32780;&#30830;&#20445;&#26377;&#25928;&#30340;&#20154;&#26435;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#33030;&#24369;&#24615;&#27010;&#24565;&#22312;&#27431;&#27954;&#20154;&#26435;&#27861;&#38498;&#20173;&#28982;&#27169;&#31946;&#19981;&#28165;&#65292;&#20043;&#21069;&#27809;&#26377;NLP&#30740;&#31350;&#28041;&#21450;&#21040;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VECHR&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#19987;&#23478;&#27880;&#37322;&#30340;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33030;&#24369;&#24615;&#31867;&#22411;&#20998;&#31867;&#21644;&#35299;&#37322;&#29702;&#30001;&#12290;&#25105;&#20204;&#20174;&#39044;&#27979;&#21644;&#35299;&#37322;&#24615;&#30340;&#35282;&#24230;&#23545;VECHR&#19978;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#19968;&#20219;&#21153;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29305;&#28857;&#65292;&#39044;&#27979;&#24615;&#33021;&#36739;&#20302;&#65292;&#24182;&#19988;&#27169;&#22411;&#21644;&#19987;&#23478;&#20043;&#38388;&#23384;&#22312;&#26377;&#38480;&#30340;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#22788;&#29702;&#22495;&#22806;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#21457;&#29616;&#24635;&#20307;&#19978;&#21463;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing vulnerability is crucial for understanding and implementing targeted support to empower individuals in need. This is especially important at the European Court of Human Rights (ECtHR), where the court adapts Convention standards to meet actual individual needs and thus ensures effective human rights protection. However, the concept of vulnerability remains elusive at the ECtHR and no prior NLP research has dealt with it. To enable future research in this area, we present VECHR, a novel expert-annotated multi-label dataset comprising of vulnerability type classification and explanation rationale. We benchmark the performance of state-of-the-art models on VECHR from both prediction and explainability perspectives. Our results demonstrate the challenging nature of the task with lower prediction performance and limited agreement between models and experts. Further, we analyze the robustness of these models in dealing with out-of-domain (OOD) data and observe overall limited per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;&#20174;&#20445;&#25252;&#26032;&#38395;&#25991;&#26412;&#26500;&#24314;&#19968;&#31181;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#31867;&#30340;&#22522;&#32447;&#23454;&#39564;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#27880;&#20989;&#25968;&#20379;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2310.11258</link><description>&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#29983;&#25104;&#21360;&#23612;&#20445;&#25252;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Utilizing Weak Supervision To Generate Indonesian Conservation Dataset. (arXiv:2310.11258v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24369;&#30417;&#30563;&#26041;&#27861;&#20174;&#20445;&#25252;&#26032;&#38395;&#25991;&#26412;&#26500;&#24314;&#19968;&#31181;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#31867;&#30340;&#22522;&#32447;&#23454;&#39564;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#24067;&#20102;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#27880;&#20989;&#25968;&#20379;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24555;&#36895;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21019;&#24314;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20197;&#21709;&#24212;&#21152;&#36895;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24320;&#21457;&#30340;&#38656;&#27714;&#22686;&#21152;&#12290;&#36890;&#36807;&#21033;&#29992;&#26631;&#27880;&#20989;&#25968;&#65292;&#24369;&#30417;&#30563;&#20801;&#35768;&#20174;&#20135;&#29983;&#36719;&#26631;&#31614;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#27169;&#22411;&#21019;&#24314;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#26088;&#22312;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#20174;&#20445;&#25252;&#26032;&#38395;&#25991;&#26412;&#26500;&#24314;&#21360;&#23612;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#65306;&#22810;&#31867;&#21035;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#31867;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#32447;&#23454;&#39564;&#12290;&#36825;&#20123;&#22522;&#32447;&#32467;&#26524;&#23637;&#31034;&#20102;&#24773;&#24863;&#20998;&#31867;&#20026;59.79%&#20934;&#30830;&#29575;&#21644;55.72% F1&#20998;&#25968;&#65292;&#22810;&#31867;&#21035;&#20998;&#31867;&#20026;66.87% F1&#20998;&#25968;-&#23439;&#24179;&#22343;&#65292;71.5% F1&#20998;&#25968;-&#24494;&#24179;&#22343;&#21644;83.67% ROC-AUC&#30340;&#27979;&#35797;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#27880;&#20989;&#25968;&#65292;&#20379;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#25506;&#32034;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weak supervision has emerged as a promising approach for rapid and large-scale dataset creation in response to the increasing demand for accelerated NLP development. By leveraging labeling functions, weak supervision allows practitioners to generate datasets quickly by creating learned label models that produce soft-labeled datasets. This paper aims to show how such an approach can be utilized to build an Indonesian NLP dataset from conservation news text. We construct two types of datasets: multi-class classification and sentiment classification. We then provide baseline experiments using various pretrained language models. These baseline results demonstrate test performances of 59.79% accuracy and 55.72% F1-score for sentiment classification, 66.87% F1-score-macro, 71.5% F1-score-micro, and 83.67% ROC-AUC for multi-class classification. Additionally, we release the datasets and labeling functions used in this work for further research and exploration.
&lt;/p&gt;</description></item><item><title>TRIGO&#26159;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24418;&#24335;&#25968;&#23398;&#35777;&#26126;&#20943;&#32553;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35201;&#27714;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#25353;&#27493;&#39588;&#35777;&#26126;&#20943;&#23569;&#19977;&#35282;&#34920;&#36798;&#24335;&#65292;&#36824;&#21487;&#20197;&#35780;&#20272;&#20854;&#23545;&#20844;&#24335;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#21450;&#25805;&#20316;&#12289;&#20998;&#32452;&#21644;&#22240;&#24335;&#20998;&#35299;&#25968;&#23383;&#39033;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10180</link><description>&lt;p&gt;
TRIGO:&#22522;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24418;&#24335;&#25968;&#23398;&#35777;&#26126;&#20943;&#32553;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TRIGO: Benchmarking Formal Mathematical Proof Reduction for Generative Language Models. (arXiv:2310.10180v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10180
&lt;/p&gt;
&lt;p&gt;
TRIGO&#26159;&#19968;&#20010;&#22522;&#20110;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#24418;&#24335;&#25968;&#23398;&#35777;&#26126;&#20943;&#32553;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35201;&#27714;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#25353;&#27493;&#39588;&#35777;&#26126;&#20943;&#23569;&#19977;&#35282;&#34920;&#36798;&#24335;&#65292;&#36824;&#21487;&#20197;&#35780;&#20272;&#20854;&#23545;&#20844;&#24335;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#21450;&#25805;&#20316;&#12289;&#20998;&#32452;&#21644;&#22240;&#24335;&#20998;&#35299;&#25968;&#23383;&#39033;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;(ATP)&#24050;&#25104;&#20026;&#25506;&#32034;&#26368;&#26032;&#25104;&#21151;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#21560;&#24341;&#20154;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ATP&#22522;&#20934;&#20027;&#35201;&#38598;&#20013;&#22312;&#31526;&#21495;&#25512;&#29702;&#19978;&#65292;&#24456;&#23569;&#28041;&#21450;&#22797;&#26434;&#25968;&#23383;&#32452;&#21512;&#25512;&#29702;&#30340;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;TRIGO&#65292;&#19968;&#20010;ATP&#22522;&#20934;&#27979;&#35797;&#65292;&#19981;&#20165;&#35201;&#27714;&#27169;&#22411;&#25353;&#29031;&#27493;&#39588;&#35777;&#26126;&#20943;&#23569;&#19977;&#35282;&#34920;&#36798;&#24335;&#65292;&#36824;&#35201;&#35780;&#20272;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#23545;&#20844;&#24335;&#30340;&#25512;&#29702;&#33021;&#21147;&#20197;&#21450;&#25805;&#20316;&#12289;&#20998;&#32452;&#21644;&#22240;&#24335;&#20998;&#35299;&#25968;&#23383;&#39033;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20174;&#32593;&#32476;&#19978;&#25910;&#38598;&#19977;&#35282;&#34920;&#36798;&#24335;&#21450;&#20854;&#31616;&#21270;&#24418;&#24335;&#65292;&#25163;&#21160;&#26631;&#27880;&#31616;&#21270;&#36807;&#31243;&#65292;&#24182;&#23558;&#20854;&#32763;&#35793;&#25104;Lean&#24418;&#24335;&#21270;&#35821;&#35328;&#31995;&#32479;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#26631;&#27880;&#26679;&#26412;&#33258;&#21160;&#29983;&#25104;&#39069;&#22806;&#30340;&#31034;&#20363;&#26469;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;Lean-Gym&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#21019;&#24314;&#38590;&#24230;&#21644;&#20998;&#24067;&#21508;&#24322;&#30340;&#25968;&#25454;&#38598;&#25286;&#20998;&#65292;&#20197;&#20415;&#36827;&#34892;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated theorem proving (ATP) has become an appealing domain for exploring the reasoning ability of the recent successful generative language models. However, current ATP benchmarks mainly focus on symbolic inference, but rarely involve the understanding of complex number combination reasoning. In this work, we propose TRIGO, an ATP benchmark that not only requires a model to reduce a trigonometric expression with step-by-step proofs but also evaluates a generative LM's reasoning ability on formulas and its capability to manipulate, group, and factor number terms. We gather trigonometric expressions and their reduced forms from the web, annotate the simplification process manually, and translate it into the Lean formal language system. We then automatically generate additional examples from the annotated samples to expand the dataset. Furthermore, we develop an automatic generator based on Lean-Gym to create dataset splits of varying difficulties and distributions in order to thoroug
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.09168</link><description>&lt;p&gt;
&#25506;&#32034;&#25351;&#23548;&#65306;&#36890;&#36807;&#20027;&#21160;&#25506;&#32034;&#22686;&#24378;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#29575;
&lt;/p&gt;
&lt;p&gt;
Explore-Instruct: Enhancing Domain-Specific Instruction Coverage through Active Exploration. (arXiv:2310.09168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09168
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25506;&#32034;&#25351;&#23548;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#36827;&#34892;&#20027;&#21160;&#25506;&#32034;&#65292;&#22686;&#24378;&#20102;&#39046;&#22495;&#29305;&#23450;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#21487;&#20197;&#22823;&#24133;&#20248;&#21270;&#25351;&#23548;&#35843;&#20248;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#26356;&#24191;&#27867;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#27492;&#31867;&#35843;&#20248;&#30340;&#29616;&#26377;&#25968;&#25454;&#24448;&#24448;&#23545;&#20010;&#21035;&#39046;&#22495;&#30340;&#35206;&#30422;&#19981;&#36275;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#39046;&#22495;&#20869;&#32454;&#33268;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#25506;&#32034;&#25351;&#23548;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#20027;&#21160;&#25506;&#32034;&#26469;&#22686;&#24378;&#29992;&#20110;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35843;&#20248;&#30340;&#25968;&#25454;&#35206;&#30422;&#12290;&#25506;&#32034;&#25351;&#23548;&#22522;&#20110;&#20856;&#22411;&#30340;&#39046;&#22495;&#20351;&#29992;&#26696;&#20363;&#65292;&#36890;&#36807;&#23454;&#29616;&#25628;&#32034;&#31639;&#27861;&#26469;&#33719;&#21462;&#22810;&#26679;&#21270;&#21644;&#38754;&#21521;&#39046;&#22495;&#30340;&#25351;&#23548;&#35843;&#20248;&#25968;&#25454;&#30340;&#22810;&#31181;&#21464;&#20307;&#25110;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#20013;&#24515;&#20998;&#26512;&#39564;&#35777;&#20102;&#27492;&#26041;&#27861;&#22312;&#25913;&#36827;&#29305;&#23450;&#39046;&#22495;&#25351;&#23548;&#35206;&#30422;&#33539;&#22260;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#26174;&#31034;&#20986;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuning can be substantially optimized through enhanced diversity, resulting in models capable of handling a broader spectrum of tasks. However, existing data employed for such tuning often exhibit an inadequate coverage of individual domains, limiting the scope for nuanced comprehension and interactions within these areas. To address this deficiency, we propose Explore-Instruct, a novel approach to enhance the data coverage to be used in domain-specific instruction-tuning through active exploration via Large Language Models (LLMs). Built upon representative domain use cases, Explore-Instruct explores a multitude of variations or possibilities by implementing a search algorithm to obtain diversified and domain-focused instruction-tuning data. Our data-centric analysis validates the effectiveness of this proposed approach in improving domain-specific instruction coverage. Moreover, our model's performance demonstrates considerable advancements over multiple baselines, includi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PuoBERTa&#30340;&#23450;&#21046;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22622;&#33576;&#29926;&#32435;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20419;&#36827;&#22622;&#33576;&#29926;&#32435;&#35821;&#31561;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09141</link><description>&lt;p&gt;
PuoBERTa:&#35757;&#32451;&#21644;&#35780;&#20272;&#19968;&#31181;&#20026;&#22622;&#33576;&#29926;&#32435;&#35821;&#23450;&#21046;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PuoBERTa: Training and evaluation of a curated language model for Setswana. (arXiv:2310.09141v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PuoBERTa&#30340;&#23450;&#21046;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#38024;&#23545;&#22622;&#33576;&#29926;&#32435;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#20419;&#36827;&#22622;&#33576;&#29926;&#32435;&#35821;&#31561;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65288;&#22914;&#22622;&#33576;&#29926;&#32435;&#35821;&#65289;&#26041;&#38754;&#21364;&#28382;&#21518;&#12290;&#26412;&#25991;&#36890;&#36807;&#20171;&#32461;PuoBERTa&#65292;&#19968;&#31181;&#19987;&#38376;&#20026;&#22622;&#33576;&#29926;&#32435;&#35821;&#35757;&#32451;&#30340;&#23450;&#21046;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#20171;&#32461;&#22914;&#20309;&#25910;&#38598;&#12289;&#31579;&#36873;&#21644;&#20934;&#22791;&#22810;&#26679;&#21270;&#30340;&#21333;&#35821;&#25991;&#26412;&#65292;&#20026;PuoBERTa&#30340;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#12290;&#22312;&#20043;&#21069;&#20026;&#22622;&#33576;&#29926;&#32435;&#35821;&#21019;&#24314;&#21333;&#35821;&#36164;&#28304;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;PuoBERTa&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#26032;&#38395;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22622;&#33576;&#29926;&#32435;&#35821;&#26032;&#38395;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;PuoBERTa&#30340;&#21021;&#22987;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;PuoBERTa&#22312;&#20419;&#36827;&#22622;&#33576;&#29926;&#32435;&#35821;&#31561;&#23569;&#30740;&#31350;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing (NLP) has made significant progress for well-resourced languages such as English but lagged behind for low-resource languages like Setswana. This paper addresses this gap by presenting PuoBERTa, a customised masked language model trained specifically for Setswana. We cover how we collected, curated, and prepared diverse monolingual texts to generate a high-quality corpus for PuoBERTa's training. Building upon previous efforts in creating monolingual resources for Setswana, we evaluated PuoBERTa across several NLP tasks, including part-of-speech (POS) tagging, named entity recognition (NER), and news categorisation. Additionally, we introduced a new Setswana news categorisation dataset and provided the initial benchmarks using PuoBERTa. Our work demonstrates the efficacy of PuoBERTa in fostering NLP capabilities for understudied languages like Setswana and paves the way for future research directions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20498;&#21521;&#39564;&#35777;&#30340;&#33258;&#26816;&#26041;&#27861;&#21644;&#19968;&#20010;&#21517;&#20026;PHD&#30340;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#35813;&#26041;&#27861;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.06498</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#21644;&#20498;&#21521;&#39564;&#35777;&#26041;&#27861;&#29992;&#20110;&#27573;&#33853;&#32423;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection. (arXiv:2310.06498v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20498;&#21521;&#39564;&#35777;&#30340;&#33258;&#26816;&#26041;&#27861;&#21644;&#19968;&#20010;&#21517;&#20026;PHD&#30340;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#35813;&#26041;&#27861;&#22312;&#27573;&#33853;&#32423;&#21035;&#19978;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19982;&#20154;&#31867;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#21327;&#20316;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#24456;&#23481;&#26131;&#29983;&#25104;&#24187;&#35273;&#65292;&#21363;&#32534;&#36896;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#20449;&#24687;&#65292;&#36825;&#22312;&#37096;&#32626;&#20110;&#37325;&#35201;&#20219;&#21153;&#20013;&#26102;&#21487;&#33021;&#36896;&#25104;&#37325;&#22823;&#25439;&#23475;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20498;&#21521;&#39564;&#35777;&#30340;&#33258;&#26816;&#26041;&#27861;&#65292;&#20197;&#38646;&#36164;&#28304;&#30340;&#26041;&#24335;&#33258;&#21160;&#26816;&#27979;&#20107;&#23454;&#38169;&#35823;&#12290;&#20026;&#20102;&#20415;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#35780;&#20272;&#19981;&#21516;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;PHD&#30340;&#24187;&#35273;&#26816;&#27979;&#22522;&#20934;&#65292;&#35813;&#22522;&#20934;&#30001;ChatGPT&#29983;&#25104;&#24182;&#30001;&#20154;&#31867;&#26631;&#27880;&#12290;&#19982;&#20197;&#24448;&#30340;&#38646;&#36164;&#28304;&#24187;&#35273;&#26816;&#27979;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#22522;&#20934;&#38598;&#19987;&#27880;&#20110;&#27573;&#33853;&#32423;&#21035;&#30340;&#26816;&#27979;&#65292;&#32780;&#19981;&#26159;&#21477;&#23376;&#32423;&#21035;&#30340;&#26816;&#27979;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21644;&#29616;&#26377;&#30340;&#38646;&#36164;&#28304;&#26816;&#27979;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#21516;&#26102;&#25104;&#26412;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown their ability to collaborate effectively with humans in real-world scenarios. However, LLMs are apt to generate hallucinations, i.e., makeup incorrect text and unverified information, which can cause significant damage when deployed for mission-critical tasks. In this paper, we propose a self-check approach based on reverse validation to detect factual errors automatically in a zero-resource fashion. To facilitate future studies and assess different methods, we construct a hallucination detection benchmark named PHD, which is generated by ChatGPT and annotated by human annotators. Contrasting previous studies of zero-resource hallucination detection, our method and benchmark concentrate on passage-level detection instead of sentence-level. We empirically evaluate our method and existing zero-resource detection methods on two datasets. The experimental results demonstrate that the proposed method considerably outperforms the baselines while costin
&lt;/p&gt;</description></item><item><title>GROVE&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#22797;&#26434;&#25925;&#20107;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20248;&#31168;&#20154;&#31867;&#20889;&#20316;&#25925;&#20107;&#30340;&#20449;&#24687;&#21644;&#32454;&#33410;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#32780;&#21487;&#20449;&#30340;&#24773;&#33410;&#30340;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2310.05388</link><description>&lt;p&gt;
GROVE: &#19968;&#31181;&#24102;&#26377;&#35777;&#25454;&#26862;&#26519;&#30340;&#26816;&#32034;&#22686;&#24378;&#22797;&#26434;&#25925;&#20107;&#29983;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GROVE: A Retrieval-augmented Complex Story Generation Framework with A Forest of Evidence. (arXiv:2310.05388v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05388
&lt;/p&gt;
&lt;p&gt;
GROVE&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#22797;&#26434;&#25925;&#20107;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20248;&#31168;&#20154;&#31867;&#20889;&#20316;&#25925;&#20107;&#30340;&#20449;&#24687;&#21644;&#32454;&#33410;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#32780;&#21487;&#20449;&#30340;&#24773;&#33410;&#30340;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26465;&#20214;&#25925;&#20107;&#29983;&#25104;&#22312;&#20154;&#26426;&#20132;&#20114;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#24773;&#33410;&#30340;&#25925;&#20107;&#26041;&#38754;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21253;&#25324;&#25925;&#20107;&#29983;&#25104;&#65292;&#20294;&#26159;&#29983;&#25104;&#26082;&#22797;&#26434;&#21448;&#26377;&#21019;&#24847;&#30340;&#25925;&#20107;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#35814;&#32454;&#30340;&#25552;&#31034;&#26469;&#24341;&#23548;LLMs&#28385;&#36275;&#30446;&#26631;&#26465;&#20214;&#65292;&#36825;&#26080;&#24847;&#38388;&#38480;&#21046;&#20102;&#29983;&#25104;&#25925;&#20107;&#30340;&#21019;&#36896;&#28508;&#21147;&#12290;&#25105;&#20204;&#35748;&#20026;&#21033;&#29992;&#20248;&#31168;&#30340;&#20154;&#31867;&#20889;&#20316;&#25925;&#20107;&#30340;&#20449;&#24687;&#26377;&#21161;&#20110;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#30340;&#24773;&#33410;&#12290;&#28145;&#20837;&#30740;&#31350;&#25925;&#20107;&#32454;&#33410;&#26377;&#21161;&#20110;&#26500;&#24314;&#22797;&#26434;&#32780;&#21487;&#20449;&#30340;&#24773;&#33410;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26816;&#32034;&#22686;&#24378;&#30340;&#24102;&#26377;&#35777;&#25454;&#26862;&#26519;&#30340;&#25925;&#20107;&#29983;&#25104;&#26694;&#26550;&#65288;GROVE&#65289;&#26469;&#22686;&#24378;&#25925;&#20107;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26465;&#20214;&#30340;&#26816;&#32034;&#23384;&#20648;&#24211;&#65292;&#20197;&#20135;&#29983;&#23569;&#26679;&#26412;&#31034;&#20363;&#26469;&#24341;&#23548;LLMs&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#35810;&#38382;&#20026;&#20160;&#20040;&#8221;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional story generation is significant in human-machine interaction, particularly in producing stories with complex plots. While Large language models (LLMs) perform well on multiple NLP tasks, including story generation, it is challenging to generate stories with both complex and creative plots. Existing methods often rely on detailed prompts to guide LLMs to meet target conditions, which inadvertently restrict the creative potential of the generated stories. We argue that leveraging information from exemplary human-written stories facilitates generating more diverse plotlines. Delving deeper into story details helps build complex and credible plots. In this paper, we propose a retrieval-au\textbf{G}mented sto\textbf{R}y generation framework with a f\textbf{O}rest of e\textbf{V}id\textbf{E}nce (GROVE) to enhance stories' complexity. We build a retrieval repository for target conditions to produce few-shot examples to prompt LLMs. Additionally, we design an ``asking-why'' promptin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;</title><link>http://arxiv.org/abs/2310.05374</link><description>&lt;p&gt;
&#36890;&#36807;&#26377;&#25928;&#21033;&#29992;&#25991;&#26412;&#25968;&#25454;&#21512;&#25104;&#25552;&#39640;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Improving End-to-End Speech Processing by Efficient Text Data Utilization with Latent Synthesis. (arXiv:2310.05374v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#26469;&#22686;&#24378;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;LaSyn&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#22521;&#35757;&#39640;&#24615;&#33021;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#25991;&#26412;&#25968;&#25454;&#30456;&#27604;&#65292;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#36890;&#24120;&#26356;&#21152;&#31232;&#32570;&#21644;&#26114;&#36149;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaSyn&#30340;&#26377;&#25928;&#30340;&#25991;&#26412;&#25968;&#25454;&#21033;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#35821;&#38899;&#22788;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#35757;&#32451;&#19968;&#20010;&#28508;&#21464;&#21512;&#25104;&#22120;&#23558;&#25991;&#26412;&#25968;&#25454;&#36716;&#25442;&#20026;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#20013;&#38388;&#28508;&#21464;&#34920;&#31034;&#12290;&#36825;&#20123;&#20266;&#22768;&#23398;&#34920;&#31034;&#29992;&#20110;&#22686;&#24378;&#27169;&#22411;&#35757;&#32451;&#30340;&#22768;&#23398;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20302;&#36164;&#28304;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;LaSyn&#12290;&#23545;&#20110;ASR&#65292;LaSyn&#25913;&#36827;&#20102;&#22312;LibriSpeech train-clean-100&#19978;&#35757;&#32451;&#30340;E2E&#22522;&#32447;&#65292;&#22312;&#19981;&#21516;&#30340;&#27979;&#35797;&#38598;&#19978;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#20943;&#23569;&#20102;22.3%&#12290;&#23545;&#20110;SLU&#65292;LaSyn&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;E2E&#22522;&#32447;&#65292;&#32477;&#23545;&#24847;&#22270;&#20998;&#31867;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;4.1%&#12290;
&lt;/p&gt;
&lt;p&gt;
Training a high performance end-to-end speech (E2E) processing model requires an enormous amount of labeled speech data, especially in the era of data-centric artificial intelligence. However, labeled speech data are usually scarcer and more expensive for collection, compared to textual data. We propose Latent Synthesis (LaSyn), an efficient textual data utilization framework for E2E speech processing models. We train a latent synthesizer to convert textual data into an intermediate latent representation of a pre-trained speech model. These pseudo acoustic representations of textual data augment acoustic data for model training. We evaluate LaSyn on low-resource automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. For ASR, LaSyn improves an E2E baseline trained on LibriSpeech train-clean-100, with relative word error rate reductions over 22.3% on different test sets. For SLU, LaSyn improves our E2E baseline by absolute 4.1% for intent classification accurac
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21453;&#22270;&#28789;&#27979;&#35797;&#65288;CT^2&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#38754;&#23545;&#29983;&#25104;AI&#30340;&#39118;&#38505;&#21644;&#21518;&#26524;&#24341;&#36215;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;AI&#29983;&#25104;&#20316;&#21697;&#24402;&#23646;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.05030</link><description>&lt;p&gt;
&#21453;&#22270;&#28789;&#27979;&#35797;CT^2&#65306;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#27809;&#26377;&#20320;&#24819;&#35937;&#30340;&#37027;&#20040;&#23481;&#26131;&#8212;&#8212;&#24341;&#20837;AI&#21487;&#26816;&#27979;&#24615;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think -- Introducing AI Detectability Index. (arXiv:2310.05030v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05030
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;&#21453;&#22270;&#28789;&#27979;&#35797;&#65288;CT^2&#65289;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#38754;&#23545;&#29983;&#25104;AI&#30340;&#39118;&#38505;&#21644;&#21518;&#26524;&#24341;&#36215;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#65292;&#35299;&#20915;AI&#29983;&#25104;&#20316;&#21697;&#24402;&#23646;&#38382;&#39064;&#21464;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#29983;&#25104;AI&#30340;&#20852;&#36215;&#65292;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#39118;&#38505;&#21644;&#21518;&#26524;&#36234;&#26469;&#36234;&#24341;&#36215;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#23545;AI&#29983;&#25104;&#20316;&#21697;&#30340;&#24402;&#23646;&#38382;&#39064;&#65292;&#32654;&#22269;&#29256;&#26435;&#23616;&#21457;&#24067;&#20102;&#19968;&#20221;&#22768;&#26126;&#65292;&#25351;&#20986;&#8220;&#22914;&#26524;&#19968;&#20214;&#20316;&#21697;&#30340;&#20256;&#32479;&#21019;&#20316;&#20803;&#32032;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#37027;&#20040;&#36825;&#20214;&#20316;&#21697;&#23601;&#32570;&#20047;&#20154;&#31867;&#21019;&#20316;&#65292;&#29256;&#26435;&#23616;&#23558;&#19981;&#20250;&#30331;&#35760;&#23427;&#8221;&#12290;&#27492;&#22806;&#65292;&#32654;&#22269;&#21644;&#27431;&#30431;&#25919;&#24220;&#26368;&#36817;&#20063;&#33609;&#25311;&#20102;&#20851;&#20110;AI&#30417;&#31649;&#26694;&#26550;&#30340;&#21021;&#27493;&#25552;&#26696;&#12290;&#22312;&#36825;&#31181;&#23545;&#29983;&#25104;AI&#30340;&#20851;&#27880;&#20013;&#65292;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#21463;&#21040;&#30740;&#31350;&#31435;&#21363;&#20851;&#27880;&#30340;&#35805;&#39064;&#65292;&#19968;&#20123;&#21021;&#27493;&#26041;&#27861;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#38543;&#21518;&#20986;&#29616;&#20102;&#32469;&#36807;&#26816;&#27979;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#21453;&#22270;&#28789;&#27979;&#35797;&#65288;CT^2&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#65292;&#26088;&#22312;&#20840;&#38754;&#35780;&#20272;&#29616;&#26377;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#25216;&#26415;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rise of prolific ChatGPT, the risk and consequences of AI-generated text has increased alarmingly. To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office released a statement stating that 'If a work's traditional elements of authorship were produced by a machine, the work lacks human authorship and the Office will not register it'. Furthermore, both the US and the EU governments have recently drafted their initial proposals regarding the regulatory framework for AI. Given this cynosural spotlight on generative AI, AI-generated text detection (AGTD) has emerged as a topic that has already received immediate attention in research, with some initial methods having been proposed, soon followed by emergence of techniques to bypass detection. This paper introduces the Counter Turing Test (CT^2), a benchmark consisting of techniques aiming to offer a comprehensive evaluation of the robustness of existing AGTD techniques. Our em
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;</title><link>http://arxiv.org/abs/2310.03368</link><description>&lt;p&gt;
&#35780;&#20272;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Evaluating Hallucinations in Chinese Large Language Models. (arXiv:2310.03368v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#65292;&#36890;&#36807;&#24314;&#31435;HalluQA&#22522;&#20934;&#27979;&#35797;&#21644;&#20351;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#65292;&#21457;&#29616;18&#20010;&#27169;&#22411;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#31867;&#22411;&#21644;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;HalluQA&#65288;&#20013;&#25991;&#24187;&#35273;&#38382;&#31572;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#34913;&#37327;&#20013;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#29616;&#35937;&#12290;HalluQA&#21253;&#21547;450&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#38382;&#39064;&#65292;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#32771;&#34385;&#20102;&#20013;&#22269;&#21382;&#21490;&#25991;&#21270;&#12289;&#39118;&#20439;&#21644;&#31038;&#20250;&#29616;&#35937;&#12290;&#22312;&#26500;&#24314;HalluQA&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24187;&#35273;&#31867;&#22411;&#65306;&#27169;&#20223;&#24615;&#34394;&#20551;&#21644;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#22522;&#20110;GLM-130B&#21644;ChatGPT&#26500;&#24314;&#23545;&#25239;&#26679;&#26412;&#12290;&#20026;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#26041;&#27861;&#26469;&#21028;&#26029;&#27169;&#22411;&#36755;&#20986;&#26159;&#21542;&#26159;&#24187;&#35273;&#12290;&#25105;&#20204;&#23545;24&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;ERNIE-Bot&#12289;Baichuan2&#12289;ChatGLM&#12289;Qwen&#12289;SparkDesk&#31561;&#12290;&#22312;&#36825;24&#20010;&#27169;&#22411;&#20013;&#65292;&#26377;18&#20010;&#30340;&#38750;&#24187;&#35273;&#29575;&#20302;&#20110;50%&#12290;&#36825;&#34920;&#26126;HalluQA&#20855;&#26377;&#24456;&#39640;&#30340;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#19981;&#21516;&#31867;&#22411;&#27169;&#22411;&#20013;&#20027;&#35201;&#30340;&#24187;&#35273;&#31867;&#22411;&#21450;&#20854;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we establish a benchmark named HalluQA (Chinese Hallucination Question-Answering) to measure the hallucination phenomenon in Chinese large language models. HalluQA contains 450 meticulously designed adversarial questions, spanning multiple domains, and takes into account Chinese historical culture, customs, and social phenomena. During the construction of HalluQA, we consider two types of hallucinations: imitative falsehoods and factual errors, and we construct adversarial samples based on GLM-130B and ChatGPT. For evaluation, we design an automated evaluation method using GPT-4 to judge whether a model output is hallucinated. We conduct extensive experiments on 24 large language models, including ERNIE-Bot, Baichuan2, ChatGLM, Qwen, SparkDesk and etc. Out of the 24 models, 18 achieved non-hallucination rates lower than 50%. This indicates that HalluQA is highly challenging. We analyze the primary types of hallucinations in different types of models and their causes. Add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2310.03128</link><description>&lt;p&gt;
MetaTool&#22522;&#20934;&#65306;&#20915;&#23450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#21644;&#36873;&#25321;&#20351;&#29992;&#21738;&#20010;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
MetaTool Benchmark: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MetaTool&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#12290;&#22522;&#20934;&#20013;&#21253;&#21547;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#29992;&#20110;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;LLMs&#30340;&#24037;&#20855;&#21033;&#29992;&#33021;&#21147;&#12290;&#23427;&#20204;&#20027;&#35201;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#26377;&#25928;&#22320;&#19982;&#32473;&#23450;&#30340;&#29305;&#23450;&#24037;&#20855;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;LLMs&#20805;&#24403;&#26234;&#33021;&#20307;&#30340;&#22330;&#26223;&#20013;&#65292;&#20363;&#22914;AutoGPT&#21644;MetaGPT&#24212;&#29992;&#20013;&#65292;LLMs&#34987;&#26399;&#26395;&#21442;&#19982;&#28041;&#21450;&#26159;&#21542;&#20351;&#29992;&#24037;&#20855;&#20197;&#21450;&#20174;&#21487;&#29992;&#24037;&#20855;&#38598;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#24037;&#20855;&#26469;&#28385;&#36275;&#29992;&#25143;&#35831;&#27714;&#30340;&#22797;&#26434;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MetaTool&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#26159;&#21542;&#20855;&#26377;&#24037;&#20855;&#20351;&#29992;&#24847;&#35782;&#24182;&#19988;&#33021;&#22815;&#27491;&#30830;&#36873;&#25321;&#24037;&#20855;&#30340;&#22522;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#35813;&#22522;&#20934;&#20013;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;ToolE&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20197;&#35302;&#21457;LLMs&#20351;&#29992;&#24037;&#20855;&#30340;&#25552;&#31034;&#24418;&#24335;&#20986;&#29616;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#29992;&#25143;&#26597;&#35810;&#65292;&#21253;&#25324;&#21333;&#19968;&#24037;&#20855;&#21644;&#22810;&#31181;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention due to their impressive natural language processing (NLP) capabilities. Recently, many studies have focused on the tool utilization ability of LLMs. They primarily investigated how LLMs effectively collaborate with given specific tools. However, in scenarios where LLMs serve as intelligent agents, as seen in applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate decision-making processes that involve deciding whether to employ a tool and selecting the most suitable tool(s) from a collection of available tools to fulfill user requests. Therefore, in this paper, we introduce MetaTool, a benchmark designed to evaluate whether LLMs have tool usage awareness and can correctly choose tools. Specifically, we create a dataset called ToolE within the benchmark. This dataset contains various types of user queries in the form of prompts that trigger LLMs to use tools, including both single-tool and multi-too
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#29983;&#25104;&#21644;&#36328;&#35821;&#35328;ASR&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;</title><link>http://arxiv.org/abs/2310.02971</link><description>&lt;p&gt;
&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompting and Adapter Tuning for Self-supervised Encoder-Decoder Speech Model. (arXiv:2310.02971v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02971
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#20026;&#33258;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#24207;&#21015;&#29983;&#25104;&#21644;&#36328;&#35821;&#35328;ASR&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#24050;&#32463;&#25104;&#20026;&#32454;&#35843;&#65288;FT&#65289;&#26041;&#27861;&#30340;&#26377;&#25928;&#26367;&#20195;&#21697;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#20110;&#35821;&#38899;&#25552;&#31034;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24182;&#22312;&#26356;&#22797;&#26434;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36866;&#37197;&#22120;&#35843;&#20248;&#20027;&#35201;&#24212;&#29992;&#20110;&#20165;&#32534;&#30721;&#22120;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;Wav2Seq&#36825;&#20010;&#33258;&#30417;&#30563;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#19978;&#36827;&#34892;&#25552;&#31034;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#22312;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#30740;&#31350;&#25104;&#26524;&#12290;&#23427;&#22312;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#19978;&#23454;&#29616;&#20102;53&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#65292;&#22312;&#27133;&#22635;&#20805;&#30340;F1&#20998;&#25968;&#19978;&#23454;&#29616;&#20102;27&#65285;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#31034;&#26041;&#27861;&#19982;FT&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;Wav2Seq&#19978;&#36890;&#36807;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;ASR&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#24403;&#28041;&#21450;&#26377;&#38480;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#26102;&#65292;&#25552;&#31034;&#21644;&#36866;&#37197;&#22120;&#35843;&#20248;&#22987;&#32456;&#20248;&#20110;&#20256;&#32479;&#30340;FT&#26041;&#27861;&#22312;7&#31181;&#35821;&#35328;&#20013;&#30340;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#65292;&#25552;&#31034;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#36866;&#37197;&#22120;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting and adapter tuning have emerged as efficient alternatives to fine-tuning (FT) methods. However, existing studies on speech prompting focused on classification tasks and failed on more complex sequence generation tasks. Besides, adapter tuning is primarily applied with a focus on encoder-only self-supervised models. Our experiments show that prompting on Wav2Seq, a self-supervised encoder-decoder model, surpasses previous works in sequence generation tasks. It achieves a remarkable 53% relative improvement in word error rate for ASR and a 27% in F1 score for slot filling. Additionally, prompting competes with the FT method in the low-resource scenario. Moreover, we show the transferability of prompting and adapter tuning on Wav2Seq in cross-lingual ASR. When limited trainable parameters are involved, prompting and adapter tuning consistently outperform conventional FT across 7 languages. Notably, in the low-resource scenario, prompting consistently outperforms adapter tuning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01320</link><description>&lt;p&gt;
Avalon&#30340;&#24605;&#32771;&#28216;&#25103;&#65306;&#36890;&#36807;&#36882;&#24402;&#24605;&#32771;&#23545;&#25239;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation. (arXiv:2310.01320v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#22312;LLM&#20316;&#20026;&#26234;&#33021;&#20307;&#39046;&#22495;&#30340;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#19968;&#31181;&#26222;&#36941;&#30340;&#20551;&#35774;&#26159;LLM&#22788;&#29702;&#30340;&#20449;&#24687;&#22987;&#32456;&#26159;&#35802;&#23454;&#30340;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#31038;&#20250;&#21644;AI&#29983;&#25104;&#20869;&#23481;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#27450;&#39575;&#25110;&#35823;&#23548;&#24615;&#20449;&#24687;&#12290;&#36825;&#20010;&#30095;&#24573;&#20351;&#24471;LLM&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#21487;&#33021;&#23548;&#33268;&#19981;&#21033;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;Avalon&#28216;&#25103;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#25506;&#32034;LLM&#22312;&#27450;&#39575;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#12290;Avalon&#20805;&#28385;&#20102;&#38169;&#35823;&#20449;&#24687;&#65292;&#24182;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#65292;&#34920;&#29616;&#20026;&#8220;&#24605;&#32771;&#30340;&#28216;&#25103;&#8221;&#12290;&#21463;&#21040;&#20154;&#31867;&#22312;Avalon&#28216;&#25103;&#20013;&#36882;&#24402;&#24605;&#32771;&#21644;&#36879;&#35270;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36882;&#24402;&#24605;&#32771;&#65288;ReCon&#65289;&#65292;&#20197;&#22686;&#24378;LLM&#35782;&#21035;&#21644;&#23545;&#25239;&#27450;&#39575;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;ReCon&#32467;&#21512;&#20102;&#20844;&#24335;&#21270;&#24605;&#32771;&#21644;&#23436;&#21892;&#24605;&#32771;&#30340;&#36807;&#31243;&#65307;&#20844;&#24335;&#21270;&#24605;&#32771;&#20135;&#29983;&#21021;&#22987;&#24605;&#32771;&#65292;&#23436;&#21892;&#24605;&#32771;&#23545;&#21021;&#22987;&#24605;&#32771;&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a "Game-of-Thoughts". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial tho
&lt;/p&gt;</description></item><item><title>NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13230</link><description>&lt;p&gt;
NJUNLP&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#30340;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
NJUNLP's Participation for the WMT2023 Quality Estimation Shared Task. (arXiv:2309.13230v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13230
&lt;/p&gt;
&lt;p&gt;
NJUNLP&#22242;&#38431;&#23545;WMT2023&#36136;&#37327;&#35780;&#20272;&#20849;&#20139;&#20219;&#21153;&#36827;&#34892;&#20102;&#25237;&#31295;&#65292;&#36890;&#36807;&#20351;&#29992;&#20266;&#25968;&#25454;&#26041;&#27861;&#21644;&#26680;&#24515;&#36229;&#21442;&#25968;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#20182;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#36136;&#37327;&#39044;&#27979;&#21644;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#19978;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;NJUNLP&#22242;&#38431;&#22312;WMT 2023&#36136;&#37327;&#20272;&#35745;&#65288;QE&#65289;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#25237;&#31295;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#25552;&#20132;&#20102;&#23545;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#25152;&#26377;&#20004;&#20010;&#23376;&#20219;&#21153;&#30340;&#39044;&#27979;&#65306;&#65288;i&#65289;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#36136;&#37327;&#39044;&#27979;&#65307;&#65288;ii&#65289;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#12290;&#20170;&#24180;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#22522;&#20110;NJUQE&#26694;&#26550;&#65288;https://github.com/NJUNLP/njuqe&#65289;&#30340;&#20266;&#25968;&#25454;&#26041;&#27861;&#36827;&#34892;QE&#12290;&#25105;&#20204;&#20351;&#29992;WMT&#32763;&#35793;&#20219;&#21153;&#30340;&#24182;&#34892;&#25968;&#25454;&#29983;&#25104;&#20266;MQM&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20266;QE&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;XLMR&#22823;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#30495;&#23454;QE&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#20004;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#20849;&#21516;&#23398;&#20064;&#21477;&#23376;&#32423;&#20998;&#25968;&#21644;&#21333;&#35789;&#32423;&#26631;&#31614;&#12290;&#22312;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36827;&#34892;&#23454;&#39564;&#26469;&#23547;&#25214;&#25913;&#21892;&#24615;&#33021;&#30340;&#20851;&#38190;&#36229;&#21442;&#25968;&#12290;&#22312;&#25216;&#26415;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#35789;&#32423;&#36755;&#20986;&#36716;&#25442;&#20026;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#32467;&#26524;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33521;&#24503;&#35821;&#35328;&#23545;&#30340;&#21333;&#35789;&#32423;&#21035;&#21644;&#32454;&#31890;&#24230;&#38169;&#35823;&#36328;&#24230;&#26816;&#27979;&#23376;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the submissions of the NJUNLP team to the WMT 2023 Quality Estimation (QE) shared task. Our team submitted predictions for the English-German language pair on all two sub-tasks: (i) sentence- and word-level quality prediction; and (ii) fine-grained error span detection. This year, we further explore pseudo data methods for QE based on NJUQE framework (https://github.com/NJUNLP/njuqe). We generate pseudo MQM data using parallel data from the WMT translation task. We pre-train the XLMR large model on pseudo QE data, then fine-tune it on real QE data. At both stages, we jointly learn sentence-level scores and word-level tags. Empirically, we conduct experiments to find the key hyper-parameters that improve the performance. Technically, we propose a simple method that covert the word-level outputs to fine-grained error span results. Overall, our models achieved the best results in English-German for both word-level and fine-grained error span detection sub-tasks by a considera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12871</link><description>&lt;p&gt;
&#35282;&#24230;&#20248;&#21270;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
AnglE-Optimized Text Embeddings. (arXiv:2309.12871v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AnglE&#30340;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#26469;&#32531;&#35299;&#25991;&#26412;&#23884;&#20837;&#20013;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#36896;&#25104;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;STS&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#65292;&#24182;&#22312;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#20013;&#23637;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#23884;&#20837;&#23545;&#20110;&#25552;&#21319;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#20123;&#20219;&#21153;&#21448;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20010;&#26222;&#36941;&#25361;&#25112;&#26159;&#28176;&#21464;&#28040;&#22833;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#23427;&#20204;&#22312;&#20248;&#21270;&#30446;&#26631;&#20013;&#20381;&#36182;&#20313;&#24358;&#20989;&#25968;&#65292;&#32780;&#20313;&#24358;&#20989;&#25968;&#20855;&#26377;&#39281;&#21644;&#21306;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;AnglE&#30340;&#26032;&#22411;&#35282;&#24230;&#20248;&#21270;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#12290;AnglE&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22797;&#26434;&#31354;&#38388;&#20013;&#24341;&#20837;&#35282;&#24230;&#20248;&#21270;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#20313;&#24358;&#20989;&#25968;&#39281;&#21644;&#21306;&#22495;&#20135;&#29983;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#38459;&#30861;&#26799;&#24230;&#24182;&#38459;&#30861;&#20248;&#21270;&#36807;&#31243;&#12290;&#20026;&#20102;&#24314;&#31435;&#20840;&#38754;&#30340;STS&#35780;&#20272;&#65292;&#25105;&#20204;&#22312;&#29616;&#26377;&#30340;&#30701;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#21644;&#20174;GitHub Issues&#20013;&#26032;&#25910;&#38598;&#30340;&#38271;&#25991;&#26412;STS&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#29305;&#23450;&#39046;&#22495;STS&#22330;&#26223;&#65292;&#24182;&#25506;&#35752;&#20102;AnglE&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08030</link><description>&lt;p&gt;
AV2Wav&#65306;&#22522;&#20110;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#30340;&#25193;&#25955;&#37325;&#21512;&#25104;&#25216;&#26415;&#29992;&#20110;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
AV2Wav: Diffusion-Based Re-synthesis from Continuous Self-supervised Features for Audio-Visual Speech Enhancement. (arXiv:2309.08030v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AV2Wav&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21033;&#29992;&#36830;&#32493;&#33258;&#30417;&#30563;&#29305;&#24449;&#21644;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#65292;&#20811;&#26381;&#20102;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#22768;&#30721;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#20248;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#24178;&#20928;&#21644;&#22122;&#22768;&#35821;&#38899;&#23545;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#20013;&#65292;&#24178;&#20928;&#30340;&#25968;&#25454;&#19981;&#22815;&#22810;&#65307;&#22823;&#22810;&#25968;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#37117;&#26159;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#25910;&#38598;&#30340;&#65292;&#21253;&#21547;&#32972;&#26223;&#22122;&#22768;&#21644;&#28151;&#21709;&#65292;&#36825;&#38459;&#30861;&#20102;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AV2Wav&#65292;&#19968;&#31181;&#22522;&#20110;&#37325;&#21512;&#25104;&#30340;&#38899;&#39057;-&#35270;&#35273;&#35821;&#38899;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#29616;&#23454;&#35757;&#32451;&#25968;&#25454;&#30340;&#25361;&#25112;&#19979;&#29983;&#25104;&#24178;&#20928;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#36136;&#37327;&#20272;&#35745;&#22120;&#20174;&#38899;&#39057;-&#35270;&#35273;&#35821;&#26009;&#24211;&#20013;&#33719;&#21462;&#20960;&#20046;&#24178;&#20928;&#30340;&#35821;&#38899;&#23376;&#38598;&#65292;&#24182;&#22312;&#27492;&#23376;&#38598;&#19978;&#35757;&#32451;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#26469;&#33258;AV-HuBERT&#30340;&#36830;&#32493;&#35821;&#38899;&#34920;&#31034;&#29983;&#25104;&#22768;&#27874;&#24418;&#65292;&#20855;&#26377;&#22122;&#22768;&#40065;&#26834;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#32780;&#19981;&#26159;&#31163;&#25955;&#34920;&#31034;&#26469;&#20445;&#30041;&#38901;&#24459;&#21644;&#35828;&#35805;&#32773;&#20449;&#24687;&#12290;&#20165;&#20165;&#36890;&#36807;&#22768;&#30721;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23601;&#27604;&#22522;&#20110;&#25513;&#34109;&#30340;&#22522;&#32447;&#26356;&#22909;&#22320;&#25191;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;fine-tune&#27169;&#22411;&#65292;&#20197;&#36716;&#21270;&#20026;&#22312;&#22810;&#20219;&#21153;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#32852;&#21512;&#22810;&#24103;&#22768;&#23398;&#21040;&#35821;&#38899;&#36716;&#21270;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech enhancement systems are typically trained using pairs of clean and noisy speech. In audio-visual speech enhancement (AVSE), there is not as much ground-truth clean data available; most audio-visual datasets are collected in real-world environments with background noise and reverberation, hampering the development of AVSE. In this work, we introduce AV2Wav, a resynthesis-based audio-visual speech enhancement approach that can generate clean speech despite the challenges of real-world training data. We obtain a subset of nearly clean speech from an audio-visual corpus using a neural quality estimator, and then train a diffusion model on this subset to generate waveforms conditioned on continuous speech representations from AV-HuBERT with noise-robust training. We use continuous rather than discrete representations to retain prosody and speaker information. With this vocoding task alone, the model can perform speech enhancement better than a masking-based baseline. We further fine-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.07430</link><description>&lt;p&gt;
&#20020;&#24202;&#25991;&#26412;&#25688;&#35201;: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24212;&#29992;&#20248;&#20110;&#20154;&#31867;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Clinical Text Summarization: Adapting Large Language Models Can Outperform Human Experts. (arXiv:2309.07430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#20843;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#23454;&#39564;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;&#27169;&#22411;&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#24037;&#20316;&#20013;&#65292;&#27983;&#35272;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#24182;&#24635;&#32467;&#20851;&#38190;&#20449;&#24687;&#23545;&#20020;&#24202;&#21307;&#29983;&#30340;&#26102;&#38388;&#20998;&#37197;&#36896;&#25104;&#20102;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21508;&#31181;&#20020;&#24202;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20005;&#26684;&#30340;&#26816;&#39564;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#20010;LLMs&#36827;&#34892;&#20102;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#21644;&#22235;&#20010;&#19981;&#21516;&#30340;&#25688;&#35201;&#20219;&#21153;&#65306;&#25918;&#23556;&#23398;&#25253;&#21578;&#12289;&#24739;&#32773;&#38382;&#39064;&#12289;&#30149;&#21382;&#35760;&#24405;&#21644;&#21307;&#24739;&#23545;&#35805;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23450;&#37327;&#35780;&#20272;&#65292;&#21457;&#29616;&#27169;&#22411;&#21644;&#36866;&#24212;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;LLMs&#30340;&#26368;&#26032;&#36827;&#23637;&#21487;&#33021;&#19981;&#20250;&#24102;&#26469;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;&#20845;&#21517;&#21307;&#29983;&#36827;&#34892;&#30340;&#20020;&#24202;&#38405;&#35835;&#32773;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#26368;&#20339;&#36866;&#24212;&#30340;LLM&#30340;&#25688;&#35201;&#22312;&#23436;&#25972;&#24615;&#21644;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20154;&#31867;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#36827;&#19968;&#27493;&#23450;&#24615;&#20998;&#26512;&#25581;&#31034;&#20102;LLMs&#21644;&#20154;&#31867;&#22312;&#38754;&#23545;&#30340;&#20849;&#21516;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sifting through vast textual data and summarizing key information imposes a substantial burden on how clinicians allocate their time. Although large language models (LLMs) have shown immense promise in natural language processing (NLP) tasks, their efficacy across diverse clinical summarization tasks has not yet been rigorously examined. In this work, we employ domain adaptation methods on eight LLMs, spanning six datasets and four distinct summarization tasks: radiology reports, patient questions, progress notes, and doctor-patient dialogue. Our thorough quantitative assessment reveals trade-offs between models and adaptation methods in addition to instances where recent advances in LLMs may not lead to improved results. Further, in a clinical reader study with six physicians, we depict that summaries from the best adapted LLM are preferable to human summaries in terms of completeness and correctness. Our ensuing qualitative analysis delineates mutual challenges faced by both LLMs and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.06132</link><description>&lt;p&gt;
&#22312;&#25991;&#26412;&#20013;&#27979;&#37327;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#65306;&#20174;&#31526;&#21495;&#21040;&#31070;&#32463;&#32593;&#32476;&#30340;VAGO
&lt;/p&gt;
&lt;p&gt;
Measuring vagueness and subjectivity in texts: from symbolic to neural VAGO. (arXiv:2309.06132v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#20197;&#21450;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26041;&#27861;&#22312;&#22266;&#23450;&#35821;&#26009;&#24211;&#21644;&#22810;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#33258;&#21160;&#27979;&#37327;&#25991;&#26412;&#20013;&#30340;&#27169;&#31946;&#24615;&#21644;&#20027;&#35266;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19987;&#23478;&#31995;&#32479;VAGO&#65292;&#24182;&#22312;&#19968;&#23567;&#32452;&#20107;&#23454;&#19982;&#35266;&#28857;&#21477;&#23376;&#30340;&#22522;&#20934;&#19978;&#23545;&#20854;&#36827;&#34892;&#20102;&#35828;&#26126;&#65292;&#24182;&#22312;&#26356;&#22823;&#30340;&#27861;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;FreSaDa&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#30830;&#35748;&#35773;&#21050;&#24615;&#25991;&#26412;&#20013;&#20027;&#35266;&#26631;&#35760;&#30340;&#26356;&#39640;&#27969;&#34892;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;BERT-like&#26550;&#26500;&#30340;VAGO&#31070;&#32463;&#20811;&#38534;&#65292;&#35813;&#26550;&#26500;&#22522;&#20110;&#22312;FreSaDa&#19978;&#33719;&#24471;&#30340;&#31526;&#21495;VAGO&#20998;&#25968;&#36827;&#34892;&#35757;&#32451;&#12290;&#20351;&#29992;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#65288;LIME&#65289;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#31070;&#32463;&#29256;&#26412;&#22312;&#20016;&#23500;&#31526;&#21495;&#29256;&#26412;&#30340;&#35789;&#20856;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#29256;&#26412;&#26041;&#38754;&#30340;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a hybrid approach to the automated measurement of vagueness and subjectivity in texts. We first introduce the expert system VAGO, we illustrate it on a small benchmark of fact vs. opinion sentences, and then test it on the larger French press corpus FreSaDa to confirm the higher prevalence of subjective markers in satirical vs. regular texts. We then build a neural clone of VAGO, based on a BERT-like architecture, trained on the symbolic VAGO scores obtained on FreSaDa. Using explainability tools (LIME), we show the interest of this neural version for the enrichment of the lexicons of the symbolic version, and for the production of versions in other languages.
&lt;/p&gt;</description></item><item><title>nanoT5&#26159;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;T5&#27169;&#22411;&#30340;PyTorch&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#35757;&#32451;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#36825;&#20010;&#24320;&#28304;&#26694;&#26550;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#24314;&#27169;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#26131;&#29992;&#30340;T5&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2309.02373</link><description>&lt;p&gt;
nanoT5:&#19968;&#31181;&#29992;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;T5&#39118;&#26684;&#27169;&#22411;&#30340;PyTorch&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
nanoT5: A PyTorch Framework for Pre-training and Fine-tuning T5-style Models with Limited Resources. (arXiv:2309.02373v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02373
&lt;/p&gt;
&lt;p&gt;
nanoT5&#26159;&#19968;&#20010;&#29992;&#20110;&#39640;&#25928;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;T5&#27169;&#22411;&#30340;PyTorch&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#20010;GPU&#19978;&#22312;&#30701;&#26102;&#38388;&#20869;&#36827;&#34892;&#39044;&#35757;&#32451;&#32780;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;&#36825;&#20010;&#24320;&#28304;&#26694;&#26550;&#20026;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#24314;&#27169;&#30740;&#31350;&#25552;&#20379;&#20102;&#26356;&#26131;&#29992;&#30340;T5&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;T5&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26684;&#23616;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#27714;&#38480;&#21046;&#20102;&#22823;&#37096;&#20998;&#30740;&#31350;&#31038;&#21306;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;nanoT5&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#29305;&#21035;&#20248;&#21270;&#30340;PyTorch&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#23545;T5&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#20511;&#37492;&#20248;&#21270;&#22120;&#30340;&#24046;&#24322;&#21644;&#20248;&#21270;&#25928;&#29575;&#65292;nanoT5&#20351;&#24471;&#19968;&#20010;T5-Base&#27169;&#22411;&#33021;&#22815;&#22312;&#21333;&#20010;GPU&#19978;&#21482;&#38656;16&#23567;&#26102;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#32780;&#19988;&#19981;&#20250;&#25439;&#22833;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#36825;&#20010;&#24320;&#28304;&#26694;&#26550;&#65292;&#25105;&#20204;&#24076;&#26395;&#25193;&#22823;&#35821;&#35328;&#24314;&#27169;&#30740;&#31350;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#24182;&#28385;&#36275;&#31038;&#21306;&#23545;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#30340;T5&#65288;&#32534;&#30721;-&#35299;&#30721;&#65289;&#23454;&#29616;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#37197;&#32622;&#12289;&#20195;&#30721;&#24211;&#12289;&#39044;&#35757;&#32451;&#27934;&#23519;&#21147;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25552;&#20379;&#32473;&#20844;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art language models like T5 have revolutionized the NLP landscape, but their computational demands hinder a large portion of the research community. To address this challenge, we present nanoT5, a specially-optimized PyTorch framework for efficient pre-training and fine-tuning of T5 models. Drawing on insights from optimizer differences and prioritizing efficiency, nanoT5 allows a T5-Base model to be pre-trained on a single GPU in just 16 hours, without any loss in performance. With the introduction of this open-source framework, we hope to widen the accessibility to language modelling research and cater to the community's demand for more user-friendly T5 (Encoder-Decoder) implementations. We make our contributions, including configurations, codebase, pre-training insights, and pre-trained models, available to the public.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;</title><link>http://arxiv.org/abs/2308.07308</link><description>&lt;p&gt;
LLM&#33258;&#21355;&#65306;&#36890;&#36807;&#33258;&#26816;&#65292;LLMs&#24847;&#35782;&#21040;&#23427;&#20204;&#34987;&#24858;&#24324;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked. (arXiv:2308.07308v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#26816;&#26469;&#38450;&#24481;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#21363;&#35753;&#27169;&#22411;&#33258;&#34892;&#36807;&#28388;&#22238;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#26410;&#23545;&#40784;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#65292;&#20173;&#28982;&#21487;&#20197;&#38450;&#27490;&#27169;&#22411;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#33021;&#22815;&#23545;&#20154;&#31867;&#25552;&#31034;&#20570;&#20986;&#39640;&#36136;&#37327;&#25991;&#26412;&#22238;&#24212;&#32780;&#21464;&#24471;&#38750;&#24120;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#24212;&#29992;&#25143;&#25552;&#31034;&#26102;&#21487;&#33021;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#32473;&#29992;&#25143;&#25552;&#20379;&#29359;&#32618;&#25351;&#23548;&#65289;&#12290;&#25991;&#29486;&#20013;&#24050;&#32463;&#30528;&#37325;&#30740;&#31350;&#22914;&#20309;&#36890;&#36807;&#26041;&#27861;&#65288;&#20363;&#22914;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23558;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65289;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#20063;&#23481;&#26131;&#21463;&#21040;&#32469;&#36807;&#29983;&#25104;&#26377;&#23475;&#25991;&#26412;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#38450;&#24481;&#36825;&#20123;&#25915;&#20987;&#65292;&#21363;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#33258;&#24049;&#30340;&#22238;&#24212;&#36827;&#34892;&#36807;&#28388;&#12290;&#25105;&#20204;&#30446;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#27809;&#26377;&#34987;&#24494;&#35843;&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#65292;&#20063;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#39564;&#35777;&#20869;&#23481;&#26469;&#38450;&#27490;&#20854;&#21521;&#29992;&#25143;&#21576;&#29616;&#26377;&#23475;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have skyrocketed in popularity in recent years due to their ability to generate high-quality text in response to human prompting. However, these models have been shown to have the potential to generate harmful content in response to user prompting (e.g., giving users instructions on how to commit crimes). There has been a focus in the literature on mitigating these risks, through methods like aligning models with human values through reinforcement learning. However, it has been shown that even aligned language models are susceptible to adversarial attacks that bypass their restrictions on generating harmful text. We propose a simple approach to defending against these attacks by having a large language model filter its own responses. Our current results show that even if a model is not fine-tuned to be aligned with human values, it is possible to stop it from presenting harmful content to users by validating the content using a language model.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;Bard&#21644;ChatGPT&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#26041;&#35328;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#21830;&#19994;&#31995;&#32479;&#65292;&#20294;&#22312;&#21476;&#20856;&#38463;&#25289;&#20271;&#35821;&#21644;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#33853;&#21518;&#20110;&#35895;&#27468;&#32763;&#35793;&#31561;&#21830;&#19994;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2308.03051</link><description>&lt;p&gt;
&#12298;TARJAMAT: Bard&#21644;ChatGPT&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#26426;&#22120;&#32763;&#35793;&#19978;&#30340;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
TARJAMAT: Evaluation of Bard and ChatGPT on Machine Translation of Ten Arabic Varieties. (arXiv:2308.03051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03051
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;Bard&#21644;ChatGPT&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#26041;&#35328;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#21830;&#19994;&#31995;&#32479;&#65292;&#20294;&#22312;&#21476;&#20856;&#38463;&#25289;&#20271;&#35821;&#21644;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#26041;&#38754;&#33853;&#21518;&#20110;&#35895;&#27468;&#32763;&#35793;&#31561;&#21830;&#19994;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;ChatGPT&#21644;Bard&#36825;&#26679;&#30340;&#32463;&#36807;&#25351;&#23548;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34987;&#35748;&#20026;&#22312;&#22810;&#35821;&#35328;&#19978;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#21253;&#23481;&#24615;&#36824;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#30340;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#23545;Bard&#21644;ChatGPT&#65288;&#21253;&#25324;GPT-3.5&#21644;GPT-4&#65289;&#22312;&#21313;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#30340;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#21476;&#20856;&#38463;&#25289;&#20271;&#35821;&#65288;CA&#65289;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#65288;MSA&#65289;&#21644;&#20960;&#31181;&#22269;&#23478;&#26041;&#35328;&#21464;&#20307;&#31561;&#22810;&#31181;&#38463;&#25289;&#20271;&#35821;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;LLM&#22312;&#23384;&#22312;&#23569;&#37327;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#26041;&#35328;&#19978;&#21487;&#33021;&#20250;&#36935;&#21040;&#25361;&#25112;&#65292;&#20294;&#24179;&#22343;&#32780;&#35328;&#65292;&#23427;&#20204;&#27604;&#29616;&#26377;&#30340;&#21830;&#19994;&#31995;&#32479;&#26356;&#25797;&#38271;&#32763;&#35793;&#26041;&#35328;&#12290;&#28982;&#32780;&#65292;&#22312;CA&#21644;MSA&#26041;&#38754;&#65292;&#32463;&#36807;&#25351;&#23548;&#35843;&#25972;&#30340;LLM&#19982;&#35895;&#27468;&#32763;&#35793;&#31561;&#21830;&#19994;&#31995;&#32479;&#30456;&#27604;&#20173;&#28982;&#26377;&#25152;&#19981;&#36275;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#23637;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#65292;&#20197;&#23457;&#26597;&#30456;&#23545;&#36739;&#26032;&#30340;&#27169;&#22411;Bard&#22312;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the purported multilingual proficiency of instruction-finetuned large language models (LLMs) such as ChatGPT and Bard, the linguistic inclusivity of these models remains insufficiently explored. Considering this constraint, we present a thorough assessment of Bard and ChatGPT (encompassing both GPT-3.5 and GPT-4) regarding their machine translation proficiencies across ten varieties of Arabic. Our evaluation covers diverse Arabic varieties such as Classical Arabic (CA), Modern Standard Arabic (MSA), and several country-level dialectal variants. Our analysis indicates that LLMs may encounter challenges with dialects for which minimal public datasets exist, but on average are better translators of dialects than existing commercial systems. On CA and MSA, instruction-tuned LLMs, however, trail behind commercial systems such as Google Translate. Finally, we undertake a human-centric study to scrutinize the efficacy of the relatively recent model, Bard, in following human instructio
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#20013;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#24403;&#25945;&#24072;&#27169;&#22411;&#22312;&#36275;&#22815;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#33976;&#39311;&#21487;&#20197;&#20445;&#25345;&#29978;&#33267;&#36229;&#36807;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02019</link><description>&lt;p&gt;
Baby Llama&#65306;&#20174;&#19968;&#32452;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#20013;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#65292;&#26080;&#24615;&#33021;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Baby Llama: knowledge distillation from an ensemble of teachers trained on a small dataset with no performance penalty. (arXiv:2308.02019v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#25945;&#24072;&#20013;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#24403;&#25945;&#24072;&#27169;&#22411;&#22312;&#36275;&#22815;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#33976;&#39311;&#21487;&#20197;&#20445;&#25345;&#29978;&#33267;&#36229;&#36807;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#23545;BabyLM&#25361;&#25112;[arXiv:2301.11796]&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#30446;&#26631;&#26159;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;&#20197;&#21457;&#23637;&#24615;&#20026;&#22522;&#30784;&#30340;10M&#35789;&#35821;&#30340;BabyLM&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;&#30001;GPT-2&#21644;&#23567;&#22411;LLaMA&#27169;&#22411;&#32452;&#25104;&#30340;&#38598;&#21512;&#65292;&#28982;&#21518;&#23558;&#20854;&#33976;&#39311;&#20026;&#19968;&#20010;&#23567;&#22411;&#30340;58M&#21442;&#25968;LLaMA&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;&#20004;&#20010;&#25945;&#24072;&#27169;&#22411;&#20197;&#21450;&#19968;&#20010;&#27809;&#26377;&#36827;&#34892;&#33976;&#39311;&#35757;&#32451;&#30340;&#31867;&#20284;&#27169;&#22411;&#12290;&#36825;&#34920;&#26126;&#65292;&#24403;&#25945;&#24072;&#27169;&#22411;&#22312;&#36275;&#22815;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#65292;&#33976;&#39311;&#19981;&#20165;&#21487;&#20197;&#20445;&#25345;&#25945;&#24072;&#27169;&#22411;&#30340;&#20840;&#37096;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#36229;&#36807;&#23427;&#65292;&#24182;&#23548;&#33268;&#27604;&#30452;&#25509;&#35757;&#32451;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present our proposed solution to the BabyLM challenge [arXiv:2301.11796], whose goal was to improve the sample efficiency of language models. We trained an ensemble consisting of a GPT-2 and small LLaMA models on the developmentally-plausible, 10M-word BabyLM dataset, then distilled it into a small, 58M-parameter LLaMA model, which exceeds in performance both of its teachers as well as a similar model trained without distillation. This suggests that distillation can not only retain the full performance of the teacher model when the latter is trained on a sufficiently small dataset; it can exceed it, and lead to significantly better performance than direct training.
&lt;/p&gt;</description></item><item><title>WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.13854</link><description>&lt;p&gt;
WebArena: &#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
WebArena: A Realistic Web Environment for Building Autonomous Agents. (arXiv:2307.13854v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13854
&lt;/p&gt;
&lt;p&gt;
WebArena&#26159;&#19968;&#20010;&#29992;&#20110;&#26500;&#24314;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#30495;&#23454;&#32593;&#32476;&#29615;&#22659;&#65292;&#23427;&#21253;&#21547;&#20102;&#23436;&#20840;&#21151;&#33021;&#30340;&#32593;&#31449;&#65292;&#24182;&#19988;&#36890;&#36807;&#24341;&#20837;&#24037;&#20855;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#26469;&#40723;&#21169;&#26234;&#33021;&#20307;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;WebArena&#36824;&#21457;&#24067;&#20102;&#19968;&#32452;&#29992;&#20110;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#36827;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#28508;&#21147;&#36880;&#28176;&#26174;&#29616;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26234;&#33021;&#20307;&#20027;&#35201;&#26159;&#22312;&#31616;&#21270;&#30340;&#21512;&#25104;&#29615;&#22659;&#20013;&#21019;&#24314;&#21644;&#27979;&#35797;&#30340;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#39640;&#24230;&#36924;&#30495;&#19988;&#21487;&#22797;&#29616;&#30340;&#26234;&#33021;&#20307;&#25351;&#20196;&#21644;&#25511;&#21046;&#29615;&#22659;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#32593;&#31449;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26234;&#33021;&#20307;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;&#22235;&#20010;&#24120;&#35265;&#39046;&#22495;&#30340;&#23436;&#20840;&#21151;&#33021;&#32593;&#31449;&#30340;&#29615;&#22659;&#65292;&#20998;&#21035;&#26159;&#30005;&#23376;&#21830;&#21153;&#12289;&#31038;&#20132;&#35770;&#22363;&#35752;&#35770;&#12289;&#21327;&#21516;&#36719;&#20214;&#24320;&#21457;&#21644;&#20869;&#23481;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20351;&#29992;&#24037;&#20855;&#65288;&#22914;&#22320;&#22270;&#65289;&#21644;&#22806;&#37096;&#30693;&#35782;&#24211;&#65288;&#22914;&#29992;&#25143;&#25163;&#20876;&#65289;&#26469;&#40723;&#21169;&#20687;&#20154;&#31867;&#19968;&#26679;&#35299;&#20915;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#32452;&#37325;&#28857;&#35780;&#20272;&#20219;&#21153;&#23436;&#25104;&#21151;&#33021;&#27491;&#30830;&#24615;&#30340;&#22522;&#20934;&#20219;&#21153;&#12290;&#25105;&#20204;&#22522;&#20934;&#20219;&#21153;&#20855;&#26377;&#22810;&#26679;&#24615;&#21644;&#38271;&#36828;&#30340;&#35270;&#37326;&#65292;&#24182;&#19988;&#34987;&#35774;&#35745;&#20026;&#40723;&#21169;&#26234;&#33021;&#20307;&#36827;&#34892;&#26356;&#28145;&#23618;&#27425;&#30340;&#20219;&#21153;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
With generative AI advances, the exciting potential for autonomous agents to manage daily tasks via natural language commands has emerged. However, cur rent agents are primarily created and tested in simplified synthetic environments, substantially limiting real-world scenario representation. In this paper, we build an environment for agent command and control that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on websites, and we create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and are desi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.03838</link><description>&lt;p&gt;
RADAR: &#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
RADAR: Robust AI-Text Detection via Adversarial Learning. (arXiv:2307.03838v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#23454;&#29616;&#20102;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#20889;&#19981;&#20855;&#22791;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20197;&#21450;ChatGPT&#31867;&#24212;&#29992;&#30340;&#26222;&#21450;&#24050;&#32463;&#27169;&#31946;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;&#30340;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23545;&#25105;&#20204;&#30340;&#25216;&#26415;&#21644;&#31038;&#20250;&#39044;&#26399;&#30340;&#38761;&#21629;&#24615;&#21464;&#21270;&#22806;&#65292;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#65288;AI&#25991;&#26412;&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22256;&#38590;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#28389;&#29992;&#21644;&#20844;&#24179;&#24615;&#25361;&#25112;&#65292;&#20363;&#22914;&#34394;&#20551;&#20869;&#23481;&#29983;&#25104;&#65292;&#25220;&#34989;&#20197;&#21450;&#23545;&#26080;&#36764;&#20316;&#32773;&#30340;&#38169;&#35823;&#25351;&#25511;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#24403;&#21069;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;&#22522;&#20110;LLM&#30340;&#25913;&#20889;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;RADAR&#30340;&#26032;&#26694;&#26550;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23545;&#25239;&#24615;&#23398;&#20064;&#20849;&#21516;&#35757;&#32451;&#20102;&#19968;&#20010;&#40065;&#26834;&#30340;AI&#25991;&#26412;&#26816;&#27979;&#22120;&#12290;RADAR&#22522;&#20110;&#19968;&#20010;&#25913;&#20889;&#22120;&#21644;&#19968;&#20010;&#26816;&#27979;&#22120;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#25913;&#20889;&#22120;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#36924;&#30495;&#30340;&#20869;&#23481;&#20197;&#35268;&#36991;AI&#25991;&#26412;&#26816;&#27979;&#12290;RADAR&#20351;&#29992;&#26469;&#33258;&#26816;&#27979;&#22120;&#30340;&#21453;&#39304;&#26469;&#26356;&#26032;&#25913;&#20889;&#22120;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) and the intensifying popularity of ChatGPT-like applications have blurred the boundary of high-quality text generation between humans and machines. However, in addition to the anticipated revolutionary changes to our technology and society, the difficulty of distinguishing LLM-generated texts (AI-text) from human-generated texts poses new challenges of misuse and fairness, such as fake content generation, plagiarism, and false accusation of innocent writers. While existing works show that current AI-text detectors are not robust to LLM-based paraphrasing, this paper aims to bridge this gap by proposing a new framework called RADAR, which jointly trains a Robust AI-text Detector via Adversarial leaRning. RADAR is based on adversarial training of a paraphraser and a detector. The paraphraser's goal is to generate realistic contents to evade AI-text detection. RADAR uses the feedback from the detector to update the paraphraser, and vice vers
&lt;/p&gt;</description></item><item><title>&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.16339</link><description>&lt;p&gt;
&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#65292;&#19981;&#35201;&#23436;&#20840;&#20449;&#20219;GPT
&lt;/p&gt;
&lt;p&gt;
Don't Trust GPT When Your Question Is Not In English. (arXiv:2305.16339v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16339
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#65292;GPT-3&#34920;&#29616;&#36739;&#24046;&#65292;&#29305;&#21035;&#26159;&#24403;&#38382;&#39064;&#19981;&#26159;&#29992;&#33521;&#35821;&#25552;&#20986;&#26102;&#12290;&#36825;&#19982;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#22312;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;LLMs&#20027;&#35201;&#20351;&#29992;&#33521;&#35821;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#22810;&#39033;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#20854;&#20182;&#35821;&#35328;&#20013;&#30340;&#30456;&#23545;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;LLMs&#22914;&#20309;&#33719;&#24471;&#23427;&#20204;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#20197;&#21450;&#34920;&#29616;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#22522;&#26412;&#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#23545;LLMs&#30340;&#30740;&#31350;&#38750;&#24120;&#20851;&#38190;&#65292;&#22240;&#20026;&#29992;&#25143;&#21644;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#26469;&#33258;&#22810;&#31181;&#35821;&#35328;&#32972;&#26223;&#65292;&#21487;&#33021;&#24433;&#21709;&#20182;&#20204;&#23545;LLMs&#32467;&#26524;&#30340;&#21033;&#29992;&#21644;&#35299;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#20197;&#23450;&#24615;&#35780;&#20272;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;LLMs&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#36328;&#35821;&#35328;&#27867;&#21270;&#29616;&#35937;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21363;&#19981;&#20805;&#36275;&#30340;&#22810;&#35821;&#35328;&#35757;&#32451;&#25968;&#25454;&#23548;&#33268;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#30340;&#65292;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#35821;&#35328;&#36827;&#34892;&#20102;GPT-3&#30340;&#23454;&#39564;&#65292;&#36825;&#20123;&#35821;&#35328;&#28085;&#30422;&#20102;&#20174;&#21360;&#27431;&#35821;&#31995;&#21040;&#38750;&#21360;&#27431;&#35821;&#31995;&#30340;&#21508;&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21644;&#39564;&#35777;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#35813;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20294;&#22914;&#26524;&#36755;&#20837;&#38382;&#39064;&#19981;&#26159;&#33521;&#35821;&#65292;GPT-3&#22312;&#20854;&#20182;&#35821;&#35328;&#19979;&#30340;&#34920;&#29616;&#26174;&#33879;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#19982;&#35757;&#32451;&#35821;&#35328;&#21644;&#36755;&#20837;&#38382;&#39064;&#30340;&#35821;&#35328;&#24046;&#24322;&#26377;&#20851;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36827;&#34892;&#38750;&#33521;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#26102;&#65292;&#38656;&#35201;&#35880;&#24910;&#20351;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional natural language understanding abilities and have excelled in a variety of natural language processing (NLP)tasks in recent years. Despite the fact that most LLMs are trained predominantly in English, multiple studies have demonstrated their comparative performance in many other languages. However, fundamental questions persist regarding how LLMs acquire their multi-lingual abilities and how performance varies across different languages. These inquiries are crucial for the study of LLMs since users and researchers often come from diverse language backgrounds, potentially influencing their utilization and interpretation of LLMs' results. In this work, we propose a systematic way of qualifying the performance disparities of LLMs under multilingual settings. We investigate the phenomenon of across-language generalizations in LLMs, wherein insufficient multi-lingual training data leads to advanced multi-lingual capabilities. To acc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21477;&#23376;&#30456;&#20284;&#24615;&#21644;&#37325;&#25490;&#24207;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15077</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23545;&#27604;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Sentence Embeddings from Scratch. (arXiv:2305.15077v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#38646;&#24320;&#22987;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21477;&#23376;&#30456;&#20284;&#24615;&#21644;&#37325;&#25490;&#24207;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#19968;&#30452;&#26159;&#35757;&#32451;&#26368;&#20808;&#36827;&#30340;&#21477;&#23376;&#23884;&#20837;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25968;&#25454;&#25110;&#36890;&#36807;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#30340;&#21477;&#23376;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22312;&#26576;&#20123;&#39046;&#22495;&#33719;&#21462;&#25968;&#25454;&#26679;&#26412;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SynCSE&#65292;&#19968;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#21477;&#23376;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#23545;&#27604;&#23398;&#20064;&#25152;&#38656;&#30340;&#25968;&#25454;&#26679;&#26412;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20135;&#29983;&#32473;&#23450;&#26080;&#26631;&#31614;&#21477;&#23376;&#30340;&#27491;&#36127;&#26631;&#27880;&#65288;SynCSE-partial&#65289;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20174;&#38646;&#24320;&#22987;&#29983;&#25104;&#21477;&#23376;&#21450;&#20854;&#30456;&#24212;&#30340;&#26631;&#27880;&#65288;SynCSE-scratch&#65289;&#12290;&#23545;&#21477;&#23376;&#30456;&#20284;&#24615;&#21644;&#37325;&#25490;&#24207;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SynCSE-partial&#21644;SynCSE-scratch&#20004;&#32773;&#37117;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been the dominant approach to train state-of-the-art sentence embeddings. Previous studies have typically learned sentence embeddings either through the use of human-annotated natural language inference (NLI) data or via large-scale unlabeled sentences in an unsupervised manner. However, even in the case of unlabeled data, their acquisition presents challenges in certain domains due to various reasons. To address these issues, we present SynCSE, a contrastive learning framework that trains sentence embeddings with synthesized data. Specifically, we explore utilizing large language models to synthesize the required data samples for contrastive learning, including (1) producing positive and negative annotations given unlabeled sentences (SynCSE-partial), and (2) generating sentences along with their corresponding annotations from scratch (SynCSE-scratch). Experimental results on sentence similarity and reranking tasks indicate that both SynCSE-partial and SynCSE-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15068</link><description>&lt;p&gt;
ToMChallenges: &#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#29992;&#20110;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
ToMChallenges: A Principle-Guided Dataset and Diverse Evaluation Tasks for Exploring Theory of Mind. (arXiv:2305.15068v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21407;&#21017;&#30340;&#25968;&#25454;&#38598;&#21644;&#22810;&#26679;&#21270;&#35780;&#20272;&#20219;&#21153;&#65292;&#21517;&#20026;ToMChallenges&#65292;&#20197;&#25506;&#32034;&#24515;&#26234;&#29702;&#35770;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#26234;&#29702;&#35770;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#29702;&#35299;&#19981;&#21516;&#20010;&#20307;&#24515;&#26234;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#23545;&#20110;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#65292;&#20851;&#20110;&#23427;&#20204;&#26159;&#21542;&#33021;&#22815;&#25191;&#34892;ToM&#20219;&#21153;&#23384;&#22312;&#28608;&#28872;&#30340;&#20105;&#35758;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#25552;&#31034;&#26469;&#27979;&#35797;LLMs&#19978;&#30340;ToM&#65292;&#32467;&#26524;&#19981;&#19968;&#33268;&#65306;&#19968;&#20123;&#30740;&#31350;&#35748;&#20026;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23637;&#31034;ToM&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#25345;&#30456;&#21453;&#35266;&#28857;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ToMChallenges&#65292;&#19968;&#20010;&#22522;&#20110;Sally-Anne&#21644;Smarties&#27979;&#35797;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20840;&#38754;&#35780;&#20272;&#24515;&#26234;&#29702;&#35770;&#24182;&#21253;&#21547;&#22810;&#26679;&#21270;&#30340;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#35780;&#20998;&#22120;&#26469;&#31616;&#21270;&#31572;&#26696;&#35780;&#20272;&#36807;&#31243;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#19977;&#20010;&#27169;&#22411;&#65306;davinci&#12289;turbo&#21644;gpt-4&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#21644;&#38169;&#35823;&#20998;&#26512;&#26174;&#31034;&#65292;LLMs&#22312;&#25552;&#31034;&#21644;&#20219;&#21153;&#20043;&#38388;&#34920;&#29616;&#19981;&#19968;&#33268;&#12290;&#23545;LLMs&#26469;&#35828;&#65292;&#31283;&#23450;&#22320;&#25191;&#34892;ToM&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of Mind (ToM), the capacity to comprehend the mental states of distinct individuals, is essential for numerous practical applications. With the development of large language models (LLMs), there is a heated debate about whether they are able to perform ToM tasks. Previous studies have used different tasks and prompts to test the ToM on LLMs and the results are inconsistent: some studies asserted these models are capable of exhibiting ToM, while others suggest the opposite. In this study, We present ToMChallenges, a dataset for comprehensively evaluating the Theory of Mind based on the Sally-Anne and Smarties tests with a diverse set of tasks. In addition, we also propose an auto-grader to streamline the answer evaluation process. We tested three models: davinci, turbo, and gpt-4. Our evaluation results and error analyses show that LLMs have inconsistent behaviors across prompts and tasks. Performing the ToM tasks robustly remains a challenge for the LLMs. In addition, our paper 
&lt;/p&gt;</description></item><item><title>Dior-CVAE&#26159;&#19968;&#31181;&#20855;&#26377;&#25193;&#25955;&#20808;&#39564;&#30340;&#20998;&#23618;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#21464;&#20998;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#21518;&#39564;&#23849;&#28291;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15025</link><description>&lt;p&gt;
Dior-CVAE&#65306;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#20808;&#39564;&#29992;&#20110;&#21464;&#20998;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation. (arXiv:2305.15025v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15025
&lt;/p&gt;
&lt;p&gt;
Dior-CVAE&#26159;&#19968;&#31181;&#20855;&#26377;&#25193;&#25955;&#20808;&#39564;&#30340;&#20998;&#23618;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25193;&#25955;&#27169;&#22411;&#26469;&#35299;&#20915;&#21464;&#20998;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#22810;&#26679;&#24615;&#21644;&#21518;&#39564;&#23849;&#28291;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#21464;&#20998;&#23545;&#35805;&#27169;&#22411;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#26469;&#21442;&#25968;&#21270;&#20284;&#28982;&#21644;&#21518;&#39564;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#23545;&#20808;&#39564;&#20998;&#24067;&#30340;&#39640;&#26031;&#20551;&#35774;&#19982;&#36825;&#20123;&#20998;&#24067;&#19981;&#20860;&#23481;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#29983;&#25104;&#21709;&#24212;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#36824;&#23384;&#22312;&#21518;&#39564;&#23849;&#28291;&#38382;&#39064;&#65292;&#21363;&#35299;&#30721;&#22120;&#20542;&#21521;&#20110;&#36890;&#36807;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#30452;&#25509;&#35775;&#38382;&#32534;&#30721;&#22120;&#20013;&#25429;&#33719;&#30340;&#20449;&#24687;&#32780;&#24573;&#30053;&#28508;&#21464;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dior-CVAE&#65292;&#19968;&#31181;&#24102;&#26377;&#25193;&#25955;&#20808;&#39564;&#30340;&#20998;&#23618;&#26465;&#20214;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;CVAE&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#22686;&#21152;&#20808;&#39564;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#65292;&#20351;&#20854;&#19982;PLM&#20135;&#29983;&#30340;&#20998;&#24067;&#20860;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35760;&#24518;&#20002;&#24323;&#26426;&#21046;&#26469;&#25913;&#36827;&#20132;&#21449;&#27880;&#24847;&#26426;&#21046;&#65292;&#31215;&#26497;&#40723;&#21169;&#20351;&#29992;&#28508;&#21464;&#37327;&#36827;&#34892;&#21709;&#24212;&#29983;&#25104;&#12290;&#24635;&#20307;&#19978;&#65292;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23637;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Overall, experiments across two commonly used open-domain dialog 
&lt;/p&gt;</description></item><item><title>ACL OCL&#26159;&#19968;&#20010;&#23398;&#26415;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25512;&#21160;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#31185;&#30740;&#12290;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#12289;PDF&#25991;&#20214;&#12289;&#24341;&#29992;&#22270;&#21644;&#32467;&#26500;&#21270;&#20840;&#25991;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35266;&#23519;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#12290;ACL OCL&#30340;&#36129;&#29486;&#22312;&#20110;&#21457;&#29616;&#20102;"&#21477;&#27861;&#65306;&#26631;&#27880;&#12289;&#20998;&#22359;&#21644;&#35299;&#26512;"&#20852;&#36259;&#20943;&#24369;&#65292;"&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;"&#20852;&#36259;&#22797;&#33487;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.14996</link><description>&lt;p&gt;
ACL OCL&#35821;&#26009;&#24211;&#65306;&#25512;&#21160;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
The ACL OCL Corpus: Advancing Open Science in Computational Linguistics. (arXiv:2305.14996v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14996
&lt;/p&gt;
&lt;p&gt;
ACL OCL&#26159;&#19968;&#20010;&#23398;&#26415;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25512;&#21160;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#31185;&#30740;&#12290;&#23427;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#12289;PDF&#25991;&#20214;&#12289;&#24341;&#29992;&#22270;&#21644;&#32467;&#26500;&#21270;&#20840;&#25991;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#29992;&#20110;&#35266;&#23519;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#12290;ACL OCL&#30340;&#36129;&#29486;&#22312;&#20110;&#21457;&#29616;&#20102;"&#21477;&#27861;&#65306;&#26631;&#27880;&#12289;&#20998;&#22359;&#21644;&#35299;&#26512;"&#20852;&#36259;&#20943;&#24369;&#65292;"&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;"&#20852;&#36259;&#22797;&#33487;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;ACL OCL&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;ACL Anthology&#20013;&#34893;&#29983;&#20986;&#26469;&#30340;&#23398;&#26415;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#25512;&#21160;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#24320;&#25918;&#31185;&#30740;&#12290;ACL OCL&#25972;&#21512;&#21644;&#22686;&#24378;&#20102;&#20197;&#21069;&#29256;&#26412;&#30340;ACL Anthology&#65292;&#25552;&#20379;&#20102;&#20803;&#25968;&#25454;&#12289;PDF&#25991;&#20214;&#12289;&#24341;&#29992;&#22270;&#21644;&#38468;&#21152;&#30340;&#32467;&#26500;&#21270;&#20840;&#25991;&#20449;&#24687;&#65292;&#21253;&#25324;&#33410;&#12289;&#22270;&#21644;&#38142;&#25509;&#21040;&#22823;&#22411;&#30693;&#35782;&#36164;&#28304;&#65288;Semantic Scholar&#65289;&#12290;ACL OCL&#27178;&#36328;&#19971;&#21313;&#24180;&#65292;&#21253;&#21547;73K&#31687;&#35770;&#25991;&#21644;210K&#20010;&#22270;&#34920;&#12290;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#22914;&#20309;&#21033;&#29992;ACL OCL&#26469;&#35266;&#23519;&#35745;&#31639;&#35821;&#35328;&#23398;&#39046;&#22495;&#30340;&#36235;&#21183;&#12290;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#27169;&#22411;&#26816;&#27979;&#35770;&#25991;&#20027;&#39064;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#20110;&#8220;&#21477;&#27861;&#65306;&#26631;&#27880;&#12289;&#20998;&#22359;&#21644;&#35299;&#26512;&#8221;&#30340;&#20852;&#36259;&#27491;&#22312;&#20943;&#24369;&#65292;&#32780;&#8220;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#8221;&#27491;&#22312;&#22797;&#33487;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#20174;HuggingFace&#65288;https://huggingface.co/datasets/WINGNUS/ACL-OCL&#65289;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ACL OCL, a scholarly corpus derived from the ACL Anthology to assist Open scientific research in the Computational Linguistics domain. Integrating and enhancing the previous versions of the ACL Anthology, the ACL OCL contributes metadata, PDF files, citation graphs and additional structured full texts with sections, figures, and links to a large knowledge resource (Semantic Scholar). The ACL OCL spans seven decades, containing 73K papers, alongside 210K figures.  We spotlight how ACL OCL applies to observe trends in computational linguistics. By detecting paper topics with a supervised neural model, we note that interest in "Syntax: Tagging, Chunking and Parsing" is waning and "Natural Language Generation" is resurging. Our dataset is available from HuggingFace (https://huggingface.co/datasets/WINGNUS/ACL-OCL).
&lt;/p&gt;</description></item><item><title>Dolphin&#26159;&#19968;&#20010;&#38754;&#21521;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#21464;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#21547;&#20102;13&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#38598;&#12290;&#23427;&#20026;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#38463;&#25289;&#20271;&#35821;NLG&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14989</link><description>&lt;p&gt;
Dolphin: &#19968;&#20010;&#25361;&#25112;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38463;&#25289;&#20271;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Dolphin: A Challenging and Diverse Benchmark for Arabic NLG. (arXiv:2305.14989v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14989
&lt;/p&gt;
&lt;p&gt;
Dolphin&#26159;&#19968;&#20010;&#38754;&#21521;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#21464;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#21547;&#20102;13&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;&#25552;&#20379;&#20102;&#22823;&#37327;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#21644;&#27979;&#35797;&#38598;&#12290;&#23427;&#20026;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#26377;&#21161;&#20110;&#25512;&#21160;&#38463;&#25289;&#20271;&#35821;NLG&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Dolphin&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#28385;&#36275;&#23545;&#22810;&#31181;&#38463;&#25289;&#20271;&#35821;&#35328;&#21644;&#21464;&#20307;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#38656;&#27714;&#12290;&#25152;&#25552;&#20986;&#30340;&#22522;&#20934;&#21253;&#25324;13&#31181;&#19981;&#21516;&#30340;NLG&#20219;&#21153;&#65292;&#21253;&#25324;&#23545;&#35805;&#29983;&#25104;&#12289;&#38382;&#39064;&#22238;&#31572;&#12289;&#26426;&#22120;&#32763;&#35793;&#12289;&#25688;&#35201;&#31561;&#24191;&#27867;&#33539;&#22260;&#12290;Dolphin&#21253;&#25324;&#19968;&#20010;&#22823;&#22411;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;40&#20010;&#22810;&#26679;&#21270;&#21644;&#20195;&#34920;&#24615;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#20998;&#24067;&#22312;50&#20010;&#27979;&#35797;&#38598;&#20013;&#65292;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#20197;&#21453;&#26144;&#30495;&#23454;&#19990;&#30028;&#24773;&#26223;&#21644;&#38463;&#25289;&#20271;&#35821;&#30340;&#35821;&#35328;&#20016;&#23500;&#24615;&#12290;&#23427;&#20026;&#35780;&#20272;&#38463;&#25289;&#20271;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#35774;&#23450;&#20102;&#26032;&#30340;&#26631;&#20934;&#65292;&#26377;&#26395;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25512;&#21160;&#24403;&#21069;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#23545;Dolphin&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#26512;&#65292;&#31361;&#20986;&#20854;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#20102;&#24403;&#21069;&#38463;&#25289;&#20271;&#35821;NLG&#30740;&#31350;&#20013;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20114;&#21160;&#21644;&#27169;&#22359;&#21270;&#30340;&#20844;&#20849;&#25490;&#34892;&#27036;&#65292;&#24182;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;&#20960;&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Dolphin, a novel benchmark that addresses the need for a natural language generation (NLG) evaluation framework dedicated to the wide collection of Arabic languages and varieties. The proposed benchmark encompasses a broad range of 13 different NLG tasks, including dialogue generation, question answering, machine translation, summarization, among others. Dolphin comprises a substantial corpus of 40 diverse and representative public datasets across 50 test splits, carefully curated to reflect real-world scenarios and the linguistic richness of Arabic. It sets a new standard for evaluating the performance and generalization capabilities of Arabic and multilingual models, promising to enable researchers to push the boundaries of current methodologies. We provide an extensive analysis of Dolphin, highlighting its diversity and identifying gaps in current Arabic NLG research. We also offer a public leaderboard that is both interactive and modular and evaluate several models on ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;RLHF-LMs&#65292;&#20687;ChatGPT&#12289;GPT-4&#21644;Claude&#36825;&#26679;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#35821;&#35328;&#21270;&#32622;&#20449;&#24230;&#36890;&#24120;&#27604;&#26465;&#20214;&#27010;&#29575;&#26356;&#22909;&#22320;&#36827;&#34892;&#20102;&#26631;&#23450;&#12290;</title><link>http://arxiv.org/abs/2305.14975</link><description>&lt;p&gt;
&#21482;&#38656;&#25552;&#38382;&#21363;&#21487;&#36827;&#34892;&#26631;&#23450;: &#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#21462;&#26631;&#23450;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Just Ask for Calibration: Strategies for Eliciting Calibrated Confidence Scores from Language Models Fine-Tuned with Human Feedback. (arXiv:2305.14975v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14975
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20174;&#20154;&#31867;&#21453;&#39304;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23545;&#20110;RLHF-LMs&#65292;&#20687;ChatGPT&#12289;GPT-4&#21644;Claude&#36825;&#26679;&#30340;&#27169;&#22411;&#36755;&#20986;&#30340;&#35821;&#35328;&#21270;&#32622;&#20449;&#24230;&#36890;&#24120;&#27604;&#26465;&#20214;&#27010;&#29575;&#26356;&#22909;&#22320;&#36827;&#34892;&#20102;&#26631;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#23454;&#38469;&#39044;&#27979;&#31995;&#32479;&#24212;&#35813;&#20135;&#29983;&#33391;&#22909;&#26631;&#23450;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#20854;&#23545;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#24212;&#35813;&#33021;&#22815;&#34920;&#26126;&#31572;&#26696;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#22312;&#32622;&#20449;&#24230;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23547;&#27714;&#19987;&#23478;&#24847;&#35265;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20135;&#29983;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#24120;&#22909;&#22320;&#36827;&#34892;&#20102;&#26631;&#23450;&#12290;&#28982;&#32780;&#65292;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;LMs&#26159;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#31934;&#35843;&#65288;RLHF-LMs&#65289;&#65292;&#19968;&#20123;&#30740;&#31350;&#25351;&#20986;RLHF-LMs&#20135;&#29983;&#30340;&#26465;&#20214;&#27010;&#29575;&#38750;&#24120;&#24046;&#22320;&#36827;&#34892;&#26631;&#23450;&#12290;&#37492;&#20110;&#36825;&#31181;&#30693;&#35273;&#19978;&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#23545;&#20174;RLHF-LMs&#20013;&#25552;&#21462;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#23545;&#20110;&#20687;ChatGPT&#12289;GPT-4&#21644;Claude&#36825;&#26679;&#30340;RLHF-LMs&#65292;&#25105;&#20204;&#21457;&#29616;&#36755;&#20986;&#26631;&#35760;&#20013;&#21457;&#20986;&#30340;&#35821;&#35328;&#21270;&#30340;&#32622;&#20449;&#24230;&#36890;&#24120;&#27604;&#27169;&#22411;&#22312;TriviaQA&#12289;SciQ&#21644;TruthfulQA ben&#19978;&#30340;&#26465;&#20214;&#27010;&#29575;&#26356;&#22909;&#22320;&#36827;&#34892;&#20102;&#26631;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
A trustworthy real-world prediction system should produce well-calibrated confidence scores; that is, its confidence in an answer should be indicative of the likelihood that the answer is correct, enabling deferral to an expert in cases of low-confidence predictions. Recent studies have shown that unsupervised pre-training produces large language models (LMs) whose conditional probabilities are remarkably well-calibrated. However, the most widely-used LMs are fine-tuned with reinforcement learning from human feedback (RLHF-LMs), and some studies have suggested that RLHF-LMs produce conditional probabilities that are very poorly calibrated. In light of this perceived weakness, we conduct a broad evaluation of methods for extracting confidence scores from RLHF-LMs. For RLHF-LMs such as ChatGPT, GPT-4, and Claude, we find that verbalized confidences emitted as output tokens are typically better-calibrated than the model's conditional probabilities on the TriviaQA, SciQ, and TruthfulQA ben
&lt;/p&gt;</description></item><item><title>GRACE&#26159;&#19968;&#31181;&#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#26469;&#35780;&#20998;&#19979;&#19968;&#27493;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#23481;&#26131;&#24471;&#21040;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;GRACE&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.14934</link><description>&lt;p&gt;
GRACE: &#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GRACE: Discriminator-Guided Chain-of-Thought Reasoning. (arXiv:2305.14934v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14934
&lt;/p&gt;
&lt;p&gt;
GRACE&#26159;&#19968;&#31181;&#21028;&#21035;&#22120;&#24341;&#23548;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#26469;&#35780;&#20998;&#19979;&#19968;&#27493;&#20505;&#36873;&#65292;&#35299;&#20915;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#27493;&#25512;&#29702;&#20013;&#23481;&#26131;&#24471;&#21040;&#38169;&#35823;&#31572;&#26696;&#30340;&#38382;&#39064;&#12290;&#22312;&#22810;&#20010;&#25968;&#23398;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#20013;&#65292;GRACE&#30456;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26126;&#26174;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#27493;&#25512;&#29702;&#30340;&#32972;&#26223;&#19979;&#65292;&#20363;&#22914;&#20351;&#29992;&#24605;&#32500;&#38142;&#65292;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#20250;&#23545;&#38169;&#35823;&#30340;&#27493;&#39588;&#20998;&#37197;&#36739;&#39640;&#30340;&#21487;&#33021;&#24615;&#12290;&#22240;&#27492;&#65292;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#24615;&#30340;&#35299;&#30721;&#31574;&#30053;&#24448;&#24448;&#20250;&#20135;&#29983;&#38169;&#35823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;GRACE&#30340;&#24341;&#23548;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36880;&#27493;&#35299;&#30721;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#27491;&#30830;&#24615;&#21028;&#21035;&#22120;&#35757;&#32451;&#26469;&#24341;&#23548;&#35299;&#30721;&#36807;&#31243;&#20135;&#29983;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;GRACE&#20351;&#29992;&#19968;&#20010;&#22312;&#27491;&#30830;&#21644;&#38169;&#35823;&#27493;&#39588;&#19978;&#36827;&#34892;&#23545;&#27604;&#25439;&#22833;&#35757;&#32451;&#30340;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#22522;&#20110;&#27491;&#30830;&#24615;&#23545;&#19979;&#19968;&#27493;&#20505;&#36873;&#36827;&#34892;&#35780;&#20998;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;GRACE&#21482;&#38656;&#35201;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#25105;&#20204;&#20351;&#29992;FLAN-T5&#21644;LLaMA&#31995;&#21015;&#30340;&#27169;&#22411;&#65292;&#23545;&#22235;&#20010;&#25968;&#23398;&#21644;&#20004;&#20010;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#36827;&#34892;&#20102;GRACE&#30340;&#35780;&#20272;&#65292;&#22312;&#22823;&#22810;&#25968;&#35774;&#32622;&#20013;&#65292;&#19982;&#36138;&#23146;&#35299;&#30721;&#12289;&#39564;&#35777;&#22120;&#21644;&#33258;&#19968;&#33268;&#24615;&#30456;&#27604;&#65292;GRACE&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of multi-step reasoning, e.g., with chain-of-thought, language models (LMs) can easily assign a high likelihood to incorrect steps. As a result, decoding strategies that optimize for solution likelihood often yield incorrect solutions. To address this issue, we propose Guiding chain-of-thought ReAsoning with a CorrectnEss Discriminator (GRACE), a stepwise decoding approach that steers the decoding process towards producing correct reasoning steps. GRACE employs a discriminator trained with a contrastive loss over correct and incorrect steps, which is used during decoding to score next-step candidates based on their correctness. Importantly, GRACE only requires sampling from the LM, without the need for LM training or fine-tuning. Using models from FLAN-T5 and LLaMA families, we evaluate GRACE over four math and two symbolic reasoning tasks, where it exhibits substantial performance gains compared to greedy decoding, verifiers, and self-consistency in most settings. When 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;Python&#20195;&#30721;&#24418;&#24335;&#34920;&#36798;&#30340;&#25991;&#23383;&#28216;&#25103;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#28216;&#25103;&#20316;&#20026;&#27169;&#26495;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#27169;&#25311;&#36924;&#30495;&#24230;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.14879</link><description>&lt;p&gt;
ByteSized32: &#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20197;&#25991;&#23383;&#28216;&#25103;&#24418;&#24335;&#34920;&#36798;&#30340;&#20219;&#21153;&#29305;&#23450;&#19990;&#30028;&#27169;&#22411;&#30340;&#35821;&#26009;&#24211;&#21644;&#25361;&#25112;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
ByteSized32: A Corpus and Challenge Task for Generating Task-Specific World Models Expressed as Text Games. (arXiv:2305.14879v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14879
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#19990;&#30028;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#29983;&#25104;&#20197;Python&#20195;&#30721;&#24418;&#24335;&#34920;&#36798;&#30340;&#25991;&#23383;&#28216;&#25103;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#35777;&#26126;GPT-4&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#28216;&#25103;&#20316;&#20026;&#27169;&#26495;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#27169;&#25311;&#36924;&#30495;&#24230;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#31185;&#23398;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#30340;&#26126;&#30830;&#12289;&#21487;&#35299;&#37322;&#21644;&#20114;&#21160;&#19990;&#30028;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#20219;&#21153;&#25805;&#20316;&#21270;&#20026;&#29983;&#25104;&#20197;Python&#20195;&#30721;&#24418;&#24335;&#34920;&#36798;&#30340;&#25991;&#23383;&#28216;&#25103;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20415;&#20110;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ByteSized32&#65292;&#19968;&#20010;&#21253;&#21547;32&#20010;&#20197;&#25512;&#29702;&#20026;&#37325;&#28857;&#30340;&#25991;&#23383;&#28216;&#25103;&#30340;&#35821;&#26009;&#24211;&#65292;&#24635;&#20849;&#26377;2&#19975;&#34892;Python&#20195;&#30721;&#12290;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#35777;&#26126;GPT-4&#21487;&#20197;&#20351;&#29992;&#36825;&#20123;&#28216;&#25103;&#20316;&#20026;&#21333;&#27425;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#27169;&#26495;&#65292;&#22312;28%&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#29983;&#25104;&#26410;&#35265;&#36807;&#20027;&#39064;&#30340;&#21487;&#36816;&#34892;&#28216;&#25103;&#12290;&#24403;&#20801;&#35768;&#33258;&#25105;&#21453;&#24605;&#31243;&#24207;&#38169;&#35823;&#26102;&#65292;&#28216;&#25103;&#30340;&#21487;&#36816;&#34892;&#24615;&#22823;&#22823;&#25552;&#39640;&#33267;57%&#12290;&#34429;&#28982;&#35780;&#20272;&#27169;&#25311;&#36924;&#30495;&#24230;&#27604;&#36739;&#36153;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#22871;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#26469;&#35780;&#20272;&#28216;&#25103;&#30340;&#36924;&#30495;&#24230;&#12289;&#25216;&#26415;&#26377;&#25928;&#24615;&#12289;&#19982;&#20219;&#21153;&#35268;&#26684;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#21487;&#33719;&#32988;&#24615;&#65292;&#26174;&#31034;&#20986;&#19982;&#19987;&#23478;&#20154;&#24037;&#35780;&#32423;&#30456;&#24403;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the capacity of language models to generate explicit, interpretable, and interactive world models of scientific and common-sense reasoning tasks. We operationalize this as a task of generating text games, expressed as hundreds of lines of Python code. To facilitate this task, we introduce ByteSized32 (Code: github.com/cognitiveailab/BYTESIZED32), a corpus of 32 reasoning-focused text games totaling 20k lines of Python code. We empirically demonstrate that GPT-4 can use these games as templates for single-shot in-context learning, successfully producing runnable games on unseen topics in 28% of cases. When allowed to self-reflect on program errors, game runnability substantially increases to 57%. While evaluating simulation fidelity is labor-intensive, we introduce a suite of automated metrics to assess game fidelity, technical validity, adherence to task specifications, and winnability, showing a high degree of agreement with expert human ratings. We pose t
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#32763;&#35793;&#21518;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#21487;&#38752;&#30340;&#32534;&#36753;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#24182;&#28040;&#38500;&#20102;&#21508;&#31867;&#37325;&#35201;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2305.14878</link><description>&lt;p&gt;
&#21033;&#29992;GPT-4&#36827;&#34892;&#33258;&#21160;&#32763;&#35793;&#21518;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Leveraging GPT-4 for Automatic Translation Post-Editing. (arXiv:2305.14878v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14878
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#32763;&#35793;&#21518;&#32534;&#36753;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#21487;&#38752;&#30340;&#32534;&#36753;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#24182;&#28040;&#38500;&#20102;&#21508;&#31867;&#37325;&#35201;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#20195;&#34920;&#20102;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#30340;&#39046;&#20808;&#26041;&#27861;&#65292;&#20294;NMT&#27169;&#22411;&#30340;&#36755;&#20986;&#20173;&#38656;&#35201;&#36827;&#34892;&#32763;&#35793;&#21518;&#32534;&#36753;&#20197;&#32416;&#27491;&#38169;&#35823;&#24182;&#22312;&#20851;&#38190;&#29615;&#22659;&#19979;&#25552;&#39640;&#36136;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#30452;&#25509;&#32763;&#35793;&#21518;&#32534;&#36753;&#20219;&#21153;&#24418;&#24335;&#21270;&#20026;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#25506;&#32034;&#20351;&#29992;GPT-4&#33258;&#21160;&#32763;&#35793;&#21518;&#32534;&#36753;NMT&#36755;&#20986;&#30340;&#22810;&#31181;&#35821;&#35328;&#23545;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292; GPT-4&#22312;&#32763;&#35793;&#21518;&#32534;&#36753;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20135;&#29983;&#26377;&#24847;&#20041;&#19988;&#21487;&#38752;&#30340;&#32534;&#36753;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#20854;&#24635;&#20307;&#36136;&#37327;&#24182;&#28040;&#38500;&#21508;&#31867;&#37325;&#35201;&#38169;&#35823;&#12290;&#29305;&#21035;&#26159;&#65292;&#20154;&#31867;&#23545;&#32534;&#36753;&#21487;&#38752;&#24615;&#36827;&#34892;&#35780;&#20272;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;LLM&#21462;&#24471;&#20102;&#22823;&#24133;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;GPT-4&#30340;&#32763;&#35793;&#21518;&#32534;&#36753;&#22312;WMT-22&#30340;&#33521;&#20013;&#12289;&#33521;&#24503;&#12289;&#20013;&#33521;&#21644;&#24503;&#33521;&#35821;&#31181;&#23545;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#26368;&#20808;&#36827;&#24615;&#33021;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Neural Machine Translation (NMT) represents the leading approach to Machine Translation (MT), the outputs of NMT models still require translation post-editing to rectify errors and enhance quality under critical settings. In this work, we formalize the task of direct translation post-editing with Large Language Models (LLMs) and explore the use of GPT-4 to automatically post-edit NMT outputs across several language pairs. Our results demonstrate that GPT-4 is adept at translation post-editing, producing meaningful and trustworthy edits to translations that help improve its general quality as well as remove different classes of major errors in translations. In particular, human evaluations on assessing edit trustworthiness show that GPT-4 exhibits a large improvement over the prior state-of-the-art LLM. Notably, we improve upon state-of-the-art performance on WMT-22 English-Chinese, English-German, Chinese-English and German-English language pairs using GPT-4 based post-editing, a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#36328;&#35821;&#35328;&#22810;&#27169;&#24577;&#30340;MAML&#65292;&#20351;&#24471;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#20174;&#32780;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14843</link><description>&lt;p&gt;
&#35270;&#35273;&#19982;&#35821;&#35328;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-learning For Vision-and-language Cross-lingual Transfer. (arXiv:2305.14843v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#24494;&#35843;&#26694;&#26550;&#65292;&#36890;&#36807;&#35774;&#35745;&#36328;&#35821;&#35328;&#22810;&#27169;&#24577;&#30340;MAML&#65292;&#20351;&#24471;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#20174;&#32780;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26032;&#39062;&#30340;&#22810;&#35821;&#35328;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#12290;&#20294;&#26159;&#65292;&#24403;&#36825;&#20123;&#27169;&#22411;&#29992;&#20110;&#22810;&#27169;&#24577;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#36164;&#28304;&#26377;&#38480;&#30340;&#35821;&#35328;&#26102;&#65292;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#24494;&#35843;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20197;&#36328;&#35821;&#35328;&#22810;&#27169;&#24577;&#30340;&#26041;&#24335;&#35774;&#35745;MAML&#65292;&#20351;&#24471;&#24403;&#21069;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#35270;&#35273;-&#35821;&#35328;&#22330;&#26223;&#20013;&#30340;&#26032;&#35821;&#35328;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#65288;XVNLI&#65292;xGQA&#65292;MaRVL&#65292;xFlicker&amp;Co&#65289;&#19978;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current pre-trained vison-language models (PVLMs) achieve excellent performance on a range of multi-modal datasets. Recent work has aimed at building multilingual models, and a range of novel multilingual multi-modal datasets have been proposed. Current PVLMs typically perform poorly on these datasets when used for multi-modal zero-shot or few-shot cross-lingual transfer, especially for low-resource languages. To alleviate this problem, we propose a novel meta-learning fine-tuning framework. Our framework makes current PVLMs rapidly adaptive to new languages in vision-language scenarios by designing MAML in a cross-lingual multi-modal manner. Experiments show that our method boosts the performance of current state-of-the-art PVLMs in both zero-shot and few-shot cross-lingual transfer on a range of vision-language understanding tasks and datasets (XVNLI, xGQA, MaRVL, xFlicker&amp;Co)
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20026;&#25105;&#20204;&#20174;&#35745;&#31639;&#35748;&#30693;&#35821;&#35328;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#26159;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2305.14616</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#25551;&#36848;&#20013;&#25506;&#35752;&#23545;&#22522;&#30784;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Exploring the Grounding Issues in Image Caption. (arXiv:2305.14616v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14616
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20026;&#25105;&#20204;&#20174;&#35745;&#31639;&#35748;&#30693;&#35821;&#35328;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25506;&#35752;&#20102;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#26159;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#25551;&#36848;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#20174;&#35745;&#31639;&#35748;&#30693;&#35821;&#35328;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#35821;&#20041;&#34920;&#24449;&#20013;&#30340;&#22522;&#30784;&#38382;&#39064;&#12290;&#37319;&#29992;&#20102;&#24863;&#30693;&#24615;&#12289;&#21151;&#33021;&#24615;&#12289;&#26174;&#33879;&#24615;&#12289;&#27880;&#24847;&#21147;&#21644;&#29983;&#24577;&#23398;&#22810;&#26679;&#24615;&#20851;&#32852;&#31561;&#20116;&#20010;&#22522;&#30784;&#23646;&#24615;&#36827;&#34892;&#27880;&#37322;&#21644;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;Flickr30k&#25968;&#25454;&#38598;&#20013;&#25152;&#36873;&#30340;&#22270;&#20687;&#36827;&#34892;&#25506;&#32034;&#24615;&#20998;&#26512;&#21644;&#32479;&#35745;&#24314;&#27169;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#19968;&#20010;&#23545;&#35937;&#25110;&#20107;&#20214;&#30340;&#20840;&#38754;&#29702;&#35299;&#38656;&#35201;&#35748;&#30693;&#27880;&#24847;&#21147;&#12289;&#35821;&#20041;&#34920;&#36798;&#30340;&#35821;&#20041;&#21306;&#20998;&#21644;&#22810;&#27169;&#24577;&#26500;&#24314;&#12290;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#35266;&#23519;&#32773;&#23558;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#34701;&#20837;&#21040;&#22810;&#27169;&#24577;&#35821;&#20041;&#24403;&#20013;&#65292;&#23558;&#20854;&#24041;&#22266;&#21040;&#21253;&#21547;&#35270;&#35273;&#21644;&#25991;&#26412;&#20803;&#32032;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#25551;&#36848;&#20013;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#24773;&#22659;&#21547;&#20041;&#21644;&#21151;&#33021;&#24615;&#23545;&#20110;&#22522;&#30784;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#29983;&#25104;&#36866;&#24403;&#30340;&#22270;&#20687;&#25551;&#36848;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the grounding issue concerning multimodal semantic representation from a computational cognitive-linguistic view. Five perceptual properties of groundedness are annotated and analyzed: Affordance, Perceptual salience, Object number, Gaze cueing, and Ecological Niche Association (ENA). We annotated selected images from the Flickr30k dataset with exploratory analyses and statistical modeling of their captions. Our findings suggest that a comprehensive understanding of an object or event requires cognitive attention, semantic distinctions in linguistic expression, and multimodal construction. During this construction process, viewers integrate situated meaning and affordance into multimodal semantics, which is consolidated into image captions used in the image-text dataset incorporating visual and textual elements. Our findings suggest that situated meaning and affordance grounding are critical for grounded natural language understanding systems to generate appropriate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#20687;&#32032;&#34920;&#31034;&#26377;&#25928;&#22320;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20027;&#35201;&#20307;&#29616;&#22312;&#23545;&#22810;&#31181;&#35821;&#35328;&#21644;&#25991;&#23383;&#30340;&#34920;&#29616;&#20248;&#21270;&#12289;&#20687;&#32032;&#34920;&#31034;&#30340;&#23646;&#24615;&#25506;&#32034;&#20197;&#21450;&#23454;&#29616;&#26080;&#32541;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.14280</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#20687;&#32032;&#34920;&#31034;&#29992;&#20110;&#32763;&#35793;&#21644;&#26377;&#25928;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Multilingual Pixel Representations for Translation and Effective Cross-lingual Transfer. (arXiv:2305.14280v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#20687;&#32032;&#34920;&#31034;&#26377;&#25928;&#22320;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20854;&#20013;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#20027;&#35201;&#20307;&#29616;&#22312;&#23545;&#22810;&#31181;&#35821;&#35328;&#21644;&#25991;&#23383;&#30340;&#34920;&#29616;&#20248;&#21270;&#12289;&#20687;&#32032;&#34920;&#31034;&#30340;&#23646;&#24615;&#25506;&#32034;&#20197;&#21450;&#23454;&#29616;&#26080;&#32541;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20687;&#32032;&#34920;&#31034;&#26377;&#25928;&#22320;&#35757;&#32451;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#19981;&#21516;&#30340;&#25968;&#25454;&#35774;&#32622;&#19979;&#36827;&#34892;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#22810;&#31181;&#35821;&#35328;&#21644;&#25991;&#23383;&#65292;&#19982;&#23376;&#35789;&#23884;&#20837;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;&#32032;&#34920;&#31034;&#30340;&#21508;&#31181;&#23646;&#24615;&#65292;&#22914;&#33050;&#26412;&#20869;&#21644;&#33050;&#26412;&#38388;&#30340;&#21442;&#25968;&#20849;&#20139;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#22312;&#20309;&#22788;&#23454;&#29616;&#20102;&#31215;&#26497;&#30340;&#36801;&#31227;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#23646;&#24615;&#19981;&#20165;&#20351;&#24471;&#26080;&#32541;&#36328;&#35821;&#35328;&#36716;&#31227;&#21040;&#26410;&#35265;&#36807;&#30340;&#25991;&#23383;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#19988;&#20351;&#20687;&#32032;&#34920;&#31034;&#27604;&#20854;&#20182;&#26041;&#27861;&#22914;&#35789;&#27719;&#25193;&#23637;&#26356;&#20855;&#25968;&#25454;&#25928;&#29575;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#20026;&#25152;&#26377;&#35821;&#35328;&#21644;&#25991;&#23383;&#21019;&#36896;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#30340;&#22810;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce and demonstrate how to effectively train multilingual machine translation models with pixel representations. We experiment with two different data settings with a variety of language and script coverage, demonstrating improved performance compared to subword embeddings. We explore various properties of pixel representations such as parameter sharing within and across scripts to better understand where they lead to positive transfer. We observe that these properties not only enable seamless cross-lingual transfer to unseen scripts, but make pixel representations more data-efficient than alternatives such as vocabulary expansion. We hope this work contributes to more extensible multilingual models for all languages and scripts.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13999</link><description>&lt;p&gt;
&#36808;&#21521;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Towards A Unified View of Sparse Feed-Forward Network in Pretraining Large Language Model. (arXiv:2305.13999v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13999
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#26469;&#20998;&#26512;&#31232;&#30095;&#21069;&#39304;&#32593;&#32476;&#22312;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#24179;&#22343;&#32858;&#21512;&#38544;&#34255;&#29366;&#24577;&#30340;&#36873;&#25321;&#26041;&#27861;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;MoE&#26550;&#26500;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#19988;&#31232;&#30095;&#30340;&#21069;&#39304;&#23618;&#65288;S-FFN&#65289;&#65292;&#22914;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#25193;&#22823;Transformer&#27169;&#22411;&#35268;&#27169;&#20197;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#36890;&#36807;&#20165;&#28608;&#27963;&#37096;&#20998;&#20381;&#36182;&#20110;&#36755;&#20837;&#30340;FFN&#21442;&#25968;&#65292;S-FFN&#22312;&#20445;&#25345;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#65288;&#20197;FLOPs&#35745;&#31639;&#65289;&#19981;&#21464;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22312;&#31232;&#30095;&#31070;&#32463;&#35760;&#24518;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#19979;&#65292;&#20998;&#26512;&#20102;S-FFN&#30340;&#20004;&#20010;&#20027;&#35201;&#35774;&#35745;&#36873;&#25321;&#65306;&#20869;&#23384;&#22359;&#65288;&#21363;&#19987;&#23478;&#65289;&#22823;&#23567;&#21644;&#20869;&#23384;&#22359;&#36873;&#25321;&#26041;&#27861;&#12290;&#21033;&#29992;&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20960;&#31181;&#29992;&#20110;&#35821;&#35328;&#24314;&#27169;&#30340;S-FFN&#26550;&#26500;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#21457;&#29616;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#36873;&#25321;&#26041;&#27861; - Avg-K&#65292;&#36890;&#36807;&#22343;&#20540;&#32858;&#21512;&#30340;&#38544;&#34255;&#29366;&#24577;&#26469;&#36873;&#25321;&#22359;&#65292;&#30456;&#27604;&#21253;&#25324;Switch Transformer&#65288;Fedus&#31561;&#65292;2021&#65289;&#21644;HashLaye&#22312;&#20869;&#30340;&#29616;&#26377;MoE&#26550;&#26500;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and sparse feed-forward layers (S-FFN) such as Mixture-of-Experts (MoE) have proven effective in scaling up Transformers model size for \textit{pretraining} large language models. By only activating part of the FFN parameters conditioning on input, S-FFN improves generalization performance while keeping training and inference costs (in FLOPs) fixed. In this work, we analyzed two major design choices of S-FFN: the memory block (a.k.a. expert) size and the memory block selection method under a general conceptual framework of sparse neural memory. Using this unified framework, we compare several S-FFN architectures for language modeling and provide insights into their relative efficacy and efficiency. We found a simpler selection method -\textbf{\texttt{Avg-K}} that selects blocks through their mean aggregated hidden states, achieving lower perplexity in language model pretraining compared to existing MoE architectures including Switch Transformer (Fedus et al., 2021) and HashLaye
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.13981</link><description>&lt;p&gt;
&#20445;&#25345;&#30693;&#35782;&#19981;&#21464;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#24320;&#25918;&#20449;&#24687;&#25277;&#21462;&#30340;&#40065;&#26834;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Preserving Knowledge Invariance: Rethinking Robustness Evaluation of Open Information Extraction. (arXiv:2305.13981v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#27169;&#25311;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#36890;&#36807;&#21028;&#26029;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#26159;&#21542;&#22987;&#32456;&#20934;&#30830;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#26159;&#30830;&#20445;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#32780;&#35328;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#35780;&#20272;&#22522;&#20934;&#37117;&#19987;&#27880;&#20110;&#39564;&#35777;&#37197;&#23545;&#21305;&#37197;&#30340;&#27491;&#30830;&#24615;&#65292;&#24573;&#30053;&#20102;&#20851;&#38190;&#30340;&#40065;&#26834;&#24615;&#27979;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#27169;&#25311;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#35780;&#20272;&#24320;&#25918;&#24335;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#30340;&#24773;&#20917;&#65292;&#20854;&#20013;&#21516;&#19968;&#30693;&#35782;&#21547;&#20041;&#30340;&#21477;&#27861;&#21644;&#34920;&#36798;&#20998;&#24067;&#20250;&#21508;&#19981;&#30456;&#21516;&#12290;&#25105;&#20204;&#35774;&#35745;&#21644;&#27880;&#37322;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#27979;&#35797;&#24179;&#21488;&#65292;&#20854;&#20013;&#27599;&#20010;&#31034;&#20363;&#37117;&#26159;&#19968;&#20010;&#30693;&#35782;&#19981;&#21464;&#30340;&#22242;&#20307;&#65292;&#30001;&#20855;&#26377;&#30456;&#21516;&#21547;&#20041;&#20294;&#32467;&#26500;&#19981;&#21516;&#30340;&#21477;&#23376;&#32452;&#25104;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#38416;&#36848;&#40065;&#26834;&#24615;&#25351;&#26631;&#65292;&#24403;&#27169;&#22411;&#22312;&#25972;&#20010;&#22242;&#20307;&#19978;&#30340;&#34920;&#29616;&#22987;&#32456;&#20934;&#30830;&#26102;&#65292;&#34987;&#21028;&#23450;&#20026;&#40065;&#26834;&#24615;&#24378;&#12290;&#25105;&#20204;&#23545;&#36807;&#21435;&#21313;&#24180;&#20013;&#21457;&#34920;&#30340;&#20960;&#31181;&#20856;&#22411;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The robustness to distribution changes ensures that NLP models can be successfully applied in the realistic world, especially for information extraction tasks. However, most prior evaluation benchmarks have been devoted to validating pairwise matching correctness, ignoring the crucial measurement of robustness. In this paper, we present the first benchmark that simulates the evaluation of open information extraction models in the real world, where the syntactic and expressive distributions under the same knowledge meaning may drift variously. We design and annotate a large-scale testbed in which each example is a knowledge-invariant clique that consists of sentences with structured knowledge of the same meaning but with different syntactic and expressive forms. By further elaborating the robustness metric, a model is judged to be robust if its performance is consistently accurate on the overall cliques. We perform experiments on typical models published in the last decade as well as a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550; SALAM&#65292;&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#26469;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25910;&#38598;&#38169;&#35823;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.13829</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learn from Mistakes through Cooperative Interaction with Study Assistant. (arXiv:2305.13829v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13829
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550; SALAM&#65292;&#36890;&#36807;&#21327;&#20316;&#20132;&#20114;&#19982;&#23398;&#20064;&#21161;&#25163;&#26469;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#25910;&#38598;&#38169;&#35823;&#24182;&#22312;&#25512;&#29702;&#26102;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#33258;&#25105;&#21453;&#24605;&#21644;&#25913;&#36827;&#29983;&#25104;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21453;&#39304;&#26426;&#21046;&#38754;&#20020;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#33021;&#20445;&#35777;&#27491;&#30830;&#24615;&#21644;&#23545;&#27169;&#22411;&#24369;&#28857;&#32570;&#20047;&#20840;&#23616;&#27934;&#23519;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; Study Assistant for Large Language Model (SALAM)&#65292;&#20197;&#24110;&#21161; LLM &#22312;&#21453;&#24605;&#21644;&#25913;&#36827;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#26681;&#25454;&#20154;&#31867;&#21161;&#29702;&#30740;&#31350;&#30340;&#28789;&#24863;&#65292;&#36890;&#36807;&#23558;&#20808;&#21069;&#30340;&#21709;&#24212;&#19982;&#30495;&#23454;&#20540;&#36827;&#34892;&#23450;&#37327;&#20998;&#32423;&#65292;&#24182;&#22312;&#35757;&#32451;&#38454;&#27573;&#25910;&#38598;&#38169;&#35823;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#23427;&#26681;&#25454;&#38169;&#35823;&#25910;&#38598;&#30830;&#23450;&#24120;&#35265;&#35823;&#35299;&#65292;&#24182;&#25552;&#20379;&#25351;&#23548;&#26041;&#38024;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#22312;&#25512;&#29702;&#26399;&#38388;&#36991;&#20813;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;SALAM &#26159;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;&#25552;&#20379;&#19968;&#33324;&#24615;&#30340;&#21453;&#39304;&#65292;&#24182;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545; SALAM &#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#23427;&#22312;&#21508;&#31181;&#22522;&#32447;&#19978;&#37117;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated their ability to self-reflect and refine their generation, which can further improve their performance. However, this feedback mechanism faces challenges such as no guarantee of correctness and the lack of global insight into the model's weaknesses. In this paper, we propose a novel framework, Study Assistant for Large Language Model (SALAM), to aid LLMs in the reflection and refinement process. Motivated by the human study assistant, this framework grades previous responses with the ground truth and collects mistakes in the training phase. During inference, it identifies common misunderstandings based on the mistake collections and provides guidelines for the model to help the model avoid similar mistakes during inference. SALAM is a model-agnostic framework, focusing on providing general feedback and can adapt to any base model. Our evaluation of SALAM on two challenging benchmarks demonstrated a significant improvement over various baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20026;&#30495;&#23454;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#35774;&#32622;&#65292;&#27573;&#33853;&#32423;&#32763;&#35793;(para2para)&#65292;&#20197;&#21450;&#25910;&#38598;&#20102;&#19968;&#20221;&#26032;&#30340;&#20013;&#33521;&#23567;&#35828;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.13751</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Challenges in Context-Aware Neural Machine Translation. (arXiv:2305.13751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#26356;&#20026;&#30495;&#23454;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#35774;&#32622;&#65292;&#27573;&#33853;&#32423;&#32763;&#35793;(para2para)&#65292;&#20197;&#21450;&#25910;&#38598;&#20102;&#19968;&#20221;&#26032;&#30340;&#20013;&#33521;&#23567;&#35828;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#28041;&#21450;&#21033;&#29992;&#21477;&#23376;&#32423;&#21035;&#19978;&#19979;&#25991;&#20043;&#22806;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#21477;&#38469;&#35805;&#35821;&#20381;&#36182;&#20851;&#31995;&#21644;&#25552;&#39640;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#65292;&#24341;&#36215;&#20102;&#35768;&#22810;&#25216;&#26415;&#26041;&#38754;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#26126;&#26234;&#30340;&#30452;&#35273;&#65292;&#22823;&#22810;&#25968;&#19978;&#19979;&#25991;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#21482;&#33021;&#26174;&#31034;&#20986;&#36866;&#24230;&#30340;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38459;&#30861;&#35813;&#39046;&#22495;&#36827;&#23637;&#30340;&#20960;&#20010;&#25361;&#25112;&#65292;&#28041;&#21450;&#35805;&#35821;&#29616;&#35937;&#12289;&#19978;&#19979;&#25991;&#20351;&#29992;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#25991;&#26723;&#32423;&#35780;&#20272;&#31561;&#26041;&#38754;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#20026;&#30495;&#23454;&#30340;&#25991;&#26723;&#32423;&#32763;&#35793;&#35774;&#32622;&#65292;&#31216;&#20026;&#27573;&#33853;&#32423;&#32763;&#35793;(para2para)&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20221;&#26032;&#30340;&#20013;&#33521;&#23567;&#35828;&#25968;&#25454;&#38598;&#20197;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context-aware neural machine translation involves leveraging information beyond sentence-level context to resolve inter-sentential discourse dependencies and improve document-level translation quality, and has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models show only modest improvements over sentence-level systems. In this work, we investigate several challenges that impede progress within this field, relating to discourse phenomena, context usage, model architectures, and document-level evaluation. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (para2para) translation, and collect a new dataset of Chinese-English novels to promote future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;StemCorrupt&#24102;&#26469;&#30340;&#26681;&#26412;&#24615;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#36873;&#25321;&#39640;&#22810;&#26679;&#24615;&#21644;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#23376;&#38598;&#26159;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;StemCorrupt&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#35789;&#24418;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.13658</link><description>&lt;p&gt;
&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#30340;&#32452;&#21512;&#25968;&#25454;&#22686;&#24378;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding compositional data augmentation in automatic morphological inflection. (arXiv:2305.13658v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;StemCorrupt&#24102;&#26469;&#30340;&#26681;&#26412;&#24615;&#21464;&#21270;&#65292;&#24182;&#35777;&#26126;&#36873;&#25321;&#39640;&#22810;&#26679;&#24615;&#21644;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#23376;&#38598;&#26159;&#25552;&#39640;&#20854;&#25968;&#25454;&#25928;&#29575;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;StemCorrupt&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#35789;&#24418;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#31232;&#30095;&#30340;&#38382;&#39064;&#65292;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20302;&#36164;&#28304;&#30340;&#33258;&#21160;&#35789;&#24418;&#21464;&#21270;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#20840;&#37096;&#24433;&#21709;&#20173;&#28982;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;StemCorrupt&#30340;&#29702;&#35770;&#26041;&#38754;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#38543;&#26426;&#26367;&#25442;&#29616;&#26377;&#30340;&#40644;&#37329;&#26631;&#20934;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#35789;&#24178;&#23383;&#31526;&#26469;&#29983;&#25104;&#21512;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;StemCorrupt&#24102;&#26469;&#20102;&#26681;&#26412;&#24615;&#30340;&#21464;&#21270;&#65292;&#22312;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#20013;&#23637;&#29616;&#20102;&#22266;&#26377;&#30340;&#32452;&#21512;&#36830;&#25509;&#32467;&#26500;&#12290;&#20026;&#20102;&#34917;&#20805;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;StemCorrupt&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#36890;&#36807;&#23545;&#19971;&#31181;&#31867;&#22411;&#23398;&#19981;&#21516;&#30340;&#35821;&#35328;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#36873;&#25321;&#39640;&#22810;&#26679;&#24615;&#21644;&#39640;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#28857;&#23376;&#38598;&#26174;&#33879;&#25552;&#39640;&#20102;StemCorrupt&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#30456;&#27604;&#31454;&#20105;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;StemCorrupt&#33021;&#22815;&#23398;&#20064;&#21487;&#25512;&#24191;&#30340;&#35789;&#24418;&#35268;&#21017;&#65292;&#36825;&#26159;&#36890;&#36807;&#20854;&#22312;&#22495;&#22806;&#20445;&#30041;&#25968;&#25454;&#19978;&#34920;&#29616;&#20986;&#30340;&#20248;&#24322;&#24615;&#33021;&#25152;&#35777;&#26126;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation techniques are widely used in low-resource automatic morphological inflection to address the issue of data sparsity. However, the full implications of these techniques remain poorly understood. In this study, we aim to shed light on the theoretical aspects of the data augmentation strategy StemCorrupt, a method that generates synthetic examples by randomly substituting stem characters in existing gold standard training examples. Our analysis uncovers that StemCorrupt brings about fundamental changes in the underlying data distribution, revealing inherent compositional concatenative structure. To complement our theoretical analysis, we investigate the data-efficiency of StemCorrupt. Through evaluation across a diverse set of seven typologically distinct languages, we demonstrate that selecting a subset of datapoints with both high diversity and high predictive uncertainty significantly enhances the data-efficiency of StemCorrupt compared to competitive baselines. Furth
&lt;/p&gt;</description></item><item><title>Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2305.13627</link><description>&lt;p&gt;
Instruct-Align&#65306;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#23558;&#26032;&#35821;&#35328;&#25945;&#32473;LLM
&lt;/p&gt;
&lt;p&gt;
Instruct-Align: Teaching Novel Languages with to LLMs through Alignment-based Cross-Lingual Instruction. (arXiv:2305.13627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13627
&lt;/p&gt;
&lt;p&gt;
Instruct-Align&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#26694;&#26550;&#65292;&#20351;&#24471;&#25945;&#23398;&#35843;&#25972;&#30340;LLMs&#33021;&#22815;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#19988;&#19981;&#20250;&#21457;&#29983;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#23398;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#35821;&#35328;&#21644;&#22810;&#31181;&#20219;&#21153;&#19978;&#30340;&#21331;&#36234;&#27867;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#19981;&#21516;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#20250;&#26377;&#25152;&#19981;&#21516;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#23569;&#25968;&#35821;&#35328;&#25110;&#32773;&#26159;&#26410;&#30693;&#35821;&#35328;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#21457;&#29616;&#65292;&#31616;&#21333;&#22320;&#23558;&#26032;&#35821;&#35328;&#36866;&#24212;&#21040;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#20013;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#20174;&#32780;&#23548;&#33268;&#36825;&#20123;LLM&#22833;&#21435;&#22810;&#20219;&#21153;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31216;&#20026;Instruct-Align&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22522;&#20110;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#25945;&#23398;&#35843;&#25972;&#65292;&#20351;&#24471;&#32463;&#36807;&#25945;&#23398;&#35843;&#25972;&#30340;LLM&#33021;&#22815;&#23398;&#20064;&#21040;&#30475;&#19981;&#35265;&#30340;&#21644;&#20043;&#21069;&#23398;&#20064;&#30340;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#22312;BLOOMZ-560M&#25968;&#25454;&#38598;&#19978;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;Instruct-Align&#33021;&#22815;&#22312;&#20165;&#20351;&#29992;&#26377;&#38480;&#37327;&#30340;&#24179;&#34892;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#35821;&#35328;&#65292;&#24182;&#19988;&#36890;&#36807;&#25345;&#32493;&#30340;&#25945;&#23398;&#35843;&#25972;&#65292;&#38450;&#27490;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction-tuned large language models (LLMs) have shown remarkable generalization capability over multiple tasks in multiple languages. Nevertheless, their generalization towards different languages varies especially to underrepresented languages or even to unseen languages. Prior works on adapting new languages to LLMs find that naively adapting new languages to instruction-tuned LLMs will result in catastrophic forgetting, which in turn causes the loss of multitasking ability in these LLMs. To tackle this, we propose the Instruct-Align a.k.a (IA)$^1$ framework, which enables instruction-tuned LLMs to learn cross-lingual alignment between unseen and previously learned languages via alignment-based cross-lingual instruction-tuning. Our preliminary result on BLOOMZ-560M shows that (IA)$^1$ is able to learn a new language effectively with only a limited amount of parallel data and at the same time prevent catastrophic forgetting by applying continual instruction-tuning through experien
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.13245</link><description>&lt;p&gt;
GQA:&#20174;&#22810;&#22836;&#26816;&#26597;&#28857;&#35757;&#32451;&#24191;&#20041;&#22810;&#26597;&#35810;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. (arXiv:2305.13245v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#30340;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#26469;&#35299;&#20915;MQA&#21487;&#33021;&#23548;&#33268;&#30340;&#36136;&#37327;&#19979;&#38477;&#38382;&#39064;&#12290;&#36890;&#36807;&#21319;&#32423;&#21518;&#30340;GQA&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#36136;&#37327;&#65292;&#24182;&#20855;&#22791;&#19982;MQA&#30456;&#24403;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;MQA&#65289;&#20165;&#20351;&#29992;&#19968;&#20010;&#38190;&#20540;&#22836;&#65292;&#22823;&#22823;&#21152;&#24555;&#20102;&#35299;&#30721;&#22120;&#25512;&#29702;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;MQA&#21487;&#33021;&#23548;&#33268;&#36136;&#37327;&#19979;&#38477;&#65292;&#24182;&#19988;&#20026;&#20102;&#26356;&#24555;&#22320;&#25512;&#29702;&#32780;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#12290;&#25105;&#20204;&#65288;1&#65289;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#65292;&#21033;&#29992;&#21407;&#22987;&#39044;&#35757;&#32451;&#35745;&#31639;&#37327;&#30340;5&#65285;&#65292;&#23558;&#29616;&#26377;&#30340;&#22810;&#22836;&#35821;&#35328;&#27169;&#22411;&#26816;&#26597;&#28857;&#21319;&#32423;&#20026;&#20855;&#26377;MQA&#30340;&#27169;&#22411;&#65292;&#24182;&#65288;2&#65289;&#24341;&#20837;&#20102;&#32676;&#32452;&#26597;&#35810;&#27880;&#24847;&#21147;&#65288;GQA&#65289;&#65292;&#23427;&#26159;&#22810;&#26597;&#35810;&#27880;&#24847;&#21147;&#30340;&#24191;&#20041;&#24418;&#24335;&#65292;&#20351;&#29992;&#20013;&#38388;&#25968;&#37327;&#30340;&#38190;&#20540;&#22836;&#65288;&#22810;&#20110;&#19968;&#20010;&#65292;&#23569;&#20110;&#26597;&#35810;&#22836;&#30340;&#25968;&#37327;&#65289;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#32463;&#36807;&#21319;&#32423;&#30340;GQA&#23454;&#29616;&#20102;&#19982;&#22810;&#22836;&#27880;&#24847;&#21147;&#30456;&#24403;&#30340;&#36895;&#24230;&#65292;&#24182;&#19988;&#20855;&#26377;&#25509;&#36817;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.
&lt;/p&gt;</description></item><item><title>&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;&#23558;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20196;&#29260;&#32423;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#22312;NMT&#30340;&#36755;&#20986;&#25237;&#24433;&#23618;&#19978;&#38544;&#24335;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#20197;&#36798;&#21040;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.13034</link><description>&lt;p&gt;
&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;&#26159;&#36755;&#20986;&#25237;&#24433;&#23618;&#19978;&#30340;&#20803;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Machine Translation is Meta-Optimizer on Output Projection Layer. (arXiv:2305.13034v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13034
&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;&#23558;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20196;&#29260;&#32423;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#36890;&#36807;&#22312;NMT&#30340;&#36755;&#20986;&#25237;&#24433;&#23618;&#19978;&#38544;&#24335;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#65292;&#20197;&#36798;&#21040;&#27169;&#22411;&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#37051;&#26426;&#22120;&#32763;&#35793;&#65288;$k$NN-MT&#65289;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#19982;&#39046;&#22495;&#29305;&#23450;&#30340;&#20196;&#29260;&#32423;&#26816;&#32034;&#30456;&#32467;&#21512;&#65292;&#22312;&#39046;&#22495;&#36866;&#24212;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#30340;&#21407;&#22240;&#23578;&#26410;&#24471;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;$k$NN-MT&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;$k$NN-MT&#30340;&#24037;&#20316;&#26426;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#23558;&#20854;&#35270;&#20026;&#22312;NMT&#30340;&#36755;&#20986;&#25237;&#24433;&#23618;&#19978;&#38544;&#24335;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#30340;&#39640;&#25928;&#25216;&#26415;&#65292;&#34920;&#26126;&#23427;&#26159;&#27169;&#22411;&#24494;&#35843;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22810;&#39046;&#22495;&#23454;&#39564;&#35777;&#26126;&#21644;&#35789;&#32423;&#20998;&#26512;&#65292;&#20197;&#26816;&#26597;$k$NN-MT&#21644;&#25972;&#20307;&#27169;&#22411;&#24494;&#35843;&#20043;&#38388;&#24615;&#33021;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;1&#65289;&#23558;$k$NN-MT&#19982;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#65292;&#22312;&#39046;&#22495;&#20869;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;&#21487;&#27604;&#36739;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#39046;&#22495;&#22806;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nearest Neighbor Machine Translation ($k$NN-MT) has achieved great success in domain adaptation tasks by integrating pre-trained Neural Machine Translation (NMT) models with domain-specific token-level retrieval. However, the reasons underlying its success have not been thoroughly investigated. In this paper, we comprehensively analyze $k$NN-MT through theoretical and empirical studies. Initially, we provide new insights into the working mechanism of $k$NN-MT as an efficient technique to implicitly execute gradient descent on the output projection layer of NMT, indicating that it is a specific case of model fine-tuning. Subsequently, we conduct multi-domain experiments and word-level analysis to examine the differences in performance between $k$NN-MT and entire-model fine-tuning. Our findings suggest that: (1) Incorporating $k$NN-MT with adapters yields comparable translation performance to fine-tuning on in-domain test sets, while achieving better performance on out-of-domain test set
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#35299;&#37322;&#24615;&#30340;&#33258;&#21160;&#23398;&#29983;&#31572;&#26696;&#35780;&#20272;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#27169;&#26495;&#25351;&#23548;ChatGPT&#25910;&#38598;&#29702;&#30001;&#65292;&#20462;&#27491;&#19981;&#19968;&#33268;&#30340;&#29702;&#30001;&#20197;&#31526;&#21512;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#19968;&#20010;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#35780;&#20272;&#23398;&#29983;&#31572;&#26696;&#21644;&#25552;&#20379;&#29702;&#30001;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;ChatGPT&#23558;&#25972;&#20307;QWK&#35780;&#20998;&#25552;&#39640;&#20102;11%&#12290;</title><link>http://arxiv.org/abs/2305.12962</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#23398;&#29983;&#31572;&#26696;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Distilling ChatGPT for Explainable Automated Student Answer Assessment. (arXiv:2305.12962v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;ChatGPT&#36827;&#34892;&#35299;&#37322;&#24615;&#30340;&#33258;&#21160;&#23398;&#29983;&#31572;&#26696;&#35780;&#20272;&#30340;&#26032;&#26694;&#26550;&#12290;&#36890;&#36807;&#37319;&#29992;&#19981;&#21516;&#30340;&#27169;&#26495;&#25351;&#23548;ChatGPT&#25910;&#38598;&#29702;&#30001;&#65292;&#20462;&#27491;&#19981;&#19968;&#33268;&#30340;&#29702;&#30001;&#20197;&#31526;&#21512;&#26631;&#20934;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#19968;&#20010;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#35780;&#20272;&#23398;&#29983;&#31572;&#26696;&#21644;&#25552;&#20379;&#29702;&#30001;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#30456;&#23545;&#20110;ChatGPT&#23558;&#25972;&#20307;QWK&#35780;&#20998;&#25552;&#39640;&#20102;11%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#21487;&#35299;&#37322;&#21644;&#21487;&#20449;&#30340;&#21453;&#39304;&#23545;&#20110;&#33258;&#21160;&#23398;&#29983;&#31572;&#26696;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#25506;&#32034;&#20351;&#29992;ChatGPT&#65292;&#19968;&#31181;&#23574;&#31471;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#29983;&#31572;&#26696;&#35780;&#20998;&#21644;&#29702;&#30001;&#29983;&#25104;&#30340;&#24182;&#21457;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#27169;&#26495;&#25552;&#31034;ChatGPT&#25910;&#38598;&#29702;&#30001;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#35828;&#26126;&#65292;&#20854;&#20013;&#19981;&#19968;&#33268;&#30340;&#29702;&#30001;&#34987;&#20462;&#27491;&#20197;&#31526;&#21512;&#26631;&#35760;&#26631;&#20934;&#12290;&#31934;&#32454;&#35843;&#25972;&#30340;ChatGPT&#36755;&#20986;&#20351;&#25105;&#20204;&#33021;&#22815;&#24494;&#35843;&#19968;&#20010;&#26356;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#35780;&#20272;&#23398;&#29983;&#31572;&#26696;&#24182;&#25552;&#20379;&#29702;&#30001;&#12290;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;ChatGPT&#30456;&#27604;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#25972;&#20307;QWK&#35780;&#20998;&#25552;&#39640;&#20102;11%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24443;&#24213;&#20998;&#26512;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#29702;&#30001;&#19982;ChatGPT&#30340;&#29702;&#30001;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#33258;&#21160;&#35780;&#20272;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Providing explainable and faithful feedback is crucial for automated student answer assessment. In this paper, we introduce a novel framework that explores using ChatGPT, a cutting-edge large language model, for the concurrent tasks of student answer scoring and rationale generation. We identify the appropriate instructions by prompting ChatGPT with different templates to collect the rationales, where inconsistent rationales are refined to align with marking standards. The refined ChatGPT outputs enable us to fine-tune a smaller language model that simultaneously assesses student answers and provides rationales. Extensive experiments on the benchmark dataset show that the proposed method improves the overall QWK score by 11% compared to ChatGPT. Furthermore, our thorough analysis and human evaluation demonstrate that the rationales generated by our proposed method are comparable to those of ChatGPT. Our approach provides a viable solution to achieve explainable automated assessment in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#26469;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20219;&#21153;&#21644;&#26041;&#27861;&#26159;&#39537;&#21160;&#30740;&#31350;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.12920</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#33539;&#24335;&#36716;&#21464;&#30340;&#21382;&#26102;&#20998;&#26512;&#65306;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#20026;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Diachronic Analysis of Paradigm Shifts in NLP Research: When, How, and Why?. (arXiv:2305.12920v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#26694;&#26550;&#26469;&#20998;&#26512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#36235;&#21183;&#65292;&#25581;&#31034;&#20102;&#20219;&#21153;&#21644;&#26041;&#27861;&#26159;&#39537;&#21160;&#30740;&#31350;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31185;&#23398;&#39046;&#22495;&#30340;&#22522;&#26412;&#27010;&#24565;&#21644;&#36235;&#21183;&#23545;&#20110;&#36319;&#19978;&#20854;&#25345;&#32493;&#21457;&#23637;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22240;&#26524;&#21457;&#29616;&#21644;&#25512;&#29702;&#25216;&#26415;&#26469;&#20998;&#26512;&#31185;&#23398;&#39046;&#22495;&#20013;&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19977;&#20010;&#21464;&#37327;&#65292;&#20197;&#28085;&#30422;NLP&#30740;&#31350;&#20027;&#39064;&#28436;&#21464;&#30340;&#22810;&#20010;&#26041;&#38754;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#25581;&#31034;&#36825;&#20123;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#32467;&#26500;&#26469;&#27979;&#37327;&#36825;&#20123;&#20851;&#31995;&#30340;&#24378;&#24230;&#12290;&#36890;&#36807;&#22312;ACL Anthology&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#21457;&#29616;&#24191;&#27867;&#30340;NLP&#30740;&#31350;&#20027;&#39064;&#30340;&#28436;&#21464;&#36235;&#21183;&#21644;&#28508;&#22312;&#21407;&#22240;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#20219;&#21153;&#21644;&#26041;&#27861;&#26159;&#25512;&#21160;NLP&#30740;&#31350;&#30340;&#20027;&#35201;&#22240;&#32032;&#65292;&#32780;&#25968;&#25454;&#38598;&#32039;&#38543;&#20854;&#21518;&#65292;&#32780;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the fundamental concepts and trends in a scientific field is crucial for keeping abreast of its continuous advancement. In this study, we propose a systematic framework for analyzing the evolution of research topics in a scientific field using causal discovery and inference techniques. We define three variables to encompass diverse facets of the evolution of research topics within NLP and utilize a causal discovery algorithm to unveil the causal connections among these variables using observational data. Subsequently, we leverage this structure to measure the intensity of these relationships. By conducting extensive experiments on the ACL Anthology corpus, we demonstrate that our framework effectively uncovers evolutionary trends and the underlying causes for a wide range of NLP research topics. Specifically, we show that tasks and methods are primary drivers of research in NLP, with datasets following, while metrics have minimal impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35777;&#26126;&#35299;&#37322;&#22312;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#24615;&#33021;&#24433;&#21709;&#19981;&#26174;&#33879;&#65292;&#20294;&#22312;&#25552;&#31034;&#27169;&#22411;&#26102;&#20351;&#29992;&#35299;&#37322;&#21487;&#25552;&#39640;&#27169;&#22411;&#22312;&#26576;&#20123;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12001</link><description>&lt;p&gt;
OPT-R: &#25506;&#31350;&#35299;&#37322;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#19982;&#25552;&#31034;&#25512;&#29702;&#25216;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models. (arXiv:2305.12001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35777;&#26126;&#35299;&#37322;&#22312;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#24615;&#33021;&#24433;&#21709;&#19981;&#26174;&#33879;&#65292;&#20294;&#22312;&#25552;&#31034;&#27169;&#22411;&#26102;&#20351;&#29992;&#35299;&#37322;&#21487;&#25552;&#39640;&#27169;&#22411;&#22312;&#26576;&#20123;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#20195;&#34920;&#36825;&#31181;&#27169;&#22411;&#30340;Open Pretrained Transformers&#65288;OPT&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#20102;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;OPT&#65292;&#24471;&#21040;&#20102;&#20004;&#32452;&#24494;&#35843;&#27169;&#22411;&#65306;&#27809;&#26377;&#35299;&#37322;&#30340;OPT-R&#21644;&#24102;&#26377;&#35299;&#37322;&#30340;OPT-RE&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#31181;&#25552;&#31034;&#25216;&#26415;&#23545;&#25152;&#26377;&#27169;&#22411;&#22312;&#26469;&#33258;SUPER-NATURAL INSTRUCTIONS&#22522;&#20934;&#27979;&#35797;&#30340;57&#20010;&#22495;&#22806;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#28085;&#30422;26&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#33021;&#12290;&#36890;&#36807;&#19968;&#20010;&#20840;&#38754;&#30340;27&#20010;&#37197;&#32622;&#21644;6,156&#20010;&#27979;&#35797;&#35780;&#20272;&#30697;&#38453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#12289;&#25552;&#31034;&#21644;&#35268;&#27169;&#30340;&#32500;&#24230;&#65292;&#20197;&#20102;&#35299;&#22312;&#19981;&#21516;&#25512;&#29702;&#25216;&#33021;&#26041;&#38754;&#35299;&#37322;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;fewshot&#31034;&#20363;&#20013;&#26377;&#27809;&#26377;&#35299;&#37322;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#22312;&#25552;&#31034;&#27169;&#22411;&#26102;&#20351;&#29992;&#35299;&#37322;&#21487;&#25552;&#39640;&#27169;&#22411;&#22312;&#26576;&#20123;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35814;&#32454;&#38416;&#36848;&#35270;&#20026;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;ElabQUD&#23545;&#20316;&#32773;&#38416;&#36848;&#20449;&#24687;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.10387</link><description>&lt;p&gt;
&#20316;&#20026;&#38544;&#21547;&#35752;&#35770;&#38382;&#39064;&#30340;&#35814;&#32454;&#31616;&#21270;
&lt;/p&gt;
&lt;p&gt;
Elaborative Simplification as Implicit Questions Under Discussion. (arXiv:2305.10387v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35814;&#32454;&#38416;&#36848;&#35270;&#20026;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;ElabQUD&#23545;&#20316;&#32773;&#38416;&#36848;&#20449;&#24687;&#30340;&#26041;&#24335;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#19979;&#20174;&#22797;&#26434;&#21477;&#23376;&#21040;&#31616;&#21270;&#21477;&#23376;&#30340;&#21333;&#35821;&#32763;&#35793;&#24037;&#20316;&#65292;&#26377;&#21161;&#20110;&#20351;&#25991;&#26412;&#26356;&#26131;&#20110;&#35753;&#20799;&#31461;&#21644;&#26032;&#20852;&#21452;&#35821;&#32773;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35266;&#28857;&#24573;&#30053;&#20102;&#35814;&#32454;&#31616;&#21270;&#30340;&#24773;&#20917;&#65292;&#21363;&#22312;&#31616;&#21270;&#25991;&#26412;&#20013;&#28155;&#21152;&#26032;&#20449;&#24687;&#30340;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#35814;&#32454;&#31616;&#21270;&#35270;&#20026;&#35752;&#35770;&#38382;&#39064;&#26694;&#26550;&#65288;QUD&#65289;&#30340;&#19968;&#37096;&#20998;&#65292;&#23558;&#35814;&#32454;&#38416;&#36848;&#30340;&#20449;&#24687;&#35270;&#20026;&#23545;&#38544;&#21547;&#38382;&#39064;&#30340;&#26126;&#30830;&#22238;&#31572;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#30740;&#31350;&#20316;&#32773;&#38416;&#36848;&#21738;&#20123;&#20449;&#24687;&#12289;&#22914;&#20309;&#38416;&#36848;&#20197;&#21450;&#38416;&#36848;&#23558;&#22914;&#20309;&#36866;&#24212;&#35805;&#35821;&#32972;&#26223;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ElabQUD&#65292;&#20854;&#20013;&#21253;&#25324;1.3K&#30340;&#35814;&#32454;&#38416;&#36848;&#21644;&#38544;&#21547;&#30340;QUD&#65292;&#20197;&#30740;&#31350;&#36825;&#20123;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated text simplification, a technique useful for making text more accessible to people such as children and emergent bilinguals, is often thought of as a monolingual translation task from complex sentences to simplified sentences using encoder-decoder models. This view fails to account for elaborative simplification, where new information is added into the simplified text. This paper proposes to view elaborative simplification through the lens of the Question Under Discussion (QUD) framework, providing a robust way to investigate what writers elaborate upon, how they elaborate, and how elaborations fit into the discourse context by viewing elaborations as explicit answers to implicit questions. We introduce ElabQUD, consisting of 1.3K elaborations accompanied with implicit QUDs, to study these phenomena. We show that explicitly modeling QUD (via question generation) not only provides essential understanding of elaborative simplification and how the elaborations connect with the re
&lt;/p&gt;</description></item><item><title>CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09857</link><description>&lt;p&gt;
CoEdIT&#65306;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09857
&lt;/p&gt;
&lt;p&gt;
CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#25110;&#20462;&#35746;&#26159;&#20154;&#31867;&#20889;&#20316;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#21151;&#33021;&#12290;&#29702;&#35299;LLMs&#22312;&#36827;&#34892;&#39640;&#36136;&#37327;&#20462;&#35746;&#21644;&#19982;&#20154;&#31867;&#20889;&#20316;&#32773;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#26500;&#24314;&#26377;&#25928;&#20889;&#20316;&#21161;&#25163;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;LLMs&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#20808;&#21069;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20462;&#35746;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CoEdIT&#65292;&#36825;&#26159;&#19968;&#27454;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#12290;CoEdIT&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#25351;&#20196;&#65292;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#8220;&#20351;&#21477;&#23376;&#26356;&#31616;&#21333;&#8221;&#25110;&#8220;&#20197;&#26356;&#20013;&#31435;&#30340;&#39118;&#26684;&#20889;&#20316;&#8221;&#65292;&#24182;&#36755;&#20986;&#32534;&#36753;&#21518;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;1&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#65288;2&#65289;&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text editing or revision is an essential function of the human writing process. Understanding the capabilities of LLMs for making high-quality revisions and collaborating with human writers is a critical step toward building effective writing assistants. With the prior success of LLMs and instruction tuning, we leverage instruction-tuned LLMs for text revision to improve the quality of user-generated text and improve the efficiency of the process. We introduce CoEdIT, a state-of-the-art text editing model for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as "Make the sentence simpler" or "Write it in a more neutral style," and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.01278</link><description>&lt;p&gt;
&#27178;&#21521;&#36801;&#31227;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#22312;VL-LLMs&#20043;&#38388;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Transfer Visual Prompt Generator across LLMs. (arXiv:2305.01278v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01278
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#23558;&#24050;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#36830;&#25509;&#21040;&#35270;&#35273;-&#35821;&#35328;LLM&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;VPGTrans&#65292;&#35813;&#26041;&#26696;&#22312;VQA&#21644;NLVR2&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21033;&#29992;&#29616;&#26377;&#30340;&#36731;&#37327;&#21270;&#35270;&#35273;&#25552;&#31034;&#21457;&#29983;&#22120;&#65288;VPG&#65289;&#36830;&#25509;&#24050;&#26377;&#30340;&#35270;&#35273;-&#35821;&#35328;LLM&#65288;VL-LLM&#65289;&#20197;&#20943;&#23569;&#36164;&#28304;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#36328;&#19981;&#21516;&#22823;&#23567;&#21644;&#31867;&#22411;&#30340;LLMs&#30340;VPG&#36716;&#31227;&#26041;&#26696;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;VPGTrans&#30340;&#20004;&#38454;&#27573;&#36716;&#31227;&#26694;&#26550;&#65292;&#23427;&#22312;VQA&#21644;NLVR2&#20004;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#36716;&#31227;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
While developing a new vision-language LLM (VL-LLM) by pre-training on tremendous image-text pairs from scratch can be exceedingly resource-consuming, connecting an existing LLM with a comparatively lightweight visual prompt generator (VPG) becomes a feasible paradigm. However, further tuning the VPG part of the VL-LLM still suffers from indispensable computational costs, i.e., requiring thousands of GPU hours and millions of training data. One alternative solution is to transfer an existing VPG from any existing VL-LLMs for the target VL-LLM.  In this work, we for the first time investigate the VPG transferability across LLMs, and explore a solution to reduce the cost of VPG transfer. We first study the VPG transfer across different LLM sizes (e.g., small-to-large), and across different LLM types, through which we diagnose the key factors to maximize the transfer efficiency. Based on our observation, we design a two-stage transfer framework named VPGTrans, which is simple yet highly e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00586</link><description>&lt;p&gt;
GPT-2&#26159;&#22914;&#20309;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#30340;&#65311;&#35299;&#37322;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
How does GPT-2 compute greater-than?: Interpreting mathematical abilities in a pre-trained language model. (arXiv:2305.00586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36816;&#29992;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#20102;GPT-2 Small&#30340;&#25968;&#23398;&#33021;&#21147;&#65292;&#24182;&#30830;&#23450;&#20102;&#23427;&#30340;&#35745;&#31639;&#22270;&#20013;&#30340;&#19968;&#20010;&#23567;&#30005;&#36335;&#29992;&#20110;&#35745;&#31639;&#22823;&#20110;&#31526;&#21495;&#65292;&#35813;&#30005;&#36335;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#65292;&#24182;&#19988;&#35813;&#30005;&#36335;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#34987;&#26126;&#30830;&#35757;&#32451;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#21151;&#33021;&#21364;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#36890;&#36807;&#26426;&#26800;&#24335;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20855;&#26377;&#30340;&#22522;&#26412;&#25968;&#23398;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20197;GPT-2 Small&#20026;&#20363;&#65292;&#30740;&#31350;&#20854;&#33021;&#21542;&#36890;&#36807;&#36755;&#20837;"&#25112;&#20105;&#25345;&#32493;&#26102;&#38388;&#26159;&#20174;1732&#24180;&#21040;17&#24180;"&#65292;&#39044;&#27979;&#20986;&#26377;&#25928;&#30340;&#20004;&#20301;&#25968;&#23383;&#30340;&#25130;&#27490;&#24180;&#20221; (&#22823;&#20110;32&#24180;)&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#19968;&#20010;&#30005;&#36335;&#65292;&#21363;GPT-2 Small&#35745;&#31639;&#22270;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#65292;&#29992;&#20110;&#35745;&#31639;&#36825;&#20010;&#20219;&#21153;&#30340;&#36755;&#20986;&#65292;&#28982;&#21518;&#25105;&#20204;&#35299;&#37322;&#20102;&#27599;&#20010;&#30005;&#36335;&#32452;&#20214;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;GPT-2 Small&#30340;&#26368;&#32456;&#22810;&#23618;&#24863;&#30693;&#22120;&#25552;&#39640;&#20102;&#32467;&#26463;&#24180;&#20221;&#22823;&#20110;&#24320;&#22987;&#24180;&#20221;&#30340;&#27010;&#29575;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30005;&#36335;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#22312;&#20854;&#20182;&#22823;&#20110;&#22330;&#26223;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models can be surprisingly adept at tasks they were not explicitly trained on, but how they implement these capabilities is poorly understood. In this paper, we investigate the basic mathematical abilities often acquired by pre-trained language models. Concretely, we use mechanistic interpretability techniques to explain the (limited) mathematical abilities of GPT-2 small. As a case study, we examine its ability to take in sentences such as "The war lasted from the year 1732 to the year 17", and predict valid two-digit end years (years &gt; 32). We first identify a circuit, a small subset of GPT-2 small's computational graph that computes this task's output. Then, we explain the role of each circuit component, showing that GPT-2 small's final multi-layer perceptrons boost the probability of end years greater than the start year. Finally, we show that our circuit generalizes to other tasks, playing a role in other greater-than scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#21709;&#24212;&#27969;&#30021;&#20294;&#20165;&#26377;51.5%&#30340;&#29983;&#25104;&#21477;&#23376;&#24471;&#21040;&#20102;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65292;&#20165;&#26377;74.5%&#30340;&#24341;&#29992;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#12290;</title><link>http://arxiv.org/abs/2304.09848</link><description>&lt;p&gt;
&#35780;&#20272;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#20013;&#30340;&#21487;&#39564;&#35777;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Verifiability in Generative Search Engines. (arXiv:2304.09848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22235;&#20010;&#27969;&#34892;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#21487;&#39564;&#35777;&#24615;&#65292;&#21457;&#29616;&#29616;&#26377;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#21709;&#24212;&#27969;&#30021;&#20294;&#20165;&#26377;51.5%&#30340;&#29983;&#25104;&#21477;&#23376;&#24471;&#21040;&#20102;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65292;&#20165;&#26377;74.5%&#30340;&#24341;&#29992;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30452;&#25509;&#20026;&#29992;&#25143;&#26597;&#35810;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#25552;&#20379;&#20869;&#32852;&#24341;&#29992;&#12290;&#19968;&#20010;&#20540;&#24471;&#20449;&#36182;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#30340;&#20808;&#20915;&#26465;&#20214;&#26159;&#21487;&#39564;&#35777;&#24615;&#65292;&#21363;&#31995;&#32479;&#24212;&#20840;&#38754;&#24341;&#29992;&#65288;&#39640;&#24341;&#29992;&#22238;&#24518;&#29575;&#65292;&#25152;&#26377;&#35821;&#21477;&#37117;&#26377;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65289;&#21644;&#20934;&#30830;&#65288;&#39640;&#24341;&#29992;&#31934;&#24230;&#65292;&#27599;&#20010;&#24341;&#29992;&#37117;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#65289;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#27969;&#34892;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#8212;&#8212;Bing Chat&#12289;NeevaAI&#12289;perplexity.ai&#21644;YouChat&#8212;&#8212;&#36827;&#34892;&#20102;&#20154;&#31867;&#35780;&#20272;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#26469;&#28304;&#30340;&#22810;&#26679;&#21270;&#26597;&#35810;&#65288;&#20363;&#22914;&#21382;&#21490;&#19978;&#30340;Google&#29992;&#25143;&#26597;&#35810;&#12289;Reddit&#19978;&#21160;&#24577;&#25910;&#38598;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#31561;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#29983;&#25104;&#24335;&#25628;&#32034;&#24341;&#25806;&#21709;&#24212;&#27969;&#30021;&#19988;&#20449;&#24687;&#20016;&#23500;&#65292;&#20294;&#24120;&#24120;&#21253;&#21547;&#19981;&#25903;&#25345;&#30340;&#35821;&#21477;&#21644;&#19981;&#20934;&#30830;&#30340;&#24341;&#29992;&#65306;&#24179;&#22343;&#32780;&#35328;&#65292;&#20165;&#26377;51.5%&#30340;&#29983;&#25104;&#21477;&#23376;&#24471;&#21040;&#20102;&#23436;&#25972;&#30340;&#24341;&#29992;&#25903;&#25345;&#65292;&#21482;&#26377;74.5%&#30340;&#24341;&#29992;&#25903;&#25345;&#20854;&#30456;&#20851;&#35821;&#21477;&#12290;&#25105;&#20204;&#35748;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Generative search engines directly generate responses to user queries, along with in-line citations. A prerequisite trait of a trustworthy generative search engine is verifiability, i.e., systems should cite comprehensively (high citation recall; all statements are fully supported by citations) and accurately (high citation precision; every cite supports its associated statement). We conduct human evaluation to audit four popular generative search engines -- Bing Chat, NeevaAI, perplexity.ai, and YouChat -- across a diverse set of queries from a variety of sources (e.g., historical Google user queries, dynamically-collected open-ended questions on Reddit, etc.). We find that responses from existing generative search engines are fluent and appear informative, but frequently contain unsupported statements and inaccurate citations: on average, a mere 51.5% of generated sentences are fully supported by citations and only 74.5% of citations support their associated sentence. We believe that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.02210</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Chat-GPT&#21487;&#20197;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#29983;&#25104;&#36830;&#36143;&#65292;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#27969;&#30021;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#25552;&#20379;&#20102;LLMs&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20851;&#27880;&#19977;&#20010;&#26041;&#38754;&#65306;1&#65289;&#35821;&#31687;&#24863;&#30693;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#25552;&#31034;&#23545;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#21644;&#35821;&#31687;&#29616;&#35937;&#30340;&#24433;&#21709;&#65307;2&#65289;&#32763;&#35793;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#27604;&#36739;Chat-GPT&#19982;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#30340;&#32763;&#35793;&#24615;&#33021;&#65307;3&#65289;&#35821;&#31687;&#24314;&#27169;&#33021;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#20013;&#32534;&#30721;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#24182;&#30740;&#31350;&#22521;&#35757;&#25216;&#26415;&#23545;&#35821;&#31687;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35780;&#20272;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;1&#65289;&#21033;&#29992;&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#65292;ChatGPT&#22312;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#65307;2&#65289;&#20462;&#25913;&#26126;&#30830;&#38024;&#23545;&#35821;&#31687;&#29616;&#35937;&#30340;&#25552;&#31034;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65307;3&#65289;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#22521;&#35757;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09752</link><description>&lt;p&gt;
CoLT5: &#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;&#24555;&#36895;&#38271;&#36317;&#31163;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09752
&lt;/p&gt;
&lt;p&gt;
CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#20294;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#25991;&#26723;&#24456;&#26114;&#36149;&#8212;&#8212;&#36825;&#19981;&#20165;&#26159;&#22240;&#20026;&#20108;&#27425;&#27880;&#24847;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#21069;&#39304;&#21644;&#25237;&#24433;&#23618;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#26631;&#35760;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoLT5&#65292;&#19968;&#31181;&#38271;&#36755;&#20837;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#35745;&#31639;&#26469;&#21033;&#29992;&#27492;&#30452;&#35273;&#65292;&#22312;&#21069;&#39304;&#21644;&#27880;&#24847;&#23618;&#20013;&#20026;&#37325;&#35201;&#26631;&#35760;&#25552;&#20379;&#26356;&#22810;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoLT5&#27604;LongT5&#34920;&#29616;&#26356;&#24378;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26356;&#24555;&#65292;&#22312;&#38271;&#36755;&#20837;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;SOTA&#12290;&#27492;&#22806;&#65292;CoLT5&#33021;&#22815;&#26377;&#25928;&#19988;&#21487;&#25511;&#22320;&#21033;&#29992;&#26497;&#38271;&#30340;&#36755;&#20837;&#65292;&#23637;&#31034;&#20102;&#39640;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#30340;&#24378;&#22823;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2303.06273</link><description>&lt;p&gt;
ChatGPT&#30340;&#19968;&#33268;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Consistency Analysis of ChatGPT. (arXiv:2303.06273v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;ChatGPT&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#21457;&#29616;&#23613;&#31649;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the consistency issue of ChatGPT and finds that although it has improved language understanding ability, it frequently fails to generate logically correct predictions. Therefore, further consideration is needed for its real-world applications, especially in terms of risk.
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#31572;&#23545;&#35805;&#31995;&#32479;&#65292;&#33258;&#25512;&#20986;&#20197;&#26469;&#24191;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#23427;&#22312;&#27861;&#24459;&#12289;&#21307;&#23398;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#30340;&#19987;&#19994;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#65292;&#20294;&#20063;&#26377;&#20154;&#23545;&#20854;&#21487;&#38752;&#24615;&#21644;&#20449;&#20219;&#24230;&#34920;&#31034;&#24576;&#30097;&#12290;&#26412;&#25991;&#38024;&#23545;ChatGPT&#22312;&#36923;&#36753;&#19968;&#33268;&#24615;&#26041;&#38754;&#30340;&#21487;&#20449;&#24230;&#36827;&#34892;&#20102;&#35843;&#26597;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23613;&#31649;ChatGPT&#20284;&#20046;&#20855;&#26377;&#26356;&#22909;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20173;&#28982;&#32463;&#24120;&#26080;&#27861;&#29983;&#25104;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;ChatGPT&#26159;&#19968;&#31181;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#21644;&#26377;&#21069;&#36884;&#30340;&#26032;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#22914;&#26524;&#27809;&#26377;&#32463;&#36807;&#24443;&#24213;&#30340;&#20154;&#24037;&#26816;&#26597;&#65292;&#23427;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#38656;&#35201;&#36827;&#19968;&#27493;&#32771;&#34385;&#65292;&#29305;&#21035;&#26159;&#22312;&#39118;&#38505;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT, a question-and-answer dialogue system based on a large language model, has gained huge popularity since its introduction. Its positive aspects have been reported through many media platforms, and some analyses even showed that ChatGPT achieved a decent grade in professional exams, including the law, medical, and finance domains, adding extra support to the claim that AI now can assist and, even, replace humans in industrial fields. Others, however, doubt its reliability and trustworthiness. In this paper, we investigate ChatGPT's trustworthiness regarding logically consistent behaviours. Our findings suggest that, although ChatGPT seems to achieve an improved language understanding ability, it still fails to generate logically correct predictions frequently. Hence, while it is true that ChatGPT is an impressive and promising new technique, we conclude that its usage in real-world applications without thorough human inspection requires further consideration, especially for risk
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2303.04048</link><description>&lt;p&gt;
ChatGPT&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#35780;&#20215;&#25351;&#26631;&#21487;&#38752;&#21527;&#65311;&#21021;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Is ChatGPT a Good NLG Evaluator? A Preliminary Study. (arXiv:2303.04048v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04048
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#21644;&#26041;&#38754;&#29305;&#23450;&#65292;&#25105;&#20204;&#22312;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#34920;&#26126;ChatGPT&#20316;&#20026;NLG&#35780;&#20272;&#25351;&#26631;&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#65292;&#23588;&#20854;&#26159;&#22312;&#27969;&#30021;&#24230;&#26041;&#38754;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#30340;&#20986;&#29616;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#20197;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#20026;&#22522;&#30784;&#33719;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ChatGPT&#20316;&#20026;&#19968;&#31181;&#35780;&#20272;&#25351;&#26631;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#32771;&#34385;&#21040;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#27169;&#22411;&#30340;&#36136;&#37327;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#24182;&#19988;NLG&#25351;&#26631;&#20197;&#20854;&#31967;&#31957;&#30340;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#32780;&#38395;&#21517;&#65292;&#22240;&#27492;&#25105;&#20204;&#26159;&#21542;&#20250;&#35748;&#20026;ChatGPT&#26159;&#19968;&#20010;&#22909;&#30340;NLG&#35780;&#20272;&#25351;&#26631;&#12290;&#22312;&#36825;&#31687;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#23545;ChatGPT&#36827;&#34892;&#20102;&#21021;&#27493;&#30340;&#20803;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;ChatGPT&#20316;&#20026;NLG&#25351;&#26631;&#30340;&#21487;&#38752;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;ChatGPT&#35270;&#20026;&#20154;&#31867;&#35780;&#20272;&#22120;&#65292;&#24182;&#38024;&#23545;&#20219;&#21153;&#29305;&#23450;&#65288;&#20363;&#22914;&#25688;&#35201;&#65289;&#21644;&#26041;&#38754;&#29305;&#23450;&#65288;&#20363;&#22914;&#30456;&#20851;&#24615;&#65289;&#36827;&#34892;&#35828;&#26126;&#65292;&#20197;&#20419;&#20351;ChatGPT&#35780;&#20272;NLG&#27169;&#22411;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#25688;&#35201;&#12289;&#25925;&#20107;&#29983;&#25104;&#21644;&#32763;&#35793;&#22312;&#20869;&#30340;&#20116;&#20010;NLG&#20803;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#26576;&#20123;&#26041;&#38754;&#65288;&#20363;&#22914;&#27969;&#30021;&#24230;&#65289;&#65292;ChatGPT&#24182;&#19981;&#24635;&#26159;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#19968;&#33268;&#12290;&#36825;&#25552;&#37266;&#20154;&#20204;&#22312;&#20351;&#29992;ChatGPT&#20316;&#20026;&#21807;&#19968;&#30340;&#33258;&#21160;NLG&#35780;&#20272;&#25351;&#26631;&#26102;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and 
&lt;/p&gt;</description></item><item><title>CoSyn&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.03387</link><description>&lt;p&gt;
CoSyn&#65306;&#20351;&#29992;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#21452;&#26354;&#32447;&#32593;&#32476;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;
&lt;/p&gt;
&lt;p&gt;
CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network. (arXiv:2303.03387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03387
&lt;/p&gt;
&lt;p&gt;
CoSyn&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#32534;&#30721;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#23545;&#35805;&#20013;&#38544;&#21547;&#30340;&#20167;&#24680;&#35328;&#35770;&#23545;&#26469;&#33258;&#21508;&#20010;&#32676;&#20307;&#30340;&#20154;&#20204;&#20135;&#29983;&#20102;&#37325;&#35201;&#24433;&#21709;&#65292;&#22240;&#27492;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#36234;&#26469;&#36234;&#22810;&#12290;&#22823;&#37096;&#20998;&#20043;&#21069;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#20110;&#26816;&#27979;&#26126;&#30830;&#30340;&#20167;&#24680;&#35328;&#35770;&#65292;&#36825;&#20123;&#35328;&#35770;&#26126;&#26174;&#19988;&#21033;&#29992;&#20102;&#20167;&#24680;&#30701;&#35821;&#65292;&#23545;&#20110;&#26816;&#27979;&#38544;&#21547;&#25110;&#36890;&#36807;&#38388;&#25509;&#25110;&#32534;&#30721;&#35821;&#35328;&#34920;&#36798;&#20986;&#30340;&#20167;&#24680;&#35328;&#35770;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoSyn&#65292;&#19968;&#20010;&#19978;&#19979;&#25991;&#21327;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26126;&#30830;&#22320;&#32467;&#21512;&#20102;&#29992;&#25143;&#21644;&#23545;&#35805;&#19978;&#19979;&#25991;&#26469;&#26816;&#27979;&#22312;&#32447;&#23545;&#35805;&#20013;&#30340;&#38544;&#21547;&#20167;&#24680;&#35328;&#35770;&#12290;CoSyn&#24341;&#20837;&#20102;&#26032;&#30340;&#26041;&#27861;&#26469;&#32534;&#30721;&#36825;&#20123;&#22806;&#37096;&#19978;&#19979;&#25991;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19978;&#19979;&#25991;&#20132;&#20114;&#26426;&#21046;&#65292;&#28165;&#26224;&#22320;&#25429;&#25417;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29420;&#31435;&#35780;&#20272;&#20102;&#20174;&#36825;&#20123;&#22024;&#26434;&#30340;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#30340;&#20449;&#24687;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#36827;&#34892;&#25152;&#26377;&#36825;&#20123;&#25805;&#20316;&#65292;&#20197;&#36866;&#24212;&#31038;&#20132;&#23186;&#20307;&#30340;&#26080;&#26631;&#24230;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
The tremendous growth of social media users interacting in online conversations has led to significant growth in hate speech, affecting people from various demographics. Most of the prior works focus on detecting explicit hate speech, which is overt and leverages hateful phrases, with very little work focusing on detecting hate speech that is implicit or denotes hatred through indirect or coded language. In this paper, we present CoSyn, a context-synergized neural network that explicitly incorporates user- and conversational context for detecting implicit hate speech in online conversations. CoSyn introduces novel ways to encode these external contexts and employs a novel context interaction mechanism that clearly captures the interplay between them, making independent assessments of the amounts of information to be retrieved from these noisy contexts. Additionally, it carries out all these operations in the hyperbolic space to account for the scale-free dynamics of social media. We de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2302.14229</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#25191;&#34892;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#65292;&#24182;&#25104;&#21151;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#20854;&#20013;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#19968;&#20010;&#28304;&#35821;&#35328;&#25991;&#26412;&#65292;&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;CLS&#65289;&#26088;&#22312;&#29983;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#27604;&#22914;GPT-3.5&#12289;ChatGPT&#21644;GPT-4&#65292;&#24341;&#36215;&#20102;&#35745;&#31639;&#35821;&#35328;&#23398;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLM&#22312;CLS&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;&#12290;&#26412;&#25991;&#23454;&#39564;&#24615;&#22320;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#26469;&#25351;&#23548;LLM&#20174;&#19981;&#21516;&#30340;&#33539;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;&#21644;&#27969;&#27700;&#32447;&#65289;&#25191;&#34892;&#38646;&#26679;&#26412;CLS&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#25688;&#35201;&#36827;&#34892;&#21021;&#27493;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#21644;GPT-4&#21407;&#26412;&#26356;&#21916;&#27426;&#29983;&#25104;&#35814;&#32454;&#20449;&#24687;&#30340;&#38271;&#25688;&#35201;&#12290;&#20294;&#36825;&#20004;&#20010;LLM&#22312;&#20132;&#20114;&#24335;&#25552;&#31034;&#30340;&#24110;&#21161;&#19979;&#21487;&#20197;&#36827;&#19968;&#27493;&#24179;&#34913;&#20449;&#24687;&#37327;&#21644;&#31616;&#27905;&#24615;&#65292;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;CLS&#24615;&#33021;&#12290;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;CLS&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;CLS&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#26041;&#38754;&#19982;&#26368;&#20339;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a document in a source language, cross-lingual summarization (CLS) aims to generate a summary in a different target language. Recently, the emergence of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has attracted wide attention from the computational linguistics community. However, it is not yet known the performance of LLMs on CLS. In this report, we empirically use various prompts to guide LLMs to perform zero-shot CLS from different paradigms (i.e., end-to-end and pipeline), and provide a preliminary evaluation on the generated summaries. We find that ChatGPT and GPT-4 originally prefer to produce lengthy summaries with detailed information. These two LLMs can further balance informativeness and conciseness with the help of an interactive prompt, significantly improving their CLS performance. Experimental results on three widely-used CLS datasets show that GPT-4 achieves state-of-the-art zero-shot CLS performance, and performs competitively compared with th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2302.03169</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Data Selection for Language Models via Importance Resampling. (arXiv:2302.03169v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03169
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#36873;&#25321;&#26694;&#26550;&#65288;DSIR&#65289;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;KL&#20943;&#23569;&#20316;&#20026;&#25968;&#25454;&#24230;&#37327;&#26469;&#30830;&#23450;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#24182;&#22312;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#20197;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#23545;&#20110;&#36890;&#29992;&#39046;&#22495;&#65288;&#22914;GPT-3&#65289;&#21644;&#29305;&#23450;&#39046;&#22495;&#65288;&#22914;Codex&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20174;&#22823;&#22411;&#21407;&#22987;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19968;&#20010;&#23376;&#38598;&#65292;&#20197;&#21305;&#37197;&#32473;&#23450;&#19968;&#20123;&#26080;&#26631;&#31614;&#30446;&#26631;&#26679;&#26412;&#30340;&#25152;&#38656;&#30446;&#26631;&#20998;&#24067;&#12290;&#37492;&#20110;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#24230;&#65292;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#19987;&#23478;&#25163;&#21160;&#31574;&#21010;&#25968;&#25454;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#22312;LM&#25968;&#25454;&#36873;&#25321;&#20013;&#20351;&#29992;&#30340;&#32463;&#20856;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#26041;&#27861;&#65292;&#20197;&#20302;&#32500;&#31354;&#38388;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#25968;&#25454;&#36873;&#25321;&#19982;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#65288;DSIR&#65289;&#65292;&#23427;&#22312;&#19968;&#20010;&#38477;&#32500;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#37325;&#35201;&#24615;&#26435;&#37325;&#65292;&#20197;&#20415;&#26681;&#25454;&#36825;&#20123;&#26435;&#37325;&#36827;&#34892;&#37325;&#35201;&#24615;&#37325;&#37319;&#26679;&#25968;&#25454;&#36873;&#25321;&#12290;&#20026;&#20102;&#30830;&#23450;&#19968;&#20010;&#21512;&#36866;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;KL&#20943;&#23569;&#65292;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#34913;&#37327;&#25152;&#36873;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#30446;&#26631;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#25968;&#25454;&#24230;&#37327;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting a suitable pretraining dataset is crucial for both general-domain (e.g., GPT-3) and domain-specific (e.g., Codex) language models (LMs). We formalize this problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics or use experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. We propose Data Selection with Importance Resampling (DSIR), an efficient and scalable framework that estimates importance weights in a reduced feature space for tractability and selects data with importance resampling according to these weights. To determine an appropriate feature space, we show that KL reduction, a data metric that measures the proximity between selected pretraining data and the target in a feature space, has high correlat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#34920;&#29616;&#21147;&#24378;&#19988;&#33021;&#22788;&#29702;&#38271;&#26399;&#32467;&#26500;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;Mo^usai&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22810;&#20998;&#38047;&#39640;&#36136;&#37327;&#38899;&#20048;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26631;&#20934;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2301.11757</link><description>&lt;p&gt;
Mo^usai: &#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#28508;&#22312;&#25193;&#25955;&#36827;&#34892;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mo\^usai: Text-to-Music Generation with Long-Context Latent Diffusion. (arXiv:2301.11757v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#34920;&#29616;&#21147;&#24378;&#19988;&#33021;&#22788;&#29702;&#38271;&#26399;&#32467;&#26500;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;Mo^usai&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22810;&#20998;&#38047;&#39640;&#36136;&#37327;&#38899;&#20048;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#22312;&#22810;&#20010;&#26631;&#20934;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#22312;&#25991;&#26412;&#39046;&#22495;&#21462;&#24471;&#20102;&#24555;&#36895;&#21457;&#23637;&#65307;&#28982;&#32780;&#65292;&#23545;&#20110;&#25991;&#26412;&#19982;&#21478;&#19968;&#31181;&#8220;&#35821;&#35328;&#8221;&#8212;&#8212;&#38899;&#20048;&#30340;&#20851;&#32852;&#20851;&#31995;&#65292;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#38899;&#20048;&#19982;&#25991;&#26412;&#19968;&#26679;&#65292;&#21487;&#20197;&#20256;&#36798;&#24773;&#24863;&#12289;&#25925;&#20107;&#21644;&#24605;&#24819;&#65292;&#20855;&#26377;&#33258;&#24049;&#29420;&#29305;&#30340;&#32467;&#26500;&#21644;&#35821;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#19968;&#31181;&#39640;&#25928;&#12289;&#34920;&#29616;&#21147;&#24378;&#19988;&#33021;&#22815;&#22788;&#29702;&#38271;&#26399;&#32467;&#26500;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#19982;&#38899;&#20048;&#32852;&#31995;&#36215;&#26469;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Mo^usai&#65292;&#36825;&#26159;&#19968;&#20010;&#32423;&#32852;&#30340;&#20004;&#38454;&#27573;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#22810;&#20998;&#38047;&#30340;&#39640;&#36136;&#37327;48kHz&#31435;&#20307;&#22768;&#38899;&#20048;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#28040;&#36153;&#32423;GPU&#19978;&#36827;&#34892;&#23454;&#26102;&#25512;&#26029;&#65292;&#24182;&#20855;&#26377;&#21512;&#29702;&#30340;&#36895;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#22312;&#22810;&#31181;&#26631;&#20934;&#19979;&#30456;&#23545;&#20110;&#29616;&#26377;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#25512;&#21160;&#24320;&#28304;&#25991;&#21270;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#22871;&#24320;&#28304;&#30340;&#24037;&#20855;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another "language" of communication -- music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Mo\^usai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model's competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source
&lt;/p&gt;</description></item><item><title>&#25209;&#37327;&#25552;&#31034;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#30340;&#35745;&#31639;&#21644;&#36130;&#21153;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#19979;&#28216;&#24615;&#33021;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25209;&#37327;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#20960;&#20046;&#20197;&#20498;&#25968;&#32447;&#24615;&#20851;&#31995;&#38477;&#20302;&#20102;&#25512;&#26029;&#25104;&#26412;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;Chat-based LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#25209;&#37327;&#25552;&#31034;&#20063;&#20855;&#26377;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2301.08721</link><description>&lt;p&gt;
&#25209;&#37327;&#25552;&#31034;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;API&#36827;&#34892;&#39640;&#25928;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Batch Prompting: Efficient Inference with Large Language Model APIs. (arXiv:2301.08721v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08721
&lt;/p&gt;
&lt;p&gt;
&#25209;&#37327;&#25552;&#31034;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25512;&#26029;&#30340;&#35745;&#31639;&#21644;&#36130;&#21153;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#19979;&#28216;&#24615;&#33021;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#25209;&#37327;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#20960;&#20046;&#20197;&#20498;&#25968;&#32447;&#24615;&#20851;&#31995;&#38477;&#20302;&#20102;&#25512;&#26029;&#25104;&#26412;&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#39564;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;Chat-based LLMs&#65292;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#25209;&#37327;&#25552;&#31034;&#20063;&#20855;&#26377;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#21644;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#22823;&#37327;&#26679;&#26412;&#30340;&#25512;&#26029;&#21487;&#33021;&#20250;&#22312;&#35745;&#31639;&#21644;&#36130;&#21153;&#19978;&#20195;&#20215;&#39640;&#26114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;LLM&#33021;&#22815;&#25209;&#37327;&#36827;&#34892;&#25512;&#26029;&#65292;&#32780;&#19981;&#26159;&#36880;&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20943;&#23569;&#20102;&#20196;&#29260;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#23569;&#26679;&#26412;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#30528;&#27599;&#25209;&#26679;&#26412;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#25512;&#26029;&#25104;&#26412;&#20960;&#20046;&#20197;&#20498;&#25968;&#32447;&#24615;&#20851;&#31995;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24120;&#35782;&#38382;&#31572;&#12289;&#31639;&#26415;&#25512;&#29702;&#21644;NLI/NLU&#31561;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#24191;&#27867;&#39564;&#35777;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65306;&#25209;&#37327;&#25552;&#31034;&#26174;&#33879;&#65288;&#27599;&#25209;&#20845;&#20010;&#26679;&#26412;&#26102;&#26368;&#39640;&#21487;&#20943;&#23569;5&#20493;&#65289;&#38477;&#20302;&#20102;LLM&#65288;Codex&#65289;&#30340;&#25512;&#26029;&#20196;&#29260;&#21644;&#26102;&#38388;&#25104;&#26412;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#22909;&#25110;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32842;&#22825;&#30340;LLM&#65292;&#20363;&#22914;GPT-3.5&#21644;GPT-4&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25209;&#37327;&#25552;&#31034;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Performing inference on large volumes of samples with large language models (LLMs) can be computationally and financially costly in industry and real-world use. We propose batch prompting, a simple yet effective prompting approach that enables the LLM to run inference in batches, instead of one sample at a time. Our method reduces both token and time costs while retaining downstream performance. We theoretically demonstrate that under a few-shot in-context learning setting, the inference costs decrease almost inverse linearly with the number of samples in each batch. We extensively validate the effectiveness of batch prompting on ten datasets across commonsense QA, arithmetic reasoning, and NLI/NLU: batch prompting significantly~(up to 5x with six samples in batch) reduces the LLM (Codex) inference token and time costs while achieving better or comparable performance. For state-of-the-art Chat-based LLMs, e.g., GPT-3.5 and GPT-4, we show the benefits of batch prompting also hold. Furth
&lt;/p&gt;</description></item><item><title>T-Projection&#26159;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#12290;&#23427;&#23558;&#26631;&#27880;&#25237;&#24433;&#20219;&#21153;&#20998;&#35299;&#20026;&#20505;&#36873;&#29983;&#25104;&#21644;&#20505;&#36873;&#36873;&#25321;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24207;&#21015;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2212.10548</link><description>&lt;p&gt;
T-Projection&#65306;&#39640;&#36136;&#37327;&#30340;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#30340;&#27880;&#37322;&#25237;&#24433;
&lt;/p&gt;
&lt;p&gt;
T-Projection: High Quality Annotation Projection for Sequence Labeling Tasks. (arXiv:2212.10548v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10548
&lt;/p&gt;
&lt;p&gt;
T-Projection&#26159;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#25237;&#24433;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#12290;&#23427;&#23558;&#26631;&#27880;&#25237;&#24433;&#20219;&#21153;&#20998;&#35299;&#20026;&#20505;&#36873;&#29983;&#25104;&#21644;&#20505;&#36873;&#36873;&#25321;&#20004;&#20010;&#23376;&#20219;&#21153;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24207;&#21015;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32570;&#23569;&#29305;&#23450;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#26631;&#27880;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#27880;&#37322;&#25237;&#24433;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#21487;&#33021;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#27880;&#37322;&#25968;&#25454;&#12290;&#27880;&#37322;&#25237;&#24433;&#36890;&#24120;&#34987;&#24418;&#24335;&#21270;&#20026;&#22312;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#23558;&#28304;&#35821;&#35328;&#20013;&#32473;&#23450;&#36328;&#24230;&#30340;&#26631;&#27880;&#20256;&#36755;&#21040;&#30446;&#26631;&#35821;&#35328;&#20013;&#30456;&#24212;&#30340;&#36328;&#24230;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;T-Projection&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#21644;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#32763;&#35793;&#25216;&#26415;&#23454;&#29616;&#27880;&#37322;&#25237;&#24433;&#30340;&#26032;&#26041;&#27861;&#12290;T-Projection&#23558;&#26631;&#27880;&#25237;&#24433;&#20219;&#21153;&#20998;&#35299;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(i)&#20505;&#36873;&#29983;&#25104;&#27493;&#39588;&#65292;&#20351;&#29992;&#22810;&#35821;&#35328;T5&#27169;&#22411;&#29983;&#25104;&#19968;&#32452;&#25237;&#24433;&#20505;&#36873;&#65292;(ii)&#20505;&#36873;&#36873;&#25321;&#27493;&#39588;&#65292;&#26681;&#25454;&#32763;&#35793;&#27010;&#29575;&#23545;&#29983;&#25104;&#30340;&#20505;&#36873;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#22312;5&#20010;&#21360;&#27431;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#20869;&#22312;&#21644;&#22806;&#22312;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the absence of readily available labeled data for a given sequence labeling task and language, annotation projection has been proposed as one of the possible strategies to automatically generate annotated data. Annotation projection has often been formulated as the task of transporting, on parallel corpora, the labels pertaining to a given span in the source language into its corresponding span in the target language. In this paper we present T-Projection, a novel approach for annotation projection that leverages large pretrained text-to-text language models and state-of-the-art machine translation technology. T-Projection decomposes the label projection task into two subtasks: (i) A candidate generation step, in which a set of projection candidates using a multilingual T5 model is generated and, (ii) a candidate selection step, in which the generated candidates are ranked based on translation probabilities. We conducted experiments on intrinsic and extrinsic tasks in 5 Indo-Europea
&lt;/p&gt;</description></item><item><title>SODA&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#30334;&#19975;&#32423;&#21035;&#39640;&#36136;&#37327;&#31038;&#20132;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#19978;&#19979;&#25991;&#21270;&#31038;&#20132;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#20102;COSMO&#65292;&#20854;&#27604;&#30446;&#21069;&#26368;&#20339;&#34920;&#29616;&#30340;&#23545;&#35805;&#27169;&#22411;&#26356;&#20026;&#33258;&#28982;&#21644;&#19968;&#33268;&#65292;&#36825;&#26377;&#21161;&#20110;&#20102;&#35299;&#30693;&#35782;&#20016;&#23500;&#22411;&#23545;&#35805;&#21644;&#33258;&#28982;&#31038;&#20132;&#38386;&#32842;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2212.10465</link><description>&lt;p&gt;
SODA: &#20855;&#26377;&#31038;&#20132;&#24120;&#35782;&#35821;&#22659;&#21270;&#30340;&#30334;&#19975;&#35268;&#27169;&#23545;&#35805;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
SODA: Million-scale Dialogue Distillation with Social Commonsense Contextualization. (arXiv:2212.10465v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10465
&lt;/p&gt;
&lt;p&gt;
SODA&#26159;&#31532;&#19968;&#20010;&#20844;&#24320;&#21457;&#24067;&#30340;&#30334;&#19975;&#32423;&#21035;&#39640;&#36136;&#37327;&#31038;&#20132;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#19978;&#19979;&#25991;&#21270;&#31038;&#20132;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#33976;&#39311;&#65292;&#25105;&#20204;&#35757;&#32451;&#20986;&#20102;COSMO&#65292;&#20854;&#27604;&#30446;&#21069;&#26368;&#20339;&#34920;&#29616;&#30340;&#23545;&#35805;&#27169;&#22411;&#26356;&#20026;&#33258;&#28982;&#21644;&#19968;&#33268;&#65292;&#36825;&#26377;&#21161;&#20110;&#20102;&#35299;&#30693;&#35782;&#20016;&#23500;&#22411;&#23545;&#35805;&#21644;&#33258;&#28982;&#31038;&#20132;&#38386;&#32842;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;SODA&#65306;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30334;&#19975;&#35268;&#27169;&#39640;&#36136;&#37327;&#31038;&#20132;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20247;&#21253;&#23567;&#35268;&#27169;&#23545;&#35805;&#35821;&#26009;&#24211;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#30693;&#35782;&#22270;&#35889;&#65288;Atomic10x; West&#31561;&#20154;&#65292;2022&#65289;&#20013;&#30340;&#31038;&#20132;&#24120;&#35782;&#30693;&#35782;&#36827;&#34892;&#19978;&#19979;&#25991;&#21270;&#65292;&#25552;&#28860;&#20102;150&#19975;&#20010;&#31038;&#20132;&#23545;&#35805;&#12290;&#20154;&#31867;&#35780;&#20272;&#34920;&#26126;&#65292;SODA&#20013;&#30340;&#23545;&#35805;&#27604;&#20197;&#21069;&#30340;&#30001;&#20154;&#31867;&#25776;&#20889;&#30340;&#25968;&#25454;&#38598;&#26356;&#19968;&#33268;&#12289;&#26356;&#20855;&#20307;&#19988;&#65288;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#26356;&#33258;&#28982;&#12290;&#25105;&#20204;&#20351;&#29992;SODA&#35757;&#32451;&#20102;COSMO&#65306;&#19968;&#20010;&#36890;&#29992;&#30340;&#23545;&#35805;&#27169;&#22411;&#65292;&#22312;&#26410;&#30693;&#30340;&#25968;&#25454;&#38598;&#19978;&#27604;&#26368;&#20339;&#34920;&#29616;&#30340;&#23545;&#35805;&#27169;&#22411;&#65288;&#20363;&#22914;GODEL&#12289;BlenderBot-1&#12289;Koala&#12289;Vicuna&#65289;&#26356;&#33258;&#28982;&#21644;&#19968;&#33268;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COSMO&#26377;&#26102;&#29978;&#33267;&#34987;&#35748;&#20026;&#20248;&#20110;&#21407;&#22987;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#26631;&#20934;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#30693;&#35782;&#20016;&#23500;&#22411;&#23545;&#35805;&#21644;&#33258;&#28982;&#31038;&#20132;&#38386;&#32842;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SODA: the first publicly available, million-scale high-quality social dialogue dataset. In contrast to most existing crowdsourced, small-scale dialogue corpora, we distill 1.5M socially-grounded dialogues from a large language model (InstructGPT; Ouyang et al., 2022). Dialogues are distilled by contextualizing social commonsense knowledge from a knowledge graph (Atomic10x; West et al., 2022). Human evaluation shows that dialogues in SODA are more consistent, specific, and (surprisingly) natural than those in prior human-authored datasets.  Using SODA, we train COSMO: a generalizable conversation model that is significantly more natural and consistent on unseen datasets than best-performing conversation models (e.g., GODEL, BlenderBot-1, Koala, Vicuna). Experiments reveal COSMO is sometimes even preferred to the original human-written gold responses. Additionally, our results shed light on the distinction between knowledge-enriched conversations and natural social chitchats. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#21644;&#24314;&#27169;&#12290;&#36890;&#36807;&#22522;&#20110;&#32763;&#35793;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#21644;&#39640;&#25928;&#30340;&#27880;&#37322;&#21327;&#35758;&#65292;&#21019;&#24314;&#20102;&#21253;&#21547;7&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;mVQA&#22522;&#20934;MaXM&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#31616;&#21333;&#12289;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#30340;VQA&#27169;&#22411;&#12290;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;mVQA&#12290;</title><link>http://arxiv.org/abs/2209.05401</link><description>&lt;p&gt;
MaXM&#65306;&#36208;&#21521;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
MaXM: Towards Multilingual Visual Question Answering. (arXiv:2209.05401v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.05401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;&#30340;&#21487;&#25193;&#23637;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#25968;&#25454;&#29983;&#25104;&#21644;&#24314;&#27169;&#12290;&#36890;&#36807;&#22522;&#20110;&#32763;&#35793;&#30340;&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#21644;&#39640;&#25928;&#30340;&#27880;&#37322;&#21327;&#35758;&#65292;&#21019;&#24314;&#20102;&#21253;&#21547;7&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;mVQA&#22522;&#20934;MaXM&#12290;&#21516;&#26102;&#65292;&#24320;&#21457;&#20102;&#31616;&#21333;&#12289;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#30340;VQA&#27169;&#22411;&#12290;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;mVQA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;(VQA)&#20027;&#35201;&#36890;&#36807;&#33521;&#35821;&#36827;&#34892;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20197;&#21516;&#26679;&#30340;&#26041;&#24335;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#35299;&#20915;VQA&#38382;&#39064;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#35270;&#35273;&#38382;&#31572;(mVQA)&#65292;&#21253;&#25324;&#25968;&#25454;&#21644;&#24314;&#27169;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32763;&#35793;&#30340;mVQA&#25968;&#25454;&#29983;&#25104;&#26694;&#26550;&#65292;&#27604;&#30452;&#25509;&#25910;&#38598;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20154;&#24037;&#27880;&#37322;&#24037;&#20316;&#37327;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;Crossmodal-3600&#25968;&#25454;&#38598;&#20013;&#30340;&#22810;&#35821;&#35328;&#23383;&#24149;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27880;&#37322;&#21327;&#35758;&#65292;&#21019;&#24314;&#20102;MaXM&#65292;&#19968;&#20010;&#21253;&#21547;7&#31181;&#19981;&#21516;&#35821;&#35328;&#30340;&#20165;&#29992;&#20110;&#27979;&#35797;&#30340;VQA&#22522;&#20934;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#12289;&#36731;&#37327;&#32423;&#12289;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#22522;&#20110;&#27492;&#26500;&#24314;&#20102;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;VQA&#27169;&#22411;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#22522;&#20934;&#33021;&#22815;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;mVQA&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual Question Answering (VQA) has been primarily studied through the lens of the English language. Yet, tackling VQA in other languages in the same manner would require a considerable amount of resources. In this paper, we propose scalable solutions to multilingual visual question answering (mVQA), on both data and modeling fronts. We first propose a translation-based framework to mVQA data generation that requires much less human annotation efforts than the conventional approach of directly collection questions and answers. Then, we apply our framework to the multilingual captions in the Crossmodal-3600 dataset and develop an efficient annotation protocol to create MaXM, a test-only VQA benchmark in 7 diverse languages. Finally, we develop a simple, lightweight, and effective approach as well as benchmark state-of-the-art English and multilingual VQA models. We hope that our benchmark encourages further research on mVQA.
&lt;/p&gt;</description></item><item><title>SCL-RAI&#26159;&#19968;&#31181;&#24212;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#26080;&#26631;&#31614;&#23454;&#20307;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#36328;&#24230;&#23545;&#27604;&#23398;&#20064;&#20943;&#23567;&#20102;&#21516;&#31867;&#36328;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#22686;&#22823;&#20102;&#19981;&#21516;&#31867;&#36328;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#26080;&#26631;&#31614;&#23454;&#20307;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#25512;&#29702;&#65292;&#36824;&#35299;&#20915;&#20102;&#20915;&#31574;&#36793;&#30028;&#20559;&#31227;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.01646</link><description>&lt;p&gt;
SCL-RAI: &#22522;&#20110;&#36328;&#24230;&#23545;&#27604;&#23398;&#20064;&#19982;&#26816;&#32034;&#22686;&#24378;&#25512;&#29702;&#30340;&#26080;&#26631;&#31614;&#23454;&#20307;&#38382;&#39064;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SCL-RAI: Span-based Contrastive Learning with Retrieval Augmented Inference for Unlabeled Entity Problem in NER. (arXiv:2209.01646v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01646
&lt;/p&gt;
&lt;p&gt;
SCL-RAI&#26159;&#19968;&#31181;&#24212;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#26080;&#26631;&#31614;&#23454;&#20307;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#23427;&#21033;&#29992;&#36328;&#24230;&#23545;&#27604;&#23398;&#20064;&#20943;&#23567;&#20102;&#21516;&#31867;&#36328;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#12289;&#22686;&#22823;&#20102;&#19981;&#21516;&#31867;&#36328;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23545;&#26080;&#26631;&#31614;&#23454;&#20307;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#25512;&#29702;&#65292;&#36824;&#35299;&#20915;&#20102;&#20915;&#31574;&#36793;&#30028;&#20559;&#31227;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#23450;&#20301;&#21644;&#20998;&#31867;&#25991;&#26412;&#20013;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;NER&#25968;&#25454;&#38598;&#20013;&#30340;&#26080;&#26631;&#31614;&#23454;&#20307;&#38382;&#39064;&#20005;&#37325;&#38459;&#30861;&#20102;NER&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;SCL-RAI&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#36328;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#20943;&#23567;&#20102;&#20855;&#26377;&#30456;&#21516;&#26631;&#31614;&#30340;&#36328;&#24230;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#21516;&#26102;&#22686;&#21152;&#20102;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#36328;&#24230;&#34920;&#31034;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;&#23454;&#20307;&#20043;&#38388;&#30340;&#27495;&#20041;&#65292;&#24182;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26080;&#26631;&#31614;&#23454;&#20307;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26816;&#32034;&#22686;&#24378;&#25512;&#29702;&#26469;&#20943;&#36731;&#20915;&#31574;&#36793;&#30028;&#20559;&#31227;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;F1&#20998;&#25968;&#20998;&#21035;&#27604;&#20043;&#21069;&#30340;SOTA&#26041;&#27861;&#25552;&#39640;&#20102;4.21%&#21644;8.64%&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition is the task to locate and classify the entities in the text. However, Unlabeled Entity Problem in NER datasets seriously hinders the improvement of NER performance. This paper proposes SCL-RAI to cope with this problem. Firstly, we decrease the distance of span representations with the same label while increasing it for different ones via span-based contrastive learning, which relieves the ambiguity among entities and improves the robustness of the model over unlabeled entities. Then we propose retrieval augmented inference to mitigate the decision boundary shifting problem. Our method significantly outperforms the previous SOTA method by 4.21% and 8.64% F1-score on two real-world datasets.
&lt;/p&gt;</description></item><item><title>JAMES&#26159;&#19968;&#20010;&#29992;&#20110;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19977;&#31181;&#29420;&#29305;&#23884;&#20837;&#21644;&#20351;&#29992;&#21327;&#21516;&#27880;&#24847;&#26426;&#21046;&#21644;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#25429;&#25417;&#23703;&#20301;&#32844;&#31216;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#38750;&#35268;&#33539;&#21270;&#29992;&#25143;&#21019;&#24314;&#30340;&#32844;&#31216;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#23703;&#20301;&#32844;&#31216;&#31561;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2202.10739</link><description>&lt;p&gt;
JAMES: &#22522;&#20110;&#22810;&#26041;&#38754;&#22270;&#23884;&#20837;&#21644;&#25512;&#29702;&#30340;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
JAMES: Normalizing Job Titles with Multi-Aspect Graph Embeddings and Reasoning. (arXiv:2202.10739v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10739
&lt;/p&gt;
&lt;p&gt;
JAMES&#26159;&#19968;&#20010;&#29992;&#20110;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#26500;&#24314;&#19977;&#31181;&#29420;&#29305;&#23884;&#20837;&#21644;&#20351;&#29992;&#21327;&#21516;&#27880;&#24847;&#26426;&#21046;&#21644;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#25429;&#25417;&#23703;&#20301;&#32844;&#31216;&#30340;&#21508;&#31181;&#29305;&#24449;&#65292;&#24182;&#35299;&#20915;&#20102;&#35821;&#20041;&#30456;&#20284;&#24615;&#12289;&#38750;&#35268;&#33539;&#21270;&#29992;&#25143;&#21019;&#24314;&#30340;&#32844;&#31216;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#23703;&#20301;&#32844;&#31216;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#32844;&#20301;&#24066;&#22330;&#20013;&#65292;&#24314;&#31435;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#23703;&#20301;&#32844;&#31216;&#20998;&#31867;&#20307;&#31995;&#23545;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#24037;&#20316;&#25512;&#33616;&#12289;&#29992;&#25143;&#32844;&#19994;&#20998;&#26512;&#21644;&#31163;&#32844;&#39044;&#27979;&#65289;&#33267;&#20851;&#37325;&#35201;&#12290;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#26159;&#23558;&#29992;&#25143;&#21019;&#24314;&#30340;&#38750;&#26631;&#20934;&#23703;&#20301;&#32844;&#31216;&#20998;&#31867;&#20026;&#35268;&#33539;&#21270;&#32844;&#31216;&#30340;&#19968;&#20010;&#28165;&#27905;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#23703;&#20301;&#32844;&#31216;&#35268;&#33539;&#21270;&#38382;&#39064;&#24182;&#19981;&#23481;&#26131;&#65292;&#38754;&#20020;&#30528;&#20197;&#19979;&#25361;&#25112;&#65306;(1)&#19981;&#21516;&#23703;&#20301;&#32844;&#31216;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;(2)&#38750;&#35268;&#33539;&#21270;&#30340;&#29992;&#25143;&#21019;&#24314;&#30340;&#23703;&#20301;&#32844;&#31216;&#65292;&#20197;&#21450;(3)&#23454;&#38469;&#24212;&#29992;&#20013;&#22823;&#35268;&#27169;&#21644;&#38271;&#23614;&#20998;&#24067;&#30340;&#23703;&#20301;&#32844;&#31216;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;JAMES&#30340;&#26032;&#39062;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26500;&#24314;&#30446;&#26631;&#23703;&#20301;&#32844;&#31216;&#30340;&#19977;&#31181;&#29420;&#29305;&#23884;&#20837;&#65288;&#21363;&#22270;&#12289;&#19978;&#19979;&#25991;&#21644;&#21477;&#27861;&#65289;&#65292;&#20197;&#26377;&#25928;&#25429;&#25417;&#20854;&#21508;&#31181;&#29305;&#24449;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#21327;&#21516;&#27880;&#24847;&#26426;&#21046;&#26469;&#27880;&#24847;&#22320;&#32467;&#21512;&#36825;&#20123;&#23884;&#20837;&#65292;&#36824;&#20351;&#29992;&#31070;&#32463;&#36923;&#36753;&#25512;&#29702;&#34920;&#31034;&#20849;&#21516;&#20272;&#35745;&#28151;&#20081;&#30340;&#23703;&#20301;&#32844;&#31216;&#19982;&#35268;&#33539;&#21270;&#23703;&#20301;&#32844;&#31216;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In online job marketplaces, it is important to establish a well-defined job title taxonomy for various downstream tasks (e.g., job recommendation, users' career analysis, and turnover prediction). Job Title Normalization (JTN) is such a cleaning step to classify user-created non-standard job titles into normalized ones. However, solving the JTN problem is non-trivial with challenges: (1) semantic similarity of different job titles, (2) non-normalized user-created job titles, and (3) large-scale and long-tailed job titles in real-world applications. To this end, we propose a novel solution, named JAMES, that constructs three unique embeddings (i.e., graph, contextual, and syntactic) of a target job title to effectively capture its various traits. We further propose a multi-aspect co-attention mechanism to attentively combine these embeddings, and employ neural logical reasoning representations to collaboratively estimate similarities between messy job titles and normalized job titles in
&lt;/p&gt;</description></item><item><title>ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2007.01777</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24207;&#21015;&#20998;&#31867;&#36890;&#36807;&#21407;&#22411;&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2007.01777
&lt;/p&gt;
&lt;p&gt;
ProtoryNet&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#21644;&#21407;&#22411;&#30340;&#36817;&#20284;&#31243;&#24230;&#26469;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#30452;&#35266;&#21644;&#32454;&#33268;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;ProtoryNet&#65292;&#23427;&#22522;&#20110;&#21407;&#22411;&#36712;&#36857;&#30340;&#26032;&#27010;&#24565;&#12290;&#21463;&#29616;&#20195;&#35821;&#35328;&#23398;&#20013;&#30340;&#21407;&#22411;&#29702;&#35770;&#30340;&#21551;&#21457;&#65292;ProtoryNet&#36890;&#36807;&#20026;&#25991;&#26412;&#24207;&#21015;&#20013;&#30340;&#27599;&#20010;&#21477;&#23376;&#25214;&#21040;&#26368;&#30456;&#20284;&#30340;&#21407;&#22411;&#65292;&#24182;&#23558;&#27599;&#20010;&#21477;&#23376;&#19982;&#30456;&#24212;&#30340;&#27963;&#21160;&#21407;&#22411;&#30340;&#25509;&#36817;&#31243;&#24230;&#36755;&#20837;&#21040;RNN&#20027;&#24178;&#20013;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#21518;&#65292;RNN&#20027;&#24178;&#25429;&#25417;&#21040;&#21407;&#22411;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21407;&#22411;&#36712;&#36857;&#12290;&#21407;&#22411;&#36712;&#36857;&#33021;&#22815;&#30452;&#35266;&#32780;&#32454;&#33268;&#22320;&#35299;&#37322;RNN&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#20998;&#26512;&#25991;&#26412;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#21407;&#22411;&#20462;&#21098;&#36807;&#31243;&#65292;&#20197;&#20943;&#23569;&#27169;&#22411;&#20351;&#29992;&#30340;&#21407;&#22411;&#24635;&#25968;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;ProtoryNet&#27604;&#22522;&#32447;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26356;&#20934;&#30830;&#65292;&#24182;&#20943;&#23569;&#20102;&#19982;&#29616;&#26377;&#27169;&#22411;&#30456;&#27604;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
&lt;/p&gt;</description></item></channel></rss>