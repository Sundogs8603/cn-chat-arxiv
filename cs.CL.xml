<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02822</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#35782;&#21035;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#30340;&#27668;&#20505;&#30446;&#26631;
&lt;/p&gt;
&lt;p&gt;
Identifying Climate Targets in National Laws and Policies using Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#20986;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65289;&#65292;&#24182;&#35843;&#26597;&#20102;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23450;&#37327;&#25919;&#31574;&#30446;&#26631;&#26159;&#27668;&#20505;&#25919;&#31574;&#30340;&#22522;&#26412;&#35201;&#32032;&#65292;&#36890;&#24120;&#20197;&#39046;&#22495;&#29305;&#23450;&#21644;&#25216;&#26415;&#24615;&#35821;&#35328;&#20026;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#31579;&#36873;&#20840;&#29699;&#27668;&#20505;&#25919;&#31574;&#30446;&#26631;&#30340;&#26041;&#27861;&#28041;&#21450;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;&#30446;&#21069;&#24456;&#23569;&#26377;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#20174;&#22269;&#23478;&#27861;&#24459;&#25110;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#65292;&#36825;&#38480;&#21046;&#20102;&#25919;&#31574;&#21046;&#23450;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#35780;&#20272;&#31169;&#33829;&#21644;&#20844;&#20849;&#37096;&#38376;&#19982;&#20840;&#29699;&#30446;&#26631;&#30340;&#19968;&#33268;&#24615;&#20197;&#21450;&#20026;&#25919;&#31574;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22269;&#23478;&#27861;&#24459;&#21644;&#25919;&#31574;&#20013;&#25552;&#21462;&#27668;&#20505;&#30446;&#26631;&#25552;&#21450;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;&#20102;&#19977;&#31867;&#30446;&#26631;&#65288;&#8220;&#20928;&#38646;&#8221;&#65292;&#8220;&#20943;&#23569;&#8221;&#21644;&#8220;&#20854;&#20182;&#8221;&#65288;&#20363;&#22914;&#21487;&#20877;&#29983;&#33021;&#28304;&#30446;&#26631;&#65289;&#65289;&#65292;&#24182;&#35757;&#32451;&#20102;&#19968;&#20010;&#21487;&#38752;&#22320;&#22312;&#25991;&#26412;&#20013;&#35782;&#21035;&#23427;&#20204;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#19982;&#25105;&#20204;&#27169;&#22411;&#30456;&#20851;&#30340;&#20559;&#24046;&#21644;&#20844;&#24179;&#24433;&#21709;&#65292;&#24182;&#30830;&#23450;&#20102;&#29305;&#23450;&#24180;&#20221;&#21644;&#22269;&#23478;&#21517;&#31216;&#20316;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02822v1 Announce Type: cross  Abstract: Quantified policy targets are a fundamental element of climate policy, typically characterised by domain-specific and technical language. Current methods for curating comprehensive views of global climate policy targets entail significant manual effort. At present there are few scalable methods for extracting climate targets from national laws or policies, which limits policymakers' and researchers' ability to (1) assess private and public sector alignment with global goals and (2) inform policy decisions. In this paper we present an approach for extracting mentions of climate targets from national laws and policies. We create an expert-annotated dataset identifying three categories of target ('Net Zero', 'Reduction' and 'Other' (e.g. renewable energy targets)) and train a classifier to reliably identify them in text. We investigate bias and equity impacts related to our model and identify specific years and country names as problemati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02761</link><description>&lt;p&gt;
AQuA --&#32467;&#21512;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#35266;&#28857;&#65292;&#21033;&#29992;LLMs&#35780;&#20272;&#22312;&#32447;&#35752;&#35770;&#20013;&#30340;&#30923;&#21830;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
AQuA -- Combining Experts' and Non-Experts' Views To Assess Deliberation Quality in Online Discussions Using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02761
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;AQuA&#65292;&#19968;&#31181;&#32508;&#21512;&#30340;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#35745;&#31639;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#25552;&#21462;&#21508;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#24471;&#20998;&#65292;&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25919;&#27835;&#22312;&#32447;&#35752;&#35770;&#20013;&#34913;&#37327;&#36129;&#29486;&#36136;&#37327;&#23545;&#20110;&#30740;&#31350;&#30923;&#21830;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36827;&#27493;&#65292;&#33258;&#21160;&#34913;&#37327;&#36825;&#20123;&#25351;&#26631;&#21464;&#24471;&#21487;&#34892;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AQuA&#65292;&#23427;&#26159;&#19968;&#20010;&#28155;&#21152;&#20998;&#25968;&#65292;&#20174;&#22810;&#20010;&#25351;&#26631;&#20013;&#35745;&#31639;&#27599;&#20010;&#35752;&#35770;&#24086;&#23376;&#30340;&#32479;&#19968;&#30923;&#21830;&#36136;&#37327;&#24471;&#20998;&#12290;&#19982;&#20854;&#20182;&#29305;&#23450;&#20998;&#25968;&#19981;&#21516;&#65292;AQuA&#20445;&#30041;&#20102;&#35780;&#35770;&#20013;&#23384;&#22312;&#30340;&#30923;&#21830;&#26041;&#38754;&#30340;&#20449;&#24687;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02761v1 Announce Type: cross  Abstract: Measuring the quality of contributions in political online discussions is crucial in deliberation research and computer science. Research has identified various indicators to assess online discussion quality, and with deep learning advancements, automating these measures has become feasible. While some studies focus on analyzing specific quality indicators, a comprehensive quality score incorporating various deliberative aspects is often preferred. In this work, we introduce AQuA, an additive score that calculates a unified deliberative quality score from multiple indices for each discussion post. Unlike other singular scores, AQuA preserves information on the deliberative aspects present in comments, enhancing model transparency. We develop adapter models for 20 deliberative indices, and calculate correlation coefficients between experts' annotations and the perceived deliberativeness by non-experts to weigh the individual indices int
&lt;/p&gt;</description></item><item><title>CSEPrompts&#26159;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#32534;&#31243;&#32451;&#20064;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2404.02540</link><description>&lt;p&gt;
CSEPrompts: &#21021;&#32423;&#35745;&#31639;&#26426;&#31185;&#23398;&#25552;&#31034;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CSEPrompts: A Benchmark of Introductory Computer Science Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02540
&lt;/p&gt;
&lt;p&gt;
CSEPrompts&#26159;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#32534;&#31243;&#32451;&#20064;&#25552;&#31034;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#29702;&#35299;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#36827;&#23637;&#23548;&#33268;&#20102;&#26032;&#19968;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#28023;&#37327;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#24120;&#25317;&#26377;&#25968;&#19975;&#20159;&#21442;&#25968;&#12290;&#21830;&#19994;&#24212;&#29992;&#65288;&#22914;ChatGPT&#65289;&#24050;&#20351;&#36825;&#39033;&#25216;&#26415;&#38754;&#21521;&#26222;&#36890;&#22823;&#20247;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;LLMs&#20026;&#23398;&#26415;&#21644;&#19987;&#19994;&#29992;&#36884;&#29983;&#25104;&#39640;&#36136;&#37327;&#25991;&#26412;&#12290;&#23398;&#26657;&#21644;&#22823;&#23398;&#24847;&#35782;&#21040;&#23398;&#29983;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#30001;AI&#29983;&#25104;&#30340;&#20869;&#23481;&#65292;&#20182;&#20204;&#19968;&#30452;&#22312;&#30740;&#31350;&#36825;&#31181;&#26032;&#25216;&#26415;&#21450;&#20854;&#28508;&#22312;&#30340;&#28389;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#65288;CS&#65289;&#21450;&#30456;&#20851;&#39046;&#22495;&#30340;&#25945;&#32946;&#39033;&#30446;&#23588;&#20854;&#21463;&#21040;&#24433;&#21709;&#65292;&#22240;&#20026;LLMs&#20063;&#33021;&#22815;&#29983;&#25104;&#21508;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#32534;&#31243;&#20195;&#30721;&#12290;&#20026;&#20102;&#24110;&#21161;&#29702;&#35299;&#35745;&#31639;&#26426;&#31185;&#23398;&#25945;&#32946;&#20013;&#20844;&#24320;&#21487;&#29992;LLMs&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CSEPrompts&#65292;&#19968;&#20010;&#21253;&#21547;&#25968;&#30334;&#20010;&#32534;&#31243;&#32451;&#20064;&#25552;&#31034;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02540v1 Announce Type: new  Abstract: Recent advances in AI, machine learning, and NLP have led to the development of a new generation of Large Language Models (LLMs) that are trained on massive amounts of data and often have trillions of parameters. Commercial applications (e.g., ChatGPT) have made this technology available to the general public, thus making it possible to use LLMs to produce high-quality texts for academic and professional purposes. Schools and universities are aware of the increasing use of AI-generated content by students and they have been researching the impact of this new technology and its potential misuse. Educational programs in Computer Science (CS) and related fields are particularly affected because LLMs are also capable of generating programming code in various programming languages. To help understand the potential impact of publicly available LLMs in CS education, we introduce CSEPrompts, a framework with hundreds of programming exercise prom
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34913;&#37327;&#20102;&#21069;&#32512;&#21542;&#23450;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#21305;&#37197;&#65292;&#27169;&#22411;&#25972;&#20307;&#19978;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#21069;&#32512;&#21542;&#23450;&#30340;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2404.02421</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20122;&#23383;&#35789;&#26631;&#35760;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#32512;&#21542;&#23450;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Revisiting subword tokenization: A case study on affixal negation in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02421
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34913;&#37327;&#20102;&#21069;&#32512;&#21542;&#23450;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#19981;&#21305;&#37197;&#65292;&#27169;&#22411;&#25972;&#20307;&#19978;&#33021;&#22815;&#21487;&#38752;&#22320;&#35782;&#21035;&#21069;&#32512;&#21542;&#23450;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#34913;&#37327;&#20102;&#21069;&#32512;&#21542;&#23450;&#23545;&#29616;&#20195;&#33521;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24433;&#21709;&#12290;&#22312;&#21069;&#32512;&#21542;&#23450;&#20013;&#65292;&#21542;&#23450;&#30340;&#21547;&#20041;&#36890;&#36807;&#19968;&#20010;&#36127;&#38754;&#24418;&#24577;&#32032;&#26469;&#34920;&#36798;&#65292;&#36825;&#23545;LLMs&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#26631;&#35760;&#22120;&#36890;&#24120;&#19981;&#20855;&#22791;&#24418;&#24577;&#23398;&#19978;&#30340;&#21512;&#29702;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#19981;&#21516;&#20122;&#23383;&#35789;&#26631;&#35760;&#26041;&#27861;&#30340;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#36825;&#20123;&#23454;&#39564;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#26631;&#35760;&#24615;&#33021;&#19982;&#21542;&#23450;&#25935;&#24863;&#24615;&#20043;&#38388;&#20132;&#20114;&#20316;&#29992;&#30340;&#20960;&#28857;&#35265;&#35299;&#12290;&#23613;&#31649;&#22312;&#26631;&#35760;&#20934;&#30830;&#24615;&#21644;&#21542;&#23450;&#26816;&#27979;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#19968;&#20123;&#26377;&#36259;&#30340;&#19981;&#21305;&#37197;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#27169;&#22411;&#25972;&#20307;&#19978;&#21487;&#20197;&#21487;&#38752;&#22320;&#35782;&#21035;&#21069;&#32512;&#21542;&#23450;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02421v1 Announce Type: new  Abstract: In this work, we measure the impact of affixal negation on modern English large language models (LLMs). In affixal negation, the negated meaning is expressed through a negative morpheme, which is potentially challenging for LLMs as their tokenizers are often not morphologically plausible. We conduct extensive experiments using LLMs with different subword tokenization methods, which lead to several insights on the interaction between tokenization performance and negation sensitivity. Despite some interesting mismatches between tokenization accuracy and negation detection performance, we show that models can, on the whole, reliably recognize the meaning of affixal negation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2404.02060</link><description>&lt;p&gt;
&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#36935;&#21040;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Long-context LLMs Struggle with Long In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02060
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934; LIConBench&#65292;&#32858;&#28966;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21457;&#29616;&#38271;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#20013;&#24615;&#33021;&#33391;&#22909;&#65292;&#23588;&#20854;&#22312;&#26631;&#35760;&#38271;&#24230;&#19981;&#36229;&#36807;20K&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#36229;&#36807;32K&#26631;&#35760;&#30340;&#38271;&#24207;&#21015;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#35780;&#20272;&#20027;&#35201;&#23616;&#38480;&#22312;&#22256;&#24785;&#24230;&#21644;&#21512;&#25104;&#20219;&#21153;&#31561;&#25351;&#26631;&#19978;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#23427;&#20204;&#22312;&#26356;&#24494;&#22937;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22522;&#20934;&#65288;LIConBench&#65289;&#65292;&#30528;&#37325;&#20110;&#38271;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#26497;&#31471;&#26631;&#31614;&#20998;&#31867;&#39046;&#22495;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20845;&#20010;&#25968;&#25454;&#38598;&#65292;&#20854;&#26631;&#31614;&#33539;&#22260;&#36328;&#24230;&#20026;28&#33267;174&#31867;&#65292;&#28085;&#30422;&#20102;&#20174;2K&#21040;50K&#30340;&#19981;&#21516;&#36755;&#20837;&#65288;&#23569;&#37327;&#28436;&#31034;&#65289;&#38271;&#24230;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#35201;&#27714;LLMs&#29702;&#35299;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#35782;&#21035;&#24222;&#22823;&#30340;&#26631;&#31614;&#31354;&#38388;&#20197;&#36827;&#34892;&#27491;&#30830;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;13&#20010;&#38271;&#19978;&#19979;&#25991;LLMs&#12290;&#25105;&#20204;&#21457;&#29616;&#38271;&#19978;&#19979;&#25991;LLMs&#22312;&#26631;&#35760;&#38271;&#24230;&#20026;20K&#20197;&#19979;&#26102;&#34920;&#29616;&#30456;&#23545;&#36739;&#22909;&#65292;&#24182;&#19988;&#21033;&#29992;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#20250;&#24102;&#26469;&#24615;&#33021;&#19978;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02060v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have made significant strides in handling long sequences exceeding 32K tokens. However, their performance evaluation has largely been confined to metrics like perplexity and synthetic tasks, which may not fully capture their abilities in more nuanced, real-world scenarios. This study introduces a specialized benchmark (LIConBench) focusing on long in-context learning within the realm of extreme-label classification. We meticulously selected six datasets with a label range spanning 28 to 174 classes covering different input (few-shot demonstration) length from 2K to 50K. Our benchmark requires LLMs to comprehend the entire input to recognize the massive label spaces to make correct prediction. We evaluate 13 long-context LLMs on our benchmarks. We find that the long-context LLMs perform relatively well under the token length of 20K and the performance benefits from utilizing the long context window. However,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERTopic&#20998;&#26512;&#32929;&#24066;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#65292;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26174;&#31034;&#24773;&#24863;&#20998;&#26512;&#26174;&#33879;&#25552;&#21319;&#20102;&#32929;&#24066;&#39044;&#27979;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;NLP&#22312;&#20016;&#23500;&#37329;&#34701;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02053</link><description>&lt;p&gt;
BERTopic&#39537;&#21160;&#30340;&#32929;&#24066;&#39044;&#27979;&#65306;&#35299;&#26512;&#24773;&#24863;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
BERTopic-Driven Stock Market Predictions: Unraveling Sentiment Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21033;&#29992;BERTopic&#20998;&#26512;&#32929;&#24066;&#35780;&#35770;&#20013;&#30340;&#24773;&#24863;&#65292;&#25972;&#21512;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#26174;&#31034;&#24773;&#24863;&#20998;&#26512;&#26174;&#33879;&#25552;&#21319;&#20102;&#32929;&#24066;&#39044;&#27979;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;NLP&#22312;&#20016;&#23500;&#37329;&#34701;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#37329;&#34701;&#20998;&#26512;&#30340;&#20132;&#21449;&#39046;&#22495;&#65292;&#37325;&#28857;&#20851;&#27880;&#24773;&#24863;&#20998;&#26512;&#22312;&#32929;&#20215;&#39044;&#27979;&#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;BERTopic&#65292;&#19968;&#31181;&#20808;&#36827;&#30340;NLP&#25216;&#26415;&#65292;&#20998;&#26512;&#20174;&#32929;&#24066;&#35780;&#35770;&#20013;&#24471;&#20986;&#30340;&#20027;&#39064;&#30340;&#24773;&#24863;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#36825;&#31181;&#24773;&#24863;&#20998;&#26512;&#19982;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#25972;&#21512;&#65292;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#21644;&#32929;&#31080;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#38395;&#21517;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25972;&#21512;&#20027;&#39064;&#24773;&#24863;&#26174;&#33879;&#25552;&#21319;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32929;&#24066;&#35780;&#35770;&#20013;&#30340;&#20027;&#39064;&#25552;&#20379;&#20102;&#23545;&#32929;&#24066;&#27874;&#21160;&#21644;&#20215;&#26684;&#36235;&#21183;&#30340;&#38544;&#21547;&#12289;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23637;&#31034;NLP&#22312;&#20016;&#23500;&#37329;&#34701;&#20998;&#26512;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#20026;&#23454;&#26102;&#24773;&#24863;&#20998;&#26512;&#21644;&#25506;&#32034;&#24773;&#24863;&#21644;&#24773;&#26223;&#30456;&#20851;&#24615;&#25171;&#24320;&#20102;&#30740;&#31350;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02053v1 Announce Type: new  Abstract: This paper explores the intersection of Natural Language Processing (NLP) and financial analysis, focusing on the impact of sentiment analysis in stock price prediction. We employ BERTopic, an advanced NLP technique, to analyze the sentiment of topics derived from stock market comments. Our methodology integrates this sentiment analysis with various deep learning models, renowned for their effectiveness in time series and stock prediction tasks. Through comprehensive experiments, we demonstrate that incorporating topic sentiment notably enhances the performance of these models. The results indicate that topics in stock market comments provide implicit, valuable insights into stock market volatility and price trends. This study contributes to the field by showcasing the potential of NLP in enriching financial analysis and opens up avenues for further research into real-time sentiment analysis and the exploration of emotional and contextua
&lt;/p&gt;</description></item><item><title>CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;</title><link>https://arxiv.org/abs/2404.01663</link><description>&lt;p&gt;
CMAT: &#29992;&#20110;&#22686;&#24378;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#35843;&#25972;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01663
&lt;/p&gt;
&lt;p&gt;
CMAT&#26694;&#26550;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#65292;&#22686;&#24378;&#20102;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26174;&#33879;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;LLMs&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#25805;&#20316;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#20154;&#31867;&#36755;&#20837;&#26469;&#20934;&#30830;&#24341;&#23548;&#23545;&#35805;&#27969;&#31243;&#65292;&#26234;&#33021;&#20307;&#35843;&#25972;&#26159;&#19968;&#31181;&#20851;&#38190;&#30340;&#20248;&#21270;&#25216;&#26415;&#65292;&#28041;&#21450;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#35843;&#25972;&#65292;&#20197;&#26356;&#22909;&#22320;&#21709;&#24212;&#36825;&#31181;&#24341;&#23548;&#12290;&#38024;&#23545;&#36825;&#19968;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;TinyAgent&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Collaborative Multi-Agent Tuning&#65288;CMAT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#24615;&#31995;&#32479;&#65292;&#26088;&#22312;&#36890;&#36807;&#26681;&#25454;&#29615;&#22659;&#21453;&#39304;&#36827;&#34892;&#33258;&#36866;&#24212;&#26435;&#37325;&#26356;&#26032;&#26469;&#22686;&#24378;&#35821;&#35328;&#26234;&#33021;&#20307;&#30340;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#20419;&#36827;&#20102;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#23398;&#20064;&#21644;&#23454;&#26102;&#36866;&#24212;&#65292;&#22686;&#24378;&#20102;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01663v1 Announce Type: new  Abstract: Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this resear
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;</title><link>https://arxiv.org/abs/2404.01616</link><description>&lt;p&gt;
&#23558;LLMs&#36716;&#21270;&#20026;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Transforming LLMs into Cross-modal and Cross-lingual RetrievalSystems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01616
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#65292;&#23454;&#29616;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#35821;&#38899;&#25968;&#25454;&#65292;&#19988;&#30456;&#27604;&#20808;&#21069;&#31995;&#32479;&#21462;&#24471;10%&#30340;Recall@1&#32477;&#23545;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#22312;&#20165;&#22522;&#20110;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#36825;&#36229;&#20986;&#20102;&#20855;&#26377;&#37197;&#23545;&#35821;&#38899;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#35821;&#35328;&#33539;&#22260;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21452;&#32534;&#30721;&#22120;&#65288;DE&#65289;&#30340;&#26816;&#32034;&#31995;&#32479;&#23558;&#26597;&#35810;&#21644;&#25991;&#26723;&#25237;&#24433;&#21040;&#30456;&#21516;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#24182;&#22312;&#26816;&#32034;&#21644;&#21452;&#35821;&#25991;&#26412;&#25366;&#25496;&#20013;&#23637;&#31034;&#20102;&#25104;&#21151;&#12290;&#20026;&#20102;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;LLMs&#21021;&#22987;&#21270;&#22810;&#27169;&#24577;DE&#26816;&#32034;&#31995;&#32479;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;LLM&#39044;&#35757;&#32451;&#26399;&#38388;&#19981;&#38656;&#35201;&#35821;&#38899;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#20197;&#21033;&#29992;LLM&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#29702;&#35299;&#33021;&#21147;&#26469;&#21305;&#37197;&#26816;&#32034;&#35757;&#32451;&#26399;&#38388;&#30475;&#19981;&#35265;&#30340;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#21644;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#22810;&#27169;&#24577;LLM-based&#26816;&#32034;&#31995;&#32479;&#33021;&#22815;&#22312;102&#31181;&#35821;&#35328;&#20013;&#21305;&#37197;&#35821;&#38899;&#21644;&#25991;&#26412;&#65292;&#23613;&#31649;&#21482;&#22312;21&#31181;&#35821;&#35328;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#20248;&#20110;&#20808;&#21069;&#19987;&#38376;&#22312;&#25152;&#26377;102&#31181;&#35821;&#35328;&#19978;&#35757;&#32451;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#25105;&#20204;&#22312;Recall@1&#19978;&#23454;&#29616;&#20102;10&#65285;&#30340;&#32477;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01616v1 Announce Type: new  Abstract: Large language models (LLMs) are trained on text-only data that go far beyond the languages with paired speech and text data. At the same time, Dual Encoder (DE) based retrieval systems project queries and documents into the same embedding space and have demonstrated their success in retrieval and bi-text mining. To match speech and text in many languages, we propose using LLMs to initialize multi-modal DE retrieval systems. Unlike traditional methods, our system doesn't require speech data during LLM pre-training and can exploit LLM's multilingual text understanding capabilities to match speech and text in languages unseen during retrieval training. Our multi-modal LLM-based retrieval system is capable of matching speech and text in 102 languages despite only training on 21 languages. Our system outperforms previous systems trained explicitly on all 102 languages. We achieve a 10% absolute improvement in Recall@1 averaged across these l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Fine-tuned&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#27880;&#37322;&#35821;&#26009;&#24211;PedSHAC&#65292;&#24182;&#33258;&#21160;&#25552;&#21462;&#20799;&#31185;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#35814;&#32454;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2404.00826</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20799;&#31185;&#24739;&#32773;&#30149;&#21382;&#20013;&#25552;&#21462;&#20581;&#24247;&#30340;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;&#65306;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#21644;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Extracting Social Determinants of Health from Pediatric Patient Notes Using Large Language Models: Novel Corpus and Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Fine-tuned&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#27880;&#37322;&#35821;&#26009;&#24211;PedSHAC&#65292;&#24182;&#33258;&#21160;&#25552;&#21462;&#20799;&#31185;&#24739;&#32773;&#30149;&#21382;&#20013;&#30340;&#35814;&#32454;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20581;&#24247;&#30340;&#31038;&#20250;&#20915;&#23450;&#22240;&#32032;(SDoH)&#22312;&#22609;&#36896;&#20581;&#24247;&#32467;&#26524;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#20799;&#31185;&#20154;&#32676;&#20013;&#65292;&#24178;&#39044;&#25514;&#26045;&#21487;&#33021;&#20855;&#26377;&#38271;&#26399;&#24433;&#21709;&#12290;SDoH&#32463;&#24120;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#20013;&#36827;&#34892;&#30740;&#31350;&#65292;EHR&#20026;&#22810;&#26679;&#21270;&#30340;&#24739;&#32773;&#25968;&#25454;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#24211;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24102;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#20799;&#31185;&#31038;&#20250;&#21490;&#27880;&#37322;&#35821;&#26009;&#24211;(PedSHAC)&#65292;&#24182;&#35780;&#20272;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24494;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#33258;&#21160;&#25552;&#21462;&#35814;&#32454;&#30340;SDoH&#34920;&#24449;&#12290;PedSHAC&#21253;&#25324;&#26469;&#33258;&#21326;&#30427;&#39039;&#22823;&#23398;(UW)&#21307;&#38498;&#31995;&#32479;&#20869;&#30340;1,260&#20221;&#20020;&#24202;&#35760;&#24405;&#20013;&#27880;&#37322;&#30340;&#31038;&#20250;&#21490;&#33410;&#12290;&#37319;&#29992;&#20107;&#20214;&#20026;&#22522;&#30784;&#30340;&#27880;&#37322;&#26041;&#26696;&#65292;PedSHAC&#25429;&#25417;&#20102;&#21313;&#20010;&#19981;&#21516;&#30340;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#65292;&#28085;&#30422;&#20102;&#29983;&#27963;&#21644;&#32463;&#27982;&#31283;&#23450;&#24615;&#65292;&#20197;&#21069;&#30340;&#21019;&#20260;&#65292;&#25945;&#32946;&#33719;&#21462;&#65292;&#29289;&#36136;&#20351;&#29992;&#21490;&#20197;&#21450;&#24515;&#29702;&#20581;&#24247;&#31561;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00826v1 Announce Type: new  Abstract: Social determinants of health (SDoH) play a critical role in shaping health outcomes, particularly in pediatric populations where interventions can have long-term implications. SDoH are frequently studied in the Electronic Health Record (EHR), which provides a rich repository for diverse patient data. In this work, we present a novel annotated corpus, the Pediatric Social History Annotation Corpus (PedSHAC), and evaluate the automatic extraction of detailed SDoH representations using fine-tuned and in-context learning methods with Large Language Models (LLMs). PedSHAC comprises annotated social history sections from 1,260 clinical notes obtained from pediatric patients within the University of Washington (UW) hospital system. Employing an event-based annotation scheme, PedSHAC captures ten distinct health determinants to encompass living and economic stability, prior trauma, education access, substance use history, and mental health with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.00450</link><description>&lt;p&gt;
&#35268;&#21010;&#21644;&#32534;&#36753;&#26816;&#32034;&#20197;&#22686;&#24378;&#24037;&#20855;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Planning and Editing What You Retrieve for Enhanced Tool Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00450
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#8221;&#33539;&#24335;&#65292;&#36890;&#36807;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;LLM-based&#26597;&#35810;&#35268;&#21010;&#22120;&#25552;&#39640;&#20102;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#23558;&#22806;&#37096;&#24037;&#20855;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38598;&#25104;&#26041;&#38754;&#21462;&#24471;&#30340;&#36827;&#23637;&#25171;&#24320;&#20102;&#26032;&#30340;&#39046;&#22495;&#65292;&#24212;&#29992;&#33539;&#22260;&#28085;&#30422;&#25968;&#23398;&#25512;&#29702;&#12289;&#20195;&#30721;&#29983;&#25104;&#22120;&#21644;&#26234;&#33021;&#21161;&#25163;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#31616;&#21333;&#30340;&#19968;&#27425;&#24615;&#26816;&#32034;&#31574;&#30053;&#65292;&#26080;&#27861;&#26377;&#25928;&#20934;&#30830;&#22320;&#31579;&#36873;&#30456;&#20851;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#35268;&#21010;&#19982;&#26816;&#32034;&#65288;P&amp;R&#65289;&#8221;&#21644;&#8220;&#32534;&#36753;&#19982;&#30830;&#35748;&#65288;E&amp;G&#65289;&#8221;&#33539;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#20102;&#31070;&#32463;&#26816;&#32034;&#27169;&#22359;&#21644;&#22522;&#20110;LLM&#30340;&#26597;&#35810;&#35268;&#21010;&#22120;&#65292;&#20197;&#22686;&#24378;&#24037;&#20855;&#21033;&#29992;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00450v1 Announce Type: new  Abstract: Recent advancements in integrating external tools with Large Language Models (LLMs) have opened new frontiers, with applications in mathematical reasoning, code generators, and smart assistants. However, existing methods, relying on simple one-time retrieval strategies, fall short on effectively and accurately shortlisting relevant tools. This paper introduces a novel \modelname (\modelmeaning) approach, encompassing ``Plan-and-Retrieve (P\&amp;R)'' and ``Edit-and-Ground (E\&amp;G)'' paradigms. The P\&amp;R paradigm consists of a neural retrieval module for shortlisting relevant tools and an LLM-based query planner that decomposes complex queries into actionable tasks, enhancing the effectiveness of tool utilization. The E\&amp;G paradigm utilizes LLMs to enrich tool descriptions based on user scenarios, bridging the gap between user queries and tool functionalities. Experiment results demonstrate that these paradigms significantly improve the recall an
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#30340;&#24494;&#35843;&#21644;&#26410;&#24494;&#35843;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20145</link><description>&lt;p&gt;
&#20026;&#33258;&#21160;&#35786;&#26029;&#31579;&#26597;&#24635;&#32467;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Automated Diagnostic Screening Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20145
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#30340;&#24494;&#35843;&#21644;&#26410;&#24494;&#35843;&#65292;&#21457;&#29616;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#29983;&#25104;&#25688;&#35201;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#23637;&#20013;&#22269;&#23478;&#25913;&#21892;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38656;&#27714;&#12290;&#19968;&#31181;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#24320;&#21457;&#21487;&#25193;&#23637;&#30340;&#33258;&#21160;&#31995;&#32479;&#36827;&#34892;&#35786;&#26029;&#31579;&#26597;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#36731;&#24515;&#29702;&#20581;&#24247;&#19987;&#19994;&#20154;&#21592;&#30340;&#36127;&#25285;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#21644;&#26410;&#24494;&#35843;&#65292;&#29992;&#20110;&#20174;&#24515;&#29702;&#29366;&#24577;&#26816;&#26597;&#20013;&#29983;&#25104;&#31616;&#26126;&#25688;&#35201;&#12290;&#25105;&#20204;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;ROUGE&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#36755;&#20837;&#65292;&#23545;&#22235;&#31181;&#19981;&#21516;&#30340;&#25688;&#35201;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#65292;&#20998;&#21035;&#23454;&#29616;&#20102;0.810&#21644;0.764&#30340;ROUGE-1&#21644;ROUGE-L&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#24494;&#35843;&#27169;&#22411;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;D4&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#20196;&#20154;&#40723;&#33310;&#65292;&#34920;&#26126;&#20854;&#28508;&#22312;&#36866;&#29992;&#24615;&#36229;&#20986;&#25105;&#20204;&#30340;&#33258;&#23450;&#20041;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20145v1 Announce Type: new  Abstract: Improving mental health support in developing countries is a pressing need. One potential solution is the development of scalable, automated systems to conduct diagnostic screenings, which could help alleviate the burden on mental health professionals. In this work, we evaluate several state-of-the-art Large Language Models (LLMs), with and without fine-tuning, on our custom dataset for generating concise summaries from mental state examinations. We rigorously evaluate four different models for summary generation using established ROUGE metrics and input from human evaluators. The results highlight that our top-performing fine-tuned model outperforms existing models, achieving ROUGE-1 and ROUGE-L values of 0.810 and 0.764, respectively. Furthermore, we assessed the fine-tuned model's generalizability on a publicly available D4 dataset, and the outcomes were promising, indicating its potential applicability beyond our custom dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2403.18933</link><description>&lt;p&gt;
SemEval&#20219;&#21153;1&#65306;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
SemEval Task 1: Semantic Textual Relatedness for African and Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18933
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#20219;&#21153;&#28041;&#21450;14&#31181;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65292;&#26088;&#22312;&#32771;&#23519;&#36328;&#35821;&#35328;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#20849;&#20139;&#20219;&#21153;&#12290;&#32780;&#20808;&#21069;&#30340;&#20849;&#20139;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#21017;&#35843;&#26597;&#20102;&#36328;&#36234;14&#31181;&#35821;&#35328;&#65288;&#21253;&#25324;&#21335;&#38750;&#33655;&#20848;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#21360;&#23612;&#35821;&#12289;&#22522;&#23612;&#20122;&#40065;&#23433;&#36798;&#35821;&#12289;&#39532;&#25289;&#22320;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#29616;&#20195;&#26631;&#20934;&#38463;&#25289;&#20271;&#35821;&#12289;&#26049;&#36974;&#26222;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#27888;&#21346;&#22266;&#35821;&#65289;&#30340;&#26356;&#24191;&#27867;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#35821;&#35328;&#26469;&#33258;&#20116;&#20010;&#19981;&#21516;&#30340;&#35821;&#31995;&#65292;&#24182;&#20027;&#35201;&#22312;&#38750;&#27954;&#21644;&#20122;&#27954;&#22320;&#21306;&#20351;&#29992;&#65292;&#36825;&#20123;&#22320;&#21306;&#30340;&#29305;&#28857;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#36164;&#28304;&#30340;&#30456;&#23545;&#26377;&#38480;&#12290;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#23454;&#20363;&#37117;&#26159;&#19968;&#20010;&#19982;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#21477;&#23545;&#65292;&#35813;&#20998;&#25968;&#34920;&#31034;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#31243;&#24230;&#12290;&#21442;&#19982;&#31995;&#32479;&#34987;&#35201;&#27714;&#22312;&#19977;&#20010;&#20027;&#35201;&#36712;&#36947;&#20013;&#30340;14&#31181;&#35821;&#35328;&#20013;&#25353;&#23427;&#20204;&#22312;&#24847;&#20041;&#19978;&#30340;&#25509;&#36817;&#31243;&#24230;&#65288;&#21363;&#23427;&#20204;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#31243;&#24230;&#65289;&#23545;&#21477;&#23545;&#36827;&#34892;&#25490;&#21517;&#65306;(a) &#30417;&#30563;&#65292;(b) &#26080;&#30417;&#30563;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18933v1 Announce Type: new  Abstract: We present the first shared task on Semantic Textual Relatedness (STR). While earlier shared tasks primarily focused on semantic similarity, we instead investigate the broader phenomenon of semantic relatedness across 14 languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu. These languages originate from five distinct language families and are predominantly spoken in Africa and Asia -- regions characterised by the relatively limited availability of NLP resources. Each instance in the datasets is a sentence pair associated with a score that represents the degree of semantic textual relatedness between the two sentences. Participating systems were asked to rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 languages in three main tracks: (a) supervised, (b) unsupervi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.18802</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Long-form factuality in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18802
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#21333;&#20010;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#65292;&#35780;&#20272;&#20107;&#23454;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#25193;&#23637;&#20102;F1&#20998;&#25968;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22238;&#31572;&#24320;&#25918;&#24615;&#20027;&#39064;&#30340;&#20107;&#23454;&#24615;&#25552;&#31034;&#26102;&#65292;&#32463;&#24120;&#29983;&#25104;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#22312;&#24320;&#25918;&#39046;&#22495;&#20013;&#23545;&#27169;&#22411;&#30340;&#38271;&#31687;&#20107;&#23454;&#24615;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;GPT-4&#29983;&#25104;&#20102;&#19968;&#20010;&#21517;&#20026;LongFact&#30340;&#25552;&#31034;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#25968;&#21315;&#20010;&#22218;&#25324;38&#20010;&#20027;&#39064;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;LLM&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;Search-Augmented Factuality Evaluator&#65288;SAFE&#65289;&#30340;&#26041;&#27861;&#20316;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#22120;&#12290;SAFE&#21033;&#29992;LLM&#23558;&#38271;&#31687;&#22238;&#24212;&#20998;&#35299;&#20026;&#19968;&#32452;&#21333;&#29420;&#30340;&#20107;&#23454;&#65292;&#24182;&#36890;&#36807;&#21457;&#36865;&#25628;&#32034;&#26597;&#35810;&#21040;Google&#25628;&#32034;&#20197;&#21450;&#30830;&#23450;&#19968;&#20010;&#20107;&#23454;&#26159;&#21542;&#24471;&#21040;&#25628;&#32034;&#32467;&#26524;&#25903;&#25345;&#30340;&#22810;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#35780;&#20272;&#27599;&#20010;&#20107;&#23454;&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#35758;&#23558;F1&#20998;&#25968;&#25193;&#23637;&#20026;&#38271;&#31687;&#20107;&#23454;&#24615;&#30340;&#32858;&#21512;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24179;&#34913;&#20102;&#22238;&#24212;&#20013;&#25903;&#25345;&#20107;&#23454;&#30340;&#30334;&#20998;&#27604;&#65288;&#31934;&#24230;&#65289;&#19982;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18802v1 Announce Type: cross  Abstract: Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model's long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MMIDR&#26694;&#26550;&#65292;&#29992;&#20110;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.14171</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#25945;&#25480;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
MMIDR: Teaching Large Language Model to Interpret Multimodal Misinformation via Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MMIDR&#26694;&#26550;&#65292;&#29992;&#20110;&#25945;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#35299;&#37322;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#30340;&#25991;&#26412;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#33258;&#21160;&#26816;&#27979;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#25496;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#20197;&#25104;&#26412;&#25928;&#30410;&#21644;&#26131;&#20110;&#35775;&#38382;&#30340;&#26041;&#24335;&#25945;&#23548;LLMs&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MMIDR&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25945;&#23548;LLMs&#20026;&#20854;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#27969;&#30021;&#21644;&#39640;&#36136;&#37327;&#25991;&#26412;&#35299;&#37322;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#23558;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#36716;&#21270;&#20026;&#36866;&#24403;&#30340;&#25351;&#20196;&#25191;&#34892;&#26684;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#25454;&#22686;&#24378;&#35270;&#35282;&#21644;&#31649;&#36947;&#12290;&#35813;&#31649;&#36947;&#21253;&#25324;&#19968;&#20010;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#27169;&#22359;&#21644;&#19968;&#20010;&#35777;&#25454;&#26816;&#32034;&#27169;&#22359;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22788;&#29702;&#36807;&#30340;&#20869;&#23481;&#25552;&#31034;&#19987;&#26377;&#30340;LLMs&#20026;&#35299;&#37322;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#25552;&#21462;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14171v1 Announce Type: new  Abstract: Automatic detection of multimodal misinformation has gained a widespread attention recently. However, the potential of powerful Large Language Models (LLMs) for multimodal misinformation detection remains underexplored. Besides, how to teach LLMs to interpret multimodal misinformation in cost-effective and accessible way is still an open question. To address that, we propose MMIDR, a framework designed to teach LLMs in providing fluent and high-quality textual explanations for their decision-making process of multimodal misinformation. To convert multimodal misinformation into an appropriate instruction-following format, we present a data augmentation perspective and pipeline. This pipeline consists of a visual information processing module and an evidence retrieval module. Subsequently, we prompt the proprietary LLMs with processed contents to extract rationales for interpreting the authenticity of multimodal misinformation. Furthermore
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Agent Group Chat&#27169;&#25311;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#25925;&#20107;&#24773;&#33410;&#19979;&#65292;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#20102;&#24847;&#26009;&#20043;&#22806;&#19988;&#37325;&#35201;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#36890;&#36807;&#35843;&#25972;&#29615;&#22659;&#35774;&#32622;&#21487;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.13433</link><description>&lt;p&gt;
&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#65306;&#19968;&#31181;&#20132;&#20114;&#24335;&#32676;&#32452;&#32842;&#22825;&#25311;&#30495;&#20307;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#24341;&#21457;&#38598;&#20307;&#26032;&#20852;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Agent Group Chat&#27169;&#25311;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#25925;&#20107;&#24773;&#33410;&#19979;&#65292;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#20102;&#24847;&#26009;&#20043;&#22806;&#19988;&#37325;&#35201;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#36890;&#36807;&#35843;&#25972;&#29615;&#22659;&#35774;&#32622;&#21487;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#35752;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#27169;&#25311;&#65292;&#27169;&#25311;&#22810;&#20195;&#29702;&#20043;&#38388;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#20132;&#20114;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#22312;&#35813;&#27169;&#25311;&#20013;&#33258;&#30001;&#32842;&#22825;&#65292;&#22522;&#20110;&#20854;&#35282;&#33394;&#35774;&#23450;&#36861;&#27714;&#21508;&#33258;&#30340;&#30446;&#30340;&#65292;&#26088;&#22312;&#35266;&#23519;&#20195;&#29702;&#20154;&#23637;&#29616;&#20986;&#26082;&#24847;&#26009;&#19981;&#21040;&#21448;&#26174;&#33879;&#30340;&#26032;&#20852;&#34892;&#20026;&#12290;&#23558;&#22235;&#20010;&#21465;&#20107;&#22330;&#26223;&#65288;&#32487;&#25215;&#20105;&#35758;&#12289;&#27861;&#24237;&#36777;&#35770;&#12289;&#21746;&#23398;&#36766;&#35828;&#12289;&#30005;&#24433;&#35282;&#33394;&#20105;&#35758;&#65289;&#25972;&#21512;&#21040;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#25903;&#25345;&#22810;&#26679;&#21270;&#25925;&#20107;&#24773;&#33410;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#20013;&#37197;&#32622;&#29305;&#23450;&#30340;&#29615;&#22659;&#35774;&#32622;&#65292;&#25105;&#20204;&#33021;&#22815;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#35282;&#33394;&#21457;&#35328;&#30340;&#25152;&#26377;&#20869;&#23481;&#30340;n-gram Shannon&#29109;&#26469;&#35780;&#20272;&#29615;&#22659;&#20013;&#30340;&#28151;&#20081;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20195;&#29702;&#20154;&#20855;&#26377;&#23376;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13433v1 Announce Type: cross  Abstract: To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing subs
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;</title><link>https://arxiv.org/abs/2403.09057</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#21307;&#30103;&#35760;&#24405;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Continued Pretrained LLM Approach for Automatic Medical Note Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09057
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#30103;&#35760;&#24405;&#29983;&#25104;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;LLM&#26041;&#27861;&#65292;&#22312;PubMedQA&#26041;&#38754;&#24615;&#33021;&#20248;&#20110;GPT-4&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36807;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#27491;&#22312;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#26368;&#24378;&#22823;&#30340;LLM&#23545;&#20110;&#22823;&#22810;&#25968;&#39046;&#22495;&#29305;&#23450;&#22330;&#26223;&#26469;&#35828;&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36830;&#32493;&#35757;&#32451;&#30340;130&#20159;&#21442;&#25968; Llama2-basd LLM&#65292;&#19987;&#20026;&#21307;&#30103;&#23545;&#35805;&#32780;&#35774;&#35745;&#65292;&#24182;&#22312;&#33258;&#21160;&#35760;&#24405;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;PubMedQA&#20013;&#30340;&#20934;&#30830;&#29575;&#39640;&#36798;76.6&#65285;&#65292;&#22312;&#24635;&#32467;&#21307;&#30103;&#23545;&#35805;&#20026;SOAP&#31508;&#35760;&#26041;&#38754;&#19982;GPT-4&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25429;&#25417;&#27491;&#30830;&#30340;&#21307;&#30103;&#27010;&#24565;&#26041;&#38754;&#36229;&#36807;&#20102;GPT-4&#65292;&#24182;&#19988;&#22312;&#27491;&#30830;&#24615;&#21644;&#23436;&#25972;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20154;&#31867;&#25220;&#20889;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09057v1 Announce Type: cross  Abstract: LLMs are revolutionizing NLP tasks. However, the most powerful LLM, like GPT-4, is too costly for most domain-specific scenarios. We present the first continuously trained 13B Llama2-based LLM that is purpose-built for medical conversations and measured on automated scribing. Our results show that our model outperforms GPT-4 in PubMedQA with 76.6\% accuracy and matches its performance in summarizing medical conversations into SOAP notes. Notably, our model exceeds GPT-4 in capturing a higher number of correct medical concepts and outperforms human scribes with higher correctness and completeness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.04182</link><description>&lt;p&gt;
&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Metric-aware LLM inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#33258;&#23450;&#20041;&#25351;&#26631;&#26469;&#25913;&#36827;&#25512;&#26029;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#36890;&#24120;&#65292;&#36755;&#20986;&#26159;&#36890;&#36807;&#20174;LLM&#30340;&#22522;&#30784;&#20998;&#24067;&#20013;&#36827;&#34892;&#33258;&#22238;&#24402;&#37319;&#26679;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#31181;&#25512;&#26029;&#31574;&#30053;&#23545;&#20110;&#19968;&#31995;&#21015;&#20219;&#21153;&#21644;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24230;&#37327;&#24863;&#30693;&#30340;LLM&#25512;&#26029;&#65306;&#19968;&#31181;&#22312;&#25512;&#26029;&#26102;&#38024;&#23545;&#33258;&#23450;&#20041;&#25351;&#26631;&#36827;&#34892;&#20248;&#21270;&#30340;&#20915;&#31574;&#29702;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#23398;&#26415;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21487;&#29992;&#27169;&#22411;&#19978;&#25253;&#21578;&#20102;&#30456;&#23545;&#22522;&#32447;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04182v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated strong results on a range of NLP tasks. Typically, outputs are obtained via autoregressive sampling from the LLM's underlying distribution. We show that this inference strategy can be suboptimal for a range of tasks and associated evaluation metrics. As a remedy, we propose metric aware LLM inference: a decision theoretic approach optimizing for custom metrics at inference time. We report improvements over baselines on academic benchmarks and publicly available models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02975</link><description>&lt;p&gt;
&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
A General and Flexible Multi-concept Parsing Framework for Multilingual Semantic Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02975
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#22810;&#27010;&#24565;&#35299;&#26512;&#26694;&#26550;&#29992;&#20110;&#22810;&#35821;&#35328;&#35821;&#20041;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#35782;&#21035;&#20197;&#21450;&#22806;&#37096;NER&#20381;&#36182;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#35821;&#20041;&#21305;&#37197;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#22312;&#31038;&#21306;&#38382;&#31572;&#12289;&#25628;&#32034;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#25512;&#33616;&#31561;&#21508;&#31181;&#37325;&#35201;&#22330;&#26223;&#20013;&#20855;&#26377;&#30456;&#24403;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DC-Match&#26469;&#35299;&#24320;&#21477;&#23376;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#21305;&#37197;&#24615;&#33021;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20808;&#36827;&#27169;&#22411;&#30452;&#25509;&#27169;&#25311;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#21333;&#35789;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#32780;&#24573;&#30053;&#20851;&#38190;&#35789;&#21644;&#24847;&#22270;&#27010;&#24565;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;DC-Match&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35821;&#20041;&#21305;&#37197;&#26041;&#27861;&#65292;&#20294;&#23427;&#39640;&#24230;&#20381;&#36182;&#22806;&#37096;NER&#25216;&#26415;&#26469;&#35782;&#21035;&#21477;&#23376;&#30340;&#20851;&#38190;&#35789;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#27425;&#35201;&#35821;&#35328;&#30340;&#35821;&#20041;&#21305;&#37197;&#24615;&#33021;&#65292;&#22240;&#20026;&#36890;&#24120;&#24456;&#38590;&#33719;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;NER&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02975v1 Announce Type: cross  Abstract: Sentence semantic matching is a research hotspot in natural language processing, which is considerably significant in various key scenarios, such as community question answering, searching, chatbot, and recommendation. Since most of the advanced models directly model the semantic relevance among words between two sentences while neglecting the \textit{keywords} and \textit{intents} concepts of them, DC-Match is proposed to disentangle keywords from intents and utilizes them to optimize the matching performance. Although DC-Match is a simple yet effective method for semantic matching, it highly depends on the external NER techniques to identify the keywords of sentences, which limits the performance of semantic matching for minor languages since satisfactory NER tools are usually hard to obtain. In this paper, we propose to generally and flexibly resolve the text into multi concepts for multilingual semantic matching to liberate the mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.01216</link><description>&lt;p&gt;
API&#23601;&#22815;&#20102;&#65306;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#20307;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
API Is Enough: Conformal Prediction for Large Language Models Without Logit-Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01216
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#38656;&#35775;&#38382;&#23545;&#25968;&#30340;API-only LLMs&#30340;&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#22823;&#23567;&#24182;&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#26080;&#27861;&#35775;&#38382;&#23545;&#25968;&#26102;&#22914;&#20309;&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36825;&#19968;&#26222;&#36941;&#25361;&#25112;&#12290;&#25972;&#20307;&#39044;&#27979;&#65288;CP&#65289;&#20197;&#20854;&#19982;&#27169;&#22411;&#26080;&#20851;&#21644;&#26080;&#38656;&#20998;&#24067;&#30340;&#29305;&#28857;&#32780;&#38395;&#21517;&#65292;&#26159;&#21508;&#31181;LLMs&#21644;&#25968;&#25454;&#20998;&#24067;&#30340;&#29702;&#24819;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;LLMs&#25972;&#20307;&#39044;&#27979;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#21487;&#20197;&#35775;&#38382;&#23545;&#25968;&#65292;&#36825;&#23545;&#20110;&#19968;&#20123;&#20165;&#25903;&#25345;API&#30340;LLMs&#26469;&#35828;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#27492;&#22806;&#65292;&#24050;&#30693;&#23545;&#25968;&#21487;&#33021;&#23384;&#22312;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#65292;&#21487;&#33021;&#23548;&#33268;&#25972;&#20307;&#39044;&#27979;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;CP&#26041;&#27861;&#65292;&#65288;1&#65289;&#19987;&#20026;&#26080;&#38656;&#23545;&#25968;&#35775;&#38382;&#30340;API-only LLMs&#37327;&#36523;&#23450;&#21046;; (2) &#26368;&#23567;&#21270;&#39044;&#27979;&#38598;&#30340;&#22823;&#23567;; &#20197;&#21450;(3)&#30830;&#20445;&#29992;&#25143;&#23450;&#20041;&#30340;&#35206;&#30422;&#33539;&#22260;&#20855;&#26377;&#32479;&#35745;&#20445;&#35777;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#31895;&#31890;&#24230;&#65288;&#20363;&#22914;&#65292;&#26679;&#26412;&#39057;&#29575;&#65289;&#21644;&#32454;&#31890;&#24230;&#19981;&#30830;&#23450;&#24615;&#27010;&#24565;&#65288;&#20363;&#22914;&#65292;&#35821;&#20041;&#30456;&#20284;&#24615;&#65289;&#26469;&#21046;&#23450;&#19981;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01216v1 Announce Type: cross  Abstract: This study aims to address the pervasive challenge of quantifying uncertainty in large language models (LLMs) without logit-access. Conformal Prediction (CP), known for its model-agnostic and distribution-free features, is a desired approach for various LLMs and data distributions. However, existing CP methods for LLMs typically assume access to the logits, which are unavailable for some API-only LLMs. In addition, logits are known to be miscalibrated, potentially leading to degraded CP performance. To tackle these challenges, we introduce a novel CP method that (1) is tailored for API-only LLMs without logit-access; (2) minimizes the size of prediction sets; and (3) ensures a statistical guarantee of the user-defined coverage. The core idea of this approach is to formulate nonconformity measures using both coarse-grained (i.e., sample frequency) and fine-grained uncertainty notions (e.g., semantic similarity). Experimental results on 
&lt;/p&gt;</description></item><item><title>BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14151</link><description>&lt;p&gt;
BIRCO&#65306;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BIRCO: A Benchmark of Information Retrieval Tasks with Complex Objectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14151
&lt;/p&gt;
&lt;p&gt;
BIRCO&#22522;&#20934;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#23545;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#30340;&#26816;&#32034;&#33021;&#21147;&#65292;&#21457;&#29616;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#21644;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26159;&#35299;&#20915;&#22797;&#26434;&#29992;&#25143;&#38656;&#27714;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#22797;&#26434;&#30446;&#26631;&#30340;&#20449;&#24687;&#26816;&#32034;(IR)&#20219;&#21153;&#22522;&#20934;(BIRCO)&#12290; BIRCO&#35780;&#20272;IR&#31995;&#32479;&#26681;&#25454;&#22810;&#26041;&#38754;&#29992;&#25143;&#30446;&#26631;&#26816;&#32034;&#25991;&#26723;&#30340;&#33021;&#21147;&#12290; &#35813;&#22522;&#20934;&#30340;&#22797;&#26434;&#24615;&#21644;&#32039;&#20945;&#22823;&#23567;&#20351;&#20854;&#36866;&#29992;&#20110;&#35780;&#20272;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#12290; &#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#30740;&#31350;&#21487;&#33021;&#24433;&#21709;LLM&#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#30340;&#22240;&#32032;&#65292;&#24182;&#30830;&#23450;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21644;&#26356;&#22797;&#26434;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290; &#27809;&#26377;&#19968;&#31181;&#26041;&#27861;&#22312;&#25152;&#26377;&#22522;&#20934;&#20219;&#21153;&#19978;&#22343;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#38656;&#35201;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#21644;&#26032;&#30340;&#26816;&#32034;&#21327;&#35758;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#29992;&#25143;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14151v1 Announce Type: cross  Abstract: We present the Benchmark of Information Retrieval (IR) tasks with Complex Objectives (BIRCO). BIRCO evaluates the ability of IR systems to retrieve documents given multi-faceted user objectives. The benchmark's complexity and compact size make it suitable for evaluating large language model (LLM)-based information retrieval systems. We present a modular framework for investigating factors that may influence LLM performance on retrieval tasks, and identify a simple baseline model which matches or outperforms existing approaches and more complex alternatives. No approach achieves satisfactory performance on all benchmark tasks, suggesting that stronger models and new retrieval protocols are necessary to address complex user needs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17377</link><description>&lt;p&gt;
&#26080;&#38480;-gram&#65306;&#23558;&#26080;&#38480;n-gram&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#19975;&#20159;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;
Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17377
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;n-gram&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#20197;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#35745;&#31639;&#20219;&#24847;n&#30340;n-gram&#27010;&#29575;&#65292;&#20351;&#24471;&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#25991;&#26412;&#36827;&#34892;&#26356;&#20934;&#30830;&#30340;&#20998;&#26512;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26102;&#20195;&#65292;n-gram&#35821;&#35328;&#27169;&#22411;&#36824;&#20855;&#26377;&#30456;&#20851;&#24615;&#21527;&#65311;&#25105;&#20204;&#30340;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#24182;&#19988;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#26512;&#21644;&#25913;&#36827;&#31070;&#32463;LLM&#26041;&#38754;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#22312;&#20004;&#20010;&#26041;&#38754;&#23545;n-gram&#27169;&#22411;&#36827;&#34892;&#29616;&#20195;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#31070;&#32463;LLM&#30456;&#21516;&#30340;&#25968;&#25454;&#35268;&#27169;&#35757;&#32451;- 1.4&#19975;&#20159;&#20010;&#26631;&#35760;&#12290;&#36825;&#26159;&#36804;&#20170;&#20026;&#27490;&#26500;&#24314;&#30340;&#26368;&#22823;&#30340;n-gram&#27169;&#22411;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;n-gram&#27169;&#22411;&#20351;&#29992;&#30340;n&#24456;&#23567;&#65292;&#36825;&#22952;&#30861;&#20102;&#23427;&#20204;&#30340;&#24615;&#33021;&#65307;&#30456;&#21453;&#65292;&#25105;&#20204;&#20801;&#35768;n&#21487;&#20197;&#26159;&#20219;&#24847;&#22823;&#30340;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#26080;&#38480;-gram LM&#19982;&#22238;&#36864;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;infini-gram&#30340;&#24341;&#25806;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#21518;&#32512;&#25968;&#32452;&#35745;&#31639;&#26080;&#38480;-gram&#65288;&#20197;&#21450;&#20219;&#24847;n&#30340;n-gram&#65289;&#27010;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#27627;&#31186;&#32423;&#30340;&#24310;&#36831;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#35745;&#31639;n-gram&#35745;&#25968;&#34920;&#65288;&#36825;&#23558;&#38750;&#24120;&#26114;&#36149;&#65289;&#12290;&#26080;&#38480;-gram&#26694;&#26550;&#21644;infini-gram&#24341;&#25806;&#20351;&#25105;&#20204;&#33021;&#22815;&#23545;&#20154;&#31867;&#20889;&#20316;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35768;&#22810;&#26032;&#39062;&#21644;&#26377;&#24847;&#24605;&#30340;&#20998;&#26512;&#65306;&#25105;&#20204;&#21457;&#29616;&#26080;&#38480;-gram LM...
&lt;/p&gt;
&lt;p&gt;
Are n-gram language models still relevant in this era of neural large language models (LLMs)? Our answer is yes, and we show their values in both text analysis and improving neural LLMs. Yet this necessitates modernizing n-gram models in two aspects. First, we train them at the same data scale as neural LLMs -- 1.4 trillion tokens. This is the largest n-gram model ever built. Second, existing n-gram models use small n which hinders their performance; we instead allow n to be arbitrarily large, by introducing a new $\infty$-gram LM with backoff. Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute $\infty$-gram (as well as n-gram with arbitrary n) probabilities with millisecond-level latency. The $\infty$-gram framework and infini-gram engine enable us to conduct many novel and interesting analyses of human-written and machine-generated text: we find that the $\infty$-gram LM 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15496</link><description>&lt;p&gt;
Baichuan2-Sum: &#20351;&#29992;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Baichuan2-Sum: Instruction Finetune Baichuan2-7B Model for Dialogue Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Baichuan2-Sum&#27169;&#22411;&#65292;&#36890;&#36807;&#25351;&#23548;&#24494;&#35843;Baichuan2-7B&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#25688;&#35201;&#65292;&#24182;&#24212;&#29992;NEFTune&#25216;&#26415;&#25913;&#36827;&#35757;&#32451;&#36807;&#31243;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;CSDS&#21644;SAMSUM&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;Llama&#12289;Baichuan&#21644;Bloom&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23545;&#35805;&#25688;&#35201;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#20026;&#23545;&#35805;&#20013;&#30340;&#19981;&#21516;&#35282;&#33394;&#29983;&#25104;&#25688;&#35201;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;&#23567;&#27169;&#22411;&#65288;&#20363;&#22914;Bart&#21644;Bert&#65289;&#36827;&#34892;&#30340;&#12290;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#22312;&#23567;&#27169;&#22411;&#19978;&#28155;&#21152;&#20219;&#21153;&#25351;&#23450;&#30340;&#20248;&#21270;&#65292;&#22914;&#21521;&#27169;&#22411;&#28155;&#21152;&#20840;&#23616;-&#23616;&#37096;&#20013;&#24515;&#24230;&#24471;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25351;&#23548;&#24494;&#35843;&#27169;&#22411;&#65306;Baichuan2-Sum&#65292;&#29992;&#20110;&#38754;&#21521;&#35282;&#33394;&#30340;&#23545;&#35805;&#25688;&#35201;&#12290;&#36890;&#36807;&#20026;&#19981;&#21516;&#35282;&#33394;&#35774;&#32622;&#19981;&#21516;&#30340;&#25351;&#20196;&#65292;&#27169;&#22411;&#21487;&#20197;&#20174;&#23545;&#35805;&#20132;&#20114;&#20013;&#23398;&#20064;&#24182;&#36755;&#20986;&#26399;&#26395;&#30340;&#25688;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;NEFTune&#25216;&#26415;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#21512;&#36866;&#30340;&#22122;&#22768;&#20197;&#25552;&#39640;&#32467;&#26524;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#20004;&#20010;&#20844;&#24320;&#30340;&#23545;&#35805;&#25688;&#35201;&#25968;&#25454;&#38598;CSDS&#21644;SAMSUM&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like Llama, Baichuan and Bloom models show remarkable ability with instruction fine-tuning in many natural language tasks. Nevertheless, for the dialogue summarization task, which aims to generate summaries for different roles in dialogue, most of the state-of-the-art methods conduct on small models (e.g Bart and Bert). Existing methods try to add task specified optimization on small models like adding global-local centrality score to models. In this paper, we propose an instruction fine-tuning model: Baichuan2-Sum, for role-oriented diaglouge summarization. By setting different instructions for different roles, the model can learn from the dialogue interactions and output the expected summaries. Furthermore, we applied NEFTune technique to add suitable noise during training to improve the results. The experiments demonstrate that the proposed model achieves the new state-of-the-art results on two public dialogue summarization datasets: CSDS and SAMSUM. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;TableInstruct&#21644;&#24320;&#21457;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#22312;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.09206</link><description>&lt;p&gt;
TableLlama&#65306;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#25918;&#22823;&#22411;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TableLlama: Towards Open Large Generalist Models for Tables
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#29992;&#20110;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26500;&#24314;&#26032;&#25968;&#25454;&#38598;TableInstruct&#21644;&#24320;&#21457;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#22312;&#34920;&#29616;&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#27604;&#25110;&#26356;&#22909;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#32467;&#26500;&#21270;&#34920;&#26684;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#23545;&#34920;&#26684;&#36827;&#34892;&#39044;&#35757;&#32451;&#25110;&#29305;&#27530;&#30340;&#27169;&#22411;&#26550;&#26500;&#35774;&#35745;&#65292;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#34920;&#26684;&#31867;&#22411;&#65292;&#25110;&#23545;&#34920;&#26684;&#21644;&#20219;&#21153;&#26377;&#31616;&#21270;&#30340;&#20551;&#35774;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20316;&#20026;&#21508;&#31181;&#22522;&#20110;&#34920;&#26684;&#20219;&#21153;&#30340;&#36890;&#29992;&#24037;&#20855;&#30340;&#31532;&#19968;&#27493;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#30495;&#23454;&#34920;&#26684;&#21644;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;TableInstruct&#65292;&#20197;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#21644;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;LongLoRA&#23545;Llama 2 (7B)&#36827;&#34892;&#24494;&#35843;&#65292;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#38754;&#21521;&#34920;&#26684;&#30340;&#24320;&#28304;&#36890;&#29992;&#27169;&#22411;TableLlama&#65292;&#20197;&#24212;&#23545;&#38271;&#19978;&#19979;&#25991;&#25361;&#25112;&#12290;&#25105;&#20204;&#22312;&#21516;&#39046;&#22495;&#21644;&#36328;&#39046;&#22495;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#22312;8&#20010;&#21516;&#39046;&#22495;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#20013;&#65292;TableLlama&#22312;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#19982;SOT&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09206v2 Announce Type: replace  Abstract: Semi-structured tables are ubiquitous. There has been a variety of tasks that aim to automatically interpret, augment, and query tables. Current methods often require pretraining on tables or special model architecture design, are restricted to specific table types, or have simplifying assumptions about tables and tasks. This paper makes the first step towards developing open-source large language models (LLMs) as generalists for a diversity of table-based tasks. Towards that end, we construct TableInstruct, a new dataset with a variety of realistic tables and tasks, for instruction tuning and evaluating LLMs. We further develop the first open-source generalist model for tables, TableLlama, by fine-tuning Llama 2 (7B) with LongLoRA to address the long context challenge. We experiment under both in-domain setting and out-of-domain setting. On 7 out of 8 in-domain tasks, TableLlama achieves comparable or better performance than the SOT
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;</title><link>https://arxiv.org/abs/2310.00492</link><description>&lt;p&gt;
&#20174;&#35821;&#35328;&#24314;&#27169;&#21040;&#25351;&#20196;&#36319;&#38543;&#65306;&#29702;&#35299;&#25351;&#20196;&#35843;&#25972;&#21518;LLMs&#20013;&#34892;&#20026;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
From Language Modeling to Instruction Following: Understanding the Behavior Shift in LLMs after Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00492
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#23545;LLMs&#20135;&#29983;&#20102;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65307;2&#65289;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;&#30340;&#19981;&#26029;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20854;&#20013;&#25351;&#20196;&#35843;&#25972;&#26159;&#23558;LLMs&#19982;&#29992;&#25143;&#24847;&#22270;&#23545;&#40784;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25351;&#20196;&#35843;&#25972;&#22914;&#20309;&#35843;&#25972;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#20869;&#22312;&#21464;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#20960;&#31181;&#26412;&#22320;&#21644;&#20840;&#23616;&#35299;&#37322;&#26041;&#27861;&#65292;&#21253;&#25324;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#36755;&#20837;&#36755;&#20986;&#24402;&#22240;&#26041;&#27861;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#37322;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#20013;&#30340;&#27169;&#24335;&#21644;&#27010;&#24565;&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#20174;&#39044;&#35757;&#32451;&#21644;&#25351;&#20196;&#35843;&#25972;&#27169;&#22411;&#20013;&#24471;&#20986;&#30340;&#35299;&#37322;&#26469;&#30740;&#31350;&#25351;&#20196;&#35843;&#25972;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20154;&#21487;&#29702;&#35299;&#30340;&#27700;&#24179;&#19978;&#25552;&#20379;&#20102;&#27169;&#22411;&#36716;&#21464;&#30340;&#20869;&#37096;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#25351;&#20196;&#35843;&#25972;&#30340;&#19977;&#20010;&#37325;&#35201;&#24433;&#21709;&#65306;1&#65289;&#23427;&#20351;LLMs&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#25351;&#20196;&#37096;&#20998;&#65292;&#24182;&#19981;&#26029;&#20419;&#36827;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00492v2 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have achieved remarkable success, where instruction tuning is the critical step in aligning LLMs with user intentions. In this work, we investigate how the instruction tuning adjusts pre-trained models with a focus on intrinsic changes. Specifically, we first develop several local and global explanation methods, including a gradient-based method for input-output attribution and techniques for interpreting patterns and concepts in self-attention and feed-forward layers. The impact of instruction tuning is then studied by comparing the explanations derived from the pre-trained and instruction-tuned models. This approach provides an internal perspective of the model shifts on a human-comprehensible level. Our findings reveal three significant impacts of instruction tuning: 1) It empowers LLMs to recognize the instruction parts from user prompts, and promotes the response generation constantly condition
&lt;/p&gt;</description></item><item><title>RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.09432</link><description>&lt;p&gt;
RoleCraft-GLM&#65306;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09432
&lt;/p&gt;
&lt;p&gt;
RoleCraft-GLM&#26159;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#65292;&#35299;&#20915;&#20102;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#21644;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#23427;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#30340;&#23545;&#35805;&#65292;&#25552;&#21319;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;RoleCraft-GLM&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22686;&#24378;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;RoleCraft-GLM&#35299;&#20915;&#20102;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#32570;&#20047;&#20010;&#24615;&#21270;&#20114;&#21160;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#35814;&#32454;&#25551;&#32472;&#24773;&#24863;&#32454;&#33147;&#30340;&#35282;&#33394;&#21051;&#30011;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#20174;&#20256;&#32479;&#30340;&#20197;&#21517;&#20154;&#20026;&#20013;&#24515;&#30340;&#35282;&#33394;&#36716;&#21464;&#20026;&#22810;&#26679;&#21270;&#30340;&#38750;&#21517;&#20154;&#35282;&#33394;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#35821;&#35328;&#24314;&#27169;&#20114;&#21160;&#30340;&#30495;&#23454;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#32454;&#33268;&#20837;&#24494;&#30340;&#35282;&#33394;&#21457;&#23637;&#65292;&#30830;&#20445;&#23545;&#35805;&#26082;&#30495;&#23454;&#21448;&#24773;&#24863;&#20849;&#40483;&#12290;&#36890;&#36807;&#22810;&#20010;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;RoleCraft-GLM&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#23427;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#33021;&#22815;&#20934;&#30830;&#21453;&#26144;&#35282;&#33394;&#30340;&#20010;&#24615;&#29305;&#24449;&#21644;&#24773;&#24863;&#65292;&#20174;&#32780;&#22686;&#24378;&#29992;&#25143;&#21442;&#19982;&#24230;&#12290;&#24635;&#20043;&#65292;RoleCraft-GLM&#26631;&#24535;&#30528;&#19968;&#20010;&#21019;&#26032;&#30340;&#37324;&#31243;&#30865;&#65292;&#25512;&#21160;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20010;&#24615;&#21270;&#35282;&#33394;&#25198;&#28436;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study presents RoleCraft-GLM, an innovative framework aimed at enhancing personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM addresses the key issue of lacking personalized interactions in conversational AI, and offers a solution with detailed and emotionally nuanced character portrayals. We contribute a unique conversational dataset that shifts from conventional celebrity-centric characters to diverse, non-celebrity personas, thus enhancing the realism and complexity of language modeling interactions. Additionally, our approach includes meticulous character development, ensuring dialogues are both realistic and emotionally resonant. The effectiveness of RoleCraft-GLM is validated through various case studies, highlighting its versatility and skill in different scenarios. Our framework excels in generating dialogues that accurately reflect characters' personality traits and emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks a sign
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#36827;&#34892;&#32929;&#31080;&#36873;&#25321;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#28304;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20379;&#20855;&#26377;&#21487;&#34892;&#35299;&#37322;&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2401.03737</link><description>&lt;p&gt;
&#33021;&#21542;&#25171;&#36133;&#21326;&#23572;&#34903;&#65311;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#22312;&#32929;&#31080;&#36873;&#25321;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection. (arXiv:2401.03737v1 [q-fin.CP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#36827;&#34892;&#32929;&#31080;&#36873;&#25321;&#30340;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#31181;&#25968;&#25454;&#28304;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#20379;&#20855;&#26377;&#21487;&#34892;&#35299;&#37322;&#30340;&#25237;&#36164;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#24066;&#22330;&#21160;&#24577;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#29615;&#22659;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;MarketSenseAI&#65292;&#19968;&#20010;&#21033;&#29992;GPT-4&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21487;&#25193;&#23637;&#32929;&#31080;&#36873;&#25321;&#30340;&#26032;&#22411;&#20154;&#24037;&#26234;&#33021;&#26694;&#26550;&#12290;MarketSenseAI&#25972;&#21512;&#20102;&#8220;&#24605;&#32500;&#38142;&#8221;&#21644;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#26041;&#27861;&#65292;&#20998;&#26512;&#21253;&#25324;&#24066;&#22330;&#20215;&#26684;&#21160;&#24577;&#12289;&#36130;&#32463;&#26032;&#38395;&#12289;&#20844;&#21496;&#22522;&#26412;&#38754;&#21644;&#23439;&#35266;&#32463;&#27982;&#25253;&#21578;&#31561;&#22810;&#31181;&#25968;&#25454;&#28304;&#65292;&#27169;&#20223;&#30693;&#21517;&#37329;&#34701;&#25237;&#36164;&#22242;&#38431;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25991;&#31456;&#35814;&#32454;&#20171;&#32461;&#20102;MarketSenseAI&#30340;&#24320;&#21457;&#12289;&#23454;&#26045;&#21644;&#23454;&#35777;&#39564;&#35777;&#65292;&#37325;&#28857;&#20851;&#27880;&#20854;&#25552;&#20379;&#20855;&#26377;&#20805;&#20998;&#35299;&#37322;&#25903;&#25745;&#30340;&#21487;&#34892;&#25237;&#36164;&#20449;&#21495;&#65288;&#20080;&#20837;&#12289;&#25345;&#26377;&#12289;&#21334;&#20986;&#65289;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#30340;&#19968;&#20010;&#26174;&#33879;&#29305;&#28857;&#26159;&#20351;&#29992;GPT-4&#19981;&#20165;&#20316;&#20026;&#39044;&#27979;&#24037;&#20855;&#65292;&#36824;&#20316;&#20026;&#35780;&#20272;&#22120;&#65292;&#25581;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#35299;&#37322;&#23545;&#25152;&#24314;&#35758;&#30340;&#25237;&#36164;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#21644;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
In the dynamic and data-driven landscape of financial markets, this paper introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI incorporates Chain of Thought and In-Context Learning methodologies to analyze a wide array of data sources, including market price dynamics, financial news, company fundamentals, and macroeconomic reports emulating the decision making process of prominent financial investment teams. The development, implementation, and empirical validation of MarketSenseAI are detailed, with a focus on its ability to provide actionable investment signals (buy, hold, sell) backed by cogent explanations. A notable aspect of this study is the use of GPT-4 not only as a predictive tool but also as an evaluator, revealing the significant impact of the AI-generated explanations on the reliability and acceptance of the suggested investment signals. In an extensive empirical evaluation
&lt;/p&gt;</description></item><item><title>GPT-who&#26159;&#19968;&#31181;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;GPT-who&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.06202</link><description>&lt;p&gt;
GPT-who&#65306;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#23494;&#24230;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
GPT-who: An Information Density-based Machine-Generated Text Detector. (arXiv:2310.06202v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06202
&lt;/p&gt;
&lt;p&gt;
GPT-who&#26159;&#19968;&#31181;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#65292;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#26469;&#24314;&#27169;&#27599;&#20010;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#22312;&#22810;&#20010;&#39046;&#22495;&#20013;&#65292;GPT-who&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65292;&#19988;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#35748;&#20026;&#20154;&#31867;&#22312;&#35821;&#35328;&#20135;&#29983;&#36807;&#31243;&#20013;&#21916;&#27426;&#24179;&#22343;&#20998;&#24067;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#25429;&#25417;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#20154;&#31867;&#29983;&#25104;&#25991;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GPT-who&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#35821;&#35328;&#23398;&#30340;&#22810;&#31867;&#39046;&#22495;&#19981;&#21487;&#30693;&#32479;&#35745;&#26816;&#27979;&#22120;&#12290;&#35813;&#26816;&#27979;&#22120;&#21033;&#29992;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#30340;&#29305;&#24449;&#24314;&#27169;&#27599;&#20010;LLM&#21644;&#20154;&#31867;&#20316;&#32773;&#30340;&#29420;&#29305;&#32479;&#35745;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#20316;&#32773;&#24402;&#23646;&#12290;&#25105;&#20204;&#20351;&#29992;4&#20010;&#22823;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;GPT-who&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#22120;&#65288;&#21253;&#25324;&#22522;&#20110;&#32479;&#35745;&#21644;&#38750;&#32479;&#35745;&#30340;&#65289;&#65292;&#22914;GLTR&#65292;GPTZero&#65292;OpenAI detector&#21644;ZeroGPT&#36229;&#36807;20&#65285;&#12290;&#38500;&#20102;&#24615;&#33021;&#20248;&#36234;&#22806;&#65292;GPT-who&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#65292;&#24182;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#20110;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#30340;&#34920;&#31034;&#30340;&#26368;&#22823;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Uniform Information Density principle posits that humans prefer to spread information evenly during language production. In this work, we examine if the UID principle can help capture differences between Large Language Models (LLMs) and human-generated text. We propose GPT-who, the first psycholinguistically-aware multi-class domain-agnostic statistical-based detector. This detector employs UID-based features to model the unique statistical signature of each LLM and human author for accurate authorship attribution. We evaluate our method using 4 large-scale benchmark datasets and find that GPT-who outperforms state-of-the-art detectors (both statistical- &amp; non-statistical-based) such as GLTR, GPTZero, OpenAI detector, and ZeroGPT by over $20$% across domains. In addition to superior performance, it is computationally inexpensive and utilizes an interpretable representation of text articles. We present the largest analysis of the UID-based representations of human and machine-genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11696</link><description>&lt;p&gt;
&#24102;&#26377;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#21327;&#20316;&#30340;&#22686;&#24378;&#35760;&#24518;&#22411;LLM&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Memory-Augmented LLM Personalization with Short- and Long-Term Memory Coordination. (arXiv:2309.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT3.5&#65292;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#38750;&#20010;&#24615;&#21270;&#29983;&#25104;&#26041;&#24335;&#21487;&#33021;&#23548;&#33268;&#29992;&#25143;&#29305;&#23450;&#32467;&#26524;&#30340;&#20122;&#20248;&#21270;&#12290;&#36890;&#24120;&#65292;&#29992;&#25143;&#26681;&#25454;&#33258;&#24049;&#30340;&#30693;&#35782;&#21644;&#20559;&#22909;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#36827;&#34892;&#23545;&#35805;&#12290;&#36825;&#23601;&#38656;&#35201;&#22686;&#24378;&#38754;&#21521;&#29992;&#25143;&#30340;LLM&#30340;&#20219;&#21153;&#65292;&#20294;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25506;&#32034;&#20102;&#22522;&#20110;&#35760;&#24518;&#30340;&#26041;&#27861;&#26469;&#23384;&#20648;&#21644;&#26816;&#32034;&#30693;&#35782;&#65292;&#20197;&#22686;&#24378;&#29983;&#25104;&#32780;&#26080;&#38656;&#20026;&#26032;&#30340;&#26597;&#35810;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#20351;&#29992;&#35760;&#24518;&#27169;&#22359;&#26080;&#27861;&#29702;&#35299;&#29992;&#25143;&#30340;&#20559;&#22909;&#65292;&#24182;&#19988;&#23436;&#20840;&#35757;&#32451;&#19968;&#20010;LLM&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35745;&#31639;&#20223;&#29983;&#35760;&#24518;&#26426;&#21046;&#65292;&#37197;&#22791;&#20102;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#27169;&#24335;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;LLMs&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as GPT3.5, have exhibited remarkable proficiency in comprehending and generating natural language. However, their unpersonalized generation paradigm may result in suboptimal user-specific outcomes. Typically, users converse differently based on their knowledge and preferences. This necessitates the task of enhancing user-oriented LLM which remains unexplored. While one can fully train an LLM for this objective, the resource consumption is unaffordable. Prior research has explored memory-based methods to store and retrieve knowledge to enhance generation without retraining for new queries. However, we contend that a mere memory module is inadequate to comprehend a user's preference, and fully training an LLM can be excessively costly. In this study, we propose a novel computational bionic memory mechanism, equipped with a parameter-efficient fine-tuning schema, to personalize LLMs. Our extensive experimental results demonstrate the effectiveness and su
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.08833</link><description>&lt;p&gt;
CMB&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
CMB: A Comprehensive Medical Benchmark in Chinese. (arXiv:2308.08833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08833
&lt;/p&gt;
&lt;p&gt;
CMB&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#20013;&#25991;&#21307;&#23398;&#22522;&#20934;&#65292;&#22522;&#20110;&#20013;&#22269;&#26412;&#22303;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#35774;&#35745;&#65292;&#33021;&#22815;&#35299;&#20915;&#23558;&#33521;&#35821;&#21307;&#23398;&#35780;&#20272;&#32763;&#35793;&#21040;&#26412;&#22320;&#29615;&#22659;&#20013;&#30340;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#22312;&#21307;&#23398;&#39046;&#22495;&#21462;&#24471;&#37325;&#22823;&#31361;&#30772;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#24314;&#31435;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#21307;&#23398;&#22522;&#20934;&#25104;&#20026;&#34913;&#37327;&#36827;&#23637;&#30340;&#22522;&#30707;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#22320;&#21306;&#30340;&#21307;&#23398;&#29615;&#22659;&#20855;&#26377;&#21508;&#33258;&#30340;&#29305;&#28857;&#65292;&#20363;&#22914;&#22312;&#20013;&#22269;&#22659;&#20869;&#20256;&#32479;&#20013;&#21307;&#30340;&#26222;&#36941;&#24615;&#21644;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#20165;&#20165;&#32763;&#35793;&#22522;&#20110;&#33521;&#35821;&#30340;&#21307;&#23398;&#35780;&#20272;&#21487;&#33021;&#23548;&#33268;&#24403;&#22320;&#29615;&#22659;&#20013;&#30340;&#8220;&#19978;&#19979;&#25991;&#19981;&#19968;&#33268;&#8221;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CMB&#65288;Comprehensive Medical Benchmark in Chinese&#65289;&#30340;&#26412;&#22320;&#21270;&#21307;&#23398;&#22522;&#20934;&#65292;&#23436;&#20840;&#35774;&#35745;&#21644;&#26681;&#26893;&#20110;&#20013;&#22269;&#26412;&#22303;&#30340;&#35821;&#35328;&#21644;&#25991;&#21270;&#26694;&#26550;&#12290;&#23613;&#31649;&#20256;&#32479;&#20013;&#21307;&#26159;&#36825;&#20010;&#35780;&#20272;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23427;&#24182;&#19981;&#26500;&#25104;&#20854;&#20840;&#37096;&#12290;&#20351;&#29992;&#36825;&#20010;&#22522;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#20010;&#30693;&#21517;&#30340;&#22823;&#35268;&#27169;LLMs&#65292;&#21253;&#25324;ChatGPT&#12289;GPT-4&#12289;&#19987;&#38376;&#30340;&#20013;&#25991;LLMs&#21644;&#19987;&#38376;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) provide a possibility to make a great breakthrough in medicine. The establishment of a standardized medical benchmark becomes a fundamental cornerstone to measure progression. However, medical environments in different regions have their local characteristics, e.g., the ubiquity and significance of traditional Chinese medicine within China. Therefore, merely translating English-based medical evaluation may result in \textit{contextual incongruities} to a local region. To solve the issue, we propose a localized medical benchmark called CMB, a Comprehensive Medical Benchmark in Chinese, designed and rooted entirely within the native Chinese linguistic and cultural framework. While traditional Chinese medicine is integral to this evaluation, it does not constitute its entirety. Using this benchmark, we have evaluated several prominent large-scale LLMs, including ChatGPT, GPT-4, dedicated Chinese LLMs, and LLMs specialized in the medical domain. It is worth not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19358</link><description>&lt;p&gt;
&#31283;&#20581;&#30340;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Stable Anisotropic Regularization. (arXiv:2305.19358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;I-STAR&#65292;&#21487;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#65292;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#32452;&#21512;&#34920;&#31034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25104;&#21151;&#65292;&#30740;&#31350;&#27169;&#22411;&#28608;&#27963;&#30340;&#23646;&#24615;&#24050;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25991;&#29486;&#26222;&#36941;&#35748;&#20026;LLMs&#34920;&#31034;&#30001;&#23569;&#25968;&#20855;&#26377;&#26497;&#39640;&#26041;&#24046;&#21644;&#24133;&#24230;&#30340;&#8220;&#24322;&#24120;&#32500;&#24230;&#8221;&#20027;&#23548;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#20960;&#39033;&#30740;&#31350;&#35797;&#22270;&#20943;&#36731;&#36825;&#20123;&#24322;&#24120;&#32500;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#36843;&#20351;LLMs&#25104;&#20026;&#21508;&#21521;&#21516;&#24615;&#65288;&#21363;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#25152;&#26377;&#32500;&#24230;&#20855;&#26377;&#22343;&#21248;&#26041;&#24046;&#65289;&#30340;&#12290;&#21508;&#21521;&#21516;&#24615;&#34987;&#35748;&#20026;&#26159;LLMs&#30340;&#19968;&#31181;&#29702;&#24819;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#24182;&#26356;&#21152;&#36148;&#36817;&#20154;&#31867;&#30452;&#35273;&#30340;&#25991;&#26412;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;NLP&#20013;&#21508;&#21521;&#21516;&#24615;&#30340;&#35768;&#22810;&#35266;&#28857;&#37117;&#26159;&#22522;&#20110;&#23884;&#20837;&#30340;&#24179;&#22343;&#20313;&#24358;&#30456;&#20284;&#24230;&#65292;&#26368;&#36817;&#24050;&#32463;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#26377;&#32570;&#38519;&#30340;&#21508;&#21521;&#21516;&#24615;&#24230;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;I-STAR&#65306;&#22522;&#20110;IsoScore$^{\star}$&#30340;&#31283;&#23450;&#21508;&#21521;&#24322;&#24615;&#27491;&#21017;&#21270;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#23450;&#24615;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the success of Large Language Models (LLMs), there has been considerable interest in studying the properties of model activations. The literature overwhelmingly agrees that LLM representations are dominated by a few ``outlier dimensions'' with exceedingly high variance and magnitude. Several studies in Natural Language Processing (NLP) have sought to mitigate the impact of such outlier dimensions and force LLMs to be isotropic (i.e., have uniform variance across all dimensions in embedding space). Isotropy is thought to be a desirable property for LLMs that improves model performance and more closely aligns textual representations with human intuition. However, many of the claims regarding isotropy in NLP have been based on the average cosine similarity of embeddings, which has recently been shown to be a flawed measure of isotropy. In this paper, we propose I-STAR: IsoScore$^{\star}$-based STable Anisotropic Regularization, a novel regularization method that can be used to incre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2304.14364</link><description>&lt;p&gt;
CONSCENDI: &#19968;&#31181;&#21453;&#23545;&#27604;&#19988;&#22330;&#26223;&#24341;&#23548;&#30340;&#33976;&#39311;&#26041;&#27861;&#26469;&#20026;&#34394;&#25311;&#21161;&#25163;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CONSCENDI: A Contrastive and Scenario-Guided Distillation Approach to Guardrail Models for Virtual Assistants. (arXiv:2304.14364v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CONSCENDI&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20219;&#21153;&#22411;&#34394;&#25311;&#21161;&#25163;&#30340;&#36755;&#20986;&#12290;&#20851;&#38190;&#26041;&#27861;&#21253;&#25324;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#20195;&#29702;&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GPT-4&#31561;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26032;&#19968;&#20195;&#30340;&#22522;&#20110;&#20219;&#21153;&#30340;&#34394;&#25311;&#21161;&#25163;&#24212;&#36816;&#32780;&#29983;&#12290;&#36825;&#20123;&#23545;&#35805;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#23458;&#25143;&#30340;&#20855;&#20307;&#29992;&#20363;&#36827;&#34892;&#23450;&#21046;&#65292;&#20294;&#30830;&#20445;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#20165;&#31526;&#21512;&#25552;&#31034;&#25351;&#20196;&#20013;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#32842;&#22825;&#26426;&#22120;&#20154;&#35774;&#35745;&#24072;&#36890;&#24120;&#20351;&#29992;&#21478;&#19968;&#20010;&#31216;&#20026;&#38450;&#25252;&#26639;&#27169;&#22411;&#30340;&#27169;&#22411;&#26469;&#39564;&#35777;&#20195;&#29702;&#36755;&#20986;&#26159;&#21542;&#19982;&#20854;&#35268;&#21017;&#21644;&#32422;&#26463;&#23545;&#40784;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#33976;&#39311;&#26041;&#27861;&#26469;&#26500;&#24314;&#38450;&#25252;&#26639;&#27169;&#22411;&#65292;&#20197;&#30417;&#25511;&#20351;&#29992;GPT-4&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#31532;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;CONSCENDI&#36807;&#31243;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#22330;&#26223;&#22686;&#24378;&#29983;&#25104;&#21644;&#23545;&#27604;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#29983;&#25104;&#23545;&#35805;&#25968;&#25454;&#26102;&#65292;&#25105;&#20204;&#20250;&#29983;&#25104;&#19968;&#32452;&#36829;&#21453;&#35268;&#21017;&#30340;&#22330;&#26223;&#65292;&#36825;&#20123;&#22330;&#26223;&#21015;&#20030;&#20102;&#36829;&#21453;&#35268;&#21017;&#30340;&#22810;&#26679;&#21270;&#39640;&#32423;&#26041;&#24335;&#12290;&#36825;&#31181;&#22330;&#26223;&#24341;&#23548;&#26041;&#27861;&#20135;&#29983;&#20102;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#36829;&#21453;&#35268;&#21017;&#30340;&#23545;&#35805;&#35757;&#32451;&#38598;&#65292;&#24182;&#19988;&#23427;&#20351;&#24471;&#27169;&#22411;&#26356;&#23481;&#26131;&#26816;&#27979;&#21040;&#20195;&#29702;&#29983;&#25104;&#30340;&#25991;&#26412;&#26159;&#21542;&#31526;&#21512;&#35774;&#35745;&#32773;&#25351;&#23450;&#30340;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wave of new task-based virtual assistants has been fueled by increasingly powerful large language models, such as GPT-4. These conversational agents can be customized to serve customer-specific use cases, but ensuring that agent-generated text conforms to designer-specified rules included in prompt instructions alone is challenging. Therefore, chatbot designers often use another model, called a guardrail model, to verify that the agent output aligns with their rules and constraints. We explore using a distillation approach to guardrail models to monitor the output of the first model using training data from GPT-4. We find two crucial steps to our CONSCENDI process: scenario-augmented generation and contrastive training examples. When generating conversational data, we generate a set of rule-breaking scenarios, which enumerate a diverse set of high-level ways a rule can be violated. This scenario-guided approach produces a diverse training set of rule-violating conversations, and it p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23558;&#20998;&#26512;&#21644;&#25968;&#20540;&#25512;&#23548;&#32467;&#21512;&#65292;&#22312;&#22522;&#20110;&#24191;&#20041; Potts &#27169;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#23545;&#32463;&#36807;&#25913;&#36827;&#36866;&#24212;&#36825;&#31181;&#27169;&#22411;&#30340;self-attention&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#32463;&#36807;&#20462;&#25913;&#30340;self-attention&#26426;&#21046;&#21487;&#20197;&#22312;&#26497;&#38480;&#37319;&#26679;&#19979;&#20934;&#30830;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#36825;&#20010;&#8220;&#20998;&#35299;&#8221;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07235</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21333;&#23618;Transformer&#23545;&#24191;&#20041;Potts&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07235
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20998;&#26512;&#21644;&#25968;&#20540;&#25512;&#23548;&#32467;&#21512;&#65292;&#22312;&#22522;&#20110;&#24191;&#20041; Potts &#27169;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#23545;&#32463;&#36807;&#25913;&#36827;&#36866;&#24212;&#36825;&#31181;&#27169;&#22411;&#30340;self-attention&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#32463;&#36807;&#20462;&#25913;&#30340;self-attention&#26426;&#21046;&#21487;&#20197;&#22312;&#26497;&#38480;&#37319;&#26679;&#19979;&#20934;&#30830;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#36825;&#20010;&#8220;&#20998;&#35299;&#8221;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#34507;&#30333;&#36136;&#31185;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36341;&#19978;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#26159;&#19968;&#20010;&#21483;&#20570;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#21046;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#21477;&#23376;&#20013;&#32570;&#22833;&#30340;&#35789;&#12290;&#23613;&#31649;Transformer&#22312;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#23454;&#36341;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#31350;&#31455;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#20197;&#21450;&#23427;&#26159;&#24590;&#20040;&#20570;&#21040;&#30340;&#36824;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;&#20174;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#20301;&#32622;&#21644; Potts &#39068;&#33394;&#20013;&#25552;&#21462;&#30340;&#25968;&#25454;&#22312;&#35757;&#32451;&#30340;Transformer&#19978;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#34429;&#28982;&#19968;&#33324;&#30340;transformer&#38656;&#35201;&#22810;&#23618;&#23398;&#20064;&#25165;&#33021;&#20934;&#30830;&#23398;&#20064;&#36825;&#20010;&#20998;&#24067;&#65292;&#20294;&#26159;&#32463;&#36807;&#23567;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#26080;&#38480;&#37319;&#26679;&#30340;&#26497;&#38480;&#19979;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35745;&#31639;&#20102;&#36825;&#20010;&#20462;&#25913;&#21518;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25152;&#35859;&#8220;&#20998;&#35299;&#8221;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#25968;&#20540;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#37322;Transformer&#30340;&#20869;&#22312;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the type of neural networks that has revolutionised natural language processing and protein science. Their key building block is a mechanism called self-attention which is trained to predict missing words in sentences. Despite the practical success of transformers in applications it remains unclear what self-attention learns from data, and how. Here, we give a precise analytical and numerical characterisation of transformers trained on data drawn from a generalised Potts model with interactions between sites and Potts colours. While an off-the-shelf transformer requires several layers to learn this distribution, we show analytically that a single layer of self-attention with a small modification can learn the Potts model exactly in the limit of infinite sampling. We show that this modified self-attention, that we call ``factored'', has the same functional form as the conditional probability of a Potts spin given the other spins, compute its generalisation error using t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#21644;&#25512;&#24191;&#19981;&#21516;&#32467;&#26500;&#31243;&#24230;&#30340;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.12239</link><description>&lt;p&gt;
&#32534;&#31243;&#20160;&#20040;&#20351;&#19968;&#31181;&#35821;&#35328;&#26131;&#20110;&#28145;&#24230;&#23398;&#20064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Makes a Language Easy to Deep-Learn?. (arXiv:2302.12239v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#21644;&#20154;&#31867;&#22312;&#23398;&#20064;&#21644;&#25512;&#24191;&#19981;&#21516;&#32467;&#26500;&#31243;&#24230;&#30340;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#36825;&#23545;&#20110;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25104;&#21151;&#12290;&#35821;&#35328;&#30340;&#19968;&#20010;&#22522;&#26412;&#23646;&#24615;&#26159;&#20854;&#32452;&#25104;&#32467;&#26500;&#65292;&#20351;&#20154;&#31867;&#33021;&#22815;&#31995;&#32479;&#22320;&#20135;&#29983;&#26032;&#30340;&#24847;&#20041;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#31995;&#32479;&#21270;&#27010;&#25324;&#26041;&#38754;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26032;&#20852;&#36890;&#20449;&#27169;&#25311;&#20013;&#19981;&#19968;&#23450;&#21463;&#30410;&#20110;&#32452;&#25104;&#32467;&#26500;&#12290;&#36825;&#23545;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#25311;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#21644;&#36827;&#21270;&#26500;&#25104;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#19988;&#26263;&#31034;&#20102;&#19981;&#21516;&#23398;&#20064;&#31995;&#32479;&#30340;&#20559;&#35265;&#30340;&#20851;&#38190;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30452;&#25509;&#27979;&#35797;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#21644;&#27010;&#25324;&#19981;&#21516;&#36755;&#20837;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#36825;&#20123;&#35821;&#35328;&#22312;&#20854;&#32467;&#26500;&#31243;&#24230;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-3.5&#65288;&#31867;&#20284;&#20110;&#25104;&#24180;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#21644;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;&#31867;&#20284;&#20110;&#20799;&#31461;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#30340;&#35760;&#24518;&#21644;&#27010;&#25324;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#20102;&#20196;&#20154;&#38663;&#24778;&#30340;
&lt;/p&gt;
&lt;p&gt;
Neural networks drive the success of natural language processing. A fundamental property of language is its compositional structure, allowing humans to produce forms for new meanings systematically. However, unlike humans, neural networks notoriously struggle with systematic generalization, and do not necessarily benefit from compositional structure in emergent communication simulations. This poses a problem for using neural networks to simulate human language learning and evolution, and suggests crucial differences in the biases of the different learning systems. Here, we directly test how neural networks compare to humans in learning and generalizing different input languages that vary in their degree of structure. We evaluate the memorization and generalization capabilities of a pre-trained language model GPT-3.5 (analagous to an adult second language learner) and recurrent neural networks trained from scratch (analaogous to a child first language learner). Our results show striking
&lt;/p&gt;</description></item></channel></rss>