<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26102;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15937</link><description>&lt;p&gt;
&#29992;&#26356;&#23569;&#30340;&#25968;&#25454;&#36827;&#34892;&#35270;&#35273;&#19978;&#26377;&#20381;&#25454;&#30340;&#23569;&#26679;&#26412;&#35789;&#27719;&#20064;&#24471;
&lt;/p&gt;
&lt;p&gt;
Visually grounded few-shot word acquisition with fewer shots. (arXiv:2305.15937v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15937
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#65292;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#22312;&#20351;&#29992;&#26356;&#23569;&#30340;&#26679;&#26412;&#26102;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20174;&#20165;&#26377;&#23569;&#37327;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20013;&#20064;&#24471;&#26032;&#30340;&#35789;&#27719;&#21450;&#20854;&#35270;&#35273;&#34920;&#31034;&#12290;&#32473;&#23450;&#19968;&#32452;&#27979;&#35797;&#22270;&#20687;&#21644;&#19968;&#20010;&#21475;&#22836;&#26597;&#35810;&#65292;&#25105;&#20204;&#35201;&#27714;&#27169;&#22411;&#25351;&#20986;&#21738;&#20010;&#22270;&#20687;&#23637;&#31034;&#20102;&#26597;&#35810;&#35789;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#20351;&#29992;&#25968;&#23383;&#35789;-&#22270;&#20687;&#23545;&#30340;&#20154;&#36896;&#29615;&#22659;&#26469;&#31616;&#21270;&#35813;&#38382;&#39064;&#65292;&#35201;&#20040;&#20351;&#29992;&#27599;&#20010;&#31867;&#21035;&#22823;&#37327;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#33258;&#28982;&#30340;&#35789;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#65292;&#20294;&#21482;&#38656;&#26356;&#23569;&#30340;&#25968;&#25454;&#65292;&#21363;&#26356;&#23569;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#32473;&#23450;&#30340;&#35789;-&#22270;&#20687;&#31034;&#20363;&#23545;&#20174;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#21644;&#22270;&#20687;&#20013;&#25366;&#25496;&#26032;&#30340;&#26080;&#30417;&#30563;&#35789;-&#22270;&#20687;&#35757;&#32451;&#23545;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35789;&#21040;&#22270;&#20687;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#30830;&#23450;&#35789;-&#22270;&#20687;&#30340;&#30456;&#20284;&#24230;&#12290;&#36890;&#36807;&#36825;&#31181;&#26032;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#27604;&#20219;&#20309;&#29616;&#26377;&#26041;&#27861;&#37117;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#21482;&#38656;&#26356;&#23569;&#30340;&#25968;&#25454;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a visually grounded speech model that acquires new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelled speech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than any existing approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.15932</link><description>&lt;p&gt;
BUCA&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
BUCA: A Binary Classification Approach to Unsupervised Commonsense Question Answering. (arXiv:2305.15932v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#26041;&#27861;&#65292;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#26681;&#25454;&#21512;&#29702;&#24615;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#24120;&#35782;&#38382;&#39064;&#22238;&#31572;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;UCR&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20026;&#33410;&#30465;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#19988;&#22312;&#33539;&#22260;&#19978;&#19981;&#21487;&#36991;&#20813;&#22320;&#21463;&#38480;&#65292;&#26080;&#30417;&#30563;&#30340;&#24120;&#35782;&#25512;&#29702;(UCR)&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;UCR&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#23558;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;(&#20363;&#22914;&#65292;&#30693;&#35782;&#22270;&#35889;)&#65292;&#20294;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26679;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#19979;&#28216;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#22238;&#31572;&#20219;&#21153;&#36716;&#25442;&#20026;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#36890;&#36807;&#23545;&#25152;&#26377;&#20505;&#36873;&#31572;&#26696;&#30340;&#21512;&#29702;&#24615;&#36827;&#34892;&#25490;&#21517;&#26469;&#23436;&#25104;&#12290;&#20026;&#20102;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23558;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36716;&#25442;&#20026;&#21512;&#29702;&#21644;&#19981;&#21512;&#29702;&#30340;&#25991;&#26412;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#22810;&#39033;&#36873;&#25321;&#38382;&#39064;&#22238;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#19982;&#20351;&#29992;KG&#30340;&#29616;&#26377;UCR&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#33410;&#30465;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/probe2/BUCA&#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as the construction of commonsense reasoning datasets is expensive, and they are inevitably limited in their scope. A popular approach to UCR is to fine-tune language models with external knowledge (e.g., knowledge graphs), but this usually requires a large number of training examples. In this paper, we propose to transform the downstream multiple choice question answering task into a simpler binary classification task by ranking all candidate answers according to their reasonableness. To this end, for training the model, we convert the knowledge graph triples into reasonable and unreasonable texts. Extensive experimental results show the effectiveness of our approach on various multiple choice question answering benchmarks. Furthermore, compared with existing UCR approaches using KGs, ours is less data hungry. Our code is available at https://github.com/probe2/BUCA.
&lt;/p&gt;</description></item><item><title>ChatGPT&#34920;&#29616;&#20986;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38899;&#38901;&#20559;&#35265;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#36741;&#38899;&#32780;&#19981;&#26159;&#20803;&#38899;&#26469;&#35782;&#21035;&#21333;&#35789;&#12290;</title><link>http://arxiv.org/abs/2305.15929</link><description>&lt;p&gt;
ChatGPT&#20013;&#20986;&#29616;&#20102;&#38899;&#38901;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Emergence of a phonological bias in ChatGPT. (arXiv:2305.15929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15929
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#34920;&#29616;&#20986;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38899;&#38901;&#20559;&#35265;&#65292;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#36741;&#38899;&#32780;&#19981;&#26159;&#20803;&#38899;&#26469;&#35782;&#21035;&#21333;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20363;&#22914;OpenAI&#30340;ChatGPT&#65292;&#22240;&#20854;&#22312;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#35777;&#26126;&#20102;ChatGPT&#26174;&#31034;&#20102;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#30340;&#38899;&#38901;&#20559;&#35265;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#23601;&#20687;&#20154;&#31867;&#19968;&#26679;&#65292;ChatGPT&#20855;&#26377;&#19968;&#20010;&#36741;&#38899;&#20559;&#35265;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#20542;&#21521;&#20110;&#20351;&#29992;&#36741;&#38899;&#32780;&#19981;&#26159;&#20803;&#38899;&#26469;&#35782;&#21035;&#21333;&#35789;&#12290;&#36825;&#22312;&#20855;&#26377;&#19981;&#21516;&#36741;&#38899;&#21644;&#20803;&#38899;&#20998;&#24067;&#27604;&#20363;&#30340;&#35821;&#35328;&#65288;&#22914;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#65289;&#20013;&#37117;&#26377;&#35266;&#23519;&#21040;&#12290;&#23613;&#31649;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35821;&#35328;&#21050;&#28608;&#21644;&#20154;&#31867;&#23156;&#20799;&#33719;&#24471;&#35821;&#35328;&#30340;&#26041;&#24335;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#36825;&#26679;&#30340;&#35757;&#32451;&#20284;&#20046;&#36275;&#20197;&#22312;ChatGPT&#20013;&#24341;&#20986;&#19968;&#20010;&#38899;&#38901;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large language models, such as OpenAI's ChatGPT, have captured the public's attention because how remarkable they are in the use of language. Here, I demonstrate that ChatGPT displays phonological biases that are a hallmark of human language processing. More concretely, just like humans, ChatGPT has a consonant bias. That is, the chatbot has a tendency to use consonants over vowels to identify words. This is observed across languages that differ in their relative distribution of consonants and vowels such as English and Spanish. Despite the differences in how current artificial intelligence language models are trained to process linguistic stimuli and how human infants acquire language, such training seems to be enough for the emergence of a phonological bias in ChatGPT
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#35328;&#21464;&#21270;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21382;&#21490;&#35821;&#35328;&#21464;&#21270;&#30340;&#29305;&#23450;&#23454;&#20363;&#20013;&#30340;&#36873;&#25321;&#24378;&#24230;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#27604;&#20197;&#21069;&#24212;&#29992;&#36807;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#12290;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#35821;&#38899;&#31616;&#21333;&#24615;&#20248;&#20808;&#20110;&#35821;&#27861;&#31616;&#21333;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#26816;&#27979;&#36873;&#25321;&#24378;&#24230;&#21464;&#21270;&#30340;&#26102;&#38388;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.15914</link><description>&lt;p&gt;
&#35821;&#35328;&#21464;&#21270;&#20013;&#36873;&#25321;&#26426;&#21046;&#30340;&#21487;&#38752;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Reliable identification of selection mechanisms in language change. (arXiv:2305.15914v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#35821;&#35328;&#21464;&#21270;&#20013;&#30340;&#36873;&#25321;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#38752;&#19988;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21382;&#21490;&#35821;&#35328;&#21464;&#21270;&#30340;&#29305;&#23450;&#23454;&#20363;&#20013;&#30340;&#36873;&#25321;&#24378;&#24230;&#12290;&#35813;&#26041;&#27861;&#34987;&#35777;&#26126;&#27604;&#20197;&#21069;&#24212;&#29992;&#36807;&#30340;&#26041;&#27861;&#26356;&#21487;&#38752;&#12290;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#35821;&#38899;&#31616;&#21333;&#24615;&#20248;&#20808;&#20110;&#35821;&#27861;&#31616;&#21333;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#26816;&#27979;&#36873;&#25321;&#24378;&#24230;&#21464;&#21270;&#30340;&#26102;&#38388;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#21464;&#21270;&#26159;&#19968;&#31181;&#25991;&#21270;&#36827;&#21270;&#36807;&#31243;&#65292;&#20854;&#20013;&#35821;&#35328;&#21464;&#37327;&#30340;&#21464;&#24322;&#36890;&#36807;&#31867;&#20284;&#20110;&#31361;&#21464;&#12289;&#36873;&#25321;&#21644;&#36951;&#20256;&#28418;&#21464;&#30340;&#36807;&#31243;&#32780;&#39057;&#32321;&#21464;&#21270;&#12290;&#26412;&#25991;&#24212;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#19968;&#31181;&#26041;&#27861;&#26469;&#23545;&#35821;&#26009;&#24211;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#21382;&#21490;&#35821;&#35328;&#21464;&#21270;&#30340;&#29305;&#23450;&#23454;&#20363;&#20013;&#30340;&#36873;&#25321;&#24378;&#24230;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#33521;&#35821;&#19981;&#35268;&#21017;&#21160;&#35789;&#30340;&#35821;&#22659;&#19979;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#27604;&#20197;&#21069;&#24212;&#29992;&#36807;&#30340;&#31867;&#20284;&#26041;&#27861;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#36825;&#39033;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#22312;&#35821;&#38899;&#31616;&#21333;&#24615;&#19982;&#35821;&#27861;&#31616;&#21333;&#24615;&#20914;&#31361;&#26102;&#65292;&#23545;&#35821;&#38899;&#31616;&#21333;&#24615;&#30340;&#20559;&#22909;&#20248;&#20808;&#20110;&#23545;&#35821;&#27861;&#31616;&#21333;&#24615;&#30340;&#20559;&#22909;&#12290;&#26368;&#21518;&#65292;&#38024;&#23545;&#35199;&#29677;&#29273;&#30340;&#25340;&#20889;&#25913;&#38761;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20063;&#21487;&#20197;&#26816;&#27979;&#36873;&#25321;&#24378;&#24230;&#21464;&#21270;&#30340;&#26102;&#38388;&#28857;&#65292;&#36825;&#26159;&#31038;&#20250;&#21160;&#26426;&#35821;&#35328;&#21464;&#21270;&#36890;&#24120;&#20855;&#26377;&#30340;&#29305;&#24449;&#12290;&#36825;&#20123;&#32467;&#26524;&#20849;&#21516;&#34920;&#26126;&#22914;&#20309;&#27979;&#35797;&#35821;&#35328;&#21464;&#21270;&#26426;&#21046;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language change is a cultural evolutionary process in which variants of linguistic variables change in frequency through processes analogous to mutation, selection and genetic drift. In this work, we apply a recently-introduced method to corpus data to quantify the strength of selection in specific instances of historical language change. We first demonstrate, in the context of English irregular verbs, that this method is more reliable and interpretable than similar methods that have previously been applied. We further extend this study to demonstrate that a bias towards phonological simplicity overrides that favouring grammatical simplicity when these are in conflict. Finally, with reference to Spanish spelling reforms, we show that the method can also detect points in time at which selection strengths change, a feature that is generically expected for socially-motivated language change. Together, these results indicate how hypotheses for mechanisms of language change can be tested qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MEMEX&#20219;&#21153;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21270;&#25216;&#26415;&#26816;&#27979;&#36855;&#22240;&#30340;&#35299;&#37322;&#24615;&#35777;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;MCC&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20998;&#23618;&#26041;&#27861;&#25429;&#25417;&#36855;&#22240;&#21644;&#19978;&#19979;&#25991;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#20381;&#36182;&#65292;&#25552;&#20986;&#20102;MIME&#22810;&#27169;&#24335;&#31070;&#32463;&#26694;&#26550;&#26469;&#35299;&#37322;&#36855;&#22240;&#12290;</title><link>http://arxiv.org/abs/2305.15913</link><description>&lt;p&gt;
MEMEX&#65306;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21270;&#26469;&#26816;&#27979;&#36855;&#22240;&#30340;&#35299;&#37322;&#24615;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
MEMEX: Detecting Explanatory Evidence for Memes via Knowledge-Enriched Contextualization. (arXiv:2305.15913v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MEMEX&#20219;&#21153;&#65292;&#36890;&#36807;&#30693;&#35782;&#22686;&#24378;&#30340;&#19978;&#19979;&#25991;&#21270;&#25216;&#26415;&#26816;&#27979;&#36855;&#22240;&#30340;&#35299;&#37322;&#24615;&#35777;&#25454;&#12290;&#36890;&#36807;&#26500;&#24314;MCC&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;&#20998;&#23618;&#26041;&#27861;&#25429;&#25417;&#36855;&#22240;&#21644;&#19978;&#19979;&#25991;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#20381;&#36182;&#65292;&#25552;&#20986;&#20102;MIME&#22810;&#27169;&#24335;&#31070;&#32463;&#26694;&#26550;&#26469;&#35299;&#37322;&#36855;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36855;&#22240;&#26159;&#31038;&#20132;&#23186;&#20307;&#19978;&#24378;&#22823;&#30340;&#20132;&#38469;&#24037;&#20855;&#65292;&#23427;&#20204;&#22312;&#25919;&#27835;&#12289;&#21382;&#21490;&#21644;&#31038;&#20250;&#25991;&#21270;&#29616;&#35937;&#20013;&#30340;&#19981;&#26029;&#21457;&#23637;&#20351;&#20854;&#25104;&#20026;&#29702;&#24819;&#30340;&#20132;&#27969;&#23186;&#20171;&#12290;&#20026;&#20102;&#29702;&#35299;&#36855;&#22240;&#20256;&#36798;&#30340;&#24494;&#22937;&#20449;&#24687;&#65292;&#24517;&#39035;&#20102;&#35299;&#20419;&#36827;&#20854;&#25972;&#20307;&#21560;&#25910;&#30340;&#32972;&#26223;&#12290;&#38500;&#20102;&#20687;knowyourmeme.com&#36825;&#26679;&#30340;&#20960;&#20010;&#32593;&#31449;&#23545;&#36855;&#22240;&#21450;&#20854;&#20803;&#25968;&#25454;&#36827;&#34892;&#25968;&#23383;&#23384;&#26723;&#22806;&#65292;&#30446;&#21069;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#21160;&#24577;&#22320;&#25512;&#26029;&#36855;&#22240;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20219;&#21153;&#65292;MEMEX&#65292;&#32473;&#23450;&#19968;&#20010;&#36855;&#22240;&#21644;&#19968;&#20010;&#30456;&#20851;&#30340;&#25991;&#26723;&#65292;&#20854;&#30446;&#30340;&#26159;&#25366;&#25496;&#31616;&#27905;&#22320;&#35299;&#37322;&#36855;&#22240;&#32972;&#26223;&#30340;&#19978;&#19979;&#25991;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;MCC&#65288;Meme Context Corpus&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;MEMEX&#35774;&#35745;&#30340;&#26032;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#22522;&#20934;&#27979;&#35797;MCC&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MIME&#65288;MultImodal Meme Explainer&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24335;&#31070;&#32463;&#26694;&#26550;&#65292;&#20351;&#29992;&#36890;&#35782;&#24378;&#21270;&#30340;&#36855;&#22240;&#34920;&#31034;&#21644;&#19968;&#31181;&#20998;&#23618;&#26041;&#27861;&#26469;&#25429;&#25417;&#36855;&#22240;&#21644;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#36328;&#27169;&#24577;&#35821;&#20041;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena makes them an ideal communication vehicle. To comprehend the subtle message conveyed within a meme, one must understand the background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme's context dynamically. In this work, we propose a novel task, MEMEX given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme. At first, we develop MCC (Meme Context Corpus), a novel dataset for MEMEX. Further, to benchmark MCC, we propose MIME (MultImodal Meme Explainer), a multimodal neural framework that uses common sense enriched meme representation and a layered approach to capture the cross-modal semantic dependencies between the meme and the context. M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;GePpeTto(GPT-2)&#21644;iT5&#31561;PLM&#27169;&#22411;&#65292;&#24182;&#23558;&#20174;LD&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#30693;&#35782;&#36827;&#34892;&#19981;&#21516;&#34920;&#31034;&#65292;&#20197;&#33719;&#24471;&#22522;&#20110;&#23454;&#20363;&#30340;&#22238;&#24212;&#29983;&#25104;&#65292;&#20197;&#27492;&#26469;&#35299;&#20915;&#23545;&#35805;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.15908</link><description>&lt;p&gt;
&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#65306;&#21738;&#31181;&#30693;&#35782;&#34920;&#31034;&#26377;&#21161;&#20110;&#65311;
&lt;/p&gt;
&lt;p&gt;
Response Generation in Longitudinal Dialogues: Which Knowledge Representation Helps?. (arXiv:2305.15908v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;GePpeTto(GPT-2)&#21644;iT5&#31561;PLM&#27169;&#22411;&#65292;&#24182;&#23558;&#20174;LD&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#30693;&#35782;&#36827;&#34892;&#19981;&#21516;&#34920;&#31034;&#65292;&#20197;&#33719;&#24471;&#22522;&#20110;&#23454;&#20363;&#30340;&#22238;&#24212;&#29983;&#25104;&#65292;&#20197;&#27492;&#26469;&#35299;&#20915;&#23545;&#35805;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#23545;&#35805;&#26159;&#20154;&#26426;&#23545;&#35805;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#31867;&#22411;&#20043;&#19968;&#12290;&#38271;&#26399;&#23545;&#35805;&#21253;&#25324;&#20010;&#20154;&#22312;&#31232;&#30095;&#30340;&#23545;&#35805;&#24207;&#21015;&#20013;&#22238;&#24518;&#30340;&#20107;&#20214;&#12289;&#20010;&#20154;&#24605;&#24819;&#21644;&#24773;&#24863;&#31561;&#20869;&#23481;&#12290;&#35774;&#35745;&#29992;&#20110;&#38271;&#26399;&#23545;&#35805;&#30340;&#23545;&#35805;&#31995;&#32479;&#24212;&#35813;&#33021;&#22815;&#22312;&#22810;&#20010;&#23545;&#35805;&#20250;&#35805;&#21644;&#38271;&#26102;&#38388;&#65288;&#20363;&#22914;&#25968;&#21608;&#65289;&#20869;&#19982;&#29992;&#25143;&#36827;&#34892;&#29420;&#29305;&#20132;&#20114;&#65292;&#24182;&#35753;&#20182;&#20204;&#21442;&#19982;&#20010;&#20154;&#23545;&#35805;&#20197;&#38416;&#36848;&#20182;&#20204;&#30340;&#24863;&#21463;&#12289;&#24605;&#24819;&#21644;&#30495;&#23454;&#29983;&#27963;&#20107;&#20214;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#38271;&#26399;&#23545;&#35805;&#20013;&#30340;&#22238;&#24212;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36890;&#29992;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26159;&#21542;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;LD&#25968;&#25454;&#38598;&#24494;&#35843;&#20102;&#20004;&#20010;PLM&#27169;&#22411;&#65292;GePpeTto (GPT-2)&#21644;iT5&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#20174;LD&#20013;&#25552;&#21462;&#30340;&#20010;&#20154;&#30693;&#35782;&#30340;&#19981;&#21516;&#34920;&#31034;&#24418;&#24335;&#65292;&#21253;&#25324;&#25552;&#21040;&#30340;&#20107;&#20214;&#21644;&#21442;&#19982;&#32773;&#30340;&#22270;&#24418;&#34920;&#31034;&#65292;&#20197;&#33719;&#24471;&#22522;&#20110;&#23454;&#20363;&#30340;&#22238;&#24212;&#29983;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#25351;&#26631;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Longitudinal Dialogues (LD) are the most challenging type of conversation for human-machine dialogue systems. LDs include the recollections of events, personal thoughts, and emotions specific to each individual in a sparse sequence of dialogue sessions. Dialogue systems designed for LDs should uniquely interact with the users over multiple sessions and long periods of time (e.g. weeks), and engage them in personal dialogues to elaborate on their feelings, thoughts, and real-life events. In this paper, we study the task of response generation in LDs. We evaluate whether general-purpose Pre-trained Language Models (PLM) are appropriate for this purpose. We fine-tune two PLMs, GePpeTto (GPT-2) and iT5, using a dataset of LDs. We experiment with different representations of the personal knowledge extracted from LDs for grounded response generation, including the graph representation of the mentioned events and participants. We evaluate the performance of the models via automatic metrics an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;MTCue&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#35299;&#37322;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#21487;&#36716;&#31227;&#24615;&#24182;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#65289;&#30340;&#25511;&#21046;&#12290;&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;MTCue&#30340;&#32763;&#35793;&#36136;&#37327;&#26174;&#30528;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.15904</link><description>&lt;p&gt;
MTCue&#65306;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26410;&#32467;&#26500;&#21270;&#19978;&#19979;&#25991;&#23398;&#20064;&#38646;&#26679;&#26412;&#25511;&#21046;&#39069;&#22806;&#25991;&#26412;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation. (arXiv:2305.15904v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26694;&#26550;MTCue&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#35299;&#37322;&#20026;&#25991;&#26412;&#65292;&#23454;&#29616;&#20102;&#21487;&#36716;&#31227;&#24615;&#24182;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65288;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#65289;&#30340;&#25511;&#21046;&#12290;&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;MTCue&#30340;&#32763;&#35793;&#36136;&#37327;&#26174;&#30528;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#21033;&#29992;&#25991;&#26412;&#20869;&#21644;&#25991;&#26412;&#22806;&#30340;&#19978;&#19979;&#25991;&#20173;&#26159;&#26426;&#22120;&#21644;&#20154;&#31867;&#32763;&#35793;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#36317;&#20043;&#19968;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#32763;&#35793;&#20013;&#25552;&#20379;&#20010;&#21035;&#23450;&#20041;&#33391;&#22909;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#65292;&#22914;&#21608;&#22260;&#30340;&#25991;&#26412;&#25110;&#31163;&#25955;&#30340;&#22806;&#37096;&#21464;&#37327;&#65288;&#22914;&#35828;&#35805;&#32773;&#30340;&#24615;&#21035;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MTCue&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26694;&#26550;&#65292;&#23427;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#65288;&#21253;&#25324;&#31163;&#25955;&#21464;&#37327;&#65289;&#35299;&#37322;&#20026;&#25991;&#26412;&#12290; MTCue&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#25277;&#35937;&#34920;&#36798;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#35774;&#32622;&#21644;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#65292;&#20063;&#33021;&#23454;&#29616;&#21487;&#36716;&#31227;&#24615;&#24182;&#21033;&#29992;&#31867;&#20284;&#23646;&#24615;&#12290;&#25105;&#20204;&#19981;&#26029;&#35780;&#20272;MTCue&#22312;&#22235;&#20010;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#26041;&#21521;&#19978;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#25991;&#26723;&#21644;&#20803;&#25968;&#25454;&#19978;&#19979;&#25991;&#35775;&#38382;&#26435;&#38480;&#30340;&#23545;&#35805;&#39046;&#22495;&#12290;&#19982;&#21442;&#25968;&#21305;&#37197;&#30340;&#38750;&#19978;&#19979;&#25991;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#25552;&#39640;&#65292;BLEU&#65288;+0.88&#65289;&#21644;Comet&#65288;+1.58&#65289;&#12290;&#27492;&#22806;&#65292;MTCue&#25104;&#21151;&#22320;&#23398;&#20250;&#20102;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#21033;&#29992;&#39069;&#22806;&#30340;&#25991;&#26412;&#23646;&#24615;&#65292;&#23454;&#29616;&#20102;&#35832;&#22914;&#31036;&#35980;&#21644;&#23545;&#35805;&#34892;&#20026;&#31561;&#21464;&#37327;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient utilisation of both intra- and extra-textual context remains one of the critical gaps between machine and human translation. Existing research has primarily focused on providing individual, well-defined types of context in translation, such as the surrounding text or discrete external variables like the speaker's gender. This work introduces MTCue, a novel neural machine translation (NMT) framework that interprets all context (including discrete variables) as text. MTCue learns an abstract representation of context, enabling transferability across different data settings and leveraging similar attributes in low-resource scenarios. With a focus on a dialogue domain with access to document and metadata context, we extensively evaluate MTCue in four language pairs in both translation directions. Our framework demonstrates significant improvements in translation quality over a parameter-matched non-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58). Moreover, MTCu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38598;&#20307;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22823;&#22411;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#20114;&#30456;&#30693;&#35782;&#33976;&#39311;&#26426;&#21046;&#26469;&#22686;&#24378;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15895</link><description>&lt;p&gt;
&#24102;&#20114;&#30456;&#30693;&#35782;&#33976;&#39311;&#30340;&#38598;&#20307;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Collective Knowledge Graph Completion with Mutual Knowledge Distillation. (arXiv:2305.15895v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38598;&#20307;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22823;&#22411;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#27169;&#22411;&#26469;&#26368;&#22823;&#21270;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#30340;&#38598;&#20307;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#20114;&#30456;&#30693;&#35782;&#33976;&#39311;&#26426;&#21046;&#26469;&#22686;&#24378;&#35813;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26159;&#26681;&#25454;&#30693;&#35782;&#22270;&#35889;&#20869;&#30340;&#29616;&#26377;&#20851;&#31995;&#25968;&#25454;&#39044;&#27979;&#20002;&#22833;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#26469;&#28304;&#21644;&#35821;&#35328;&#30340;&#30693;&#35782;&#22270;&#35889;&#20043;&#38388;&#30340;&#23436;&#25972;&#24615;&#24120;&#24120;&#38480;&#21046;&#20102;KGC&#26041;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#29615;&#22659;&#20013;&#65292;&#30693;&#35782;&#22270;&#35889;&#28508;&#22312;&#22320;&#20114;&#34917;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#38382;&#39064;&#65292;&#37325;&#28857;&#26159;&#20026;&#20102;&#22686;&#24378;&#20010;&#20307;&#30693;&#35782;&#22270;&#35889;&#30340;&#19981;&#23436;&#25972;&#24615;&#32780;&#26368;&#22823;&#21270;&#26469;&#33258;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#30340;&#38598;&#20307;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;CKGC-CKD&#65292;&#22312;&#20010;&#20307;&#30693;&#35782;&#22270;&#35889;&#21644;&#19968;&#20010;&#22823;&#22411;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#19978;&#20351;&#29992;&#20851;&#31995;&#24863;&#30693;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20854;&#20013;KG&#20043;&#38388;&#30340;&#31181;&#23376;&#23545;&#40784;&#34987;&#35270;&#20026;&#28040;&#24687;&#20256;&#36882;&#30340;&#36793;&#32536;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#39069;&#22806;&#30340;&#20114;&#30456;&#30693;&#35782;&#33976;&#39311;&#26426;&#21046;&#65292;&#20197;&#26368;&#22823;&#21270;&#27169;&#22411;&#20043;&#38388;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graph completion (KGC), the task of predicting missing information based on the existing relational data inside a knowledge graph (KG), has drawn significant attention in recent years. However, the predictive power of KGC methods is often limited by the completeness of the existing knowledge graphs from different sources and languages. In monolingual and multilingual settings, KGs are potentially complementary to each other. In this paper, we study the problem of multi-KG completion, where we focus on maximizing the collective knowledge from different KGs to alleviate the incompleteness of individual KGs. Specifically, we propose a novel method called CKGC-CKD that uses relation-aware graph convolutional network encoder models on both individual KGs and a large fused KG in which seed alignments between KGs are regarded as edges for message propagation. An additional mutual knowledge distillation mechanism is also employed to maximize the knowledge transfer between the models 
&lt;/p&gt;</description></item><item><title>CSS&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#36328;&#27169;&#24335;&#20013;&#25991;&#25991;&#26412;&#21040;SQL&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#38590;&#39064;&#12290;&#23427;&#21253;&#25324;2&#20010;&#25968;&#25454;&#24211;&#20013;&#30340;4,340&#20010;&#38382;&#39064;/SQL&#23545;&#21644;19&#20010;&#26032;&#25968;&#25454;&#24211;&#30340;29,280&#20010;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.15891</link><description>&lt;p&gt;
CSS: &#19968;&#20010;&#22823;&#35268;&#27169;&#36328;&#27169;&#24335;&#20013;&#25991;&#25991;&#26412;&#21040;SQL&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CSS: A Large-scale Cross-schema Chinese Text-to-SQL Medical Dataset. (arXiv:2305.15891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15891
&lt;/p&gt;
&lt;p&gt;
CSS&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#36328;&#27169;&#24335;&#20013;&#25991;&#25991;&#26412;&#21040;SQL&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#38590;&#39064;&#12290;&#23427;&#21253;&#25324;2&#20010;&#25968;&#25454;&#24211;&#20013;&#30340;4,340&#20010;&#38382;&#39064;/SQL&#23545;&#21644;19&#20010;&#26032;&#25968;&#25454;&#24211;&#30340;29,280&#20010;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#23558;&#29992;&#25143;&#38382;&#39064;&#35299;&#26512;&#20026;SQL&#65292;&#36825;&#20123;&#25968;&#25454;&#24211;&#26159;&#23436;&#20840;&#26410;&#35265;&#36807;&#30340;&#65292;&#22312;&#21516;&#19968;&#39046;&#22495;&#20869;&#36827;&#34892;&#36328;&#27169;&#24335;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20197;&#35299;&#20915;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#38590;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36328;&#27169;&#24335;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;CSS&#65292;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36328;&#27169;&#24335;&#20013;&#25991;&#25991;&#26412;&#21040;SQL&#25968;&#25454;&#38598;&#65292;&#26469;&#24320;&#23637;&#30456;&#24212;&#30340;&#30740;&#31350;&#12290;CSS&#26368;&#21021;&#30001;2&#20010;&#25968;&#25454;&#24211;&#20013;&#30340;4,340&#20010;&#38382;&#39064;/SQL&#23545;&#32452;&#25104;&#12290;&#20026;&#20102;&#23558;&#27169;&#22411;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#21307;&#30103;&#31995;&#32479;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;CSS&#24182;&#21019;&#24314;&#20102;19&#20010;&#26032;&#25968;&#25454;&#24211;&#20197;&#21450;29,280&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#31034;&#20363;&#12290;&#27492;&#22806;&#65292;CSS &#36824;&#26159;&#36827;&#34892;&#21333;&#39046;&#22495;&#20013;&#25991;&#25991;&#26412;&#21040;SQL&#30740;&#31350;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#21644;&#19968;&#31995;&#21015;&#25968;&#25454;&#32479;&#35745;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-domain text-to-SQL task aims to build a system that can parse user questions into SQL on complete unseen databases, and the single-domain text-to-SQL task evaluates the performance on identical databases. Both of these setups confront unavoidable difficulties in real-world applications. To this end, we introduce the cross-schema text-to-SQL task, where the databases of evaluation data are different from that in the training data but come from the same domain. Furthermore, we present CSS, a large-scale CrosS-Schema Chinese text-to-SQL dataset, to carry on corresponding studies. CSS originally consisted of 4,340 question/SQL pairs across 2 databases. In order to generalize models to different medical systems, we extend CSS and create 19 new databases along with 29,280 corresponding dataset examples. Moreover, CSS is also a large corpus for single-domain Chinese text-to-SQL studies. We present the data collection approach and a series of analyses of the data statistics. To show 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.15878</link><description>&lt;p&gt;
LFTK: &#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#30340;&#25163;&#24037;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
LFTK: Handcrafted Features in Computational Linguistics. (arXiv:2305.15878v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#65292;&#24182;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#24050;&#32463;&#37492;&#23450;&#20986;&#20102;&#19968;&#32452;&#20016;&#23500;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24110;&#21161;&#21508;&#31181;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#36825;&#20123;&#29305;&#24449;&#25968;&#37327;&#24222;&#22823;&#65292;&#22240;&#27492;&#38590;&#20197;&#26377;&#25928;&#22320;&#36873;&#25321;&#21644;&#21033;&#29992;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#21152;&#19978;&#22312;&#30740;&#31350;&#24037;&#20316;&#20013;&#23454;&#29616;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#30446;&#21069;&#36824;&#19981;&#23384;&#22312;&#20998;&#31867;&#26041;&#26696;&#25110;&#32773;&#32479;&#19968;&#25509;&#21463;&#30340;&#29305;&#24449;&#21517;&#31216;&#65292;&#36825;&#36896;&#25104;&#20102;&#19981;&#24517;&#35201;&#30340;&#28151;&#20081;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#24211;&#37117;&#19981;&#26159;&#24320;&#28304;&#30340;&#65292;&#25110;&#32773;&#27809;&#26377;&#24471;&#21040;&#31215;&#26497;&#30340;&#32500;&#25252;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#38656;&#35201;&#20174;&#38646;&#24320;&#22987;&#26500;&#24314;&#36825;&#26679;&#30340;&#25552;&#21462;&#31995;&#32479;&#12290;&#25105;&#20204;&#36890;&#36807;&#36807;&#21435;&#30340;&#25991;&#29486;&#25910;&#38598;&#21644;&#20998;&#31867;&#20102;&#36229;&#36807;220&#20010;&#21463;&#27426;&#36814;&#30340;&#25163;&#24037;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#30456;&#20851;&#24615;&#20998;&#26512;&#30740;&#31350;&#65292;&#24182;&#25253;&#21578;&#20102;&#27599;&#20010;&#29305;&#24449;&#30340;&#28508;&#22312;&#29992;&#36884;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22810;&#35821;&#35328;&#30340;&#25163;&#24037;&#35821;&#35328;&#29305;&#24449;&#25552;&#21462;&#31995;&#32479;&#65292;&#20197;&#31995;&#32479;&#24615;&#30340;&#21487;&#25193;&#23637;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past research has identified a rich set of handcrafted linguistic features that can potentially assist various tasks. However, their extensive number makes it difficult to effectively select and utilize existing handcrafted features. Coupled with the problem of inconsistent implementation across research works, there has been no categorization scheme or generally-accepted feature names. This creates unwanted confusion. Also, most existing handcrafted feature extraction libraries are not open-source or not actively maintained. As a result, a researcher often has to build such an extraction system from the ground up.  We collect and categorize more than 220 popular handcrafted features grounded on past literature. Then, we conduct a correlation analysis study on several task-specific datasets and report the potential use cases of each feature. Lastly, we devise a multilingual handcrafted linguistic feature extraction system in a systematically expandable manner. We open-source our system
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15875</link><description>&lt;p&gt;
&#30495;&#23454;&#22238;&#31572;&#30340;&#35821;&#35328;&#29305;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Linguistic Properties of Truthful Response. (arXiv:2305.15875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15875
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;LLM&#30340;&#19981;&#30495;&#23454;&#22238;&#31572;&#29616;&#35937;&#65292;&#21457;&#29616;GPT-3&#27169;&#22411;&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#21516;&#26102;&#65292;&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#21363;&#21487;&#20998;&#31867;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;220&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#35821;&#35328;&#29305;&#24615;&#23545;LLM&#19981;&#30495;&#23454;&#22238;&#31572;&#30340;&#29616;&#35937;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;GPT-3&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#19981;&#21516;&#22823;&#23567;&#30340;LLM&#23545;&#32473;&#23450;&#25552;&#31034;&#30340;&#22238;&#31572;&#22312;&#35821;&#35328;&#29305;&#24615;&#19978;&#24456;&#30456;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#21482;&#20381;&#36182;&#27169;&#22411;&#22238;&#31572;&#30340;&#39118;&#26684;&#25104;&#20998;&#26469;&#20998;&#31867;&#38472;&#36848;&#30495;&#23454;&#24615;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#25193;&#23637;&#20102;&#36825;&#19968;&#21457;&#29616;&#12290;&#34429;&#28982;&#25968;&#25454;&#38598;&#22823;&#23567;&#38480;&#21046;&#20102;&#25105;&#20204;&#30340;&#24403;&#21069;&#30740;&#31350;&#25104;&#26524;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#21487;&#20197;&#22312;&#19981;&#35780;&#20272;&#20869;&#23481;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#26816;&#27979;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the phenomenon of an LLM's untruthful response using a large set of 220 handcrafted linguistic features. We focus on GPT-3 models and find that the linguistic profiles of responses are similar across model sizes. That is, how varying-sized LLMs respond to given prompts stays similar on the linguistic properties level. We expand upon this finding by training support vector machines that rely only upon the stylistic components of model responses to classify the truthfulness of statements. Though the dataset size limits our current findings, we present promising evidence that truthfulness detection is possible without evaluating the content itself.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Jointprop&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#37319;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#22270;&#26469;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15872</link><description>&lt;p&gt;
Jointprop&#65306;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#32852;&#21512;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation. (arXiv:2305.15872v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15872
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Jointprop&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#65292;&#37319;&#29992;&#19968;&#20010;&#32479;&#19968;&#30340;&#24322;&#26500;&#22270;&#26469;&#20351;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#25552;&#39640;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21322;&#30417;&#30563;&#26041;&#27861;&#20998;&#21035;&#22788;&#29702;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#20004;&#20010;&#20219;&#21153;&#65292;&#24182;&#24573;&#30053;&#23454;&#20307;&#21644;&#20851;&#31995;&#23454;&#20363;&#20043;&#38388;&#30340;&#20132;&#21449;&#30456;&#20851;&#24615;&#20197;&#21450;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30456;&#20284;&#23454;&#20363;&#30340;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jointprop&#65292;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#20256;&#25773;&#30340;&#32852;&#21512;&#21322;&#30417;&#30563;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#26694;&#26550;&#65292;&#20854;&#25429;&#33719;&#21508;&#20010;&#20219;&#21153;&#20043;&#38388;&#30340;&#20840;&#23616;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#20132;&#20114;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20174;&#23454;&#20307;&#21644;&#20851;&#31995;&#20505;&#36873;&#26500;&#24314;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#24322;&#26500;&#22270;&#65292;&#24182;&#26681;&#25454;&#32622;&#20449;&#24230;&#24471;&#20998;&#20256;&#25773;&#31867;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37319;&#29992;&#20256;&#25773;&#23398;&#20064;&#26041;&#26696;&#26469;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#26679;&#26412;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#65292;&#20854;&#21487;&#20197;&#22312;&#25216;&#26415;&#39046;&#22495;&#20869;&#36798;&#21040;&#19982;&#21477;&#23376;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#36136;&#37327;&#65292;&#20294;&#22823;&#23567;&#20026;&#21518;&#32773;&#30340;&#20116;&#20998;&#20043;&#19968;&#65292;&#35745;&#31639;&#26102;&#38388;&#33021;&#24555;10&#20493;&#12290;</title><link>http://arxiv.org/abs/2305.15867</link><description>&lt;p&gt;
&#25216;&#26415;&#39046;&#22495;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#25991;&#26412;&#34920;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Extracting Text Representations for Terms and Phrases in Technical Domains. (arXiv:2305.15867v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#65292;&#20854;&#21487;&#20197;&#22312;&#25216;&#26415;&#39046;&#22495;&#20869;&#36798;&#21040;&#19982;&#21477;&#23376;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#36136;&#37327;&#65292;&#20294;&#22823;&#23567;&#20026;&#21518;&#32773;&#30340;&#20116;&#20998;&#20043;&#19968;&#65292;&#35745;&#31639;&#26102;&#38388;&#33021;&#24555;10&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#26415;&#35821;&#21644;&#30701;&#35821;&#30340;&#23494;&#38598;&#34920;&#31034;&#26159;&#38754;&#21521;&#39640;&#24230;&#25216;&#26415;&#39046;&#22495;&#30340;&#30693;&#35782;&#21457;&#29616;&#24179;&#21488;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#24120;&#29992;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#33258;&#30417;&#30563;&#35774;&#32622;&#35757;&#32451;&#39046;&#22495;&#29305;&#23450;&#30340;&#23884;&#20837;&#25110;&#20351;&#29992;&#35757;&#32451;&#36807;&#30456;&#20284;&#24615;&#20219;&#21153;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#25991;&#26412;&#32534;&#30721;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;&#23567;&#22411;&#22522;&#20110;&#23383;&#31526;&#30340;&#27169;&#22411;&#26469;&#37325;&#26500;&#22823;&#22411;&#39044;&#35757;&#32451;&#23884;&#20837;&#30697;&#38453;&#12290;&#19982;&#38745;&#24577;&#23884;&#20837;&#30456;&#27604;&#65292;&#21477;&#23376;&#32534;&#30721;&#22120;&#19981;&#20250;&#21463;&#21040;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#24433;&#21709;&#65292;&#20294;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting dense representations for terms and phrases is a task of great importance for knowledge discovery platforms targeting highly-technical fields. Dense representations are used as features for downstream components and have multiple applications ranging from ranking results in search to summarization. Common approaches to create dense representations include training domain-specific embeddings with self-supervised setups or using sentence encoder models trained over similarity tasks. In contrast to static embeddings, sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, but impose significant computational costs. In this paper, we propose a fully unsupervised approach to text encoding that consists of training small character-based models with the objective of reconstructing large pre-trained embedding matrices. Models trained with this approach can not only match the quality of sentence encoders in technical domains, but are 5 times smaller and up to 10 tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207; Integrated Gradients&#65288;SIG&#65289;&#30340;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#26469;&#26174;&#30528;&#25913;&#21892;&#35299;&#37322;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15853</link><description>&lt;p&gt;
&#39034;&#24207;Integrated Gradients&#65306;&#19968;&#31181;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequential Integrated Gradients: a simple but effective method for explaining language models. (arXiv:2305.15853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#39034;&#24207; Integrated Gradients&#65288;SIG&#65289;&#30340;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#26469;&#26174;&#30528;&#25913;&#21892;&#35299;&#37322;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#35299;&#37322;&#26041;&#27861;&#65288;&#20363;&#22914;Integrated Gradients&#65288;IG&#65289;&#65289;&#21487;&#20197;&#34987;&#25551;&#36848;&#20026;&#22522;&#20110;&#36335;&#24452;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#25968;&#25454;&#21644;&#26080;&#20449;&#24687;&#22522;&#32447;&#20043;&#38388;&#30340;&#30452;&#32447;&#12290;&#28982;&#32780;&#65292;&#24403;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#21516;&#26102;&#20026;&#27599;&#20010;&#21477;&#23376;&#21333;&#35789;&#37327;&#20135;&#29983;&#36335;&#24452;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20174;&#25554;&#20540;&#35789;&#29983;&#25104;&#30340;&#21477;&#23376;&#27809;&#26377;&#26126;&#30830;&#30340;&#21547;&#20041;&#65292;&#25110;&#32773;&#19982;&#21407;&#22987;&#21477;&#23376;&#30456;&#27604;&#26377;&#26174;&#30528;&#19981;&#21516;&#30340;&#21547;&#20041;&#12290;&#20026;&#20102;&#20351;&#36825;&#20123;&#21477;&#23376;&#30340;&#21547;&#20041;&#23613;&#21487;&#33021;&#25509;&#36817;&#21407;&#22987;&#21477;&#23376;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39034;&#24207;Integrated Gradients&#65288;SIG&#65289;&#65292;&#23427;&#36890;&#36807;&#20445;&#25345;&#20854;&#20182;&#21333;&#35789;&#19981;&#21464;&#65292;&#20165;&#22312;&#22522;&#32447;&#21644;&#24863;&#20852;&#36259;&#30340;&#21333;&#35789;&#20043;&#38388;&#21019;&#24314;&#25554;&#20540;&#26469;&#35745;&#31639;&#21477;&#23376;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36824;&#24314;&#35758;&#29992;&#35757;&#32451;&#30340;&#20196;&#29260;&#8220;mask&#8221;&#26367;&#25442;&#22522;&#32447;&#20196;&#29260;&#8220;pad&#8221;&#12290;&#34429;&#28982;&#36825;&#21482;&#26159;&#23545;&#21407;&#22987;IG&#26041;&#27861;&#30340;&#31616;&#21333;&#25913;&#36827;&#65292;&#20294;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of several language models, we also propose to replace the baseline token "pad" with the trained token "mask". While being a simple improvement over the original IG method
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.15852</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#65306;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation. (arXiv:2305.15852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30456;&#30683;&#30462;&#24187;&#35273;&#36827;&#34892;&#20102;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#65292;&#25506;&#31350;&#20102;&#36825;&#19968;&#24187;&#35273;&#24418;&#24335;&#30340;&#26222;&#36941;&#23384;&#22312;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#26694;&#26550;&#26377;&#25928;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#21457;&#29616;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#20013;&#36825;&#31181;&#29616;&#35937;&#37117;&#39057;&#32321;&#20986;&#29616;&#12290;ChatGPT&#21644;GPT-4&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#33258;&#30456;&#30683;&#30462;&#65292;&#32780;Vicuna-13B&#21017;&#26377;&#20123;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20135;&#29983;&#24187;&#24819;&#30340;&#25991;&#26412;&#12290;&#33258;&#30456;&#30683;&#30462;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#24187;&#35273;&#24418;&#24335;&#65292;&#25351;&#30340;&#26159;&#35821;&#35328;&#27169;&#22411;&#22312;&#21516;&#19968;&#35821;&#22659;&#20013;&#29983;&#25104;&#20004;&#20010;&#30683;&#30462;&#30340;&#21477;&#23376;&#12290;&#26412;&#25991;&#38024;&#23545;&#26368;&#20808;&#36827;&#12289;&#32463;&#36807;&#25351;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23545;&#33258;&#30456;&#30683;&#30462;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12289;&#35780;&#20272;&#12289;&#26816;&#27979;&#21644;&#32531;&#35299;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#35302;&#21457;&#33258;&#30456;&#30683;&#30462;&#65292;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#26080;&#35770;&#26159;&#23545;&#20110;&#33879;&#21517;&#30340;&#36824;&#26159;&#19981;&#22826;&#20986;&#21517;&#30340;&#35805;&#39064;&#65292;&#19981;&#21516;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#30456;&#30683;&#30462;&#37117;&#32463;&#24120;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (large LMs) are susceptible to producing text with hallucinated content. Self-contradiction, where the LM generates two contradictory sentences within the same context, is an important form of hallucination. In this work, we present a comprehensive analysis on self-contradiction for state-of-the-art, instruction-tuned LMs, including evaluation, detection, and mitigation. To effectively trigger self-contradictions, we design a framework that constrains LMs to generate appropriate sentence pairs. Our evaluation on these sentence pairs reveals that self-contradictions occur frequently across different LMs for both famous and lesser-known topics. Next, we prompt the LMs to detect self-contradictions. Our results indicate that ChatGPT and GPT-4 are able to accurately identify self-contradictions, while Vicuna-13B struggles to do so. For example, with our best prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on the sentence pairs generated by itself. 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;22&#31181;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.15814</link><description>&lt;p&gt;
Bhasha-Abhijnaanam&#65306;22&#31181;&#21360;&#24230;&#25991;&#23383;&#21644;&#32599;&#39532;&#25340;&#38899;&#35821;&#35328;&#37492;&#21035;&#12290; (arXiv&#65306;2305.15814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
Bhasha-Abhijnaanam: Native-script and romanized Language Identification for 22 Indic languages. (arXiv:2305.15814v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15814
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;22&#31181;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#65292;&#36824;&#25552;&#20986;&#20102;&#35299;&#20915;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#38382;&#39064;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;22&#20010;&#21360;&#24230;&#23466;&#27861;&#20013;&#21015;&#20986;&#30340;&#25152;&#26377;21&#31181;&#26412;&#22303;&#25991;&#23383;&#21644;&#32599;&#39532;&#23383;&#27597;&#30340;&#20844;&#24320;&#35821;&#35328;&#37492;&#21035;&#65288;LID&#65289;&#25968;&#25454;&#21644;&#27169;&#22411;&#12290;&#19982;&#29616;&#26377;&#30340;LID&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;Bhasha-Abhijnaanam&#22312;&#26412;&#22303;&#25991;&#23383;&#25991;&#26412;&#30340;&#35821;&#35328;&#28085;&#30422;&#33539;&#22260;&#26041;&#38754;&#26356;&#20026;&#24191;&#27867;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;IndicLID&#26159;&#19978;&#36848;&#35821;&#35328;&#30340;&#26412;&#22303;&#21644;&#32599;&#39532;&#21270;&#33050;&#26412;&#30340;&#35821;&#35328;&#37492;&#21035;&#22120;&#12290;&#23545;&#20110;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;LID&#65292;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#21644;&#24403;&#35821;&#35328;&#30456;&#20284;&#26102;&#65292;&#20302;LID&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#22312;&#20219;&#20309;&#35821;&#35328;&#20013;&#65292;&#32599;&#39532;&#21270;&#25991;&#26412;&#30340;&#30740;&#31350;&#37117;&#24456;&#26377;&#38480;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#23545;&#38656;&#35201;&#32599;&#39532;&#21270;&#35821;&#35328;&#37492;&#21035;&#30340;&#20854;&#20182;&#35821;&#35328;&#20063;&#20855;&#26377;&#21442;&#32771;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We create publicly available language identification (LID) datasets and models in all 22 Indian languages listed in the Indian constitution in both native-script and romanized text. First, we create Bhasha-Abhijnaanam, a language identification test set for native-script as well as romanized text which spans all 22 Indic languages. We also train IndicLID, a language identifier for all the above-mentioned languages in both native and romanized script. For native-script text, it has better language coverage than existing LIDs and is competitive or better than other LIDs. IndicLID is the first LID for romanized text in Indian languages. Two major challenges for romanized text LID are the lack of training data and low-LID performance when languages are similar. We provide simple and effective solutions to these problems. In general, there has been limited work on romanized text in any language, and our findings are relevant to other languages that need romanized language identification. Ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15805</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#29992;&#20110;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. (arXiv:2305.15805v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#19978;&#19979;&#25991;&#21098;&#26525;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#27169;&#22411;&#34920;&#29616;&#21147;&#30340;&#21516;&#26102;&#65292;&#21160;&#24577;&#20943;&#23569;&#26080;&#25928;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37319;&#29992;&#30340;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#38590;&#20197;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#12290;&#23613;&#31649;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#20943;&#23569;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#22823;&#22810;&#25968;LLM&#20173;&#28982;&#22312;&#25152;&#26377;&#26631;&#35760;&#23545;&#20043;&#38388;&#37319;&#29992;&#27880;&#24847;&#23618;&#65292;&#20174;&#32780;&#20135;&#29983;&#20108;&#27425;&#25104;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#27169;&#22411;&#30340;&#34920;&#29616;&#21147;&#26469;&#21160;&#24577;&#20462;&#21098;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20943;&#23569;&#20869;&#23384;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#21487;&#23398;&#20064;&#26426;&#21046;&#65292;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#30830;&#23450;&#21738;&#20123;&#26080;&#20851;&#30340;&#26631;&#35760;&#21487;&#20197;&#20174;&#19978;&#19979;&#25991;&#20013;&#21024;&#38500;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#24615;&#33021;&#38382;&#39064;&#65292;&#32780;&#19988;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20026;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#23519;&#21147;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#24494;&#35843;&#36807;&#31243;&#24212;&#29992;&#20110;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#19988;&#21098;&#26525;&#24378;&#24230;&#21487;&#20197;&#30001;&#31232;&#30095;&#24230;&#21442;&#25968;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity para
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.15769</link><description>&lt;p&gt;
MERGE: &#24555;&#36895;&#30340;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MERGE: Fast Private Text Generation. (arXiv:2305.15769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15769
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;MERGE&#65292;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MERGE&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#20102;26.5&#20493;&#30340;&#21152;&#36895;&#21644;80%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;NLP&#26381;&#21153;&#21644;Transformer&#27169;&#22411;&#30340;&#31169;&#26377;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20004;&#26041;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#20165;&#32771;&#34385;NLU&#22330;&#26223;&#65292;&#32780;&#25991;&#26412;&#29983;&#25104;&#30340;&#31169;&#26377;&#25512;&#29702;&#65292;&#22914;&#32763;&#35793;&#12289;&#23545;&#35805;&#21644;&#20195;&#30721;&#34917;&#20840;&#65292;&#20173;&#26410;&#35299;&#20915;&#12290;&#27492;&#22806;&#65292;&#23558;&#29616;&#26377;&#30340;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#36801;&#31227;&#21040;NLG&#27169;&#22411;&#26102;&#65292;&#24615;&#33021;&#34920;&#29616;&#24046;&#65292;&#32780;&#22312;&#35757;&#32451;&#38454;&#27573;&#21463;&#21040;&#25910;&#25947;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MERGE&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#31169;&#26377;&#25991;&#26412;&#29983;&#25104;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MERGE&#37325;&#29992;&#36755;&#20986;&#38544;&#34255;&#29366;&#24577;&#20316;&#20026;&#21333;&#35789;&#23884;&#20837;&#65292;&#20197;&#36339;&#36807;&#23884;&#20837;&#35745;&#31639;&#65292;&#24182;&#37325;&#26032;&#32452;&#32455;Transformer&#27169;&#22359;&#20013;&#30340;&#32447;&#24615;&#25805;&#20316;&#20197;&#21152;&#36895;&#21521;&#21069;&#36807;&#31243;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#20248;&#21270;&#65292;&#22823;&#37327;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24207;&#21015;&#38271;&#24230;&#20026;512&#26102;&#65292;MERGE&#21487;&#23454;&#29616;26.5&#20493;&#30340;&#21152;&#36895;&#65292;&#24182;&#20943;&#23569;80\%&#30340;&#36890;&#20449;&#23383;&#33410;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen increasing concerns about the private inference of NLP services and Transformer models. However, existing two-party privacy-preserving methods solely consider NLU scenarios, while the private inference of text generation such as translation, dialogue, and code completion remains unsolved. Besides, while migrated to NLG models, existing privacy-preserving methods perform poorly in terms of inference speed, and suffer from the convergence problem during the training stage. To address these issues, we propose MERGE, a fast private text generation framework for Transformer-based language models. Specifically, MERGE reuse the output hidden state as the word embedding to bypass the embedding computation, and reorganize the linear operations in the Transformer module to accelerate the forward procedure. Based on these two optimizations, extensive experiments show that MERGE can achieve a 26.5x speedup under the sequence length 512, and reduce 80\% communication bytes, w
&lt;/p&gt;</description></item><item><title>Svarah&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#21360;&#24230;65&#20010;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;117&#20010;&#35828;&#35805;&#32773;&#30340;9.6&#23567;&#26102;&#30340;&#33521;&#35821;&#38899;&#39057;&#36716;&#24405;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#33521;&#35821;ASR&#31995;&#32479;&#22312;&#21360;&#24230;&#21475;&#38899;&#19978;&#38656;&#35201;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.15760</link><description>&lt;p&gt;
Svarah: &#22312;&#21360;&#24230;&#21475;&#38899;&#19978;&#35780;&#20272;&#33521;&#35821;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Svarah: Evaluating English ASR Systems on Indian Accents. (arXiv:2305.15760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15760
&lt;/p&gt;
&lt;p&gt;
Svarah&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#21547;&#21360;&#24230;65&#20010;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;117&#20010;&#35828;&#35805;&#32773;&#30340;9.6&#23567;&#26102;&#30340;&#33521;&#35821;&#38899;&#39057;&#36716;&#24405;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#33521;&#35821;ASR&#31995;&#32479;&#22312;&#21360;&#24230;&#21475;&#38899;&#19978;&#38656;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21360;&#24230;&#26159;&#19990;&#30028;&#19978;&#31532;&#20108;&#22823;&#35762;&#33521;&#35821;&#30340;&#22269;&#23478;&#65292;&#20854;&#20351;&#29992;&#32773;&#32422;&#26377;1.3&#20159;&#20154;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#33521;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;(ASR)&#31995;&#32479;&#26469;&#35828;&#65292;&#23545;&#21360;&#24230;&#21475;&#38899;&#30340;&#35780;&#20272;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#21360;&#24230;&#35828;&#35805;&#32773;&#22312;&#29616;&#26377;&#30340;&#33521;&#35821;ASR&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22914;LibriSpeech&#12289;Switchboard&#12289;Speech Accent Archive&#31561;&#65292;&#24471;&#21040;&#30340;&#20195;&#34920;&#24615;&#38750;&#24120;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21019;&#24314;Svarah&#26469;&#35299;&#20915;&#36825;&#19968;&#32570;&#21475;&#65292;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#26469;&#33258;&#21360;&#24230;65&#20010;&#22320;&#29702;&#20301;&#32622;&#19978;&#30340;117&#20010;&#35828;&#35805;&#32773;&#30340;9.6&#23567;&#26102;&#30340;&#33521;&#35821;&#38899;&#39057;&#36716;&#24405;&#65292;&#20855;&#26377;&#21508;&#31181;&#21475;&#38899;&#21644;&#39046;&#22495;&#30340;&#38405;&#35835;&#21644;&#20250;&#35805;&#25968;&#25454;&#65292;&#22914;&#21382;&#21490;&#12289;&#25991;&#21270;&#12289;&#26053;&#28216;&#31561;&#65292;&#30830;&#20445;&#20102;&#35789;&#27719;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#22312;Svarah&#19978;&#35780;&#20272;&#20102;6&#20010;&#24320;&#25918;&#28304;&#20195;&#30721;ASR&#27169;&#22411;&#21644;2&#20010;&#21830;&#19994;ASR&#31995;&#32479;&#65292;&#24182;&#34920;&#26126;&#21360;&#24230;&#21475;&#38899;&#19978;&#23384;&#22312;&#25913;&#36827;&#30340;&#26126;&#26174;&#31354;&#38388;&#12290;Svarah&#21644;&#25105;&#20204;&#30340;&#25152;&#26377;&#20195;&#30721;&#37117;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents. Svarah as well as all our code will be publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEMP&#30340;&#26080;&#30417;&#30563;&#20266;&#26631;&#31614;&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#37197;&#28508;&#22312;&#30340;&#23433;&#20840;&#21709;&#24212;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#23545;&#35805;&#31995;&#32479;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15757</link><description>&lt;p&gt;
&#21033;&#29992;&#24369;&#30417;&#30563;&#20449;&#21495;&#27835;&#24840;&#19981;&#23433;&#20840;&#30340;&#23545;&#35805;&#22238;&#22797;
&lt;/p&gt;
&lt;p&gt;
Healing Unsafe Dialogue Responses with Weak Supervision Signals. (arXiv:2305.15757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TEMP&#30340;&#26080;&#30417;&#30563;&#20266;&#26631;&#31614;&#37319;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#37197;&#28508;&#22312;&#30340;&#23433;&#20840;&#21709;&#24212;&#65292;&#35299;&#20915;&#22823;&#35268;&#27169;&#23545;&#35805;&#31995;&#32479;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#22823;&#35268;&#27169;&#23545;&#35805;&#31995;&#32479;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#29983;&#25104;&#36234;&#26469;&#36234;&#25285;&#24551;&#65292;&#20195;&#29702;&#20250;&#20174;&#29616;&#23454;&#19990;&#30028;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#25915;&#20987;&#24615;&#25110;&#26377;&#20559;&#35265;&#30340;&#34892;&#20026;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#26816;&#27979;&#21644;&#26367;&#25442;&#19981;&#23433;&#20840;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#30340;&#27880;&#37322;&#25104;&#26412;&#39640;&#65292;&#24182;&#19988;&#22312;&#26410;&#35265;&#36807;&#30340;&#22330;&#26223;&#20197;&#21450;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#38754;&#36866;&#24212;&#24615;&#24046;&#12290;&#27492;&#22806;&#65292;&#24573;&#30053;&#25552;&#20379;&#23433;&#20840;&#30340;&#21709;&#24212;&#65288;&#20363;&#22914;&#31616;&#21333;&#22320;&#26367;&#25442;&#27169;&#26495;&#65289;&#23558;&#23548;&#33268;&#23545;&#35805;&#20449;&#24687;&#32570;&#22833;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#20266;&#26631;&#31614;&#37319;&#26679;&#26041;&#27861;TEMP&#65292;&#21487;&#20197;&#33258;&#21160;&#20998;&#37197;&#28508;&#22312;&#30340;&#23433;&#20840;&#21709;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TEMP&#26041;&#27861;&#23558;&#21709;&#24212;&#20998;&#25104;&#20960;&#20010;&#31751;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#33258;&#36866;&#24212;&#38160;&#21270;&#37319;&#26679;&#31574;&#30053;&#36827;&#34892;&#22810;&#26631;&#31614;&#37319;&#26679;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#19981;&#23433;&#20840;&#26679;&#26412;&#36890;&#24120;&#23569;&#19988;&#20998;&#24067;&#22312;&#23614;&#37096;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have seen increasing concerns about the unsafe response generation of large-scale dialogue systems, where agents will learn offensive or biased behaviors from the real-world corpus. Some methods are proposed to address the above issue by detecting and replacing unsafe training examples in a pipeline style. Though effective, they suffer from a high annotation cost and adapt poorly to unseen scenarios as well as adversarial attacks. Besides, the neglect of providing safe responses (e.g. simply replacing with templates) will cause the information-missing problem of dialogues. To address these issues, we propose an unsupervised pseudo-label sampling method, TEMP, that can automatically assign potential safe responses. Specifically, our TEMP method groups responses into several clusters and samples multiple labels with an adaptively sharpened sampling strategy, inspired by the observation that unsafe samples in the clusters are usually few and distribute in the tail. Extensive 
&lt;/p&gt;</description></item><item><title>UniTRec&#26159;&#19968;&#20010;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#32479;&#19968;&#30340;&#23616;&#37096;-&#20840;&#23616;&#27880;&#24847;&#21147;Transformer&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#29992;&#25143;&#21382;&#21490;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#20351;&#29992;Transformer&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#22256;&#24785;&#24230;&#26469;&#26500;&#24314;&#23545;&#27604;&#20449;&#21495;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15756</link><description>&lt;p&gt;
UniTRec: &#19968;&#20010;&#32479;&#19968;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#21464;&#25442;&#22120;&#21644;&#32852;&#21512;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
UniTRec: A Unified Text-to-Text Transformer and Joint Contrastive Learning Framework for Text-based Recommendation. (arXiv:2305.15756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15756
&lt;/p&gt;
&lt;p&gt;
UniTRec&#26159;&#19968;&#20010;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#37319;&#29992;&#20102;&#32479;&#19968;&#30340;&#23616;&#37096;-&#20840;&#23616;&#27880;&#24847;&#21147;Transformer&#32534;&#30721;&#22120;&#26469;&#22788;&#29702;&#29992;&#25143;&#21382;&#21490;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#20351;&#29992;Transformer&#35299;&#30721;&#22120;&#30340;&#35821;&#35328;&#22256;&#24785;&#24230;&#26469;&#26500;&#24314;&#23545;&#27604;&#20449;&#21495;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#21487;&#20197;&#25552;&#39640;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#19982;&#20197;&#24448;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23616;&#37096;-&#20840;&#23616;&#27880;&#24847;&#21147;Transformer&#32534;&#30721;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#24314;&#27169;&#29992;&#25143;&#21382;&#21490;&#30340;&#20004;&#20010;&#23618;&#27425;&#30340;&#19978;&#19979;&#25991;&#12290;&#27492;&#22806;&#65292;&#22312;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#32534;&#30721;&#30340;&#29992;&#25143;&#21382;&#21490;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;Transformer&#35299;&#30721;&#22120;&#20272;&#35745;&#20505;&#36873;&#25991;&#26412;&#39033;&#30340;&#35821;&#35328;&#22256;&#24785;&#24230;&#65292;&#36825;&#21487;&#20197;&#20316;&#20026;&#29992;&#25143;-&#29289;&#21697;&#25991;&#26412;&#21305;&#37197;&#30340;&#31616;&#21333;&#32780;&#37325;&#35201;&#30340;&#23545;&#27604;&#20449;&#21495;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;UniTRec&#23558;&#21306;&#20998;&#24615;&#21305;&#37197;&#24471;&#20998;&#21644;&#20505;&#36873;&#25991;&#26412;&#22256;&#24785;&#24230;&#30340;&#23545;&#27604;&#30446;&#26631;&#32479;&#19968;&#36215;&#26469;&#65292;&#20197;&#20849;&#21516;&#22686;&#24378;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#33616;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;UniTRec&#22312;&#19977;&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#25512;&#33616;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;SOTA&#24615;&#33021;&#12290;&#20195;&#30721;&#21487;&#22312;https://anonymous.com&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior study has shown that pretrained language models (PLM) can boost the performance of text-based recommendation. In contrast to previous works that either use PLM to encode user history as a whole input text, or impose an additional aggregation network to fuse multi-turn history representations, we propose a unified local- and global-attention Transformer encoder to better model two-level contexts of user history. Moreover, conditioned on user history encoded by Transformer encoders, our framework leverages Transformer decoders to estimate the language perplexity of candidate text items, which can serve as a straightforward yet significant contrastive signal for user-item text matching. Based on this, our framework, UniTRec, unifies the contrastive objectives of discriminative matching scores and candidate text perplexity to jointly enhance text-based recommendation. Extensive evaluation shows that UniTRec delivers SOTA performance on three text-based recommendation tasks. Code is a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#36716;&#20889;&#27861;&#65292;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#38024;&#23545;&#21313;&#31181;&#31361;&#21413;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#65292;&#20351;&#29992;Tacotron 2&#26550;&#26500;&#30340;TTS&#31995;&#32479;&#65292;&#20165;&#20351;&#29992;&#21704;&#33832;&#20811;&#35821;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#24182;&#29983;&#25104;&#20854;&#20182;&#31361;&#21413;&#35821;&#26063;&#30340;&#35821;&#38899;&#12290;</title><link>http://arxiv.org/abs/2305.15749</link><description>&lt;p&gt;
&#20351;&#29992;&#36716;&#20889;&#27861;&#30340;&#31361;&#21413;&#35821;&#22810;&#35821;&#31181;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multilingual Text-to-Speech Synthesis for Turkic Languages Using Transliteration. (arXiv:2305.15749v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15749
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#36716;&#20889;&#27861;&#65292;&#26412;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#31995;&#32479;&#65292;&#38024;&#23545;&#21313;&#31181;&#31361;&#21413;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#65292;&#20351;&#29992;Tacotron 2&#26550;&#26500;&#30340;TTS&#31995;&#32479;&#65292;&#20165;&#20351;&#29992;&#21704;&#33832;&#20811;&#35821;&#35757;&#32451;&#25968;&#25454;&#65292;&#21487;&#20197;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;&#24182;&#29983;&#25104;&#20854;&#20182;&#31361;&#21413;&#35821;&#26063;&#30340;&#35821;&#38899;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#22810;&#35821;&#31181;&#25991;&#26412;&#36716;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#31995;&#32479;&#65292;&#38024;&#23545;&#21313;&#31181;&#36164;&#28304;&#21294;&#20047;&#30340;&#31361;&#21413;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#65306;&#38463;&#22622;&#25308;&#30086;&#35821;&#12289;&#24052;&#20160;&#22522;&#23572;&#35821;&#12289;&#21704;&#33832;&#20811;&#35821;&#12289;&#26607;&#23572;&#20811;&#23388;&#35821;&#12289;&#33832;&#21704;&#35821;&#12289;&#38801;&#38780;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#12289;&#22303;&#24211;&#26364;&#35821;&#12289;&#32500;&#21566;&#23572;&#35821;&#21644;&#20044;&#20857;&#21035;&#20811;&#35821;&#12290;&#29305;&#21035;&#38024;&#23545;&#38646;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#65292;&#21363;&#20351;&#29992;&#19968;&#20010;&#35821;&#35328;&#30340;&#25968;&#25454;&#35757;&#32451;TTS&#27169;&#22411;&#65292;&#26469;&#21512;&#25104;&#26410;&#32463;&#35265;&#36807;&#30340;&#35821;&#35328;&#30340;&#35821;&#38899;&#12290;&#25105;&#20204;&#37319;&#29992;Tacotron 2&#26550;&#26500;&#30340;&#31471;&#21040;&#31471;TTS&#31995;&#32479;&#65292;&#20165;&#20351;&#29992;&#21704;&#33832;&#20811;&#35821;&#30340;&#29616;&#26377;&#25968;&#25454;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#31361;&#21413;&#35821;&#30340;&#23383;&#27597;&#26144;&#23556;&#21040;&#22269;&#38469;&#35821;&#38899;&#31526;&#21495;&#65288;IPA&#65289;&#30340;&#31526;&#21495;&#65292;&#20877;&#36716;&#25442;&#20026;&#21704;&#33832;&#20811;&#35821;&#30340;&#23383;&#27597;&#65292;&#26469;&#20026;&#20854;&#20182;&#31361;&#21413;&#35821;&#35328;&#29983;&#25104;&#35821;&#38899;&#12290;&#20026;&#20102;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#20027;&#35266;&#22320;&#35780;&#20272;&#20102;&#22810;&#35821;&#31181;&#31361;&#21413;TTS&#27169;&#22411;&#65292;&#24182;&#33719;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20351;&#23454;&#39564;&#21487;&#22797;&#21046;&#65292;&#25105;&#20204;&#22312;GitHub&#23384;&#20648;&#24211;&#20013;&#20844;&#24320;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to build a multilingual text-to-speech (TTS) synthesis system for ten lower-resourced Turkic languages: Azerbaijani, Bashkir, Kazakh, Kyrgyz, Sakha, Tatar, Turkish, Turkmen, Uyghur, and Uzbek. We specifically target the zero-shot learning scenario, where a TTS model trained using the data of one language is applied to synthesise speech for other, unseen languages. An end-to-end TTS system based on the Tacotron 2 architecture was trained using only the available data of the Kazakh language. To generate speech for the other Turkic languages, we first mapped the letters of the Turkic alphabets onto the symbols of the International Phonetic Alphabet (IPA), which were then converted to the Kazakh alphabet letters. To demonstrate the feasibility of the proposed approach, we evaluated the multilingual Turkic TTS model subjectively and obtained promising results. To enable replication of the experiments, we make our code and dataset publicly available in our GitHub repository.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;NEL&#21450;&#20854;&#23545;NIL&#39044;&#27979;&#38382;&#39064;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32570;&#22833;&#23454;&#20307;&#21644;&#38750;&#23454;&#20307;&#30701;&#35821;&#22343;&#23545;NIL&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.15725</link><description>&lt;p&gt;
&#23398;&#20064;&#19981;&#38142;&#25509;&#65306;&#25506;&#32034;&#23454;&#20307;&#38142;&#25509;&#20013;&#30340;NIL&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learn to Not Link: Exploring NIL Prediction in Entity Linking. (arXiv:2305.15725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15725
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;NEL&#21450;&#20854;&#23545;NIL&#39044;&#27979;&#38382;&#39064;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32570;&#22833;&#23454;&#20307;&#21644;&#38750;&#23454;&#20307;&#30701;&#35821;&#22343;&#23545;NIL&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#35821;&#20041;&#29305;&#24449;&#24050;&#21462;&#24471;&#37325;&#22823;&#25104;&#21151;&#65292;&#28982;&#32780;&#23545;&#20110;&#23547;&#25214;&#27809;&#26377;&#30456;&#24212;&#30693;&#35782;&#24211;&#23454;&#20307;&#30340;&#25552;&#21450;&#30340;NIL&#39044;&#27979;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#20851;&#27880;&#12290;&#25105;&#20204;&#23558;&#38142;&#25509;&#21040;NIL&#30340;&#25552;&#21450;&#20998;&#20026;&#32570;&#22833;&#23454;&#20307;&#21644;&#38750;&#23454;&#20307;&#30701;&#35821;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#20307;&#38142;&#25509;&#25968;&#25454;&#38598;NEL&#65292;&#37325;&#28857;&#20851;&#27880;NIL&#39044;&#27979;&#38382;&#39064;&#12290;NEL&#20197;&#19981;&#26126;&#30830;&#30340;&#23454;&#20307;&#20316;&#20026;&#31181;&#23376;&#65292;&#22312;&#32500;&#22522;&#30334;&#31185;&#35821;&#26009;&#24211;&#20013;&#25910;&#38598;&#30456;&#20851;&#30340;&#25552;&#21450;&#19978;&#19979;&#25991;&#65292;&#24182;&#36890;&#36807;&#20154;&#24037;&#27880;&#37322;&#21644;&#23454;&#20307;&#23631;&#34109;&#30830;&#20445;&#38142;&#25509;&#21040;NIL&#30340;&#25552;&#21450;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#20351;&#29992;&#24191;&#27867;&#20351;&#29992;&#30340;&#21452;&#32534;&#30721;&#22120;&#21644;&#20132;&#21449;&#32534;&#30721;&#22120;&#23454;&#20307;&#38142;&#25509;&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;NIL&#25552;&#21450;&#23545;NIL&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312; https://github.com/solitaryzero/NIL_EL &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking models have achieved significant success via utilizing pretrained language models to capture semantic features. However, the NIL prediction problem, which aims to identify mentions without a corresponding entity in the knowledge base, has received insufficient attention. We categorize mentions linking to NIL into Missing Entity and Non-Entity Phrase, and propose an entity linking dataset NEL that focuses on the NIL prediction problem. NEL takes ambiguous entities as seeds, collects relevant mention context in the Wikipedia corpus, and ensures the presence of mentions linking to NIL by human annotation and entity masking. We conduct a series of experiments with the widely used bi-encoder and cross-encoder entity linking models, results show that both types of NIL mentions in training data have a significant influence on the accuracy of NIL prediction. Our code and dataset can be accessed at https://github.com/solitaryzero/NIL_EL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15722</link><description>&lt;p&gt;
&#38754;&#21521;&#20195;&#30721;&#28151;&#21512;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data. (arXiv:2305.15722v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#20351;&#29992;&#19981;&#21516;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20197;&#25552;&#39640;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#31561;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#20195;&#30721;&#28151;&#21512;&#8221;&#26159;&#25351;&#22312;&#21516;&#19968;&#27573;&#25991;&#26412;&#20013;&#20351;&#29992;&#22810;&#31181;&#35821;&#35328;&#30340;&#29616;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#23384;&#22312;&#65292;&#24182;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#32435;&#12290;&#26816;&#27979;&#35821;&#35328;&#20013;&#30340;&#22806;&#26469;&#20803;&#32032;&#24182;&#27491;&#30830;&#22788;&#29702;&#23427;&#20204;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#35768;&#22810;&#20154;&#20351;&#29992;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#20854;&#20013;&#20219;&#19968;&#35821;&#35328;&#37117;&#26080;&#27861;&#29702;&#35299;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20302;&#36164;&#28304;&#30340;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#24182;&#25552;&#39640;&#19981;&#21516;&#20195;&#30721;&#28151;&#21512;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#20998;&#26512;&#12289;&#24773;&#32490;&#35782;&#21035;&#21644;&#20167;&#24680;&#35328;&#35770;&#35782;&#21035;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#39044;&#35757;&#32451;&#30340;&#19981;&#21516;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;HingBERT&#12289;HingRoBERTa&#12289;HingRoBERTa-Mixed&#12289;mBERT&#65289;&#21644;&#38750;&#20195;&#30721;&#28151;&#21512;&#27169;&#22411;&#65288;&#22914;AlBERT&#12289;BERT&#12289;RoBERTa&#65289;&#65292;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Code Mixed" refers to the use of more than one language in the same text. This phenomenon is predominantly observed on social media platforms, with an increasing amount of adaptation as time goes on. It is critical to detect foreign elements in a language and process them correctly, as a considerable number of individuals are using code-mixed languages that could not be comprehended by understanding one of those languages. In this work, we focus on low-resource Hindi-English code-mixed language and enhancing the performance of different code-mixed natural language processing tasks such as sentiment analysis, emotion recognition, and hate speech identification. We perform a comparative analysis of different Transformer-based language Models pre-trained using unsupervised approaches. We have included the code-mixed models like HingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models like AlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-En
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#8212;&#8212;&#24085;&#32047;&#25176;&#20114;&#25915;&#65288;Pareto-MD&#65289;&#65292;&#26088;&#22312;&#23558;&#24085;&#32047;&#25176;&#21069;&#27839;&#21521;&#22806;&#25512;&#36827;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#26435;&#34913;&#65292;&#20197;&#25552;&#39640;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15718</link><description>&lt;p&gt;
&#36208;&#21521;&#26356;&#39640;&#30340;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Towards Higher Pareto Frontier in Multilingual Machine Translation. (arXiv:2305.15718v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#8212;&#8212;&#24085;&#32047;&#25176;&#20114;&#25915;&#65288;Pareto-MD&#65289;&#65292;&#26088;&#22312;&#23558;&#24085;&#32047;&#25176;&#21069;&#27839;&#21521;&#22806;&#25512;&#36827;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#26435;&#34913;&#65292;&#20197;&#25552;&#39640;&#22810;&#35821;&#35328;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22810;&#35821;&#26009;&#24211;&#30340;&#38271;&#23614;&#20998;&#24067;&#24418;&#25104;&#20102;&#24085;&#32047;&#25176;&#26368;&#20248;&#21270;&#30340;&#25361;&#25112;&#65292;&#21363;&#20026;&#20102;&#20248;&#21270;&#26576;&#20123;&#35821;&#35328;&#30340;&#32763;&#35793;&#65292;&#21487;&#33021;&#25439;&#23475;&#20854;&#20182;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#24179;&#34913;&#35757;&#32451;&#31574;&#30053;&#31561;&#21516;&#20110;&#19968;&#31995;&#21015;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#65292;&#23427;&#20204;&#22312;&#24085;&#32047;&#25176;&#21069;&#27839;&#19978;&#36827;&#34892;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#8212;&#8212;&#24085;&#32047;&#25176;&#20114;&#25915;&#65288;Pareto-MD&#65289;&#65292;&#26088;&#22312;&#23558;&#24085;&#32047;&#25176;&#21069;&#27839;&#21521;&#22806;&#25512;&#36827;&#65292;&#32780;&#19981;&#26159;&#36827;&#34892;&#26435;&#34913;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;Pareto-MD&#20849;&#21516;&#35757;&#32451;&#20004;&#20010;&#20559;&#21521;&#19981;&#21516;&#35821;&#35328;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#35753;&#23427;&#20204;&#30456;&#20114;&#23398;&#20064;&#20248;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#24378;&#22823;&#30340;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#25299;&#23485;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;WMT&#21644;TED&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37117;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual neural machine translation has witnessed remarkable progress in recent years. However, the long-tailed distribution of multilingual corpora poses a challenge of Pareto optimization, i.e., optimizing for some languages may come at the cost of degrading the performance of others. Existing balancing training strategies are equivalent to a series of Pareto optimal solutions, which trade off on a Pareto frontier. In this work, we propose a new training framework, Pareto Mutual Distillation (Pareto-MD), towards pushing the Pareto frontier outwards rather than making trade-offs. Specifically, Pareto-MD collaboratively trains two Pareto optimal solutions that favor different languages and allows them to learn from the strengths of each other via knowledge distillation. Furthermore, we introduce a novel strategy to enable stronger communication between Pareto optimal solutions and broaden the applicability of our approach. Experimental results on the widely-used WMT and TED dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#36739;&#24369;&#24320;&#28304;&#27169;&#22411;&#27169;&#20223;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24230;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#34920;&#29616;&#21487;&#33021;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20173;&#26080;&#27861;&#21462;&#20195;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.15717</link><description>&lt;p&gt;
&#27169;&#20223;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#38169;&#35823;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
The False Promise of Imitating Proprietary LLMs. (arXiv:2305.15717v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#20351;&#29992;&#36739;&#24369;&#24320;&#28304;&#27169;&#22411;&#27169;&#20223;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24230;&#65292;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#34920;&#29616;&#21487;&#33021;&#38750;&#24120;&#20986;&#33394;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#20173;&#26080;&#27861;&#21462;&#20195;&#19987;&#26377;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#25552;&#39640;&#36739;&#24369;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#26032;&#26041;&#27861;&#26159;&#22522;&#20110;&#36739;&#24378;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#24494;&#35843;&#65292;&#20363;&#22914;&#19987;&#26377;&#31995;&#32479;ChatGPT&#65288;&#20363;&#22914;Alpaca&#12289;Self-Instruct&#31561;&#65289;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#36739;&#24369;&#30340;&#24320;&#28304;&#27169;&#22411;&#24265;&#20215;&#22320;&#27169;&#20223;&#19987;&#26377;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#23545;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808; &#20351;&#29992;&#19981;&#21516;&#30340;&#22522;&#30784;&#27169;&#22411;&#22823;&#23567;&#65288;1.5B-13B&#65289;&#12289;&#25968;&#25454;&#28304;&#21644;&#27169;&#20223;&#25968;&#25454;&#37327;&#65288;0.3M-150M&#20196;&#29260;&#65289;&#26469;&#24494;&#35843;&#19968;&#31995;&#21015;&#27169;&#20223;ChatGPT&#30340;LM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20247;&#21253;&#35780;&#20272;&#21644;&#35268;&#33539;&#30340;NLP&#22522;&#20934;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#26368;&#21021;&#65292;&#25105;&#20204;&#23545;&#27169;&#20223;&#27169;&#22411;&#30340;&#36755;&#20986;&#36136;&#37327;&#24863;&#21040;&#24778;&#35766;&#8212;&#8212;&#23427;&#20204;&#20284;&#20046;&#26356;&#25797;&#38271;&#25353;&#29031;&#25351;&#31034;&#36827;&#34892;&#25805;&#20316;&#65292;&#24182;&#19988;&#20247;&#21253;&#24037;&#20316;&#32773;&#23558;&#23427;&#20204;&#30340;&#36755;&#20986;&#35780;&#20026;&#19982;ChatGPT&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#36827;&#34892;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#33258;&#21160;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#26410;&#22312;&#27169;&#20223;&#27169;&#22411;&#20013;&#24471;&#21040;&#20805;&#20998;&#25903;&#25345;&#30340;&#20219;&#21153;&#30456;&#27604;&#65292;&#27169;&#20223;&#27169;&#22411;&#22312;&#32553;&#23567;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#19982;ChatGPT&#20043;&#38388;&#24046;&#36317;&#26041;&#38754;&#24110;&#21161;&#19981;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15689</link><description>&lt;p&gt;
&#20811;&#26381;&#25552;&#31034;&#25200;&#21160;&#25935;&#24863;&#24615;&#30340;&#38646;&#26679;&#26412;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38646;&#26679;&#26412;&#26041;&#27861;&#65292;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#39640;&#36136;&#37327;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#25490;&#21517;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#65292;&#24182;&#22312;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#21487;&#20197;&#24110;&#21161;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#30693;&#35782;&#36827;&#34892;&#20108;&#20803;&#21477;&#32423;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#35774;&#32622;&#65292;&#20351;&#29992;&#25163;&#21160;&#25110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25552;&#31034;&#26469;&#24494;&#35843;&#24773;&#24863;&#20998;&#31867;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#23545;&#25152;&#20351;&#29992;&#25552;&#31034;&#30340;&#25200;&#21160;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23569;&#37327;&#24102;&#26631;&#31614;&#23454;&#20363;&#36827;&#34892;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#21644;&#25552;&#31034;&#25490;&#24207;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#20026;&#25152;&#32473;&#23450;&#30340;&#20219;&#21153;&#25214;&#21040;&#39640;&#36136;&#37327;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#32473;&#23450;&#19968;&#20010;&#22522;&#30784;&#25552;&#31034;&#65292;&#37319;&#29992;&#20301;&#32622;&#12289;&#25512;&#29702;&#21644;&#37322;&#20041;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#22810;&#20010;&#31867;&#20284;&#20110;&#22522;&#30784;&#25552;&#31034;&#30340;&#25552;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#23545;&#25552;&#31034;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#20174;&#23454;&#39564;&#19978;&#35777;&#26126;&#65292;&#25490;&#21517;&#38752;&#21069;&#30340;&#25552;&#31034;&#20855;&#26377;&#24456;&#39640;&#30340;&#36136;&#37327;&#65292;&#24182;&#22312;&#25552;&#31034;&#25200;&#21160;&#40065;&#26834;&#24615;&#21644;&#25972;&#20307;&#20934;&#30830;&#24615;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#22522;&#30784;&#25552;&#31034;&#21644;&#20854;&#20182;&#29616;&#26377;&#30340;&#25552;&#31034;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have demonstrated that natural-language prompts can help to leverage the knowledge learned by pre-trained language models for the binary sentence-level sentiment classification task. Specifically, these methods utilize few-shot learning settings to fine-tune the sentiment classification model using manual or automatically generated prompts. However, the performance of these methods is sensitive to the perturbations of the utilized prompts. Furthermore, these methods depend on a few labeled instances for automatic prompt generation and prompt ranking. This study aims to find high-quality prompts for the given task in a zero-shot setting. Given a base prompt, our proposed approach automatically generates multiple prompts similar to the base prompt employing positional, reasoning, and paraphrasing techniques and then ranks the prompts using a novel metric. We empirically demonstrate that the top-ranked prompts are high-quality and significantly outperform the base prompt an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; RewriteLM&#65292;&#19968;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; OpenRewriteEval &#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#25105;&#20204;&#37319;&#29992;&#26032;&#30340;&#31574;&#30053;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#25351;&#20196;&#21644;&#20559;&#22909;&#25968;&#25454;&#29983;&#25104;&#65292;&#20174;&#32780;&#20026;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#25552;&#20379;&#26356;&#22909;&#30340;&#35780;&#20272;&#25163;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.15685</link><description>&lt;p&gt;
RewriteLM&#65306;&#19968;&#31181;&#38754;&#21521;&#25991;&#26412;&#37325;&#20889;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15685
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; RewriteLM&#65292;&#19968;&#31181;&#25351;&#20196;&#35843;&#25972;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; OpenRewriteEval &#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#31867;&#22411;&#30340;&#24320;&#25918;&#24335;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#12290;&#25105;&#20204;&#37319;&#29992;&#26032;&#30340;&#31574;&#30053;&#26469;&#20419;&#36827;&#22810;&#26679;&#30340;&#25351;&#20196;&#21644;&#20559;&#22909;&#25968;&#25454;&#29983;&#25104;&#65292;&#20174;&#32780;&#20026;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#25552;&#20379;&#26356;&#22909;&#30340;&#35780;&#20272;&#25163;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#38271;&#31687;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34920;&#36798;&#26469;&#30340;&#24778;&#20154;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28982;&#32780;&#29992;&#25143;&#23545;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#30340;&#26399;&#26395;&#20540;&#24456;&#39640;&#65292;&#27169;&#22411;&#20135;&#29983;&#30340;&#24847;&#22806;&#37325;&#20889;&#65288;&#8220;&#24187;&#35273;&#8221;&#65289;&#20250;&#23545;&#20854;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#35780;&#20272;&#22522;&#20934;&#20027;&#35201;&#20851;&#27880;&#26377;&#38480;&#30340;&#37325;&#20889;&#39118;&#26684;&#21644;&#21477;&#23376;&#32423;&#37325;&#20889;&#65292;&#32780;&#19981;&#26159;&#38271;&#31687;&#24320;&#25918;&#24335;&#37325;&#20889;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;OpenRewriteEval&#65292;&#23427;&#28085;&#30422;&#20102;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#34920;&#36798;&#30340;&#21508;&#31181;&#37325;&#20889;&#31867;&#22411;&#12290;&#23427;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#20419;&#36827;&#38271;&#31687;&#25991;&#26412;&#24320;&#25918;&#24335;&#37325;&#20889;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#22522;&#32447;&#27169;&#22411;RewriteLM&#65292;&#19968;&#20010;&#29992;&#20110;&#38271;&#31687;&#25991;&#26412;&#37325;&#20889;&#30340;&#25351;&#20196;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20123;&#26032;&#31574;&#30053;&#65292;&#20197;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#20419;&#36827;&#29983;&#25104;&#22810;&#26679;&#30340;&#25351;&#20196;&#21644;&#20559;&#22909;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities in long-form text generation tasks expressed through natural language instructions. However, user expectations for long-form text rewriting is high, and unintended rewrites (''hallucinations'') produced by the model can negatively impact its overall performance. Existing evaluation benchmarks primarily focus on limited rewriting styles and sentence-level rewriting rather than long-form open-ended rewriting.We introduce OpenRewriteEval, a novel benchmark that covers a wide variety of rewriting types expressed through natural language instructions. It is specifically designed to facilitate the evaluation of open-ended rewriting of long-form texts. In addition, we propose a strong baseline model, RewriteLM, an instruction-tuned large language model for long-form text rewriting. We develop new strategies that facilitate the generation of diverse instructions and preference data with minimal human intervention.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24178;&#25200;&#30340;&#33258;&#25105;&#30417;&#30563;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#24341;&#23548;&#27880;&#24847;&#21147;&#23398;&#20064;&#65292;&#26080;&#38656;&#20219;&#20309;&#27880;&#37322;&#24320;&#38144;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#24403;&#21069;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.15684</link><description>&lt;p&gt;
&#22522;&#20110;&#24178;&#25200;&#30340;&#33258;&#25105;&#30417;&#30563;&#27880;&#24847;&#21147;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#27880;&#24847;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Perturbation-based Self-supervised Attention for Attention Bias in Text Classification. (arXiv:2305.15684v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24178;&#25200;&#30340;&#33258;&#25105;&#30417;&#30563;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#24341;&#23548;&#27880;&#24847;&#21147;&#23398;&#20064;&#65292;&#26080;&#38656;&#20219;&#20309;&#27880;&#37322;&#24320;&#38144;&#65292;&#33021;&#22815;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#24403;&#21069;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#27604;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#20256;&#32479;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#24120;&#36807;&#20110;&#20851;&#27880;&#39057;&#32321;&#20986;&#29616;&#30340;&#21333;&#35789;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#37327;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#25165;&#33021;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24178;&#25200;&#30340;&#33258;&#25105;&#30417;&#30563;&#27880;&#24847;&#21147;&#26041;&#27861;&#26469;&#24341;&#23548;&#27880;&#24847;&#21147;&#23398;&#20064;&#65292;&#26080;&#38656;&#20219;&#20309;&#27880;&#37322;&#24320;&#38144;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23613;&#21487;&#33021;&#22320;&#28155;&#21152;&#22122;&#22768;&#21040;&#21477;&#23376;&#20013;&#30340;&#25152;&#26377;&#21333;&#35789;&#65292;&#32780;&#19981;&#25913;&#21464;&#23427;&#20204;&#30340;&#35821;&#20041;&#21644;&#39044;&#27979;&#12290;&#25105;&#20204;&#20551;&#35774;&#33021;&#22815;&#23481;&#24525;&#26356;&#22810;&#22122;&#22768;&#30340;&#21333;&#35789;&#24847;&#20041;&#26356;&#19981;&#37325;&#35201;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#35813;&#20449;&#24687;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#20998;&#24067;&#12290;&#22312;&#19977;&#20010;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24403;&#21069;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#19988;&#27604;&#29616;&#26377;&#30340;&#33258;&#25105;&#30417;&#30563;&#26041;&#27861;&#26356;&#26377;&#25928;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#21487;&#35270;&#21270;&#20998;&#26512;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In text classification, the traditional attention mechanisms usually focus too much on frequent words, and need extensive labeled data in order to learn. This paper proposes a perturbation-based self-supervised attention approach to guide attention learning without any annotation overhead. Specifically, we add as much noise as possible to all the words in the sentence without changing their semantics and predictions. We hypothesize that words that tolerate more noise are less significant, and we can use this information to refine the attention distribution. Experimental results on three text classification tasks show that our approach can significantly improve the performance of current attention-based models, and is more effective than existing self-supervised methods. We also provide a visualization analysis to verify the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MultiSim&#22522;&#20934;&#65292;&#23427;&#21253;&#21547;&#20102;27&#20010;&#36164;&#28304;&#12289;12&#31181;&#35821;&#35328;&#36229;&#36807;1.7&#30334;&#19975;&#20010;&#22797;&#26434;-&#31616;&#21333;&#30340;&#21477;&#23376;&#23545;&#12290;&#20351;&#29992;&#35813;&#22522;&#20934;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#24102;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20420;&#35821;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15678</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#38750;&#33521;&#35821;&#25991;&#26412;&#31616;&#21270;&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Revisiting non-English Text Simplification: A Unified Multilingual Benchmark. (arXiv:2305.15678v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;MultiSim&#22522;&#20934;&#65292;&#23427;&#21253;&#21547;&#20102;27&#20010;&#36164;&#28304;&#12289;12&#31181;&#35821;&#35328;&#36229;&#36807;1.7&#30334;&#19975;&#20010;&#22797;&#26434;-&#31616;&#21333;&#30340;&#21477;&#23376;&#23545;&#12290;&#20351;&#29992;&#35813;&#22522;&#20934;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#24102;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#20420;&#35821;&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#26041;&#38754;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33521;&#35821;&#33258;&#21160;&#25991;&#26412;&#31616;&#21270;&#65288;ATS&#65289;&#30740;&#31350;&#20013;&#39640;&#36136;&#37327;&#12289;&#22823;&#35268;&#27169;&#30340;&#33521;&#35821;&#36164;&#28304;&#30340;&#36827;&#23637;&#23558;&#33521;&#35821;ATS&#30740;&#31350;&#30340;&#21069;&#27839;&#25512;&#21521;&#20102;&#26356;&#39640;&#30340;&#27700;&#24179;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#19968;&#20010;&#35206;&#30422;&#22810;&#31181;&#35821;&#35328;&#20013;&#30340;&#22797;&#26434;-&#31616;&#27905;&#21477;&#23376;&#23545;&#30340;&#22810;&#26679;&#21270;&#35780;&#20272;&#22522;&#20934;&#65292;&#23545;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#30340;&#30740;&#31350;&#24037;&#20316;&#36739;&#23569;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MultiSim&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#25910;&#38598;&#20102;12&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#36229;&#36807;1.7&#30334;&#19975;&#20010;&#22797;&#26434;-&#31616;&#21333;&#21477;&#23376;&#23545;&#30340;27&#20010;&#36164;&#28304;&#30340;&#38598;&#21512;&#12290;&#36825;&#20010;&#22522;&#20934;&#23558;&#40723;&#21169;&#30740;&#31350;&#24320;&#21457;&#26356;&#26377;&#25928;&#30340;&#22810;&#35821;&#35328;&#25991;&#26412;&#31616;&#21270;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;MultiSim&#19982;&#39044;&#35757;&#32451;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#22312;&#38750;&#33521;&#35821;&#29615;&#22659;&#20013;&#36827;&#34892;&#22810;&#35821;&#35328;&#35757;&#32451;&#21487;&#20197;&#24102;&#26469;&#20196;&#20154;&#20852;&#22859;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20420;&#35821;&#22312;&#38646;-shot&#36328;&#35821;&#35328;&#36716;&#31227;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20351;&#29992;BLOOM-176b&#30340;&#23569;&#37327;&#25552;&#31034;&#21487;&#20197;&#36798;&#21040;&#21487;&#27604;&#30340;&#21442;&#32771;&#31616;&#21270;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplif
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#35299;&#37322;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#35777;&#25454;&#21333;&#35789;&#21644;&#35821;&#27861;&#38169;&#35823;&#31867;&#22411;&#27880;&#37322;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#32447;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#29702;&#35299;&#36825;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#27861;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.15676</link><description>&lt;p&gt;
&#29992;&#35299;&#37322;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Grammatical Error Correction Systems with Explanations. (arXiv:2305.15676v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#35299;&#37322;&#25552;&#39640;&#35821;&#27861;&#38169;&#35823;&#20462;&#27491;&#31995;&#32479;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#21547;&#35777;&#25454;&#21333;&#35789;&#21644;&#35821;&#27861;&#38169;&#35823;&#31867;&#22411;&#27880;&#37322;&#30340;&#22823;&#25968;&#25454;&#38598;&#65292;&#25214;&#21040;&#38169;&#35823;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#32447;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#29702;&#35299;&#36825;&#20010;&#20219;&#21153;&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#35299;&#37322;&#21487;&#20197;&#24110;&#21161;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;&#35821;&#27861;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#26657;&#27491;&#31995;&#32479;&#36890;&#36807;&#26816;&#27979;&#21644;&#26356;&#27491;&#35821;&#35328;&#38169;&#35823;&#26469;&#25552;&#21319;&#20070;&#20889;&#20132;&#27969;&#12290;&#20026;&#20102;&#24110;&#21161;&#35821;&#35328;&#23398;&#20064;&#32773;&#26356;&#22909;&#22320;&#29702;&#35299;GEC&#31995;&#32479;&#20026;&#20160;&#20040;&#20570;&#20986;&#26576;&#31181;&#26356;&#27491;&#65292;&#38169;&#35823;&#30340;&#21407;&#22240;&#65288;&#35777;&#25454;&#21333;&#35789;&#65289;&#21644;&#30456;&#24212;&#30340;&#38169;&#35823;&#31867;&#22411;&#26159;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#20026;&#20102;&#29992;&#35299;&#37322;&#22686;&#24378;GEC&#31995;&#32479;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;EXPECT&#65292;&#19968;&#20010;&#22823;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#35777;&#25454;&#21333;&#35789;&#21644;&#35821;&#27861;&#38169;&#35823;&#31867;&#22411;&#30340;&#27880;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#32447;&#21644;&#20998;&#26512;&#26041;&#27861;&#26469;&#29702;&#35299;&#36825;&#20010;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#35780;&#20272;&#35777;&#26126;&#65292;&#25105;&#20204;&#21487;&#35299;&#37322;&#30340;GEC&#31995;&#32479;&#30340;&#35299;&#37322;&#33021;&#22815;&#24110;&#21161;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#30830;&#23450;&#26159;&#21542;&#25509;&#21463;&#26356;&#27491;&#24314;&#35758;&#65292;&#24182;&#29702;&#35299;&#30456;&#20851;&#30340;&#35821;&#27861;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction systems improve written communication by detecting and correcting language mistakes. To help language learners better understand why the GEC system makes a certain correction, the causes of errors (evidence words) and the corresponding error types are two key factors. To enhance GEC systems with explanations, we introduce EXPECT, a large dataset annotated with evidence words and grammatical error types. We propose several baselines and anlysis to understand this task. Furthermore, human evaluation verifies our explainable GEC system's explanations can assist second-language learners in determining whether to accept a correction suggestion and in understanding the associated grammar rule.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#22270;&#20070;&#25512;&#33616;&#26694;&#26550;BookGPT&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#25216;&#26415;&#24212;&#29992;&#20110;&#22270;&#20070;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#19977;&#31181;&#20219;&#21153;&#65292;&#21363;&#22270;&#20070;&#35780;&#20998;&#25512;&#33616;&#12289;&#29992;&#25143;&#35780;&#20998;&#25512;&#33616;&#21644;&#22270;&#20070;&#25688;&#35201;&#25512;&#33616;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20070;&#25512;&#33616;&#30340;&#26377;&#21147;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.15673</link><description>&lt;p&gt;
BookGPT&#65306;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#22270;&#20070;&#25512;&#33616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BookGPT: A General Framework for Book Recommendation Empowered by Large Language Model. (arXiv:2305.15673v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#22270;&#20070;&#25512;&#33616;&#26694;&#26550;BookGPT&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#25216;&#26415;&#24212;&#29992;&#20110;&#22270;&#20070;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#19977;&#31181;&#20219;&#21153;&#65292;&#21363;&#22270;&#20070;&#35780;&#20998;&#25512;&#33616;&#12289;&#29992;&#25143;&#35780;&#20998;&#25512;&#33616;&#21644;&#22270;&#20070;&#25688;&#35201;&#25512;&#33616;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#20070;&#25512;&#33616;&#30340;&#26377;&#21147;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#21464;&#21270;&#65292;&#21508;&#20010;&#39046;&#22495;&#30340;&#35768;&#22810;&#32463;&#20856;&#22330;&#26223;&#37325;&#26032;&#23637;&#29616;&#20986;&#26032;&#30340;&#26426;&#36935;&#12290;&#26412;&#25991;&#23558;ChatGPT&#20316;&#20026;&#24314;&#27169;&#23545;&#35937;&#65292;&#39318;&#27425;&#23558;LLM&#25216;&#26415;&#24182;&#20837;&#20256;&#32479;&#30340;&#22270;&#20070;&#36164;&#28304;&#29702;&#35299;&#21644;&#25512;&#33616;&#22330;&#26223;&#20013;&#65292;&#24182;&#20184;&#35832;&#23454;&#36341;&#12290;&#26412;&#25991;&#22522;&#20110;ChatGPT&#26500;&#24314;&#20102;&#31867;&#20284;&#20110;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22270;&#20070;&#25512;&#33616;&#31995;&#32479;&#26694;&#26550;&#65288;BookGPT&#65289;&#65292;&#35797;&#22270;&#23558;ChatGPT&#24212;&#29992;&#20110;&#19977;&#31181;&#20856;&#22411;&#20219;&#21153;&#30340;&#25512;&#33616;&#24314;&#27169;&#65306;&#22270;&#20070;&#35780;&#20998;&#25512;&#33616;&#65292;&#29992;&#25143;&#35780;&#20998;&#25512;&#33616;&#21644;&#22270;&#20070;&#25688;&#35201;&#25512;&#33616;&#65292;&#25506;&#32034;LLM&#25216;&#26415;&#22312;&#22270;&#20070;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#26681;&#25454;&#19981;&#21516;&#30340;&#22270;&#20070;&#25512;&#33616;&#20219;&#21153;&#35780;&#20272;&#26041;&#26696;&#21644;&#29616;&#26377;&#30340;&#32463;&#20856;&#25512;&#33616;&#27169;&#22411;&#65292;&#35752;&#35770;&#20102;BookGPT&#22312;&#22270;&#20070;&#25512;&#33616;&#22330;&#26223;&#19979;&#30340;&#20248;&#32570;&#28857;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#35777;&#27604;&#36739;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#22522;&#20110;LLM&#25216;&#26415;&#30340;BookGPT&#26694;&#26550;&#21487;&#20197;&#20026;&#22270;&#20070;&#25512;&#33616;&#39046;&#22495;&#24102;&#26469;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the continuous development and change exhibited by large language model (LLM) technology, represented by generative pretrained transformers (GPTs), many classic scenarios in various fields have re-emerged with new opportunities. This paper takes ChatGPT as the modeling object, incorporates LLM technology into the typical book resource understanding and recommendation scenario for the first time, and puts it into practice. By building a ChatGPT-like book recommendation system (BookGPT) framework based on ChatGPT, this paper attempts to apply ChatGPT to recommendation modeling for three typical tasks, book rating recommendation, user rating recommendation, and book summary recommendation, and explores the feasibility of LLM technology in book recommendation scenarios. At the same time, based on different evaluation schemes for book recommendation tasks and the existing classic recommendation models, this paper discusses the advantages and disadvantages of the BookGPT in book recomme
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#22810;&#35821;&#35328;Conformer&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#23618;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#20165;&#28608;&#27963;&#23376;&#38598;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;WRE&#24615;&#33021;&#25552;&#21319;&#32780;&#19982;&#36866;&#37197;&#22120;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#35821;&#35328;&#20449;&#24687;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#35821;&#35328;&#27973;&#34701;&#21512;&#36824;&#23454;&#29616;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.15663</link><description>&lt;p&gt;
&#27969;&#24335;&#22810;&#35821;&#35328;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#19987;&#23478;&#28151;&#21512;Conformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixture-of-Expert Conformer for Streaming Multilingual ASR. (arXiv:2305.15663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27969;&#24335;&#22810;&#35821;&#35328;Conformer&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;&#19987;&#23478;&#23618;&#65292;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#20165;&#28608;&#27963;&#23376;&#38598;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#19982;&#22522;&#20934;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#26174;&#33879;&#30340;WRE&#24615;&#33021;&#25552;&#21319;&#32780;&#19982;&#36866;&#37197;&#22120;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31867;&#20284;&#30340;&#24615;&#33021;&#65292;&#26080;&#38656;&#35821;&#35328;&#20449;&#24687;&#65292;&#21516;&#26102;&#21033;&#29992;&#22810;&#35821;&#35328;&#27973;&#34701;&#21512;&#36824;&#23454;&#29616;&#20102;&#36827;&#19968;&#27493;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#23481;&#37327;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#23545;&#20110;&#35774;&#22791;&#24212;&#29992;&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#23618;&#30340;&#27969;&#24335;&#30495;&#27491;&#22810;&#35821;&#35328;Conformer&#65292;&#35813;&#23618;&#33021;&#22815;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#20013;&#23398;&#20064;&#20165;&#28608;&#27963;&#23376;&#38598;&#21442;&#25968;&#12290;MoE&#23618;&#21253;&#25324;&#19968;&#20010;softmax&#38376;&#65292;&#35813;&#38376;&#22312;&#21069;&#39304;&#20256;&#25773;&#20013;&#36873;&#25321;&#22810;&#20010;&#19987;&#23478;&#20013;&#30340;&#26368;&#20339;&#20004;&#20010;&#12290;&#25152;&#25552;&#20986;&#30340;MoE&#23618;&#36890;&#36807;&#28608;&#27963;&#22266;&#23450;&#25968;&#37327;&#30340;&#21442;&#25968;&#26469;&#25552;&#20379;&#39640;&#25928;&#30340;&#25512;&#29702;&#65292;&#38543;&#30528;&#19987;&#23478;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#28608;&#27963;&#30340;&#21442;&#25968;&#25968;&#37327;&#20063;&#20250;&#22686;&#21152;&#12290;&#25105;&#20204;&#22312;12&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#33719;&#24471;&#30456;&#23545;&#22522;&#32447;&#30340;&#24179;&#22343;11.9&#65285;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#25913;&#36827;&#12290;&#19982;&#20351;&#29992;&#22522;&#20934;&#20449;&#24687;&#30340;&#36866;&#37197;&#22120;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;MoE&#27169;&#22411;&#23454;&#29616;&#20102;&#31867;&#20284;&#30340;&#35782;&#21035;&#38169;&#35823;&#29575;&#65292;&#24182;&#19988;&#28608;&#27963;&#20102;&#30456;&#20284;&#25968;&#37327;&#30340;&#21442;&#25968;&#65292;&#20294;&#19981;&#38656;&#35201;&#20219;&#20309;&#35821;&#35328;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22810;&#35821;&#35328;&#27973;&#34701;&#21512;&#32422;3&#65285;&#30340;&#30456;&#23545;&#35782;&#21035;&#38169;&#35823;&#29575;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end models with large capacity have significantly improved multilingual automatic speech recognition, but their computation cost poses challenges for on-device applications. We propose a streaming truly multilingual Conformer incorporating mixture-of-expert (MoE) layers that learn to only activate a subset of parameters in training and inference. The MoE layer consists of a softmax gate which chooses the best two experts among many in forward propagation. The proposed MoE layer offers efficient inference by activating a fixed number of parameters as the number of experts increases. We evaluate the proposed model on a set of 12 languages, and achieve an average 11.9% relative improvement in WER over the baseline. Compared to an adapter model using ground truth information, our MoE model achieves similar WER and activates similar number of parameters but without any language information. We further show around 3% relative WER improvement by multilingual shallow fusion.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;</title><link>http://arxiv.org/abs/2305.15645</link><description>&lt;p&gt;
ConvGQR&#65306;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;&#29983;&#25104;&#24335;&#26597;&#35810;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
ConvGQR: Generative Query Reformulation for Conversational Search. (arXiv:2305.15645v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#20250;&#35805;&#25628;&#32034;&#30340;ConvGQR&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#21512;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26469;&#37325;&#26032;&#26500;&#36896;&#26597;&#35810;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20250;&#35805;&#25628;&#32034;&#20013;&#65292;&#29992;&#25143;&#24403;&#21069;&#25628;&#32034;&#24847;&#22270;&#20381;&#36182;&#20110;&#20808;&#21069;&#30340;&#23545;&#35805;&#21382;&#21490;&#12290;&#20174;&#25972;&#20010;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#30830;&#23450;&#19968;&#20010;&#33391;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#36991;&#20813;&#26597;&#35810;&#32534;&#30721;&#22120;&#30340;&#26114;&#36149;&#37325;&#26032;&#35757;&#32451;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#26041;&#27861;&#23581;&#35797;&#23398;&#20064;&#19968;&#20010;&#37325;&#20889;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#20223;&#25163;&#21160;&#26597;&#35810;&#37325;&#20889;&#26469;&#21435;&#38500;&#24403;&#21069;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#37325;&#20889;&#30340;&#26597;&#35810;&#24182;&#19981;&#24635;&#26159;&#26368;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#35757;&#32451;&#37325;&#20889;&#27169;&#22411;&#20250;&#38480;&#21046;&#27169;&#22411;&#20135;&#29983;&#33391;&#22909;&#25628;&#32034;&#26597;&#35810;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;ConvGQR&#65292;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#19968;&#20010;&#29992;&#20110;&#26597;&#35810;&#37325;&#20889;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#28508;&#22312;&#31572;&#26696;&#65292;&#20197;&#37325;&#26032;&#26500;&#36896;&#20250;&#35805;&#26597;&#35810;&#12290;&#36890;&#36807;&#32467;&#21512;&#20004;&#32773;&#65292;ConvGQR&#21487;&#20197;&#25552;&#20379;&#26356;&#22909;&#30340;&#25628;&#32034;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23558;&#26597;&#35810;&#37325;&#26500;&#19982;&#26816;&#32034;&#24615;&#33021;&#32852;&#31995;&#36215;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#36873;&#25321;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#27169;&#22411;&#65292;&#29992;&#20110;&#39564;&#35777;ConvGQR&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In conversational search, the user's real search intent for the current turn is dependent on the previous conversation history. It is challenging to determine a good search query from the whole conversation context. To avoid the expensive re-training of the query encoder, most existing methods try to learn a rewriting model to de-contextualize the current query by mimicking the manual query rewriting. However, manually rewritten queries are not always the best search queries. Training a rewriting model on them would limit the model's ability to produce good search queries. Another useful hint is the potential answer to the question. In this paper, we propose ConvGQR, a new framework to reformulate conversational queries based on generative pre-trained language models (PLMs), one for query rewriting and another for generating potential answers. By combining both, ConvGQR can produce better search queries. In addition, to relate query reformulation to retrieval performance, we propose a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35789;&#24418;&#21464;&#21270;&#20219;&#21153;&#23384;&#22312;&#30340;&#39640;&#24615;&#33021;&#21644;&#39640;&#21487;&#21464;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35780;&#20272;&#31574;&#30053;&#20197;&#25913;&#21892;&#32467;&#26524;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#35789;&#24418;&#21464;&#21270;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#20570;&#20986;&#20102;&#26032;&#30340;&#35266;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.15637</link><description>&lt;p&gt;
&#35789;&#24418;&#21464;&#21270;&#65306;&#19968;&#20010;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Morphological Inflection: A Reality Check. (arXiv:2305.15637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35789;&#24418;&#21464;&#21270;&#20219;&#21153;&#23384;&#22312;&#30340;&#39640;&#24615;&#33021;&#21644;&#39640;&#21487;&#21464;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#25552;&#20986;&#26032;&#30340;&#25968;&#25454;&#37319;&#26679;&#21644;&#35780;&#20272;&#31574;&#30053;&#20197;&#25913;&#21892;&#32467;&#26524;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#36825;&#20123;&#31574;&#30053;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#35789;&#24418;&#21464;&#21270;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#20570;&#20986;&#20102;&#26032;&#30340;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#24418;&#21464;&#21270;&#26159;&#19968;&#20010;&#20855;&#26377;&#23454;&#36341;&#21644;&#35748;&#30693;&#24212;&#29992;&#30340;&#20122;&#35789;&#27719;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#22810;&#24180;&#26469;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;&#25253;&#21578;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#35821;&#35328;&#20013;&#39640;&#20294;&#20063;&#39640;&#24230;&#21487;&#21464;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#24615;&#33021;&#21644;&#39640;&#21487;&#21464;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#20102;&#25968;&#25454;&#38598;&#21019;&#24314;&#21644;&#35780;&#20272;&#20013;&#30340;&#20960;&#20010;&#26041;&#38754;&#22312;&#31995;&#32479;&#19978;&#31995;&#32479;&#22320;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#25513;&#30422;&#20102;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#25913;&#21892;&#32467;&#26524;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#22909;&#22320;&#21453;&#26144;&#21487;&#33021;&#29992;&#20363;&#30340;&#26032;&#25968;&#25454;&#37319;&#26679;&#21644;&#35780;&#20272;&#31574;&#30053;&#12290;&#20351;&#29992;&#36825;&#20123;&#26032;&#31574;&#30053;&#65292;&#25105;&#20204;&#23601;&#24403;&#21069;&#21464;&#24418;&#31995;&#32479;&#30340;&#27867;&#21270;&#33021;&#21147;&#20570;&#20986;&#20102;&#26032;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Morphological inflection is a popular task in sub-word NLP with both practical and cognitive applications. For years now, state-of-the-art systems have reported high, but also highly variable, performance across data sets and languages. We investigate the causes of this high performance and high variability; we find several aspects of data set creation and evaluation which systematically inflate performance and obfuscate differences between languages. To improve generalizability and reliability of results, we propose new data sampling and evaluation strategies that better reflect likely use-cases. Using these new strategies, we make new observations on the generalization abilities of current inflection systems.
&lt;/p&gt;</description></item><item><title>TAGREAL&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26597;&#35810;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#25903;&#25345;&#20449;&#24687;&#20197;&#20174;PLM&#20013;&#25506;&#27979;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#31361;&#20986;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15597</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#22686;&#24378;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models. (arXiv:2305.15597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15597
&lt;/p&gt;
&lt;p&gt;
TAGREAL&#26159;&#19968;&#31181;&#21487;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#26597;&#35810;&#25552;&#31034;&#20449;&#24687;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#25903;&#25345;&#20449;&#24687;&#20197;&#20174;PLM&#20013;&#25506;&#27979;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#65292;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#20855;&#26377;&#31361;&#20986;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#26159;&#20174;&#24050;&#30693;&#20107;&#23454;&#20013;&#25552;&#21462;&#26032;&#30340;&#21457;&#29616;&#12290;&#29616;&#26377;&#30340;&#22686;&#24378;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#20107;&#23454;&#19977;&#20803;&#32452;&#20197;&#25193;&#22823;&#22270;&#25512;&#29702;&#31354;&#38388;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#25552;&#31034;&#20449;&#24687;&#20197;&#20174;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#30693;&#35782;&#65292;&#36825;&#20123;&#26041;&#27861;&#24615;&#33021;&#26377;&#38480;&#65292;&#38656;&#35201;&#19987;&#23478;&#26114;&#36149;&#30340;&#24037;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TAGREAL&#65292;&#23427;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26597;&#35810;&#25552;&#31034;&#20449;&#24687;&#65292;&#24182;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#26816;&#32034;&#25903;&#25345;&#20449;&#24687;&#20197;&#20174;PLM&#20013;&#25506;&#27979;&#30693;&#35782;&#20197;&#23436;&#25104;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TAGREAL&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21363;&#20351;&#26159;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;TAGREAL&#30340;&#24615;&#33021;&#20173;&#28982;&#38750;&#24120;&#31361;&#20986;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#22522;&#20110;&#23884;&#20837;&#12289;&#22522;&#20110;&#22270;&#21644;&#22522;&#20110;PLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TAGREAL achieves state-of-the-art performance on two benchmark datasets. We find that TAGREAL has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#36719;&#25552;&#31034;&#21644;&#36890;&#36807;&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#36827;&#34892;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#25552;&#31034;&#25968;&#25454;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15594</link><description>&lt;p&gt;
&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#65306;&#29992;&#24046;&#20998;&#38544;&#31169;&#20419;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models. (arXiv:2305.15594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;&#36719;&#25552;&#31034;&#21644;&#36890;&#36807;&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#36827;&#34892;&#30340;&#31163;&#25955;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#25552;&#31034;&#25968;&#25454;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290; &#28982;&#32780;&#65292;&#25552;&#31034;&#20013;&#21253;&#21547;&#30340;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#24341;&#36215;&#20102;&#38544;&#31169;&#38382;&#39064;&#12290;&#25991;&#31456;&#39318;&#20808;&#35777;&#26126;&#20102;&#36825;&#20123;&#38382;&#39064;&#26159;&#21512;&#29702;&#30340;&#65306;&#25105;&#20204;&#23545;&#29992;&#20110;&#25552;&#31034;LLMs&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#31169;&#26377;&#30340;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#31169;&#26377;&#30340;&#36719;&#25552;&#31034;&#21487;&#20197;&#36890;&#36807;&#19979;&#28216;&#25968;&#25454;&#30340;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#12290;&#32780;&#31163;&#25955;&#25552;&#31034;&#21017;&#38656;&#35201;&#29992;&#22810;&#20010;LLMs&#36827;&#34892;&#22024;&#26434;&#30340;&#34920;&#20915;&#65292;&#21363;&#38543;&#26426;&#40550;&#40521;&#32676;&#20307;&#65292;&#26469;&#23558;&#20854;&#30693;&#35782;&#20256;&#36882;&#21040;&#19968;&#20010;&#20844;&#20849;&#25552;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are excellent in-context learners. However, the sensitivity of data contained in prompts raises privacy concerns. Our work first shows that these concerns are valid: we instantiate a simple but highly effective membership inference attack against the data used to prompt LLMs. To address this vulnerability, one could forego prompting and resort to fine-tuning LLMs with known algorithms for private gradient descent. However, this comes at the expense of the practicality and efficiency offered by prompting. Therefore, we propose to privately learn to prompt. We first show that soft prompts can be obtained privately through gradient descent on downstream data. However, this is not the case for discrete prompts. Thus, we orchestrate a noisy vote among an ensemble of LLMs presented with different prompts, i.e., a flock of stochastic parrots. The vote privately transfers the flock's knowledge into a single public prompt. We show that LLMs prompted with our private
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20154;&#20204;&#23545;&#25239;&#24615;&#25991;&#26412;&#26679;&#26412;&#30340;&#21487;&#24863;&#30693;&#24615;&#65292;&#24471;&#20986;&#29616;&#26377;&#25991;&#26412;&#25915;&#20987;&#22312;&#20154;&#31867;&#21442;&#19982;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25552;&#20379;&#20102;&#26356;&#20026;&#29616;&#23454;&#30340;&#23545;NLP&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.15587</link><description>&lt;p&gt;
&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#23545;&#25239;&#25991;&#26412;&#65311;&#23545;&#22522;&#20110;&#35789;&#35821;&#23545;&#25239;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#33258;&#28982;&#24615;&#36827;&#34892;&#29616;&#23454;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks. (arXiv:2305.15587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;&#20154;&#20204;&#23545;&#25239;&#24615;&#25991;&#26412;&#26679;&#26412;&#30340;&#21487;&#24863;&#30693;&#24615;&#65292;&#24471;&#20986;&#29616;&#26377;&#25991;&#26412;&#25915;&#20987;&#22312;&#20154;&#31867;&#21442;&#19982;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#25552;&#20379;&#20102;&#26356;&#20026;&#29616;&#23454;&#30340;&#23545;NLP&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#8212;&#8212;&#24694;&#24847;&#31639;&#27861;&#20250;&#24494;&#23567;&#22320;&#20462;&#25913;&#36755;&#20837;&#25991;&#26412;&#65292;&#23548;&#33268;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#30340;&#35780;&#20272;&#24573;&#30053;&#20102;&#19981;&#21487;&#23519;&#35273;&#24615;&#36136;&#25110;&#32773;&#22312;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#24847;&#21619;&#30528;&#23545;&#25239;&#25200;&#21160;&#19981;&#20250;&#36890;&#36807;&#20219;&#20309;&#20154;&#31867;&#36136;&#37327;&#27979;&#35797;&#65292;&#20063;&#19981;&#20250;&#23545;&#36890;&#36807;&#20154;&#24037;&#26816;&#26597;&#30340;NLP&#31995;&#32479;&#26500;&#25104;&#30495;&#27491;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#32469;&#36807;&#36825;&#20010;&#38480;&#21046;&#24182;&#23454;&#29616;NLP&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#36866;&#24403;&#35780;&#20272;&#65288;&#20197;&#21450;&#21518;&#26469;&#30340;&#25913;&#36827;&#65289;&#65292;&#25105;&#20204;&#23545;378&#21517;&#20154;&#31867;&#21442;&#19982;&#32773;&#23601;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25915;&#20987;&#26041;&#27861;&#29983;&#20135;&#30340;&#25991;&#26412;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#24863;&#30693;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#25915;&#20987;&#22312;&#20154;&#31867;&#21442;&#19982;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#36825;&#19982;&#20808;&#21069;&#35268;&#27169;&#36739;&#23567;&#30340;&#20154;&#31867;&#30740;&#31350;&#30456;&#30683;&#30462;&#65292;&#21518;&#32773;&#25253;&#36947;&#20102;&#25915;&#20987;&#25104;&#21151;&#30340;&#36807;&#20110;&#20048;&#35266;&#32467;&#35770;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#24076;&#26395;&#20026;&#24403;&#21069;&#23545;NLP&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#20570;&#20986;&#31215;&#26497;&#36129;&#29486;&#65292;&#25552;&#20379;&#23545;&#20854;&#28508;&#22312;&#24433;&#21709;&#30340;&#26356;&#29616;&#23454;&#30340;&#30475;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks -- malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to positi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#36755;&#20837;&#22810;&#26679;&#24615;&#23545;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#39118;&#26684;&#20998;&#24067;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#22810;&#31181;&#39118;&#26684;&#25511;&#21046;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15582</link><description>&lt;p&gt;
&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#23545;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#30340;&#24179;&#34913;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Balancing Effect of Training Dataset Distribution of Multiple Styles for Multi-Style Text Transfer. (arXiv:2305.15582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#36755;&#20837;&#22810;&#26679;&#24615;&#23545;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#24179;&#34913;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#39118;&#26684;&#20998;&#24067;&#65292;&#21487;&#20197;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#22810;&#31181;&#39118;&#26684;&#25511;&#21046;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#20013;&#19968;&#20010;&#28608;&#21160;&#20154;&#24515;&#30340;&#20219;&#21153;&#65292;&#20294;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25104;&#23545;&#25968;&#25454;&#12290;&#23545;&#20110;&#22810;&#23646;&#24615;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#65292;&#35757;&#32451;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#36275;&#22815;&#25903;&#25345;&#25152;&#26377;&#32771;&#34385;&#39118;&#26684;&#23646;&#24615;&#32452;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#22686;&#21152;&#20102;&#35757;&#32451;&#27169;&#22411;&#30340;&#38590;&#24230;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#36755;&#20837;&#22810;&#26679;&#24615;&#23545;&#22810;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#29983;&#25104;&#25991;&#26412;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#21551;&#21457;&#24335;&#26041;&#27861;&#35843;&#25972;&#35757;&#32451;&#26679;&#26412;&#20013;&#30340;&#39118;&#26684;&#20998;&#24067;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#20266;&#24179;&#34892;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36793;&#38469;&#21644;&#32852;&#21512;&#20998;&#24067;&#24179;&#34913;&#25105;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#25105;&#20204;&#30340;&#39118;&#26684;&#36716;&#25442;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#27604;&#19981;&#24179;&#34913;&#25110;&#20542;&#26012;&#30340;&#25968;&#25454;&#38598;&#20135;&#29983;&#26356;&#26377;&#25928;&#30340;&#22810;&#31181;&#39118;&#26684;&#25511;&#21046;&#25928;&#26524;&#12290;&#36890;&#36807;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#22810;&#31181;&#39118;&#26684;&#20998;&#24067;&#23545;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text style transfer is an exciting task within the field of natural language generation that is often plagued by the need for high-quality paired datasets. Furthermore, training a model for multi-attribute text style transfer requires datasets with sufficient support across all combinations of the considered stylistic attributes, adding to the challenges of training a style transfer model. This paper explores the impact of training data input diversity on the quality of the generated text from the multi-style transfer model. We construct a pseudo-parallel dataset by devising heuristics to adjust the style distribution in the training samples. We balance our training dataset using marginal and joint distributions to train our style transfer models. We observe that a balanced dataset produces more effective control effects over multiple styles than an imbalanced or skewed one. Through quantitative analysis, we explore the impact of multiple style distributions in training data on style-t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15542</link><description>&lt;p&gt;
&#32858;&#28966;&#26159;&#36801;&#31227;&#23398;&#20064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
Refocusing Is Key to Transfer Learning. (arXiv:2305.15542v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; TOAST &#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;&#36873;&#25321;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20803;&#32032;&#24182;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#28041;&#21450;&#23558;&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24403;&#21069;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#24120;&#24120;&#26080;&#27861;&#32858;&#28966;&#20110;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#31639;&#27861;-Top-Down Attention Steering&#65288;TOAST&#65289;&#65292;&#23427;&#20445;&#25345;&#39044;&#20808;&#35757;&#32451;&#30340;&#39592;&#24178;&#32467;&#26500;&#19981;&#21464;&#65292;&#21516;&#26102;&#36873;&#25321;&#36755;&#20986;&#20013;&#19982;&#20219;&#21153;&#26377;&#20851;&#30340;&#20803;&#32032;&#65292;&#24182;&#23558;&#23427;&#20204;&#21453;&#39304;&#22238;&#27169;&#22411;&#65292;&#20197;&#24341;&#23548;&#20854;&#27880;&#24847;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#20165;&#36890;&#36807;&#37325;&#26032;&#32858;&#28966;&#27880;&#24847;&#21147;&#65292;TOAST&#22312;&#35768;&#22810;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20855;&#26377;&#23567;&#37096;&#20998;&#21487;&#35843;&#21442;&#25968;&#12290;&#19982;&#23436;&#20840;&#24494;&#35843;&#12289;LoRA&#21644;&#25552;&#31034;&#24494;&#35843;&#30456;&#27604;&#65292;TOAST&#22312;&#19968;&#31995;&#21015;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#65288;&#20363;&#22914;&#65292;&#22312; FGVC &#19978;&#20174; 81.1% &#25552;&#39640;&#21040; 86.2%&#65289;&#26174;&#30528;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;TOAST&#22312;&#25351;&#20196;&#36319;&#38543;&#26041;&#38754;&#20063;&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#30340; Alpaca &#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning involves adapting a pre-trained model to novel downstream tasks. However, we observe that current transfer learning methods often fail to focus on task-relevant features. In this work, we emphasize the importance of refocusing the attention in transfer learning. We introduce Top-Down Attention Steering (TOAST), a novel transfer learning algorithm that keeps the pre-trained backbone frozen, while selecting the task-relevant elements in the output and feeding them back to the model to steer its attention to the task-specific features. By refocusing the attention only, TOAST achieves state-of-the-art results on a number of transfer learning benchmarks, while having a small portion of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt tuning, TOAST substantially improves performance across a range of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% on FGVC). TOAST also outperforms the fully fine-tuned Alpaca model on instruction-following
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25903;&#25345;&#27861;&#24459;&#20174;&#19994;&#32773;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#22788;&#29702;&#21644;&#25552;&#21462;&#27861;&#24459;&#26696;&#20214;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#20449;&#24687;&#65292;&#24182;&#22312;&#21152;&#25343;&#22823;&#30340;&#38590;&#27665;&#27861;&#24459;&#26696;&#20363;&#30740;&#31350;&#20013;&#25193;&#23637;&#29616;&#26377;&#27169;&#22411;&#65292;&#25552;&#21462;19&#20010;&#26377;&#29992;&#31867;&#21035;&#30340;&#26465;&#27454;&#12290;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27861;&#24459;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.15533</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#38590;&#27665;&#26696;&#20363;&#20998;&#26512;&#65306;&#25903;&#25345;&#27861;&#24459;&#20174;&#19994;&#32773;&#30340;NLP&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
Automated Refugee Case Analysis: An NLP Pipeline for Supporting Legal Practitioners. (arXiv:2305.15533v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25903;&#25345;&#27861;&#24459;&#20174;&#19994;&#32773;&#30340;&#33258;&#21160;&#21270;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#22788;&#29702;&#21644;&#25552;&#21462;&#27861;&#24459;&#26696;&#20214;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#20449;&#24687;&#65292;&#24182;&#22312;&#21152;&#25343;&#22823;&#30340;&#38590;&#27665;&#27861;&#24459;&#26696;&#20363;&#30740;&#31350;&#20013;&#25193;&#23637;&#29616;&#26377;&#27169;&#22411;&#65292;&#25552;&#21462;19&#20010;&#26377;&#29992;&#31867;&#21035;&#30340;&#26465;&#27454;&#12290;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27861;&#24459;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#26816;&#32034;&#12289;&#22788;&#29702;&#21644;&#25552;&#21462;&#27861;&#24459;&#26696;&#20214;&#20013;&#30340;&#26377;&#38024;&#23545;&#24615;&#20449;&#24687;&#12290;&#36890;&#36807;&#22312;&#21152;&#25343;&#22823;&#30340;&#38590;&#27665;&#27861;&#24459;&#26696;&#20363;&#30740;&#31350;&#20013;&#35843;&#26597;&#19968;&#20010;&#23569;&#26377;&#30740;&#31350;&#30340;&#27861;&#24459;&#39046;&#22495;&#12290;&#25628;&#32034;&#36807;&#21435;&#31867;&#20284;&#26696;&#20363;&#30340;&#26696;&#20363;&#27861;&#26159;&#24459;&#24072;&#21644;&#27861;&#23448;&#30340;&#27861;&#24459;&#24037;&#20316;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#22522;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26631;&#31614;&#65288;&#22914;&#26085;&#26399;&#65289;&#22312;&#27861;&#24459;&#24037;&#20316;&#20013;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#20449;&#24687;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#25193;&#23637;&#29616;&#26377;&#27169;&#22411;&#65292;&#20174;&#38590;&#27665;&#26696;&#20214;&#20013;&#26816;&#32034;19&#20010;&#26377;&#29992;&#31867;&#21035;&#30340;&#26465;&#27454;&#12290;&#22312;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26696;&#20363;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;&#25105;&#20204;&#27979;&#35797;&#20102;&#21253;&#25324;&#20004;&#31181;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#20869;&#30340;&#19981;&#21516;&#26550;&#26500;&#65292;&#20351;&#29992;&#19978;&#19979;&#25991;&#21644;&#38750;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#27604;&#36739;&#20102;&#36890;&#29992;&#30446;&#30340;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27861;&#24459;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#65292;&#23613;&#31649;&#20182;&#20204;&#30340;&#35268;&#27169;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce an end-to-end pipeline for retrieving, processing, and extracting targeted information from legal cases. We investigate an under-studied legal domain with a case study on refugee law in Canada. Searching case law for past similar cases is a key part of legal work for both lawyers and judges, the potential end-users of our prototype. While traditional named-entity recognition labels such as dates provide meaningful information in legal work, we propose to extend existing models and retrieve a total of 19 useful categories of items from refugee cases. After creating a novel data set of cases, we perform information extraction based on state-of-the-art neural named-entity recognition (NER). We test different architectures including two transformer models, using contextual and non-contextual embeddings, and compare general purpose versus domain-specific pre-training. The results demonstrate that models pre-trained on legal data perform best despite their smaller
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#20581;&#24247;&#24212;&#29992;&#65292;&#21482;&#38656;&#23569;&#37327;&#35843;&#25972;&#20415;&#33021;&#25429;&#25417;&#20581;&#24247;&#39046;&#22495;&#30340;&#25968;&#23383;&#25968;&#25454;&#24182;&#22312;&#20020;&#24202;&#21644;&#20581;&#24247;&#29615;&#22659;&#19979;&#25512;&#29702;&#21450;&#21442;&#19982;&#21508;&#39033;&#20581;&#24247;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15525</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#20581;&#24247;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Few-Shot Health Learners. (arXiv:2305.15525v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#29992;&#20110;&#20581;&#24247;&#24212;&#29992;&#65292;&#21482;&#38656;&#23569;&#37327;&#35843;&#25972;&#20415;&#33021;&#25429;&#25417;&#20581;&#24247;&#39046;&#22495;&#30340;&#25968;&#23383;&#25968;&#25454;&#24182;&#22312;&#20020;&#24202;&#21644;&#20581;&#24247;&#29615;&#22659;&#19979;&#25512;&#29702;&#21450;&#21442;&#19982;&#21508;&#39033;&#20581;&#24247;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25429;&#25417;&#23454;&#29616;&#23454;&#38469;&#20219;&#21153;&#20013;&#26377;&#29992;&#30340;&#20016;&#23500;&#27010;&#24565;&#34920;&#31034;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20165;&#26377;&#35821;&#35328;&#30340;&#27169;&#22411;&#20855;&#26377;&#23616;&#38480;&#24615;&#12290;&#20581;&#24247;&#24212;&#29992;&#35201;&#27714;&#27169;&#22411;&#22312;&#25968;&#23383;&#25968;&#25454;(&#20363;&#22914;&#65292;&#20020;&#24202;&#39046;&#22495;&#20013;&#30340;&#29983;&#21629;&#20307;&#24449;&#12289;&#23454;&#39564;&#23460;&#20540;&#65307;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#30340;&#27493;&#25968;&#12289;&#36816;&#21160;)&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#32780;&#36825;&#20123;&#25968;&#23383;&#25968;&#25454;&#22312;&#29616;&#26377;&#35757;&#32451;&#35821;&#26009;&#20013;&#24456;&#38590;&#25110;&#19981;&#33021;&#29992;&#25991;&#26412;&#36731;&#26494;&#34920;&#36798;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#38656;&#36827;&#34892;&#23569;&#37327;&#35843;&#25972;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20415;&#33021;&#22815;&#23558;&#21508;&#31181;&#29983;&#29702;&#21644;&#34892;&#20026;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19982;&#22810;&#31181;&#20581;&#24247;&#20219;&#21153;&#32852;&#31995;&#36215;&#26469;&#65292;&#36866;&#29992;&#20110;&#20020;&#24202;&#21644;&#20581;&#24247;&#29615;&#22659;&#12290;&#20351;&#29992;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#21307;&#30103;&#20256;&#24863;&#22120;&#35760;&#24405;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#33021;&#21147;&#65292;&#24182;&#24212;&#29992;&#20110;&#24515;&#33039;&#20449;&#21495;&#20998;&#26512;&#12289;&#29289;&#29702;&#27963;&#21160;&#35782;&#21035;&#12289;&#20195;&#35874;&#35745;&#31639;(&#20363;&#22914;&#65292;&#29123;&#28903;&#30340;&#21345;&#36335;&#37324;)&#20197;&#21450;&#21387;&#21147;&#25253;&#21578;&#21644;&#24515;&#29702;&#20581;&#24247;&#31579;&#26597;&#30340;&#20272;&#35745;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can capture rich representations of concepts that are useful for real-world tasks. However, language alone is limited. While existing LLMs excel at text-based inferences, health applications require that models be grounded in numerical data (e.g., vital signs, laboratory values in clinical domains; steps, movement in the wellness domain) that is not easily or readily expressed as text in existing training corpus. We demonstrate that with only few-shot tuning, a large language model is capable of grounding various physiological and behavioral time-series data and making meaningful inferences on numerous health tasks for both clinical and wellness contexts. Using data from wearable and medical sensor recordings, we evaluate these capabilities on the tasks of cardiac signal analysis, physical activity recognition, metabolic calculation (e.g., calories burned), and estimation of stress reports and mental health screeners.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#25200;&#21160;&#35299;&#37322;&#20063;&#33021;&#22815;&#36798;&#21040;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.15520</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#33258;&#21160;&#25200;&#21160;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Automatically Perturbed Natural Language Explanations in Relation Extraction. (arXiv:2305.15520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#22312;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21457;&#29616;&#25200;&#21160;&#35299;&#37322;&#20063;&#33021;&#22815;&#36798;&#21040;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20026;&#27169;&#22411;&#25552;&#20379;&#37325;&#35201;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20854;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#36825;&#20123;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#12290;&#24341;&#20154;&#27880;&#30446;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24102;&#26377;&#38477;&#20302;&#24402;&#32435;&#20559;&#22909;&#30340;&#25200;&#21160;&#35299;&#37322;&#21487;&#20197;&#36798;&#21040;&#19982;&#21407;&#22987;&#35299;&#37322;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#29305;&#24449;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#65306;&#65288;1&#65289;&#35299;&#37322;&#30340;&#24433;&#21709;&#22240;&#35757;&#32451;&#39118;&#26684;&#21644;&#25968;&#25454;&#38598;&#32780;&#24322;&#65292;&#20197;&#21069;&#35748;&#20026;&#30340;&#25913;&#36827;&#20027;&#35201;&#22312;&#20923;&#32467;&#35821;&#35328;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#12290;&#65288;2&#65289;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#23558;&#35299;&#37322;&#30340;&#24433;&#21709;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23436;&#20840;&#25200;&#21160;&#35299;&#37322;&#65292;&#25928;&#26524;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#35748;&#20026;&#20027;&#35201;&#24433;&#21709;&#26159;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#21644;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research has demonstrated that natural language explanations provide valuable inductive biases that guide models, thereby improving the generalization ability and data efficiency. In this paper, we undertake a systematic examination of the effectiveness of these explanations. Remarkably, we find that corrupted explanations with diminished inductive biases can achieve competitive or superior performance compared to the original explanations. Our findings furnish novel insights into the characteristics of natural language explanations in the following ways: (1) the impact of explanations varies across different training styles and datasets, with previously believed improvements primarily observed in frozen language models. (2) While previous research has attributed the effect of explanations solely to their inductive biases, our study shows that the effect persists even when the explanations are completely corrupted. We propose that the main effect is due to the provision of add
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#26356;&#21152;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23558;&#25991;&#26412;&#24120;&#35782;&#25551;&#36848;&#38598;&#25104;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#24120;&#35782;&#25551;&#36848;&#30340;&#26679;&#26412;&#20998;&#25209;&#27425;&#36827;&#34892;&#32534;&#30721;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.15516</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#39640;&#25928;&#25991;&#26412;&#24120;&#35782;&#34701;&#21512;&#30340;&#20813;&#36153;&#21320;&#39184;
&lt;/p&gt;
&lt;p&gt;
Free Lunch for Efficient Textual Commonsense Integration in Language Models. (arXiv:2305.15516v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#19978;&#26356;&#21152;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#23558;&#25991;&#26412;&#24120;&#35782;&#25551;&#36848;&#38598;&#25104;&#21040;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#24120;&#35782;&#25551;&#36848;&#30340;&#26679;&#26412;&#20998;&#25209;&#27425;&#36827;&#34892;&#32534;&#30721;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#25991;&#26412;&#24120;&#35782;&#30693;&#35782;&#24211;&#30340;&#20986;&#29616;&#26088;&#22312;&#25552;&#20379;&#26356;&#21152;&#32454;&#33268;&#21644;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#23558;&#22806;&#37096;&#24120;&#35782;&#34701;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#26159;&#25512;&#36827;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#32534;&#30721;&#20256;&#32479;&#31526;&#21495;&#30693;&#35782;&#30456;&#27604;&#65292;&#23558;&#25991;&#26412;&#24120;&#35782;&#25551;&#36848;&#21512;&#24182;&#26159;&#35745;&#31639;&#19978;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#20854;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#20855;&#26377;&#30456;&#20284;&#24120;&#35782;&#25551;&#36848;&#30340;&#35757;&#32451;&#26679;&#26412;&#20998;&#25104;&#19968;&#20010;&#25209;&#27425;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26679;&#26412;&#20013;&#37325;&#22797;&#20351;&#29992;&#32534;&#30721;&#25551;&#36848;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#25209;&#27425;&#21010;&#20998;&#30340;&#19978;&#38480;&#21487;&#20197;&#32553;&#23567;&#21040;&#32463;&#20856;&#30340;&#22270;k&#20998;&#21106;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35889;&#32858;&#31867;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25209;&#27425;&#21010;&#20998;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the emergence of textual commonsense knowledge bases, aimed at providing more nuanced and context-rich knowledge. The integration of external commonsense into language models has been shown to be a key enabler in advancing the state-of-the-art for a wide range of NLP tasks. However, incorporating textual commonsense descriptions is computationally expensive, as compared to encoding conventional symbolic knowledge. In this paper, we propose a method to improve its efficiency without modifying the model. We group training samples with similar commonsense descriptions into a single batch, thus reusing the encoded description across multiple samples. One key observation is that the upper bound of batch partitioning can be reduced to the classic {\it graph k-cut problem}. Consequently, we propose a spectral clustering-based algorithm to solve this problem. Extensive experiments illustrate that the proposed batch partitioning approach effectively reduces the compu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;Python&#26631;&#35782;&#31526;&#20132;&#25442;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#24433;&#21709;&#19979;&#34920;&#29616;&#26356;&#20026;&#26174;&#33879;&#12290;&#36825;&#34920;&#26126;LLM&#32570;&#20047;&#28145;&#21051;&#12289;&#25277;&#35937;&#30340;&#29702;&#35299;&#65292;&#26080;&#27861;&#32988;&#20219;&#19982;&#35757;&#32451;&#20559;&#24046;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.15507</link><description>&lt;p&gt;
&#36234;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#36234;&#38590;&#20197;&#25417;&#25720;&#65306;Python&#20013;&#30340;&#26631;&#35782;&#31526;&#20132;&#25442;&#19981;&#34987;&#35821;&#35328;&#27169;&#22411;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python. (arXiv:2305.15507v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15507
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;Python&#26631;&#35782;&#31526;&#20132;&#25442;&#30340;&#35782;&#21035;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#22312;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#24433;&#21709;&#19979;&#34920;&#29616;&#26356;&#20026;&#26174;&#33879;&#12290;&#36825;&#34920;&#26126;LLM&#32570;&#20047;&#28145;&#21051;&#12289;&#25277;&#35937;&#30340;&#29702;&#35299;&#65292;&#26080;&#27861;&#32988;&#20219;&#19982;&#35757;&#32451;&#20559;&#24046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#36825;&#20123;&#27169;&#22411;&#23545;&#32534;&#31243;&#30340;&#29702;&#35299;&#31243;&#24230;&#22914;&#20309;&#12290;&#20256;&#32479;&#30340;&#32534;&#31243;&#35821;&#35328;&#20855;&#26377;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#65292;&#20154;&#31867;&#31243;&#24207;&#21592;&#33021;&#30452;&#35266;&#22320;&#29702;&#35299;&#21644;&#21033;&#29992;&#36825;&#20123;&#24615;&#36136;&#65292;&#22914;&#26631;&#35782;&#31526;&#37325;&#21629;&#21517;&#65288;&#36817;&#20284;&#65289;&#19981;&#21464;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLM&#22312;&#40664;&#35748;&#20989;&#25968;&#21517;&#31216;&#20132;&#25442;&#26102;&#19981;&#20165;&#26080;&#27861;&#27491;&#30830;&#29983;&#25104;Python&#20195;&#30721;&#65292;&#26377;&#20123;&#27169;&#22411;&#22312;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#26102;&#29978;&#33267;&#21464;&#24471;&#26356;&#21152;&#33258;&#20449;&#22320;&#36827;&#34892;&#38169;&#35823;&#39044;&#27979;&#65292;&#36825;&#26159;&#26368;&#36817;&#21457;&#29616;&#30340;&#36870;&#27604;&#20363;&#32553;&#25918;&#29616;&#35937;&#30340;&#23454;&#20363;&#65292;&#19982;&#36890;&#24120;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#22823;&#23567;&#22686;&#21152;&#20250;&#25552;&#39640;&#39044;&#27979;&#36136;&#37327;&#30340;&#36235;&#21183;&#30456;&#21453;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;LLM&#30340;&#20856;&#22411;&#24773;&#20917;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#32570;&#20047;&#19968;&#20010;&#28145;&#21051;&#30340;&#12289;&#25277;&#35937;&#30340;&#29702;&#35299;&#25454;&#20197;&#25805;&#32437;&#20869;&#23481;&#65292;&#20351;&#23427;&#20204;&#26080;&#27861;&#32988;&#20219;&#19982;&#35757;&#32451;&#20559;&#24046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#26174;&#24335;&#32852;&#21512;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#25214;&#21040;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25509;&#36817;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26465;&#20214;&#21487;&#20197;&#36229;&#36807;&#21407;&#22987;&#30340;MLM&#12290;</title><link>http://arxiv.org/abs/2305.15501</link><description>&lt;p&gt;
&#20174;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deriving Language Models from Masked Language Models. (arXiv:2305.15501v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20013;&#25512;&#23548;&#26174;&#24335;&#32852;&#21512;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#25214;&#21040;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#25509;&#36817;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#26465;&#20214;&#21487;&#20197;&#36229;&#36807;&#21407;&#22987;&#30340;MLM&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#27809;&#26377;&#26126;&#30830;&#23450;&#20041;&#35821;&#35328;&#30340;&#20998;&#24067;&#65292;&#21363;&#23427;&#20204;&#26412;&#36523;&#24182;&#19981;&#26159;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#22312;&#29983;&#25104;&#21644;&#35780;&#20998;&#30340;&#30446;&#30340;&#19978;&#23558;&#23427;&#20204;&#38544;&#21547;&#22320;&#35270;&#20026;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;MLM&#20013;&#23548;&#20986;&#26174;&#24335;&#32852;&#21512;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#20851;&#27880;&#20004;&#20010;&#26631;&#35760;&#30340;&#20998;&#24067;&#65292;&#36825;&#26679;&#21487;&#20197;&#35745;&#31639;&#31934;&#30830;&#30340;&#20998;&#24067;&#29305;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#31181;&#22522;&#20110;&#35782;&#21035;&#26465;&#20214;&#25509;&#36817;&#20110;MLM&#30340;&#26465;&#20214;&#30340;&#32852;&#32467;&#30340;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#29616;&#65292;&#36825;&#20010;&#25512;&#23548;&#27169;&#22411;&#30340;&#26465;&#20214;&#29978;&#33267;&#26377;&#26102;&#21487;&#20197;&#36229;&#36807;&#21407;&#22987;MLM&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked language models (MLM) do not explicitly define a distribution over language, i.e., they are not language models per se. However, recent work has implicitly treated them as such for the purposes of generation and scoring. This paper studies methods for deriving explicit joint distributions from MLMs, focusing on distributions over two tokens, which makes it possible to calculate exact distributional properties. We find that an approach based on identifying joints whose conditionals are closest to those of the MLM works well and outperforms existing Markov random field-based approaches. We further find that this derived model's conditionals can even occasionally outperform the original MLM's conditionals.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#20852;&#36259;&#26053;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15498</link><description>&lt;p&gt;
&#29992;&#25143;&#20852;&#36259;&#26053;&#31243;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for User Interest Journeys. (arXiv:2305.15498v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15498
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23450;&#20041;&#20852;&#36259;&#26053;&#31243;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26088;&#22312;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#20196;&#20154;&#30633;&#30446;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#29992;&#25143;&#21644;&#25913;&#21892;&#20010;&#24615;&#21270;&#25512;&#33616;&#24179;&#21488;&#20307;&#39564;&#26041;&#38754;&#30340;&#28508;&#21147;&#36824;&#36828;&#26410;&#34987;&#21457;&#25381;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#23545;&#29992;&#25143;&#20852;&#36259;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23450;&#20041;&#20102;&#20852;&#36259;&#26053;&#31243;&#20316;&#20026;&#29992;&#25143;&#22522;&#20110;&#20182;&#20204;&#30340;&#27963;&#21160;&#32780;&#36941;&#21382;&#36807;&#30340;&#20852;&#36259;&#29366;&#24577;&#24207;&#21015;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#29992;&#25143;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25512;&#33616;&#30340;&#36136;&#37327;&#65292;&#24182;&#19988;&#29983;&#25104;&#30340;&#20852;&#36259;&#26053;&#31243;&#20026;&#25512;&#33616;&#36807;&#31243;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#26032;&#39062;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation. Their potential for deeper user understanding and improved personalized user experience on recommendation platforms is, however, largely untapped. This paper aims to address this gap. Recommender systems today capture users' interests through encoding their historical activities on the platforms. The generated user representations are hard to examine or interpret. On the other hand, if we were to ask people about interests they pursue in their life, they might talk about their hobbies, like I just started learning the ukulele, or their relaxation routines, e.g., I like to watch Saturday Night Live, or I want to plant a vertical garden. We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would.  We define interest journe
&lt;/p&gt;</description></item><item><title>PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15444</link><description>&lt;p&gt;
PromptNER: &#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15444
&lt;/p&gt;
&lt;p&gt;
PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29616;&#22312;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29616;&#25104;&#26041;&#27861;&#65292;&#20026;&#21508;&#31181;&#32463;&#20856;&#30340;NLP&#38382;&#39064;&#25552;&#20379;&#20102;&#23569;&#37327;&#26679;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#31471;&#21040;&#31471;&#32467;&#26500;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#22312;&#26631;&#20934;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptNER&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#21644;&#36328;&#39046;&#22495;NER&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#20219;&#20309;&#26032;&#30340;NER&#20219;&#21153;&#65292;PromptNER&#38656;&#35201;&#25552;&#20379;&#19968;&#32452;&#23454;&#20307;&#23450;&#20041;&#65292;&#38500;&#22522;&#26412;&#30340;&#23569;&#26679;&#26412;&#26679;&#20363;&#20197;&#22806;&#12290;&#32473;&#23450;&#36755;&#20837;&#21477;&#23376;&#65292;PromptNER&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#35777;&#26126;&#23427;&#20204;&#19982;&#25552;&#20379;&#30340;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;&#20860;&#23481;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PromptNER&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WikiAnn&#25968;&#25454;&#38598;&#19978;&#20026;&#36328;&#39046;&#22495;NER&#35774;&#23450;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;</title><link>http://arxiv.org/abs/2305.15425</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Language Model Tokenizers Introduce Unfairness Between Languages. (arXiv:2305.15425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15425
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#22120;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#24341;&#20837;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#22240;&#20026;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#33021;&#20250;&#23548;&#33268;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#22810;&#35821;&#35328;&#24615;&#33021;&#65292;&#21363;&#20351;&#27809;&#26377;&#26126;&#30830;&#20026;&#27492;&#36827;&#34892;&#36807;&#35757;&#32451;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#36755;&#20986;&#36136;&#37327;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20998;&#35789;&#38454;&#27573;&#20986;&#29616;&#20102;&#19981;&#21516;&#35821;&#35328;&#30340;&#22788;&#29702;&#24046;&#24322;&#65292;&#29978;&#33267;&#22312;&#27169;&#22411;&#34987;&#35843;&#29992;&#20043;&#21069;&#23601;&#24050;&#32463;&#20986;&#29616;&#20102;&#12290;&#21516;&#19968;&#27573;&#25991;&#26412;&#32763;&#35793;&#25104;&#19981;&#21516;&#30340;&#35821;&#35328;&#21487;&#20197;&#26377;&#26497;&#22823;&#30340;&#20998;&#35789;&#38271;&#24230;&#24046;&#24322;&#65292;&#26377;&#20123;&#24773;&#20917;&#19979;&#24046;&#24322;&#21487;&#39640;&#36798;15&#20493;&#12290;&#36825;&#20123;&#24046;&#24322;&#22312;&#25105;&#20204;&#35780;&#20272;&#30340;17&#31181;&#20998;&#35789;&#22120;&#20013;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#20351;&#23427;&#20204;&#26159;&#26377;&#24847;&#20026;&#22810;&#35821;&#35328;&#25903;&#25345;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#26576;&#20123;&#35821;&#35328;&#23545;&#30340;&#23383;&#31526;&#32423;&#21644;&#23383;&#33410;&#32423;&#27169;&#22411;&#20063;&#26174;&#31034;&#20986;4&#20493;&#20197;&#19978;&#30340;&#32534;&#30721;&#38271;&#24230;&#24046;&#24322;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20123;&#35821;&#35328;&#31038;&#21306;&#22312;&#33719;&#21462;&#21830;&#19994;&#35821;&#35328;&#26381;&#21153;&#30340;&#25104;&#26412;&#12289;&#22788;&#29702;&#26102;&#38388;&#21644;&#24310;&#36831;&#20197;&#21450;&#25552;&#20379;&#32473;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#23481;&#37327;&#26041;&#38754;&#23384;&#22312;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have shown impressive multilingual performance, even when not explicitly trained for it. Despite this, concerns have been raised about the quality of their outputs across different languages. In this paper, we show how disparity in the treatment of different languages arises at the tokenization stage, well before a model is even invoked. The same text translated into different languages can have drastically different tokenization lengths, with differences up to 15 times in some cases. These disparities persist across the 17 tokenizers we evaluate, even if they are intentionally trained for multilingual support. Character-level and byte-level models also exhibit over 4 times the difference in the encoding length for some language pairs. This induces unfair treatment for some language communities in regard to the cost of accessing commercial language services, the processing time and latency, as well as the amount of content that can be provided as context to the m
&lt;/p&gt;</description></item><item><title>RefGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#24182;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.14994</link><description>&lt;p&gt;
RefGPT: GPT&#27169;&#22411;&#20013;&#22522;&#20110;&#21442;&#32771;&#30340;&#30495;&#23454;&#19988;&#21487;&#23398;&#20064;&#21270;&#30340;&#23545;&#35805;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
RefGPT: Reference -&gt; Truthful &amp; Customized Dialogues Generation by GPTs and for GPTs. (arXiv:2305.14994v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14994
&lt;/p&gt;
&lt;p&gt;
RefGPT&#26159;&#19968;&#31181;&#22522;&#20110;&#21442;&#32771;&#30340;&#23545;&#35805;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#24182;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#31561;&#36890;&#29992;&#30340;&#32842;&#22825;&#27169;&#22411;&#24050;&#32463;&#36890;&#36807;&#20351;&#29992;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#20154;&#31867;&#32534;&#20889;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#23588;&#20854;&#26159;&#22810;&#36718;&#23545;&#35805;&#65292;&#23545;&#22823;&#22810;&#25968;&#20154;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#20197;&#24448;&#30340;&#30740;&#31350;&#24050;&#32463;&#20351;&#29992;&#20102;&#24378;&#22823;&#30340;LLMs&#26469;&#33258;&#21160;&#29983;&#25104;&#23545;&#35805;&#65292;&#20294;&#30001;&#20110;LLMs&#23384;&#22312;&#24187;&#35273;&#65292;&#36825;&#20123;&#23545;&#35805;&#37117;&#26080;&#27861;&#23436;&#20840;&#30495;&#23454;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RefGPT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#37327;&#30495;&#23454;&#19988;&#23450;&#21046;&#21270;&#30340;&#23545;&#35805;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#27169;&#22411;&#24187;&#35273;&#36896;&#25104;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;RefGPT&#36890;&#36807;&#38480;&#21046;LLMs&#20351;&#29992;&#32473;&#23450;&#21442;&#32771;&#32780;&#19981;&#26159;&#22238;&#24518;&#33258;&#24049;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#23545;&#35805;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#27169;&#22411;&#24187;&#35273;&#12290;&#27492;&#22806;&#65292;RefGPT&#23545;&#27599;&#20010;&#35805;&#35821;&#37117;&#28155;&#21152;&#20102;&#35814;&#32454;&#30340;&#25511;&#21046;&#65292;&#20351;&#20854;&#20855;&#26377;&#39640;&#24230;&#23450;&#21046;&#21270;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20197;&#24448;&#30740;&#31350;&#25152;&#24573;&#30053;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
General chat models, like ChatGPT, have attained impressive capability to resolve a wide range of NLP tasks by tuning Large Language Models (LLMs) with high-quality instruction data. However, collecting human-written high-quality data, especially multi-turn dialogues, is expensive and unattainable for most people. Though previous studies have used powerful LLMs to generate the dialogues automatically, but they all suffer from generating untruthful dialogues because of the LLMs hallucination. Therefore, we propose a method called RefGPT to generate enormous truthful and customized dialogues without worrying about factual errors caused by the model hallucination. RefGPT solves the model hallucination in dialogue generation by restricting the LLMs to leverage the given reference instead of reciting their own knowledge to generate dialogues. Additionally, RefGPT adds detailed controls on every utterances to enable highly customization capability, which previous studies have ignored. On the
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#35299;&#37322;&#20026;&#22522;&#20110;&#25928;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26174;&#31034;&#30340;&#20559;&#22909;&#30340;&#24207;&#25968;&#25928;&#29992;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;SGD&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#23398;&#20064;&#21160;&#24577;&#35270;&#20026;&#23558;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21040;&#26368;&#20248;&#25928;&#29992;&#20989;&#25968;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2305.14859</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;-&#27010;&#29575;&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
Utility-Probability Duality of Neural Networks. (arXiv:2305.14859v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14859
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#35299;&#37322;&#20026;&#22522;&#20110;&#25928;&#29992;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23558;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26174;&#31034;&#30340;&#20559;&#22909;&#30340;&#24207;&#25968;&#25928;&#29992;&#20989;&#25968;&#65292;&#21487;&#20197;&#23558;SGD&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#23398;&#20064;&#21160;&#24577;&#35270;&#20026;&#23558;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21040;&#26368;&#20248;&#25928;&#29992;&#20989;&#25968;&#30340;&#36845;&#20195;&#36807;&#31243;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#25311;&#21512;&#25152;&#38656;&#36755;&#20986;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#22312;&#35768;&#22810;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#35266;&#23519;&#21040;&#30340;&#24726;&#35770;&#29616;&#35937;&#35753;&#20154;&#20204;&#24576;&#30097;&#36825;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#35299;&#37322;&#26159;&#21542;&#33021;&#30495;&#27491;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#30340;&#32463;&#39564;&#25104;&#21151;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26631;&#20934;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#35299;&#37322;&#20026;&#22522;&#20110;&#25928;&#29992;&#30340;&#35299;&#37322;&#12290;&#22522;&#26412;&#24605;&#24819;&#26159;&#23558;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#19981;&#35299;&#37322;&#20026;&#27010;&#29575;&#27169;&#22411;&#65292;&#32780;&#35299;&#37322;&#20026;&#32534;&#30721;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26174;&#31034;&#30340;&#20559;&#22909;&#30340;&#24207;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#23545;&#24212;&#20110;&#19968;&#20010;&#25928;&#29992;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#25152;&#26377;&#20855;&#26377;softmax&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65288;MLE&#65289;&#30340;SGD&#23398;&#20064;&#21160;&#24577;&#21487;&#20197;&#34987;&#35270;&#20026;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#23558;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#21040;&#26368;&#20248;&#25928;&#29992;&#20989;&#25968;&#12290;&#36825;&#20010;&#26694;&#26550;&#19981;&#20165;&#25552;&#20379;&#20102;&#23545;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#30340;&#26032;&#35299;&#37322;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#19968;&#20010;&#35774;&#35745;&#26356;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is typically understood that the training of modern neural networks is a process of fitting the probability distribution of desired output. However, recent paradoxical observations in a number of language generation tasks let one wonder if this canonical probability-based explanation can really account for the empirical success of deep learning.  To resolve this issue, we propose an alternative utility-based explanation to the standard supervised learning procedure in deep learning. The basic idea is to interpret the learned neural network not as a probability model but as an ordinal utility function that encodes the preference revealed in training data. In this perspective, training of the neural network corresponds to a utility learning process. Specifically, we show that for all neural networks with softmax outputs, the SGD learning dynamic of maximum likelihood estimation (MLE) can be seen as an iteration process that optimizes the neural network toward an optimal utility functi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#29942;&#39048;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65288;DBF&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32454;&#31890;&#24230;&#22320;&#36807;&#28388;&#25481;&#20887;&#20313;&#21644;&#22122;&#22768;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14652</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#21435;&#22122;&#29942;&#39048;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Bottleneck with Mutual Information Maximization for Video Multimodal Fusion. (arXiv:2305.14652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#29942;&#39048;&#21644;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65288;DBF&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#32454;&#31890;&#24230;&#22320;&#36807;&#28388;&#25481;&#20887;&#20313;&#21644;&#22122;&#22768;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26174;&#33879;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#26088;&#22312;&#23558;&#35270;&#39057;&#20013;&#30340;&#22810;&#27169;&#24577;&#20449;&#21495;&#65288;&#22914;&#35270;&#35273;&#12289;&#38899;&#39057;&#21644;&#25991;&#26412;&#65289;&#25972;&#21512;&#65292;&#20197;&#20351;&#29992;&#22810;&#27169;&#24577;&#20869;&#23481;&#36827;&#34892;&#34917;&#20805;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#19982;&#20854;&#20182;&#22270;&#20687;-&#25991;&#26412;&#22810;&#27169;&#24577;&#20219;&#21153;&#19981;&#21516;&#65292;&#35270;&#39057;&#20855;&#26377;&#26356;&#38271;&#30340;&#22810;&#27169;&#24577;&#24207;&#21015;&#65292;&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#27169;&#24577;&#20013;&#23384;&#22312;&#26356;&#22810;&#30340;&#20887;&#20313;&#21644;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#39057;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#21435;&#22122;&#29942;&#39048;&#34701;&#21512;&#65288;DBF&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#19968;&#26041;&#38754;&#37319;&#29992;&#29942;&#39048;&#26426;&#21046;&#65292;&#20197;&#38480;&#21046;&#30340;&#24863;&#21463;&#37326;&#36807;&#28388;&#22122;&#22768;&#21644;&#20887;&#20313;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#27169;&#22359;&#26469;&#35843;&#33410;&#36807;&#28388;&#27169;&#22359;&#65292;&#20197;&#20445;&#30041;&#19981;&#21516;&#27169;&#24577;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;DBF&#27169;&#22411;&#22312;&#22810;&#35821;&#35328;&#35270;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video multimodal fusion aims to integrate multimodal signals in videos, such as visual, audio and text, to make a complementary prediction with multiple modalities contents. However, unlike other image-text multimodal tasks, video has longer multimodal sequences with more redundancy and noise in both visual and audio modalities. Prior denoising methods like forget gate are coarse in the granularity of noise filtering. They often suppress the redundant and noisy information at the risk of losing critical information. Therefore, we propose a denoising bottleneck fusion (DBF) model for fine-grained video multimodal fusion. On the one hand, we employ a bottleneck mechanism to filter out noise and redundancy with a restrained receptive field. On the other hand, we use a mutual information maximization module to regulate the filter-out module to preserve key information within different modalities. Our DBF model achieves significant improvement over current state-of-the-art baselines on mult
&lt;/p&gt;</description></item><item><title>CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14635</link><description>&lt;p&gt;
CMOT: &#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#36328;&#27169;&#24577;Mixup&#65292;&#29992;&#20110;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation. (arXiv:2305.14635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14635
&lt;/p&gt;
&lt;p&gt;
CMOT&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#27169;&#24577;&#35821;&#38899;&#32763;&#35793;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#24182;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#26159;&#23558;&#28304;&#35821;&#35328;&#20013;&#30340;&#35821;&#38899;&#20449;&#21495;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#25991;&#26412;&#30340;&#20219;&#21153;&#12290;&#20316;&#20026;&#19968;&#39033;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#31471;&#21040;&#31471;ST&#22312;&#26377;&#38480;&#25968;&#25454;&#19979;&#36827;&#34892;&#35757;&#32451;&#38750;&#24120;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#23581;&#35797;&#20174;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20294;&#20854;&#24615;&#33021;&#30001;&#20110;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Cross-modal Mixup via Optimal Transport&#65288;CMOT&#65289;&#26469;&#20811;&#26381;&#27169;&#24577;&#24046;&#36317;&#12290;&#25105;&#20204;&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#25214;&#21040;&#35821;&#38899;&#21644;&#25991;&#26412;&#24207;&#21015;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#28982;&#21518;&#20351;&#29992;&#23545;&#40784;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24207;&#21015;&#12290;&#22312;MuST-C ST&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;CMOT&#22312;8&#20010;&#32763;&#35793;&#26041;&#21521;&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;BLEU&#20540;&#20026;30.0&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;CMOT&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#25214;&#21040;&#27169;&#24577;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#26377;&#21161;&#20110;&#32531;&#35299;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#20195;&#30721;&#20844;&#24320;&#21487;&#29992;&#20110; https://github.com/ic
&lt;/p&gt;
&lt;p&gt;
End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ic
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35821;&#35328;&#26041;&#38754;&#65288;&#22914;&#30456;&#20114;&#26234;&#33021;&#24615;&#25110;&#35821;&#35328;&#30456;&#20851;&#24615;&#65289;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#33778;&#24459;&#23486;&#35821;&#35328;&#30340;&#30701;&#31687;&#23567;&#35828;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21457;&#29616;&#24212;&#29992;&#19987;&#19994;&#29305;&#24449;CrossNGO&#21487;&#20197;&#25913;&#21892;ARA&#12290;</title><link>http://arxiv.org/abs/2305.13478</link><description>&lt;p&gt;
&#30456;&#36817;&#35821;&#35328;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Automatic Readability Assessment for Closely Related Languages. (arXiv:2305.13478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#35821;&#35328;&#26041;&#38754;&#65288;&#22914;&#30456;&#20114;&#26234;&#33021;&#24615;&#25110;&#35821;&#35328;&#30456;&#20851;&#24615;&#65289;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65292;&#24182;&#20351;&#29992;&#19977;&#31181;&#33778;&#24459;&#23486;&#35821;&#35328;&#30340;&#30701;&#31687;&#23567;&#35828;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21457;&#29616;&#24212;&#29992;&#19987;&#19994;&#29305;&#24449;CrossNGO&#21487;&#20197;&#25913;&#21892;ARA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#21487;&#35835;&#24615;&#35780;&#20272;&#65288;ARA&#65289;&#30340;&#20027;&#35201;&#30740;&#31350;&#37325;&#28857;&#24050;&#36716;&#21521;&#20351;&#29992;&#26114;&#36149;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65292;&#20256;&#32479;&#30340;&#25163;&#24037;&#21046;&#20316;&#29305;&#24449;&#20173;&#28982;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#32570;&#20047;&#29616;&#26377;&#30340;NLP&#24037;&#20855;&#26469;&#25552;&#21462;&#26356;&#28145;&#23618;&#27425;&#30340;&#35821;&#35328;&#34920;&#31034;&#12290;&#25105;&#20204;&#20174;&#25216;&#26415;&#32452;&#20214;&#19978;&#36864;&#19968;&#27493;&#65292;&#30528;&#37325;&#25506;&#35752;&#22914;&#20309;&#36890;&#36807;&#35832;&#22914;&#30456;&#20114;&#26234;&#33021;&#24615;&#25110;&#35821;&#35328;&#30456;&#20851;&#24615;&#30340;&#35821;&#35328;&#26041;&#38754;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;ARA&#12290;&#25105;&#20204;&#25910;&#38598;&#22312;&#33778;&#24459;&#23486;&#30340;&#19977;&#31181;&#35821;&#35328;&#65288;&#20182;&#21152;&#31108;&#35821;&#65292;&#27604;&#31185;&#23572;&#35821;&#21644;&#23487;&#21153;&#35821;&#65289;&#20013;&#32534;&#20889;&#30340;&#30701;&#31687;&#23567;&#35828;&#26469;&#35757;&#32451;&#21487;&#35835;&#24615;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#21508;&#31181;&#36328;&#35821;&#35328;&#35774;&#32622;&#20013;&#30340;&#25968;&#25454;&#21644;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290; &#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20855;&#26377;&#39640;&#30456;&#20114;&#21487;&#29702;&#35299;&#24615;&#30340;&#35821;&#35328;&#20013;&#24212;&#29992;n-gram&#37325;&#21472;&#30340;&#26032;&#22411;&#19987;&#19994;&#29305;&#24449;CrossNGO&#21487;&#20197;&#25913;&#21892;ARA&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the main focus of research on automatic readability assessment (ARA) has shifted towards using expensive deep learning-based methods with the primary goal of increasing models' accuracy. This, however, is rarely applicable for low-resource languages where traditional handcrafted features are still widely used due to the lack of existing NLP tools to extract deeper linguistic representations. In this work, we take a step back from the technical component and focus on how linguistic aspects such as mutual intelligibility or degree of language relatedness can improve ARA in a low-resource setting. We collect short stories written in three languages in the Philippines-Tagalog, Bikol, and Cebuano-to train readability assessment models and explore the interaction of data and features in various cross-lingual setups. Our results show that the inclusion of CrossNGO, a novel specialized feature exploiting n-gram overlap applied to languages with high mutual intelligibility, sig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#35774;&#35745;&#65288;PAF&#65289;&#26550;&#26500;&#39564;&#35777;&#20102;&#21069;&#39304;&#32593;&#32476;&#22312;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#34920;&#26126;FFN&#22359;&#30340;&#20027;&#35201;&#21151;&#33021;&#26159;&#20445;&#25345;&#21508;&#21521;&#21516;&#24615;&#24182;&#38450;&#27490;&#36864;&#21270;&#65292;&#27880;&#24847;&#22359;&#20013;&#35745;&#31639;&#30340;&#27531;&#24046;&#33539;&#25968;&#36828;&#23567;&#20110;&#36755;&#20837;&#20196;&#29260;&#23884;&#20837;&#33539;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.13297</link><description>&lt;p&gt;
&#21033;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#35774;&#35745;&#30740;&#31350;Transformer&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Investigating the Role of Feed-Forward Networks in Transformers Using Parallel Attention and Feed-Forward Net Design. (arXiv:2305.13297v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#35774;&#35745;&#65288;PAF&#65289;&#26550;&#26500;&#39564;&#35777;&#20102;&#21069;&#39304;&#32593;&#32476;&#22312;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#34920;&#26126;FFN&#22359;&#30340;&#20027;&#35201;&#21151;&#33021;&#26159;&#20445;&#25345;&#21508;&#21521;&#21516;&#24615;&#24182;&#38450;&#27490;&#36864;&#21270;&#65292;&#27880;&#24847;&#22359;&#20013;&#35745;&#31639;&#30340;&#27531;&#24046;&#33539;&#25968;&#36828;&#23567;&#20110;&#36755;&#20837;&#20196;&#29260;&#23884;&#20837;&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#35774;&#35745;&#65288;PAF&#65289;&#26550;&#26500;&#65292;&#24182;&#23558;&#20854;&#19982;&#31995;&#21015;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#35774;&#35745;&#65288;SAF&#65289;&#36827;&#34892;&#27604;&#36739;&#65292;&#30740;&#31350;&#21069;&#39304;&#32593;&#32476;&#22312;&#21464;&#21387;&#22120;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290; PAF&#30340;&#26377;&#25928;&#24615;&#20851;&#38190;&#22312;&#20110;&#20004;&#20010;&#20027;&#35201;&#20551;&#35774;&#65292;&#21363;FFN&#22359;&#21644;&#23618;&#20869;&#27880;&#24847;&#22359;&#30340;&#20027;&#35201;&#21151;&#33021;&#20026;&#20445;&#25345;&#20196;&#29260;&#23884;&#20837;&#30340;&#21508;&#21521;&#21516;&#24615;&#24182;&#38450;&#27490;&#20854;&#36864;&#21270;&#65292;&#20197;&#21450;&#27880;&#24847;&#21147;&#22359;&#20013;&#35745;&#31639;&#30340;&#27531;&#24046;&#33539;&#25968;&#36828;&#23567;&#20110;&#36755;&#20837;&#20196;&#29260;&#23884;&#20837;&#33539;&#25968;&#12290;&#20026;&#20102;&#23454;&#35777;&#36825;&#20123;&#20551;&#35774;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;RoBERTa-large&#21644;bert-large-uncased&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;PAF&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;PAF&#35774;&#35745;&#20013;&#65292;&#36825;&#20004;&#20010;&#20551;&#35774;&#37117;&#25104;&#31435;&#12290;&#26412;&#30740;&#31350;&#26377;&#21161;&#20110;&#28145;&#20837;&#20102;&#35299;Transformer&#26550;&#26500;&#20013;FFN&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#20043;&#38388;&#30340;&#20316;&#29992;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the key role of Feed-Forward Networks (FFNs) in transformer models by utilizing the Parallel Attention and Feed-Forward Net Design (PAF) architecture, and comparing it to their Series Attention and Feed-Forward Net Design (SAF) counterparts. Central to the effectiveness of PAF are two main assumptions regarding the FFN block and the attention block within a layer: 1) the primary function of the FFN block is to maintain isotropy among token embeddings and prevent their degeneration, and 2) the residual norm computed in the attention block is substantially smaller than the input token embedding norm. To empirically validate these assumptions, we train PAF variants of two large language models (RoBERTa-large and bert-large-uncased). Our results demonstrate that both assumptions hold true in the PAF design. This study contributes to a deeper understanding of the roles and interactions between FFNs and self-attention mechanisms in transformer architectures.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EMNS/Imz/ Corpus&#30340;&#24773;&#24863;&#21333;&#35828;&#32773;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#20132;&#20114;&#24335;&#21465;&#36848;&#39537;&#21160;&#31995;&#32479;&#20013;&#23545;&#35805;&#30340;&#34920;&#29616;&#21147;&#21644;&#24773;&#24863;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#20256;&#36798;&#24773;&#24863;&#21644;&#34920;&#29616;&#21147;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#23588;&#20854;&#22312;&#20849;&#20139;&#24773;&#24863;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.13137</link><description>&lt;p&gt;
EMNS / Imz / Corpus&#65306;&#24773;&#24863;&#21333;&#35828;&#32773;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#28216;&#25103;&#12289;&#30005;&#35270;&#21644;&#28459;&#30011;&#20013;&#30340;&#21465;&#36848;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels. (arXiv:2305.13137v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13137
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;EMNS/Imz/ Corpus&#30340;&#24773;&#24863;&#21333;&#35828;&#32773;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#20132;&#20114;&#24335;&#21465;&#36848;&#39537;&#21160;&#31995;&#32479;&#20013;&#23545;&#35805;&#30340;&#34920;&#29616;&#21147;&#21644;&#24773;&#24863;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#22312;&#20256;&#36798;&#24773;&#24863;&#21644;&#34920;&#29616;&#21147;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65292;&#23588;&#20854;&#22312;&#20849;&#20139;&#24773;&#24863;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#23383;&#21040;&#35821;&#38899;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#23548;&#33268;&#20102;&#23545;&#20110;&#36866;&#24212;&#23545;&#35805;&#32972;&#26223;&#21644;&#24773;&#24863;&#35821;&#27668;&#30340;&#33258;&#28982;&#24773;&#24863;&#35821;&#38899;&#30340;&#38656;&#27714;&#12290;&#24773;&#24863;&#21465;&#36848;&#25925;&#20107;&#65288;EMNS&#65289;&#35821;&#26009;&#24211;&#26159;&#19968;&#20010;&#29420;&#29305;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#20132;&#20114;&#24335;&#21465;&#36848;&#39537;&#21160;&#31995;&#32479;&#20013;&#23545;&#35805;&#30340;&#34920;&#29616;&#21147;&#21644;&#24773;&#24863;&#36136;&#37327;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;&#19968;&#20301;&#22899;&#24615;&#28436;&#35828;&#32773;&#35762;&#36848;&#26631;&#35760;&#35805;&#35821;&#30340;2.3&#23567;&#26102;&#24405;&#38899;&#65292;&#28085;&#30422;&#20102;&#20843;&#31181;&#34920;&#28436;&#24773;&#24863;&#29366;&#24577;&#65292;&#20998;&#24067;&#22343;&#21248;&#65292;&#26041;&#24046;&#20026;0.68&#65285;&#65292;&#20197;&#21450;&#34920;&#29616;&#21147;&#27700;&#24179;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#21644;&#35789;&#37325;&#38899;&#26631;&#31614;&#12290;&#23545;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#38899;&#39057;&#26679;&#26412;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;EMNS&#35821;&#26009;&#24211;&#22312;&#20934;&#30830;&#20256;&#36798;&#24773;&#24863;&#21644;&#34920;&#29616;&#21147;&#26041;&#38754;&#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;&#20998;&#12290;&#23427;&#22312;&#34920;&#36798;&#20849;&#20139;&#24773;&#24863;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#65292;&#24182;&#36798;&#21040;&#20102;&#21487;&#27604;&#30340;&#30495;&#23454;&#27700;&#24179;&#12290;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#35777;&#23454;&#20102;&#20934;&#30830;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing adoption of text-to-speech technologies has led to a growing demand for natural and emotive voices that adapt to a conversation's context and emotional tone. The Emotive Narrative Storytelling (EMNS) corpus is a unique speech dataset created to enhance conversations' expressiveness and emotive quality in interactive narrative-driven systems. The corpus consists of a 2.3-hour recording featuring a female speaker delivering labelled utterances. It encompasses eight acted emotional states, evenly distributed with a variance of 0.68%, along with expressiveness levels and natural language descriptions with word emphasis labels. The evaluation of audio samples from different datasets revealed that the EMNS corpus achieved the highest average scores in accurately conveying emotions and demonstrating expressiveness. It outperformed other datasets in conveying shared emotions and achieved comparable levels of genuineness. A classification task confirmed the accurate representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#23398;&#20064;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#22312;&#25152;&#26377;&#24310;&#36831;&#24773;&#26223;&#19979;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12774</link><description>&lt;p&gt;
&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#23398;&#20064;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#30340;&#26368;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Learning Optimal Policy for Simultaneous Machine Translation via Binary Search. (arXiv:2305.12774v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#23398;&#20064;&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#26368;&#20248;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#22312;&#25152;&#26377;&#24310;&#36831;&#24773;&#26223;&#19979;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#26426;&#22120;&#32763;&#35793;&#65288;SiMT&#65289;&#22312;&#38405;&#35835;&#28304;&#21477;&#23376;&#26102;&#24320;&#22987;&#36755;&#20986;&#32763;&#35793;&#65292;&#24182;&#38656;&#35201;&#31934;&#30830;&#30340;&#31574;&#30053;&#26469;&#20915;&#23450;&#20309;&#26102;&#36755;&#20986;&#29983;&#25104;&#30340;&#32763;&#35793;&#12290;&#22240;&#27492;&#65292;&#35813;&#31574;&#30053;&#20915;&#23450;&#20102;&#22312;&#32763;&#35793;&#27599;&#20010;&#30446;&#26631;&#20196;&#29260;&#26399;&#38388;&#35835;&#21462;&#30340;&#28304;&#26631;&#35760;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#31934;&#30830;&#30340;&#32763;&#35793;&#31574;&#30053;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24310;&#36831;&#36136;&#37327;&#26435;&#34913;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#27809;&#26377;&#19982;&#24182;&#34892;&#21477;&#23376;&#23545;&#24212;&#30340;&#40644;&#37329;&#31574;&#30053;&#20316;&#20026;&#26174;&#24335;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20108;&#20998;&#25628;&#32034;&#22312;&#32447;&#26500;&#24314;&#26368;&#20248;&#31574;&#30053;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#37319;&#29992;&#26174;&#24335;&#30417;&#30563;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;SiMT&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#65292;&#36825;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23436;&#25104;&#32763;&#35793;&#12290;&#22312;&#22235;&#20010;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#25152;&#26377;&#24310;&#36831;&#26041;&#26696;&#19979;&#36229;&#36234;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous machine translation (SiMT) starts to output translation while reading the source sentence and needs a precise policy to decide when to output the generated translation. Therefore, the policy determines the number of source tokens read during the translation of each target token. However, it is difficult to learn a precise translation policy to achieve good latency-quality trade-offs, because there is no golden policy corresponding to parallel sentences as explicit supervision. In this paper, we present a new method for constructing the optimal policy online via binary search. By employing explicit supervision, our approach enables the SiMT model to learn the optimal policy, which can guide the model in completing the translation during inference. Experiments on four translation tasks show that our method can exceed strong baselines across all latency scenarios.
&lt;/p&gt;</description></item><item><title>FIT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#32452;&#65292;&#20351;&#29992;&#23616;&#37096;&#23618;&#21644;&#20840;&#23616;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#20132;&#38169;&#20351;&#29992;&#36825;&#20123;&#23618;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#65292;FIT&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12689</link><description>&lt;p&gt;
FIT&#65306;&#36828;&#31243;&#20132;&#38169;Transformer
&lt;/p&gt;
&lt;p&gt;
FIT: Far-reaching Interleaved Transformers. (arXiv:2305.12689v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12689
&lt;/p&gt;
&lt;p&gt;
FIT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#32452;&#65292;&#20351;&#29992;&#23616;&#37096;&#23618;&#21644;&#20840;&#23616;&#23618;&#36827;&#34892;&#25805;&#20316;&#12290;&#36890;&#36807;&#20132;&#38169;&#20351;&#29992;&#36825;&#20123;&#23618;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#65292;FIT&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#22343;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FIT&#65306;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#39640;&#25928;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#33258;&#36866;&#24212;&#35745;&#31639;&#33021;&#21147;&#12290;&#19982;&#21407;&#22987;Transformer&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#26631;&#35760;&#20998;&#25104;&#32452;&#65292;&#27599;&#20010;&#32452;&#26159;&#19968;&#20010;&#36739;&#30701;&#30340;&#26631;&#35760;&#24207;&#21015;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#31867;&#22411;&#30340;Transformer&#23618;&#65306;&#23616;&#37096;&#23618;&#22312;&#27599;&#20010;&#32452;&#20869;&#25805;&#20316;&#25968;&#25454;&#26631;&#35760;&#65292;&#32780;&#20840;&#23616;&#23618;&#22312;&#19968;&#20010;&#26356;&#23567;&#30340;&#24341;&#20837;&#30340;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512;&#19978;&#25805;&#20316;&#12290;&#36825;&#20123;&#23618;&#21253;&#25324;&#19982;&#26631;&#20934;Transformer&#30456;&#21516;&#30340;&#33258;&#25105;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#23618;&#65292;&#34987;&#20132;&#38169;&#20351;&#29992;&#65292;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#22312;&#21516;&#19968;&#32452;&#20869;&#25968;&#25454;&#21644;&#28508;&#22312;&#26631;&#35760;&#20043;&#38388;&#20419;&#36827;&#20449;&#24687;&#20132;&#25442;&#12290;&#27599;&#20010;&#22823;&#23567;&#20026;n&#30340;&#32452;&#20869;&#30340;&#27880;&#24847;&#21147;&#22797;&#26434;&#24230;&#20026;$O(n^2)$&#65292;&#20294;&#23545;&#20110;&#38271;&#24230;&#20026;L&#30340;&#24207;&#21015;&#65292;&#21487;&#20197;&#22312;&#20840;&#23616;&#33539;&#22260;&#20869;&#36798;&#21040;$O(L^{{4}/{3}})$&#12290;&#36890;&#36807;&#26356;&#22810;&#22320;&#20381;&#36182;&#25191;&#34892;&#20351;&#29992;&#26356;&#23567;&#28508;&#22312;&#26631;&#35760;&#38598;&#21512;&#30340;&#20840;&#23616;&#23618;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;FIT&#26159;&#19968;&#31181;&#22810;&#29992;&#36884;&#30340;&#26550;&#26500;&#65292;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#20219;&#21153;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#21644;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FIT: a transformer-based architecture with efficient self-attention and adaptive computation. Unlike original transformers, which operate on a single sequence of data tokens, we divide the data tokens into groups, with each group being a shorter sequence of tokens. We employ two types of transformer layers: local layers operate on data tokens within each group, while global layers operate on a smaller set of introduced latent tokens. These layers, comprising the same set of self-attention and feed-forward layers as standard transformers, are interleaved, and cross-attention is used to facilitate information exchange between data and latent tokens within the same group. The attention complexity is $O(n^2)$ locally within each group of size $n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. The efficiency can be further enhanced by relying more on global layers that perform adaptive computation using a smaller set of latent tokens. FIT is a versatile arch
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20132;&#21449;&#27169;&#24577;&#32467;&#26500;&#26530;&#32445;&#23545;&#36328;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22330;&#26223;&#22270;&#21644;&#21477;&#27861;&#21477;&#23376;&#26641;&#32467;&#26500;&#26469;&#25552;&#39640;&#23383;&#24149;&#29983;&#25104;&#30456;&#20851;&#24615;&#21644;&#27969;&#30021;&#24615;&#65292;&#24182;&#20351;&#29992;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24335;&#30340;&#36820;&#35793;&#35757;&#32451;&#20197;&#23436;&#20840;&#23545;&#40784;&#23383;&#24149;&#29983;&#25104;&#21644;&#32763;&#35793;&#38454;&#27573;&#12290;</title><link>http://arxiv.org/abs/2305.12260</link><description>&lt;p&gt;
Cross2StrA: &#22522;&#20110;&#36328;&#35821;&#35328;&#20132;&#21449;&#27169;&#24577;&#32467;&#26500;&#26530;&#32445;&#23545;&#36827;&#34892;&#38750;&#37197;&#23545;&#36328;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment. (arXiv:2305.12260v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12260
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#35821;&#35328;&#20132;&#21449;&#27169;&#24577;&#32467;&#26500;&#26530;&#32445;&#23545;&#36328;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22330;&#26223;&#22270;&#21644;&#21477;&#27861;&#21477;&#23376;&#26641;&#32467;&#26500;&#26469;&#25552;&#39640;&#23383;&#24149;&#29983;&#25104;&#30456;&#20851;&#24615;&#21644;&#27969;&#30021;&#24615;&#65292;&#24182;&#20351;&#29992;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24335;&#30340;&#36820;&#35793;&#35757;&#32451;&#20197;&#23436;&#20840;&#23545;&#40784;&#23383;&#24149;&#29983;&#25104;&#21644;&#32763;&#35793;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36716;&#31227;&#36807;&#31243;&#20013;&#35821;&#20041;&#22330;&#26223;&#21644;&#35821;&#27861;&#23646;&#24615;&#30340;&#19981;&#19968;&#33268;&#24615;&#65292;&#38750;&#37197;&#23545;&#36328;&#35821;&#35328;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#38754;&#20020;&#30528;&#19981;&#30456;&#20851;&#21644;&#35821;&#27861;&#19981;&#27969;&#30021;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#32467;&#21512;&#22330;&#26223;&#22270;&#21644;&#21477;&#27861;&#21477;&#23376;&#26641;&#32467;&#26500;&#26469;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23383;&#24149;&#29983;&#25104;&#22120;&#21253;&#21547;&#35821;&#20041;&#32467;&#26500;&#24341;&#23548;&#30340;&#22270;&#20687;&#21040;&#26530;&#32445;&#65288;&#33521;&#25991;&#65289;&#23383;&#24149;&#29983;&#25104;&#21644;&#35821;&#27861;&#32467;&#26500;&#24341;&#23548;&#19979;&#30340;&#26530;&#32445;&#65288;&#20013;&#25991;&#65289;&#21040;&#30446;&#26631;&#35821;&#35328;&#65288;&#20013;&#25991;&#65289;&#32763;&#35793;&#65292;&#20004;&#32773;&#20043;&#38388;&#36890;&#36807;&#26530;&#32445;&#35821;&#35328;&#36830;&#25509;&#12290;&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#21644;&#21477;&#27861;&#21477;&#23376;&#26641;&#32467;&#26500;&#20316;&#20026;&#26530;&#32445;&#23545;&#36328;&#27169;&#24335;&#35821;&#20041;&#32467;&#26500;&#23545;&#40784;&#21644;&#36328;&#35821;&#35328;&#35821;&#27861;&#32467;&#26500;&#23545;&#40784;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#36328;&#35821;&#35328;&#21644;&#36328;&#27169;&#24335;&#30340;&#36820;&#35793;&#35757;&#32451;&#65292;&#20197;&#23436;&#20840;&#23545;&#40784;&#23383;&#24149;&#29983;&#25104;&#21644;&#32763;&#35793;&#38454;&#27573;&#12290;&#22312;&#33521;&#27721;&#36716;&#31227;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25552;&#39640;&#23383;&#24149;&#29983;&#25104;&#30456;&#20851;&#24615;&#21644;&#27969;&#30021;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unpaired cross-lingual image captioning has long suffered from irrelevancy and disfluency issues, due to the inconsistencies of the semantic scene and syntax attributes during transfer. In this work, we propose to address the above problems by incorporating the scene graph (SG) structures and the syntactic constituency (SC) trees. Our captioner contains the semantic structure-guided image-to-pivot captioning and the syntactic structure-guided pivot-to-target translation, two of which are joined via pivot language. We then take the SG and SC structures as pivoting, performing cross-modal semantic structure alignment and cross-lingual syntactic structure alignment learning. We further introduce cross-lingual&amp;cross-modal back-translation training to fully align the captioning and translation stages. Experiments on English-Chinese transfers show that our model shows great superiority in improving captioning relevancy and fluency.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#20043;&#38388;&#24046;&#36317;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.12258</link><description>&lt;p&gt;
&#20026;&#26080;&#20559;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#26500;&#24314;&#28151;&#21512;&#32534;&#30721;&#30340;&#36890;&#29992;&#20381;&#23384;&#26862;&#26519;
&lt;/p&gt;
&lt;p&gt;
Constructing Code-mixed Universal Dependency Forest for Unbiased Cross-lingual Relation Extraction. (arXiv:2305.12258v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#65292;&#26377;&#21161;&#20110;&#24357;&#21512;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#20043;&#38388;&#24046;&#36317;&#20174;&#32780;&#23454;&#29616;&#26174;&#33879;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#37319;&#29992;&#36890;&#29992;&#20381;&#23384;&#65288;UD&#65289;&#36164;&#28304;&#30340;&#35821;&#35328;&#19968;&#33268;&#24615;&#32467;&#26500;&#29305;&#24449;&#65292;&#20294;&#30001;&#20110;&#35821;&#35328;&#30340;&#19981;&#21487;&#36991;&#20813;&#24046;&#24322;&#65292;&#24456;&#23481;&#26131;&#36973;&#21463;&#21463;&#20559;&#36716;&#31227;&#65288;&#20363;&#22914;&#30446;&#26631;&#20559;&#24046;&#25110;&#28304;&#20559;&#24046;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#31867;&#22411;&#30340;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#12290;&#39318;&#20808;&#23558;&#28304;&#35821;&#35328;&#30340;&#21477;&#23376;&#32763;&#35793;&#25104;&#24179;&#34892;&#30340;&#30446;&#26631;&#35821;&#35328;&#65292;&#23545;&#20004;&#31181;&#35821;&#35328;&#37117;&#20998;&#21035;&#35299;&#26512;UD&#26641;&#65292;&#28982;&#21518;&#23558;&#28304;&#35821;&#35328;/&#30446;&#26631;&#35821;&#35328;&#30340;UD&#32467;&#26500;&#21512;&#24182;&#20026;&#32479;&#19968;&#30340;&#28151;&#21512;&#32534;&#30721;UD&#26641;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#26862;&#26519;&#29305;&#24449;&#65292;UD&#26641;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#35757;&#32451;&#21644;&#39044;&#27979;&#38454;&#27573;&#20043;&#38388;&#30340;&#24046;&#36317;&#21487;&#20197;&#26377;&#25928;&#32553;&#23567;&#12290;&#25105;&#20204;&#22312;ACE XRE&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#28151;&#21512;&#32534;&#30721;UD&#26862;&#26519;&#26377;&#21161;&#20110;&#26080;&#20559;&#30340;UD&#26641;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#36716;&#31227;&#65292;&#24182;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#36328;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latest efforts on cross-lingual relation extraction (XRE) aggressively leverage the language-consistent structural features from the universal dependency (UD) resource, while they may largely suffer from biased transfer (e.g., either target-biased or source-biased) due to the inevitable linguistic disparity between languages. In this work, we investigate an unbiased UD-based XRE transfer by constructing a type of code-mixed UD forest. We first translate the sentence of the source language to the parallel target-side language, for both of which we parse the UD tree respectively. Then, we merge the source-/target-side UD structures as a unified code-mixed UD forest. With such forest features, the gaps of UD-based XRE between the training and predicting phases can be effectively closed. We conduct experiments on the ACE XRE benchmark datasets, where the results demonstrate that the proposed code-mixed UD forests help unbiased UD-based XRE transfer, with which we achieve significant XRE pe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#26223;&#22270;&#30340;&#36724;&#24515;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20266;&#35270;&#35273;&#24773;&#26223;&#22270;&#26469;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#32431;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.12256</link><description>&lt;p&gt;
&#20197;&#24773;&#26223;&#22270;&#20026;&#36724;&#24515;: &#25512;&#29702;&#26102;&#22522;&#20110;&#22270;&#20687;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#35270;&#35273;&#24773;&#26223;&#24187;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scene Graph as Pivoting: Inference-time Image-free Unsupervised Multimodal Machine Translation with Visual Scene Hallucination. (arXiv:2305.12256v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24773;&#26223;&#22270;&#30340;&#36724;&#24515;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#29983;&#25104;&#20266;&#35270;&#35273;&#24773;&#26223;&#22270;&#26469;&#23454;&#29616;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#32431;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26356;&#29616;&#23454;&#30340;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#65288;UMMT&#65289;&#30340;&#25512;&#29702;&#26102;&#22522;&#20110;&#22270;&#20687;&#26080;&#30417;&#30563;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;(Ummt)&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#21644;&#35821;&#35328;&#24773;&#26223;&#22270;(SG)&#26469;&#34920;&#31034;&#36755;&#20837;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#65292;&#36890;&#36807;SG&#34920;&#31034;&#26041;&#27861;&#21487;&#20197;&#30830;&#20445;&#23545;&#35821;&#20041;&#30340;&#25972;&#20307;&#29702;&#35299;&#12290;&#20026;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#32431;&#25991;&#26412;&#36755;&#20837;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#35270;&#35273;&#24773;&#26223;&#24187;&#21270;&#26426;&#21046;&#65292;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#25991;&#26412;SG&#21160;&#24577;&#29983;&#25104;&#20266;&#35270;&#35273;SG&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#31181;&#22522;&#20110;SG&#36724;&#24515;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#32763;&#35793;&#35757;&#32451;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;Multi30K&#19978;&#65292;&#25105;&#20204;&#30340;SG&#26041;&#27861;&#22312;&#20219;&#21153;&#21644;&#35774;&#32622;&#19978;&#30340; BLEU &#24471;&#20998;&#26174;&#33879;&#39640;&#20110;&#26368;&#20339;&#22522;&#32447;&#65292;&#26377;&#21161;&#20110;&#20135;&#29983;&#26356;&#23436;&#25972;&#12289;&#30456;&#20851;&#21644;&#27969;&#30021;&#30340;&#32763;&#35793;&#65292;&#32780;&#19981;&#38656;&#35201;&#20381;&#36182;&#25104;&#23545;&#30340;&#22270;&#20687;&#12290;&#36827;&#19968;&#27493;&#30340;&#28145;&#20837;&#20998;&#26512;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22914;&#20309;&#25512;&#36827;UMMT&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate a more realistic unsupervised multimodal machine translation (UMMT) setup, inference-time image-free UMMT, where the model is trained with source-text image pairs, and tested with only source-text inputs. First, we represent the input images and texts with the visual and language scene graphs (SG), where such fine-grained vision-language features ensure a holistic understanding of the semantics. To enable pure-text input during inference, we devise a visual scene hallucination mechanism that dynamically generates pseudo visual SG from the given textual SG. Several SG-pivoting based learning objectives are introduced for unsupervised translation training. On the benchmark Multi30K data, our SG-based method outperforms the best-performing baseline by significant BLEU scores on the task and setup, helping yield translations with better completeness, relevance and fluency without relying on paired images. Further in-depth analyses reveal how our model advances 
&lt;/p&gt;</description></item><item><title>&#21069;&#32512;&#20256;&#25773;&#26159;&#19968;&#31181;&#38024;&#23545;&#38271;&#24207;&#21015;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;50%&#20943;&#23569;&#21442;&#25968;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20219;&#21153;&#26102;&#20855;&#26377;&#26356;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12086</link><description>&lt;p&gt;
&#21069;&#32512;&#20256;&#25773;: &#38024;&#23545;&#38271;&#24207;&#21015;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prefix Propagation: Parameter-Efficient Tuning for Long Sequences. (arXiv:2305.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12086
&lt;/p&gt;
&lt;p&gt;
&#21069;&#32512;&#20256;&#25773;&#26159;&#19968;&#31181;&#38024;&#23545;&#38271;&#24207;&#21015;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;50%&#20943;&#23569;&#21442;&#25968;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20219;&#21153;&#26102;&#20855;&#26377;&#26356;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26088;&#22312;&#20943;&#36731;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#20869;&#23384;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#21069;&#32512;&#20256;&#25773;&#36825;&#19968;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24357;&#34917;&#30446;&#21069;&#21069;&#32512;&#35843;&#25972;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#21069;&#32512;&#20256;&#25773;&#19982;&#21069;&#32512;&#35843;&#25972;&#30456;&#27604;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20219;&#21153;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25152;&#38656;&#21442;&#25968;&#20063;&#21482;&#26377;&#21069;&#32773;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model's parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#23558;3D&#22330;&#26223;&#29305;&#24449;&#32435;&#20837;VSD&#26041;&#27861;&#65292;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;3D&#31354;&#38388;&#22330;&#26223;&#22270;(Go3D-S2G)&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.11768</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#20307;3D&#22330;&#26223;&#29702;&#35299;&#29983;&#25104;&#35270;&#35273;&#31354;&#38388;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generating Visual Spatial Description via Holistic 3D Scene Understanding. (arXiv:2305.11768v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23558;3D&#22330;&#26223;&#29305;&#24449;&#32435;&#20837;VSD&#26041;&#27861;&#65292;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;3D&#31354;&#38388;&#22330;&#26223;&#22270;(Go3D-S2G)&#65292;&#25552;&#20986;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#31354;&#38388;&#25551;&#36848;(VSD)&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#25551;&#36848;&#22270;&#20687;&#20013;&#32473;&#23450;&#23545;&#35937;&#31354;&#38388;&#20851;&#31995;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;VSD&#24037;&#20316;&#20165;&#27169;&#25311;2D&#20960;&#20309;&#35270;&#35273;&#29305;&#24449;&#65292;&#22240;&#27492;&#19981;&#21487;&#36991;&#20813;&#22320;&#38519;&#20837;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#29702;&#35299;&#20542;&#26012;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;3D&#22330;&#26223;&#29305;&#24449;&#32435;&#20837;VSD&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22806;&#37096;3D&#22330;&#26223;&#25552;&#21462;&#22120;&#65292;&#25105;&#20204;&#33719;&#21462;&#36755;&#20837;&#22270;&#20687;&#30340;3D&#23545;&#35937;&#21644;&#22330;&#26223;&#29305;&#24449;&#65292;&#22522;&#20110;&#27492;&#26500;&#24314;&#30446;&#26631;&#23545;&#35937;&#20026;&#20013;&#24515;&#30340;3D&#31354;&#38388;&#22330;&#26223;&#22270;(Go3D-S2G)&#65292;&#20174;&#32780;&#27169;&#25311;&#30446;&#26631;&#23545;&#35937;&#22312;&#25972;&#20307;3D&#22330;&#26223;&#20013;&#30340;&#31354;&#38388;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22330;&#26223;&#23376;&#22270;&#36873;&#25321;&#26426;&#21046;&#65292;&#20174;Go3D-S2G&#20013;&#37319;&#26679;&#25299;&#25169;&#22810;&#26679;&#30340;&#23376;&#22270;&#65292;&#23548;&#33322;&#19981;&#21516;&#30340;&#23616;&#37096;&#32467;&#26500;&#29305;&#24449;&#20197;&#20135;&#29983;&#31354;&#38388;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#29983;&#25104;&#12290;&#23545;&#20004;&#20010;VSD&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#65292;&#29305;&#21035;&#26159;&#22312;one-split&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual spatial description (VSD) aims to generate texts that describe the spatial relations of the given objects within images. Existing VSD work merely models the 2D geometrical vision features, thus inevitably falling prey to the problem of skewed spatial understanding of target objects. In this work, we investigate the incorporation of 3D scene features for VSD. With an external 3D scene extractor, we obtain the 3D objects and scene features for input images, based on which we construct a target object-centered 3D spatial scene graph (Go3D-S2G), such that we model the spatial semantics of target objects within the holistic 3D scenes. Besides, we propose a scene subgraph selecting mechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where the diverse local structure features are navigated to yield spatially-diversified text generation. Experimental results on two VSD datasets demonstrate that our framework outperforms the baselines significantly, especially improving on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20869;&#37096;&#20449;&#24687;&#31579;&#36873;&#21644;&#22806;&#37096;&#20449;&#24687;&#21033;&#29992;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#35270;&#35273;&#21644;&#25991;&#26412;&#22330;&#26223;&#22270;&#34920;&#31034;&#36755;&#20837;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#36827;&#34892;&#32467;&#26500;&#32454;&#21270;&#21644;&#29305;&#24449;&#21435;&#22122;&#65292;&#21516;&#26102;&#36816;&#29992;&#20027;&#39064;&#24314;&#27169;&#20016;&#23500;&#19978;&#19979;&#25991;&#65292;&#35813;&#31995;&#32479;&#22312;&#22522;&#20934;MRE&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.11719</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#21435;&#22122;&#21644;&#22810;&#27169;&#24577;&#20027;&#39064;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#20449;&#24687;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling. (arXiv:2305.11719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#20869;&#37096;&#20449;&#24687;&#31579;&#36873;&#21644;&#22806;&#37096;&#20449;&#24687;&#21033;&#29992;&#30340;&#24605;&#24819;&#12290;&#36890;&#36807;&#35270;&#35273;&#21644;&#25991;&#26412;&#22330;&#26223;&#22270;&#34920;&#31034;&#36755;&#20837;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32467;&#26500;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#36827;&#34892;&#32467;&#26500;&#32454;&#21270;&#21644;&#29305;&#24449;&#21435;&#22122;&#65292;&#21516;&#26102;&#36816;&#29992;&#20027;&#39064;&#24314;&#27169;&#20016;&#23500;&#19978;&#19979;&#25991;&#65292;&#35813;&#31995;&#32479;&#22312;&#22522;&#20934;MRE&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#20851;&#31995;&#25277;&#21462;(MRE)&#30740;&#31350;&#38754;&#20020;&#30528;&#20004;&#20010;&#20849;&#23384;&#30340;&#25361;&#25112;&#65292;&#21363;&#20869;&#37096;&#20449;&#24687;&#36807;&#24230;&#21033;&#29992;&#21644;&#22806;&#37096;&#20449;&#24687;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#20869;&#37096;&#20449;&#24687;&#31579;&#36873;&#21644;&#22806;&#37096;&#20449;&#24687;&#21033;&#29992;&#30340;&#24605;&#24819;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#22330;&#26223;&#22270;&#34920;&#31034;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32454;&#31890;&#24230;&#35821;&#20041;&#32467;&#26500;&#65292;&#23558;&#20854;&#36827;&#19968;&#27493;&#34701;&#21512;&#25104;&#19968;&#20010;&#32479;&#19968;&#30340;&#36328;&#27169;&#24577;&#22270;(CMG)&#12290;&#22522;&#20110;CMG&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#24418;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#36827;&#34892;&#32467;&#26500;&#32454;&#21270;&#65292;&#20027;&#21160;&#21435;&#38500;&#19981;&#22826;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23545;&#36755;&#20837;&#22270;&#20687;&#21644;&#25991;&#26412;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#65292;&#23558;&#28508;&#22312;&#30340;&#22810;&#27169;&#24577;&#20027;&#39064;&#29305;&#24449;&#34701;&#20837;&#20854;&#20013;&#20197;&#20016;&#23500;&#19978;&#19979;&#25991;&#12290;&#22312;&#22522;&#20934;MRE&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26368;&#20339;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#28145;&#20837;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MRE&#20219;&#21153;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing research on multimodal relation extraction (MRE) faces two co-existing challenges, internal-information over-utilization and external-information under-exploitation. To combat that, we propose a novel framework that simultaneously implements the idea of internal-information screening and external-information exploiting. First, we represent the fine-grained semantic structures of the input image and text with the visual and textual scene graphs, which are further fused into a unified cross-modal graph (CMG). Based on CMG, we perform structure refinement with the guidance of the graph information bottleneck principle, actively denoising the less-informative features. Next, we perform topic modeling over the input image and text, incorporating latent multimodal topic features to enrich the contexts. On the benchmark MRE dataset, our system outperforms the current best model significantly. With further in-depth analyses, we reveal the great potential of our method for the MRE task
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.11442</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Text Classification via Self-Supervised Tuning. (arXiv:2305.11442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#35843;&#25972;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#20219;&#21153;&#30340;&#38646;&#26679;&#26412;&#25512;&#26029;&#65292;&#27169;&#22411;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#65292;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#19981;&#38169;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65307;&#35201;&#20040;&#20381;&#36182;&#20110;&#22823;&#37327;&#30456;&#20851;&#20219;&#21153;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#26469;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;&#33258;&#30417;&#30563;&#35843;&#25972;&#12290;&#36890;&#36807;&#25506;&#32034;&#33258;&#30001;&#25991;&#26412;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#65292;&#31216;&#20026;&#39318;&#21477;&#39044;&#27979;&#65292;&#20197;&#24357;&#21512;&#26080;&#26631;&#31614;&#25968;&#25454;&#21644;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#35843;&#25972;&#27169;&#22411;&#20197;&#23398;&#20064;&#26681;&#25454;&#21097;&#20313;&#25991;&#26412;&#26469;&#39044;&#27979;&#27573;&#33853;&#20013;&#30340;&#31532;&#19968;&#21477;&#35805;&#21518;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#25512;&#26029;&#20986;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#65292;&#22914;&#20027;&#39064;&#20998;&#31867;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;10&#20010;&#20219;&#21153;&#20013;&#30340;7&#20010;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#27169;&#26495;&#30340;&#36873;&#25321;&#19981;&#25935;&#24863;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#20803;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.11255</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Reasoning Implicit Sentiment with Chain-of-Thought Prompting. (arXiv:2305.11255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#32034;&#24341;&#30340;&#38544;&#24335;&#24773;&#24863;&#25512;&#26029;&#26694;&#26550;&#65288;THOR&#65289;&#65292;&#36890;&#36807;&#19977;&#27425;&#36339;&#25512;&#29702;&#27169;&#20223;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#65292;&#25903;&#25345;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#20197;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#65292;&#24182;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#65292;&#23454;&#29616;&#20102;&#22312;&#30417;&#30563;&#21644;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#22823;&#24133;&#25552;&#39640;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#31995;&#32479;&#36890;&#36807;&#20998;&#26512;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#20851;&#38190;&#35266;&#28857;&#34920;&#36798;&#26469;&#30830;&#23450;&#32473;&#23450;&#30446;&#26631;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#32780;&#22312;&#38544;&#24335;&#24773;&#24863;&#20998;&#26512;&#65288;ISA&#65289;&#20013;&#65292;&#35266;&#28857;&#25552;&#31034;&#20197;&#19968;&#31181;&#38544;&#21547;&#21644;&#27169;&#31946;&#30340;&#26041;&#24335;&#20986;&#29616;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#38544;&#24335;&#24773;&#24863;&#38656;&#35201;&#24120;&#35782;&#21644;&#22810;&#36339;&#25512;&#29702;&#33021;&#21147;&#26469;&#25512;&#26029;&#24847;&#35265;&#30340;&#28508;&#22312;&#24847;&#22270;&#12290;&#21463;&#26368;&#36817;&#24605;&#32500;&#38142;&#32034;&#24341;&#65288;CoT&#65289;&#24605;&#24819;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#19977;&#27425;&#36339;&#25512;&#29702;&#65288;THOR&#65289;CoT&#26694;&#26550;&#65292;&#27169;&#20223;ISA&#30340;&#31867;&#20154;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#20026;THOR&#35774;&#35745;&#20102;&#19968;&#20010;&#19977;&#27493;&#25552;&#31034;&#21407;&#21017;&#65292;&#20197;&#36880;&#27493;&#35825;&#23548;&#38544;&#24335;&#26041;&#38754;&#12289;&#24847;&#35265;&#21644;&#26368;&#32456;&#24773;&#24863;&#26497;&#24615;&#12290;&#25105;&#20204;&#30340;THOR+Flan-T5&#65288;11B&#65289;&#22312;&#30417;&#30563;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25512;&#36827;&#20102;&#36229;&#36807;6&#65285;&#30340;F1&#20540;&#12290;&#26356;&#20026;&#26174;&#33879;&#30340;&#26159;&#65292;THOR+GPT3&#65288;175B&#65289;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19978;&#23558;&#25216;&#26415;&#27700;&#24179;&#25552;&#21319;&#20102;&#36229;&#36807;50&#65285;&#30340;F1&#20540;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20301;&#20110;https://github.com/scofield7419/THOR-ISA &#12290;
&lt;/p&gt;
&lt;p&gt;
While sentiment analysis systems try to determine the sentiment polarities of given targets based on the key opinion expressions in input texts, in implicit sentiment analysis (ISA) the opinion cues come in an implicit and obscure manner. Thus detecting implicit sentiment requires the common-sense and multi-hop reasoning ability to infer the latent intent of opinion. Inspired by the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop Reasoning (THOR) CoT framework to mimic the human-like reasoning process for ISA. We design a three-step prompting principle for THOR to step-by-step induce the implicit aspect, opinion, and finally the sentiment polarity. Our THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50% F1 on zero-shot setting. Our code is at https://github.com/scofield7419/THOR-ISA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CIF-Aligned&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#38750;&#33258;&#22238;&#24402;E2E ASR&#27169;&#22411;-Paraformer&#30340;&#29305;&#24615;&#65292;&#29983;&#25104;&#31526;&#21495;&#21516;&#27493;&#30340;&#22768;&#23398;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2305.10680</link><description>&lt;p&gt;
&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#30340;&#20934;&#30830;&#21487;&#38752;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate and Reliable Confidence Estimation Based on Non-Autoregressive End-to-End Speech Recognition System. (arXiv:2305.10680v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;CIF-Aligned&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;&#65292;&#21033;&#29992;&#20102;&#38750;&#33258;&#22238;&#24402;E2E ASR&#27169;&#22411;-Paraformer&#30340;&#29305;&#24615;&#65292;&#29983;&#25104;&#31526;&#21495;&#21516;&#27493;&#30340;&#22768;&#23398;&#23884;&#20837;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#21487;&#38752;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;ASR&#39046;&#22495;&#20013;&#65292;&#20272;&#35745;&#35782;&#21035;&#32467;&#26524;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#26159;&#19968;&#39033;&#32463;&#20856;&#20219;&#21153;&#65292;&#23545;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#35757;&#32451;&#31574;&#30053;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#31471;&#21040;&#31471;(E2E)&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;(CEM)&#39044;&#27979;&#19982;&#36755;&#20837;&#36716;&#24405;&#25991;&#26412;&#38271;&#24230;&#30456;&#31561;&#30340;&#24471;&#20998;&#24207;&#21015;&#65292;&#23548;&#33268;&#22312;&#21024;&#38500;&#21644;&#25554;&#20837;&#38169;&#35823;&#21457;&#29983;&#26102;&#20272;&#35745;&#19981;&#21487;&#38752;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;CIF&#23545;&#40784;&#32622;&#20449;&#24230;&#20272;&#35745;&#27169;&#22411;(CA-CEM)&#65292;&#21033;&#29992;&#36830;&#32493;&#31215;&#20998;&#21644;&#28779; CIF &#26426;&#21046;&#26469;&#29983;&#25104;&#31526;&#21495;&#21516;&#27493;&#30340;&#22768;&#23398;&#23884;&#20837;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#30340;&#20272;&#35745;&#22833;&#36133;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#20351;&#29992;AUC&#21644;RMSE&#20197;&#21450;utterance&#32423;&#21035;&#19978;&#30340;&#19968;&#31181;&#25552;&#20986;&#24230;&#37327;ECE-U&#26469;&#34913;&#37327;&#20272;&#35745;&#36136;&#37327;&#12290;CA-CEM&#22312;ECE-U&#19978;&#33719;&#24471;&#20102;24%&#21644;19%&#30340;&#30456;&#23545;&#38477;&#20302;&#29575;&#65292;&#24182;&#19988;&#22312;&#20004;&#20010;&#27979;&#35797;&#38598;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;AUC&#21644;RMSE&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating confidence scores for recognition results is a classic task in ASR field and of vital importance for kinds of downstream tasks and training strategies. Previous end-to-end~(E2E) based confidence estimation models (CEM) predict score sequences of equal length with input transcriptions, leading to unreliable estimation when deletion and insertion errors occur. In this paper we proposed CIF-Aligned confidence estimation model (CA-CEM) to achieve accurate and reliable confidence estimation based on novel non-autoregressive E2E ASR model - Paraformer. CA-CEM utilizes the modeling character of continuous integrate-and-fire (CIF) mechanism to generate token-synchronous acoustic embedding, which solves the estimation failure issue above. We measure the quality of estimation with AUC and RMSE in token level and ECE-U - a proposed metrics in utterance level. CA-CEM gains 24% and 19% relative reduction on ECE-U and also better AUC and RMSE on two test sets. Furthermore, we conduct anal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#21517;&#35789;&#22797;&#21512;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#21517;&#35789;&#22797;&#21512;&#35789;&#35299;&#37322;&#30340;&#20219;&#21153;&#21644;&#21517;&#35789;&#22797;&#21512;&#35789;&#27010;&#24565;&#21270;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;GPT-3&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.10568</link><description>&lt;p&gt;
&#20174;&#24039;&#20811;&#21147;&#20820;&#21040;&#24039;&#20811;&#21147;&#40132;&#40060;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21517;&#35789;&#22797;&#21512;&#35789;&#65311;
&lt;/p&gt;
&lt;p&gt;
From chocolate bunny to chocolate crocodile: Do Language Models Understand Noun Compounds?. (arXiv:2305.10568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#21517;&#35789;&#22797;&#21512;&#35789;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#21517;&#35789;&#22797;&#21512;&#35789;&#35299;&#37322;&#30340;&#20219;&#21153;&#21644;&#21517;&#35789;&#22797;&#21512;&#35789;&#27010;&#24565;&#21270;&#30340;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616;GPT-3&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21517;&#35789;&#22797;&#21512;&#35789;&#26159;&#25351;&#23558;&#22810;&#20010;&#21517;&#35789;&#32452;&#21512;&#25104;&#19968;&#20010;&#26032;&#35789;&#65292;&#22914;&#8220;&#24039;&#20811;&#21147;&#20820;&#8221;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#35789;&#22797;&#21512;&#35789;&#35299;&#37322;&#30340;&#20219;&#21153;&#24182;&#20462;&#25913;&#20102;&#35813;&#20219;&#21153;&#30340;&#25968;&#25454;&#21644;&#35780;&#20272;&#35774;&#32622;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3&#65288;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65289;&#20960;&#20046;&#21487;&#20197;&#23436;&#32654;&#22320;&#35299;&#20915;&#35813;&#20219;&#21153;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25506;&#35752;&#20102;&#21517;&#35789;&#22797;&#21512;&#35789;&#27010;&#24565;&#21270;&#30340;&#20219;&#21153;&#65292;&#21363;&#35299;&#37322;&#26032;&#39062;&#25110;&#32597;&#35265;&#30340;&#21517;&#35789;&#32452;&#21512;&#35789;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;GPT-3&#25512;&#29702;&#19990;&#30028;&#30340;&#31243;&#24230;&#65292;&#24182;&#21457;&#29616;&#23427;&#30340;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Noun compound interpretation is the task of expressing a noun compound (e.g. chocolate bunny) in a free-text paraphrase that makes the relationship between the constituent nouns explicit (e.g. bunny-shaped chocolate). We propose modifications to the data and evaluation setup of the standard task (Hendrickx et al., 2013), and show that GPT-3 solves it almost perfectly. We then investigate the task of noun compound conceptualization, i.e. paraphrasing a novel or rare noun compound. E.g., chocolate crocodile is a crocodile-shaped chocolate. This task requires creativity, commonsense, and the ability to generalize knowledge about similar concepts. While GPT-3's performance is not perfect, it is better than that of humans -- likely thanks to its access to vast amounts of knowledge, and because conceptual processing is effortful for people (Connell and Lynott, 2012). Finally, we estimate the extent to which GPT-3 is reasoning about the world vs. parroting its training data. We find that the 
&lt;/p&gt;</description></item><item><title>sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.08711</link><description>&lt;p&gt;
sustain.AI: &#19968;&#31181;&#20998;&#26512;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
sustain.AI: a Recommender System to analyze Sustainability Reports. (arXiv:2305.08711v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08711
&lt;/p&gt;
&lt;p&gt;
sustain.AI&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#65292;&#24182;&#36890;&#36807;&#19982;GRI&#26631;&#20934;&#21305;&#37197;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#25512;&#33616;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;sustain.AI&#65292;&#36825;&#26159;&#19968;&#20010;&#26234;&#33021;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#21487;&#24110;&#21161;&#23457;&#35745;&#24072;&#12289;&#37329;&#34701;&#25237;&#36164;&#32773;&#20197;&#21450;&#24191;&#22823;&#20844;&#20247;&#39640;&#25928;&#22320;&#20998;&#26512;&#20844;&#21496;&#30340;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#12290;&#35813;&#24037;&#20855;&#21033;&#29992;&#20102;&#31471;&#21040;&#31471;&#21487;&#35757;&#32451;&#30340;&#26550;&#26500;&#65292;&#23558;&#22522;&#20110;BERT&#30340;&#32534;&#30721;&#27169;&#22359;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#22836;&#30456;&#32467;&#21512;&#65292;&#23558;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#20013;&#30340;&#30456;&#20851;&#25991;&#26412;&#27573;&#33853;&#19982;&#20840;&#29699;&#25253;&#21578;&#20513;&#35758;&#65288;GRI&#65289;&#26631;&#20934;&#20013;&#30340;&#30456;&#24212;&#27861;&#24459;&#27861;&#35268;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26032;&#39062;&#30340;&#24503;&#22269;&#21487;&#25345;&#32493;&#24615;&#25253;&#21578;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#23454;&#29616;&#20102;&#19982;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#26356;&#39640;&#30340;&#25512;&#33616;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;sustain.AI&#24050;&#32463;&#20844;&#24320;&#22312;https://sustain.ki.nrw/&#19978;&#25552;&#20379;&#32473;&#25152;&#26377;&#20154;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present $\text{sustain.AI}$, an intelligent, context-aware recommender system that assists auditors and financial investors as well as the general public to efficiently analyze companies' sustainability reports. The tool leverages an end-to-end trainable architecture that couples a BERT-based encoding module with a multi-label classification head to match relevant text passages from sustainability reports to their respective law regulations from the Global Reporting Initiative (GRI) standards. We evaluate our model on two novel German sustainability reporting data sets and consistently achieve a significantly higher recommendation performance compared to multiple strong baselines. Furthermore, $\text{sustain.AI}$ is publicly available for everyone at https://sustain.ki.nrw/.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;</title><link>http://arxiv.org/abs/2305.08339</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#65306;&#26412;&#22320;&#35821;&#27861;&#20998;&#26512;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis. (arXiv:2305.08339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#33258;&#21160;&#26631;&#27880;&#25991;&#26412;&#30340;&#28508;&#21147;&#65292;&#37325;&#28857;&#32771;&#23519;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#27169;&#22411;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;ChatGPT&#21644;&#20154;&#31867;&#26631;&#27880;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;LLMs&#22312;&#21327;&#21161;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#33258;&#21160;&#26631;&#27880;&#20026;&#29305;&#23450;&#35821;&#35328;&#20449;&#24687;&#31867;&#21035;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#26412;&#22320;&#35821;&#27861;&#30340;&#35282;&#24230;&#35266;&#23519;&#36947;&#27465;&#35328;&#35821;&#34892;&#20026;&#26500;&#25104;&#30340;&#21151;&#33021;&#20803;&#32032;&#30340;&#31243;&#24230;&#65292;&#36890;&#36807;&#27604;&#36739;&#22522;&#20110;GPT-3.5&#30340;ChatGPT&#12289;&#22522;&#20110;GPT-4&#30340;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#32534;&#30721;&#22120;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#30528;&#20248;&#20110;ChatGPT&#12290;&#19982;&#20154;&#31867;&#26631;&#27880;&#21592;&#30456;&#27604;&#65292;Bing&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25972;&#20307;&#34920;&#29616;&#30053;&#20302;&#20110;&#20154;&#31867;&#26631;&#27880;&#21592;&#30340;&#34920;&#29616;&#65292;&#20294;&#24050;&#32463;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;F1&#24471;&#20998;:&#36947;&#27465;&#26631;&#35760;99.95&#65285;&#65292;&#21407;&#22240;&#26631;&#35760;91.91&#65285;&#65292;&#36947;&#27465;&#32773;&#26631;&#35760;95.35&#65285;&#65292;&#34987;&#36947;&#27465;&#32773;&#26631;&#35760;89.74&#65285;&#21644;&#21152;&#24378;&#26631;&#35760;96.47&#65285;&#12290;&#36825;&#34920;&#26126;&#65292;&#22312;&#35821;&#35328;&#31867;&#21035;&#28165;&#26224;&#19988;&#21487;&#20197;&#36731;&#26494;&#35782;&#21035;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;LLM&#36741;&#21161;&#27880;&#37322;&#36827;&#34892;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots based on Large Language Models (LLMs) have shown strong capabilities in language understanding. In this study, we explore the potential of LLMs in assisting corpus-based linguistic studies through automatic annotation of texts with specific categories of linguistic information. Specifically, we examined to what extent LLMs understand the functional elements constituting the speech act of apology from a local grammar perspective, by comparing the performance of ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and a human coder in the annotation task. The results demonstrate that the Bing chatbot significantly outperformed ChatGPT in the task. Compared to human annotator, the overall performance of the Bing chatbot was slightly less satisfactory. However, it already achieved high F1 scores: 99.95% for the tag of APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% for APOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible to use LLM-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;</title><link>http://arxiv.org/abs/2305.07759</link><description>&lt;p&gt;
TinyStories: &#35821;&#35328;&#27169;&#22411;&#33021;&#31616;&#23567;&#21040;&#20160;&#20040;&#31243;&#24230;&#21364;&#20381;&#28982;&#33021;&#22815;&#35762;&#36848;&#36830;&#36143;&#30340;&#33521;&#25991;&#25925;&#20107;&#65311;
&lt;/p&gt;
&lt;p&gt;
TinyStories: How Small Can Language Models Be and Still Speak Coherent English?. (arXiv:2305.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36830;&#36143;&#30340;&#33521;&#25991;&#25991;&#26412;&#38590;&#39064;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598; TinyStories&#65292;&#24182;&#25506;&#32034;&#23567;&#22411;&#27169;&#22411;&#35268;&#27169;&#12289;&#32467;&#26500;&#22797;&#26434;&#24230;&#21644;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#30340;&#24433;&#21709;&#65292;&#35777;&#26126;&#20102;&#20165;&#21547; 200 &#19975;&#21442;&#25968;&#30340;&#31616;&#21333;&#35821;&#35328;&#27169;&#22411;&#20063;&#33021;&#20135;&#29983;&#36830;&#36143;&#30340;&#30701;&#25925;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#22312;&#23567;&#22411;&#21270;&#26102;&#32463;&#24120;&#38590;&#20197;&#20135;&#29983;&#36830;&#36143;&#21644;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026; TinyStories &#30340;&#21512;&#25104;&#25925;&#20107;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#35268;&#27169;&#23567;&#12289;&#22797;&#26434;&#24230;&#20302;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#30701;&#25925;&#20107;&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet stil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.07677</link><description>&lt;p&gt;
Masked Audio Text Encoders &#22312;&#22810;&#27169;&#24577;&#37325;&#25171;&#20998;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Audio Text Encoders are Effective Multi-Modal Rescorers. (arXiv:2305.07677v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Masked Audio Text Encoders&#65288;MATE&#65289;&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#20351;&#29992;MATE&#23545;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36827;&#34892;&#22810;&#27169;&#24577;&#25171;&#20998;&#65292;&#21363;&#20351;&#22312;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#36275;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#25552;&#39640;&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#19979;&#23601;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#30340;&#20108;&#27425;&#25171;&#20998;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Masked Audio Text Encoder&#65288;MATE&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#25171;&#20998;&#22120;&#65292;&#23558;&#22768;&#23398;&#34920;&#31034;&#24418;&#24335;&#24182;&#20837;&#21040;MLM&#30340;&#36755;&#20837;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#36890;&#36807;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#26469;&#26377;&#25928;&#22320;&#23545;&#40784;&#21508;&#31181;&#27169;&#24577;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#30446;&#26631;&#22495;&#25968;&#25454;&#19981;&#21487;&#29992;&#26102;&#65292;&#20351;&#29992;&#22810;&#27169;&#24577;&#37325;&#26032;&#25171;&#20998;&#22120;&#23545;ASR&#31995;&#32479;&#30340;&#39046;&#22495;&#27867;&#21270;&#24456;&#26377;&#22909;&#22788;&#12290;&#19982;&#20165;&#25991;&#26412;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#22312;&#22495;&#20869;&#25968;&#25454;&#32452;&#19978;&#65292;MATE &#21487;&#20197;&#23558;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#38477;&#20302;4&#65285;-16&#65285;&#65292;&#22312;&#22495;&#22806;&#25968;&#25454;&#32452;&#19978;&#21487;&#23558;WER&#38477;&#20302;3&#65285;-7&#65285;&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;0.8&#23567;&#26102;&#65289;&#65292;MATE&#23601;&#21487;&#20197;&#23558;WER&#27604;&#19968;&#27425;&#25171;&#20998;&#30340;&#22522;&#32447;&#38477;&#20302;8&#65285;-23&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Models (MLMs) have proven to be effective for second-pass rescoring in Automatic Speech Recognition (ASR) systems. In this work, we propose Masked Audio Text Encoder (MATE), a multi-modal masked language model rescorer which incorporates acoustic representations into the input space of MLM. We adopt contrastive learning for effectively aligning the modalities by learning shared representations. We show that using a multi-modal rescorer is beneficial for domain generalization of the ASR system when target domain data is unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and 3%-7% on out-of-domain datasets, over the text-only baseline. Additionally, with very limited amount of training data (0.8 hours), MATE achieves a WER reduction of 8%-23% over the first-pass baseline.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SemEval-2023 Task 2&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;12&#31181;&#35821;&#35328;&#20013;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#26368;&#20248;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26159;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#23454;&#20307;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06586</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#65306;&#32454;&#31890;&#24230;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER 2&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). (arXiv:2305.06586v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;SemEval-2023 Task 2&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35782;&#21035;12&#31181;&#35821;&#35328;&#20013;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#26368;&#20248;&#26041;&#27861;&#26159;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#65292;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#26159;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#23454;&#20307;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SemEval-2023&#20219;&#21153;2&#20851;&#20110;&#32454;&#31890;&#24230;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MultiCoNER 2&#65289;&#30340;&#30740;&#31350;&#21457;&#29616;&#12290;&#35813;&#20219;&#21153;&#20998;&#20026;13&#20010;&#36712;&#36947;&#65292;&#37325;&#28857;&#20851;&#27880;12&#31181;&#35821;&#35328;&#21644;&#21333;&#35821;&#12289;&#22810;&#35821;&#21644;&#22024;&#26434;&#29615;&#22659;&#19979;&#35782;&#21035;&#22797;&#26434;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#65288;&#22914;WRITTENWORK&#12289;VEHICLE&#12289;MUSICALGRP&#65289;&#30340;&#26041;&#27861;&#12290;&#20219;&#21153;&#20351;&#29992;MultiCoNER V2&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;Bangla&#12289;Chinese&#12289;English&#12289;Farsi&#12289;French&#12289;German&#12289;Hindi&#12289;Italian&#12289;Portuguese&#12289;Spanish&#12289;Swedish&#21644;Ukrainian&#32452;&#25104;&#65292;&#20849;&#26377;220&#19975;&#20010;&#23454;&#20363;&#12290;MultiCoNER 2&#26159;SemEval-2023&#20013;&#26368;&#21463;&#27426;&#36814;&#30340;&#20219;&#21153;&#20043;&#19968;&#65292;&#21560;&#24341;&#20102;47&#20010;&#38431;&#20237;&#25552;&#20132;842&#20010;&#32467;&#26524;&#65292;&#20854;&#20013;34&#20010;&#38431;&#20237;&#25552;&#20132;&#20102;&#31995;&#32479;&#35770;&#25991;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23186;&#20307;&#26631;&#39064;&#21644;&#20135;&#21697;&#21517;&#31216;&#31561;&#22797;&#26434;&#23454;&#20307;&#31867;&#22411;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#65292;&#23558;&#22806;&#37096;&#30693;&#35782;&#34701;&#20837;transformer&#27169;&#22411;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;Creative Work&#21644;Group&#31867;&#21035;&#19978;&#33719;&#24471;&#20102;&#26368;&#22823;&#22686;&#30410;&#65292;&#21363;&#20351;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the findings of SemEval-2023 Task 2 on Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the task focused on methods to identify complex fine-grained named entities (like WRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual and multilingual scenarios, as well as noisy settings. The task used the MultiCoNER V2 dataset, composed of 2.2 million instances in Bangla, Chinese, English, Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, and Ukrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. It attracted 842 submissions from 47 teams, and 34 teams submitted system papers. Results showed that complex entity types such as media titles and product names were the most challenging. Methods fusing external knowledge into transformer models achieved the best performance, and the largest gains were on the Creative Work and Group classes, which are still challenging even with external kn
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;</title><link>http://arxiv.org/abs/2305.04835</link><description>&lt;p&gt;
&#22914;&#20309;&#24433;&#21709;&#19978;&#19979;&#25991;&#33539;&#20363;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Do In-Context Examples Affect Compositional Generalization?. (arXiv:2305.04835v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CoFe&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#29702;&#35299;&#30475;&#19981;&#35265;&#30340;&#24050;&#30693;&#21407;&#22987;&#32452;&#21512;&#8212;&#8212;&#26159;&#20154;&#31867;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;AI&#31038;&#21306;&#20027;&#35201;&#36890;&#36807;&#22312;&#35768;&#22810;&#35757;&#32451;&#26679;&#26412;&#19978;&#24494;&#35843;&#31070;&#32463;&#32593;&#32476;&#26469;&#30740;&#31350;&#36825;&#31181;&#33021;&#21147;&#65292;&#28982;&#32780;&#36824;&#19981;&#28165;&#26970;&#19978;&#19979;&#25991;&#23398;&#20064;&#8212;&#8212;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#35201;&#23569;&#26679;&#26412;&#33539;&#24335;&#8212;&#8212;&#26159;&#21542;&#23637;&#31034;&#32452;&#21512;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CoFe&#65292;&#19968;&#20010;&#27979;&#35797;&#22871;&#20214;&#26469;&#35843;&#26597;&#19978;&#19979;&#25991;&#32452;&#21512;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#32452;&#21512;&#27867;&#21270;&#24615;&#33021;&#24456;&#23481;&#26131;&#21463;&#21040;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#30740;&#31350;&#38382;&#39064;&#65306;&#20160;&#20040;&#26159;&#22312;&#32452;&#21512;&#27867;&#21270;&#20013;&#21046;&#20316;&#22909;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#20010;&#28508;&#22312;&#22240;&#32032;&#65306;&#30456;&#20284;&#24615;&#12289;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#32452;&#21512;&#36890;&#29992;&#24615;&#20013;&#65292;&#19978;&#19979;&#25991;&#31034;&#20363;&#24212;&#35813;&#22312;&#32467;&#26500;&#19978;&#19982;&#27979;&#35797;&#29992;&#20363;&#31867;&#20284;&#65292;&#30456;&#20114;&#20043;&#38388;&#24212;&#35813;&#19981;&#21516;&#65292;&#32780;&#19988;&#21333;&#29420;&#22320;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compositional generalization--understanding unseen combinations of seen primitives--is an essential reasoning capability in human intelligence. The AI community mainly studies this capability by fine-tuning neural networks on lots of training samples, while it is still unclear whether and how in-context learning--the prevailing few-shot paradigm based on large language models--exhibits compositional generalization. In this paper, we present CoFe, a test suite to investigate in-context compositional generalization. We find that the compositional generalization performance can be easily affected by the selection of in-context examples, thus raising the research question what the key factors are to make good in-context examples for compositional generalization. We study three potential factors: similarity, diversity and complexity. Our systematic experiments indicate that in-context examples should be structurally similar to the test case, diverse from each other, and individually simple.
&lt;/p&gt;</description></item><item><title>DEnsity &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#35780;&#20272;&#26032;&#26041;&#27861;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#35780;&#20272;&#21709;&#24212;&#21487;&#33021;&#24615;&#26469;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.04720</link><description>&lt;p&gt;
DEnsity: &#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#35780;&#20272;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
DEnsity: Open-domain Dialogue Evaluation Metric using Density Estimation. (arXiv:2305.04720v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04720
&lt;/p&gt;
&lt;p&gt;
DEnsity &#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#30340;&#24320;&#25918;&#22495;&#23545;&#35805;&#35780;&#20272;&#26032;&#26041;&#27861;&#65292;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#35780;&#20272;&#21709;&#24212;&#21487;&#33021;&#24615;&#26469;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#26500;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#35780;&#20272;&#24230;&#37327;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#31867;&#27169;&#22411;&#30340;&#21487;&#23398;&#20064;&#24230;&#37327;&#65292;&#23427;&#20204;&#34987;&#35757;&#32451;&#29992;&#20110;&#21306;&#20998;&#27491;&#30830;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#20998;&#31867;&#22120;&#23545;&#20110;&#26469;&#33258;&#26410;&#35265;&#20998;&#24067;&#30340;&#26679;&#26412;&#20250;&#20570;&#20986;&#36807;&#20110;&#33258;&#20449;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; DEsity&#65292;&#21033;&#29992;&#31070;&#32463;&#20998;&#31867;&#22120;&#20174;&#29305;&#24449;&#31354;&#38388;&#27966;&#29983;&#29305;&#24449;&#65292;&#24182;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#26469;&#35780;&#20272;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#24230;&#37327;&#22120;&#27979;&#37327;&#21709;&#24212;&#22312;&#20154;&#31867;&#23545;&#35805;&#20998;&#24067;&#20013;&#20986;&#29616;&#30340;&#21487;&#33021;&#24615;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25552;&#39640; DEnsity &#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#19968;&#27493;&#21387;&#32553;&#20102;&#29305;&#24449;&#31354;&#38388;&#12290;&#22810;&#20010;&#21709;&#24212;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DEnsity &#19982;&#29616;&#26377;&#24230;&#37327;&#22120;&#30456;&#27604;&#26356;&#22909;&#22320;&#19982;&#20154;&#31867;&#35780;&#20272;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312; https://github.com/ddehun/DEnsity &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent advances in open-domain dialogue systems, building a reliable evaluation metric is still a challenging problem. Recent studies proposed learnable metrics based on classification models trained to distinguish the correct response. However, neural classifiers are known to make overly confident predictions for examples from unseen distributions. We propose DEnsity, which evaluates a response by utilizing density estimation on the feature space derived from a neural classifier. Our metric measures how likely a response would appear in the distribution of human conversations. Moreover, to improve the performance of DEnsity, we utilize contrastive learning to further compress the feature space. Experiments on multiple response evaluation datasets show that DEnsity correlates better with human evaluations than the existing metrics. Our code is available at https://github.com/ddehun/DEnsity.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RE-KBQA&#65292;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20851;&#31995;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#30417;&#30563;&#12290;&#22312;&#19977;&#20010;&#26041;&#38754;&#25506;&#32034;&#20851;&#31995;&#25351;&#23548;&#65292;&#21253;&#25324;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#12289;&#25506;&#32034;&#39069;&#22806;&#30417;&#30563;&#20197;&#21450;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#20851;&#31995;&#25351;&#23548;&#30340;&#37325;&#25490;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02118</link><description>&lt;p&gt;
&#20851;&#27880;&#20851;&#31995;&#25506;&#32034;&#65292;&#25552;&#21319;&#30693;&#35782;&#24211;&#38382;&#31572;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Pay More Attention to Relation Exploration for Knowledge Base Question Answering. (arXiv:2305.02118v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02118
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RE-KBQA&#65292;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20851;&#31995;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#30417;&#30563;&#12290;&#22312;&#19977;&#20010;&#26041;&#38754;&#25506;&#32034;&#20851;&#31995;&#25351;&#23548;&#65292;&#21253;&#25324;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#12289;&#25506;&#32034;&#39069;&#22806;&#30417;&#30563;&#20197;&#21450;&#36827;&#34892;&#21518;&#22788;&#29702;&#30340;&#22522;&#20110;&#20851;&#31995;&#25351;&#23548;&#30340;&#37325;&#25490;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#20013;&#26816;&#32034;&#27491;&#30830;&#31572;&#26696;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#23454;&#20307;&#34920;&#31034;&#21644;&#26368;&#32456;&#31572;&#26696;&#25512;&#29702;&#65292;&#23548;&#33268;&#23545;&#27492;&#20219;&#21153;&#30340;&#38480;&#21046;&#24615;&#30417;&#30563;&#12290;&#27492;&#22806;&#65292;&#20851;&#31995;&#22312;&#26368;&#36817;&#30340;&#25216;&#26415;&#20013;&#24182;&#26410;&#34987;&#20805;&#20998;&#32771;&#34385;&#65292;&#32780;&#20851;&#31995;&#23454;&#38469;&#19978;&#20915;&#23450;&#20102;&#25512;&#29702;&#36335;&#24452;&#30340;&#36873;&#25321;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;RE-KBQA&#65292;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20851;&#31995;&#22686;&#24378;&#23454;&#20307;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#20174;&#19977;&#20010;&#26041;&#38754;&#25506;&#32034;&#20851;&#31995;&#25351;&#23548;&#65292;&#21253;&#25324;&#65288;1&#65289;&#36890;&#36807;&#20351;&#29992;&#21464;&#20998;&#22270;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#20851;&#31995;&#37325;&#35201;&#24615;&#26469;&#21306;&#20998;&#30456;&#20284;&#23454;&#20307;&#65307;&#65288;2&#65289;&#36890;&#36807;&#39044;&#27979;&#20851;&#31995;&#20998;&#24067;&#20316;&#20026;&#36719;&#26631;&#31614;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#26041;&#26696;&#25506;&#32034;&#39069;&#22806;&#30417;&#30563;&#65307;&#65288;3&#65289;&#35774;&#35745;&#22522;&#20110;&#20851;&#31995;&#25351;&#23548;&#30340;&#37325;&#25490;&#31639;&#27861;&#36827;&#34892;&#21518;&#22788;&#29702;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge base question answering (KBQA) is a challenging task that aims to retrieve correct answers from large-scale knowledge bases. Existing attempts primarily focus on entity representation and final answer reasoning, which results in limited supervision for this task. Moreover, the relations, which empirically determine the reasoning path selection, are not fully considered in recent advancements. In this study, we propose a novel framework, RE-KBQA, that utilizes relations in the knowledge base to enhance entity representation and introduce additional supervision. We explore guidance from relations in three aspects, including (1) distinguishing similar entities by employing a variational graph auto-encoder to learn relation importance; (2) exploring extra supervision by predicting relation distributions as soft labels with a multi-task scheme; (3) designing a relation-guided re-ranking algorithm for post-processing. Experimental results on two benchmark datasets demonstrate the e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.01901</link><description>&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#65306;&#32463;&#39564;&#30740;&#31350;&#21644;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Few-shot Event Detection: An Empirical Study and a Unified View. (arXiv:2305.01901v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#20986;&#21457;&#65292;&#20998;&#26512;&#27604;&#36739;&#20102;&#21313;&#31181;&#20195;&#34920;&#24615;&#30340;&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#24402;&#32435;&#24635;&#32467;&#20986;&#20102;&#21407;&#22411;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#36234;&#24615;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#20107;&#20214;&#26816;&#27979; (ED) &#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#36825;&#20063;&#24102;&#26469;&#20102;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#20363;&#22914;&#21508;&#31181;&#21160;&#26426;&#12289;&#20219;&#21153;&#21644;&#23454;&#39564;&#35774;&#32622;&#65292;&#36825;&#20123;&#24046;&#24322;&#22952;&#30861;&#20102;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#26410;&#26469;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#24443;&#24213;&#30340;&#32463;&#39564;&#30740;&#31350;&#12289;&#19968;&#20010;ED&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#21644;&#19968;&#20010;&#26356;&#22909;&#30340;&#32479;&#19968;&#22522;&#20934;&#32447;&#12290;&#20026;&#20102;&#20844;&#24179;&#35780;&#20272;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#20004;&#20010;&#23454;&#29992;&#30340;&#35774;&#32622;&#65306;&#20302;&#36164;&#28304;&#35774;&#32622;&#26469;&#35780;&#20272;&#27867;&#21270;&#33021;&#21147;&#21644;&#31867;&#36716;&#31227;&#35774;&#32622;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#21313;&#31181;&#20195;&#34920;&#24615;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22823;&#33268;&#34987;&#20998;&#20026;&#22522;&#20110;&#25552;&#31034;&#21644;&#22522;&#20110;&#21407;&#22411;&#30340;&#27169;&#22411;&#36827;&#34892;&#35814;&#32454;&#20998;&#26512;&#12290;&#20026;&#20102;&#35843;&#26597;&#22522;&#20110;&#21407;&#22411;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20998;&#35299;&#20102;&#35774;&#35745;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#19981;&#20165;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#22312;&#20302;&#36164;&#28304;&#35774;&#32622;&#19979;&#33719;&#24471;2.7&#65285;F1&#25910;&#30410;&#65289;&#65292;&#32780;&#19988;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot event detection (ED) has been widely studied, while this brings noticeable discrepancies, e.g., various motivations, tasks, and experimental settings, that hinder the understanding of models for future progress. This paper presents a thorough empirical study, a unified view of ED models, and a better unified baseline. For fair evaluation, we choose two practical settings: low-resource setting to assess generalization ability and class-transfer setting for transferability. We compare ten representative methods on three datasets, which are roughly grouped into prompt-based and prototype-based models for detailed analysis. To investigate the superior performance of prototype-based methods, we break down the design and build a unified framework. Based on that, we not only propose a simple yet effective method (e.g., 2.7% F1 gains under low-resource setting) but also offer many valuable research insights for future research.
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#21457;&#29616;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#22343;&#23384;&#22312;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#36825;&#19968;&#30740;&#31350;&#22635;&#34917;&#20102;&#30740;&#31350;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.05783</link><description>&lt;p&gt;
&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#37327;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Measuring Gender Bias in West Slavic Language Models. (arXiv:2304.05783v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#65292;&#21457;&#29616;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#22343;&#23384;&#22312;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#36825;&#19968;&#30740;&#31350;&#22635;&#34917;&#20102;&#30740;&#31350;&#38750;&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;&#24615;&#21035;&#20559;&#35265;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#20250;&#23558;&#22522;&#30784;&#25968;&#25454;&#38598;&#20013;&#30340;&#20559;&#35265;&#24310;&#32493;&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#22823;&#22810;&#22522;&#20110;&#33521;&#35821;&#30340;&#21333;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#38024;&#23545;&#25193;&#23637;&#21040;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#30340;&#30740;&#31350;&#24456;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#24046;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#27425;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#26495;&#30340;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#25463;&#20811;&#35821;&#12289;&#27874;&#20848;&#35821;&#21644;&#26031;&#27931;&#20240;&#20811;&#35821;&#65289;&#65292;&#20197;&#27979;&#37327;&#38024;&#23545;&#30007;&#24615;&#12289;&#22899;&#24615;&#21644;&#38750;&#20108;&#36827;&#21046;&#20027;&#20307;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#20351;&#29992;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#27169;&#22411;&#26469;&#23436;&#25104;&#36825;&#20123;&#21477;&#23376;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#26159;&#21542;&#36866;&#21512;&#20110;&#34987;&#36974;&#30422;&#30340;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#37327;&#21270;&#29983;&#25104;&#21333;&#35789;&#30340;&#26377;&#27602;&#24615;&#21644;&#24615;&#21035;&#29305;&#24449;&#26469;&#27979;&#37327;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#35821;&#21477;&#20250;&#22240;&#20027;&#20307;&#30340;&#24615;&#21035;&#32780;&#20135;&#29983;&#20260;&#23475;&#24615;&#30340;&#23436;&#25104;&#24230;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25463;&#20811;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#21644;&#27874;&#20848;&#35821;&#22343;&#26174;&#31034;&#20986;&#30456;&#20284;&#31243;&#24230;&#30340;&#24615;&#21035;&#20559;&#35265;&#12290;&#26412;&#30740;&#31350;&#23545;&#20110;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#20559;&#35265;&#30340;&#26085;&#30410;&#22686;&#38271;&#30340;&#30740;&#31350;&#20307;&#31995;&#20570;&#20986;&#36129;&#29486;&#65292;&#24182;&#20026;&#35780;&#20272;&#21644;&#20943;&#23569;&#35199;&#26031;&#25289;&#22827;&#35821;&#35328;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models have been known to perpetuate biases from the underlying datasets to downstream tasks. However, these findings are predominantly based on monolingual language models for English, whereas there are few investigative studies of biases encoded in language models for languages beyond English. In this paper, we fill this gap by analysing gender bias in West Slavic language models. We introduce the first template-based dataset in Czech, Polish, and Slovak for measuring gender bias towards male, female and non-binary subjects. We complete the sentences using both mono- and multilingual language models and assess their suitability for the masked language modelling objective. Next, we measure gender bias encoded in West Slavic language models by quantifying the toxicity and genderness of the generated words. We find that these language models produce hurtful completions that depend on the subject's gender. Perhaps surprisingly, Czech, Slovak, and Polish language mode
&lt;/p&gt;</description></item><item><title>LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.00457</link><description>&lt;p&gt;
LLMMaps&#8212;&#8212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#23618;&#35780;&#20215;&#30340;&#21487;&#35270;&#21270;&#38544;&#21947;
&lt;/p&gt;
&lt;p&gt;
LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00457
&lt;/p&gt;
&lt;p&gt;
LLMMaps&#26159;&#19968;&#31181;&#20998;&#23618;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25581;&#31034;&#21462;&#24471;&#39640;&#20934;&#30830;&#24230;&#21644;&#20135;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#25351;&#23548;&#27169;&#22411;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23481;&#26131;&#20135;&#29983;&#24187;&#35273;&#65292;&#21363;&#27169;&#22411;&#22312;&#21709;&#24212;&#20013;&#26292;&#38706;&#20986;&#19981;&#27491;&#30830;&#25110;&#38169;&#35823;&#30340;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#24517;&#39035;&#37319;&#29992;&#21220;&#22859;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#34429;&#28982;LLM&#22312;&#29305;&#23450;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#36890;&#24120;&#26159;&#22522;&#20110;&#38382;&#31572;(Q&amp;A)&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#36825;&#20123;&#35780;&#20272;&#36890;&#24120;&#20165;&#25253;&#21578;&#25972;&#20010;&#39046;&#22495;&#30340;&#21333;&#20010;&#20934;&#30830;&#24230;&#25968;&#23383;&#65292;&#36825;&#19968;&#31243;&#24207;&#22312;&#36879;&#26126;&#24230;&#21644;&#27169;&#22411;&#25913;&#36827;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#20998;&#23618;&#35780;&#20272;&#21487;&#20197;&#25581;&#31034;&#21487;&#33021;&#26356;&#23481;&#26131;&#21457;&#29983;&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#35780;&#20272;LLMs&#30340;&#39118;&#38505;&#24182;&#25351;&#23548;&#23427;&#20204;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#25903;&#25345;&#36825;&#26679;&#30340;&#20998;&#23618;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLMMaps&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;Q&amp;A&#25968;&#25454;&#38598;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#12290;LLMMaps&#25552;&#20379;&#20102;&#23545;LLMs&#22312;&#19981;&#21516;&#23376;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#20998;&#24067;&#30340;&#35814;&#32454;&#27934;&#23519;&#65292;&#20801;&#35768;&#29992;&#25143;&#25918;&#22823;&#39046;&#22495;&#30340;&#29305;&#23450;&#37096;&#20998;&#24182;&#25506;&#32034;&#27169;&#22411;&#24615;&#33021;&#19978;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;LLMMaps&#26377;&#21161;&#20110;&#35782;&#21035;&#20986;&#26356;&#23481;&#26131;&#20986;&#29616;LLM&#24187;&#35273;&#30340;&#23376;&#39046;&#22495;&#65292;&#24182;&#21487;&#20197;&#25351;&#23548;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20197;&#25913;&#21892;&#36825;&#20123;&#39046;&#22495;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized natural language processing and demonstrated impressive capabilities in various tasks. Unfortunately, they are prone to hallucinations, where the model exposes incorrect or false information in its responses, which renders diligent evaluation approaches mandatory. While LLM performance in specific knowledge fields is often evaluated based on question and answer (Q&amp;A) datasets, such evaluations usually report only a single accuracy number for the entire field, a procedure which is problematic with respect to transparency and model improvement. A stratified evaluation could instead reveal subfields, where hallucinations are more likely to occur and thus help to better assess LLMs' risks and guide their further development. To support such stratified evaluations, we propose LLMMaps as a novel visualization technique that enables users to evaluate LLMs' performance with respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs' kn
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.08624</link><description>&lt;p&gt;
InstructABSA: &#22522;&#20110;&#25351;&#20196;&#23398;&#20064;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis. (arXiv:2302.08624v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08624
&lt;/p&gt;
&lt;p&gt;
InstructABSA&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#30340;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;Aspect Term Extraction&#12289;Aspect Term Sentiment Classification&#12289;&#21644;Joint Task subtasks&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#36229;&#36807;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;InstructABSA&#65292;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#23398;&#20064;&#33539;&#24335;&#36827;&#34892;Aspect Based Sentiment Analysis (ABSA) &#25152;&#26377;&#23376;&#20219;&#21153;&#65288;Aspect Term Extraction (ATE)&#65292;Aspect Term Sentiment Classification (ATSC)&#65292;&#20197;&#21450;Joint Task modeling&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#27599;&#20010;&#35757;&#32451;&#26679;&#26412;&#24341;&#20837;&#20102;&#27491;&#38754;&#12289;&#36127;&#38754;&#12289;&#21644;&#20013;&#24615;&#30340;&#20363;&#23376;&#65292;&#24182;&#20351;&#29992;&#25351;&#20196;&#26469;&#35843;&#25972;&#27599;&#20010;ABSA&#23376;&#20219;&#21153;&#30340;&#27169;&#22411;&#65288;Tk-Instruct&#65289;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#22312;Sem Eval 2014&#12289;2015&#21644;2016&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#19977;&#20010;ABSA&#23376;&#20219;&#21153;&#65288;ATE&#12289;ATSC&#21644;Joint Task&#65289;&#19978;&#65292;InstructABSA&#22312;&#24615;&#33021;&#19978;&#37117;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65288;SOTA&#65289;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#34920;&#29616;&#36229;&#36807;&#20102;7&#20493;&#22823;&#30340;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;Rest14 ATE&#23376;&#20219;&#21153;&#19978;&#65292;InstructABSA&#36229;&#36807;&#20102;SOTA 7.31%&#30340;&#24471;&#20998;&#65292;Rest15 ATSC&#23376;&#20219;&#21153;&#19978;&#20063;&#26377;&#25552;&#21319;&#65292;&#24182;&#19988;&#22312;Lapt14 Joint Task&#19978;&#30340;&#34920;&#29616;&#25552;&#21319;&#20102;8.63%&#28857;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36824;&#34920;&#26126;&#65292;&#23545;&#20110;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;InstructABSA&#20855;&#26377;&#24378;&#22823;&#30340;&#26032;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present InstructABSA, Aspect Based Sentiment Analysis (ABSA) using the instruction learning paradigm for all ABSA subtasks: Aspect Term Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and Joint Task modeling. Our method introduces positive, negative, and neutral examples to each training sample, and instruction tunes the model (Tk-Instruct) for each ABSA subtask, yielding significant performance improvements. Experimental results on the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSA outperforms the previous state-of-the-art (SOTA) approaches on all three ABSA subtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7x larger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATE subtask by 7.31% points, Rest15 ATSC subtask by and on the Lapt14 Joint Task by 8.63% points. Our results also suggest a strong generalization ability to new domains across all three subtasks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65306;&#33258;&#21160;&#35782;&#21035;&#21465;&#36848;&#20013;&#30340;&#26032;&#20107;&#20214;&#65292;&#20197;&#35782;&#21035;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#20182;&#20204;&#23558;&#20107;&#20214;&#23450;&#20041;&#20026;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#30340;&#19977;&#20803;&#32452;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#26032;&#20107;&#20214;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20854;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#26469;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.07748</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#26032;&#20107;&#20214;&#65311;&#22312;&#21465;&#20107;&#20013;&#35782;&#21035;&#26032;&#20107;&#20214;&#30340;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65306;&#33258;&#21160;&#35782;&#21035;&#21465;&#36848;&#20013;&#30340;&#26032;&#20107;&#20214;&#65292;&#20197;&#35782;&#21035;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#20182;&#20204;&#23558;&#20107;&#20214;&#23450;&#20041;&#20026;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#30340;&#19977;&#20803;&#32452;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#26032;&#20107;&#20214;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20854;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#26469;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#21253;&#21547;&#20102;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#30340;&#20016;&#23500;&#20107;&#20214;&#36164;&#28304;&#12290;&#23545;&#36825;&#20123;&#20107;&#20214;&#30340;&#33258;&#21160;&#29702;&#35299;&#25552;&#20379;&#20102;&#25688;&#35201;&#29702;&#35299;&#65292;&#20197;&#20379;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;(&#22914;&#25512;&#29702;)&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20107;&#20214;&#30340;&#20449;&#24687;&#29366;&#24577;(IS)&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;:&#33258;&#21160;&#35782;&#21035;&#21465;&#36848;&#20013;&#30340;&#26032;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#20107;&#20214;&#23450;&#20041;&#20026;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#30340;&#19977;&#20803;&#32452;&#12290;&#35813;&#20107;&#20214;&#30456;&#23545;&#20110;&#35805;&#35821;&#19978;&#19979;&#25991;&#34987;&#24402;&#31867;&#20026;&#26032;&#20107;&#20214;&#65292;&#24182;&#21462;&#20915;&#20110;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#26469;&#25512;&#23548;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#26631;&#27880;&#32773;&#22312;&#20844;&#24320;&#30340;&#21465;&#36848;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#21035;&#30340;&#26032;&#20107;&#20214;&#26631;&#27880;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26631;&#27880;&#21327;&#35758;&#65292;&#24182;&#30740;&#31350;&#20102;&#27880;&#37322;&#30340;&#36136;&#37327;&#21644;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#27880;&#26448;&#26009;&#21644;&#29992;&#20110;&#21465;&#36848;&#29702;&#35299;&#20013;&#26032;&#20107;&#20214;&#25552;&#21462;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narratives include a rich source of events unfolding over time and context. Automatic understanding of these events provides a summarised comprehension of the narrative for further computation (such as reasoning). In this paper, we study the Information Status (IS) of the events and propose a novel challenging task: the automatic identification of \textit{new} events in a narrative. We define an event as a triplet of subject, predicate, and object. The event is categorized as new with respect to the discourse context and whether it can be inferred through commonsense reasoning. We annotated a publicly available corpus of narratives with the new events at sentence level using human annotators. We present the annotation protocol and study the quality of the annotation and the difficulty of the task. We publish the annotated dataset, annotation materials, and machine learning baseline models for the task of new event extraction for narrative understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#19982;&#20998;&#31867;&#20102;Transformer&#27169;&#22411;&#31995;&#21015;&#20013;&#26368;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20154;&#31867;&#21442;&#19982;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#20013;&#21019;&#26032;&#24615;&#30340;&#26041;&#38754;&#20570;&#20102;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2302.07730</link><description>&lt;p&gt;
Transformer&#27169;&#22411;&#65306;&#20171;&#32461;&#19982;&#30446;&#24405;
&lt;/p&gt;
&lt;p&gt;
Transformer models: an introduction and catalog. (arXiv:2302.07730v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#19982;&#20998;&#31867;&#20102;Transformer&#27169;&#22411;&#31995;&#21015;&#20013;&#26368;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#20154;&#31867;&#21442;&#19982;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#20854;&#20013;&#21019;&#26032;&#24615;&#30340;&#26041;&#38754;&#20570;&#20102;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#31995;&#21015;&#30340;&#22522;&#30784;&#27169;&#22411;&#22914;&#38632;&#21518;&#26149;&#31499;&#33324;&#28044;&#29616;&#20986;&#26469;&#65292;&#23427;&#20204;&#20013;&#26377;&#20123;&#20855;&#26377;&#20196;&#20154;&#38590;&#24536;&#30340;&#12289;&#26377;&#26102;&#29978;&#33267;&#28369;&#31293;&#26377;&#36259;&#20294;&#21364;&#19981;&#20855;&#33258;&#35299;&#37322;&#24615;&#30340;&#21517;&#31216;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#30456;&#23545;&#20840;&#38754;&#20294;&#31616;&#21333;&#30340;Transformer&#27169;&#22411;&#30446;&#24405;&#21644;&#20998;&#31867;&#65292;&#24182;&#20171;&#32461;Transformer&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#30340;&#26041;&#38754;&#21644;&#21019;&#26032;&#12290;&#30446;&#24405;&#20013;&#30340;&#27169;&#22411;&#21253;&#25324;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65288;&#20363;&#22914;BERT&#25110;GPT3&#65289;&#30340;&#27169;&#22411;&#65292;&#20197;&#21450;&#36827;&#19968;&#27493;&#36890;&#36807;&#20154;&#31867;&#21442;&#19982;&#35757;&#32451;&#65288;&#20363;&#22914;ChatGPT&#20351;&#29992;&#30340;InstructGPT&#27169;&#22411;&#65289;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past few years we have seen the meteoric appearance of dozens of foundation models of the Transformer family, all of which have memorable and sometimes funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovations in Transformer models. Our catalog will include models that are trained using self-supervised learning (e.g., BERT or GPT3) as well as those that are further trained using a human-in-the-loop (e.g. the InstructGPT model used by ChatGPT).
&lt;/p&gt;</description></item><item><title>READIN&#26159;&#19968;&#20010;&#20013;&#25991;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#21547;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#22122;&#22768;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#27880;&#37322;&#31649;&#36947;&#34987;&#35774;&#35745;&#26469;&#26368;&#22823;&#21270;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.07324</link><description>&lt;p&gt;
READIN&#65306;&#19968;&#20010;&#24102;&#26377;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#36755;&#20837;&#22122;&#22768;&#30340;&#20013;&#25991;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
READIN: A Chinese Multi-Task Benchmark with Realistic and Diverse Input Noises. (arXiv:2302.07324v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07324
&lt;/p&gt;
&lt;p&gt;
READIN&#26159;&#19968;&#20010;&#20013;&#25991;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#23427;&#21253;&#21547;&#30495;&#23454;&#21644;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#22122;&#22768;&#65292;&#26088;&#22312;&#27979;&#35797;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#27880;&#37322;&#31649;&#36947;&#34987;&#35774;&#35745;&#26469;&#26368;&#22823;&#21270;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#65292;&#29992;&#25143;&#29983;&#25104;&#30340;&#36755;&#20837;&#36890;&#24120;&#21253;&#21547;&#30001;&#20110;&#35821;&#35328;&#21464;&#20307;&#25110;&#25171;&#23383;&#38169;&#35823;&#24341;&#36215;&#30340;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#31561;&#21508;&#31181;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#27979;&#35797;&#27169;&#22411;&#22312;&#24102;&#26377;&#30495;&#23454;&#36755;&#20837;&#22122;&#22768;&#30340;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#20197;&#30830;&#20445;&#20854;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#25991;&#20013;&#26500;&#24314;&#36825;&#26679;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#30740;&#31350;&#24456;&#23569;&#65292;&#32780;&#23454;&#38469;&#24773;&#20917;&#20013;&#21508;&#31181;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#36755;&#20837;&#22122;&#22768;&#23649;&#35265;&#19981;&#40092;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#37325;&#35201;&#31354;&#30333;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;READIN&#30340;&#20013;&#25991;&#22810;&#20219;&#21153;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#24182;&#35201;&#27714;&#27880;&#37322;&#32773;&#20351;&#29992;&#20004;&#31181;&#24120;&#29992;&#30340;&#20013;&#25991;&#36755;&#20837;&#26041;&#27861;&#8212;&#8212;&#25340;&#38899;&#36755;&#20837;&#21644;&#35821;&#38899;&#36755;&#20837;&#65292;&#37325;&#26032;&#36755;&#20837;&#21407;&#22987;&#27979;&#35797;&#25968;&#25454;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#27880;&#37322;&#31649;&#36947;&#20197;&#26368;&#22823;&#21270;&#22810;&#26679;&#24615;&#65292;&#20363;&#22914;&#36890;&#36807;&#25351;&#31034;&#27880;&#37322;&#32773;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#36755;&#20837;&#27861;&#32534;&#36753;&#22120;&#65288;IME&#65289;&#26469;&#33719;&#24471;&#38190;&#30424;&#22122;&#22768;&#65292;&#24182;&#25307;&#21215;&#26469;&#33258;&#19981;&#21516;&#26041;&#35328;&#32452;&#30340;&#28436;&#35762;&#32773;&#33719;&#21462;&#35821;&#38899;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
For many real-world applications, the user-generated inputs usually contain various noises due to speech recognition errors caused by linguistic variations1 or typographical errors (typos). Thus, it is crucial to test model performance on data with realistic input noises to ensure robustness and fairness. However, little study has been done to construct such benchmarks for Chinese, where various language-specific input noises happen in the real world. In order to fill this important gap, we construct READIN: a Chinese multi-task benchmark with REalistic And Diverse Input Noises. READIN contains four diverse tasks and requests annotators to re-enter the original test data with two commonly used Chinese input methods: Pinyin input and speech input. We designed our annotation pipeline to maximize diversity, for example by instructing the annotators to use diverse input method editors (IMEs) for keyboard noises and recruiting speakers from diverse dialectical groups for speech noises. We e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.09656</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#35299;&#37322;&#65306;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#23545;&#40784;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Selective Explanations: Leveraging Human Input to Align Explainable AI. (arXiv:2301.09656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#20154;&#31867;&#36755;&#20837;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#22312;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#22823;&#37327;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31639;&#27861;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#22240;&#19982;&#20154;&#31867;&#35299;&#37322;&#30340;&#29983;&#20135;&#21644;&#28040;&#36153;&#26041;&#24335;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#32780;&#21463;&#21040;&#25209;&#35780;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;XAI&#25216;&#26415;&#24448;&#24448;&#38590;&#20197;&#20351;&#29992;&#24182;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#20351;AI&#35299;&#37322;&#20855;&#26377;&#36873;&#25321;&#24615;&#65288;&#36825;&#26159;&#20154;&#31867;&#35299;&#37322;&#30340;&#22522;&#26412;&#23646;&#24615;&#20043;&#19968;&#65289;&#26469;&#24357;&#21512;&#36825;&#20123;&#24046;&#36317;&#65292;&#36890;&#36807;&#26681;&#25454;&#25509;&#25910;&#26041;&#30340;&#20559;&#22909;&#26377;&#36873;&#25321;&#24615;&#22320;&#21576;&#29616;&#22823;&#37327;&#27169;&#22411;&#21407;&#22240;&#30340;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#26679;&#26412;&#19978;&#30340;&#20154;&#31867;&#36755;&#20837;&#26469;&#29983;&#25104;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#24320;&#36767;&#20102;&#19968;&#20010;&#20016;&#23500;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#36873;&#25321;&#24615;&#30446;&#26631;&#12289;&#36755;&#20837;&#31867;&#22411;&#31561;&#12290;&#20316;&#20026;&#19968;&#20010;&#23637;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#20915;&#31574;&#25903;&#25345;&#20219;&#21153;&#26469;&#25506;&#32034;&#22522;&#20110;&#20915;&#31574;&#32773;&#35748;&#20026;&#30456;&#20851;&#30340;&#36873;&#25321;&#24615;&#35299;&#37322;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#39033;&#23454;&#39564;&#30740;&#31350;&#65292;&#20197;&#26816;&#26597;&#20174;&#22823;&#19968;&#32452;&#27169;&#22411;&#21407;&#22240;&#20013;&#36873;&#25321;&#30340;&#19977;&#20010;&#23376;&#38598;&#19982;&#26410;&#36873;&#25321;&#30340;&#23376;&#38598;&#30456;&#27604;&#65292;&#36873;&#25321;&#24615;&#35299;&#37322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a vast collection of explainable AI (XAI) algorithms have been developed in recent years, they are often criticized for significant gaps with how humans produce and consume explanations. As a result, current XAI techniques are often found to be hard to use and lack effectiveness. In this work, we attempt to close these gaps by making AI explanations selective -- a fundamental property of human explanations -- by selectively presenting a subset from a large set of model reasons based on what aligns with the recipient's preferences. We propose a general framework for generating selective explanations by leveraging human input on a small sample. This framework opens up a rich design space that accounts for different selectivity goals, types of input, and more. As a showcase, we use a decision-support task to explore selective explanations based on what the decision-maker would consider relevant to the decision task. We conducted two experimental studies to examine three out of a bro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.10823</link><description>&lt;p&gt;
&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Continual Contrastive Finetuning Improves Low-Resource Relation Extraction. (arXiv:2212.10823v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#23545;&#27604;&#24494;&#35843;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#20851;&#31995;&#25552;&#21462;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#65292;&#20197;&#21450;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#26469;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#30340;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#65288;RE&#65289;&#20381;&#36182;&#32467;&#26500;&#21270;&#27880;&#37322;&#35821;&#26009;&#24211;&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#65292;&#35813;&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#26399;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#35299;&#20915;&#20302;&#36164;&#28304;&#30340;RE&#65292;&#20854;&#20013;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#36890;&#36807;RE&#30446;&#26631;&#39044;&#35757;&#32451;&#20851;&#31995;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#20998;&#31867;&#20026;&#22522;&#30784;&#30340;&#30446;&#26631;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#23427;&#38459;&#27490;RE&#27169;&#22411;&#20805;&#20998;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#30693;&#35782;&#12290;&#26412;&#25991;&#26088;&#22312;&#24357;&#21512;&#24046;&#36317;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#19968;&#33268;&#30340;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;RE&#27169;&#22411;&#12290;&#30001;&#20110;&#22312;&#36825;&#31181;&#34920;&#31034;&#23398;&#20064;&#33539;&#24335;&#20013;&#65292;&#19968;&#20010;&#20851;&#31995;&#21487;&#33021;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36731;&#26494;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#65292;&#22240;&#27492;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22810;&#20013;&#24515;&#23545;&#27604;&#25439;&#22833;&#65292;&#20801;&#35768;&#19968;&#20010;&#20851;&#31995;&#24418;&#25104;&#22810;&#20010;&#32858;&#31867;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#25991;&#26723;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#21644;&#39046;&#22495;&#20013;&#26174;&#30528;&#25552;&#39640;&#20851;&#31995;&#25552;&#21462;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE), which has relied on structurally annotated corpora for model training, has been particularly challenging in low-resource scenarios and domains. Recent literature has tackled low-resource RE by self-supervised learning, where the solution involves pretraining the relation embedding by RE-based objective and finetuning on labeled data by classification-based objective. However, a critical challenge to this approach is the gap in objectives, which prevents the RE model from fully utilizing the knowledge in pretrained representations. In this paper, we aim at bridging the gap and propose to pretrain and finetune the RE model using consistent objectives of contrastive learning. Since in this kind of representation learning paradigm, one relation may easily form multiple clusters in the representation space, we further propose a multi-center contrastive loss that allows one relation to form multiple clusters to better align with pretraining. Experiments on two docum
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21629;&#39064;&#32423;&#21035;&#20998;&#21106;&#21644;&#21253;&#21547;&#20851;&#31995;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;PropSegmEnt&#35299;&#20915;&#20102;NLI&#20013;&#23545;&#35821;&#20041;&#21333;&#20803;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10750</link><description>&lt;p&gt;
PropSegmEnt: &#19968;&#20010;&#29992;&#20110;&#21629;&#39064;&#32423;&#21035;&#20998;&#21106;&#21644;&#21253;&#21547;&#20851;&#31995;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
PropSegmEnt: A Large-Scale Corpus for Proposition-Level Segmentation and Entailment Recognition. (arXiv:2212.10750v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10750
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#21629;&#39064;&#32423;&#21035;&#20998;&#21106;&#21644;&#21253;&#21547;&#20851;&#31995;&#35782;&#21035;&#30340;&#35821;&#26009;&#24211;PropSegmEnt&#35299;&#20915;&#20102;NLI&#20013;&#23545;&#35821;&#20041;&#21333;&#20803;&#30340;&#35782;&#21035;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#30340;&#20219;&#21153;&#38656;&#35201;&#35782;&#21035;&#19968;&#20010;&#25991;&#26412;&#26159;&#21542;&#21487;&#20197;&#20174;&#21478;&#19968;&#20010;&#25991;&#26412;&#20013;&#25512;&#26029;&#20986;&#26469;&#65292;&#36890;&#24120;&#22312;&#21477;&#23376;&#25110;&#27573;&#33853;&#32423;&#21035;&#19978;&#23450;&#20041;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#21477;&#23376;&#20063;&#24448;&#24448;&#21253;&#21547;&#22810;&#20010;&#21629;&#39064;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#22312;&#21477;&#23376;&#20013;&#35782;&#21035;&#27599;&#20010;&#21629;&#39064;&#30340;&#25991;&#26412;&#21253;&#21547;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#30340;PropSegmEnt&#25968;&#25454;&#38598;&#21253;&#25324;&#30001;&#19987;&#23478;&#35780;&#20272;&#21592;&#26631;&#27880;&#30340;&#36229;&#36807;45K&#20010;&#21629;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#32467;&#26500;&#31867;&#20284;&#20110;(1)&#23558;&#25991;&#26723;&#20013;&#30340;&#21477;&#23376;&#20998;&#27573;&#20026;&#21629;&#39064;&#30340;&#38598;&#21512;&#65292;&#20197;&#21450;(2)&#30456;&#23545;&#20110;&#19968;&#20010;&#19981;&#21516;&#30340;&#20294;&#19982;&#20027;&#39064;&#23545;&#40784;&#30340;&#20013;&#24515;&#21477;&#23376;&#65292;&#23545;&#27599;&#20010;&#21629;&#39064;&#36827;&#34892;&#21253;&#21547;&#20851;&#31995;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widely studied task of Natural Language Inference (NLI) requires a system to recognize whether one piece of text is textually entailed by another, i.e. whether the entirety of its meaning can be inferred from the other. In current NLI datasets and models, textual entailment relations are typically defined on the sentence- or paragraph-level. However, even a simple sentence often contains multiple propositions, i.e. distinct units of meaning conveyed by the sentence. As these propositions can carry different truth values in the context of a given premise, we argue for the need to recognize the textual entailment relation of each proposition in a sentence individually.  We propose PropSegmEnt, a corpus of over 45K propositions annotated by expert human raters. Our dataset structure resembles the tasks of (1) segmenting sentences within a document to the set of propositions, and (2) classifying the entailment relation of each proposition with respect to a different yet topically-align
&lt;/p&gt;</description></item><item><title>&#31934;&#24515;&#25972;&#29702;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#20197;&#26497;&#22823;&#22320;&#31283;&#23450;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;ICL&#31639;&#27861;&#36827;&#34892;&#20854;&#20182;&#26356;&#25913;&#12290;CondAcc&#36890;&#36807;&#23558;&#35757;&#32451;&#31034;&#20363;&#19982;&#38543;&#26426;&#35757;&#32451;&#31034;&#20363;&#32452;&#21512;&#26102;&#30340;&#24179;&#22343;&#24320;&#21457;&#38598;ICL&#20934;&#30830;&#24615;&#26469;&#35780;&#20998;&#35757;&#32451;&#31034;&#20363;&#65292;&#32780;DataModels&#23398;&#20064;&#32447;&#24615;&#22238;&#24402;&#22120;&#65292;&#20272;&#35745;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;LLM&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2212.10378</link><description>&lt;p&gt;
&#20165;&#36890;&#36807;&#25968;&#25454;&#25972;&#29702;&#21487;&#20197;&#31283;&#23450;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Data Curation Alone Can Stabilize In-context Learning. (arXiv:2212.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10378
&lt;/p&gt;
&lt;p&gt;
&#31934;&#24515;&#25972;&#29702;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#20197;&#26497;&#22823;&#22320;&#31283;&#23450;&#19978;&#19979;&#25991;&#23398;&#20064;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;ICL&#31639;&#27861;&#36827;&#34892;&#20854;&#20182;&#26356;&#25913;&#12290;CondAcc&#36890;&#36807;&#23558;&#35757;&#32451;&#31034;&#20363;&#19982;&#38543;&#26426;&#35757;&#32451;&#31034;&#20363;&#32452;&#21512;&#26102;&#30340;&#24179;&#22343;&#24320;&#21457;&#38598;ICL&#20934;&#30830;&#24615;&#26469;&#35780;&#20998;&#35757;&#32451;&#31034;&#20363;&#65292;&#32780;DataModels&#23398;&#20064;&#32447;&#24615;&#22238;&#24402;&#22120;&#65292;&#20272;&#35745;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;LLM&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#19968;&#31995;&#21015;&#35757;&#32451;&#26679;&#20363;&#21487;&#20197;&#25191;&#34892;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;ICL&#23545;&#35757;&#32451;&#26679;&#20363;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#65306;&#20174;&#35757;&#32451;&#38598;&#20013;&#38543;&#26426;&#25277;&#26679;&#23548;&#33268;&#24615;&#33021;&#39640;&#24230;&#21464;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31934;&#24515;&#25972;&#29702;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#21487;&#20197;&#26497;&#22823;&#22320;&#31283;&#23450;ICL&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;ICL&#31639;&#27861;&#30340;&#20854;&#20182;&#26356;&#25913;&#65288;&#20363;&#22914;&#25552;&#31034;&#26816;&#32034;&#25110;&#26657;&#20934;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#36873;&#25321;&#22521;&#35757;&#23376;&#38598;&#30340;&#26041;&#27861;&#8212;&#8212;&#20004;&#32773;&#37117;&#21333;&#29420;&#35780;&#20998;&#22521;&#35757;&#31034;&#20363;&#65292;&#28982;&#21518;&#36873;&#25321;&#24471;&#20998;&#26368;&#39640;&#30340;&#31034;&#20363;&#12290;CondAcc&#36890;&#36807;&#23558;&#35757;&#32451;&#31034;&#20363;&#19982;&#38543;&#26426;&#35757;&#32451;&#31034;&#20363;&#32452;&#21512;&#26102;&#30340;&#24179;&#22343;&#24320;&#21457;&#38598;ICL&#20934;&#30830;&#24615;&#26469;&#35780;&#20998;&#35757;&#32451;&#31034;&#20363;&#65292;&#32780;DataModels&#23398;&#20064;&#32447;&#24615;&#22238;&#24402;&#22120;&#65292;&#20272;&#35745;&#27599;&#20010;&#35757;&#32451;&#31034;&#20363;&#30340;&#23384;&#22312;&#22914;&#20309;&#24433;&#21709;LLM&#36755;&#20986;&#12290;&#22312;&#20116;&#20010;&#20219;&#21153;&#21644;&#20004;&#20010;LLM&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#30001;CondAcc&#21644;DataModels&#36873;&#25321;&#30340;&#31283;&#23450;&#23376;&#38598;&#20013;&#25277;&#26679;&#21487;&#20197;&#25552;&#39640;&#24179;&#22343;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) enables large language models (LLMs) to perform new tasks by prompting them with a sequence of training examples. However, it is known that ICL is very sensitive to the choice of training examples: randomly sampling examples from a training set leads to high variance in performance. In this paper, we show that carefully curating a subset of training data greatly stabilizes ICL performance without any other changes to the ICL algorithm (e.g., prompt retrieval or calibration). We introduce two methods to choose training subsets -- both score training examples individually, then select the highest-scoring ones. CondAcc scores a training example by its average dev-set ICL accuracy when combined with random training examples, while Datamodels learns linear regressors that estimate how the presence of each training example influences LLM outputs. Across five tasks and two LLMs, sampling from stable subsets selected by CondAcc and Datamodels improves average accuracy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;NLP&#27169;&#22411;HINT&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#20219;&#21153;&#25351;&#20196;&#21644;&#31034;&#20363;&#36716;&#25442;&#20026;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#26080;&#38656;&#23558;&#25351;&#20196;&#21253;&#21547;&#22312;&#27169;&#22411;&#36755;&#20837;&#20013;&#65292;&#24182;&#21487;&#20026;&#35299;&#30721;&#26399;&#38388;&#25552;&#20379;&#32534;&#30721;&#25351;&#20196;&#12290;HINT&#27169;&#22411;&#22312;&#35745;&#31639;&#37327;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#27604;&#26368;&#26032;&#30340;&#22522;&#32447;&#27169;&#22411;&#24378;10%&#20197;&#19978;&#65292;&#35299;&#20915;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.10315</link><description>&lt;p&gt;
HINT&#65306;&#29992;&#20110;&#39640;&#25928;&#38646;&#21450;&#23569;&#26679;&#26412;&#27867;&#21270;&#30340;&#36229;&#32593;&#32476;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
HINT: Hypernetwork Instruction Tuning for Efficient Zero- &amp; Few-Shot Generalisation. (arXiv:2212.10315v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;NLP&#27169;&#22411;HINT&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#23558;&#20219;&#21153;&#25351;&#20196;&#21644;&#31034;&#20363;&#36716;&#25442;&#20026;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#65292;&#20174;&#32780;&#26080;&#38656;&#23558;&#25351;&#20196;&#21253;&#21547;&#22312;&#27169;&#22411;&#36755;&#20837;&#20013;&#65292;&#24182;&#21487;&#20026;&#35299;&#30721;&#26399;&#38388;&#25552;&#20379;&#32534;&#30721;&#25351;&#20196;&#12290;HINT&#27169;&#22411;&#22312;&#35745;&#31639;&#37327;&#30456;&#31561;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#27604;&#26368;&#26032;&#30340;&#22522;&#32447;&#27169;&#22411;&#24378;10%&#20197;&#19978;&#65292;&#35299;&#20915;&#20102;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;NLP&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#22312;&#26032;&#20219;&#21153;&#20013;&#21482;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#23601;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#8220;&#38646;&#26679;&#26412;&#8221;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#20887;&#38271;&#30340;&#25351;&#20196;&#19982;&#27599;&#20010;&#36755;&#20837;&#31034;&#20363;&#36830;&#25509;&#65292;&#23548;&#33268;&#25351;&#20196;&#30340;&#26114;&#36149;&#37325;&#26032;&#22788;&#29702;&#65292;&#22240;&#27492;&#35768;&#22810;&#26041;&#27861;&#23384;&#22312;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#36229;&#32423;&#32593;&#32476;&#65288;HINT&#65289;&#65292;&#23427;&#23558;&#20219;&#21153;&#25351;&#20196;&#21644;&#31034;&#20363;&#36716;&#25442;&#20026;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#23558;&#20854;&#25554;&#20837;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26080;&#38656;&#23558;&#25351;&#20196;&#21253;&#21547;&#22312;&#27169;&#22411;&#36755;&#20837;&#20013;&#12290;HINT&#20013;&#30340;&#36229;&#32593;&#32476;&#36824;&#20135;&#29983;&#20102;&#19968;&#31181;&#32534;&#30721;&#25351;&#20196;&#65292;&#25105;&#20204;&#22312;&#35299;&#30721;&#26399;&#38388;&#23558;&#20854;&#19982;&#32534;&#30721;&#36755;&#20837;&#36830;&#25509;&#36215;&#26469;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#22312;&#25511;&#21046;&#35745;&#31639;&#65288;&#20197;FLOPs&#35745;&#37327;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;HINT&#27169;&#22411;&#30340;&#34920;&#29616;&#20248;&#20110;&#24378;&#26377;&#21147;&#30340;&#26368;&#26032;&#22522;&#32447;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#25351;&#20196;&#36716;&#25442;&#20026;&#27169;&#22359;&#65292;HINT&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#24573;&#30053;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent NLP models have shown the remarkable ability to effectively generalise `zero-shot' to new tasks using only natural language instructions as guidance. However, many of these approaches suffer from high computational costs due to their reliance on concatenating lengthy instructions with every input example, resulting in costly reprocessing of the instruction. To avoid this, we introduce Hypernetworks for INstruction Tuning (HINT), which convert task instructions and examples into parameter-efficient modules inserted into an underlying model using a pretrained text encoder, eliminating the need to include instructions in the model input. The hypernetwork in HINT also produces an encoded instruction, which we concatenate with encoded inputs during decoding to further improve performance. HINT models outperform strong state-of-the-art baselines by over 10% when controlling for compute (measured in FLOPs). By converting instructions into modules, HINT models can effectively disregard 
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.10029</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do language models have coherent mental models of everyday things?. (arXiv:2212.10029v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10029
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#32570;&#20047;&#23545;&#26085;&#24120;&#29289;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65292;&#20250;&#22240;&#27492;&#20986;&#29616;&#33618;&#35884;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#36825;&#31181;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20154;&#20204;&#24819;&#21040;&#20687;&#8220;&#40481;&#34507;&#8221;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#26102;&#65292;&#36890;&#24120;&#20250;&#26377;&#19968;&#20010;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#24515;&#29702;&#22270;&#20687;&#12290;&#36825;&#31181;&#24120;&#35782;&#24615;&#30693;&#35782;&#26377;&#21161;&#20110;&#25105;&#20204;&#29702;&#35299;&#36825;&#20123;&#26085;&#24120;&#29992;&#21697;&#30340;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#22914;&#20309;&#19982;&#23427;&#20204;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#31995;&#32479;&#23545;&#36825;&#26679;&#30340;&#26085;&#24120;&#29992;&#21697;&#27809;&#26377;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#27604;&#22914;&#35748;&#20026;&#40481;&#34507;&#40644;&#21253;&#22260;&#30528;&#22771;&#65292;&#37027;&#20040;&#23427;&#21487;&#33021;&#19981;&#24471;&#19981;&#37319;&#21462;&#33618;&#35884;&#30340;&#26041;&#27861;&#65292;&#27604;&#22914;&#35797;&#22270;&#25226;&#40481;&#34507;&#40644;&#21038;&#19979;&#22771;&#25918;&#20837;&#24179;&#24213;&#38149;&#20013;&#29006;&#29038;&#12290;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36825;&#31181;&#26085;&#24120;&#29992;&#21697;&#30340;&#19968;&#33268;&#24615;&#24515;&#29702;&#27169;&#22411;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#31181;&#26085;&#24120;&#29992;&#21697;&#12289;&#23427;&#20204;&#30340;&#37096;&#20214;&#20197;&#21450;&#36825;&#20123;&#37096;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20687;GPT-3&#21644;Macaw&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#36825;&#20123;&#23454;&#20307;&#30340;&#30693;&#35782;&#30862;&#29255;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#20026;&#25152;&#26377;&#23454;&#20307;&#20135;&#29983;&#19968;&#33268;&#19988;&#27491;&#30830;&#30340;&#24515;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#23545;&#36825;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#23427;&#20204;&#22312;&#26576;&#20123;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
When people think of everyday things like an "egg," they typically have a mental image associated with it. This commonsense knowledge helps us understand how these everyday things work and how to interact with them. For example, when someone tries to make a fried egg, they know that it has a shell and that it can be cracked open to reveal the egg white and yolk inside. However, if a system does not have a coherent picture of such everyday things, thinking that the egg yolk surrounds the shell, then it might have to resort to ridiculous approaches such as trying to scrape the egg yolk off the shell into the pan. Do language models have a coherent picture of such everyday things? To investigate this, we propose a benchmark dataset consisting of 100 everyday things, their parts, and the relationships between these parts. We observe that state-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw have fragments of knowledge about these entities, but they fail to produce consist
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;&#24182;&#35782;&#21035;&#25903;&#25345;&#23427;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#12290;&#22312;COVID-19&#27835;&#30103;&#30340;&#26696;&#20363;&#20013;&#65292;&#22522;&#20110;&#29616;&#20195;NLP&#26041;&#27861;&#24320;&#21457;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#27599;&#23567;&#26102;&#21487;&#20197;&#35782;&#21035;&#20986;&#36829;&#21453;Twitter&#20851;&#20110;COVID-19&#34394;&#20551;&#20449;&#24687;&#26041;&#38024;&#30340;124&#26465;&#25512;&#25991;&#12290;</title><link>http://arxiv.org/abs/2212.09683</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26089;&#26399;&#35823;&#20256;&#20449;&#24687;&#26816;&#27979;&#65306;COVID-19&#27835;&#30103;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-loop Evaluation for Early Misinformation Detection: A Case Study of COVID-19 Treatments. (arXiv:2212.09683v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09683
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;&#24182;&#35782;&#21035;&#25903;&#25345;&#23427;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#12290;&#22312;COVID-19&#27835;&#30103;&#30340;&#26696;&#20363;&#20013;&#65292;&#22522;&#20110;&#29616;&#20195;NLP&#26041;&#27861;&#24320;&#21457;&#22522;&#32447;&#31995;&#32479;&#65292;&#24182;&#23637;&#31034;&#20102;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#27599;&#23567;&#26102;&#21487;&#20197;&#35782;&#21035;&#20986;&#36829;&#21453;Twitter&#20851;&#20110;COVID-19&#34394;&#20551;&#20449;&#24687;&#26041;&#38024;&#30340;124&#26465;&#25512;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#26426;&#21327;&#21516;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#20107;&#23454;&#26680;&#26597;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#22768;&#26126;&#24182;&#35782;&#21035;&#25903;&#25345;&#23427;&#20204;&#30340;&#31038;&#20132;&#23186;&#20307;&#28040;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#20540;&#24471;&#26680;&#26597;&#30340;&#22768;&#26126;&#65292;&#36825;&#20123;&#22768;&#26126;&#34987;&#32858;&#21512;&#24182;&#25490;&#21517;&#20197;&#20415;&#22797;&#23457;&#12290;&#28982;&#21518;&#20351;&#29992;&#31435;&#22330;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#25903;&#25345;&#26032;&#34394;&#20551;&#20449;&#24687;&#30003;&#36848;&#30340;&#25512;&#25991;&#65292;&#36827;&#19968;&#27493;&#26816;&#26597;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#36829;&#21453;&#30456;&#20851;&#25919;&#31574;&#12290;&#20026;&#20102;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#22312;COVID-19&#27835;&#30103;&#39046;&#22495;&#22522;&#20110;&#29616;&#20195;NLP&#26041;&#27861;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#32447;&#31995;&#32479;&#29992;&#20110;&#20154;&#26426;&#21327;&#21516;&#20107;&#23454;&#26680;&#26597;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#31867;&#20107;&#23454;&#26680;&#26597;&#20154;&#21592;&#27599;&#23567;&#26102;&#33021;&#22815;&#35782;&#21035;&#20986;&#36829;&#21453;Twitter&#20851;&#20110;COVID-19&#34394;&#20551;&#20449;&#24687;&#26041;&#38024;&#30340;124&#26465;&#25512;&#25991;&#12290;&#25105;&#20204;&#23558;&#25552;&#20379;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#12289;&#22522;&#32447;&#27169;&#22411;&#21644;&#35814;&#32454;&#27880;&#37322;&#25351;&#21335;&#26469;&#25903;&#25345;&#20154;&#26426;&#21327;&#21516;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#30452;&#25509;&#20174;&#21407;&#22987;&#29992;&#25143;&#29983;&#25104;&#30340;&#20869;&#23481;&#20013;&#35782;&#21035;&#26032;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a human-in-the-loop evaluation framework for fact-checking novel misinformation claims and identifying social media messages that support them. Our approach extracts check-worthy claims, which are aggregated and ranked for review. Stance classifiers are then used to identify tweets supporting novel misinformation claims, which are further reviewed to determine whether they violate relevant policies. To demonstrate the feasibility of our approach, we develop a baseline system based on modern NLP methods for human-in-the-loop fact-checking in the domain of COVID-19 treatments. Using our baseline system, we show that human fact-checkers can identify 124 tweets per hour that violate Twitter's policies on COVID-19 misinformation. We will make our code, data, baseline models, and detailed annotation guidelines available to support the evaluation of human-in-the-loop systems that identify novel misinformation directly from raw user-generated content.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#20960;&#21313;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#20195;&#30721;&#20132;&#25442;&#30340;&#30740;&#31350;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#24635;&#32467;&#20102;&#36235;&#21183;&#21644;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.09660</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20195;&#30721;&#20132;&#25442;&#30740;&#31350;&#65306;&#36235;&#21183;&#21644;&#25361;&#25112;&#30340;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
The Decades Progress on Code-Switching Research in NLP: A Systematic Survey on Trends and Challenges. (arXiv:2212.09660v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#35843;&#26597;&#20102;&#20960;&#21313;&#24180;&#26469;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20851;&#20110;&#20195;&#30721;&#20132;&#25442;&#30340;&#30740;&#31350;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#24635;&#32467;&#20102;&#36235;&#21183;&#21644;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20132;&#25442;&#22312;&#20070;&#38754;&#25991;&#26412;&#21644;&#21475;&#35821;&#20132;&#27969;&#20013;&#26159;&#19968;&#31181;&#24120;&#35265;&#29616;&#35937;&#65292;&#24050;&#32463;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30028;&#30340;&#30740;&#31350;&#12290;&#26368;&#21021;&#65292;&#36890;&#36807;&#36816;&#29992;&#35821;&#35328;&#23398;&#29702;&#35770;&#26469;&#28145;&#20837;&#25506;&#32034;&#20195;&#30721;&#20132;&#25442;&#65292;&#30446;&#21069;&#21017;&#37319;&#29992;&#26356;&#22810;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#24320;&#21457;&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31687;&#20840;&#38754;&#30340;&#31995;&#32479;&#35843;&#26597;&#65292;&#26088;&#22312;&#20102;&#35299;&#36807;&#21435;&#20960;&#21313;&#24180;&#20195;&#30721;&#20132;&#25442;&#30740;&#31350;&#30340;&#36827;&#23637;&#24773;&#20917;&#65292;&#24182;&#26500;&#24605;&#20195;&#30721;&#20132;&#25442;&#20027;&#39064;&#19978;&#30340;&#25361;&#25112;&#21644;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#36235;&#21183;&#21644;&#21457;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#26041;&#21521;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code-Switching, a common phenomenon in written text and conversation, has been studied over decades by the natural language processing (NLP) research community. Initially, code-switching is intensively explored by leveraging linguistic theories and, currently, more machine-learning oriented approaches to develop models. We introduce a comprehensive systematic survey on code-switching research in natural language processing to understand the progress of the past decades and conceptualize the challenges and tasks on the code-switching topic. Finally, we summarize the trends and findings and conclude with a discussion for future direction and open questions for further investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2212.09535</link><description>&lt;p&gt;
BLOOM+1&#65306;&#20026;&#38646;&#26679;&#26412;&#25552;&#31034;&#28155;&#21152;&#35821;&#35328;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting. (arXiv:2212.09535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;BLOOM&#27169;&#22411;&#20013;&#24212;&#29992;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#65292;&#23558;&#20854;&#36866;&#24212;&#21040;&#26032;&#35821;&#35328;&#19978;&#65292;&#24182;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#65292;&#25552;&#31034;&#24615;&#33021;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BLOOM&#27169;&#22411;&#26159;&#19968;&#20010;&#22823;&#22411;&#20844;&#24320;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#20854;&#39044;&#35757;&#32451;&#20165;&#38480;&#20110;46&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#23558;BLOOM&#30340;&#22909;&#22788;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#36807;&#39640;&#30340;&#25104;&#26412;&#65292;&#26377;&#24517;&#35201;&#23558;BLOOM&#36866;&#24212;&#21040;&#26032;&#30340;&#35821;&#35328;&#19978;&#12290;&#26412;&#25991;&#23558;&#29616;&#26377;&#30340;&#35821;&#35328;&#36866;&#24212;&#31574;&#30053;&#24212;&#29992;&#20110;BLOOM&#65292;&#24182;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#23545;&#20854;&#22312;&#20843;&#31181;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#25552;&#31034;&#34920;&#29616;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35821;&#35328;&#36866;&#24212;&#23545;&#20110;&#25552;&#39640;&#26032;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#26159;&#26377;&#25928;&#30340;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36866;&#37197;&#22120;&#24494;&#35843;&#27604;&#22823;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#24615;&#33021;&#19981;&#20250;&#21463;&#21040;&#35821;&#35328;&#29305;&#23450;&#24615;&#30340;&#26174;&#30528;&#24433;&#21709;&#65292;&#22914;&#20070;&#20889;&#31995;&#32479;&#12290;&#23427;&#20027;&#35201;&#30001;&#35821;&#35328;&#36866;&#24212;&#25968;&#25454;&#30340;&#22823;&#23567;&#30830;&#23450;&#12290;&#25105;&#20204;&#36824;&#21521;BLOOMZ&#28155;&#21152;&#20102;&#26032;&#35821;&#35328;&#65292;&#36825;&#26159;BLOOM&#30340;&#22810;&#20219;&#21153;&#24494;&#35843;&#29256;&#26412;&#65292;&#33021;&#22815;&#36319;&#38543;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BLOOM model is a large publicly available multilingual language model, but its pretraining was limited to 46 languages. To extend the benefits of BLOOM to other languages without incurring prohibitively large costs, it is desirable to adapt BLOOM to new languages not seen during pretraining. In this work, we apply existing language adaptation strategies to BLOOM and benchmark its zero-shot prompting performance on eight new languages in a resource-constrained setting. We find language adaptation to be effective at improving zero-shot performance in new languages. Surprisingly, we find that adapter-based finetuning is more effective than continued pretraining for large models. In addition, we discover that prompting performance is not significantly affected by language specifics, such as the writing system. It is primarily determined by the size of the language adaptation data. We also add new languages to BLOOMZ, which is a multitask finetuned version of BLOOM capable of following 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#65292;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#34920;&#29616;&#20063;&#26377;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2212.04325</link><description>&lt;p&gt;
&#26080;&#26684;&#26629;&#24207;&#21015;&#37492;&#21035;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;
&lt;/p&gt;
&lt;p&gt;
Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers. (arXiv:2212.04325v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#29992;&#20110;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#65292;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#34920;&#29616;&#20063;&#26377;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;RNN-Transducer&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#30528;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#26080;&#26684;&#26629;&#24207;&#21015;&#37492;&#21035;&#35757;&#32451;&#26041;&#27861;&#65292;&#22312;&#28151;&#21512;&#27169;&#22411;&#20013;&#33719;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;RNN-Transducer&#20013;&#24456;&#23569;&#34987;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26080;&#26684;&#26629;&#35757;&#32451;&#30446;&#26631;&#65292;&#21363;&#26080;&#26684;&#26629;&#26368;&#22823;&#20114;&#20449;&#24687;&#12289;&#26080;&#26684;&#26629;&#27573;&#32423;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#21644;&#26080;&#26684;&#26629;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65292;&#29992;&#20110;&#20855;&#26377;&#26377;&#38480;&#19978;&#19979;&#25991;&#20381;&#36182;&#24615;&#30340;&#22522;&#20110;&#38899;&#32032;&#30340;&#31070;&#32463;&#20256;&#36882;&#22120;&#30340;&#26368;&#32456;&#21518;&#39564;&#36755;&#20986;&#12290;&#19982;&#20351;&#29992;N-best&#21015;&#34920;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#35757;&#32451;&#26399;&#38388;&#28040;&#38500;&#20102;&#20551;&#35774;&#29983;&#25104;&#30340;&#35299;&#30721;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#24207;&#21015;&#32423;&#20132;&#21449;&#29109;&#35757;&#32451;&#27169;&#22411;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#22312;&#21333;&#35789;&#38169;&#35823;&#29575;&#19978;&#33719;&#24471;&#20102;&#39640;&#36798;6.5&#65285;&#30340;&#30456;&#23545;&#25913;&#36827;&#12290;&#19982;&#22522;&#20110;N-best&#21015;&#34920;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#30446;&#26631;&#30456;&#27604;&#65292;&#26080;&#26684;&#26629;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#34892;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;N-best&#21015;&#34920;&#20013;&#20855;&#26377;&#19968;&#20123;&#22122;&#22768;&#21644;&#38169;&#35823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, RNN-Transducers have achieved remarkable results on various automatic speech recognition tasks. However, lattice-free sequence discriminative training methods, which obtain superior performance in hybrid models, are rarely investigated in RNN-Transducers. In this work, we propose three lattice-free training objectives, namely lattice-free maximum mutual information, lattice-free segment-level minimum Bayes risk, and lattice-free minimum Bayes risk, which are used for the final posterior output of the phoneme-based neural transducer with a limited context dependency. Compared to criteria using N-best lists, lattice-free methods eliminate the decoding step for hypotheses generation during training, which leads to more efficient training. Experimental results show that lattice-free methods gain up to 6.5% relative improvement in word error rate compared to a sequence-level cross-entropy trained model. Compared to the N-best-list based minimum Bayes risk objectives, lattice-free 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#36328;&#20219;&#21153;&#26368;&#36817;&#37051;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#20219;&#21153;&#25968;&#25454;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#30340;&#26368;&#30456;&#20284;&#26631;&#35760;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21462;&#24471;&#20102;&#27604;&#24378;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.00196</link><description>&lt;p&gt;
&#37319;&#29992;&#36328;&#20219;&#21153;&#26368;&#36817;&#37051;&#30340;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Finetuning Using Cross-Task Nearest Neighbors. (arXiv:2212.00196v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#36328;&#20219;&#21153;&#26368;&#36817;&#37051;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#23569;&#37327;&#30446;&#26631;&#20219;&#21153;&#25968;&#25454;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#30340;&#26368;&#30456;&#20284;&#26631;&#35760;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23545;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#65292;&#21462;&#24471;&#20102;&#27604;&#24378;&#22522;&#20934;&#27169;&#22411;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#29992;&#20110;&#35757;&#32451;&#24863;&#20852;&#36259;&#20219;&#21153;&#30340;&#26631;&#35760;&#25968;&#25454;&#36890;&#24120;&#20195;&#20215;&#39640;&#26114;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#22810;&#20219;&#21153;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#24182;&#21152;&#19978;&#20219;&#21153;&#25551;&#36848;&#65288;&#25552;&#31034;&#65289;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#20256;&#36882;&#32473;&#26032;&#20219;&#21153;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#25105;&#20204;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#23569;&#37327;&#65288;32-1000&#65289;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#20219;&#21153;&#31034;&#20363;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#31034;&#20363;&#20174;&#21253;&#21547;&#25552;&#31034;&#30340;&#22823;&#37327;&#22810;&#20219;&#21153;&#25968;&#25454;&#20013;&#26816;&#32034;&#26368;&#30456;&#20284;&#30340;&#26631;&#35760;&#31034;&#20363;&#12290;&#19982;&#24403;&#21069;&#22312;&#22343;&#21248;&#37319;&#26679;&#25552;&#31034;&#20219;&#21153;&#22810;&#20219;&#21153;&#25968;&#25454;&#65288;&#20363;&#22914;&#65306;FLAN&#12289;T0&#65289;&#19978;&#24494;&#35843;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#19978;&#26174;&#33879;&#26356;&#39640;&#12290;&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#35760;&#30340;&#30446;&#26631;&#20219;&#21153;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20351;&#29992; P3 &#27744;&#20013; 2% &#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312; 12 &#20010;&#20195;&#34920;&#20445;&#30041;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#27861;&#24459;&#21644;&#31185;&#23398;&#25991;&#26723; QA&#65289;&#20013;&#30340;&#24615;&#33021;&#35201;&#27604;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#39640;&#20986; 3-30%&#12290;&#37319;&#29992;&#36328;&#20219;&#21153;&#26368;&#36817;&#37051;&#35757;&#32451;&#30340;&#27169;&#22411;&#25928;&#26524;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining labeled data to train a model for a task of interest is often expensive. Prior work shows training models on multitask data augmented with task descriptions (prompts) effectively transfers knowledge to new tasks. Towards efficiently building task-specific models, we assume access to a small number (32-1000) of unlabeled target-task examples and use those to retrieve the most similar labeled examples from a large pool of multitask data augmented with prompts. Compared to the current practice of finetuning models on uniformly sampled prompted multitask data (e.g.: FLAN, T0), our approach of finetuning on cross-task nearest neighbors is significantly more data-efficient. Using only 2% of the data from the P3 pool without any labeled target-task data, our models outperform strong baselines trained on all available data by 3-30% on 12 out of 14 datasets representing held-out tasks including legal and scientific document QA. Similarly, models trained on cross-task nearest neighbors
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30495;&#23454;&#35821;&#38899;&#21644;&#21512;&#25104;&#35821;&#38899;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#32479;&#35745;&#23398;&#26041;&#27861;&#37327;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;10%&#30340;&#36317;&#31163;&#32553;&#23567;&#12290;</title><link>http://arxiv.org/abs/2211.16049</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#32553;&#23567;&#21512;&#25104;&#35821;&#38899;&#21644;&#30495;&#23454;&#35821;&#38899;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Evaluating and reducing the distance between synthetic and real speech distributions. (arXiv:2211.16049v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30495;&#23454;&#35821;&#38899;&#21644;&#21512;&#25104;&#35821;&#38899;&#30340;&#20998;&#24067;&#65292;&#20351;&#29992;&#32479;&#35745;&#23398;&#26041;&#27861;&#37327;&#21270;&#23427;&#20204;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;10%&#30340;&#36317;&#31163;&#32553;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#20195;TTS&#31995;&#32479;&#21487;&#20197;&#20135;&#29983;&#33258;&#28982;&#27969;&#30021;&#30340;&#35821;&#38899;&#65292;&#20294;&#23427;&#20204;&#20173;&#26080;&#27861;&#22797;&#29616;&#33258;&#28982;&#35821;&#38899;&#25968;&#25454;&#20013;&#21457;&#29616;&#30340;&#20840;&#37096;&#22810;&#26679;&#24615;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#32452;&#21457;&#38899;&#32773;&#25152;&#33021;&#20135;&#29983;&#30340;&#25152;&#26377;&#21487;&#33021;&#30495;&#23454;&#35821;&#38899;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#20351;&#29992;&#29305;&#23450;TTS&#31995;&#32479;&#21487;&#20197;&#29983;&#25104;&#30340;&#25152;&#26377;&#21512;&#25104;&#26679;&#26412;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#19982;&#21457;&#38899;&#32773;&#23646;&#24615;&#12289;&#35821;&#38899;&#38901;&#24459;&#21644;&#22768;&#23398;&#29615;&#22659;&#30456;&#20851;&#30340;&#35805;&#35821;&#27700;&#24179;&#32479;&#35745;&#20449;&#24687;&#26469;&#37327;&#21270;&#30495;&#23454;&#35821;&#38899;&#21644;&#21512;&#25104;&#35821;&#38899;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;Wasserstein&#36317;&#31163;&#35780;&#20272;&#36825;&#20123;&#32479;&#35745;&#20449;&#24687;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#36890;&#36807;&#22312;&#29983;&#25104;&#26102;&#25552;&#20379;&#22522;&#20934;&#20540;&#65292;&#25105;&#20204;&#32553;&#23567;&#20102;&#36825;&#20123;&#36317;&#31163;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26469;&#37327;&#21270;&#25972;&#20307;&#20998;&#24067;&#36317;&#31163;&#30340;&#25913;&#36827;&#24773;&#20917;&#12290;&#22312;&#25105;&#20204;&#30340;&#26368;&#20339;&#31995;&#32479;&#20013;&#65292;&#20998;&#24067;&#36317;&#31163;&#32553;&#23567;&#20102;10&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern Text-to-Speech (TTS) systems can produce natural-sounding speech, they remain unable to reproduce the full diversity found in natural speech data. We consider the distribution of all possible real speech samples that could be generated by these speakers alongside the distribution of all synthetic samples that could be generated for the same set of speakers, using a particular TTS system. We set out to quantify the distance between real and synthetic speech via a range of utterance-level statistics related to properties of the speaker, speech prosody and acoustic environment. Differences in the distribution of these statistics are evaluated using the Wasserstein distance. We reduce these distances by providing ground-truth values at generation time, and quantify the improvements to the overall distribution distance, approximated using an automatic speech recognition system. Our best system achieves a 10\% reduction in distribution distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20840;&#23616;&#21644;&#26412;&#22320;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#26469;&#25552;&#21319;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.13873</link><description>&lt;p&gt;
&#20840;&#23616;&#21644;&#26412;&#22320;&#20998;&#23618;&#24863;&#30693;&#23545;&#27604;&#26694;&#26550;&#29992;&#20110;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20840;&#23616;&#21644;&#26412;&#22320;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#26469;&#25552;&#21319;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#26174;&#24335;&#30340;&#36830;&#25509;&#35789;&#65292;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;(IDRR)&#20173;&#28982;&#26159;&#31687;&#31456;&#20998;&#26512;&#20013;&#30340;&#38590;&#39064;&#12290;IDRR&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#23398;&#20064;&#20004;&#20010;&#35770;&#28857;&#20043;&#38388;&#39640;&#36136;&#37327;&#30340;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36235;&#21521;&#20110;&#23558;&#25972;&#20010;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#20013;&#36827;&#34892;&#22810;&#32423;&#21035;&#24863;&#30693;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#25972;&#21512;&#21253;&#21547;&#25152;&#26377;&#24863;&#30693;&#30340;&#38745;&#24577;&#20998;&#23618;&#32467;&#26500;&#65288;&#23450;&#20041;&#20026;&#20840;&#23616;&#20998;&#23618;&#32467;&#26500;&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#19982;&#27599;&#20010;&#23454;&#20363;&#23545;&#24212;&#30340;&#23618;&#27425;&#24863;&#30693;&#26631;&#31614;&#24207;&#21015;&#65288;&#23450;&#20041;&#20026;&#26412;&#22320;&#20998;&#23618;&#32467;&#26500;&#65289;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20840;&#23616;&#21644;&#26412;&#22320;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#21644;&#26412;&#22320;&#20998;&#23618;&#24863;&#30693;&#23545;&#27604;&#26694;&#26550;(GOLF)&#65292;&#20511;&#21161;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#27169;&#25311;&#20004;&#31181;&#23618;&#27425;&#24863;&#30693;&#12290;&#22312;PDTB 2.0&#21644;PDTB-EDT&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;GOLF&#22312;IDRR&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all senses (defined as global hierarchy), and ignore the hierarchical sense label sequence corresponding to each instance (defined as local hierarchy). For the purpose of sufficiently exploiting global and local hierarchies of senses to learn better discourse relation representations, we propose a novel GlObal and Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of hierarchies with the aid of multi-task learning and contrastive learning. Experimental results on PDTB 2.0 and PDTB
&lt;/p&gt;</description></item><item><title>NCTE&#35760;&#24405;&#25552;&#20379;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#23567;&#23398;&#25968;&#23398;&#35838;&#22530;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#23427;&#26377;&#21161;&#20110;&#30740;&#31350;&#35838;&#22530;&#23545;&#35805;&#24182;&#25913;&#21892;&#25945;&#23398;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2211.11772</link><description>&lt;p&gt;
NCTE &#35760;&#24405;&#65306;&#19968;&#20010;&#23567;&#23398;&#25968;&#23398;&#35838;&#22530;&#35760;&#24405;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The NCTE Transcripts: A Dataset of Elementary Math Classroom Transcripts. (arXiv:2211.11772v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11772
&lt;/p&gt;
&lt;p&gt;
NCTE&#35760;&#24405;&#25552;&#20379;&#20102;&#19968;&#20010;&#24222;&#22823;&#30340;&#23567;&#23398;&#25968;&#23398;&#35838;&#22530;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#23427;&#26377;&#21161;&#20110;&#30740;&#31350;&#35838;&#22530;&#23545;&#35805;&#24182;&#25913;&#21892;&#25945;&#23398;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#22530;&#35805;&#35821;&#26159;&#19968;&#31181;&#26680;&#24515;&#30340;&#25945;&#23398;&#23186;&#20171;-&#20998;&#26512;&#23427;&#21487;&#20197;&#25552;&#20379;&#31397;&#35270;&#25945;&#23398;&#21644;&#23398;&#20064;&#30340;&#35270;&#31383;&#65292;&#24182;&#25512;&#21160;&#24320;&#21457;&#25913;&#21892;&#25945;&#23398;&#30340;&#26032;&#24037;&#20855;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#30740;&#31350;&#32773;&#21487;&#29992;&#30340;&#26368;&#22823;&#30340;&#25968;&#23398;&#35838;&#22530;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;&#25945;&#23398;&#36136;&#37327;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;2010-2013&#24180;&#38388; NCTE &#25910;&#38598;&#30340; 1,660&#26465; 45-60 &#20998;&#38047;&#30340;&#23567;&#23398;&#22235;&#20116;&#24180;&#32423;&#25968;&#23398;&#35838;&#22530;&#35760;&#24405;&#25968;&#25454;&#65292;&#26159;&#26368;&#22823;&#30340;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#36825;&#20123;&#21311;&#21517;&#35760;&#24405;&#26469;&#33258;&#20110; 4 &#20010;&#20027;&#35201;&#26381;&#21153;&#20110;&#21382;&#21490;&#19978;&#22788;&#20110;&#36793;&#32536;&#21270;&#30340;&#23398;&#29983;&#30340;&#23398;&#21306;&#30340; 317 &#21517;&#25945;&#24072;&#12290;&#23427;&#20204;&#24102;&#26377;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#65292;&#21253;&#25324;&#23545;&#35805;&#30424;&#28857;&#19978;&#30340;&#27880;&#37322;&#12289;&#35838;&#22530;&#35266;&#23519;&#24471;&#20998;&#12289;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#12289;&#35843;&#26597;&#22238;&#31572;&#21644;&#23398;&#29983;&#27979;&#35797;&#25104;&#32489;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;&#23545;&#35805;&#30424;&#28857;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21487;&#20197;&#20197;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24230;&#23398;&#20064;&#22312;&#35838;&#22530;&#19978;&#35782;&#21035;&#23545;&#35805;&#30424;&#28857;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27492;&#25968;&#25454;&#38598;&#22914;&#20309;&#29992;&#20110;&#30740;&#31350;&#35838;&#22530;&#23545;&#35805;&#24182;&#25913;&#21892;&#25945;&#23398;&#30340;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classroom discourse is a core medium of instruction - analyzing it can provide a window into teaching and learning as well as driving the development of new tools for improving instruction. We introduce the largest dataset of mathematics classroom transcripts available to researchers, and demonstrate how this data can help improve instruction. The dataset consists of 1,660 45-60 minute long 4th and 5th grade elementary mathematics observations collected by the National Center for Teacher Effectiveness (NCTE) between 2010-2013. The anonymized transcripts represent data from 317 teachers across 4 school districts that serve largely historically marginalized students. The transcripts come with rich metadata, including turn-level annotations for dialogic discourse moves, classroom observation scores, demographic information, survey responses and student test scores. We demonstrate that our natural language processing model, trained on our turn-level annotations, can learn to identify dialo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.11300</link><description>&lt;p&gt;
&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text. (arXiv:2211.11300v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#34701;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#25991;&#26412;&#20013;&#30340;&#31163;&#32676;&#26816;&#27979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#21482;&#20351;&#29992;&#20869;&#20998;&#24067;(ID)&#26679;&#20363;&#25991;&#26412;&#36827;&#34892;&#31163;&#32676;&#26816;&#27979;&#30340;&#23453;&#36149;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#36890;&#36807;&#20351;&#29992;ID&#26679;&#20363;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#23558;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#22256;&#24785;&#24230;&#20316;&#20026;&#31163;&#32676;&#24471;&#20998;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#31181;&#31163;&#32676;&#26816;&#27979;&#26041;&#27861;&#30340;&#20114;&#34917;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#23427;&#20204;&#20248;&#21183;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#30340;&#22810;&#23618;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#20316;&#20026;&#32769;&#24072;&#65292;&#22312;ID&#31034;&#20363;&#19978;&#25945;&#25480;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#38500;&#20102;&#39044;&#27979;&#23618;&#33976;&#39311;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20013;&#38388;&#23618;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#20840;&#38754;&#25506;&#32034;&#32769;&#24072;&#27169;&#22411;&#30340;&#34920;&#31034;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#23398;&#20064;&#30340;&#23398;&#29983;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#31034;ID&#25968;&#25454;&#27969;&#24418;&#65292;&#21516;&#26102;&#33719;&#24471;&#26356;&#24378;&#30340;&#23558;OoD&#31034;&#20363;&#26144;&#23556;&#21040;&#27969;&#24418;&#20043;&#22806;&#30340;&#33021;&#21147;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#31454;&#20105;&#22522;&#32447;&#30456;&#27604;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised representation learning has proved to be a valuable component for out-of-distribution (OoD) detection with only the texts of in-distribution (ID) examples. These approaches either train a language model from scratch or fine-tune a pre-trained language model using ID examples, and then take the perplexity output by the language model as OoD scores. In this paper, we analyze the complementary characteristics of both OoD detection methods and propose a multi-level knowledge distillation approach that integrates their strengths while mitigating their limitations. Specifically, we use a fine-tuned model as the teacher to teach a randomly initialized student model on the ID examples. Besides the prediction layer distillation, we present a similarity-based intermediate layer distillation method to thoroughly explore the representation space of the teacher model. In this way, the learned student can better represent the ID data manifold while gaining a stronger ability to map O
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2211.05985</link><description>&lt;p&gt;
&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#26469;&#35299;&#37322;&#21644;&#26816;&#27979;&#20581;&#24247;&#38169;&#35823;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
Using Persuasive Writing Strategies to Explain and Detect Health Misinformation. (arXiv:2211.05985v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#26469;&#22686;&#21152;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26032;&#23618;&#27425;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#27880;&#37322;&#26041;&#26696;&#21644;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#20449;&#24687;&#30340;&#20256;&#25773;&#26159;&#24403;&#20170;&#31038;&#20250;&#30340;&#19968;&#22823;&#38382;&#39064;&#65292;&#35768;&#22810;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#21162;&#21147;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#20110;&#27599;&#22825;&#21019;&#36896;&#30340;&#34394;&#20551;&#20449;&#24687;&#25968;&#37327;&#24040;&#22823;&#65292;&#23558;&#27492;&#20219;&#21153;&#30041;&#32473;&#20154;&#24037;&#20107;&#23454;&#26816;&#26597;&#21592;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#30740;&#31350;&#20154;&#21592;&#22810;&#24180;&#26469;&#19968;&#30452;&#33268;&#21147;&#20110;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#65292;&#20294;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#20026;&#33258;&#21160;&#21270;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#28155;&#21152;&#19968;&#20010;&#26032;&#23618;&#27425;&#65307;&#20351;&#29992;&#20855;&#26377;&#35828;&#26381;&#24615;&#20889;&#20316;&#25216;&#24039;&#30340;&#25991;&#26412;&#27573;&#33853;&#36827;&#34892;&#20998;&#31867;&#65292;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29702;&#30001;&#65292;&#35828;&#26126;&#20026;&#20160;&#20040;&#36825;&#31687;&#25991;&#31456;&#21487;&#20197;&#26631;&#35760;&#20026;&#34394;&#20551;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#35768;&#22810;&#24120;&#35265;&#35828;&#26381;&#24615;&#20889;&#20316;&#31574;&#30053;&#30340;&#26032;&#27880;&#37322;&#26041;&#26696;&#65292;&#20197;&#21450;&#30456;&#24212;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992; RoBERTa &#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#26469;&#23436;&#25104;&#27492;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#20855;&#26377;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2211.05523</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Adversarial Training on Robustness and Generalizability of Language Models. (arXiv:2211.05523v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#35757;&#32451;&#20013;&#20351;&#29992;&#30340;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#36825;&#20123;&#25913;&#36827;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#30340;&#26368;&#26377;&#25928;&#25163;&#27573;&#12290;&#20294;&#26159;&#65292;&#24050;&#32463;&#30830;&#35748;&#23545;&#25239;&#35757;&#32451;&#27169;&#22411;&#21516;&#26102;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#38656;&#35201;&#36827;&#34892;&#26435;&#34913;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#28145;&#20837;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21644;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#23545;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#25110;&#35757;&#32451;&#26102;&#38388;&#36755;&#20837;&#25200;&#21160;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#23884;&#20837;&#31354;&#38388;&#25200;&#21160;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#22320;&#25552;&#39640;&#27867;&#21270;&#24615;&#12290;&#23398;&#20064;&#27169;&#22411;&#31070;&#32463;&#20803;&#30340;&#35821;&#35328;&#30456;&#20851;&#24615;&#20998;&#26512;&#34920;&#26126;&#65292;&#25913;&#21892;&#27867;&#21270;&#24615;&#26159;&#30001;&#20110;&#23384;&#22312;&#8220;&#26356;&#19987;&#19994;&#8221;&#30340;&#31070;&#32463;&#20803;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#20010;&#23545;&#35821;&#35328;&#27169;&#22411;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#26041;&#27861;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is widely acknowledged as the most effective defense against adversarial attacks. However, it is also well established that achieving both robustness and generalization in adversarially trained models involves a trade-off. The goal of this work is to provide an in depth comparison of different approaches for adversarial training in language models. Specifically, we study the effect of pre-training data augmentation as well as training time input perturbations vs. embedding space perturbations on the robustness and generalization of transformer-based language models. Our findings suggest that better robustness can be achieved by pre-training data augmentation or by training with input space perturbation. However, training with embedding space perturbation significantly improves generalization. A linguistic correlation analysis of neurons of the learned models reveals that the improved generalization is due to 'more specialized' neurons. To the best of our knowledge,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#20998;&#26512;&#21644;&#25512;&#29702;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#26174;&#24335;&#22320;&#32534;&#30721;&#25104;&#36923;&#36753;&#32467;&#26500;&#65292; &#36827;&#32780;&#33719;&#24471;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.03252</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#38646;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Classification by Logical Reasoning on Natural Language Explanations. (arXiv:2211.03252v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.03252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#25512;&#29702;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#20998;&#26512;&#21644;&#25512;&#29702;&#65292;&#23558;&#25991;&#26412;&#20449;&#24687;&#26174;&#24335;&#22320;&#32534;&#30721;&#25104;&#36923;&#36753;&#32467;&#26500;&#65292; &#36827;&#32780;&#33719;&#24471;&#21487;&#38752;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#23545;&#20854;&#35821;&#35328;&#35299;&#37322;&#30340;&#25512;&#29702;&#26469;&#20998;&#31867;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#31867;&#21035;&#12290;&#36825;&#31181;&#33021;&#21147;&#28304;&#20110;&#35821;&#35328;&#30340;&#32452;&#25104;&#24615;&#36136;&#65306;&#25105;&#20204;&#21487;&#20197;&#32452;&#21512;&#20197;&#21069;&#30475;&#21040;&#30340;&#23646;&#24615;&#26469;&#25551;&#36848;&#26032;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#36827;&#34892;&#36923;&#36753;&#20998;&#26512;&#21644;&#25512;&#29702;&#65292;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLORE&#26694;&#26550;&#65288;Classification by LOgical Reasoning on Explanations&#65289;&#12290; CLORE&#23558;&#20197;&#21069;&#30340;&#26041;&#27861;&#25152;&#38544;&#21547;&#30340;&#25991;&#26412;&#20449;&#24687;&#35299;&#26512;&#25104;&#36923;&#36753;&#32467;&#26500;&#24182;&#27839;&#30528;&#36825;&#20123;&#32467;&#26500;&#25512;&#29702;&#65292;&#20197;&#20135;&#29983;&#20998;&#31867;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans can classify data of an unseen category by reasoning on its language explanations. This ability is owing to the compositional nature of language: we can combine previously seen attributes to describe the new category. For example, we might describe a sage thrasher as "it has a slim straight relatively short bill, yellow eyes and a long tail", so that others can use their knowledge of attributes "slim straight relatively short bill", "yellow eyes" and "long tail" to recognize a sage thrasher. Inspired by this observation, in this work we tackle zero-shot classification task by logically parsing and reasoning on natural language expla-nations. To this end, we propose the framework CLORE (Classification by LOgical Reasoning on Explanations). While previous methods usually regard textual information as implicit features, CLORE parses explanations into logical structures and then explicitly reasons along thess structures on the input to produce a classification score. Experimental re
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20316;&#20026;&#25991;&#26412;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31435;&#22330;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#26410;&#34987;&#25552;&#21069;&#35265;&#36807;&#30340;&#30446;&#26631;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2211.01874</link><description>&lt;p&gt;
&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#31435;&#22330;&#26816;&#27979;&#20013;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
Contextual information integration for stance detection via cross-attention. (arXiv:2211.01874v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#26469;&#33258;&#24322;&#26500;&#25968;&#25454;&#28304;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#20316;&#20026;&#25991;&#26412;&#25972;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31435;&#22330;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#26410;&#34987;&#25552;&#21069;&#35265;&#36807;&#30340;&#30446;&#26631;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31435;&#22330;&#26816;&#27979;&#26088;&#22312;&#30830;&#23450;&#20316;&#32773;&#23545;&#30446;&#26631;&#30340;&#31435;&#22330;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31435;&#22330;&#26816;&#27979;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26469;&#27491;&#30830;&#22320;&#25512;&#26029;&#31435;&#22330;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21363;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#20316;&#20026;&#25991;&#26412;&#36827;&#34892;&#25972;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#22914;&#32467;&#26500;&#21270;&#30693;&#35782;&#28304;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25972;&#21512;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#21487;&#20197;&#20811;&#26381;&#26631;&#20934;&#30693;&#35782;&#24211;&#30340;&#22270;&#24418;&#32467;&#26500;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38598;&#25104;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#20010;&#22823;&#22411;&#21644;&#22810;&#26679;&#21270;&#30340;&#31435;&#22330;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#20013;&#21487;&#20197;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#65292;&#22312;&#20132;&#21449;&#30446;&#26631;&#35774;&#32622;&#20013;&#65292;&#21363;&#38024;&#23545;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#23545;&#22122;&#22768;&#19978;&#19979;&#25991;&#26356;&#21152;&#40065;&#26834;&#65292;&#24182;&#19988;&#21487;&#20197;&#20026;&#26631;&#31614;&#21644;&#30446;&#26631;&#29305;&#23450;&#35789;&#27719;&#20043;&#38388;&#30340;&#19981;&#24517;&#35201;&#30456;&#20851;&#24615;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#26368;&#21518;&#65292;&#23427;&#26159;&#29420;&#31435;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stance detection deals with identifying an author's stance towards a target. Most existing stance detection models are limited because they do not consider relevant contextual information which allows for inferring the stance correctly. Complementary context can be found in knowledge bases but integrating the context into pretrained language models is non-trivial due to the graph structure of standard knowledge bases. To overcome this, we explore an approach to integrate contextual information as text which allows for integrating contextual information from heterogeneous sources, such as structured knowledge sources and by prompting large language models. Our approach can outperform competitive baselines on a large and diverse stance detection benchmark in a cross-target setup, i.e. for targets unseen during training. We demonstrate that it is more robust to noisy context and can regularize for unwanted correlations between labels and target-specific vocabulary. Finally, it is independ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#35805;&#35821;&#25340;&#25509;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#30701;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#35805;&#35821;&#38271;&#24230;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#38271;&#35805;&#35821;&#30340;&#35782;&#21035;&#29575;&#65292;&#21516;&#26102;&#23545;&#30701;&#35805;&#35821;&#30340;&#24615;&#33021;&#27809;&#26377;&#19979;&#38477;&#65292;&#24182;&#21462;&#24471;&#20102;5.72%&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2210.15876</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#35805;&#35821;&#25340;&#25509;&#30340;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#25552;&#39640;&#30701;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Random Utterance Concatenation Based Data Augmentation for Improving Short-video Speech Recognition. (arXiv:2210.15876v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#35805;&#35821;&#25340;&#25509;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#30701;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#35805;&#35821;&#38271;&#24230;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21487;&#26174;&#33879;&#25552;&#39640;&#38271;&#35805;&#35821;&#30340;&#35782;&#21035;&#29575;&#65292;&#21516;&#26102;&#23545;&#30701;&#35805;&#35821;&#30340;&#24615;&#33021;&#27809;&#26377;&#19979;&#38477;&#65292;&#24182;&#21462;&#24471;&#20102;5.72%&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#26694;&#26550;&#30340;&#23616;&#38480;&#24615;&#20043;&#19968;&#26159;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#35805;&#35821;&#38271;&#24230;&#19981;&#21305;&#37197;&#26102;&#65292;&#20854;&#24615;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21363;&#26102;&#22522;&#20110;&#38543;&#26426;&#35805;&#35821;&#25340;&#25509;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#29992;&#20110;&#32531;&#35299;&#30701;&#35270;&#39057;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#35805;&#35821;&#38271;&#24230;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#38024;&#23545;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;&#36716;&#24405;&#30340;&#35757;&#32451;&#35805;&#35821;&#24448;&#24448;&#23545;&#20110;&#30701;&#35270;&#39057;&#33258;&#21457;&#35821;&#38899;&#65288;&#24179;&#22343;&#32422;3&#31186;&#65289;&#35201;&#30701;&#24471;&#22810;&#30340;&#24773;&#20917;&#65292;&#32780;&#30001;&#35821;&#38899;&#27963;&#21160;&#26816;&#27979;&#21069;&#31471;&#29983;&#25104;&#30340;&#27979;&#35797;&#35805;&#35821;&#21017;&#35201;&#38271;&#24471;&#22810;&#65288;&#24179;&#22343;&#32422;10&#31186;&#65289;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#34920;&#29616;&#27425;&#20248;&#12290;&#23454;&#35777;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;RUC&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#35805;&#35821;&#30340;&#35782;&#21035;&#29575;&#65292;&#32780;&#23545;&#30701;&#35805;&#35821;&#30340;&#24615;&#33021;&#27809;&#26377;&#19979;&#38477;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;15&#31181;&#35821;&#35328;&#65292;&#35813;&#26041;&#27861;&#24179;&#22343;&#23454;&#29616;&#20102;5.72&#65285;&#30340;&#35789;&#38169;&#35823;&#29575;&#38477;&#20302;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#21508;&#31181;&#35805;&#35821;&#38271;&#24230;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of limitations in end-to-end automatic speech recognition (ASR) framework is its performance would be compromised if train-test utterance lengths are mismatched. In this paper, we propose an on-the-fly random utterance concatenation (RUC) based data augmentation method to alleviate train-test utterance length mismatch issue for short-video ASR task. Specifically, we are motivated by observations that our human-transcribed training utterances tend to be much shorter for short-video spontaneous speech (~3 seconds on average), while our test utterance generated from voice activity detection front-end is much longer (~10 seconds on average). Such a mismatch can lead to suboptimal performance. Empirically, it's observed the proposed RUC method significantly improves long utterance recognition without performance drop on short one. Overall, it achieves 5.72% word error rate reduction on average for 15 languages and improved robustness to various utterance length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#26356;&#21152;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#26102;&#38388;&#23545;&#40784;&#20219;&#21153;&#20013;&#65292;&#24182;&#36890;&#36807;&#23558;PLMs&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;TVG&#27169;&#22411;&#20174;PLM&#38598;&#25104;&#21644;&#24494;&#35843;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;</title><link>http://arxiv.org/abs/2209.13359</link><description>&lt;p&gt;
&#20026;&#20102;&#26356;&#21152;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#26102;&#38388;&#23545;&#40784;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding. (arXiv:2209.13359v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#26356;&#21152;&#39640;&#25928;&#22320;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#35270;&#39057;&#26102;&#38388;&#23545;&#40784;&#20219;&#21153;&#20013;&#65292;&#24182;&#36890;&#36807;&#23558;PLMs&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;TVG&#27169;&#22411;&#20174;PLM&#38598;&#25104;&#21644;&#24494;&#35843;&#20013;&#21463;&#30410;&#21290;&#27973;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#35270;&#39057;&#26102;&#38388;&#23545;&#40784;&#65288;TVG&#65289;&#20219;&#21153;&#65292;&#21363;&#22312;&#32473;&#23450;&#26410;&#20462;&#21098;&#35270;&#39057;&#21644;&#33258;&#28982;&#35821;&#35328;&#21477;&#23376;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#65292;&#35782;&#21035;&#24182;&#30830;&#23450;&#19982;&#26597;&#35810;&#25551;&#36848;&#30340;&#35270;&#39057;&#21160;&#20316;&#23454;&#20363;&#30340;&#26102;&#38388;&#36793;&#30028;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#36739;&#22823;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#25913;&#36827;&#26597;&#35810;&#36755;&#20837;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#20195;&#20215;&#26159;&#26356;&#26114;&#36149;&#30340;&#35757;&#32451;&#36153;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#38598;&#25104;&#30340;&#25928;&#26524;&#36824;&#19981;&#28165;&#26970;&#65292;&#22240;&#20026;&#36825;&#20123;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#25913;&#36827;&#35270;&#35273;&#36755;&#20837;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;PLM&#22312;TVG&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#20102;&#20351;&#29992;NLP&#36866;&#37197;&#22120;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#27969;&#34892;&#30340;PLM&#19982;&#29616;&#26377;&#26041;&#27861;&#30340;&#36873;&#25321;&#32467;&#21512;&#20351;&#29992;&#65292;&#24182;&#27979;&#35797;&#19981;&#21516;&#30340;&#36866;&#37197;&#22120;&#20197;&#20943;&#23569;&#39069;&#22806;&#21442;&#25968;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19981;&#25913;&#21464;&#35270;&#35273;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#65292;TVG&#27169;&#22411;&#20174;PLM&#38598;&#25104;&#21644;&#24494;&#35843;&#20013;&#21463;&#30410;&#21290;&#27973;&#65292;&#24378;&#35843;&#20102;&#21477;&#23376;&#26597;&#35810;&#34920;&#31034;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the task of Temporal Video Grounding (TVG) where, given an untrimmed video and a natural language sentence query, the goal is to recognize and determine temporal boundaries of action instances in the video described by the query. Recent works tackled this task by improving query inputs with large pre-trained language models (PLM) at the cost of more expensive training. However, the effects of this integration are unclear, as these works also propose improvements in the visual inputs. Therefore, this paper studies the effects of PLMs in TVG and assesses the applicability of parameter-efficient training with NLP adapters. We couple popular PLMs with a selection of existing approaches and test different adapters to reduce the impact of the additional parameters. Our results on three challenging datasets show that, without changing the visual inputs, TVG models greatly benefited from the PLM integration and fine-tuning, stressing the importance of sentence query represe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26102;&#38388;&#21464;&#21270;&#30340;MLM&#25513;&#34109;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#25513;&#34109;&#27604;&#20363;&#21644;&#25513;&#34109;&#20869;&#23481;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10806</link><description>&lt;p&gt;
&#23398;&#20064;&#26356;&#22909;&#30340;&#25513;&#34109;&#31574;&#30053;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Learning Better Masking for Better Language Model Pre-training. (arXiv:2208.10806v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26102;&#38388;&#21464;&#21270;&#30340;MLM&#25513;&#34109;&#31574;&#30053;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#25513;&#34109;&#27604;&#20363;&#21644;&#25513;&#34109;&#20869;&#23481;&#65292;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PrLM&#65289;&#20013;&#30340;&#21435;&#22122;&#30446;&#26631;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#37319;&#29992;&#38543;&#26426;&#25513;&#34109;&#31574;&#30053;&#65292;&#20854;&#20013;&#24212;&#29992;&#22266;&#23450;&#30340;&#25513;&#34109;&#27604;&#20363;&#65292;&#24182;&#19988;&#20197;&#30456;&#31561;&#30340;&#27010;&#29575;&#25513;&#34109;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#21463;&#21040;&#39044;&#35757;&#32451;&#29366;&#24577;&#30340;&#22797;&#26434;&#24433;&#21709;&#65292;&#36825;&#31181;&#24433;&#21709;&#20250;&#38543;&#30528;&#35757;&#32451;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26102;&#38388;&#19981;&#21464;&#30340;MLM&#35774;&#32622;&#21487;&#33021;&#26080;&#27861;&#20135;&#29983;&#26368;&#20339;&#32467;&#26524;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#26102;&#38388;&#21464;&#21270;&#30340;MLM&#35774;&#32622;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35745;&#21010;&#25513;&#34109;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#38454;&#27573;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#25513;&#34109;&#27604;&#20363;&#21644;&#25513;&#34109;&#30340;&#20869;&#23481;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#20851;&#20110;&#27604;&#29575;&#21644;&#20869;&#23481;&#30340;&#26102;&#38388;&#21464;&#21270;&#25513;&#34109;&#31574;&#30053;&#30340;&#20808;&#39537;&#30740;&#31350;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#24212;&#29992;&#36825;&#20123;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked Language Modeling (MLM) has been widely used as the denoising objective in pre-training language models (PrLMs). Existing PrLMs commonly adopt a Random-Token Masking strategy where a fixed masking ratio is applied and different contents are masked by an equal probability throughout the entire training. However, the model may receive complicated impact from pre-training status, which changes accordingly as training time goes on. In this paper, we show that such time-invariant MLM settings on masking ratio and masked content are unlikely to deliver an optimal outcome, which motivates us to explore the influence of time-variant MLM settings. We propose two scheduled masking approaches that adaptively tune the masking ratio and masked content in different training stages, which improves the pre-training efficiency and effectiveness verified on the downstream tasks. Our work is a pioneer study on time-variant masking strategy on ratio and content and gives a better understanding of h
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;DICE&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#21644;&#29305;&#27530;&#26631;&#35760;&#65292;&#20849;&#21516;&#35757;&#32451;&#23454;&#20307;&#25552;&#21450;&#21644;&#20107;&#20214;&#25277;&#21462;&#31561;&#36741;&#21161;&#20219;&#21153;&#65292;&#25152;&#25552;&#20986;&#30340;MACCROBAT-EE&#25968;&#25454;&#38598;&#20026;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2208.07989</link><description>&lt;p&gt;
DICE&#65306;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
DICE: Data-Efficient Clinical Event Extraction with Generative Models. (arXiv:2208.07989v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.07989
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#31283;&#20581;&#39640;&#25928;&#30340;&#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#26041;&#27861;DICE&#65292;&#24341;&#20837;&#20102;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#21644;&#29305;&#27530;&#26631;&#35760;&#65292;&#20849;&#21516;&#35757;&#32451;&#23454;&#20307;&#25552;&#21450;&#21644;&#20107;&#20214;&#25277;&#21462;&#31561;&#36741;&#21161;&#20219;&#21153;&#65292;&#25152;&#25552;&#20986;&#30340;MACCROBAT-EE&#25968;&#25454;&#38598;&#20026;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25552;&#20379;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39046;&#22495;&#30340;&#20107;&#20214;&#25277;&#21462;&#26159;&#19968;&#20010;&#26410;&#34987;&#20805;&#20998;&#30740;&#31350;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#21450;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#25968;&#37327;&#20247;&#22810;&#21644;&#23454;&#20307;&#30028;&#38480;&#27169;&#31946;&#65292;&#20351;&#24471;&#36825;&#39033;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DICE&#65292;&#19968;&#31181;&#31283;&#20581;&#12289;&#39640;&#25928;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#29983;&#25104;&#27169;&#22411;&#12290;DICE&#23558;&#20107;&#20214;&#25277;&#21462;&#20316;&#20026;&#26465;&#20214;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#23545;&#27604;&#24615;&#23398;&#20064;&#30446;&#26631;&#65292;&#20197;&#20934;&#30830;&#30830;&#23450;&#29983;&#29289;&#21307;&#23398;&#25552;&#21450;&#30340;&#36793;&#30028;&#12290;DICE&#36824;&#32852;&#21512;&#35757;&#32451;&#36741;&#21161;&#25552;&#21450;&#26631;&#35782;&#20219;&#21153;&#21644;&#20107;&#20214;&#25277;&#21462;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#30830;&#23450;&#23454;&#20307;&#25552;&#21450;&#36793;&#30028;&#65292;&#24182;&#36827;&#19968;&#27493;&#24341;&#20837;&#29305;&#27530;&#30340;&#26631;&#35760;&#26469;&#20316;&#20026;&#35302;&#21457;&#22120;&#21644;&#21442;&#25968;&#20505;&#36873;&#39033;&#65292;&#20197;&#21253;&#21547;&#20854;&#21508;&#33258;&#30340;&#20219;&#21153;&#20013;&#30340;&#30830;&#23450;&#23454;&#20307;&#38382;&#39064;&#12290;&#20026;&#20102;&#23545;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#26681;&#25454;&#29616;&#26377;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#25968;&#25454;&#38598;MACCRO&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#21442;&#25968;&#25209;&#27880;&#30340;&#20020;&#24202;&#20107;&#20214;&#25277;&#21462;&#25968;&#25454;&#38598;MACCROBAT-EE&#12290;
&lt;/p&gt;
&lt;p&gt;
Event extraction for the clinical domain is an under-explored research area. The lack of training data along with the high volume of domain-specific terminologies with vague entity boundaries makes the task especially challenging. In this paper, we introduce DICE, a robust and data-efficient generative model for clinical event extraction. DICE frames event extraction as a conditional generation problem and introduces a contrastive learning objective to accurately decide the boundaries of biomedical mentions. DICE also trains an auxiliary mention identification task jointly with event extraction tasks to better identify entity mention boundaries, and further introduces special markers to incorporate identified entity mentions as trigger and argument candidates for their respective tasks. To benchmark clinical event extraction, we compose MACCROBAT-EE, the first clinical event extraction dataset with argument annotation, based on an existing clinical information extraction dataset MACCRO
&lt;/p&gt;</description></item><item><title>Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2207.14116</link><description>&lt;p&gt;
Claim-Dissector: &#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction. (arXiv:2207.14116v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14116
&lt;/p&gt;
&lt;p&gt;
Claim-Dissector&#26159;&#19968;&#27454;&#32852;&#21512;&#37325;&#25490;&#21644;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#30340;&#20107;&#23454;&#26680;&#26597;&#31995;&#32479;&#65292;&#21487;&#20197;&#35782;&#21035;&#19982;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#24182;&#30830;&#23450;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#31995;&#32479;&#30340;&#20010;&#20154;&#36129;&#29486;&#20197;&#21450;&#35777;&#25454;&#25152;&#25903;&#25345;&#25110;&#21453;&#39539;&#22768;&#26126;&#30340;&#36129;&#29486;&#37117;&#21487;&#20197;&#34987;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Claim-Dissector&#65292;&#19968;&#31181;&#38024;&#23545;&#20107;&#23454;&#26680;&#26597;&#21644;&#20998;&#26512;&#30340;&#26032;&#22411;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#32473;&#20986;&#19968;&#20010;&#22768;&#26126;&#21644;&#19968;&#32452;&#26816;&#32034;&#21040;&#30340;&#35777;&#25454;&#65292;&#32852;&#21512;&#23398;&#20064;&#35782;&#21035;&#65306;&#65288;i&#65289;&#19982;&#32473;&#23450;&#22768;&#26126;&#30456;&#20851;&#30340;&#35777;&#25454;&#65292;&#65288;ii&#65289;&#22768;&#26126;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24314;&#35758;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#24320;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#21450;&#20854;&#23545;&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#30340;&#24433;&#21709;-&#26368;&#32456;&#30495;&#23454;&#24615;&#27010;&#29575;&#19982;&#27599;&#20010;&#35777;&#25454;&#30456;&#20851;&#24615;&#27010;&#29575;&#30340;&#32447;&#24615;&#25972;&#21512;&#25104;&#27604;&#20363;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#27599;&#20010;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#27010;&#29575;&#30340;&#20010;&#20154;&#36129;&#29486;&#12290;&#22312;&#27599;&#20010;&#35777;&#25454;&#30340;&#30456;&#20851;&#24615;&#27010;&#29575;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#21306;&#20998;&#27599;&#20010;&#30456;&#20851;&#35777;&#25454;&#26159;&#25903;&#25345;&#65288;S&#65289;&#36824;&#26159;&#21453;&#39539;&#65288;R&#65289;&#22768;&#26126;&#12290;&#36825;&#26679;&#21487;&#20197;&#37327;&#21270;S/R&#27010;&#29575;&#23545;&#26368;&#32456;&#32467;&#35770;&#30340;&#36129;&#29486;&#25110;&#26816;&#27979;&#26377;&#24322;&#35758;&#30340;&#35777;&#25454;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#31995;&#32479;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#22312;FEVER&#31454;&#36187;&#20013;&#65292;&#20854;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Claim-Dissector: a novel latent variable model for fact-checking and analysis, which given a claim and a set of retrieved evidences jointly learns to identify: (i) the relevant evidences to the given claim, (ii) the veracity of the claim. We propose to disentangle the per-evidence relevance probability and its contribution to the final veracity probability in an interpretable way -- the final veracity probability is proportional to a linear ensemble of per-evidence relevance probabilities. In this way, the individual contributions of evidences towards the final predicted probability can be identified. In per-evidence relevance probability, our model can further distinguish whether each relevant evidence is supporting (S) or refuting (R) the claim. This allows to quantify how much the S/R probability contributes to the final verdict or to detect disagreeing evidence.  Despite its interpretable nature, our system achieves results competitive with state-of-the-art on the FEVER 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2205.12505</link><description>&lt;p&gt;
GENEVA&#65306;&#8220;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#8221;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65292;&#28085;&#30422;&#25968;&#30334;&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;&#35770;&#20803;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
GENEVA: Benchmarking Generalizability for Event Argument Extraction with Hundreds of Event Types and Argument Roles. (arXiv:2205.12505v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;105&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#21253;&#21547;&#22312;&#20869;&#65292;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#21019;&#24314;&#20102;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;GENEVA&#65292;&#20849;&#21253;&#21547;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20107;&#20214;&#35770;&#20803;&#25552;&#21462;&#65288;EAE&#65289;&#30340;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#20197;&#36866;&#24212;&#26032;&#30340;&#20107;&#20214;&#31867;&#22411;&#21644;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#22914;ACE&#21644;ERE&#21482;&#28085;&#30422;&#19981;&#21040;40&#31181;&#20107;&#20214;&#31867;&#22411;&#21644;25&#31181;&#38754;&#21521;&#23454;&#20307;&#30340;&#35770;&#20803;&#35282;&#33394;&#12290;&#25968;&#25454;&#38598;&#30340;&#26377;&#38480;&#22810;&#26679;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#24433;&#21709;&#20102;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;EAE&#27169;&#22411;&#36890;&#29992;&#24615;&#30340;&#20805;&#20998;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#32780;&#20840;&#30340;EAE&#26412;&#20307;&#35770;&#65292;&#22312;FrameNet&#30340;&#22522;&#30784;&#19978;&#21019;&#24314;&#20102;&#21253;&#21547;115&#20010;&#20107;&#20214;&#21644;220&#20010;&#35770;&#20803;&#35282;&#33394;&#30340;&#26412;&#20307;&#35770;&#65292;&#20854;&#20013;&#35768;&#22810;&#35282;&#33394;&#19981;&#26159;&#23454;&#20307;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#26412;&#20307;&#35770;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;GENEVA&#65292;&#19968;&#31181;&#22810;&#26679;&#21270;&#30340;&#36890;&#29992;&#24615;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;&#27979;&#35797;&#22871;&#20214;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22788;&#29702;&#26377;&#38480;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works in Event Argument Extraction (EAE) have focused on improving model generalizability to cater to new events and domains. However, standard benchmarking datasets like ACE and ERE cover less than 40 event types and 25 entity-centric argument roles. Limited diversity and coverage hinder these datasets from adequately evaluating the generalizability of EAE models. In this paper, we first contribute by creating a large and diverse EAE ontology. This ontology is created by transforming FrameNet, a comprehensive semantic role labeling (SRL) dataset for EAE, by exploiting the similarity between these two tasks. Then, exhaustive human expert annotations are collected to build the ontology, concluding with 115 events and 220 argument roles, with a significant portion of roles not being entities. We utilize this ontology to further introduce GENEVA, a diverse generalizability benchmarking dataset comprising four test suites, aimed at evaluating models' ability to handle limited data a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#21464;&#38761;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#31185;&#23398;&#21457;&#29616;&#21644;&#20132;&#27969;&#12290;&#36825;&#20010;&#26694;&#26550;&#26377;&#24456;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#30340;&#21021;&#22987;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2205.02007</link><description>&lt;p&gt;
&#31185;&#23398;&#21457;&#29616;&#30340;&#35745;&#31639;&#21464;&#38761;
&lt;/p&gt;
&lt;p&gt;
A Computational Inflection for Scientific Discovery. (arXiv:2205.02007v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#21464;&#38761;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#24378;&#31185;&#23398;&#21457;&#29616;&#21644;&#20132;&#27969;&#12290;&#36825;&#20010;&#26694;&#26550;&#26377;&#24456;&#22810;&#24212;&#29992;&#22330;&#26223;&#65292;&#20316;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#30340;&#21021;&#22987;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27491;&#31449;&#22312;&#31185;&#23398;&#21457;&#29616;&#36712;&#36857;&#19978;&#19968;&#20010;&#37325;&#35201;&#30340;&#25296;&#28857;&#19978;&#12290;&#38543;&#30528;&#31038;&#20250;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#36716;&#22411;&#65292;&#20154;&#31867;&#30340;&#31185;&#23398;&#30693;&#35782;&#21644;&#20132;&#27969;&#20063;&#22312;&#25968;&#23383;&#21270;&#30340;&#24418;&#24335;&#19979;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#29616;&#22312;&#38405;&#35835;&#21644;&#25776;&#20889;&#30340;&#35770;&#25991;&#12289;&#39044;&#21360;&#26412;&#12289;&#20070;&#31821;&#12289;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#12289;&#20250;&#35758;&#28436;&#31034;&#31295;&#20197;&#21450;&#31038;&#20132;&#32593;&#32476;&#21644;&#21327;&#20316;&#21644;&#27807;&#36890;&#24179;&#21488;&#19978;&#30340;&#20132;&#20114;&#31561;&#65292;&#22823;&#22810;&#24050;&#32463;&#20197;&#25968;&#23383;&#21270;&#30340;&#26041;&#24335;&#35760;&#24405;&#12290;&#36825;&#31181;&#36716;&#21464;&#23548;&#33268;&#20102;&#22823;&#37327;&#20449;&#24687;&#30340;&#21019;&#36896;&#21644;&#22686;&#38271;&#8212;&#8212;&#20854;&#20013;&#24456;&#22810;&#24050;&#32463;&#21487;&#20379;&#20844;&#20247;&#33719;&#21462;&#8212;&#8212;&#20026;&#20998;&#26512;&#21644;&#21033;&#29992;&#20854;&#30340;&#35745;&#31639;&#27169;&#22411;&#21644;&#31995;&#32479;&#24320;&#21551;&#20102;&#20196;&#20154;&#28608;&#21160;&#30340;&#26426;&#36935;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25968;&#25454;&#22788;&#29702;&#33021;&#21147;&#30340;&#25351;&#25968;&#22686;&#38271;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#36827;&#27493;&#65292;&#21253;&#25324;&#33021;&#22815;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#23398;&#20064;&#24378;&#22823;&#34920;&#31034;&#30340;&#22823;&#22411;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#34892;&#37325;&#22823;&#25913;&#21464;&#65292;&#20197;&#22312;&#31185;&#23398;&#30693;&#35782;&#21644;&#20132;&#27969;&#30340;&#26356;&#22823;&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#25928;&#25972;&#21512;&#36825;&#20123;&#36827;&#23637;&#65292;&#21019;&#24314;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#30340;&#31185;&#23398;&#20132;&#27969;&#33539;&#24335;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31185;&#23398;&#21457;&#29616;&#30340;&#35745;&#31639;&#21464;&#38761;&#8212;&#8212;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#22686;&#24378;&#31185;&#23398;&#21457;&#29616;&#21644;&#20132;&#27969;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#28508;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#30340;&#21021;&#22987;&#23454;&#29616;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
We stand at the foot of a significant inflection in the trajectory of scientific discovery. As society continues on its fast-paced digital transformation, so does humankind's collective scientific knowledge and discourse. We now read and write papers in digitized form, and a great deal of the formal and informal processes of science are captured digitally -including papers, preprints and books, code and datasets, conference presentations, and interactions in social networks and collaboration and communication platforms. The transition has led to the creation and growth of a tremendous amount of information -- much of which is available for public access -- opening exciting opportunities for computational models and systems that analyze and harness it. In parallel, exponential growth in data processing power has fueled remarkable advances in artificial intelligence, including large neural language models capable of learning powerful representations from unstructured text. Dramatic cha
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#24863;&#30693;&#25351;&#20196;&#32593;&#32476;TIN-SLT&#29992;&#20110;&#25163;&#35821;&#32763;&#35793;&#65292;&#24341;&#20837;&#25351;&#20196;&#27169;&#22359;&#21644;&#29305;&#24449;&#34701;&#21512;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#22810;&#23618;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#35843;&#25972;&#20102;&#25968;&#25454;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2204.05953</link><description>&lt;p&gt;
&#25506;&#32034;&#26356;&#22810;&#30340;&#25351;&#23548;&#65306;&#19968;&#31181;&#22686;&#24378;&#25968;&#25454;&#22686;&#24378;&#30340;&#20219;&#21153;&#24863;&#30693;&#25351;&#20196;&#32593;&#32476;&#29992;&#20110;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Explore More Guidance: A Task-aware Instruction Network for Sign Language Translation Enhanced with Data Augmentation. (arXiv:2204.05953v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20219;&#21153;&#24863;&#30693;&#25351;&#20196;&#32593;&#32476;TIN-SLT&#29992;&#20110;&#25163;&#35821;&#32763;&#35793;&#65292;&#24341;&#20837;&#25351;&#20196;&#27169;&#22359;&#21644;&#29305;&#24449;&#34701;&#21512;&#31574;&#30053;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32763;&#35793;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#22810;&#23618;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#35843;&#25972;&#20102;&#25968;&#25454;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#35821;&#35782;&#21035;&#21644;&#32763;&#35793;&#39318;&#20808;&#20351;&#29992;&#35782;&#21035;&#27169;&#22359;&#20174;&#25163;&#35821;&#35270;&#39057;&#20013;&#29983;&#25104;&#25163;&#35821;&#35789;&#27719;&#65292;&#28982;&#21518;&#20351;&#29992;&#32763;&#35793;&#27169;&#22359;&#23558;&#25163;&#35821;&#35789;&#27719;&#32763;&#35793;&#25104;&#21475;&#35821;&#21477;&#23376;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25163;&#35821;&#32763;&#35793;&#30340;&#20219;&#21153;&#24863;&#30693;&#25351;&#20196;&#32593;&#32476;TIN-SLT&#65292;&#36890;&#36807;&#23558;&#25351;&#20196;&#27169;&#22359;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#29305;&#24449;&#34701;&#21512;&#31574;&#30053;&#24341;&#20837;Transformer&#32593;&#32476;&#12290;&#36825;&#26679;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#35821;&#35328;&#33021;&#21147;&#21487;&#20197;&#34987;&#20805;&#20998;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#32763;&#35793;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25506;&#32034;&#25163;&#35821;&#35789;&#27719;&#21644;&#30446;&#26631;&#21475;&#35821;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#65292;&#35843;&#25972;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#20998;&#24067;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25361;&#25112;&#24615;&#22522;&#20934;&#25968;&#25454;&#38598;PHOENIX-2014-T&#21644;ASLG-PC12&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sign language recognition and translation first uses a recognition module to generate glosses from sign language videos and then employs a translation module to translate glosses into spoken sentences. Most existing works focus on the recognition step, while paying less attention to sign language translation. In this work, we propose a task-aware instruction network, namely TIN-SLT, for sign language translation, by introducing the instruction module and the learning-based feature fuse strategy into a Transformer network. In this way, the pre-trained model's language ability can be well explored and utilized to further boost the translation performance. Moreover, by exploring the representation space of sign language glosses and target spoken language, we propose a multi-level data augmentation scheme to adjust the data distribution of the training set. We conduct extensive experiments on two challenging benchmark datasets, PHOENIX-2014-T and ASLG-PC12, on which our method outperforms 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2203.07648</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Contrastive Learning of Sociopragmatic Meaning in Social Media. (arXiv:2203.07648v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07648
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#23186;&#20307;&#20013;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#22312;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#31561;&#30740;&#31350;&#36827;&#23637;&#23578;&#26410;&#24191;&#27867;&#32771;&#34385;&#31038;&#20250;&#35821;&#29992;&#24847;&#20041;&#36825;&#19968;&#31867;&#21035;&#65288;&#21363;&#19981;&#21516;&#35821;&#35328;&#31038;&#21306;&#20869;&#30340;&#20132;&#27969;&#24847;&#20041;&#65289;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#21487;&#36801;&#31227;&#33267;&#21508;&#31181;&#31038;&#20250;&#35821;&#29992;&#20219;&#21153;&#65288;&#22914;&#24773;&#24863;&#12289;&#20167;&#24680;&#35328;&#35770;&#12289;&#24189;&#40664;&#12289;&#35773;&#21050;&#65289;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#39046;&#22495;&#20869;&#21644;&#39046;&#22495;&#22806;&#25968;&#25454;&#20197;&#21450;&#19968;&#33324;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#30340;&#21508;&#31181;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#20363;&#22914;&#65292;&#19982;&#20004;&#20010;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#20165;&#29992;20&#20010;&#35757;&#32451;&#26679;&#26412;&#24494;&#35843;&#26102;&#65292;&#24179;&#22343;F1&#20540;&#22312;16&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;11.66&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress in representation and contrastive learning in NLP has not widely considered the class of \textit{sociopragmatic meaning} (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our method obtains an improvement of $11.66$ average $F_1$ on $16$ datasets when fine-tuned on only $20$ training samples per dataset.
&lt;/p&gt;</description></item><item><title>HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;</title><link>http://arxiv.org/abs/2203.03691</link><description>&lt;p&gt;
HyperMixer&#65306;&#19968;&#31181;&#22522;&#20110;MLP&#30340;&#20302;&#25104;&#26412;Transformer&#26367;&#20195;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
HyperMixer: An MLP-based Low Cost Alternative to Transformers. (arXiv:2203.03691v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.03691
&lt;/p&gt;
&lt;p&gt;
HyperMixer&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#22522;&#20110;MLP&#30340;Transformer&#26367;&#20195;&#26041;&#26696;&#65292;&#36890;&#36807;&#21160;&#24577;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#26469;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20854;&#24615;&#33021;&#27604;&#26367;&#20195;&#26041;&#26696;&#22909;&#65292;&#24182;&#21487;&#19982;Transformer&#23218;&#32654;&#65292;&#25104;&#26412;&#26356;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#25104;&#26412;&#30456;&#24403;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36755;&#20837;&#38271;&#24230;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#21487;&#33021;&#38590;&#20197;&#35843;&#25972;&#12290;&#20026;&#20102;&#38477;&#20302;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31616;&#21333;&#30340;&#22522;&#20110;MLP&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#26550;&#26500;&#65288;&#20363;&#22914;MLPMixer&#65289;&#36890;&#36807;&#38745;&#24577;&#30340;MLP&#29420;&#31435;&#22320;&#24212;&#29992;&#20110;&#27599;&#20010;&#29305;&#24449;&#65292;&#32780;&#36807;&#20110;&#33073;&#31163;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25152;&#38656;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#25913;&#36827;&#65292;&#21363;HyperMixer&#65292;&#23427;&#20351;&#29992;&#36229;&#32593;&#32476;&#21160;&#24577;&#22320;&#24418;&#25104;&#26631;&#35760;&#28151;&#21512;MLP&#12290;&#23454;&#39564;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#26367;&#20195;&#30340;&#22522;&#20110;MLP&#30340;&#27169;&#22411;&#65292;&#24182;&#19982;Transformer&#23218;&#32654;&#12290;&#19982;Transformer&#19981;&#21516;&#65292;HyperMixer&#22312;&#22788;&#29702;&#26102;&#38388;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#38754;&#20855;&#26377;&#22823;&#22823;&#38477;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.
&lt;/p&gt;</description></item><item><title>pNLP-Mixer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;MLP-Mixer&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#20197;&#36798;&#21040;&#22522;&#20110;transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21364;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2202.04350</link><description>&lt;p&gt;
pNLP-Mixer&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#20840;MLP&#26550;&#26500;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
pNLP-Mixer: an Efficient all-MLP Architecture for Language. (arXiv:2202.04350v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.04350
&lt;/p&gt;
&lt;p&gt;
pNLP-Mixer&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;MLP-Mixer&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#35774;&#22791;&#19978;&#39640;&#25928;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#21487;&#20197;&#36798;&#21040;&#22522;&#20110;transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#36817;&#30340;&#24615;&#33021;&#65292;&#21364;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#39046;&#22495;&#30340;&#26684;&#23616;&#12290;&#28982;&#32780;&#65292;&#22312;&#26234;&#33021;&#25163;&#34920;&#31561;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#23436;&#20840;&#19981;&#21487;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22823;&#23567;&#21644;&#25512;&#29702;&#25104;&#26412;&#12290;&#20316;&#20026;Transformer&#26550;&#26500;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#26368;&#36817;&#20851;&#20110;&#39640;&#25928;NLP&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26435;&#37325;&#39640;&#25928;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#20806;&#23383;&#33410;&#32423;&#30340;&#27169;&#22411;&#22823;&#23567;&#20013;&#33719;&#24471;&#31616;&#21333;&#20219;&#21153;(&#22914;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;)&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;pNLP-Mixer&#26550;&#26500;&#65292;&#19968;&#31181;&#29992;&#20110;&#35774;&#22791;&#19978;NLP&#30340;&#26080;&#23884;&#20837;MLP-Mixer&#27169;&#22411;&#65292;&#30001;&#20110;&#37319;&#29992;&#20102;&#26032;&#39062;&#30340;&#25237;&#24433;&#23618;&#65292;&#22240;&#27492;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#26435;&#37325;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22810;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;MTOP&#21644;multiATIS&#19978;&#35780;&#20272;&#20102;&#19968;&#20010;&#22823;&#23567;&#20165;&#20026;1&#20806;&#23383;&#33410;&#30340;pNLP-Mixer&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#37327;&#21270;&#27169;&#22411;&#22312;MTOP&#21644;multi-ATIS&#19978;&#23454;&#29616;&#20102;mBERT&#30340;99.4&#65285;&#21644;97.8&#65285;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#36164;&#28304;&#20165;&#20026;mBERT&#30340;170&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models based on transformer architecture have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using 170x fewer 
&lt;/p&gt;</description></item></channel></rss>