<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13804</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#23398;&#20064;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Learning from Models and Data for Visual Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13804
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SynGround&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20174;&#21508;&#31181;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;&#20174;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#30693;&#35782;&#20256;&#36882;&#24341;&#21457;&#20102;&#36890;&#36807;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#22120;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#20123;&#25551;&#36848;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#23427;&#20204;&#20316;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21512;&#25104;&#22270;&#20687;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#20316;&#20026;&#26597;&#35810;&#26469;&#21512;&#25104;&#25991;&#26412;&#65292;&#20174;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30701;&#35821;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24320;&#25918;&#35789;&#27719;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#20026;&#21512;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#26694;&#12290;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#36974;&#32617;-&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#30446;&#26631;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30446;&#26631;&#23558;&#21306;&#22495;&#27880;&#37322;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#23545;&#40784;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#25552;&#21319;&#20102;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13804v1 Announce Type: cross  Abstract: We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model. The knowledge transfer from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model. Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the grounding capabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13802</link><description>&lt;p&gt;
ZigMa&#65306;&#34623;&#34578;&#26364;&#24052;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ZigMa: Zigzag Mamba Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#32416;&#27491;&#24403;&#21069;Mamba-based&#35270;&#35273;&#26041;&#27861;&#20013;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#24573;&#35270;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#36895;&#24230;&#21644;&#20869;&#23384;&#21033;&#29992;&#65292;&#21516;&#26102;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#21463;&#21040;&#21487;&#20280;&#32553;&#24615;&#21644;&#20108;&#27425;&#22797;&#26434;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#32467;&#26500;&#20869;&#37096;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#21033;&#29992;&#19968;&#31181;&#31216;&#20026;&#26364;&#24052;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#65292;&#20197;&#25193;&#23637;&#20854;&#22312;&#35270;&#35273;&#25968;&#25454;&#29983;&#25104;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22823;&#22810;&#25968;&#24403;&#21069;&#22522;&#20110;&#26364;&#24052;&#30340;&#35270;&#35273;&#26041;&#27861;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#30095;&#24573;&#65292;&#21363;&#26364;&#24052;&#30340;&#25195;&#25551;&#26041;&#26696;&#20013;&#32570;&#20047;&#23545;&#31354;&#38388;&#36830;&#32493;&#24615;&#30340;&#32771;&#34385;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Zigzag Mamba&#30340;&#31616;&#21333;&#12289;&#21363;&#25554;&#21363;&#29992;&#12289;&#38646;&#21442;&#25968;&#26041;&#27861;&#65292;&#23427;&#20248;&#20110;&#22522;&#20110;&#26364;&#24052;&#30340;&#22522;&#32447;&#65292;&#24182;&#34920;&#29616;&#20986;&#27604;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#22522;&#32447;&#26356;&#24555;&#36895;&#21644;&#26356;&#22909;&#30340;&#20869;&#23384;&#21033;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;Zigzag Mamba&#38598;&#25104;&#21040;&#38543;&#26426;&#25554;&#20540;&#26694;&#26550;&#20013;&#65292;&#20197;&#30740;&#31350;&#27169;&#22411;&#22312;&#22823;&#20998;&#36776;&#29575;&#35270;&#35273;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;FacesHQ $1024\times 1024$&#21644;UCF101&#65292;MultiModal-CelebA-HQ&#65289;&#19978;&#30340;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13802v1 Announce Type: cross  Abstract: The diffusion model has long been plagued by scalability and quadratic complexity issues, especially within transformer-based structures. In this study, we aim to leverage the long sequence modeling capability of a State-Space Model called Mamba to extend its applicability to visual data generation. Firstly, we identify a critical oversight in most current Mamba-based vision methods, namely the lack of consideration for spatial continuity in the scan scheme of Mamba. Secondly, building upon this insight, we introduce a simple, plug-and-play, zero-parameter method named Zigzag Mamba, which outperforms Mamba-based baselines and demonstrates improved speed and memory utilization compared to transformer-based baselines. Lastly, we integrate Zigzag Mamba with the Stochastic Interpolant framework to investigate the scalability of the model on large-resolution visual datasets, such as FacesHQ $1024\times 1024$ and UCF101, MultiModal-CelebA-HQ
&lt;/p&gt;</description></item><item><title>&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#22352;&#26631;&#32423;&#25511;&#21046;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13801</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#25919;&#31574;&#65306;&#19982;LLMs&#19968;&#36215;&#36827;&#34892;&#22352;&#26631;&#32423;&#20307;&#24577;&#25511;&#21046;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Natural Language as Polices: Reasoning for Coordinate-Level Embodied Control with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13801
&lt;/p&gt;
&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#36827;&#34892;&#22352;&#26631;&#32423;&#25511;&#21046;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#36801;&#31227;&#21040;&#26032;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;LLMs&#19968;&#36215;&#35299;&#20915;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#38382;&#39064;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#26368;&#36817;&#65292;LLMs&#24050;&#32463;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#34892;&#21160;&#35268;&#21010;&#65292;&#29305;&#21035;&#26159;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#26041;&#27861;&#23558;&#22797;&#26434;&#30340;&#39640;&#32423;&#25351;&#20196;&#36716;&#25442;&#20026;&#20013;&#32423;&#31574;&#30053;&#20195;&#30721;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#33719;&#21462;&#20219;&#21153;&#21644;&#22330;&#26223;&#23545;&#35937;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21046;&#23450;&#34892;&#21160;&#35268;&#21010;&#65292;&#24182;&#36755;&#20986;&#22352;&#26631;&#32423;&#25511;&#21046;&#21629;&#20196;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20316;&#20026;&#25919;&#31574;&#30340;&#20013;&#38388;&#34920;&#31034;&#20195;&#30721;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#25552;&#31034;&#20223;&#30495;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#34920;&#26126;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#23454;&#39564;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#19982;&#32570;&#24109;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26377;&#28508;&#21147;&#23558;&#26426;&#22120;&#20154;&#25216;&#33021;&#20174;&#24050;&#30693;&#20219;&#21153;&#36716;&#31227;&#21040;&#20197;&#21069;&#26410;&#35265;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13801v1 Announce Type: cross  Abstract: We demonstrate experimental results with LLMs that address robotics action planning problems. Recently, LLMs have been applied in robotics action planning, particularly using a code generation approach that converts complex high-level instructions into mid-level policy codes. In contrast, our approach acquires text descriptions of the task and scene objects, then formulates action planning through natural language reasoning, and outputs coordinate level control commands, thus reducing the necessity for intermediate representation code as policies. Our approach is evaluated on a multi-modal prompt simulation benchmark, demonstrating that our prompt engineering experiments with natural language reasoning significantly enhance success rates compared to its absence. Furthermore, our approach illustrates the potential for natural language descriptions to transfer robotics skills from known tasks to previously unseen tasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13799</link><description>&lt;p&gt;
&#36870;&#21521;&#35757;&#32451;&#20197;&#28040;&#38500;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Reverse Training to Nurse the Reversal Curse
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13799
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#20445;&#30041;&#36873;&#23450;&#23376;&#20018;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23384;&#22312;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#22833;&#36133;&#29616;&#35937;&#65306;&#24403;&#35757;&#32451;&#27169;&#22411;&#20197;"A&#20855;&#26377;&#29305;&#24449;B"&#20026;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#26080;&#27861;&#27867;&#21270;&#21040;"B&#26159;A&#30340;&#29305;&#24449;"&#65292;&#36825;&#34987;&#31216;&#20026;&#36870;&#36716;&#35781;&#21650;&#12290;&#21363;&#20351;&#22312;&#20351;&#29992;&#25968;&#19975;&#20159;&#20196;&#29260;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#30001;&#20110;&#40784;&#22827;&#23450;&#24459;&#30340;&#23384;&#22312;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#24847;&#21619;&#30528;&#21363;&#20351;&#25105;&#20204;&#22312;&#25972;&#20010;&#20114;&#32852;&#32593;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#38382;&#39064;&#20173;&#28982;&#20250;&#20986;&#29616;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36870;&#21521;&#35757;&#32451;&#30340;&#26367;&#20195;&#35757;&#32451;&#26041;&#26696;&#65292;&#22312;&#20854;&#20013;&#25152;&#26377;&#21333;&#35789;&#34987;&#20351;&#29992;&#20004;&#27425;&#65292;&#20174;&#32780;&#20351;&#21487;&#29992;&#20196;&#29260;&#25968;&#37327;&#21152;&#20493;&#12290;&#35813;LLM&#22312;&#27491;&#21521;&#21644;&#36870;&#21521;&#26041;&#21521;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#39072;&#20498;&#35757;&#32451;&#23383;&#31526;&#20018;&#26469;&#39072;&#20498;&#35757;&#32451;&#36807;&#31243;&#65292;&#21516;&#26102;&#20445;&#30041;&#65288;&#21363;&#19981;&#39072;&#20498;&#65289;&#36873;&#23450;&#30340;&#23376;&#20018;&#65292;&#22914;&#23454;&#20307;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#26631;&#20934;&#20219;&#21153;&#19978;&#27604;&#26631;&#20934;&#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#31168;&#65292;&#24182;&#19988;&#35745;&#31639;&#21305;&#37197;&#30340;&#36870;&#21521;&#35757;&#32451;&#27169;&#22411;&#22312;&#36870;&#36716;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36828;&#36828;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13799v1 Announce Type: new  Abstract: Large language models (LLMs) have a surprising failure: when trained on "A has a feature B", they do not generalize to "B is a feature of A", which is termed the Reversal Curse. Even when training with trillions of tokens this issue still appears due to Zipf's law - hence even if we train on the entire internet. This work proposes an alternative training scheme, called reverse training, whereby all words are used twice, doubling the amount of available tokens. The LLM is trained in both forward and reverse directions by reversing the training strings while preserving (i.e., not reversing) chosen substrings, such as entities. We show that data-matched reverse-trained models provide superior performance to standard models on standard tasks, and compute-matched reverse-trained models provide far superior performance on reversal tasks, helping resolve the reversal curse issue.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;Chain-of-Interaction (CoI)&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#24773;&#22659;&#26469;&#20026;&#31934;&#31070;&#20915;&#31574;&#25903;&#25345;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20197;&#35299;&#20915;&#31934;&#31070;&#27835;&#30103;&#20013;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#24573;&#35270;&#24739;&#32773;-&#27835;&#30103;&#24072;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13786</link><description>&lt;p&gt;
Chain-of-Interaction: &#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#32972;&#26223;&#22686;&#24378;&#29992;&#20110;&#31934;&#31070;&#34892;&#20026;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#8220;Chain-of-Interaction (CoI)&#8221;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#24773;&#22659;&#26469;&#20026;&#31934;&#31070;&#20915;&#31574;&#25903;&#25345;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#20197;&#35299;&#20915;&#31934;&#31070;&#27835;&#30103;&#20013;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#24573;&#35270;&#24739;&#32773;-&#27835;&#30103;&#24072;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32534;&#30721;&#24739;&#32773;&#34892;&#20026;&#23545;&#20110;&#25903;&#25345;&#31934;&#31070;&#27835;&#30103;&#24072;&#22312;&#28608;&#21169;&#24615;&#38754;&#35848;&#65288;MI&#65289;&#26399;&#38388;&#20570;&#20986;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;MI&#26159;&#19968;&#31181;&#21327;&#20316;&#27807;&#36890;&#24178;&#39044;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31934;&#31070;&#38382;&#39064;&#65292;&#22914;&#37202;&#31934;&#21644;&#33647;&#29289;&#25104;&#30270;&#12290;&#23613;&#31649;&#34892;&#20026;&#32534;&#30721;&#20219;&#21153;&#24050;&#36805;&#36895;&#20511;&#21161;&#26426;&#22120;&#23398;&#20064;&#26469;&#39044;&#27979;MI&#20250;&#35805;&#26399;&#38388;&#24739;&#32773;&#29366;&#24577;&#65292;&#20294;&#32570;&#20047;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#24182;&#24573;&#35270;&#24739;&#32773;-&#27835;&#30103;&#24072;&#20132;&#20114;&#26159;&#22312;&#23454;&#38469;&#23454;&#36341;&#20013;&#24320;&#21457;&#21644;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Chain-of-Interaction (CoI)&#25552;&#31034;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20108;&#20803;&#20132;&#20114;&#24773;&#22659;&#26469;&#20026;&#31934;&#31070;&#20915;&#31574;&#25903;&#25345;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12290;CoI&#25552;&#31034;&#26041;&#27861;&#31995;&#32479;&#22320;&#23558;&#32534;&#30721;&#20219;&#21153;&#20998;&#35299;&#20026;&#19977;&#20010;&#20851;&#38190;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#21462;&#24739;&#32773;&#21442;&#19982;&#24230;&#65292;&#23398;&#20064;&#27835;&#30103;&#24072;&#25552;&#38382;&#31574;&#30053;&#65292;&#24182;&#25972;&#21512;&#20108;&#20803;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13786v1 Announce Type: new  Abstract: Automatic coding patient behaviors is essential to support decision making for psychotherapists during the motivational interviewing (MI), a collaborative communication intervention approach to address psychiatric issues, such as alcohol and drug addiction. While the behavior coding task has rapidly adapted machine learning to predict patient states during the MI sessions, lacking of domain-specific knowledge and overlooking patient-therapist interactions are major challenges in developing and deploying those models in real practice. To encounter those challenges, we introduce the Chain-of-Interaction (CoI) prompting method aiming to contextualize large language models (LLMs) for psychiatric decision support by the dyadic interactions. The CoI prompting approach systematically breaks down the coding task into three key reasoning steps, extract patient engagement, learn therapist question strategies, and integrates dyadic interactions bet
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#31934;&#28860;&#29983;&#25104;&#22120;</title><link>https://arxiv.org/abs/2403.13780</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#20449;&#24687;&#35770;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Information-Theoretic Distillation for Reference-less Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#23454;&#29616;&#20102;&#26080;&#21442;&#32771;&#25688;&#35201;&#30340;&#31934;&#28860;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#33258;&#21160;&#25688;&#35201;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#20351;&#29992;&#19987;&#26377;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#65292;&#25110;&#32773;&#20174;&#23427;&#20204;&#20316;&#20026;&#25945;&#24072;&#27169;&#22411;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoSumm&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#36890;&#36807;&#20449;&#24687;&#35770;&#30446;&#26631;&#36827;&#34892;&#31934;&#28860;&#24378;&#22823;&#30340;&#25688;&#35201;&#29983;&#25104;&#22120;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;LLM&#30340;&#33021;&#21147;&#25110;&#20154;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13780v1 Announce Type: new  Abstract: The current winning recipe for automatic summarization is using proprietary large-scale language models (LLMs) such as ChatGPT as is, or imitation learning from them as teacher models. While increasingly ubiquitous dependence on such large-scale language models is convenient, there remains an important question of whether small-scale models could have achieved competitive results, if we were to seek an alternative learning method -- that allows for a more cost-efficient, controllable, yet powerful summarizer. We present InfoSumm, a novel framework to distill a powerful summarizer based on the information-theoretic objective for summarization, without relying on either the LLM's capability or human-written references. To achieve this, we first propose a novel formulation of the desiderata of summarization (saliency, faithfulness and brevity) through the lens of mutual information between the original document and the summary. Based on thi
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#26696;&#22312;&#35199;&#29677;&#29273;&#35821;&#25968;&#35789;&#19968;&#33268;&#24615;&#20013;&#34920;&#29616;&#30456;&#20284;&#65292;&#32467;&#26524;&#34920;&#26126;&#35789;&#24418;&#23545;&#40784;&#30340;&#20998;&#35789;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#20294;&#24182;&#38750;&#23545;&#24615;&#33021;&#32477;&#23545;&#24517;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.13754</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#26696;&#22312;&#35199;&#29677;&#29273;&#35821;&#25968;&#35789;&#19968;&#33268;&#24615;&#20013;&#34920;&#29616;&#30456;&#20284;
&lt;/p&gt;
&lt;p&gt;
Different Tokenization Schemes Lead to Comparable Performance in Spanish Number Agreement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13754
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#35789;&#26041;&#26696;&#22312;&#35199;&#29677;&#29273;&#35821;&#25968;&#35789;&#19968;&#33268;&#24615;&#20013;&#34920;&#29616;&#30456;&#20284;&#65292;&#32467;&#26524;&#34920;&#26126;&#35789;&#24418;&#23545;&#40784;&#30340;&#20998;&#35789;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#20294;&#24182;&#38750;&#23545;&#24615;&#33021;&#32477;&#23545;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#35789;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#19981;&#21516;&#20998;&#35789;&#26041;&#26696;&#23545;&#35199;&#29677;&#29273;&#35821;&#22797;&#25968;&#19968;&#33268;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35789;&#24418;&#23545;&#40784;&#30340;&#20998;&#35789;&#26041;&#26696;&#19979;&#65292;&#24615;&#33021;&#19982;&#20854;&#20182;&#20998;&#35789;&#26041;&#26696;&#31867;&#20284;&#65292;&#21363;&#20351;&#23545;&#20110;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#20250;&#20197;&#36825;&#31181;&#26041;&#24335;&#20998;&#35789;&#30340;&#35789;&#36827;&#34892;&#20154;&#24037;&#35825;&#23548;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25506;&#32034;&#24615;&#20998;&#26512;&#32467;&#26524;&#65292;&#34920;&#26126;&#19981;&#21516;&#22797;&#25968;&#20998;&#35789;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#22312;&#26368;&#22823;&#21306;&#20998;&#21333;&#25968;&#21517;&#35789;&#21644;&#22797;&#25968;&#21517;&#35789;&#30340;&#23884;&#20837;&#31354;&#38388;&#36724;&#19978;&#20855;&#26377;&#30456;&#20284;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#24418;&#23545;&#40784;&#30340;&#20998;&#35789;&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#20998;&#35789;&#26041;&#27861;&#65292;&#32780;&#29616;&#26377;&#27169;&#22411;&#24050;&#32463;&#23558;&#26576;&#20123;&#35789;&#24418;&#27169;&#24335;&#25512;&#24191;&#21040;&#26032;&#39033;&#30446;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24418;&#24577;&#20998;&#35789;&#24182;&#38750;&#23545;&#24615;&#33021;&#32477;&#23545;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13754v1 Announce Type: new  Abstract: The relationship between language model tokenization and performance is an open area of research. Here, we investigate how different tokenization schemes impact number agreement in Spanish plurals. We find that morphologically-aligned tokenization performs similarly to other tokenization schemes, even when induced artificially for words that would not be tokenized that way during training. We then present exploratory analyses demonstrating that language model embeddings for different plural tokenizations have similar distributions along the embedding space axis that maximally distinguishes singular and plural nouns. Our results suggest that morphologically-aligned tokenization is a viable tokenization approach, and existing models already generalize some morphological patterns to new items. However, our results indicate that morphological tokenization is not strictly required for performance.
&lt;/p&gt;</description></item><item><title>EthioLLM&#20026;&#22467;&#22622;&#20420;&#27604;&#20122;&#20116;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#20197;&#21450;&#33521;&#35821;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Ethiobenchmark&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13737</link><description>&lt;p&gt;
EthioLLM&#65306;&#29992;&#20110;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21450;&#20219;&#21153;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
EthioLLM: Multilingual Large Language Models for Ethiopian Languages with Task Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13737
&lt;/p&gt;
&lt;p&gt;
EthioLLM&#20026;&#22467;&#22622;&#20420;&#27604;&#20122;&#20116;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#20197;&#21450;&#33521;&#35821;&#24341;&#20837;&#20102;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;Ethiobenchmark&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26469;&#22240;&#20854;&#22312;&#21508;&#31181;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22791;&#21463;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35757;&#32451;LLMs&#30340;&#36164;&#28304;&#19981;&#36275;&#65292;&#20302;&#36164;&#28304;&#35821;&#35328;&#20173;&#33853;&#21518;&#20110;NLP&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#25317;&#26377;&#26174;&#33879;&#30340;&#35821;&#35328;&#22810;&#26679;&#24615;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#25991;&#23383;&#31995;&#32479;&#65292;&#24182;&#23500;&#26377;&#28145;&#36828;&#30340;&#23447;&#25945;&#21644;&#25991;&#21270;&#24847;&#20041;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EthioLLM - &#20116;&#31181;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#30422;&#20234;&#20857;&#35821;&#12289;&#38463;&#26041;&#22885;&#32599;&#33707;&#35821;&#12289;&#32034;&#39532;&#37324;&#35821;&#21644;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#65289;&#21644;&#33521;&#35821;&#30340;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#21450;Ethiobenchmark - &#29992;&#20110;&#21508;&#31181;&#19979;&#28216;NLP&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20116;&#20010;&#19979;&#28216;NLP&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24320;&#28304;&#25105;&#20204;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#12289;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#31934;&#35843;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13737v1 Announce Type: new  Abstract: Large language models (LLMs) have gained popularity recently due to their outstanding performance in various downstream Natural Language Processing (NLP) tasks. However, low-resource languages are still lagging behind current state-of-the-art (SOTA) developments in the field of NLP due to insufficient resources to train LLMs. Ethiopian languages exhibit remarkable linguistic diversity, encompassing a wide array of scripts, and are imbued with profound religious and cultural significance. This paper introduces EthioLLM -- multilingual large language models for five Ethiopian languages (Amharic, Ge'ez, Afan Oromo, Somali, and Tigrinya) and English, and Ethiobenchmark -- a new benchmark dataset for various downstream NLP tasks. We evaluate the performance of these models across five downstream NLP tasks. We open-source our multilingual language models, new benchmark datasets for various downstream tasks, and task-specific fine-tuned languag
&lt;/p&gt;</description></item><item><title>PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13681</link><description>&lt;p&gt;
PARAMANU-AYN&#65306;&#19968;&#31181;&#26377;&#25928;&#30340;&#26032;&#22411;&#29983;&#25104;&#24335;&#12289;&#38754;&#21521;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN: An Efficient Novel Generative and Instruction-tuned Language Model for Indian Legal Case Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13681
&lt;/p&gt;
&lt;p&gt;
PARAMANU-AYN&#26159;&#19968;&#31181;&#22522;&#20110;&#21360;&#24230;&#27861;&#24459;&#26696;&#20363;&#25991;&#20214;&#30340;&#39640;&#25928;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#37319;&#29992;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#32463;&#36807;&#38754;&#21521;&#25351;&#20196;&#30340;&#24494;&#35843;&#65292;&#22312;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PARAMANU-AYN&#65292;&#36825;&#26159;&#19968;&#20010;&#20165;&#22522;&#20110;&#21360;&#24230;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#25991;&#20214;&#12289;&#21360;&#24230;&#23466;&#27861;&#21644;&#21360;&#24230;&#21009;&#27861;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#22120;&#30340;&#27169;&#22411;&#26159;&#20174;&#22836;&#24320;&#22987;&#22312;&#19978;&#19979;&#25991;&#22823;&#23567;&#20026;8192&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#22312;&#22256;&#24785;&#24230;&#25351;&#26631;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27861;&#24459;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#19968;&#32452;&#21253;&#25324;&#21508;&#31181;&#27861;&#24459;&#20219;&#21153;&#65288;&#22914;&#27861;&#24459;&#25512;&#29702;&#12289;&#21028;&#20915;&#35299;&#37322;&#12289;&#27861;&#24459;&#26465;&#27454;&#29983;&#25104;&#12289;&#27861;&#24459;&#33609;&#25311;&#12289;&#27861;&#24459;&#21512;&#21516;&#33609;&#25311;&#12289;&#26696;&#20214;&#25688;&#35201;&#12289;&#23466;&#27861;&#38382;&#39064;&#22238;&#31572;&#31561;&#65289;&#30340;10,763&#26465;&#25351;&#20196;&#36827;&#34892;&#20102;&#38024;&#23545;&#24615;&#35757;&#32451;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;GPT-3.5-Turbo&#23545;&#38754;&#21521;&#25351;&#20196;&#30340;&#27169;&#22411;&#30340;&#25552;&#31034;&#21709;&#24212;&#36827;&#34892;&#20102;&#22312;10&#20998;&#21046;&#24230;&#19978;&#30340;&#28165;&#26224;&#24230;&#12289;&#30456;&#20851;&#24615;&#12289;&#23436;&#25972;&#24615;&#21644;&#27861;&#24459;&#25512;&#29702;&#25351;&#26631;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;CPU&#19978;&#36816;&#34892;&#65292;&#24182;&#23454;&#29616;&#27599;&#31186;42.46&#20010;&#20196;&#29260;&#30340;CPU&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13681v1 Announce Type: new  Abstract: In this paper, we present PARAMANU-AYN, a language model based exclusively on case documents of the Supreme Court of India, the Constitution of India, and the Indian Penal Code. The novel Auto Regressive (AR) decoder based model is pretrained from scratch at a context size of 8192. We evaluated our pretrained legal model on perplexity metrics. We also instruction-tuned our pretrained model on a set of 10,763 instructions covering various legal tasks such as legal reasoning, judgement explanation, legal clause generation, legal drafting, legal contract drafting, case summarization, constitutional question-answering, etc. We also evaluated the responses of prompts for instruction-tuned models by GPT-3.5-Turbo on clarity, relevance, completeness, and legal reasoning metrics in a scale of 10. Our model can be run on CPU and achieved 42.46 tokens/sec CPU inference speed. We found that our models, despite not being pretrained on legal books, v
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.13679</link><description>&lt;p&gt;
RoleInteract&#65306;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;&#30340;&#31038;&#20132;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13679
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#21508;&#31181;AI&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#27169;&#20223;&#19981;&#21516;&#35282;&#33394;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RoleInteract&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#22312;&#31038;&#20132;&#26041;&#38754;&#34920;&#29616;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#20174;&#21508;&#31181;&#26469;&#28304;&#26500;&#24314;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#22810;&#36718;&#35282;&#33394;&#25198;&#28436;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in
&lt;/p&gt;</description></item><item><title>&#32431;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#35937;&#20301;&#32622;&#20449;&#24687;&#24182;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#26469;&#23398;&#20064;&#33853;&#23454;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#39044;&#35757;&#32451;LM&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13666</link><description>&lt;p&gt;
&#22312;&#32431;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#20013;&#33853;&#23454;&#31354;&#38388;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Grounding Spatial Relations in Text-Only Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13666
&lt;/p&gt;
&lt;p&gt;
&#32431;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#35937;&#20301;&#32622;&#20449;&#24687;&#24182;&#32463;&#36807;&#36866;&#24403;&#35757;&#32451;&#26469;&#23398;&#20064;&#33853;&#23454;&#31354;&#38388;&#20851;&#31995;&#65292;&#36825;&#21487;&#20197;&#36890;&#36807;&#39044;&#35757;&#32451;LM&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#22914;&#26524;&#20026;&#32431;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#25552;&#20379;&#20102;&#23545;&#35937;&#30340;&#26174;&#24335;&#20301;&#32622;&#20449;&#24687;&#24182;&#36827;&#34892;&#20102;&#36866;&#24403;&#30340;&#35757;&#32451;&#20197;&#21033;&#29992;&#36825;&#20123;&#20301;&#32622;&#20449;&#24687;&#65292;&#23427;&#20204;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#20687;&#8220;&#24038;&#20391;&#8221;&#25110;&#8220;&#19979;&#26041;&#8221;&#36825;&#26679;&#30340;&#31354;&#38388;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#19968;&#31181;&#21475;&#22836;&#21270;&#29256;&#26412;&#30340;&#35270;&#35273;&#31354;&#38388;&#25512;&#29702;&#65288;VSR&#65289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22270;&#20687;&#19982;&#21253;&#21547;&#22270;&#20687;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30495;&#20551;&#31354;&#38388;&#20851;&#31995;&#30340;&#25991;&#26412;&#38472;&#36848;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#23545;&#22270;&#20687;&#36827;&#34892;&#21475;&#22836;&#25551;&#36848;&#65292;&#24182;&#22312;&#27599;&#20010;&#23545;&#35937;&#26631;&#31614;&#20013;&#28155;&#21152;&#20301;&#32622;&#26631;&#35760;&#65292;&#20197;&#20195;&#34920;&#23427;&#20204;&#30340;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#24418;&#24335;&#12290;&#37492;&#20110;VSR&#25968;&#25454;&#38598;&#35268;&#27169;&#36739;&#23567;&#65292;&#24403;&#20351;&#29992;&#20301;&#32622;&#20449;&#24687;&#26102;&#25105;&#20204;&#27809;&#26377;&#35266;&#23519;&#21040;&#20219;&#20309;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;&#30001;&#25105;&#20204;&#33258;&#21160;&#23548;&#20986;&#30340;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#23545;LM&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#26126;&#20301;&#32622;&#20449;&#24687;&#20351;LM&#33021;&#22815;&#33853;&#23454;&#31354;&#38388;&#20851;&#31995;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;LM&#22312;&#24615;&#33021;&#19978;&#32988;&#36807;&#20102;Vi&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13666v1 Announce Type: new  Abstract: This paper shows that text-only Language Models (LM) can learn to ground spatial relations like "left of" or "below" if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Translationese&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;LMs&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;TinyLMs&#39044;&#35757;&#32451;&#26469;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13638</link><description>&lt;p&gt;
&#19981;&#24517;&#25285;&#24515;&#22914;&#26524;&#24744;&#27809;&#26377;&#25968;&#25454;&#65306;&#21033;&#29992;Translationese&#26500;&#24314;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Do Not Worry if You Do Not Have Data: Building Pretrained Language Models Using Translationese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;Translationese&#21512;&#25104;&#25968;&#25454;&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#20013;&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;LMs&#39044;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#36807;&#20351;&#29992;&#36731;&#37327;&#32423;TinyLMs&#39044;&#35757;&#32451;&#26469;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#26426;&#22120;&#32763;&#35793;&#21019;&#24314;&#30340;&#21512;&#25104;&#25968;&#25454;Translationese&#29992;&#20316;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#23454;&#29992;&#24615;&#12290;&#39044;&#35757;&#32451;&#38656;&#35201;&#22823;&#37327;&#30340;&#21333;&#35821;&#25968;&#25454;&#65292;&#23545;&#20110;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#65292;&#36825;&#20123;&#25968;&#25454;&#22823;&#37096;&#20998;&#26159;&#19981;&#21487;&#29992;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#26469;&#35299;&#20915;&#36825;&#31181;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#20197;&#33521;&#35821;&#21644;Indic&#35821;&#35328;&#20026;&#20363;&#65292;&#23558;&#32593;&#32476;&#25235;&#21462;&#30340;&#21333;&#35821;&#25991;&#26723;&#65288;&#24178;&#20928;&#30340;&#65289;&#32763;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;&#36825;&#20123;Translationese&#25968;&#25454;&#65288;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#35757;&#32451;&#21253;&#21547;28M&#21644;85M&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#19982;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;LMs&#30456;&#27604;&#65292;NLU&#20219;&#21153;&#30340;&#24615;&#33021;&#20165;&#24046;3.56&#65285;&#65292;NLG&#20219;&#21153;&#30340;&#24046;&#24322;&#20026;1.51&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#36731;&#37327;&#32423;TinyLMs&#26469;&#39640;&#25928;&#36807;&#28388;&#21512;&#25104;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36825;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13638v1 Announce Type: new  Abstract: In this paper, we explore the utility of \textit{Translationese} as synthetic data created using machine translation for pre-training language models (LMs). Pre-training requires vast amounts of monolingual data, which is mostly unavailable for languages other than English. Recently, there has been a growing interest in using synthetic data to address this data scarcity. We take the case of English and Indic languages and translate web-crawled monolingual documents (clean) into the target language. Then, we train language models containing 28M and 85M parameters on this translationese data (synthetic). We show that their performance on downstream natural language understanding and generative tasks is only 3.56\% poorer on NLU tasks and 1.51\% on NLG tasks than LMs pre-trained on clean data. Further, we propose the use of lightweight \textit{TinyLMs} pre-trained on clean data to filter synthetic data efficiently which significantly improv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#25972;LLama Chat&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20272;&#20854;&#22312;&#27431;&#30431;&#25919;&#27835;&#20013;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#22269;&#23478;&#25919;&#20826;&#31435;&#22330;&#30340;&#20805;&#20998;&#20102;&#35299;&#65292;&#24182;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20026;&#23558;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#29992;&#20110;&#25919;&#27835;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13592</link><description>&lt;p&gt;
&#25289;&#39532;&#36935;&#19978;&#27431;&#30431;&#65306;&#36890;&#36807;LLMs&#25506;&#31350;&#27431;&#27954;&#25919;&#27835;&#20809;&#35889;
&lt;/p&gt;
&lt;p&gt;
Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;LLama Chat&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20272;&#20854;&#22312;&#27431;&#30431;&#25919;&#27835;&#20013;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#22269;&#23478;&#25919;&#20826;&#31435;&#22330;&#30340;&#20805;&#20998;&#20102;&#35299;&#65292;&#24182;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20026;&#23558;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#29992;&#20110;&#25919;&#27835;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13592v1 &#31867;&#22411;&#65306;&#26032;&#25991;&#31456; &#25688;&#35201;&#65306;&#32454;&#21270;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#20250;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#25193;&#23637;&#21040;&#32654;&#22269;&#20004;&#20826;&#21046;&#20197;&#22806;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23457;&#35745; Llama Chat&#65292;&#20197;&#20998;&#26512;&#35813;&#27169;&#22411;&#23545;&#27431;&#30431;&#25919;&#27835;&#30340;&#20102;&#35299;&#31243;&#24230;&#21450;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36866;&#24212;&#65292;&#21363;&#36827;&#19968;&#27493;&#32454;&#21270;&#65292;Llama Chat &#22522;&#20110;&#27431;&#27954;&#35758;&#20250;&#36777;&#35770;&#20013;&#20010;&#21035;&#27431;&#27954;&#25919;&#20826;&#30340;&#28436;&#35762;&#36827;&#34892;&#36866;&#24212;&#24615;&#35843;&#25972;&#65292;&#20197;&#26681;&#25454;EUandI&#38382;&#21367;&#37325;&#26032;&#35780;&#20272;&#20854;&#25919;&#27835;&#20542;&#21521;&#12290;Llama Chat &#26174;&#33879;&#20102;&#35299;&#21508;&#22269;&#25919;&#20826;&#30340;&#31435;&#22330;&#65292;&#24182;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#12290;&#32463;&#35843;&#25972;&#30340;&#12289;&#29305;&#23450;&#25919;&#20826;&#30340;&#27169;&#22411;&#22312;&#30456;&#24212;&#31435;&#22330;&#19978;&#26377;&#26126;&#26174;&#30340;&#37325;&#26032;&#35843;&#25972;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#23558;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#20316;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#35805;&#24341;&#25806;&#29992;&#20110;&#21327;&#21161;&#25919;&#27835;&#31185;&#23398;&#30740;&#31350;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13592v1 Announce Type: new  Abstract: Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#22521;&#35757;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#23558;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#21435;&#20559;&#35265;&#32769;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25345;&#29305;&#23450;&#20219;&#21153;&#19981;&#21464;&#24615;&#26102;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13590</link><description>&lt;p&gt;
&#32769;&#24072;-&#23398;&#29983;&#22521;&#35757;&#29992;&#20110;&#21435;&#20559;&#35265;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#32622;&#25442;&#21435;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Teacher-Student Training for Debiasing: General Permutation Debiasing for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25945;&#24072;-&#23398;&#29983;&#22521;&#35757;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#23558;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#21435;&#20559;&#35265;&#32769;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#33976;&#39311;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#65292;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20445;&#25345;&#29305;&#23450;&#20219;&#21153;&#19981;&#21464;&#24615;&#26102;&#30340;&#20302;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#33021;&#21147;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#28982;&#32780;&#23427;&#20204;&#26377;&#26102;&#20250;&#26080;&#27861;&#20445;&#25345;&#29305;&#23450;&#20219;&#21153;&#30340;&#20851;&#38190;&#19981;&#21464;&#24615;&#12290;&#19968;&#20010;&#20363;&#23376;&#23601;&#26159;&#32622;&#25442;&#25935;&#24863;&#24615;&#65292;LLMs&#30340;&#36755;&#20986;&#21487;&#33021;&#20250;&#26681;&#25454;&#36755;&#20837;&#36873;&#39033;&#30340;&#39034;&#24207;&#19981;&#21516;&#32780;&#26126;&#26174;&#19981;&#21516;&#12290;&#34429;&#28982;&#21435;&#20559;&#35265;&#25216;&#26415;&#21487;&#20197;&#20943;&#36731;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#25512;&#29702;&#26102;&#24448;&#24448;&#20250;&#20276;&#38543;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#31181;&#25512;&#29702;&#26102;&#30340;&#20302;&#25928;&#29575;&#12290;&#20854;&#30446;&#30340;&#26159;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;&#12289;&#21435;&#20559;&#35265;&#30340;&#32769;&#24072;&#27169;&#22411;&#30340;&#33021;&#21147;&#33976;&#39311;&#21040;&#19968;&#20010;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20004;&#31181;&#23398;&#29983;&#27169;&#22411;&#30340;&#21464;&#31181;&#65306;&#19968;&#31181;&#22522;&#20110;&#32431;&#31929;&#33976;&#39311;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#32416;&#27491;&#26041;&#27861;&#65292;&#38024;&#23545;&#26356;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23398;&#29983;&#36890;&#36807;&#32416;&#27491;&#32769;&#24072;&#30340;&#19968;&#20010;&#26377;&#20559;&#20915;&#31574;&#26469;&#23454;&#29616;&#21435;&#20559;&#35265;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#29992;&#30340;&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#21644;&#35821;&#35328;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13590v1 Announce Type: new  Abstract: Large Language Models (LLMs) have demonstrated impressive zero-shot capabilities and versatility in NLP tasks, however they sometimes fail to maintain crucial invariances for specific tasks. One example is permutation sensitivity, where LLMs' outputs may significantly vary depending on the order of the input options. While debiasing techniques can mitigate these issues, and yield better performance and reliability, they often come with a high computational cost at inference. This paper addresses this inefficiency at inference time. The aim is to distill the capabilities of a computationally intensive, debiased, teacher model into a more compact student model. We explore two variants of student models: one based on pure distillation, and the other on an error-correction approach for more complex tasks, where the student corrects a single biased decision from the teacher to achieve a debiased output. Our approach is general and can be appl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#33258;&#21160;&#35774;&#35745;&#25552;&#31034;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#20195;&#30721;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#20013;&#38656;&#35201;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13588</link><description>&lt;p&gt;
&#22522;&#20110;&#36951;&#20256;&#33258;&#21160;&#25552;&#31034;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#20195;&#30721;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Genetic Auto-prompt Learning for Pre-trained Code Intelligence Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13588
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36951;&#20256;&#31639;&#27861;&#30340;&#33258;&#21160;&#35774;&#35745;&#25552;&#31034;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#20195;&#30721;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#20013;&#38656;&#35201;&#20154;&#24037;&#35774;&#35745;&#25552;&#31034;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#20195;&#30721;&#26234;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#35745;&#31639;&#25104;&#26412;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#12290;&#26368;&#36817;&#21457;&#23637;&#36215;&#26469;&#30340;&#25552;&#31034;&#23398;&#20064;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25552;&#31034;&#23398;&#20064;&#22312;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#20854;&#20381;&#36182;&#20110;&#20154;&#24037;&#35774;&#35745;&#30340;&#25552;&#31034;&#65292;&#36825;&#24448;&#24448;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#21644;&#19987;&#19994;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#30340;&#33258;&#21160;&#25552;&#31034;&#35774;&#35745;&#26041;&#27861;&#22312;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#20013;&#21463;&#21040;&#24456;&#22823;&#38480;&#21046;&#65292;&#21407;&#22240;&#21253;&#25324;&#26799;&#24230;&#20381;&#36182;&#24615;&#12289;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#26377;&#38480;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#22240;&#33258;&#21160;&#25552;&#31034;&#65288;GenAP&#65289;&#65292;&#23427;&#21033;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#36951;&#20256;&#31639;&#27861;&#26469;&#33258;&#21160;&#35774;&#35745;&#25552;&#31034;&#12290;&#36890;&#36807;GenAP&#65292;&#38750;&#19987;&#23478;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#36229;&#32423;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13588v1 Announce Type: cross  Abstract: As Pre-trained Language Models (PLMs), a popular approach for code intelligence, continue to grow in size, the computational cost of their usage has become prohibitively expensive. Prompt learning, a recent development in the field of natural language processing, emerges as a potential solution to address this challenge. In this paper, we investigate the effectiveness of prompt learning in code intelligence tasks. We unveil its reliance on manually designed prompts, which often require significant human effort and expertise. Moreover, we discover existing automatic prompt design methods are very limited to code intelligence tasks due to factors including gradient dependence, high computational demands, and limited applicability. To effectively address both issues, we propose Genetic Auto Prompt (GenAP), which utilizes an elaborate genetic algorithm to automatically design prompts. With GenAP, non-experts can effortlessly generate super
&lt;/p&gt;</description></item><item><title>CONLINE&#26694;&#26550;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13583</link><description>&lt;p&gt;
CONLINE: &#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#19982;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#30340;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
CONLINE: Complex Code Generation and Refinement with Online Searching and Correctness Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13583
&lt;/p&gt;
&lt;p&gt;
CONLINE&#26694;&#26550;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#32447;&#25628;&#32034;&#21644;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26174;&#33879;&#25552;&#39640;&#20102;&#20195;&#30721;&#29983;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#36716;&#25442;&#20026;&#21487;&#25191;&#34892;&#20195;&#30721;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#29983;&#25104;&#22797;&#26434;&#20195;&#30721;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#22312;&#20110;&#22797;&#26434;&#30340;&#32467;&#26500;&#12289;&#24494;&#22937;&#30340;&#38169;&#35823;&#12289;&#23545;&#39640;&#32423;&#25968;&#25454;&#31867;&#22411;&#30340;&#29702;&#35299;&#20197;&#21450;&#32570;&#23569;&#36741;&#21161;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CONLINE&#26694;&#26550;&#65292;&#36890;&#36807;&#35745;&#21010;&#30340;&#22312;&#32447;&#25628;&#32034;&#20449;&#24687;&#26816;&#32034;&#21644;&#33258;&#21160;&#27491;&#30830;&#24615;&#27979;&#35797;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#65292;&#36827;&#34892;&#36845;&#20195;&#31934;&#28860;&#12290;CONLINE&#36824;&#20018;&#34892;&#21270;&#20102;&#22797;&#26434;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#20197;&#25913;&#21892;&#29702;&#35299;&#65292;&#24182;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#65292;&#30830;&#20445;&#26694;&#26550;&#36866;&#29992;&#20110;&#29616;&#23454;&#24212;&#29992;&#12290;CONLINE&#36890;&#36807;&#23545;DS-1000&#21644;ClassEval&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#23454;&#39564;&#39564;&#35777;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;CONLINE&#26174;&#33879;&#25552;&#39640;&#20102;&#22797;&#26434;&#20195;&#30721;&#29983;&#25104;&#30340;&#36136;&#37327;&#65292;&#31361;&#26174;&#20102;&#20854;&#25552;&#21319;&#23454;&#36341;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13583v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have revolutionized code generation ability by converting natural language descriptions into executable code. However, generating complex code within real-world scenarios remains challenging due to intricate structures, subtle bugs, understanding of advanced data types, and lack of supplementary contents. To address these challenges, we introduce the CONLINE framework, which enhances code generation by incorporating planned online searches for information retrieval and automated correctness testing for iterative refinement. CONLINE also serializes the complex inputs and outputs to improve comprehension and generate test case to ensure the framework's adaptability for real-world applications. CONLINE is validated through rigorous experiments on the DS-1000 and ClassEval datasets. It shows that CONLINE substantially improves the quality of complex code generation, highlighting its potential to enhance the pra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#22312;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36172;&#21338;&#26041;&#27861;DynOpt&#21644;C-DynaOpt&#65292;&#21160;&#24577;&#35843;&#25972;&#22810;&#20010;&#22870;&#21169;&#26435;&#37325;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13578</link><description>&lt;p&gt;
&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21160;&#24577;&#22870;&#21169;&#35843;&#25972;&#29992;&#20110;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dynamic Reward Adjustment in Multi-Reward Reinforcement Learning for Counselor Reflection Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#22312;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36172;&#21338;&#26041;&#27861;DynOpt&#21644;C-DynaOpt&#65292;&#21160;&#24577;&#35843;&#25972;&#22810;&#20010;&#22870;&#21169;&#26435;&#37325;&#65292;&#36890;&#36807;&#23454;&#39564;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#37325;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#20197;&#20849;&#21516;&#20248;&#21270;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#22810;&#20010;&#25991;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#20851;&#27880;&#36741;&#23548;&#21592;&#21453;&#24605;&#29983;&#25104;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#20248;&#21270;&#29983;&#25104;&#22120;&#20197;&#21516;&#26102;&#25552;&#39640;&#29983;&#25104;&#30340;&#36741;&#23548;&#21592;&#22238;&#22797;&#30340;&#27969;&#30021;&#24615;&#12289;&#36830;&#36143;&#24615;&#21644;&#21453;&#24605;&#36136;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#36172;&#21338;&#26041;&#27861;&#65292;DynaOpt&#21644;C-DynaOpt&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#23558;&#22870;&#21169;&#32452;&#21512;&#25104;&#19968;&#20010;&#21333;&#19968;&#20540;&#24182;&#21516;&#26102;&#20248;&#21270;&#23427;&#20204;&#30340;&#24191;&#27867;&#31574;&#30053;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#38750;&#24773;&#22659;&#21644;&#24773;&#22659;&#22810;&#33218;&#36172;&#21338;&#26469;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21160;&#24577;&#35843;&#25972;&#22810;&#20010;&#22870;&#21169;&#26435;&#37325;&#12290;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;DynaOpt&#21644;C-DynaOpt&#20248;&#20110;&#29616;&#26377;&#30340;&#26420;&#32032;&#21644;&#36172;&#21338;&#22522;&#32447;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13578v1 Announce Type: new  Abstract: In this paper, we study the problem of multi-reward reinforcement learning to jointly optimize for multiple text qualities for natural language generation. We focus on the task of counselor reflection generation, where we optimize the generators to simultaneously improve the fluency, coherence, and reflection quality of generated counselor responses. We introduce two novel bandit methods, DynaOpt and C-DynaOpt, which rely on the broad strategy of combining rewards into a single value and optimizing them simultaneously. Specifically, we employ non-contextual and contextual multi-arm bandits to dynamically adjust multiple reward weights during training. Through automatic and manual evaluations, we show that our proposed techniques, DynaOpt and C-DynaOpt, outperform existing naive and bandit baselines, showcasing their potential for enhancing language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22686;&#24378;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;eRST&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#25299;&#23637;&#30340;&#35745;&#31639;&#35805;&#35821;&#20998;&#26512;&#30340;&#26032;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;RST&#21644;&#20854;&#20182;&#29616;&#26377;&#26694;&#26550;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24037;&#20855;&#21644;&#33521;&#25991;&#35821;&#26009;&#24211;&#12290;</title><link>https://arxiv.org/abs/2403.13560</link><description>&lt;p&gt;
eRST&#65306;&#19968;&#31181;&#34920;&#24449;&#35805;&#35821;&#20851;&#31995;&#21644;&#32452;&#32455;&#30340;&#20449;&#21495;&#22270;&#35770;
&lt;/p&gt;
&lt;p&gt;
eRST: A Signaled Graph Theory of Discourse Relations and Organization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13560
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22686;&#24378;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;eRST&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#25299;&#23637;&#30340;&#35745;&#31639;&#35805;&#35821;&#20998;&#26512;&#30340;&#26032;&#29702;&#35770;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;RST&#21644;&#20854;&#20182;&#29616;&#26377;&#26694;&#26550;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#24037;&#20855;&#21644;&#33521;&#25991;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;eRST&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#25299;&#23637;&#30340;&#35745;&#31639;&#35805;&#35821;&#20998;&#26512;&#30340;&#26032;&#29702;&#35770;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20855;&#26377;&#26641;&#29366;&#25171;&#26029;&#12289;&#38750;&#25237;&#23556;&#21644;&#24182;&#21457;&#20851;&#31995;&#30340;&#35805;&#35821;&#20851;&#31995;&#22270;&#65292;&#20197;&#21450;&#32473;&#20986;&#25105;&#20204;&#20998;&#26512;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#38544;&#24335;&#21644;&#26174;&#24335;&#20449;&#21495;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;RST&#21644;&#20854;&#20182;&#29616;&#26377;&#26694;&#26550;&#65288;&#22914;&#20998;&#27573;&#35805;&#35821;&#34920;&#31034;&#29702;&#35770;&#65288;SDRT&#65289;&#12289;&#23486;&#22805;&#27861;&#23612;&#20122;&#35805;&#35821;&#26641;&#24211;&#65288;PDTB&#65289;&#21644;&#35805;&#35821;&#20381;&#36182;&#65289;&#30340;&#32570;&#38519;&#65292;&#24182;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#29702;&#35770;&#20013;&#30340;&#26500;&#24314;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#20026;&#25968;&#25454;&#25552;&#20379;&#20102;&#27880;&#37322;&#12289;&#25628;&#32034;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#24182;&#25552;&#20379;&#21644;&#35780;&#20272;&#20102;&#19968;&#20010;&#26681;&#25454;&#25105;&#20204;&#30340;&#26694;&#26550;&#26631;&#27880;&#30340;&#33521;&#25991;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;12&#31181;&#21475;&#22836;&#21644;&#20070;&#38754;&#20307;&#35009;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;200K&#35789;&#20803;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#33258;&#21160;&#35299;&#26512;&#12289;&#35780;&#20272;&#24230;&#37327;&#21644;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13560v1 Announce Type: new  Abstract: In this article we present Enhanced Rhetorical Structure Theory (eRST), a new theoretical framework for computational discourse analysis, based on an expansion of Rhetorical Structure Theory (RST). The framework encompasses discourse relation graphs with tree-breaking, nonprojective and concurrent relations, as well as implicit and explicit signals which give explainable rationales to our analyses. We survey shortcomings of RST and other existing frameworks, such as Segmented Discourse Representation Theory (SDRT), the Penn Discourse Treebank (PDTB) and Discourse Dependencies, and address these using constructs in the proposed theory. We provide annotation, search and visualization tools for data, and present and evaluate a freely available corpus of English annotated according to our framework, encompassing 12 spoken and written genres with over 200K tokens. Finally, we discuss automatic parsing, evaluation metrics and applications for 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#23454;&#39564;&#65292;&#30830;&#23450;&#20102;ORCA&#20013;&#23884;&#20837;&#22120;&#35757;&#32451;&#23545;2D&#20219;&#21153;&#26080;&#24110;&#21161;&#12289;1D&#20219;&#21153;&#38656;&#35201;&#36866;&#37327;&#23884;&#20837;&#22120;&#35757;&#32451;&#12289;&#20197;&#21450;&#27169;&#22411;&#24494;&#35843;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#32467;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.13537</link><description>&lt;p&gt;
&#35299;&#37322;ORCA&#20132;&#21449;&#27169;&#24577;&#24494;&#35843;&#25104;&#21151;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What explains the success of cross-modal fine-tuning with ORCA?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13537
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#31995;&#21015;&#28040;&#34701;&#23454;&#39564;&#65292;&#30830;&#23450;&#20102;ORCA&#20013;&#23884;&#20837;&#22120;&#35757;&#32451;&#23545;2D&#20219;&#21153;&#26080;&#24110;&#21161;&#12289;1D&#20219;&#21153;&#38656;&#35201;&#36866;&#37327;&#23884;&#20837;&#22120;&#35757;&#32451;&#12289;&#20197;&#21450;&#27169;&#22411;&#24494;&#35843;&#23545;&#24615;&#33021;&#24433;&#21709;&#26368;&#22823;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ORCA&#65288;Shen&#31561;&#20154;&#65292;2023&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20132;&#21449;&#27169;&#24577;&#24494;&#35843;&#25216;&#26415;&#65292;&#21363;&#23558;&#39044;&#35757;&#32451;&#30340;&#36716;&#25442;&#22120;&#27169;&#22411;&#24212;&#29992;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#20043;&#22806;&#30340;&#27169;&#24577;&#12290; &#35813;&#25216;&#26415;&#20027;&#35201;&#21253;&#25324;&#35757;&#32451;&#19968;&#20010;&#23884;&#20837;&#22120;&#24182;&#24494;&#35843;&#23884;&#20837;&#22120;&#21644;&#27169;&#22411;&#12290; &#23613;&#31649;&#23427;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#25105;&#20204;&#24182;&#19981;&#30830;&#20999;&#20102;&#35299;&#36825;&#20123;&#32452;&#20214;&#20013;&#30340;&#27599;&#20010;&#22914;&#20309;&#20419;&#25104;ORCA&#30340;&#25104;&#21151;&#12290; &#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#28040;&#34701;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#23884;&#20837;&#22120;&#35757;&#32451;&#23545;2D&#20219;&#21153;&#27627;&#26080;&#24110;&#21161;&#65292;&#19982;&#21407;&#22987;&#35770;&#25991;&#25152;&#35328;&#30456;&#21453;&#12290; &#22312;1D&#20219;&#21153;&#20013;&#65292;&#19968;&#23450;&#37327;&#30340;&#23884;&#20837;&#22120;&#35757;&#32451;&#26159;&#24517;&#35201;&#30340;&#65292;&#20294;&#26356;&#22810;&#24182;&#38750;&#24635;&#26159;&#26356;&#22909;&#12290; &#22312;&#25105;&#20204;&#23581;&#35797;&#30340;6&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;4&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#27169;&#22411;&#24494;&#35843;&#20135;&#29983;&#20102;&#26368;&#22823;&#30340;&#24046;&#24322;&#12290; &#36890;&#36807;&#25105;&#20204;&#30340;&#28040;&#34701;&#23454;&#39564;&#21644;&#22522;&#32447;&#65292;&#25105;&#20204;&#23545;ORCA&#30340;&#21508;&#20010;&#32452;&#20214;&#26377;&#20102;&#26356;&#22909;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13537v1 Announce Type: new  Abstract: ORCA (Shen et al., 2023) is a recent technique for cross-modal fine-tuning, i.e., applying pre-trained transformer models to modalities beyond their training data. The technique consists primarily of training an embedder and fine-tuning the embedder and model. Despite its high performance on a variety of downstream tasks, we do not understand precisely how each of these components contribute to ORCA's success. Therefore, we run a series of ablations and find that embedder training does not help 2D tasks at all, contrary to what the original paper posits. In 1D tasks, some amount of embedder training is necessary but more is not better. In 4 out of 6 datasets we experiment with, it is model fine-tuning that makes the biggest difference. Through our ablations and baselines, we contribute a better understanding of the individual components of ORCA.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;FineHumanML3D&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;FineMotionDiffuse&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.13518</link><description>&lt;p&gt;
&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;
&lt;/p&gt;
&lt;p&gt;
Motion Generation from Fine-grained Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#36816;&#21160;&#30340;&#26041;&#27861;&#65292;&#26500;&#24314;&#20102;FineHumanML3D&#25968;&#25454;&#38598;&#65292;&#35774;&#35745;&#20102;FineMotionDiffuse&#27169;&#22411;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#21160;&#20316;&#30340;&#20219;&#21153;&#26159;&#20174;&#32473;&#23450;&#30340;&#25991;&#23383;&#25551;&#36848;&#29983;&#25104;&#36816;&#21160;&#24207;&#21015;&#65292;&#27169;&#22411;&#24212;&#35813;&#25506;&#32034;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19982;&#20154;&#20307;&#21160;&#20316;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#23616;&#38480;&#20110;&#31895;&#31890;&#24230;&#30340;&#36816;&#21160;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#8220;&#19968;&#20010;&#20154;&#36466;&#19979;&#12290;&#8221;&#65289;&#65292;&#20960;&#20046;&#27809;&#26377;&#25506;&#32034;&#25351;&#23450;&#30456;&#20851;&#36523;&#20307;&#37096;&#20301;&#36816;&#21160;&#30340;&#32454;&#31890;&#24230;&#25551;&#36848;&#12290;&#29992;&#31895;&#31961;&#25991;&#26412;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#23398;&#20064;&#20174;&#32454;&#31890;&#24230;&#36816;&#21160;&#30456;&#20851;&#35789;&#27719;&#21040;&#36816;&#21160;&#22522;&#20803;&#30340;&#26144;&#23556;&#65292;&#23548;&#33268;&#26080;&#27861;&#20174;&#26410;&#35265;&#25551;&#36848;&#29983;&#25104;&#21160;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36755;&#20837;&#31934;&#32454;&#25552;&#31034;&#32473; GPT-3.5-turbo&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#32454;&#31890;&#24230;&#25991;&#26412;&#25551;&#36848;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;-&#21160;&#20316;&#25968;&#25454;&#38598;FineHumanML3D&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26412;&#21040;&#21160;&#20316;&#27169;&#22411;FineMotionDiffuse&#65292;&#20805;&#20998;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FineMotionDiffuse&#22312;FineHumanML3D&#19978;&#35757;&#32451;&#21518;&#33719;&#24471;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13518v1 Announce Type: cross  Abstract: The task of text2motion is to generate motion sequences from given textual descriptions, where a model should explore the interactions between natural language instructions and human body movements. While most existing works are confined to coarse-grained motion descriptions (e.g., "A man squats."), fine-grained ones specifying movements of relevant body parts are barely explored. Models trained with coarse texts may not be able to learn mappings from fine-grained motion-related words to motion primitives, resulting in the failure in generating motions from unseen descriptions. In this paper, we build a large-scale language-motion dataset with fine-grained textual descriptions, FineHumanML3D, by feeding GPT-3.5-turbo with delicate prompts. Accordingly, we design a new text2motion model, FineMotionDiffuse, which makes full use of fine-grained textual information. Our experiments show that FineMotionDiffuse trained on FineHumanML3D acqui
&lt;/p&gt;</description></item><item><title>&#25463;&#20811;BERT&#27169;&#22411;&#26696;&#20363;&#30740;&#31350;&#21457;&#29616;&#65292;BERT&#22823;&#23567;&#30340;&#27169;&#22411;&#24182;&#19981;&#20307;&#29616;&#20986;&#19982;&#25919;&#27835;&#20215;&#20540;&#35266;&#30340;&#31995;&#32479;&#23545;&#40784;&#65292;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#20559;&#35265;&#26356;&#22810;&#26159;&#30001;&#34920;&#38754;&#27169;&#20223;&#24341;&#36215;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.13514</link><description>&lt;p&gt;
&#24615;&#21035;&#22914;&#20309;&#19982;&#25919;&#27835;&#20215;&#20540;&#35266;&#20114;&#21160;&#65306;&#25463;&#20811;BERT&#27169;&#22411;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Gender Interacts with Political Values: A Case Study on Czech BERT Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13514
&lt;/p&gt;
&lt;p&gt;
&#25463;&#20811;BERT&#27169;&#22411;&#26696;&#20363;&#30740;&#31350;&#21457;&#29616;&#65292;BERT&#22823;&#23567;&#30340;&#27169;&#22411;&#24182;&#19981;&#20307;&#29616;&#20986;&#19982;&#25919;&#27835;&#20215;&#20540;&#35266;&#30340;&#31995;&#32479;&#23545;&#40784;&#65292;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#20559;&#35265;&#26356;&#22810;&#26159;&#30001;&#34920;&#38754;&#27169;&#20223;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22312;&#19981;&#21487;&#36991;&#20813;&#22320;&#21253;&#21547;&#20215;&#20540;&#36127;&#25285;&#20869;&#23481;&#30340;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#65292;&#36890;&#24120;&#25429;&#25417;&#21040;&#19981;&#33391;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#20063;&#20250;&#20307;&#29616;&#22312;&#27169;&#22411;&#20013;&#12290;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20851;&#27880;&#25463;&#20811;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#30340;&#25919;&#27835;&#20559;&#35265;&#65292;&#24182;&#23558;&#20854;&#19982;&#20195;&#34920;&#24615;&#20215;&#20540;&#35843;&#26597;&#36827;&#34892;&#27604;&#36739;&#12290;&#30001;&#20110;&#25463;&#20811;&#26159;&#19968;&#31181;&#24615;&#21035;&#21270;&#35821;&#35328;&#65292;&#25105;&#20204;&#36824;&#34913;&#37327;&#20102;&#35821;&#27861;&#24615;&#21035;&#19982;&#35843;&#26597;&#20013;&#23545;&#30007;&#24615;&#21644;&#22899;&#24615;&#22238;&#31572;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#27169;&#22411;&#30340;&#25919;&#27835;&#20215;&#20540;&#35266;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#20998;&#37197;&#38472;&#36848;&#27010;&#29575;&#26102;&#24182;&#19981;&#36981;&#24490;&#22522;&#20110;&#20215;&#20540;&#30340;&#25512;&#29702;&#65292;&#24182;&#19988;&#22899;&#24615;&#21644;&#30007;&#24615;&#21477;&#23376;&#20043;&#38388;&#27809;&#26377;&#31995;&#32479;&#24046;&#24322;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;BERT&#22823;&#23567;&#30340;&#27169;&#22411;&#24182;&#19981;&#20307;&#29616;&#20986;&#19982;&#25919;&#27835;&#20215;&#20540;&#35266;&#30340;&#31995;&#32479;&#23545;&#40784;&#65292;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#20559;&#35265;&#26356;&#22810;&#26159;&#30001;&#34920;&#38754;&#27169;&#20223;&#24341;&#36215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13514v1 Announce Type: new  Abstract: Neural language models, which reach state-of-the-art results on most natural language processing tasks, are trained on large text corpora that inevitably contain value-burdened content and often capture undesirable biases, which the models reflect. This case study focuses on the political biases of pre-trained encoders in Czech and compares them with a representative value survey. Because Czech is a gendered language, we also measure how the grammatical gender coincides with responses to men and women in the survey. We introduce a novel method for measuring the model's perceived political values. We find that the models do not assign statement probability following value-driven reasoning, and there is no systematic difference between feminine and masculine sentences. We conclude that BERT-sized models do not manifest systematic alignment with political values and that the biases observed in the models are rather due to superficial imitat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#21551;&#31034;&#65288;Counterfactual Inception&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#20107;&#23454;&#24605;&#24819;&#26893;&#20837;&#21040;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#20013;&#65292;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#25928;&#24212;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13513</link><description>&lt;p&gt;
&#22914;&#26524;......&#20250;&#24590;&#26679;&#65311;&#65306;&#21453;&#20107;&#23454;&#21551;&#31034;&#22312;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#20943;&#36731;&#24187;&#35273;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
What if...?: Counterfactual Inception to Mitigate Hallucination Effects in Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#21551;&#31034;&#65288;Counterfactual Inception&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21453;&#20107;&#23454;&#24605;&#24819;&#26893;&#20837;&#21040;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#20013;&#65292;&#21487;&#20197;&#20943;&#36731;&#24187;&#35273;&#25928;&#24212;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25552;&#39640;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#22788;&#29702;&#24187;&#35273;&#25928;&#24212;&#26041;&#38754;&#21487;&#38752;&#24615;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27169;&#22411;&#20250;&#29983;&#25104;&#19981;&#27491;&#30830;&#25110;&#26080;&#20851;&#30340;&#21709;&#24212;&#12290;&#27809;&#26377;&#39069;&#22806;&#30340;&#25351;&#23548;&#35843;&#25972;&#33539;&#24335;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21453;&#20107;&#23454;&#21551;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31934;&#24515;&#36873;&#25321;&#30340;&#12289;&#19981;&#23545;&#40784;&#30340;&#21453;&#20107;&#23454;&#20851;&#38190;&#35789;&#23558;&#21453;&#20107;&#23454;&#24605;&#24819;&#26893;&#20837;&#21040;LMMs&#20013;&#12290;&#35813;&#26041;&#27861;&#26681;&#26893;&#20110;&#21453;&#20107;&#23454;&#24605;&#32500;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#35748;&#30693;&#36807;&#31243;&#65292;&#20154;&#31867;&#22312;&#20854;&#20013;&#32771;&#34385;&#26367;&#20195;&#29616;&#23454;&#21644;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#25512;&#29702;&#26426;&#21046;&#24212;&#29992;&#21040;LMMs&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24187;&#35273;&#25928;&#24212;&#24182;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#21452;&#27169;&#24577;&#39564;&#35777;&#36807;&#31243;&#65288;DVP&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20005;&#26684;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;LMMs&#20013;&#21453;&#20107;&#23454;&#24605;&#32500;&#30340;&#26368;&#20339;&#21453;&#20107;&#23454;&#20851;&#38190;&#35789;&#65292;&#21516;&#26102;&#32771;&#34385;&#35270;&#35273;&#21644;&#35821;&#35328;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;LMMs&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13513v1 Announce Type: cross  Abstract: This paper presents a way of enhancing the reliability of Large Multimodal Models (LMMs) in addressing hallucination effects, where models generate incorrect or unrelated responses. Without additional instruction tuning paradigm, we introduce Counterfactual Inception, a novel method that implants counterfactual thoughts into LMMs using carefully chosen, misaligned counterfactual keywords. This method is grounded in the concept of counterfactual thinking, a cognitive process where humans consider alternative realities and outcomes. By applying this human-like reasoning mechanism to LMMs, we aim to reduce hallucination effects and improve the models' trustworthiness. We also propose Dual-modality Verification Process (DVP), a rigorous framework for selecting optimal counterfactual keywords to trigger counterfactual thinking into LMMs, concurrently considering visual and linguistic context. Our extensive experiments across various LMMs, i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13485</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#25991;&#26412;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Entropy-based Text Watermarking Detection Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13485
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#26041;&#27861;&#65292;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#26681;&#25454;&#20196;&#29260;&#30340;&#29109;&#35843;&#25972;&#20854;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#33021;&#22815;&#23884;&#20837;&#38544;&#34255;&#29305;&#24449;&#21040;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20197;&#20415;&#21518;&#32493;&#26816;&#27979;&#65292;&#20174;&#32780;&#32531;&#35299;&#20102;LLMs&#34987;&#35823;&#29992;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#25991;&#26412;&#27700;&#21360;&#31639;&#27861;&#22312;&#22823;&#22810;&#25968;&#39640;&#29109;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20302;&#29109;&#24773;&#20917;&#19979;&#20173;&#38656;&#35201;&#25913;&#36827;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#24212;&#20840;&#38754;&#32771;&#34385;&#20196;&#29260;&#29109;&#30340;&#24433;&#21709;&#65292;&#21363;&#24212;&#26681;&#25454;&#20854;&#29109;&#35843;&#25972;&#27599;&#20010;&#20196;&#29260;&#30340;&#37325;&#37327;&#65292;&#32780;&#19981;&#26159;&#20687;&#20197;&#21069;&#30340;&#26041;&#27861;&#20013;&#23558;&#25152;&#26377;&#20196;&#29260;&#30340;&#37325;&#37327;&#35774;&#32622;&#20026;&#30456;&#21516;&#20540;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29109;&#30340;&#27700;&#21360;&#26816;&#27979;&#65288;EWD&#65289;&#65292;&#22312;&#27700;&#21360;&#26816;&#27979;&#36807;&#31243;&#20013;&#36171;&#20104;&#39640;&#29109;&#20196;&#29260;&#26356;&#39640;&#30340;&#26435;&#37325;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#27700;&#21360;&#31243;&#24230;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#26816;&#27979;&#36807;&#31243;&#26080;&#38656;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13485v1 Announce Type: new  Abstract: Currently, text watermarking algorithms for large language models (LLMs) can embed hidden features to texts generated by LLMs to facilitate subsequent detection, thus alleviating the problem of misuse of LLMs. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we proposed that the influence of token entropy should be fully considered in the watermark detection process, that is, the weight of each token should be adjusted according to its entropy during watermark detection, rather than setting the weight of all tokens to the same value as in previous methods. Specifically, we proposed an Entropy-based Watermark Detection (EWD) that gives higher-entropy tokens higher weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free a
&lt;/p&gt;</description></item><item><title>HyperLLaVA&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#25972;&#30340;&#25237;&#24433;&#20202;&#21644;LLM&#21442;&#25968;&#65292;&#20197;&#21450;&#21160;&#24577;&#30340;&#35270;&#35273;&#19987;&#23478;&#21644;&#35821;&#35328;&#19987;&#23478;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#38745;&#24577;&#35843;&#25972;&#31574;&#30053;&#22312;&#19981;&#21516;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#24615;&#33021;&#21463;&#38480;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.13447</link><description>&lt;p&gt;
HyperLLaVA: &#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21160;&#24577;&#35270;&#35273;&#21644;&#35821;&#35328;&#19987;&#23478;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
HyperLLaVA: Dynamic Visual and Language Expert Tuning for Multimodal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13447
&lt;/p&gt;
&lt;p&gt;
HyperLLaVA&#36890;&#36807;&#24341;&#20837;&#33258;&#36866;&#24212;&#35843;&#25972;&#30340;&#25237;&#24433;&#20202;&#21644;LLM&#21442;&#25968;&#65292;&#20197;&#21450;&#21160;&#24577;&#30340;&#35270;&#35273;&#19987;&#23478;&#21644;&#35821;&#35328;&#19987;&#23478;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#38745;&#24577;&#35843;&#25972;&#31574;&#30053;&#22312;&#19981;&#21516;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#24615;&#33021;&#21463;&#38480;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#25193;&#23637;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#22312;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#24403;&#21069;&#30340;MLLM&#33539;&#24335;&#65292;&#22914;LLaVA&#65292;&#36890;&#36807;&#20351;&#29992;&#38745;&#24577;&#35270;&#35273;-&#35821;&#35328;&#26144;&#23556;&#22120;&#23558;&#35270;&#35273;&#29305;&#24449;&#36716;&#25442;&#20026;&#31867;&#20284;&#25991;&#26412;&#30340;&#26631;&#35760;&#65292;&#20174;&#32780;&#20351;&#38745;&#24577;LLMs&#36890;&#36807;&#35270;&#35273;&#25351;&#20196;&#35843;&#20248;&#33719;&#24471;&#29702;&#35299;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#26377;&#25152;&#24076;&#26395;&#65292;&#20294;&#30456;&#21516;&#21442;&#25968;&#30340;&#38745;&#24577;&#35843;&#20248;&#31574;&#30053;&#21487;&#33021;&#38480;&#21046;&#19981;&#21516;&#19979;&#28216;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HyperLLaVA&#65292;&#20854;&#20013;&#21253;&#25324;&#25237;&#24433;&#20202;&#21644;LLM&#21442;&#25968;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#65292;&#20197;&#21450;&#21160;&#24577;&#35270;&#35273;&#19987;&#23478;&#21644;&#35821;&#35328;&#19987;&#23478;&#12290;&#36825;&#20123;&#19987;&#23478;&#28304;&#33258;HyperNetworks&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#33258;&#36866;&#24212;&#21442;&#25968;&#20559;&#31227;&#26469;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13447v1 Announce Type: cross  Abstract: Recent advancements indicate that scaling up Multimodal Large Language Models (MLLMs) effectively enhances performance on downstream multimodal tasks. The prevailing MLLM paradigm, \emph{e.g.}, LLaVA, transforms visual features into text-like tokens using a \emph{static} vision-language mapper, thereby enabling \emph{static} LLMs to develop the capability to comprehend visual information through visual instruction tuning. Although promising, the \emph{static} tuning strategy~\footnote{The static tuning refers to the trained model with static parameters.} that shares the same parameters may constrain performance across different downstream multimodal tasks. In light of this, we introduce HyperLLaVA, which involves adaptive tuning of the projector and LLM parameters, in conjunction with a dynamic visual expert and language expert, respectively. These experts are derived from HyperNetworks, which generates adaptive parameter shifts throug
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Agent Group Chat&#27169;&#25311;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#25925;&#20107;&#24773;&#33410;&#19979;&#65292;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#20102;&#24847;&#26009;&#20043;&#22806;&#19988;&#37325;&#35201;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#36890;&#36807;&#35843;&#25972;&#29615;&#22659;&#35774;&#32622;&#21487;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.13433</link><description>&lt;p&gt;
&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#65306;&#19968;&#31181;&#20132;&#20114;&#24335;&#32676;&#32452;&#32842;&#22825;&#25311;&#30495;&#20307;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#24341;&#21457;&#38598;&#20307;&#26032;&#20852;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13433
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Agent Group Chat&#27169;&#25311;&#65292;&#30740;&#31350;&#20102;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;&#22312;&#19981;&#21516;&#25925;&#20107;&#24773;&#33410;&#19979;&#65292;&#20195;&#29702;&#20154;&#34920;&#29616;&#20986;&#20102;&#24847;&#26009;&#20043;&#22806;&#19988;&#37325;&#35201;&#30340;&#26032;&#20852;&#34892;&#20026;&#65292;&#36890;&#36807;&#35843;&#25972;&#29615;&#22659;&#35774;&#32622;&#21487;&#20197;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25506;&#35752;&#35821;&#35328;&#22312;&#20154;&#31867;&#38598;&#20307;&#34892;&#20026;&#20013;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#27169;&#25311;&#65292;&#27169;&#25311;&#22810;&#20195;&#29702;&#20043;&#38388;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#30340;&#35821;&#35328;&#20132;&#20114;&#12290;&#20195;&#29702;&#20154;&#34987;&#35201;&#27714;&#22312;&#35813;&#27169;&#25311;&#20013;&#33258;&#30001;&#32842;&#22825;&#65292;&#22522;&#20110;&#20854;&#35282;&#33394;&#35774;&#23450;&#36861;&#27714;&#21508;&#33258;&#30340;&#30446;&#30340;&#65292;&#26088;&#22312;&#35266;&#23519;&#20195;&#29702;&#20154;&#23637;&#29616;&#20986;&#26082;&#24847;&#26009;&#19981;&#21040;&#21448;&#26174;&#33879;&#30340;&#26032;&#20852;&#34892;&#20026;&#12290;&#23558;&#22235;&#20010;&#21465;&#20107;&#22330;&#26223;&#65288;&#32487;&#25215;&#20105;&#35758;&#12289;&#27861;&#24237;&#36777;&#35770;&#12289;&#21746;&#23398;&#36766;&#35828;&#12289;&#30005;&#24433;&#35282;&#33394;&#20105;&#35758;&#65289;&#25972;&#21512;&#21040;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#20013;&#65292;&#20197;&#35780;&#20272;&#20854;&#25903;&#25345;&#22810;&#26679;&#21270;&#25925;&#20107;&#24773;&#33410;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;&#20195;&#29702;&#20154;&#32676;&#32452;&#32842;&#22825;&#20013;&#37197;&#32622;&#29305;&#23450;&#30340;&#29615;&#22659;&#35774;&#32622;&#65292;&#25105;&#20204;&#33021;&#22815;&#35780;&#20272;&#20195;&#29702;&#20154;&#26159;&#21542;&#23637;&#29616;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#19968;&#33268;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#36890;&#36807;&#35745;&#31639;&#35282;&#33394;&#21457;&#35328;&#30340;&#25152;&#26377;&#20869;&#23481;&#30340;n-gram Shannon&#29109;&#26469;&#35780;&#20272;&#29615;&#22659;&#20013;&#30340;&#28151;&#20081;&#31243;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20195;&#29702;&#20154;&#20855;&#26377;&#23376;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13433v1 Announce Type: cross  Abstract: To investigate the role of language in human collective behaviors, we developed the Agent Group Chat simulation to simulate linguistic interactions among multi-agent in different settings. Agents are asked to free chat in this simulation for their own purposes based on their character setting, aiming to see agents exhibit emergent behaviours that are both unforeseen and significant. Four narrative scenarios, Inheritance Disputes, Law Court Debates, Philosophical Discourses, Movie Casting Contention, are integrated into Agent Group Chat to evaluate its support for diverse storylines. By configuring specific environmental settings within Agent Group Chat, we are able to assess whether agents exhibit behaviors that align with human expectations. We evaluate the disorder within the environment by computing the n-gram Shannon entropy of all the content speak by characters. Our findings reveal that under the premise of agents possessing subs
&lt;/p&gt;</description></item><item><title>LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13372</link><description>&lt;p&gt;
LlamaFactory&#65306;100&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13372
&lt;/p&gt;
&lt;p&gt;
LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamaFactory&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#22871;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20869;&#32622;&#30340;Web UI LlamaBoard &#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#24050;&#21457;&#24067;&#22312; https://github.com/hiyouga/LLaMA-Factory&#65292;&#24182;&#24050;&#33719;&#24471;&#36229;&#36807;13,000&#39063;&#26143;&#21644;1,600&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13372v1 Announce Type: new  Abstract: Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13369</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#36827;&#34892;&#23569;&#36164;&#28304;&#35821;&#35328;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Clinical information extraction for Low-resource languages with Few-shot learning using Pre-trained language models and Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13369
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#25216;&#26415;&#65292;&#22312;&#23569;&#36164;&#28304;&#35821;&#35328;&#24773;&#22659;&#19979;&#65292;&#20165;&#38656;&#23569;&#37327;&#26679;&#26412;&#35757;&#32451;&#21363;&#21487;&#25552;&#21462;&#20020;&#24202;&#20449;&#24687;&#65292;&#19988;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20020;&#24202;&#25991;&#20214;&#20013;&#33258;&#21160;&#25552;&#21462;&#21307;&#30103;&#20449;&#24687;&#38754;&#20020;&#30528;&#20960;&#20010;&#25361;&#25112;&#65306;&#25152;&#38656;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#30340;&#39640;&#25104;&#26412;&#12289;&#27169;&#22411;&#39044;&#27979;&#30340;&#26377;&#38480;&#21487;&#35299;&#37322;&#24615;&#12289;&#21463;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#20197;&#21450;&#38544;&#31169;&#27861;&#35268;&#12290;&#26368;&#36817;&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#25552;&#31034;&#26041;&#27861;&#19978;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#22312;&#20351;&#29992;&#26497;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#25104;&#29087;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#27425;&#22312;&#23569;&#36164;&#28304;&#29615;&#22659;&#20013;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#36890;&#36807;&#22312;&#24503;&#22269;&#21307;&#29983;&#20449;&#20214;&#19978;&#36827;&#34892;&#22810;&#31867;&#21035;&#27573;&#20998;&#31867;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#31867;&#21035;&#32423;&#35780;&#20272;&#65292;&#25903;&#25345; Shapley &#20540;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#23567;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19968;&#20010;&#36731;&#37327;&#32423;&#12289;&#39046;&#22495;&#36866;&#24212;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#20165;&#20165;&#25552;&#31034;&#20102; 20 &#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#20102;&#20256;&#32479;&#30340;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13369v1 Announce Type: new  Abstract: Automatic extraction of medical information from clinical documents poses several challenges: high costs of required clinical expertise, limited interpretability of model predictions, restricted computational resources and privacy regulations. Recent advances in domain-adaptation and prompting methods showed promising results with minimal training data using lightweight masked language models, which are suited for well-established interpretability methods. We are first to present a systematic evaluation of these methods in a low-resource setting, by performing multi-class section classification on German doctor's letters. We conduct extensive class-wise evaluations supported by Shapley values, to validate the quality of our small training data set and to ensure the interpretability of model predictions. We demonstrate that a lightweight, domain-adapted pretrained model, prompted with just 20 shots, outperforms a traditional classificatio
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20154;&#31867;&#21270;&#25110;&#36229;&#36234;&#20154;&#31867;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#29992;&#20110;&#28041;&#21450;&#35745;&#31639;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#32472;&#21046;&#20581;&#22766;&#32467;&#35770;&#30340;&#38656;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.13368</link><description>&lt;p&gt;
&#29992;&#20110;&#30740;&#31350;&#20154;&#33041;&#35821;&#35328;&#22788;&#29702;&#30340;&#35745;&#31639;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Computational Models to Study Language Processing in the Human Brain: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13368
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#20154;&#31867;&#21270;&#25110;&#36229;&#36234;&#20154;&#31867;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#29992;&#20110;&#28041;&#21450;&#35745;&#31639;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#32472;&#21046;&#20581;&#22766;&#32467;&#35770;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#23454;&#29616;&#21644;&#31639;&#27861;&#19978;&#19982;&#20154;&#31867;&#35821;&#35328;&#22788;&#29702;&#26426;&#21046;&#23384;&#22312;&#24046;&#24322;&#65292;&#20294;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20986;&#24341;&#20154;&#20837;&#32988;&#30340;&#20154;&#31867;&#21270;&#25110;&#36229;&#36234;&#20154;&#31867;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#24212;&#35813;&#29992;&#20110;&#30740;&#31350;&#22823;&#33041;&#65292;&#22914;&#26524;&#26159;&#65292;&#21017;&#20309;&#26102;&#20197;&#21450;&#22914;&#20309;&#65311;&#20026;&#20102;&#28145;&#20837;&#25506;&#35752;&#36825;&#20010;&#35805;&#39064;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#21033;&#29992;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#22823;&#33041;&#30740;&#31350;&#30340;&#21162;&#21147;&#65292;&#31361;&#26174;&#26032;&#20852;&#36235;&#21183;&#12290;&#20026;&#20102;&#30830;&#20445;&#20844;&#27491;&#27604;&#36739;&#65292;&#26412;&#25991;&#20351;&#29992;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#30340;&#19968;&#33268;&#24230;&#37327;&#35780;&#20272;&#20102;&#21508;&#31181;&#35745;&#31639;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;&#27809;&#26377;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#22312;&#28041;&#21450;&#35745;&#31639;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#32472;&#21046;&#20581;&#22766;&#32467;&#35770;&#30340;&#38656;&#35201;&#20351;&#29992;&#20016;&#23500;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#21644;&#20005;&#26684;&#30340;&#23454;&#39564;&#23545;&#29031;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13368v1 Announce Type: new  Abstract: Despite differing from the human language processing mechanism in implementation and algorithms, current language models demonstrate remarkable human-like or surpassing language capabilities. Should computational language models be employed in studying the brain, and if so, when and how? To delve into this topic, this paper reviews efforts in using computational models for brain research, highlighting emerging trends. To ensure a fair comparison, the paper evaluates various computational models using consistent metrics on the same dataset. Our analysis reveals that no single model outperforms others on all datasets, underscoring the need for rich testing datasets and rigid experimental control to draw robust conclusions in studies involving computational models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13362</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#36134;&#25143;&#28608;&#21169;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#28040;&#36153;
&lt;/p&gt;
&lt;p&gt;
Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#12289;&#20449;&#20219;&#19979;&#38477;&#20197;&#21450;&#23545;&#27665;&#20027;&#35268;&#33539;&#25903;&#25345;&#21160;&#25671;&#26159;&#32654;&#22269;&#27665;&#20027;&#38754;&#20020;&#30340;&#32039;&#36843;&#23041;&#32961;&#12290;&#25509;&#35302;&#39564;&#35777;&#21644;&#20248;&#36136;&#26032;&#38395;&#21487;&#33021;&#38477;&#20302;&#20010;&#20154;&#23545;&#36825;&#20123;&#23041;&#32961;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#20351;&#20844;&#27665;&#26356;&#20855;&#25239;&#20987;&#38169;&#35823;&#20449;&#24687;&#12289;&#27665;&#31929;&#20027;&#20041;&#21644;&#26497;&#31471;&#20826;&#27966;&#35328;&#35770;&#30340;&#33021;&#21147;&#12290;&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#29983;&#24577;&#26377;&#25928;&#30340;&#29615;&#22659;&#20013;&#22686;&#24378;&#29992;&#25143;&#25509;&#35302;&#21644;&#21442;&#19982;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#23545; 28,457 &#20010; Twitter &#29992;&#25143;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20026;&#26399;&#20004;&#21608;&#30340;&#30000;&#37326;&#23454;&#39564;&#65288;&#20174; 2023 &#24180; 1 &#26376; 19 &#26085;&#21040; 2 &#26376; 3 &#26085;&#65289;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102; 28 &#20010;&#21033;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#65292;&#22312;&#29992;&#25143;&#21457;&#34920;&#26377;&#20851;&#20307;&#32946;&#12289;&#23089;&#20048;&#25110;&#29983;&#27963;&#26041;&#24335;&#30340;&#25512;&#25991;&#26102;&#22238;&#22797;&#19968;&#20010;&#20869;&#23481;&#30456;&#20851;&#30340;&#22238;&#22797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#30828;&#20195;&#30721;&#20803;&#32032;&#65306;&#19968;&#20010;&#25351;&#21521;&#20248;&#36136;&#26032;&#38395;&#26426;&#26500;&#30456;&#20851;&#20027;&#39064;&#37096;&#20998;&#30340; URL &#21644;&#40723;&#21169;&#20851;&#27880;&#20854; Twitter &#36134;&#25143;&#12290;&#20026;&#36827;&#19968;&#27493;&#27979;&#35797;&#26426;&#22120;&#20154;&#23545;&#24615;&#21035;&#30340;&#24046;&#24322;&#24433;&#21709;&#65292;&#34987;&#35797;&#29992;&#25143;&#34987;&#38543;&#26426;&#20998;&#37197;&#20197;&#25509;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;User Stateful Embedding&#65288;USE&#65289;&#26469;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#29366;&#24577;&#65292;&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#12290;</title><link>https://arxiv.org/abs/2403.13344</link><description>&lt;p&gt;
&#20351;&#29992;&#65306;&#24102;&#26377;&#26377;&#29366;&#24577;&#24207;&#21015;&#27169;&#22411;&#30340;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
USE: Dynamic User Modeling with Stateful Sequence Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13344
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;User Stateful Embedding&#65288;USE&#65289;&#26469;&#35299;&#20915;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#20013;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#29366;&#24577;&#65292;&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23884;&#20837;&#22312;&#29992;&#25143;&#21442;&#19982;&#24230;&#39044;&#27979;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#24207;&#21015;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#24341;&#21457;&#20102;&#20174;&#34892;&#20026;&#25968;&#25454;&#20013;&#23398;&#20064;&#29992;&#25143;&#23884;&#20837;&#30340;&#20852;&#36259;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#34892;&#20026;&#30340;&#29992;&#25143;&#23884;&#20837;&#23398;&#20064;&#38754;&#20020;&#21160;&#24577;&#29992;&#25143;&#24314;&#27169;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#38543;&#30528;&#29992;&#25143;&#19981;&#26029;&#19982;&#24212;&#29992;&#31243;&#24207;&#20132;&#20114;&#65292;&#29992;&#25143;&#23884;&#20837;&#24212;&#23450;&#26399;&#26356;&#26032;&#20197;&#32771;&#34385;&#29992;&#25143;&#30340;&#26368;&#36817;&#21644;&#38271;&#26399;&#34892;&#20026;&#27169;&#24335;&#12290;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#20110;&#32570;&#20047;&#21382;&#21490;&#34892;&#20026;&#35760;&#24518;&#30340;&#26080;&#29366;&#24577;&#24207;&#21015;&#27169;&#22411;&#12290;&#23427;&#20204;&#24517;&#39035;&#35201;&#20040;&#20002;&#24323;&#21382;&#21490;&#25968;&#25454;&#20165;&#20351;&#29992;&#26368;&#26032;&#25968;&#25454;&#65292;&#35201;&#20040;&#37325;&#26032;&#22788;&#29702;&#26087;&#25968;&#25454;&#21644;&#26032;&#25968;&#25454;&#12290;&#20004;&#31181;&#24773;&#20917;&#22343;&#20250;&#20135;&#29983;&#22823;&#37327;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29992;&#25143;&#26377;&#29366;&#24577;&#23884;&#20837;&#65288;USE&#65289;&#12290;USE&#29983;&#25104;&#29992;&#25143;&#23884;&#20837;&#24182;&#21453;&#26144;&#29992;&#25143;&#19981;&#26029;&#21457;&#23637;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36890;&#36807;&#23384;&#20648;&#20808;&#21069;&#30340;&#27169;&#22411;&#29366;&#24577;&#26469;&#36827;&#34892;&#35814;&#23613;&#30340;&#37325;&#26032;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13344v1 Announce Type: cross  Abstract: User embeddings play a crucial role in user engagement forecasting and personalized services. Recent advances in sequence modeling have sparked interest in learning user embeddings from behavioral data. Yet behavior-based user embedding learning faces the unique challenge of dynamic user modeling. As users continuously interact with the apps, user embeddings should be periodically updated to account for users' recent and long-term behavior patterns. Existing methods highly rely on stateless sequence models that lack memory of historical behavior. They have to either discard historical data and use only the most recent data or reprocess the old and new data jointly. Both cases incur substantial computational overhead. To address this limitation, we introduce User Stateful Embedding (USE). USE generates user embeddings and reflects users' evolving behaviors without the need for exhaustive reprocessing by storing previous model states and
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;Hyacinth6B&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#65292;&#37319;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13334</link><description>&lt;p&gt;
Hyacinth6B&#65306;&#19968;&#20010;&#29992;&#20110;&#20013;&#25991;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Hyacinth6B: A large language model for Traditional Chinese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13334
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#23384;&#22312;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#65292;Hyacinth6B&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#20102;&#24179;&#34913;&#65292;&#37319;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#20027;&#35201;&#21160;&#26426;&#26159;&#24212;&#23545;&#36890;&#24120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#39640;&#30828;&#20214;&#21644;&#35745;&#31639;&#38656;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#27169;&#22411;&#36731;&#37327;&#21270;&#21644;&#24615;&#33021;&#20043;&#38388;&#25214;&#21040;&#24179;&#34913;&#65292;&#21162;&#21147;&#22312;&#20351;&#29992;&#30456;&#23545;&#36731;&#37327;&#32423;&#27169;&#22411;&#30340;&#21516;&#26102;&#26368;&#22823;&#21270;&#24615;&#33021;&#12290;Hyacinth6B&#26159;&#22522;&#20110;&#36825;&#19968;&#30446;&#26631;&#24320;&#21457;&#30340;&#65292;&#26088;&#22312;&#20805;&#20998;&#21457;&#25381;LLM&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#32780;&#19981;&#36896;&#25104;&#24040;&#22823;&#30340;&#36164;&#28304;&#25104;&#26412;&#65292;&#26377;&#25928;&#22320;&#25512;&#21160;&#36739;&#23567;&#27169;&#22411;&#30340;&#24615;&#33021;&#36793;&#30028;&#12290;&#35757;&#32451;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;LoRA&#26041;&#27861;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13334v1 Announce Type: new  Abstract: This research's primary motivation of this study is to address the high hardware and computational demands typically associated with LLMs.Therefore,our goal is to find a balance between model lightness and performance,striving to maximize performance while using a comparatively lightweight model. Hyacinth6B was developed with this objective in mind,aiming to fully leverage the core capabilities of LLMs without incurring substantial resource costs, effectively pushing the boundaries of smaller model's performance. The training approach involves parameter efficient finetuning using the LoRA method.
&lt;/p&gt;</description></item><item><title>Polaris&#26159;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#39318;&#20010;&#20197;&#23433;&#20840;&#20026;&#37325;&#28857;&#30340;LLM&#26143;&#24231;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20159;&#21442;&#25968;&#30340;&#21512;&#20316;&#20195;&#29702;&#36827;&#34892;&#38271;&#26102;&#38388;&#22810;&#36718;&#35821;&#38899;&#23545;&#35805;&#65292;&#36890;&#36807;&#20248;&#21270;&#20195;&#29702;&#30340;&#36845;&#20195;&#21327;&#21516;&#35757;&#32451;&#23454;&#29616;&#22810;&#26679;&#21270;&#30446;&#26631;&#30340;&#26368;&#22823;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.13313</link><description>&lt;p&gt;
Polaris&#65306;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#20197;&#23433;&#20840;&#20026;&#37325;&#28857;&#30340;LLM&#26143;&#24231;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Polaris: A Safety-focused LLM Constellation Architecture for Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13313
&lt;/p&gt;
&lt;p&gt;
Polaris&#26159;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#30340;&#39318;&#20010;&#20197;&#23433;&#20840;&#20026;&#37325;&#28857;&#30340;LLM&#26143;&#24231;&#26550;&#26500;&#65292;&#21033;&#29992;&#22810;&#20159;&#21442;&#25968;&#30340;&#21512;&#20316;&#20195;&#29702;&#36827;&#34892;&#38271;&#26102;&#38388;&#22810;&#36718;&#35821;&#38899;&#23545;&#35805;&#65292;&#36890;&#36807;&#20248;&#21270;&#20195;&#29702;&#30340;&#36845;&#20195;&#21327;&#21516;&#35757;&#32451;&#23454;&#29616;&#22810;&#26679;&#21270;&#30446;&#26631;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;Polaris&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20197;&#23433;&#20840;&#20026;&#37325;&#28857;&#30340;LLM&#26143;&#24231;&#65292;&#29992;&#20110;&#23454;&#26102;&#24739;&#32773; - AI&#21307;&#30103;&#20445;&#20581;&#23545;&#35805;&#12290;&#19982;&#20808;&#21069;&#20391;&#37325;&#20110;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#30340;&#21307;&#30103;&#20445;&#20581;LLM&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#19987;&#38376;&#20851;&#27880;&#38271;&#26102;&#38388;&#30340;&#22810;&#36718;&#35821;&#38899;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#20806;&#20010;&#21442;&#25968;&#26143;&#24231;&#31995;&#32479;&#30001;&#22810;&#20010;&#25968;&#21313;&#20159;&#21442;&#25968;&#30340;&#21512;&#20316;&#20195;&#29702;&#32452;&#25104;&#65306;&#19968;&#20010;&#26377;&#29366;&#24577;&#30340;&#20027;&#20195;&#29702;&#65292;&#19987;&#27880;&#20110;&#24341;&#23548;&#24341;&#20154;&#20837;&#32988;&#30340;&#23545;&#35805;&#65292;&#20197;&#21450;&#20960;&#20010;&#19987;&#19994;&#25903;&#25345;&#20195;&#29702;&#65292;&#19987;&#27880;&#20110;&#25252;&#22763;&#25191;&#34892;&#30340;&#21307;&#30103;&#20445;&#20581;&#20219;&#21153;&#65292;&#20197;&#25552;&#39640;&#23433;&#20840;&#24615;&#24182;&#20943;&#23569;&#24187;&#35273;&#12290;&#25105;&#20204;&#20026;&#20195;&#29702;&#30340;&#36845;&#20195;&#21327;&#21516;&#35757;&#32451;&#24320;&#21457;&#20102;&#22797;&#26434;&#30340;&#35757;&#32451;&#21327;&#35758;&#65292;&#20248;&#21270;&#21508;&#31181;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;&#19987;&#26377;&#25968;&#25454;&#12289;&#20020;&#24202;&#25252;&#29702;&#35745;&#21010;&#12289;&#21307;&#30103;&#20445;&#20581;&#30417;&#31649;&#25991;&#20214;&#12289;&#21307;&#30103;&#25163;&#20876;&#21644;&#20854;&#20182;&#21307;&#30103;&#25512;&#29702;&#25991;&#20214;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#35828;&#35805;&#19968;&#26679;&#65292;&#20351;&#29992;&#26377;&#26426;&#21307;&#30103;&#26415;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13313v1 Announce Type: cross  Abstract: We develop Polaris, the first safety-focused LLM constellation for real-time patient-AI healthcare conversations. Unlike prior LLM works in healthcare focusing on tasks like question answering, our work specifically focuses on long multi-turn voice conversations. Our one-trillion parameter constellation system is composed of several multibillion parameter LLMs as co-operative agents: a stateful primary agent that focuses on driving an engaging conversation and several specialist support agents focused on healthcare tasks performed by nurses to increase safety and reduce hallucinations. We develop a sophisticated training protocol for iterative co-training of the agents that optimize for diverse objectives. We train our models on proprietary data, clinical care plans, healthcare regulatory documents, medical manuals, and other medical reasoning documents. We align our models to speak like medical professionals, using organic healthcare 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Lean&#26694;&#26550;&#35299;&#20915;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#22256;&#38590;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#24182;&#22312;&#23569;&#37327;&#26679;&#26412;&#19978;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13312</link><description>&lt;p&gt;
LeanReasoner: &#29992;Lean&#25552;&#21319;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LeanReasoner: Boosting Complex Logical Reasoning with Lean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13312
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Lean&#26694;&#26550;&#35299;&#20915;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#22256;&#38590;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#24182;&#22312;&#23569;&#37327;&#26679;&#26412;&#19978;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#30001;&#20110;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#21644;&#25512;&#29702;&#22256;&#38590;&#32780;&#38754;&#20020;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;Lean&#65292;&#19968;&#20010;&#23450;&#29702;&#35777;&#26126;&#26694;&#26550;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;Lean&#20013;&#23558;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35777;&#26126;&#25110;&#35777;&#20266;&#30456;&#24212;&#30340;&#23450;&#29702;&#26469;&#35299;&#20915;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;Lean&#30340;&#31526;&#21495;&#27714;&#35299;&#22120;&#20943;&#23569;&#20102;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#30340;&#39118;&#38505;&#12290;&#23427;&#36824;&#36890;&#36807;&#20351;&#29992;Lean&#30340;&#24191;&#27867;&#23450;&#29702;&#35777;&#26126;&#24211;&#22686;&#24378;&#20102;&#25105;&#20204;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;FOLIO&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#22312;ProofWriter&#25968;&#25454;&#38598;&#19978;&#25509;&#36817;&#20102;&#36825;&#19968;&#27700;&#24179;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#32467;&#26524;&#26159;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#30340;&#23569;&#20110;100&#20010;&#39046;&#22495;&#20869;&#26679;&#26412;&#36827;&#34892;&#24494;&#35843;&#21518;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13312v1 Announce Type: new  Abstract: Large language models (LLMs) often struggle with complex logical reasoning due to logical inconsistencies and the inherent difficulty of such reasoning. We use Lean, a theorem proving framework, to address these challenges. By formalizing logical reasoning problems into theorems within Lean, we can solve them by proving or disproving the corresponding theorems. This method reduces the risk of logical inconsistencies with the help of Lean's symbolic solver. It also enhances our ability to treat complex reasoning tasks by using Lean's extensive library of theorem proofs. Our method achieves state-of-the-art performance on the FOLIO dataset and achieves performance near this level on ProofWriter. Notably, these results were accomplished by fine-tuning on fewer than 100 in-domain samples for each dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24515;&#26234;&#25512;&#26029;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#25512;&#26029;&#29992;&#25143;&#30340;&#28508;&#22312;&#30446;&#26631;&#21644;&#22522;&#26412;&#24515;&#29702;&#38656;&#27714;&#65288;FPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;LLMs&#24515;&#29702;&#25512;&#29702;&#24615;&#33021;&#30340;&#20849;&#24773;&#20934;&#30830;&#24230;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.13301</link><description>&lt;p&gt;
&#20174;&#35328;&#35821;&#20013;&#38405;&#35835;&#29992;&#25143;&#24515;&#24605;&#65306;&#22522;&#20110;LLM&#30340;&#31227;&#24773;&#24515;&#29702;&#25512;&#29702;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reading Users' Minds from What They Say: An Investigation into LLM-based Empathic Mental Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24515;&#26234;&#25512;&#26029;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#25512;&#26029;&#29992;&#25143;&#30340;&#28508;&#22312;&#30446;&#26631;&#21644;&#22522;&#26412;&#24515;&#29702;&#38656;&#27714;&#65288;FPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;LLMs&#24515;&#29702;&#25512;&#29702;&#24615;&#33021;&#30340;&#20849;&#24773;&#20934;&#30830;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#20013;&#65292;&#21457;&#23637;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#20840;&#38754;&#28145;&#20837;&#29702;&#35299;&#65292;&#21363;&#20849;&#24773;&#29702;&#35299;&#65292;&#23545;&#20110;&#35774;&#35745;&#30495;&#27491;&#28385;&#36275;&#20154;&#31867;&#38656;&#27714;&#30340;&#20135;&#21697;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#29702;&#35299;&#22823;&#20154;&#21475;&#32676;&#20307;&#30495;&#23454;&#30340;&#28508;&#22312;&#24515;&#29702;&#29366;&#24577;&#22312;&#20170;&#22825;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25191;&#34892;&#24515;&#29702;&#25512;&#29702;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#25512;&#26029;&#29992;&#25143;&#30340;&#28508;&#22312;&#30446;&#26631;&#21644;&#22522;&#26412;&#24515;&#29702;&#38656;&#27714;&#65288;FPNs&#65289;&#12290;&#20174;&#20154;&#31867;&#29992;&#25143;&#21644;&#35774;&#35745;&#32773;&#37027;&#37324;&#25910;&#38598;&#20102;&#22522;&#20934;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20197;&#24320;&#21457;&#19968;&#20010;&#20849;&#24773;&#20934;&#30830;&#24230;&#25351;&#26631;&#65292;&#29992;&#20110;&#34913;&#37327;LLMs&#30340;&#24515;&#29702;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13301v1 Announce Type: cross  Abstract: In human-centered design, developing a comprehensive and in-depth understanding of user experiences, i.e., empathic understanding, is paramount for designing products that truly meet human needs. Nevertheless, accurately comprehending the real underlying mental states of a large human population remains a significant challenge today. This difficulty mainly arises from the trade-off between depth and scale of user experience research: gaining in-depth insights from a small group of users does not easily scale to a larger population, and vice versa. This paper investigates the use of Large Language Models (LLMs) for performing mental inference tasks, specifically inferring users' underlying goals and fundamental psychological needs (FPNs). Baseline and benchmark datasets were collected from human users and designers to develop an empathic accuracy metric for measuring the mental inference performance of LLMs. The empathic accuracy of inf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#39033;&#20219;&#21153;&#65292;&#21363;&#21033;&#29992;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20174;&#23545;&#35805;&#25968;&#25454;&#20013;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#20998;&#31867;&#31038;&#21306;&#38656;&#27714;&#19982;&#36164;&#20135;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#31038;&#21306;&#38656;&#27714;&#21644;&#36164;&#20135;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.13272</link><description>&lt;p&gt;
&#31038;&#21306;&#38656;&#27714;&#19982;&#36164;&#20135;&#65306;&#31038;&#21306;&#23545;&#35805;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Community Needs and Assets: A Computational Analysis of Community Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#39033;&#20219;&#21153;&#65292;&#21363;&#21033;&#29992;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20174;&#23545;&#35805;&#25968;&#25454;&#20013;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#20998;&#31867;&#31038;&#21306;&#38656;&#27714;&#19982;&#36164;&#20135;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#31038;&#21306;&#38656;&#27714;&#21644;&#36164;&#20135;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#21306;&#38656;&#27714;&#35780;&#20272;&#26159;&#38750;&#33829;&#21033;&#32452;&#32455;&#21644;&#25919;&#24220;&#26426;&#26500;&#29992;&#26469;&#37327;&#21270;&#31038;&#21306;&#20248;&#21183;&#21644;&#38382;&#39064;&#30340;&#24037;&#20855;&#65292;&#20351;&#20182;&#20204;&#33021;&#26356;&#22909;&#22320;&#20998;&#37197;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#27491;&#36716;&#21521;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#26469;&#20998;&#26512;&#31038;&#21306;&#30340;&#38656;&#27714;&#21644;&#24050;&#26377;&#36164;&#20135;&#12290;&#28982;&#32780;&#65292;&#23545;&#25351;&#25968;&#22686;&#38271;&#30340;&#31038;&#20132;&#23186;&#20307;&#23545;&#35805;&#36827;&#34892;&#25163;&#21160;&#20998;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#31038;&#21306;&#25104;&#21592;&#22914;&#20309;&#35752;&#35770;&#31038;&#21306;&#20248;&#21183;&#21644;&#38656;&#27714;&#36827;&#34892;&#35745;&#31639;&#20998;&#26512;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21033;&#29992;&#20808;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#20174;&#23545;&#35805;&#25968;&#25454;&#20013;&#35782;&#21035;&#12289;&#25552;&#21462;&#21644;&#20998;&#31867;&#31038;&#21306;&#38656;&#27714;&#19982;&#36164;&#20135;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#20851;&#20110;&#31038;&#21306;&#38656;&#27714;&#19982;&#36164;&#20135;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;Reddit&#30340;3,511&#20010;&#23545;&#35805;&#65292;&#20351;&#29992;&#20247;&#21253;&#24037;&#20316;&#32773;&#36827;&#34892;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13272v1 Announce Type: cross  Abstract: A community needs assessment is a tool used by non-profits and government agencies to quantify the strengths and issues of a community, allowing them to allocate their resources better. Such approaches are transitioning towards leveraging social media conversations to analyze the needs of communities and the assets already present within them. However, manual analysis of exponentially increasing social media conversations is challenging. There is a gap in the present literature in computationally analyzing how community members discuss the strengths and needs of the community. To address this gap, we introduce the task of identifying, extracting, and categorizing community needs and assets from conversational data using sophisticated natural language processing methods. To facilitate this task, we introduce the first dataset about community needs and assets consisting of 3,511 conversations from Reddit, annotated using crowdsourced wor
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35299;&#26512;&#35821;&#35328;&#32467;&#26500;&#26469;&#26816;&#27979;&#25991;&#26723;&#20316;&#32773;&#36523;&#20221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#22120;&#25552;&#21462;&#30340;&#35821;&#27861;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.13253</link><description>&lt;p&gt;
&#20351;&#29992;&#35299;&#26512;&#35821;&#35328;&#32467;&#26500;&#36827;&#34892;&#25991;&#26723;&#20316;&#32773;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Document Author Classification Using Parsed Language Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#35299;&#26512;&#35821;&#35328;&#32467;&#26500;&#26469;&#26816;&#27979;&#25991;&#26723;&#20316;&#32773;&#36523;&#20221;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#22120;&#25552;&#21462;&#30340;&#35821;&#27861;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#20102;&#23454;&#35777;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#20154;&#20204;&#19968;&#30452;&#23545;&#22522;&#20110;&#25991;&#26412;&#30340;&#32479;&#35745;&#23646;&#24615;&#26469;&#26816;&#27979;&#20316;&#32773;&#36523;&#20221;&#24863;&#20852;&#36259;&#65292;&#20363;&#22914;&#20351;&#29992;&#38750;&#24773;&#22659;&#35789;&#30340;&#20986;&#29616;&#29575;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#36825;&#20123;&#25216;&#26415;&#24050;&#32463;&#34987;&#20351;&#29992;&#65292;&#20363;&#22914;&#30830;&#23450;&#12298;&#32852;&#37030;&#20826;&#20154;&#25991;&#38598;&#12299;&#20013;&#25152;&#26377;&#25991;&#31456;&#30340;&#20316;&#32773;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#20195;&#21487;&#33021;&#26377;&#21161;&#20110;&#26816;&#27979;&#20266;&#36896;&#30340;&#25110;&#30001;AI&#21019;&#20316;&#30340;&#25991;&#31456;&#12290;&#32479;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#22120;&#30340;&#36827;&#23637;&#24341;&#20837;&#20102;&#20351;&#29992;&#35821;&#27861;&#32467;&#26500;&#26469;&#26816;&#27979;&#20316;&#32773;&#36523;&#20221;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#32479;&#35745;&#33258;&#28982;&#35821;&#35328;&#35299;&#26512;&#22120;&#25552;&#21462;&#30340;&#35821;&#27861;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#20316;&#32773;&#20998;&#31867;&#30340;&#26032;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#27979;&#35797;&#20102;&#22522;&#20110;&#35821;&#27861;&#32467;&#26500;&#30340;&#20316;&#32773;&#20998;&#31867;&#26041;&#27861;&#22312;&#19968;&#32452;&#8220;&#26657;&#26679;&#25991;&#26412;&#8221;&#19978;&#30340;&#25928;&#26524;&#65292;&#12298;&#32852;&#37030;&#20826;&#20154;&#25991;&#38598;&#12299;&#21644;&#12298;&#26705;&#36842;&#39039;&#12299;&#26366;&#20316;&#20026;&#20808;&#21069;&#20316;&#32773;&#26816;&#27979;&#30740;&#31350;&#30340;&#27979;&#35797;&#26696;&#20363;&#12290;&#20174;&#20013;&#25552;&#21462;&#20102;&#20960;&#20010;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13253v1 Announce Type: new  Abstract: Over the years there has been ongoing interest in detecting authorship of a text based on statistical properties of the text, such as by using occurrence rates of noncontextual words. In previous work, these techniques have been used, for example, to determine authorship of all of \emph{The Federalist Papers}. Such methods may be useful in more modern times to detect fake or AI authorship. Progress in statistical natural language parsers introduces the possibility of using grammatical structure to detect authorship. In this paper we explore a new possibility for detecting authorship using grammatical structural information extracted using a statistical natural language parser. This paper provides a proof of concept, testing author classification based on grammatical structure on a set of "proof texts," The Federalist Papers and Sanditon which have been as test cases in previous authorship detection studies. Several features extracted fro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#26816;&#27979;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#33394;&#24773;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.13250</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#20419;&#36827;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#33394;&#24773;&#25991;&#26412;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Facilitating Pornographic Text Detection for Open-Domain Dialogue Systems via Knowledge Distillation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13250
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#26816;&#27979;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#33394;&#24773;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#23545;&#35805;&#20013;&#20986;&#29616;&#30340;&#33394;&#24773;&#20869;&#23481;&#21487;&#33021;&#23545;&#24320;&#25918;&#22495;&#23545;&#35805;&#31995;&#32479;&#30340;&#29992;&#25143;&#36896;&#25104;&#20005;&#37325;&#21103;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26377;&#20851;&#26816;&#27979;&#20154;&#26426;&#20132;&#20114;&#23545;&#35805;&#20013;&#33394;&#24773;&#35821;&#35328;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#24456;&#23569;&#34987;&#30740;&#31350;&#30340;&#37325;&#35201;&#35838;&#39064;&#12290;&#20026;&#20102;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CensorChat&#65292;&#19968;&#20010;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20250;&#35805;&#26159;&#21542;&#21253;&#21547;&#33394;&#24773;&#20869;&#23481;&#30340;&#23545;&#35805;&#30417;&#27979;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20154;&#26426;&#20132;&#20114;&#23545;&#35805;&#65292;&#24182;&#23558;&#20854;&#20998;&#35299;&#20026;&#21333;&#20010;&#35805;&#35821;&#21644;&#21333;&#36718;&#23545;&#35805;&#65292;&#26368;&#21518;&#19968;&#36718;&#30001;&#32842;&#22825;&#26426;&#22120;&#20154;&#21457;&#34920;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26469;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#65292;&#21407;&#22987;&#25968;&#25454;&#38598;&#30001;&#22235;&#20010;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#27880;&#37322;&#65292;&#36890;&#36807;&#22810;&#25968;&#31080;&#30830;&#23450;&#26631;&#31614;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#26356;&#26032;&#31532;&#19968;&#27493;&#20013;&#30340;&#31354;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13250v1 Announce Type: new  Abstract: Pornographic content occurring in human-machine interaction dialogues can cause severe side effects for users in open-domain dialogue systems. However, research on detecting pornographic language within human-machine interaction dialogues is an important subject that is rarely studied. To advance in this direction, we introduce CensorChat, a dialogue monitoring dataset aimed at detecting whether the dialogue session contains pornographic content. To this end, we collect real-life human-machine interaction dialogues in the wild and break them down into single utterances and single-turn dialogues, with the last utterance spoken by the chatbot. We propose utilizing knowledge distillation of large language models to annotate the dataset. Specifically, first, the raw dataset is annotated by four open-source large language models, with the majority vote determining the label. Second, we use ChatGPT to update the empty label from the first step
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>https://arxiv.org/abs/2403.13244</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Instruction Multi-Constraint Molecular Generation Using a Teacher-Student Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13244
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#26469;&#24110;&#21161;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#65292;&#22312;&#21508;&#31181;&#32422;&#26463;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#27169;&#22411;&#21644;&#35745;&#31639;&#24037;&#20855;&#29992;&#20110;&#20998;&#23376;&#30340;&#32467;&#26500;&#21644;&#24615;&#36136;&#20998;&#26512;&#65292;&#20294;&#29983;&#25104;&#31526;&#21512;&#25152;&#26377;&#26399;&#26395;&#32467;&#26500;&#21644;&#24615;&#36136;&#30340;&#20998;&#23376;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#32422;&#26463;&#20998;&#23376;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;TSMMG&#65292;&#31867;&#20284;&#20110;&#23398;&#29983;&#65292;&#35813;&#27169;&#22411;&#25972;&#21512;&#20102;&#26469;&#33258;&#21508;&#31181;&#23567;&#27169;&#22411;&#21644;&#24037;&#20855;&#65288;&#21363;&#8220;&#32769;&#24072;&#8221;&#65289;&#30340;&#30693;&#35782;&#12290;&#20026;&#20102;&#35757;&#32451;TSMMG&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#36825;&#20123;&#8216;&#32769;&#24072;&#8217;&#20013;&#25552;&#21462;&#30340;&#20998;&#23376;&#30693;&#35782;&#26500;&#24314;&#20102;&#22823;&#37327;&#25991;&#26412;-&#20998;&#23376;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#36890;&#36807;&#21508;&#31181;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#31526;&#21512;&#25551;&#36848;&#30340;&#26032;&#20998;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;TSMMG&#22312;&#29983;&#25104;&#31526;&#21512;&#22797;&#26434;&#12289;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20004;&#12289;&#19977;&#21644;&#22235;&#32422;&#26463;&#20219;&#21153;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24179;&#22343;&#20998;&#23376;&#26377;&#25928;&#24615;&#36229;&#36807;99&#65285;&#65292;&#25104;&#21151;&#29575;&#20998;&#21035;&#20026;88.08&#65285;&#12289;65.27&#65285;&#21644;61.44&#65285;&#12290;&#35813;&#27169;&#22411;&#36824;ex
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13244v1 Announce Type: new  Abstract: While various models and computational tools have been proposed for structure and property analysis of molecules, generating molecules that conform to all desired structures and properties remains a challenge. Here, we introduce a multi-constraint molecular generation large language model, TSMMG, which, akin to a student, incorporates knowledge from various small models and tools, namely, the 'teachers'. To train TSMMG, we construct a large set of text-molecule pairs by extracting molecular knowledge from these 'teachers', enabling it to generate novel molecules that conform to the descriptions through various text prompts. We experimentally show that TSMMG remarkably performs in generating molecules meeting complex, natural language-described property requirements across two-, three-, and four-constraint tasks, with an average molecular validity of over 99% and success ratio of 88.08%, 65.27%, and 61.44%, respectively. The model also ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#30340;&#21487;&#24494;&#20998;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#25688;&#35201;&#21644;&#32763;&#35793;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.13240</link><description>&lt;p&gt;
SumTra&#65306;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#30340;&#21487;&#24494;&#20998;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
SumTra: A Differentiable Pipeline for Few-Shot Cross-Lingual Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23569;&#26679;&#26412;&#36328;&#35821;&#35328;&#25688;&#35201;&#30340;&#21487;&#24494;&#20998;&#27969;&#27700;&#32447;&#65292;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#25688;&#35201;&#21644;&#32763;&#35793;&#27969;&#31243;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#24615;&#33021;&#21644;&#31471;&#21040;&#31471;&#30340;&#21487;&#24494;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#25688;&#35201;&#65288;XLS&#65289;&#22312;&#19982;&#36755;&#20837;&#25991;&#26723;&#35821;&#35328;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#29983;&#25104;&#25688;&#35201;&#65288;&#20363;&#22914;&#65292;&#20174;&#33521;&#35821;&#21040;&#35199;&#29677;&#29273;&#35821;&#65289;&#65292;&#20351;&#30446;&#26631;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#33021;&#22815;&#31616;&#27905;&#22320;&#20102;&#35299;&#20869;&#23481;&#12290;&#26412;&#25991;&#25552;&#20986;&#37325;&#26032;&#23457;&#35270;&#25688;&#35201;&#21644;&#32763;&#35793;&#27969;&#27700;&#32447;&#65292;&#20854;&#20013;&#25688;&#35201;&#21644;&#32763;&#35793;&#20219;&#21153;&#20197;&#24207;&#21015;&#26041;&#24335;&#25191;&#34892;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#37325;&#22797;&#20351;&#29992;&#35768;&#22810;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#36827;&#34892;&#21333;&#35821;&#25688;&#35201;&#21644;&#32763;&#35793;&#65292;&#33719;&#24471;&#38750;&#24120;&#20855;&#31454;&#20105;&#21147;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#27700;&#32447;&#26159;&#23436;&#20840;&#21487;&#24494;&#30340;&#31471;&#21040;&#31471;&#65292;&#20351;&#20854;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#23569;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13240v1 Announce Type: new  Abstract: Cross-lingual summarization (XLS) generates summaries in a language different from that of the input documents (e.g., English to Spanish), allowing speakers of the target language to gain a concise view of their content. In the present day, the predominant approach to this task is to take a performing, pretrained multilingual language model (LM) and fine-tune it for XLS on the language pairs of interest. However, the scarcity of fine-tuning samples makes this approach challenging in some cases. For this reason, in this paper we propose revisiting the summarize-and-translate pipeline, where the summarization and translation tasks are performed in a sequence. This approach allows reusing the many, publicly-available resources for monolingual summarization and translation, obtaining a very competitive zero-shot performance. In addition, the proposed pipeline is completely differentiable end-to-end, allowing it to take advantage of few-shot 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;BetterMixture&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#25968;&#25454;&#21435;&#37325;&#12289;&#36136;&#37327;&#36807;&#28388;&#21644;&#22810;&#26679;&#24615;&#36873;&#25321;&#31561;&#26041;&#27861;&#65292;&#26368;&#32456;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;Ke-Data-Juicer&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#30784;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#21644;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13233</link><description>&lt;p&gt;
&#25216;&#26415;&#25253;&#21578;&#65306;BetterMixture&#31454;&#36187;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Technical Report: Competition Solution For BetterMixture
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#38024;&#23545;BetterMixture&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#25968;&#25454;&#21435;&#37325;&#12289;&#36136;&#37327;&#36807;&#28388;&#21644;&#22810;&#26679;&#24615;&#36873;&#25321;&#31561;&#26041;&#27861;&#65292;&#26368;&#32456;&#33719;&#24471;&#31532;&#19977;&#21517;&#12290;Ke-Data-Juicer&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#30784;&#65292;&#23637;&#29616;&#20102;&#20854;&#22312;&#22788;&#29702;&#21644;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#34028;&#21187;&#21457;&#23637;&#30340;&#26102;&#20195;&#65292;&#20174;&#24222;&#22823;&#22797;&#26434;&#30340;&#25968;&#25454;&#20013;&#36873;&#25321;&#21644;&#20248;&#21270;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#36164;&#28304;&#32422;&#26463;&#19979;&#65292;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;BetterMixture&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#37325;&#28857;&#20851;&#27880;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25968;&#25454;&#28151;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33719;&#24471;&#31532;&#19977;&#21517;&#65292;&#21253;&#25324;&#25968;&#25454;&#21435;&#37325;&#12289;&#20302;&#32423;&#21644;&#39640;&#32423;&#36136;&#37327;&#36807;&#28388;&#65292;&#20197;&#21450;&#22810;&#26679;&#24615;&#36873;&#25321;&#12290;&#25105;&#20204;&#35299;&#20915;&#26041;&#26696;&#30340;&#22522;&#30784;&#26159;Ke-Data-Juicer&#65292;&#26159;Data-Juicer&#30340;&#19968;&#20010;&#25193;&#23637;&#65292;&#23637;&#31034;&#20102;&#20854;&#22788;&#29702;&#21644;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13233v1 Announce Type: new  Abstract: In the era of flourishing large-scale models, the challenge of selecting and optimizing datasets from the vast and complex sea of data, to enhance the performance of large language models within the constraints of limited computational resources, has become paramount. This paper details our solution for the BetterMixture challenge, which focuses on the fine-tuning data mixing for large language models. Our approach, which secured third place, incorporates data deduplication, low-level and high-level quality filtering, and diversity selection. The foundation of our solution is Ke-Data-Juicer, an extension of Data-Juicer, demonstrating its robust capabilities in handling and optimizing data for large language models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.13213</link><description>&lt;p&gt;
&#20174;&#34920;&#29616;&#24615;&#20260;&#23475;&#21040;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;:&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Representational Harms to Quality-of-Service Harms: A Case Study on Llama 2 Safety Safeguards
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#34920;&#29616;&#24615;&#20260;&#23475;&#21644;&#26381;&#21153;&#36136;&#37327;&#20260;&#23475;&#30340;&#32650;&#39548;2&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#23548;&#33268;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#34987;&#24191;&#27867;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#36827;&#27493;&#20063;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#23433;&#20840;&#39118;&#38505;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20854;&#23545;&#24050;&#32463;&#36793;&#32536;&#21270;&#20154;&#32676;&#30340;&#19981;&#21033;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#23384;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#20943;&#36731;&#25514;&#26045;&#26469;&#24320;&#21457;&#23433;&#20840;&#20445;&#38556;&#25514;&#26045;&#65292;&#27604;&#22914;&#30417;&#30563;&#24335;&#30340;&#23433;&#20840;&#23450;&#21521;&#24494;&#35843;&#21644;&#21033;&#29992;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#65292;&#20294;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#20869;&#22312;&#20559;&#35265;&#20173;&#23384;&#22312;&#22810;&#37325;&#20851;&#27880;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#65292;&#20026;&#20102;&#23433;&#20840;&#32780;&#20248;&#21270;&#30340;&#27169;&#22411;&#36890;&#24120;&#20250;&#23637;&#31034;&#22840;&#22823;&#30340;&#23433;&#20840;&#34892;&#20026;&#65292;&#27604;&#22914;&#20986;&#20110;&#39044;&#38450;&#25514;&#26045;&#32780;&#20542;&#21521;&#20110;&#19981;&#22238;&#24212;&#26576;&#20123;&#35831;&#27714;&#12290;&#22240;&#27492;&#65292;&#25991;&#29486;&#20013;&#24050;&#32463;&#35760;&#24405;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#30340;&#26126;&#26174;&#26435;&#34913;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23433;&#20840;&#25514;&#26045;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13213v1 Announce Type: cross  Abstract: Recent progress in large language models (LLMs) has led to their widespread adoption in various domains. However, these advancements have also introduced additional safety risks and raised concerns regarding their detrimental impact on already marginalized populations. Despite growing mitigation efforts to develop safety safeguards, such as supervised safety-oriented fine-tuning and leveraging safe reinforcement learning from human feedback, multiple concerns regarding the safety and ingrained biases in these models remain. Furthermore, previous work has demonstrated that models optimized for safety often display exaggerated safety behaviors, such as a tendency to refrain from responding to certain requests as a precautionary measure. As such, a clear trade-off between the helpfulness and safety of these models has been documented in the literature. In this paper, we further investigate the effectiveness of safety measures by evaluatin
&lt;/p&gt;</description></item><item><title>Wav2Gloss&#25552;&#20986;&#20102;&#20174;&#35821;&#38899;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#35328;&#27880;&#37322;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;Fieldwork&#65292;&#20998;&#26512;&#34920;&#26126;&#39044;&#20808;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#26377;&#21161;&#20110;&#32763;&#35793;&#21644;&#27880;&#37322;&#65292;&#24182;&#19988;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#25928;&#26524;&#36739;&#22909;&#12290;</title><link>https://arxiv.org/abs/2403.13169</link><description>&lt;p&gt;
Wav2Gloss&#65306;&#20174;&#35821;&#38899;&#29983;&#25104;&#20998;&#35789;&#21518;&#30340;&#25991;&#23383;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
Wav2Gloss: Generating Interlinear Glossed Text from Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13169
&lt;/p&gt;
&lt;p&gt;
Wav2Gloss&#25552;&#20986;&#20102;&#20174;&#35821;&#38899;&#20013;&#33258;&#21160;&#25552;&#21462;&#35821;&#35328;&#27880;&#37322;&#30340;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;Fieldwork&#65292;&#20998;&#26512;&#34920;&#26126;&#39044;&#20808;&#35757;&#32451;&#30340;&#35299;&#30721;&#22120;&#26377;&#21161;&#20110;&#32763;&#35793;&#21644;&#27880;&#37322;&#65292;&#24182;&#19988;&#31471;&#21040;&#31471;&#30340;&#31995;&#32479;&#25928;&#26524;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19990;&#30028;&#19978;&#25104;&#21315;&#19978;&#19975;&#31181;&#35821;&#35328;&#38754;&#20020;&#28781;&#32477;&#30340;&#21361;&#38505;&#65292;&#36825;&#23545;&#25991;&#21270;&#36523;&#20221;&#21644;&#20154;&#31867;&#35821;&#35328;&#22810;&#26679;&#24615;&#26500;&#25104;&#20102;&#24040;&#22823;&#23041;&#32961;&#12290;&#20998;&#35789;&#21518;&#30340;&#25991;&#23383;&#27880;&#37322;&#65288;IGT&#65289;&#26159;&#19968;&#31181;&#35821;&#35328;&#27880;&#37322;&#24418;&#24335;&#65292;&#21487;&#20197;&#25903;&#25345;&#23545;&#36825;&#20123;&#35821;&#35328;&#31038;&#21306;&#36827;&#34892;&#25991;&#26723;&#32534;&#21046;&#21644;&#36164;&#28304;&#21019;&#24314;&#12290;IGT&#36890;&#24120;&#21253;&#25324;&#65288;1&#65289;&#36716;&#24405;&#65292;&#65288;2&#65289;&#24418;&#24577;&#20998;&#21106;&#65292;&#65288;3&#65289;&#25991;&#26412;&#27880;&#37322; &#21644;&#65288;4&#65289;&#21040;&#20027;&#27969;&#35821;&#35328;&#30340;&#33258;&#30001;&#32763;&#35793;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Wav2Gloss&#65306;&#19968;&#20010;&#20219;&#21153;&#65292;&#20174;&#35821;&#38899;&#20013;&#33258;&#21160;&#25552;&#21462;&#36825;&#22235;&#20010;&#27880;&#37322;&#32452;&#20214;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#25968;&#25454;&#38598;Fieldwork&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;37&#31181;&#35821;&#35328;&#30340;&#35821;&#38899;&#35821;&#26009;&#24211;&#65292;&#25152;&#26377;&#36825;&#20123;&#37117;&#26377;&#26631;&#20934;&#26684;&#24335;&#21644;&#35757;&#32451;/&#35780;&#20272;&#38598;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13169v1 Announce Type: new  Abstract: Thousands of the world's languages are in danger of extinction--a tremendous threat to cultural identities and human language diversity. Interlinear Glossed Text (IGT) is a form of linguistic annotation that can support documentation and resource creation for these languages' communities. IGT typically consists of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4) free translations to a majority language. We propose Wav2Gloss: a task to extract these four annotation components automatically from speech, and introduce the first dataset to this end, Fieldwork: a corpus of speech with all these annotations covering 37 languages with standard formatting and train/dev/test splits. We compare end-to-end and cascaded Wav2Gloss methods, with analysis suggesting that pre-trained decoders assist with translation and glossing, that multi-task and multilingual approaches are underperformant, and that end-to-end systems perform 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#26500;&#24314;&#33258;&#29983;&#25104;&#22238;&#25918;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13130</link><description>&lt;p&gt;
&#33258;&#29983;&#25104;&#30340;&#22238;&#25918;&#35760;&#24518;&#23545;&#25345;&#32493;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Self-generated Replay Memories for Continual Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#26500;&#24314;&#33258;&#29983;&#25104;&#22238;&#25918;&#35760;&#24518;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#22810;&#31181;&#19981;&#21516;&#35821;&#35328;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#26029;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#25345;&#32493;&#23398;&#20064;&#30340;&#33021;&#21147;&#20173;&#28982;&#21463;&#21040;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#20005;&#37325;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#20851;&#38190;&#23646;&#24615;&#65292;&#21363;&#20854;&#29983;&#25104;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#25345;&#32493;&#23398;&#20064;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#33258;&#36523;&#20316;&#20026;&#24182;&#34892;&#21477;&#23376;&#29983;&#25104;&#22120;&#26469;&#26377;&#25928;&#22320;&#23398;&#20064;&#30001;&#19981;&#21516;&#35821;&#35328;&#32452;&#25104;&#30340;&#32463;&#39564;&#27969;&#12290;&#25105;&#20204;&#22312;&#23454;&#35777;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25269;&#28040;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#26080;&#38656;&#26174;&#24335;&#35760;&#24518;&#35757;&#32451;&#25968;&#25454;&#12290;&#20195;&#30721;&#23558;&#22312;&#21457;&#34920;&#21518;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13130v1 Announce Type: new  Abstract: Modern Neural Machine Translation systems exhibit strong performance in several different languages and are constantly improving. Their ability to learn continuously is, however, still severely limited by the catastrophic forgetting issue. In this work, we leverage a key property of encoder-decoder Transformers, i.e. their generative ability, to propose a novel approach to continually learning Neural Machine Translation systems. We show how this can effectively learn on a stream of experiences comprising different languages, by leveraging a replay memory populated by using the model itself as a generator of parallel sentences. We empirically demonstrate that our approach can counteract catastrophic forgetting without requiring explicit memorization of training data. Code will be publicly available upon publication. Code: https://github.com/m-resta/sg-rep
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#37197;&#32622;&#65292;&#31216;&#20026;prompt-in-decoder&#65288;PiD&#65289;&#65292;&#21487;&#20197;&#19968;&#27425;&#32534;&#30721;&#36755;&#20837;&#24182;&#24182;&#34892;&#35299;&#30721;&#36755;&#20986;&#65292;&#22312;&#32467;&#26500;&#21270;&#36755;&#20986;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#37325;&#22797;&#36755;&#20837;&#32534;&#30721;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35299;&#30721;&#22120;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.13112</link><description>&lt;p&gt;
&#19968;&#27425;&#32534;&#30721;&#65292;&#22810;&#27425;&#24182;&#34892;&#35299;&#30721;&#65306;&#39640;&#25928;Transformer&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Encode Once and Decode in Parallel: Efficient Transformer Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13112
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#37197;&#32622;&#65292;&#31216;&#20026;prompt-in-decoder&#65288;PiD&#65289;&#65292;&#21487;&#20197;&#19968;&#27425;&#32534;&#30721;&#36755;&#20837;&#24182;&#24182;&#34892;&#35299;&#30721;&#36755;&#20986;&#65292;&#22312;&#32467;&#26500;&#21270;&#36755;&#20986;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#39640;&#25928;&#29575;&#65292;&#36991;&#20813;&#20102;&#37325;&#22797;&#36755;&#20837;&#32534;&#30721;&#65292;&#22823;&#24133;&#20943;&#23569;&#20102;&#35299;&#30721;&#22120;&#30340;&#20869;&#23384;&#21344;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21151;&#33021;&#24378;&#22823;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#38480;&#21046;&#20102;&#37096;&#32626;&#22330;&#26223;&#12290;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#24494;&#35843;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22791;&#21463;&#38738;&#30544;&#65292;&#21487;&#20197;&#32988;&#36807;&#26356;&#22823;&#26356;&#36890;&#29992;&#30340;&#20165;&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#20363;&#22914;GPT-4&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#37197;&#32622;&#65292;&#21487;&#20197;&#25552;&#39640;&#22312;&#32467;&#26500;&#21270;&#36755;&#20986;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#30340;&#25928;&#29575;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#38656;&#35201;&#20174;&#21333;&#20010;&#36755;&#20837;&#20013;&#20135;&#29983;&#22810;&#20010;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;prompt-in-decoder&#65288;PiD&#65289;&#65292;&#21482;&#23545;&#36755;&#20837;&#36827;&#34892;&#19968;&#27425;&#32534;&#30721;&#65292;&#24182;&#19988;&#24182;&#34892;&#35299;&#30721;&#36755;&#20986;&#65292;&#36890;&#36807;&#36991;&#20813;&#37325;&#22797;&#36755;&#20837;&#32534;&#30721;&#65292;&#20174;&#32780;&#20943;&#23569;&#35299;&#30721;&#22120;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#25552;&#21319;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#25928;&#29575;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#35745;&#31639;&#20943;&#23569;&#65292;&#22823;&#33268;&#38543;&#23376;&#20219;&#21153;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;&#23545;&#35805;&#29366;&#24577;&#36861;&#36394;&#12289;&#25688;&#35201;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#33719;&#24471;&#39640;&#36798;4.6&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#65292;&#24182;&#19988;&#24615;&#33021;&#30456;&#24403;&#25110;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#35757;&#32451;/&#25512;&#26029;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13112v1 Announce Type: new  Abstract: Transformer-based NLP models are powerful but have high computational costs that limit deployment scenarios. Finetuned encoder-decoder models are popular in specialized domains and can outperform larger more generalized decoder-only models, such as GPT-4. We introduce a new configuration for encoder-decoder models that improves efficiency on structured output and question-answering tasks where multiple outputs are required of a single input. Our method, prompt-in-decoder (PiD), encodes the input once and decodes output in parallel, boosting both training and inference efficiency by avoiding duplicate input encoding, thereby reducing the decoder's memory footprint. We achieve computation reduction that roughly scales with the number of subtasks, gaining up to 4.6x speed-up over state-of-the-art models for dialogue state tracking, summarization, and question-answering tasks with comparable or better performance. We release our training/inf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.13107</link><description>&lt;p&gt;
&#38754;&#21521;&#27861;&#24459;&#25991;&#26412;&#30340;&#22810;&#32423;&#24635;&#32467;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards Unsupervised Question Answering System with Multi-level Summarization for Legal Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#32423;&#24635;&#32467;&#27861;&#23545;&#27861;&#24459;&#25991;&#26412;&#36827;&#34892;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102;F1&#20998;&#25968;&#30340;&#26174;&#33879;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#22242;&#38431;SCaLAR&#22312;SemEval-2024&#20219;&#21153;5&#19978;&#30340;&#24037;&#20316;&#65306;&#27665;&#20107;&#31243;&#24207;&#20013;&#30340;&#27861;&#24459;&#35770;&#35777;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#30001;&#20110;&#28041;&#21450;&#21040;&#30340;&#27861;&#24459;&#25991;&#26412;&#30340;&#22797;&#26434;&#24615;&#32780;&#20196;&#20154;&#26395;&#32780;&#21364;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24230;&#21644;&#36317;&#31163;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#26469;&#29983;&#25104;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#38598;&#25104;&#29305;&#24449;&#65288;&#21253;&#25324;CNN&#12289;GRU&#21644;LSTM&#65289;&#30340;&#22810;&#32423;Legal-Bert&#23884;&#20837;&#30340;&#34701;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#38598;&#20013;&#27861;&#24459;&#35299;&#37322;&#30340;&#20887;&#38271;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;T5&#30340;&#20998;&#27573;&#25688;&#35201;&#65292;&#25104;&#21151;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#20449;&#24687;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26080;&#30417;&#30563;&#31995;&#32479;&#22312;&#24320;&#21457;&#38598;&#19978;&#35265;&#35777;&#20102;macro F1&#20998;&#25968;&#22686;&#21152;&#20102;20&#20010;&#30334;&#20998;&#28857;&#65292;&#22312;&#27979;&#35797;&#38598;&#19978;&#22686;&#21152;&#20102;10&#20010;&#30334;&#20998;&#28857;&#65292;&#32771;&#34385;&#21040;&#20854;&#31616;&#21333;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#20196;&#20154;&#40723;&#33310;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13107v1 Announce Type: new  Abstract: This paper summarizes Team SCaLAR's work on SemEval-2024 Task 5: Legal Argument Reasoning in Civil Procedure. To address this Binary Classification task, which was daunting due to the complexity of the Legal Texts involved, we propose a simple yet novel similarity and distance-based unsupervised approach to generate labels. Further, we explore the Multi-level fusion of Legal-Bert embeddings using ensemble features, including CNN, GRU, and LSTM. To address the lengthy nature of Legal explanation in the dataset, we introduce T5-based segment-wise summarization, which successfully retained crucial information, enhancing the model's performance. Our unsupervised system witnessed a 20-point increase in macro F1-score on the development set and a 10-point increase on the test set, which is promising given its uncomplicated architecture.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#20998;&#26512;&#20102;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#21508;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#24449;&#20132;&#20114;&#22914;&#20309;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.13106</link><description>&lt;p&gt;
&#35748;&#35782;&#20320;&#30340;&#38750;&#32447;&#24615;&#65306;Shapley&#20114;&#21160;&#25581;&#31034;&#25968;&#25454;&#30340;&#28508;&#22312;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Knowing Your Nonlinearities: Shapley Interactions Reveal the Underlying Structure of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#20998;&#26512;&#20102;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#21508;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#38899;&#27169;&#22411;&#20013;&#30340;&#26032;&#39062;&#29616;&#35937;&#65292;&#24182;&#23637;&#31034;&#20102;&#29305;&#24449;&#20132;&#20114;&#22914;&#20309;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#38750;&#32447;&#24615;&#29305;&#24449;&#20132;&#20114;&#26159;&#29702;&#35299;&#35768;&#22810;&#27169;&#22411;&#20013;&#22797;&#26434;&#24402;&#22240;&#27169;&#24335;&#30340;&#19968;&#31181;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20351;&#29992;Shapley Taylor&#20114;&#21160;&#25351;&#25968;&#65288;STII&#65289;&#26469;&#20998;&#26512;&#24213;&#23618;&#25968;&#25454;&#32467;&#26500;&#23545;&#22810;&#31181;&#27169;&#24577;&#12289;&#20219;&#21153;&#21644;&#26550;&#26500;&#20013;&#27169;&#22411;&#34920;&#24449;&#30340;&#24433;&#21709;&#12290;&#22312;&#32771;&#34385;&#25513;&#30721;&#21644;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#21644;ALMs&#65289;&#20013;&#30340;&#35821;&#35328;&#32467;&#26500;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;STII&#22312;&#24815;&#29992;&#34920;&#36798;&#20013;&#22686;&#21152;&#65292;MLMs&#38543;&#21477;&#27861;&#36317;&#31163;&#25193;&#23637;STII&#65292;&#26356;&#22810;&#22320;&#20381;&#36182;&#35821;&#27861;&#22312;&#20854;&#38750;&#32447;&#24615;&#32467;&#26500;&#20013;&#30456;&#27604;ALMs&#12290;&#25105;&#20204;&#30340;&#35821;&#38899;&#27169;&#22411;&#30740;&#31350;&#21453;&#26144;&#20102;&#21475;&#33108;&#24352;&#24320;&#31243;&#24230;&#20915;&#23450;&#38899;&#32032;&#26681;&#25454;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#25968;&#37327;&#30340;&#21407;&#21017;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#22270;&#20687;&#20998;&#31867;&#22120;&#24182;&#35828;&#26126;&#29305;&#24449;&#20132;&#20114;&#30452;&#35266;&#21453;&#26144;&#23545;&#35937;&#36793;&#30028;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#32467;&#26524;&#23637;&#31034;&#20102;&#36328;&#23398;&#31185;&#24037;&#20316;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13106v1 Announce Type: cross  Abstract: Measuring nonlinear feature interaction is an established approach to understanding complex patterns of attribution in many models. In this paper, we use Shapley Taylor interaction indices (STII) to analyze the impact of underlying data structure on model representations in a variety of modalities, tasks, and architectures. Considering linguistic structure in masked and auto-regressive language models (MLMs and ALMs), we find that STII increases within idiomatic expressions and that MLMs scale STII with syntactic distance, relying more on syntax in their nonlinear structure than ALMs do. Our speech model findings reflect the phonetic principal that the openness of the oral cavity determines how much a phoneme varies based on its context. Finally, we study image classifiers and illustrate that feature interactions intuitively reflect object boundaries. Our wide range of results illustrates the benefits of interdisciplinary work and doma
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21307;&#24739;&#23545;&#35805;&#36827;&#34892;&#24635;&#32467;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#31639;&#27861;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#24635;&#32467;&#65292;&#23454;&#29616;&#20102;&#22312;&#20020;&#24202;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13089</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#33258;&#21160;&#24635;&#32467;&#21307;&#24739;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Automatic Summarization of Doctor-Patient Encounter Dialogues Using Large Language Model through Prompt Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21307;&#24739;&#23545;&#35805;&#36827;&#34892;&#24635;&#32467;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#31639;&#27861;&#25351;&#23548;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#24635;&#32467;&#65292;&#23454;&#29616;&#20102;&#22312;&#20020;&#24202;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20339;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25991;&#26412;&#24635;&#32467;&#65288;ATS&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#25216;&#26415;&#65292;&#21487;&#20197;&#24110;&#21161;&#20020;&#24202;&#21307;&#29983;&#25552;&#20379;&#25345;&#32493;&#21644;&#21327;&#35843;&#30340;&#25252;&#29702;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23545;&#21307;&#24739;&#23545;&#35805;&#36827;&#34892;&#24635;&#32467;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#25552;&#31034;&#35843;&#25972;&#31639;&#27861;&#26469;&#25351;&#23548;&#29983;&#25104;&#24335;LLMs&#23545;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#24635;&#32467;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25552;&#31034;&#35843;&#25972;&#31574;&#30053;&#12289;&#36719;&#25552;&#31034;&#30340;&#22823;&#23567;&#20197;&#21450;GatorTronGPT&#30340;few-short&#23398;&#20064;&#33021;&#21147;&#65292;&#35813;&#27169;&#22411;&#26159;&#20351;&#29992;2770&#20159;&#20020;&#24202;&#21644;&#36890;&#29992;&#33521;&#35821;&#35789;&#27719;&#24320;&#21457;&#30340;&#12289;&#25317;&#26377;&#39640;&#36798;200&#20159;&#21442;&#25968;&#30340;&#29983;&#25104;&#24335;&#20020;&#24202;LLM&#12290;&#25105;&#20204;&#23558;GatorTronGPT&#19982;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;T5&#27169;&#22411;&#24494;&#35843;&#30340;&#20808;&#21069;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20351;&#29992;&#20102;&#20020;&#24202;&#22522;&#20934;&#25968;&#25454;&#38598;MTS-DIALOG&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GatorTronGPT-20B&#27169;&#22411;&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#20339;&#24615;&#33021;&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#19981;&#26356;&#26032;LLM&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13089v1 Announce Type: new  Abstract: Automatic text summarization (ATS) is an emerging technology to assist clinicians in providing continuous and coordinated care. This study presents an approach to summarize doctor-patient dialogues using generative large language models (LLMs). We developed prompt-tuning algorithms to instruct generative LLMs to summarize clinical text. We examined the prompt-tuning strategies, the size of soft prompts, and the few-short learning ability of GatorTronGPT, a generative clinical LLM developed using 277 billion clinical and general English words with up to 20 billion parameters. We compared GatorTronGPT with a previous solution based on fine-tuning of a widely used T5 model, using a clinical benchmark dataset MTS-DIALOG. The experimental results show that the GatorTronGPT- 20B model achieved the best performance on all evaluation metrics. The proposed solution has a low computing cost as the LLM parameters are not updated during prompt-tunin
&lt;/p&gt;</description></item><item><title>BiLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#23558;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#38477;&#20302;&#20102;&#23545;&#21333;&#19968;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.13037</link><description>&lt;p&gt;
BiLoRA&#65306;&#19968;&#31181;&#38754;&#21521;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36807;&#24230;&#25311;&#21512;&#40065;&#26834;&#20302;&#31209;&#36866;&#24212;&#30340;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BiLoRA: A Bi-level Optimization Framework for Overfitting-Resilient Low-Rank Adaptation of Large Pre-trained Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13037
&lt;/p&gt;
&lt;p&gt;
BiLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#26694;&#26550;&#30340;&#20943;&#36731;&#36807;&#25311;&#21512;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#36827;&#34892;&#21442;&#25968;&#21270;&#21644;&#23558;&#35757;&#32451;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#38598;&#65292;&#38477;&#20302;&#20102;&#23545;&#21333;&#19968;&#25968;&#25454;&#38598;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#24494;&#35843;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#12290;&#23613;&#31649;LoRA&#21450;&#20854;&#21464;&#20307;&#30456;&#23545;&#20110;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#20250;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#65292;&#23548;&#33268;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#30340;&#27425;&#20248;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BiLoRA&#65292;&#19968;&#31181;&#22522;&#20110;&#21452;&#23618;&#20248;&#21270;&#65288;BLO&#65289;&#30340;&#20943;&#36731;&#36807;&#25311;&#21512;&#24494;&#35843;&#26041;&#27861;&#12290;BiLoRA&#37319;&#29992;&#20266;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#21442;&#25968;&#21270;&#20302;&#31209;&#22686;&#37327;&#30697;&#38453;&#65292;&#24182;&#23558;&#20266;&#22855;&#24322;&#21521;&#37327;&#21644;&#20540;&#30340;&#35757;&#32451;&#20998;&#25104;&#20004;&#20010;&#19981;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#12290;&#36825;&#31181;&#21010;&#20998;&#23884;&#20837;&#22312;BLO&#26694;&#26550;&#30340;&#19981;&#21516;&#23618;&#27425;&#20013;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#23545;&#21333;&#19968;&#25968;&#25454;&#38598;&#36807;&#24230;&#25311;&#21512;&#30340;&#39118;&#38505;&#12290;&#22312;&#28085;&#30422;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#30340;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#30693;&#21517;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13037v1 Announce Type: cross  Abstract: Low-rank adaptation (LoRA) is a popular method for fine-tuning large-scale pre-trained models in downstream tasks by learning low-rank incremental matrices. Though LoRA and its variants effectively reduce the number of trainable parameters compared to full fine-tuning methods, they often overfit training data, resulting in sub-optimal generalization on test data. To address this problem, we introduce BiLoRA, an overfitting-alleviating fine-tuning approach based on bi-level optimization (BLO). BiLoRA employs pseudo singular value decomposition to parameterize low-rank incremental matrices and splits the training of pseudo singular vectors and values across two different subsets of training data. This division, embedded within separate levels of the BLO framework, mitigates the risk of overfitting to a single dataset. Tested on ten datasets covering natural language understanding and generation tasks and applied to various well-known lar
&lt;/p&gt;</description></item><item><title>RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13031</link><description>&lt;p&gt;
RigorLLM&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25269;&#24481;&#19981;&#33391;&#20869;&#23481;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;
&lt;/p&gt;
&lt;p&gt;
RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13031
&lt;/p&gt;
&lt;p&gt;
RigorLLM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#21253;&#25324;&#33021;&#37327;&#25968;&#25454;&#22686;&#24378;&#12289;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#23433;&#20840;&#36755;&#20837;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25968;&#25454;&#22686;&#24378;&#30340;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#20013;&#20986;&#29616;&#30340;&#20559;&#35265;&#20197;&#21450;&#22312;&#24694;&#24847;&#36755;&#20837;&#19979;&#20135;&#29983;&#26377;&#23475;&#20869;&#23481;&#30340;&#28508;&#21147;&#65292;&#23588;&#20854;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#19979;&#65292;&#37117;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#38450;&#25252;&#26639;&#65288;RigorLLM&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#39640;&#25928;&#26377;&#25928;&#22320;&#35843;&#33410;LLMs&#30340;&#26377;&#23475;&#21644;&#19981;&#23433;&#20840;&#36755;&#20837;&#21644;&#36755;&#20986;&#12290;&#36890;&#36807;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#36890;&#36807;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#36827;&#34892;&#22522;&#20110;&#33021;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#22686;&#24378;&#12289;&#36890;&#36807;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#38024;&#23545;&#36755;&#20837;&#20248;&#21270;&#23433;&#20840;&#21518;&#32512;&#65292;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#30340;&#25968;&#25454;&#22686;&#24378;&#23558;&#40065;&#26834;KNN&#19982;LLMs&#34701;&#21512;&#30340;&#22522;&#20110;&#34701;&#21512;&#30340;&#27169;&#22411;&#65292;RigorLLM&#20026;&#26377;&#23475;&#20869;&#23481;&#30340;&#35843;&#33410;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13031v1 Announce Type: cross  Abstract: Recent advancements in Large Language Models (LLMs) have showcased remarkable capabilities across various tasks in different domains. However, the emergence of biases and the potential for generating harmful content in LLMs, particularly under malicious inputs, pose significant challenges. Current mitigation strategies, while effective, are not resilient under adversarial attacks. This paper introduces Resilient Guardrails for Large Language Models (RigorLLM), a novel framework designed to efficiently and effectively moderate harmful and unsafe inputs and outputs for LLMs. By employing a multi-faceted approach that includes energy-based training data augmentation through Langevin dynamics, optimizing a safe suffix for inputs via minimax optimization, and integrating a fusion-based model combining robust KNN with LLMs based on our data augmentation, RigorLLM offers a robust solution to harmful content moderation. Our experimental evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13002</link><description>&lt;p&gt;
AutoTRIZ&#65306;&#21033;&#29992;TRIZ&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20154;&#24037;&#21019;&#24847;
&lt;/p&gt;
&lt;p&gt;
AutoTRIZ: Artificial Ideation with TRIZ and Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#65292;&#20026;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#21644;&#21019;&#26032;&#32773;&#22312;&#24320;&#21457;&#24605;&#32500;&#26041;&#27861;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#27604;&#22914;&#24418;&#24577;&#20998;&#26512;&#21644;&#31867;&#27604;&#35774;&#35745;&#65292;&#20197;&#36741;&#21161;&#24037;&#31243;&#35774;&#35745;&#21019;&#24847;&#65292;&#35299;&#20915;&#38382;&#39064;&#21644;&#25512;&#21160;&#21019;&#26032;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;TRIZ&#20316;&#20026;&#26368;&#33879;&#21517;&#30340;&#26041;&#27861;&#33073;&#39062;&#32780;&#20986;&#65292;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#31995;&#32479;&#21270;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;TRIZ&#36164;&#28304;&#21644;&#27010;&#24565;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#20854;&#23545;&#29992;&#25143;&#30693;&#35782;&#12289;&#32463;&#39564;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#20381;&#36182;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AutoTRIZ&#65292;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#21270;&#21644;&#22686;&#24378;TRIZ&#26041;&#27861;&#30340;&#20154;&#24037;&#21019;&#24847;&#24037;&#20855;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#30340;&#24191;&#27867;&#30693;&#35782;&#21644;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#65292;AutoTRIZ&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#35774;&#35745;&#33258;&#21160;&#21270;&#21644;&#21487;&#35299;&#37322;&#21019;&#24847;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#30683;&#30462;&#26816;&#27979;&#21644;&#27604;&#36739;&#26041;&#38754;&#30340;&#19968;&#33268;&#24615;&#23454;&#39564;&#26469;&#35777;&#26126;&#24182;&#35780;&#20272;AutoTRIZ&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13002v1 Announce Type: cross  Abstract: Researchers and innovators have made enormous efforts in developing ideation methods, such as morphological analysis and design-by-analogy, to aid engineering design ideation for problem solving and innovation. Among these, TRIZ stands out as the most well-known approach, widely applied for systematic innovation. However, the complexity of TRIZ resources and concepts, coupled with its reliance on users' knowledge, experience, and reasoning capabilities, limits its practicability. This paper proposes AutoTRIZ, an artificial ideation tool that leverages large language models (LLMs) to automate and enhance the TRIZ methodology. By leveraging the broad knowledge and advanced reasoning capabilities of LLMs, AutoTRIZ offers a novel approach to design automation and interpretable ideation with artificial intelligence. We demonstrate and evaluate the effectiveness of AutoTRIZ through consistency experiments in contradiction detection and compa
&lt;/p&gt;</description></item><item><title>Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13000</link><description>&lt;p&gt;
Duwak: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21452;&#37325;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Duwak: Dual Watermarks in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13000
&lt;/p&gt;
&lt;p&gt;
Duwak&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#30340;&#27700;&#21360;&#25216;&#26415;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#26085;&#30410;&#20351;&#29992;&#65292;&#23457;&#35745;&#23427;&#20204;&#30340;&#29992;&#36884;&#12289;&#31649;&#29702;&#23427;&#20204;&#30340;&#24212;&#29992;&#24182;&#20943;&#36731;&#20854;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Duwak&#65292;&#36890;&#36807;&#22312;&#20196;&#29260;&#27010;&#29575;&#20998;&#24067;&#21644;&#25277;&#26679;&#26041;&#26696;&#20013;&#23884;&#20837;&#21452;&#37325;&#31192;&#23494;&#27169;&#24335;&#65292;&#20174;&#26681;&#26412;&#19978;&#25552;&#39640;&#20102;&#27700;&#21360;&#30340;&#25928;&#29575;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13000v1 Announce Type: cross  Abstract: As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31034;&#20363;&#36873;&#25321;&#21644;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12999</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#26679;&#26412;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#20197;&#21450;&#20854;&#22312;&#26426;&#22120;&#20154;&#25511;&#21046;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Prompt Selection and Augmentation for Few Examples Code Generation in Large Language Model and its Application in Robotics Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#31034;&#20363;&#36873;&#25321;&#21644;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#36880;&#27493;&#25512;&#29702;&#24050;&#32463;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#22312;&#20869;&#30340;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#25913;&#21892;&#25968;&#23398;&#25512;&#29702;&#21644;&#26426;&#22120;&#20154;&#33218;&#25805;&#20316;&#30340;&#25552;&#31034;&#36873;&#25321;&#21644;&#22686;&#24378;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#38454;&#27573;&#31034;&#20363;&#22686;&#24378;&#26041;&#26696;&#21644;&#31034;&#20363;&#36873;&#25321;&#26041;&#26696;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#36873;&#25321;&#19968;&#32452;&#22686;&#21152;&#22810;&#26679;&#24615;&#12289;&#26368;&#23567;&#21270;&#20887;&#20313;&#24182;&#22686;&#21152;&#19982;&#38382;&#39064;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#26469;&#25552;&#39640;LLM&#24615;&#33021;&#12290;&#24403;&#19982;&#8220;&#24605;&#32500;&#32534;&#31243;&#8221;&#25552;&#31034;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;GSM8K&#21644;SVAMP&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#24615;&#33021;&#25913;&#36827;&#65292;&#20998;&#21035;&#22686;&#21152;&#20102;0.3%&#21644;1.1%&#12290;&#27492;&#22806;&#65292;&#22312;&#27169;&#25311;&#26700;&#38754;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#36890;&#36807;&#23454;&#29616;&#25104;&#21151;&#20219;&#21153;&#23436;&#25104;&#29575;&#25552;&#39640;&#20102;3.4%&#65292;&#24182;&#19988;&#25104;&#21151;&#23436;&#25104;&#20219;&#21153;&#30340;&#26102;&#38388;&#20943;&#23569;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12999v1 Announce Type: cross  Abstract: Few-shot prompting and step-by-step reasoning have enhanced the capabilities of Large Language Models (LLMs) in tackling complex tasks including code generation. In this paper, we introduce a prompt selection and augmentation algorithm aimed at improving mathematical reasoning and robot arm operations. Our approach incorporates a multi-stage example augmentation scheme combined with an example selection scheme. This algorithm improves LLM performance by selecting a set of examples that increase diversity, minimize redundancy, and increase relevance to the question. When combined with the Program-of-Thought prompting, our algorithm demonstrates an improvement in performance on the GSM8K and SVAMP benchmarks, with increases of 0.3% and 1.1% respectively. Furthermore, in simulated tabletop environments, our algorithm surpasses the Code-as-Policies approach by achieving a 3.4% increase in successful task completions and a decrease of over 
&lt;/p&gt;</description></item><item><title>&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;</title><link>https://arxiv.org/abs/2403.12984</link><description>&lt;p&gt;
&#24403;SMILES&#25317;&#26377;&#35821;&#35328;&#65306;&#20351;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#23545;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
When SMILES have Language: Drug Classification using Text Classification Methods on Drug SMILES Strings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12984
&lt;/p&gt;
&lt;p&gt;
&#23558;&#33647;&#29289;SMILES&#23383;&#31526;&#20018;&#35270;&#20026;&#21477;&#23376;&#24182;&#21033;&#29992;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#65292;&#35777;&#23454;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#21487;&#33021;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30340;&#21270;&#23398;&#32467;&#26500;&#65292;&#22914;&#33647;&#29289;&#65292;&#36890;&#24120;&#30001;SMILES&#23383;&#31526;&#20018;&#26469;&#23450;&#20041;&#65292;&#20316;&#20026;&#20998;&#23376;&#21644;&#38190;&#30340;&#24207;&#21015;&#12290;&#36825;&#20123;SMILES&#23383;&#31526;&#20018;&#22312;&#19981;&#21516;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20851;&#30740;&#31350;&#21644;&#34920;&#31034;&#24037;&#20316;&#20013;&#20351;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25670;&#33073;&#22797;&#26434;&#30340;&#34920;&#31034;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#25105;&#20204;&#23558;&#33647;&#29289;SMILES&#35270;&#20026;&#24120;&#35268;&#21477;&#23376;&#65292;&#24182;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#20197;&#36827;&#34892;&#33647;&#29289;&#20998;&#31867;&#20250;&#24590;&#26679;&#65311;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#23454;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#33719;&#24471;&#20102;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#25968;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27599;&#20010;&#21407;&#23376;&#21644;&#38190;&#35270;&#20026;&#21477;&#23376;&#32452;&#20214;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22522;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23545;&#33647;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#65292;&#34920;&#26126;&#22797;&#26434;&#30340;&#38382;&#39064;&#20063;&#21487;&#20197;&#29992;&#26356;&#31616;&#21333;&#30340;&#35270;&#35282;&#26469;&#35299;&#20915;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#21487;&#22312;&#27492;&#22788;&#25214;&#21040;&#65306;https://github.com/azminewasi/Drug-Classification-NLP&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12984v1 Announce Type: cross  Abstract: Complex chemical structures, like drugs, are usually defined by SMILES strings as a sequence of molecules and bonds. These SMILES strings are used in different complex machine learning-based drug-related research and representation works. Escaping from complex representation, in this work, we pose a single question: What if we treat drug SMILES as conventional sentences and engage in text classification for drug classification? Our experiments affirm the possibility with very competitive scores. The study explores the notion of viewing each atom and bond as sentence components, employing basic NLP methods to categorize drug types, proving that complex problems can also be solved with simpler perspectives. The data and code are available here: https://github.com/azminewasi/Drug-Classification-NLP.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#20351;&#29992;&#25277;&#35937;&#35299;&#37322;&#25216;&#26415;&#24320;&#21457;&#20102;C&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#23545;C&#31243;&#24207;&#36827;&#34892;&#38745;&#24577;&#20998;&#26512;&#65292;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#22495;&#65292;&#25552;&#39640;&#31243;&#24207;&#39564;&#35777;&#30340;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.12973</link><description>&lt;p&gt;
C&#20998;&#26512;&#22120;&#65306;&#29992;&#20110;C&#31243;&#24207;&#30340;&#38745;&#24577;&#31243;&#24207;&#20998;&#26512;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
C Analyzer : A Static Program Analysis Tool for C Programs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#20351;&#29992;&#25277;&#35937;&#35299;&#37322;&#25216;&#26415;&#24320;&#21457;&#20102;C&#20998;&#26512;&#22120;&#65292;&#21487;&#20197;&#23545;C&#31243;&#24207;&#36827;&#34892;&#38745;&#24577;&#20998;&#26512;&#65292;&#25903;&#25345;&#22810;&#20010;&#25277;&#35937;&#22495;&#65292;&#25552;&#39640;&#31243;&#24207;&#39564;&#35777;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#36825;&#20010;&#26102;&#20195;&#65292;&#24403;&#19990;&#30028;&#36234;&#26469;&#36234;&#20381;&#36182;&#36719;&#20214;&#31243;&#24207;&#26102;&#65292;&#32534;&#20889;&#26080;bug&#12289;&#27491;&#30830;&#30340;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#22522;&#20110;&#24418;&#24335;&#26041;&#27861;&#30340;&#31243;&#24207;&#39564;&#35777;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#20013;&#30340;&#36816;&#34892;&#26102;&#38169;&#35823;&#26469;&#20445;&#35777;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#23545;&#20154;&#31867;&#29983;&#27963;&#21487;&#33021;&#36896;&#25104;&#30340;&#19981;&#21033;&#24433;&#21709;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#37329;&#38065;&#12290;&#26412;&#39033;&#30446;&#23581;&#35797;&#21033;&#29992;&#25277;&#35937;&#35299;&#37322;&#25216;&#26415;&#23545;C&#31243;&#24207;&#36827;&#34892;&#38745;&#24577;&#20998;&#26512;&#12290;C&#20998;&#26512;&#22120;&#26159;&#19968;&#31181;&#29992;&#20110;C&#31243;&#24207;&#30340;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#12290;&#35813;C&#20998;&#26512;&#22120;&#30340;&#23454;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20379;&#22810;&#20010;&#25277;&#35937;&#22495;&#20351;&#29992;&#30340;&#21363;&#25554;&#21363;&#29992;&#30340;&#26550;&#26500;&#12290;C&#20998;&#26512;&#22120;&#25903;&#25345;&#22235;&#20010;&#25277;&#35937;&#22495;&#8212;&#8212;&#21306;&#38388;&#12289;&#20843;&#35282;&#24418;&#12289;&#22810;&#38754;&#20307;&#21644;&#20301;&#30690;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#19981;&#21516;&#30340;&#22495;&#26469;&#25552;&#20379;&#31243;&#24207;&#39564;&#35777;&#25152;&#38656;&#30340;&#31934;&#24230;&#12290;C&#20998;&#26512;&#22120;&#24037;&#20855;&#20351;&#29992;LLVM C/C++&#32534;&#35793;&#22120;&#21069;&#31471;Clang API&#29983;&#25104;&#21644;&#36941;&#21382;&#32473;&#23450;C&#31243;&#24207;&#30340;&#25511;&#21046;&#27969;&#22270;&#65288;CFG&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12973v1 Announce Type: cross  Abstract: In our times, when the world is increasingly becoming more dependent on software programs, writing bug-free, correct programs is crucial. Program verification based on formal methods can guarantee this by detecting run-time errors in safety-critical systems to avoid possible adverse impacts on human life and save time and money.   This project work tries to leverage Abstract Interpretation techniques for static analysis of C programs. C Analyzer is a tool developed for static analysis of C programs. This implementation of C Analyzer provides a plug-and-play domain architecture for multiple abstract domains to be used. C Analyzer supports four abstract domains - Interval, Octagon, Polyhedra, and Bit Vector. We use these different domains for required precision in program verification. C Analyzer tool uses LLVM C/C++ compiler frontend Clang API to generate and traverse the Control Flow Graph (CFG) of a given C program. This tool generate
&lt;/p&gt;</description></item><item><title>m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.11085</link><description>&lt;p&gt;
m&amp;m's: &#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#24037;&#20855;&#20351;&#29992;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
m&amp;m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11085
&lt;/p&gt;
&lt;p&gt;
m&amp;m's&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;4K+&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#28041;&#21450;33&#31181;&#24037;&#20855;&#65292;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#35268;&#21010;&#22120;&#30340;&#35774;&#35745;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22810;&#27169;&#24577;&#38382;&#39064;&#24456;&#23569;&#30001;&#21333;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35299;&#20915;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#27493;&#39588;&#35745;&#31639;&#35745;&#21010;&#65292;&#28041;&#21450;&#25340;&#25509;&#22810;&#20010;&#27169;&#22411;&#12290; &#24037;&#20855;&#22686;&#24378;&#22411;LLM&#26497;&#26377;&#21487;&#33021;&#33258;&#21160;&#21270;&#29983;&#25104;&#36825;&#31181;&#35745;&#31639;&#35745;&#21010;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#29992;&#20110;&#35780;&#20272;LLM&#20316;&#20026;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#35268;&#21010;&#22120;&#30340;&#26631;&#20934;&#21270;&#22522;&#20934;&#65292;&#38459;&#30861;&#20102;&#23545;&#35268;&#21010;&#22120;&#35774;&#35745;&#20915;&#31574;&#30340;&#31995;&#32479;&#30740;&#31350;&#12290;LLM&#26159;&#21542;&#24212;&#19968;&#27425;&#24615;&#29983;&#25104;&#25972;&#20010;&#35745;&#21010;&#36824;&#26159;&#36880;&#27493;&#29983;&#25104;&#65311;&#23427;&#20204;&#26159;&#21542;&#24212;&#35813;&#30452;&#25509;&#20351;&#29992;Python&#20195;&#30721;&#35843;&#29992;&#24037;&#20855;&#65292;&#36824;&#26159;&#36890;&#36807;&#31867;&#20284;JSON&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#26684;&#24335;&#65311;&#21453;&#39304;&#26159;&#21542;&#25913;&#21892;&#35268;&#21010;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#20197;&#21450;&#26356;&#22810;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;m&amp;m's&#65306;&#19968;&#20010;&#22522;&#20934;&#65292;&#21253;&#21547;4K+&#20010;&#28041;&#21450;33&#31181;&#24037;&#20855;&#30340;&#22810;&#27493;&#39588;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20854;&#20013;&#21253;&#25324;&#22810;&#27169;&#24577;&#27169;&#22411;&#12289;(&#20813;&#36153;)&#20844;&#20849;API&#21644;&#22270;&#20687;&#22788;&#29702;&#27169;&#22359;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#26597;&#35810;&#65292;&#25105;&#20204;&#25552;&#20379;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#33258;&#21160;&#29983;&#25104;&#30340;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11085v1 Announce Type: cross  Abstract: Real-world multi-modal problems are rarely solved by a single machine learning model, and often require multi-step computational plans that involve stitching several models. Tool-augmented LLMs hold tremendous promise for automating the generation of such computational plans. However, the lack of standardized benchmarks for evaluating LLMs as planners for multi-step multi-modal tasks has prevented a systematic study of planner design decisions. Should LLMs generate a full plan in a single shot or step-by-step? Should they invoke tools directly with Python code or through structured data formats like JSON? Does feedback improve planning? To answer these questions and more, we introduce m&amp;m's: a benchmark containing 4K+ multi-step multi-modal tasks involving 33 tools that include multi-modal models, (free) public APIs, and image processing modules. For each of these task queries, we provide automatically generated plans using this realis
&lt;/p&gt;</description></item><item><title>MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.10943</link><description>&lt;p&gt;
MIntRec2.0&#65306;&#29992;&#20110;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0: A Large-scale Benchmark Dataset for Multimodal Intent Recognition and Out-of-scope Detection in Conversations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10943
&lt;/p&gt;
&lt;p&gt;
MIntRec2.0&#20171;&#32461;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#21644;&#23545;&#35805;&#20013;&#22330;&#22806;&#26816;&#27979;&#25361;&#25112;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;1,245&#20010;&#23545;&#35805;&#21644;15,040&#20010;&#26679;&#26412;&#65292;&#20854;&#20013;&#21253;&#25324;&#36924;&#30495;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#24182;&#20016;&#23500;&#20102;&#21457;&#35328;&#32773;&#20449;&#24687;&#20197;&#25903;&#25345;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#38656;&#35201;&#25972;&#21512;&#26469;&#33258;&#29616;&#23454;&#19990;&#30028;&#32972;&#26223;&#30340;&#38750;&#35821;&#35328;&#24418;&#24335;&#65292;&#20197;&#22686;&#24378;&#23545;&#20154;&#31867;&#24847;&#22270;&#30340;&#29702;&#35299;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#22312;&#35268;&#27169;&#19978;&#21463;&#38480;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22810;&#36718;&#23545;&#35805;&#20114;&#21160;&#20013;&#20986;&#29616;&#30340;&#22330;&#22806;&#26679;&#26412;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MIntRec2.0&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26041;&#23545;&#35805;&#20013;&#30340;&#22810;&#27169;&#24577;&#24847;&#22270;&#35782;&#21035;&#30340;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#21547;1,245&#20010;&#23545;&#35805;&#65292;15,040&#20010;&#26679;&#26412;&#65292;&#27599;&#20010;&#26679;&#26412;&#22312;30&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#30340;&#26032;&#24847;&#22270;&#20998;&#31867;&#20013;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#38500;&#20102;9,304&#20010;&#22330;&#20869;&#26679;&#26412;&#22806;&#65292;&#36824;&#21253;&#25324;5,736&#20010;&#20986;&#29616;&#22312;&#22810;&#36718;&#19978;&#19979;&#25991;&#20013;&#30340;&#22330;&#22806;&#26679;&#26412;&#65292;&#36825;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#33258;&#28982;&#21457;&#29983;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#27599;&#20010;&#35805;&#35821;&#20013;&#21457;&#35328;&#32773;&#30340;&#35814;&#32454;&#20449;&#24687;&#65292;&#20016;&#23500;&#20102;&#23427;&#22312;&#22810;&#26041;&#23545;&#35805;&#30740;&#31350;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10943v1 Announce Type: cross  Abstract: Multimodal intent recognition poses significant challenges, requiring the incorporation of non-verbal modalities from real-world contexts to enhance the comprehension of human intentions. Existing benchmark datasets are limited in scale and suffer from difficulties in handling out-of-scope samples that arise in multi-turn conversational interactions. We introduce MIntRec2.0, a large-scale benchmark dataset for multimodal intent recognition in multi-party conversations. It contains 1,245 dialogues with 15,040 samples, each annotated within a new intent taxonomy of 30 fine-grained classes. Besides 9,304 in-scope samples, it also includes 5,736 out-of-scope samples appearing in multi-turn contexts, which naturally occur in real-world scenarios. Furthermore, we provide comprehensive information on the speakers in each utterance, enriching its utility for multi-party conversational research. We establish a general framework supporting the o
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#20849;&#20139;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;&#24187;&#35273;&#65292;&#20197;&#21450;&#21442;&#19982;&#32773;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.07726</link><description>&lt;p&gt;
SemEval-2024&#20849;&#20139;&#20219;&#21153;6: SHROOM&#65292;&#19968;&#20010;&#20851;&#20110;&#24187;&#35273;&#21450;&#30456;&#20851;&#21487;&#35266;&#23519;&#36807;&#24230;&#29983;&#25104;&#38169;&#35823;&#30340;&#20849;&#20139;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
SemEval-2024 Shared Task 6: SHROOM, a Shared-task on Hallucinations and Related Observable Overgeneration Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#20849;&#20139;&#20219;&#21153;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;&#24187;&#35273;&#65292;&#20197;&#21450;&#21442;&#19982;&#32773;&#20351;&#29992;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SHROOM&#30340;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#26816;&#27979;&#24187;&#35273;&#30340;&#20849;&#20139;&#20219;&#21153;&#65306;&#21363;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#30340;&#36755;&#20986;&#27969;&#30021;&#20294;&#19981;&#20934;&#30830;&#12290;&#36825;&#31181;&#36807;&#24230;&#29983;&#25104;&#30340;&#24773;&#20917;&#21487;&#33021;&#21361;&#21450;&#35768;&#22810;NLG&#24212;&#29992;&#65292;&#20854;&#20013;&#27491;&#30830;&#24615;&#24448;&#24448;&#33267;&#20851;&#37325;&#35201;&#12290;&#20849;&#20139;&#20219;&#21153;&#20351;&#29992;&#20102;&#19968;&#20010;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;4000&#20010;&#30001;5&#20010;&#26631;&#27880;&#32773;&#26631;&#35760;&#30340;&#27169;&#22411;&#36755;&#20986;&#65292;&#28085;&#30422;&#20102;3&#20010;NLP&#20219;&#21153;&#65306;&#26426;&#22120;&#32763;&#35793;&#12289;&#37322;&#20041;&#29983;&#25104;&#21644;&#23450;&#20041;&#24314;&#27169;&#12290; &#20849;&#20139;&#20219;&#21153;&#30001;58&#20010;&#19981;&#21516;&#29992;&#25143;&#32452;&#25104;&#30340;42&#25903;&#22242;&#38431;&#20849;&#21516;&#35299;&#20915;&#65292;&#20854;&#20013;27&#25903;&#36873;&#25321;&#25776;&#20889;&#31995;&#32479;&#25551;&#36848;&#35770;&#25991;&#65307;&#20182;&#20204;&#20849;&#25552;&#20132;&#20102;&#36229;&#36807;300&#20010;&#39044;&#27979;&#38598;&#22312;&#20849;&#20139;&#20219;&#21153;&#30340;&#20004;&#20010;&#36319;&#36394;&#19978;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#34987;&#22788;&#29702;&#30340;&#19968;&#20123;&#20851;&#38190;&#36235;&#21183;--&#35768;&#22810;&#21442;&#19982;&#32773;&#20381;&#36182;&#23569;&#25968;&#27169;&#22411;&#65292;&#24182;&#32463;&#24120;&#20381;&#36182;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#25110;&#38646;&#26679;&#26412;&#25552;&#31034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07726v1 Announce Type: new  Abstract: This paper presents the results of the SHROOM, a shared task focused on detecting hallucinations: outputs from natural language generation (NLG) systems that are fluent, yet inaccurate. Such cases of overgeneration put in jeopardy many NLG applications, where correctness is often mission-critical. The shared task was conducted with a newly constructed dataset of 4000 model outputs labeled by 5 annotators each, spanning 3 NLP tasks: machine translation, paraphrase generation and definition modeling.   The shared task was tackled by a total of 58 different users grouped in 42 teams, out of which 27 elected to write a system description paper; collectively, they submitted over 300 prediction sets on both tracks of the shared task. We observe a number of key trends in how this approach was tackled -- many participants rely on a handful of model, and often rely either on synthetic data for fine-tuning or zero-shot prompting strategies. While 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35780;&#35770;&#26469;&#23454;&#29616;&#35266;&#28857;&#25688;&#35201;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#36991;&#20813;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#39640;&#26114;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.07693</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#29992;&#20110;&#21435;&#20559;&#35265;&#35266;&#28857;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07693
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#37325;&#26032;&#29983;&#25104;&#35780;&#35770;&#26469;&#23454;&#29616;&#35266;&#28857;&#25688;&#35201;&#30340;&#21435;&#20559;&#35265;&#21270;&#65292;&#36991;&#20813;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#23384;&#22312;&#30340;&#38382;&#39064;&#21644;&#39640;&#26114;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#35266;&#28857;&#25688;&#35201;&#25968;&#25454;&#38598;&#20013;&#36229;&#36807;70&#65285;&#30340;&#35780;&#35770;&#26159;&#31215;&#26497;&#30340;&#65292;&#24403;&#21069;&#30340;&#35266;&#28857;&#25688;&#35201;&#26041;&#27861;&#22312;&#32473;&#23450;&#36127;&#38754;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#19981;&#24895;&#29983;&#25104;&#36127;&#38754;&#25688;&#35201;&#65292;&#36896;&#25104;&#24773;&#24863;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#24863;&#20559;&#35265;&#65292;&#19968;&#20010;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39069;&#22806;&#30340;&#25968;&#25454;&#65292;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#32780;&#19981;&#36807;&#20998;&#20381;&#36182;&#29305;&#23450;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#38754;&#20020;&#20004;&#20010;&#32570;&#28857;&#65306;1&#65289;&#22686;&#24378;&#25968;&#25454;&#20013;&#30340;&#28508;&#22312;&#38382;&#39064;&#25110;&#27602;&#24615;&#65307;2&#65289;&#26114;&#36149;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#21644;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#65292;&#29992;&#20110;&#21435;&#20559;&#35265;&#35266;&#28857;&#25688;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#20889;&#27491;&#38754;&#25991;&#26412;&#33719;&#24471;&#20102;&#23567;&#35268;&#27169;&#21512;&#25104;&#30340;&#36127;&#38754;&#35780;&#35770;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#29983;&#25104;&#30340;&#20869;&#23481;&#35757;&#32451;&#19968;&#20010;&#35299;&#32806;&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07693v1 Announce Type: cross  Abstract: As more than 70$\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.06832</link><description>&lt;p&gt;
&#22122;&#22768;&#30340;&#21147;&#37327;&#65306;&#26397;&#30528;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Power of Noise: Toward a Unified Multi-modal Knowledge Graph Representation Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06832
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22122;&#22768;&#25513;&#27169;&#30340;Transformer-based&#26550;&#26500;SNAG&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#34920;&#31034;&#20013;&#23454;&#20307;&#23884;&#20837;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#36827;&#23637;&#20984;&#26174;&#20986;&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#65288;MMKG&#65289;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#24517;&#35201;&#24615;&#12290;&#27492;&#26694;&#26550;&#23545;&#20110;&#22312;&#35268;&#27169;&#19978;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#25972;&#21512;&#21040;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#20943;&#36731;&#30693;&#35782;&#35823;&#35299;&#21644;&#22810;&#27169;&#24577;&#24187;&#35273;&#31561;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#20934;&#30830;&#23884;&#20837;MMKG&#20013;&#30340;&#23454;&#20307;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#20010;&#24191;&#27867;&#30740;&#31350;&#30340;&#20219;&#21153;&#65306;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;MKGC&#65289;&#21644;&#22810;&#27169;&#24577;&#23454;&#20307;&#23545;&#40784;&#65288;MMEA&#65289;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;SNAG&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#37197;&#22791;&#20102;&#27169;&#24577;&#32423;&#22122;&#22768;&#25513;&#27169;&#65292;&#20197;&#22312;&#30693;&#35782;&#22270;&#20013;&#40065;&#26834;&#22320;&#38598;&#25104;&#22810;&#27169;&#24577;&#23454;&#20307;&#29305;&#24449;&#12290;&#36890;&#36807;&#20026;MKGC&#21644;MMEA&#37117;&#24341;&#20837;&#29305;&#23450;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24635;&#20849;&#21313;&#20010;&#25968;&#25454;&#38598;&#19978;&#65288;&#19977;&#20010;&#29992;&#20110;MKGC&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06832v1 Announce Type: cross  Abstract: The advancement of Multi-modal Pre-training highlights the necessity for a robust Multi-Modal Knowledge Graph (MMKG) representation learning framework. This framework is crucial for integrating structured knowledge into multi-modal Large Language Models (LLMs) at scale, aiming to alleviate issues like knowledge misconceptions and multi-modal hallucinations. In this work, to evaluate models' ability to accurately embed entities within MMKGs, we focus on two widely researched tasks: Multi-modal Knowledge Graph Completion (MKGC) and Multi-modal Entity Alignment (MMEA). Building on this foundation, we propose a novel SNAG method that utilizes a Transformer-based architecture equipped with modality-level noise masking for the robust integration of multi-modal entity features in KGs. By incorporating specific training objectives for both MKGC and MMEA, our approach achieves SOTA performance across a total of ten datasets (three for MKGC and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.03861</link><description>&lt;p&gt;
&#20026;&#23569;&#26679;&#26412;&#31034;&#20363;&#36873;&#25321;&#35774;&#35745;&#20449;&#24687;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Designing Informative Metrics for Few-Shot Example Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03861
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31034;&#20363;&#19982;&#27979;&#35797;&#21477;&#23376;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#23545;&#40784;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#25552;&#20379;&#36866;&#24403;&#26684;&#24335;&#30340;&#31034;&#20363;&#26102;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#8220;&#26368;&#20339;&#8221;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22797;&#26434;&#24230;&#30340;&#25552;&#31034;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#24207;&#21015;&#26631;&#27880;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#36991;&#20813;&#20102;&#35757;&#32451;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36873;&#25321;&#31034;&#20363;&#30340;&#27169;&#22411;&#65292;&#32780;&#26159;&#20351;&#29992;&#29305;&#23450;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#23545;&#40784;&#27979;&#35797;&#21477;&#23376;&#21644;&#31034;&#20363;&#30340;&#21477;&#27861;-&#35821;&#20041;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#21477;&#23376;&#21644;&#21333;&#35789;&#32423;&#21035;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23558;&#31034;&#20363;&#30340;&#22797;&#26434;&#24230;&#19982;&#32771;&#34385;&#20013;&#30340;&#65288;&#27979;&#35797;&#65289;&#21477;&#23376;&#36827;&#34892;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;PLMs&#20013;&#25552;&#21462;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65306;&#22312;&#23569;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CoNLL2003&#25968;&#25454;&#38598;&#19978;&#23545;GPT-4&#30340;F1&#20998;&#25968;&#23454;&#29616;&#20102;5%&#30340;&#32477;&#23545;&#25913;&#21892;&#12290;&#25105;&#20204;&#36824;&#22312;&#20687;GPT-j-6B&#36825;&#26679;&#30340;&#36739;&#23567;&#27169;&#22411;&#20013;&#30475;&#21040;&#20102;&#39640;&#36798;28.85&#20010;&#28857;&#65288;F1/Acc.&#65289;&#30340;&#26174;&#33879;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03861v1 Announce Type: new  Abstract: Pretrained language models (PLMs) have shown remarkable few-shot learning capabilities when provided with properly formatted examples. However, selecting the "best" examples remains an open challenge. We propose a complexity-based prompt selection approach for sequence tagging tasks. This approach avoids the training of a dedicated model for selection of examples, and instead uses certain metrics to align the syntactico-semantic complexity of test sentences and examples. We use both sentence- and word-level metrics to match the complexity of examples to the (test) sentence being considered. Our results demonstrate that our approach extracts greater performance from PLMs: it achieves state-of-the-art performance on few-shot NER, achieving a 5% absolute improvement in F1 score on the CoNLL2003 dataset for GPT-4. We also see large gains of upto 28.85 points (F1/Acc.) in smaller models like GPT-j-6B.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02889</link><description>&lt;p&gt;
&#22312;&#23547;&#25214;&#30495;&#30456;&#65306;&#19968;&#31181;&#23457;&#38382;&#26041;&#27861;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
In Search of Truth: An Interrogation Approach to Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02889
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26816;&#27979;&#24187;&#35273;&#30340;&#26032;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#26102;&#36935;&#21040;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#35768;&#22810;&#36827;&#23637;&#24182;&#19988;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#36895;&#24230;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#23427;&#20204;&#23545;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#30340;&#24433;&#21709;&#21644;&#25972;&#21512;&#20173;&#28982;&#26377;&#38480;&#12290;&#19968;&#20010;&#38459;&#30861;&#23427;&#20204;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#22240;&#32032;&#26159;&#24187;&#35273;&#30340;&#21457;&#29983;&#65292;&#21363;LLMs&#21019;&#36896;&#20986;&#21548;&#36215;&#26469;&#30495;&#23454;&#20294;&#20559;&#31163;&#20107;&#23454;&#30495;&#30456;&#30340;&#31572;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#65292;&#36825;&#35299;&#20915;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#29616;&#23454;&#22330;&#26223;&#20013;&#24212;&#29992;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;LLMs&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;Llama-2&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26368;&#26032;LLMs&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#26816;&#27979;&#23427;&#20204;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#29305;&#23450;&#23454;&#39564;&#20013;&#35266;&#23519;&#21040;Llama-2&#36798;&#21040;62%&#30340;&#24187;&#35273;&#27700;&#24179;&#65292;&#32780;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27809;&#26377;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;87%&#30340;&#24179;&#34913;&#20934;&#30830;&#29575;&#65288;B-ACC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02889v1 Announce Type: new  Abstract: Despite the many advances of Large Language Models (LLMs) and their unprecedented rapid evolution, their impact and integration into every facet of our daily lives is limited due to various reasons. One critical factor hindering their widespread adoption is the occurrence of hallucinations, where LLMs invent answers that sound realistic, yet drift away from factual truth. In this paper, we present a novel method for detecting hallucinations in large language models, which tackles a critical issue in the adoption of these models in various real-world scenarios. Through extensive evaluations across multiple datasets and LLMs, including Llama-2, we study the hallucination levels of various recent LLMs and demonstrate the effectiveness of our method to automatically detect them. Notably, we observe up to 62% hallucinations for Llama-2 in a specific experiment, where our method achieves a Balanced Accuracy (B-ACC) of 87%, all without relying 
&lt;/p&gt;</description></item><item><title>&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.00758</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36870;&#36716;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Mitigating Reversal Curse via Semantic-aware Permutation Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00758
&lt;/p&gt;
&lt;p&gt;
&#36870;&#36716;&#35781;&#21650;&#38382;&#39064;&#26159;&#23548;&#33268;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#36827;&#34892;&#21452;&#21521;&#25512;&#29702;&#30340;&#26681;&#26412;&#21407;&#22240;&#20043;&#19968;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#35821;&#20041;&#24863;&#30693;&#30340;&#32622;&#25442;&#35757;&#32451;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22240;&#26524;&#20851;&#31995;&#30340;LLM&#36973;&#36935;&#20102;&#8220;&#36870;&#36716;&#35781;&#21650;&#8221;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#20363;&#23376;&#26159;&#65292;&#27169;&#22411;&#30693;&#36947;&#8220;A&#30340;&#29238;&#20146;&#26159;B&#8221;&#65292;&#20294;&#26080;&#27861;&#25512;&#29702;&#20986;&#8220;B&#30340;&#23401;&#23376;&#26159;A&#8221;&#12290;&#36825;&#19968;&#23616;&#38480;&#24615;&#23545;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36827;&#23637;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#26263;&#31034;&#20102;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#24212;&#29992;&#21452;&#21521;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#39318;&#20808;&#36827;&#34892;&#20102;&#22823;&#37327;&#35780;&#20272;&#65292;&#24182;&#30830;&#23450;&#20102;&#36870;&#36716;&#35781;&#21650;&#30340;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#20043;&#38388;&#30340;&#35789;&#24207;&#19981;&#21516;&#65292;&#21363;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#39044;&#27979;&#20808;&#34892;&#35789;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#32771;&#34385;&#21040;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#36827;&#34892;&#25490;&#21015;&#21487;&#20197;&#34987;&#35270;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#36825;&#21487;&#20197;&#20351;&#27169;&#22411;&#39044;&#27979;&#20808;&#34892;&#35789;&#25110;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#25490;&#21015;&#26041;&#27861;&#21487;&#33021;&#21463;&#21040;&#25130;&#26029;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00758v1 Announce Type: cross  Abstract: While large language models (LLMs) have achieved impressive performance across diverse tasks, recent studies showcase that causal LLMs suffer from the "reversal curse". It is a typical example that the model knows "A's father is B", but is unable to reason "B's child is A". This limitation poses a challenge to the advancement of artificial general intelligence (AGI), as it suggests a gap in the models' ability to comprehend and apply bidirectional reasoning. In this paper, we first conduct substantial evaluation and identify that the root cause of the reversal curse lies in the different word order between the training and inference stage, namely, the poor ability of causal language models to predict antecedent words within the training data. Accordingly, permutation on the training data is considered as a potential solution, since this can make the model predict antecedent words or tokens. However, previous permutation methods may dis
&lt;/p&gt;</description></item><item><title>AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15506</link><description>&lt;p&gt;
AgentOhana&#65306;&#20026;&#26377;&#25928;&#26234;&#33021;&#20307;&#23398;&#20064;&#35774;&#35745;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;
&lt;/p&gt;
&lt;p&gt;
AgentOhana: Design Unified Data and Training Pipeline for Effective Agent Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15506
&lt;/p&gt;
&lt;p&gt;
AgentOhana&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#25968;&#25454;&#21644;&#35757;&#32451;&#27969;&#27700;&#32447;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#21161;&#20110;&#20811;&#26381;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#26234;&#33021;&#20307;&#20219;&#21153;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20379;&#25903;&#25345;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#24341;&#36215;&#20102;&#37325;&#22823;&#30740;&#31350;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#28508;&#21147;&#36827;&#34892;&#22522;&#20110;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#38754;&#20020;&#22256;&#38590;&#65292;&#36825;&#26159;&#30001;&#20110;&#20855;&#26377;&#22810;&#36718;&#36712;&#36857;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#28304;&#30340;&#24322;&#26500;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;AgentOhana&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#32508;&#21512;&#35299;&#20915;&#26041;&#26696;&#12290;AgentOhana&#20174;&#19981;&#21516;&#29615;&#22659;&#20013;&#32858;&#21512;&#26234;&#33021;&#20307;&#36712;&#36857;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#24773;&#26223;&#12290;&#23427;&#31934;&#24515;&#22320;&#23558;&#36825;&#20123;&#36712;&#36857;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21040;&#19968;&#33268;&#30340;&#26684;&#24335;&#20013;&#65292;&#31616;&#21270;&#20102;&#20026;&#26234;&#33021;&#20307;&#35757;&#32451;&#20248;&#21270;&#30340;&#36890;&#29992;&#25968;&#25454;&#21152;&#36733;&#22120;&#30340;&#21019;&#24314;&#12290;&#36890;&#36807;&#25968;&#25454;&#32479;&#19968;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#27969;&#27700;&#32447;&#22312;&#19981;&#21516;&#25968;&#25454;&#28304;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#21010;&#20998;&#21644;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#35774;&#22791;&#20043;&#38388;&#30340;&#29420;&#31435;&#38543;&#26426;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;xLAM-v0.1&#65292;&#19968;&#20010;&#22823;&#21160;&#20316;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15506v1 Announce Type: new  Abstract: Autonomous agents powered by large language models (LLMs) have garnered significant research attention. However, fully harnessing the potential of LLMs for agent-based tasks presents inherent challenges due to the heterogeneous nature of diverse data sources featuring multi-turn trajectories. In this paper, we introduce \textbf{AgentOhana} as a comprehensive solution to address these challenges. \textit{AgentOhana} aggregates agent trajectories from distinct environments, spanning a wide array of scenarios. It meticulously standardizes and unifies these trajectories into a consistent format, streamlining the creation of a generic data loader optimized for agent training. Leveraging the data unification, our training pipeline maintains equilibrium across different data sources and preserves independent randomness across devices during dataset partitioning and model training. Additionally, we present \textbf{xLAM-v0.1}, a large action mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.08015</link><description>&lt;p&gt;
&#22686;&#24378;Amharic-LLaMA: &#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#19982;&#29983;&#25104;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Enhancing Amharic-LLaMA: Integrating Task Specific and Generative Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#20219;&#21153;&#29305;&#23450;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;Amharic-LLaMA&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#38463;&#22982;&#21704;&#25289;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36890;&#36807;&#21019;&#24314;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22240;&#20854;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#20013;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#22240;&#32570;&#20047;&#36164;&#28304;&#32780;&#34987;&#33853;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#25972;&#21512;&#29305;&#23450;&#20219;&#21153;&#21644;&#29983;&#25104;&#25968;&#25454;&#38598;&#26469;&#22686;&#24378;LLaMA-2-Amharic&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38463;&#22982;&#21704;&#25289;&#35821;&#30340;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#38463;&#22982;&#21704;&#25289;&#35821;&#25351;&#20196;&#24494;&#35843;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;LLaMA-2-Amharic&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#24320;&#28304;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21019;&#24314;&#27969;&#31243;&#12289;&#25351;&#20196;&#25968;&#25454;&#38598;&#12289;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#36755;&#20986;&#65292;&#20197;&#20419;&#36827;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#35821;&#35328;&#29305;&#23450;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have received a lot of attention in natural language processing (NLP) research because of their exceptional performance in understanding and generating human languages. However, low-resource languages are left behind due to the unavailability of resources. In this work, we focus on enhancing the LLaMA-2-Amharic model by integrating task-specific and generative datasets to improve language model performance for Amharic. We compile an Amharic instruction fine-tuning dataset and fine-tuned LLaMA-2-Amharic model. The fine-tuned model shows promising results in different NLP tasks. We open-source our dataset creation pipeline, instruction datasets, trained models, and evaluation outputs to promote language-specific studies on these models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20542;&#21521;&#20110;&#29983;&#25104;&#20887;&#20313;&#30340;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#21363;&#20351;&#36825;&#20123;&#35745;&#31639;&#24182;&#19981;&#24517;&#35201;&#12290;</title><link>https://arxiv.org/abs/2401.11467</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36807;&#24230;&#25512;&#29702;&#21644;&#20887;&#20313;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Over-Reasoning and Redundant Calculation of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#20542;&#21521;&#20110;&#29983;&#25104;&#20887;&#20313;&#30340;&#35745;&#31639;&#21644;&#25512;&#29702;&#65292;&#21363;&#20351;&#36825;&#20123;&#35745;&#31639;&#24182;&#19981;&#24517;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#36880;&#27493;&#35299;&#20915;&#38382;&#39064;&#12290;&#23613;&#31649;&#36825;&#31181;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#25552;&#21319;&#20102;LLMs&#30340;&#24615;&#33021;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;LLMs&#20309;&#26102;&#20250;&#20351;&#29992;CoT&#65292;&#20197;&#21450;&#36825;&#20123;CoT&#26159;&#21542;&#24635;&#26159;&#24517;&#35201;&#30340;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#22312;&#19968;&#20010;&#25163;&#21160;&#26500;&#24314;&#30340;&#25968;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;GSM8K-Zero&#19978;&#29983;&#25104;&#20887;&#20313;&#30340;&#35745;&#31639;&#21644;&#25512;&#29702;&#12290;GSM8K-Zero&#34987;&#26500;&#24314;&#20026;&#36825;&#26679;&#30340;&#38382;&#31572;&#21487;&#20197;&#22312;&#19981;&#20570;&#20219;&#20309;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#22238;&#31572;&#65292;&#20294;LLMs&#65292;&#21253;&#25324;Llama-2&#27169;&#22411;&#21644;Claude-2&#65292;&#20542;&#21521;&#20110;&#29983;&#25104;&#20887;&#38271;&#19988;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#26469;&#22238;&#31572;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#23454;&#39564;&#35299;&#37322;&#20026;&#20160;&#20040;LLMs&#20250;&#29983;&#25104;&#20887;&#20313;&#30340;&#35745;&#31639;&#21644;&#25512;&#29702;&#12290;GSM8K-Zero&#21487;&#20197;&#22312;https://github.com/d223302/Over-Reasoning-of-LLMs &#21644;https://huggingface.co/datasets/dcml0714/GSM8K-Zero &#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11467v2 Announce Type: replace  Abstract: Large language models (LLMs) can solve problems step-by-step. While this chain-of-thought (CoT) reasoning boosts LLMs' performance, it is unclear if LLMs \textit{know} when to use CoT and whether those CoT are always necessary to answer the question. This paper shows that LLMs tend to generate redundant calculations and reasoning on a manually constructed math QA dataset, GSM8K-Zero. GSM8K-Zero is constructed such that the questions can be answered without any calculations, but LLMs, including Llama-2 models and Claude-2, tend to generate lengthy and unnecessary calculations to answer the questions. We also conduct experiments to explain why LLMs generate redundant calculations and reasonings. GSM8K-Zero is publicly available at https://github.com/d223302/Over-Reasoning-of-LLMs and https://huggingface.co/datasets/dcml0714/GSM8K-Zero.
&lt;/p&gt;</description></item><item><title>PsyChat &#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#23458;&#25143;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#23458;&#25143;&#34892;&#20026;&#24182;&#32467;&#21512;&#36741;&#23548;&#21592;&#31574;&#30053;&#65292;&#22312;&#32447;&#25552;&#20379;&#24515;&#29702;&#25903;&#25345;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#20013;&#23545;&#35805;&#31995;&#32479;&#24573;&#35270;&#23458;&#25143;&#34892;&#20026;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2312.04262</link><description>&lt;p&gt;
PsyChat&#65306;&#38754;&#21521;&#23458;&#25143;&#30340;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PsyChat: A Client-Centric Dialogue System for Mental Health Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04262
&lt;/p&gt;
&lt;p&gt;
PsyChat &#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#23458;&#25143;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#35782;&#21035;&#23458;&#25143;&#34892;&#20026;&#24182;&#32467;&#21512;&#36741;&#23548;&#21592;&#31574;&#30053;&#65292;&#22312;&#32447;&#25552;&#20379;&#24515;&#29702;&#25903;&#25345;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#20013;&#23545;&#35805;&#31995;&#32479;&#24573;&#35270;&#23458;&#25143;&#34892;&#20026;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#20013;&#65292;&#20197;&#24110;&#21161;&#23458;&#25143;&#20419;&#36827;&#25506;&#32034;&#65292;&#33719;&#24471;&#27934;&#23519;&#21147;&#65292;&#37319;&#21462;&#34892;&#21160;&#65292;&#26368;&#32456;&#33258;&#24840;&#12290;&#19968;&#20010;&#23454;&#29992;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#23545;&#35805;&#31995;&#32479;&#24212;&#24403;&#20197;&#23458;&#25143;&#20026;&#20013;&#24515;&#65292;&#19987;&#27880;&#20110;&#23458;&#25143;&#30340;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20844;&#24320;&#21487;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#23545;&#35805;&#31995;&#32479;&#24448;&#24448;&#21482;&#38598;&#20013;&#20110;&#36741;&#23548;&#21592;&#30340;&#31574;&#30053;&#65292;&#32780;&#24573;&#35270;&#20102;&#23458;&#25143;&#34920;&#36798;&#30340;&#34892;&#20026;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#23545;&#35805;&#31995;&#32479;&#20135;&#29983;&#19981;&#21512;&#29702;&#25110;&#19981;&#36866;&#24403;&#30340;&#36741;&#23548;&#31574;&#30053;&#20197;&#21450;&#30456;&#24212;&#30340;&#22238;&#24212;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PsyChat&#65292;&#19968;&#20010;&#36890;&#36807;&#22312;&#32447;&#32842;&#22825;&#25552;&#20379;&#24515;&#29702;&#25903;&#25345;&#30340;&#38754;&#21521;&#23458;&#25143;&#30340;&#23545;&#35805;&#31995;&#32479;&#12290;&#35813;&#23545;&#35805;&#31995;&#32479;&#21253;&#25324;&#20116;&#20010;&#27169;&#22359;&#65306;&#23458;&#25143;&#34892;&#20026;&#35782;&#21035;&#12289;&#36741;&#23548;&#21592;&#31574;&#30053;&#36873;&#25321;&#12289;&#36755;&#20837;&#25171;&#21253;&#22120;&#12289;&#22238;&#24212;&#29983;&#25104;&#22120;&#21644;&#22238;&#24212;&#36873;&#25321;&#12290;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04262v2 Announce Type: replace  Abstract: Dialogue systems are increasingly integrated into mental health support to help clients facilitate exploration, gain insight, take action, and ultimately heal themselves. A practical and user-friendly dialogue system should be client-centric, focusing on the client's behaviors. However, existing dialogue systems publicly available for mental health support often concentrate solely on the counselor's strategies rather than the behaviors expressed by clients. This can lead to unreasonable or inappropriate counseling strategies and corresponding responses generated by the dialogue system. To address this issue, we propose PsyChat, a client-centric dialogue system that provides psychological support through online chat. The client-centric dialogue system comprises five modules: client behavior recognition, counselor strategy selection, input packer, response generator, and response selection. Both automatic and human evaluations demonstr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#26576;&#20123;&#31867;&#22411;&#30340;&#20107;&#23454;&#26102;&#20855;&#26377;&#22266;&#26377;&#30340;&#32479;&#35745;&#19979;&#38480;&#65292;&#36825;&#19982;&#21464;&#21387;&#22120;LM&#26550;&#26500;&#25110;&#25968;&#25454;&#36136;&#37327;&#26080;&#20851;&#12290;</title><link>https://arxiv.org/abs/2311.14648</link><description>&lt;p&gt;
&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#24517;&#39035;&#20986;&#29616;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Calibrated Language Models Must Hallucinate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14648
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#26576;&#20123;&#31867;&#22411;&#30340;&#20107;&#23454;&#26102;&#20855;&#26377;&#22266;&#26377;&#30340;&#32479;&#35745;&#19979;&#38480;&#65292;&#36825;&#19982;&#21464;&#21387;&#22120;LM&#26550;&#26500;&#25110;&#25968;&#25454;&#36136;&#37327;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#39057;&#32321;&#29983;&#25104;&#34394;&#20551;&#20294;&#21548;&#36215;&#26469;&#20284;&#20046;&#21512;&#29702;&#30340;&#25991;&#26412;&#12290;&#36825;&#31181;&#8220;&#24187;&#35273;&#8221;&#26159;&#35821;&#35328;&#20026;&#22522;&#30784;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#29992;&#24615;&#30340;&#38556;&#30861;&#65292;&#24182;&#21487;&#33021;&#20260;&#23475;&#20381;&#36182;&#20854;&#36755;&#20986;&#30340;&#20154;&#20204;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#24187;&#24819;&#26576;&#20123;&#31867;&#22411;&#30340;&#20107;&#23454;&#20855;&#26377;&#22266;&#26377;&#30340;&#32479;&#35745;&#19979;&#38480;&#65292;&#19982;&#21464;&#21387;&#22120;LM&#26550;&#26500;&#25110;&#25968;&#25454;&#36136;&#37327;&#26080;&#20851;&#12290;&#23545;&#20110;&#37027;&#20123;&#26080;&#27861;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#30830;&#23450;&#30495;&#23454;&#24615;&#30340;&#8220;&#20219;&#24847;&#8221;&#20107;&#23454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#28385;&#36275;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36866;&#24403;&#32479;&#35745;&#26657;&#20934;&#26465;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24187;&#35273;&#24517;&#39035;&#20197;&#26576;&#31181;&#36895;&#29575;&#21457;&#29983;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22914;&#26524;&#20219;&#20309;&#20107;&#23454;&#30340;&#26368;&#22823;&#27010;&#29575;&#34987;&#38480;&#21046;&#65292;&#25105;&#20204;&#34920;&#26126;&#29983;&#25104;&#24187;&#35273;&#30340;&#27010;&#29575;&#25509;&#36817;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20165;&#21457;&#29983;&#19968;&#27425;&#30340;&#20107;&#23454;&#30340;&#27604;&#20363;&#65288;&#8220;Good-Turing&#8221;&#20272;&#35745;&#65289;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;&#21487;&#33021;&#30340;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14648v3 Announce Type: replace  Abstract: Recent language models generate false but plausible-sounding text with surprising frequency. Such "hallucinations" are an obstacle to the usability of language-based AI systems and can harm people who rely upon their outputs. This work shows that there is an inherent statistical lower-bound on the rate that pretrained language models hallucinate certain types of facts, having nothing to do with the transformer LM architecture or data quality. For "arbitrary" facts whose veracity cannot be determined from the training data, we show that hallucinations must occur at a certain rate for language models that satisfy a statistical calibration condition appropriate for generative language models. Specifically, if the maximum probability of any fact is bounded, we show that the probability of generating a hallucination is close to the fraction of facts that occur exactly once in the training data (a "Good-Turing" estimate), even assuming ide
&lt;/p&gt;</description></item><item><title>&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2311.07838</link><description>&lt;p&gt;
LLatrieval&#65306;LLM-&#39564;&#35777;&#26816;&#32034;&#29992;&#20110;&#21487;&#39564;&#35777;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LLatrieval: LLM-Verified Retrieval for Verifiable Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07838
&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#26816;&#32034;&#30340;&#25991;&#20214;&#19981;&#20165;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#65292;&#20294;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#24050;&#25104;&#20026;&#24615;&#33021;&#29942;&#39048;&#65292;&#38656;&#35201;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#39564;&#35777;&#29983;&#25104;&#26088;&#22312;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#20855;&#26377;&#25903;&#25745;&#25991;&#20214;&#30340;&#25991;&#26412;&#65292;&#36825;&#20351;&#29992;&#25143;&#33021;&#22815;&#28789;&#27963;&#39564;&#35777;&#31572;&#26696;&#65292;&#24182;&#20351;LLM&#30340;&#36755;&#20986;&#26356;&#21487;&#38752;&#12290;&#26816;&#32034;&#22312;&#21487;&#39564;&#35777;&#29983;&#25104;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26816;&#32034;&#21040;&#30340;&#25991;&#20214;&#19981;&#20165;&#34917;&#20805;&#30693;&#35782;&#20197;&#24110;&#21161;LLM&#29983;&#25104;&#27491;&#30830;&#31572;&#26696;&#65292;&#36824;&#20316;&#20026;&#25903;&#25345;&#29992;&#25143;&#39564;&#35777;LLM&#36755;&#20986;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#32034;&#22120;&#25104;&#20026;&#25972;&#20010;&#27969;&#31243;&#30340;&#29942;&#39048;&#65292;&#24182;&#38480;&#21046;&#20102;&#25972;&#20307;&#24615;&#33021;&#12290;&#30001;&#20110;&#36890;&#24120;&#20855;&#26377;&#30340;&#21442;&#25968;&#27604;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#24471;&#22810;&#65292;&#24182;&#19988;&#23578;&#26410;&#35777;&#26126;&#33021;&#22815;&#33391;&#22909;&#22320;&#25193;&#23637;&#21040;LLM&#30340;&#35268;&#27169;&#65292;&#22240;&#27492;&#23427;&#20204;&#30340;&#33021;&#21147;&#36890;&#24120;&#27604;LLMs&#24046;&#12290;&#22914;&#26524;&#26816;&#32034;&#22120;&#26410;&#33021;&#27491;&#30830;&#25214;&#21040;&#25903;&#25345;&#25991;&#20214;&#65292;&#21017;LLM&#23558;&#26080;&#27861;&#29983;&#25104;&#27491;&#30830;&#21644;&#21487;&#39564;&#35777;&#30340;&#31572;&#26696;&#65292;&#36825;&#20250;&#25513;&#30422;LLM&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07838v2 Announce Type: replace  Abstract: Verifiable generation aims to let the large language model (LLM) generate text with supporting documents, which enables the user to flexibly verify the answer and makes the LLM's output more reliable. Retrieval plays a crucial role in verifiable generation. Specifically, the retrieved documents not only supplement knowledge to help the LLM generate correct answers, but also serve as supporting evidence for the user to verify the LLM's output. However, the widely used retrievers become the bottleneck of the entire pipeline and limit the overall performance. Their capabilities are usually inferior to LLMs since they often have much fewer parameters than the large language model and have not been demonstrated to scale well to the size of LLMs. If the retriever does not correctly find the supporting documents, the LLM can not generate the correct and verifiable answer, which overshadows the LLM's remarkable abilities. To address these li
&lt;/p&gt;</description></item><item><title>AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.</title><link>https://arxiv.org/abs/2310.12963</link><description>&lt;p&gt;
AutoMix: &#33258;&#21160;&#28151;&#21512;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AutoMix: Automatically Mixing Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.12963
&lt;/p&gt;
&lt;p&gt;
AutoMix&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36873;&#25321;&#26356;&#22823;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#26597;&#35810;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#33258;&#25105;&#39564;&#35777;&#21644;&#20803;&#39564;&#35777;&#22120;&#25552;&#39640;&#20102;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#65292;&#21487;&#26174;&#33879;&#25552;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#30340;&#20248;&#21270;&#65292;&#23454;&#39564;&#35777;&#26126;&#24615;&#33021;&#20248;&#20110;&#22522;&#32447;&#26368;&#22810;86%.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29616;&#22312;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#23610;&#23544;&#21644;&#37197;&#32622;&#30340;&#20113;API&#25552;&#20379;&#21830;&#33719;&#24471;&#12290;&#34429;&#28982;&#36825;&#31181;&#22810;&#26679;&#24615;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#36873;&#25321;&#65292;&#20294;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#36873;&#39033;&#20197;&#20248;&#21270;&#35745;&#31639;&#25104;&#26412;&#21644;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoMix&#65292;&#19968;&#31181;&#26681;&#25454;&#36739;&#23567;LM&#30340;&#36755;&#20986;&#30340;&#36817;&#20284;&#27491;&#30830;&#24615;&#26469;&#31574;&#30053;&#24615;&#22320;&#23558;&#26597;&#35810;&#36335;&#30001;&#21040;&#26356;&#22823;LM&#30340;&#26041;&#27861;&#12290;AutoMix&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#23569;&#37327;&#26679;&#26412;&#30340;&#33258;&#25105;&#39564;&#35777;&#26426;&#21046;&#65292;&#23427;&#21487;&#20197;&#20272;&#35745;&#36755;&#20986;&#30340;&#21487;&#38752;&#24615;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#37492;&#20110;&#39564;&#35777;&#21487;&#33021;&#23384;&#22312;&#22122;&#22768;&#65292;&#25105;&#20204;&#22312;AutoMix&#20013;&#20351;&#29992;&#20102;&#20803;&#39564;&#35777;&#22120;&#26469;&#25552;&#39640;&#36825;&#20123;&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;LLAMA2-13B&#21644;GPT-4&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;AutoMix&#36229;&#36234;&#20102;&#24050;&#24314;&#31435;&#30340;&#22522;&#32447;&#65292;&#27599;&#21333;&#20301;&#25104;&#26412;&#30340;&#22686;&#37327;&#25928;&#30410;&#25552;&#39640;&#20102;&#26368;&#22810;86%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;https://github.c&#25214;&#21040;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.12963v3 Announce Type: replace  Abstract: Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta-verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13B and GPT-4, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 86%. Our code and data are available at https://github.c
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ParlaSent&#22810;&#35821;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#25919;&#27835;&#31185;&#23398;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#22810;&#35821;&#31181;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35758;&#20250;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#22312;&#35758;&#20250;&#20250;&#35758;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2309.09783</link><description>&lt;p&gt;
ParlaSent&#22810;&#35821;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#29992;&#20110;&#35758;&#20250;&#20250;&#35758;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
The ParlaSent Multilingual Training Dataset for Sentiment Identification in Parliamentary Proceedings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.09783
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ParlaSent&#22810;&#35821;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#39318;&#20010;&#19987;&#38376;&#38024;&#23545;&#25919;&#27835;&#31185;&#23398;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#22810;&#35821;&#31181;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#35758;&#20250;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#22312;&#35758;&#20250;&#20250;&#35758;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;7&#31181;&#35821;&#35328;&#30340;&#21477;&#23376;&#65292;&#25163;&#21160;&#26631;&#27880;&#20102;&#24773;&#24863;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#23454;&#39564;&#20013;&#29992;&#20110;&#35757;&#32451;&#19968;&#20010;&#38024;&#23545;&#35758;&#20250;&#20250;&#35758;&#30340;&#24773;&#24863;&#35782;&#21035;&#22120;&#12290;&#35813;&#35770;&#25991;&#36824;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#25919;&#27835;&#31185;&#23398;&#24212;&#29992;&#30340;&#39046;&#22495;&#29305;&#23450;&#22810;&#35821;&#31181;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#39069;&#22806;&#22312;27&#20010;&#27431;&#27954;&#35758;&#20250;&#30340;&#35758;&#20250;&#25991;&#20214;&#20013;&#39044;&#20808;&#35757;&#32451;&#20102;17.2&#20159;&#23383;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39069;&#22806;&#22312;&#35758;&#20250;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21363;&#22312;&#35758;&#20250;&#20250;&#35758;&#20013;&#36827;&#34892;&#24773;&#24863;&#35782;&#21035;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#22312;&#26410;&#22312;&#24494;&#35843;&#20013;&#35265;&#36807;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#24182;&#19988;&#26469;&#33258;&#20854;&#20182;&#35821;&#35328;&#30340;&#39069;&#22806;&#24494;&#35843;&#25968;&#25454;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#26631;&#35758;&#20250;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.09783v2 Announce Type: replace  Abstract: The paper presents a new training dataset of sentences in 7 languages, manually annotated for sentiment, which are used in a series of experiments focused on training a robust sentiment identifier for parliamentary proceedings. The paper additionally introduces the first domain-specific multilingual transformer language model for political science applications, which was additionally pre-trained on 1.72 billion words from parliamentary proceedings of 27 European parliaments. We present experiments demonstrating how the additional pre-training on parliamentary data can significantly improve the model downstream performance, in our case, sentiment identification in parliamentary proceedings. We further show that our multilingual model performs very well on languages not seen during fine-tuning, and that additional fine-tuning data from other languages significantly improves the target parliament's results. The paper makes an important 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;</title><link>https://arxiv.org/abs/2306.01931</link><description>&lt;p&gt;
&#25506;&#32034;&#30142;&#30149;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65306;&#29992;&#20110;&#20013;&#25991;&#30142;&#30149;&#35268;&#33539;&#21270;&#30340;&#31616;&#21333;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploring semantic information in disease: Simple Data Augmentation Techniques for Chinese Disease Normalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.01931
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#32454;&#24494;&#24046;&#21035;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30142;&#30149;&#21517;&#31216;&#35268;&#33539;&#21270;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#23427;&#23558;&#20197;&#21508;&#31181;&#26684;&#24335;&#32534;&#20889;&#30340;&#30142;&#30149;&#21517;&#31216;&#20998;&#31867;&#20026;&#26631;&#20934;&#21270;&#21517;&#31216;&#65292;&#20316;&#20026;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#21508;&#31181;&#19982;&#30142;&#30149;&#30456;&#20851;&#21151;&#33021;&#30340;&#22522;&#26412;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30142;&#30149;&#21517;&#31216;&#35268;&#33539;&#21270;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#22823;&#38556;&#30861;&#26159;&#35757;&#32451;&#25968;&#25454;&#20005;&#37325;&#19981;&#36275;&#12290;&#34429;&#28982;&#25968;&#25454;&#22686;&#24378;&#26159;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36890;&#24120;&#20250;&#38459;&#30861;&#20219;&#21153;&#24615;&#33021;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#30142;&#30149;&#21517;&#31216;&#30340;&#22810;&#36724;&#21644;&#22810;&#31890;&#24230;&#24615;&#36136;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#32452;&#23450;&#21046;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#26088;&#22312;&#21033;&#29992;&#30142;&#30149;&#21517;&#31216;&#20013;&#22266;&#26377;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#20123;&#25216;&#26415;&#26088;&#22312;&#22686;&#24378;&#27169;&#22411;&#23545;&#30142;&#30149;&#21517;&#31216;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#21644;&#20998;&#31867;&#32467;&#26500;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.01931v2 Announce Type: replace  Abstract: Disease name normalization is an important task in the medical domain. It classifies disease names written in various formats into standardized names, serving as a fundamental component in smart healthcare systems for various disease-related functions. Nevertheless, the most significant obstacle to existing disease name normalization systems is the severe shortage of training data. While data augmentation is a powerful approach for addressing data scarcity, our findings reveal that conventional data augmentation techniques often impede task performance, primarily due to the multi-axis and multi-granularity nature of disease names. Consequently, we introduce a set of customized data augmentation techniques designed to leverage the semantic information inherent in disease names. These techniques aim to enhance the model's understanding of the semantic intricacies and classification structure of disease names. Through extensive experime
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31163;&#25955;&#21477;&#23376;&#22686;&#24378;&#26041;&#26696;&#65306;&#26631;&#28857;&#25554;&#20837;&#12289;&#24773;&#24577;&#21160;&#35789;&#21644;&#21452;&#37325;&#21542;&#23450;&#65292;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2210.03963</link><description>&lt;p&gt;
SDA&#65306;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#31616;&#21333;&#31163;&#25955;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
SDA: Simple Discrete Augmentation for Contrastive Sentence Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.03963
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31163;&#25955;&#21477;&#23376;&#22686;&#24378;&#26041;&#26696;&#65306;&#26631;&#28857;&#25554;&#20837;&#12289;&#24773;&#24577;&#21160;&#35789;&#21644;&#21452;&#37325;&#21542;&#23450;&#65292;&#29992;&#20110;&#23545;&#27604;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#22522;&#26412;&#35201;&#32032;&#65292;&#25968;&#25454;&#22686;&#24378;&#21327;&#35758;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#23457;&#35270;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20551;&#35774;&#21512;&#29702;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#26399;&#26395;&#65292;&#25552;&#20986;&#20102;&#19977;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#31163;&#25955;&#21477;&#23376;&#22686;&#24378;&#26041;&#26696;&#65306;&#26631;&#28857;&#25554;&#20837;&#12289;&#24773;&#24577;&#21160;&#35789;&#21644;&#21452;&#37325;&#21542;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.03963v2 Announce Type: replace  Abstract: Contrastive learning has recently achieved compelling performance in unsupervised sentence representation. As an essential element, data augmentation protocols, however, have not been well explored. The pioneering work SimCSE resorting to a simple dropout mechanism (viewed as continuous augmentation) surprisingly dominates discrete augmentations such as cropping, word deletion, and synonym replacement as reported. To understand the underlying rationales, we revisit existing approaches and attempt to hypothesize the desiderata of reasonable data augmentation methods: balance of semantic consistency and expression diversity. We then develop three simple yet effective discrete sentence augmentation schemes: punctuation insertion, modal verbs, and double negation. They act as minimal noises at lexical level to produce diverse forms of sentences. Furthermore, standard negation is capitalized on to generate negative samples for alleviating
&lt;/p&gt;</description></item><item><title>&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25351;&#20196;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#39640;&#20272;&#65292;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21463;&#21040;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2205.00415</link><description>&lt;p&gt;
&#19981;&#35201;&#36131;&#24618;&#27880;&#37322;&#20154;&#21592;&#65306;&#20559;&#35265;&#24050;&#32463;&#24320;&#22987;&#20110;&#27880;&#37322;&#25351;&#20196;
&lt;/p&gt;
&lt;p&gt;
Don't Blame the Annotator: Bias Already Starts in the Annotation Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.00415
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#25351;&#20196;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#39640;&#20272;&#65292;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#30340;&#36827;&#23637;&#20027;&#35201;&#26159;&#30001;&#22522;&#20934;&#39537;&#21160;&#30340;&#12290;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#36890;&#36807;&#20247;&#21253;&#25910;&#38598;&#65292;&#27880;&#37322;&#20154;&#21592;&#26681;&#25454;&#25968;&#25454;&#38598;&#21019;&#24314;&#32773;&#21046;&#23450;&#30340;&#27880;&#37322;&#25351;&#20196;&#32534;&#20889;&#31034;&#20363;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#27880;&#37322;&#20154;&#21592;&#20250;&#27880;&#24847;&#21040;&#20247;&#21253;&#25351;&#20196;&#20013;&#30340;&#27169;&#24335;&#65292;&#20351;&#20182;&#20204;&#20889;&#20986;&#35768;&#22810;&#30456;&#20284;&#30340;&#31034;&#20363;&#65292;&#38543;&#21518;&#36825;&#20123;&#31034;&#20363;&#22312;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#34987;&#36807;&#24230;&#21576;&#29616;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#20559;&#35265;&#24418;&#24335;&#65292;&#31216;&#20043;&#20026;&#25351;&#20196;&#20559;&#35265;&#65292;&#22312;&#26368;&#36817;&#30340;14&#20010;NLU&#22522;&#20934;&#20013;&#23637;&#31034;&#20102;&#25351;&#20196;&#31034;&#20363;&#36890;&#24120;&#34920;&#29616;&#20986;&#20855;&#20307;&#27169;&#24335;&#65292;&#36825;&#20123;&#27169;&#24335;&#34987;&#24037;&#20154;&#32676;&#20307;&#20256;&#25773;&#21040;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#12290;&#36825;&#25193;&#23637;&#20102;&#20808;&#21069;&#30340;&#24037;&#20316;&#65288;Geva&#31561;&#65292;2019&#24180;&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20851;&#27880;&#28857;&#65292;&#21363;&#25105;&#20204;&#26159;&#21542;&#22312;&#27169;&#25311;&#25968;&#25454;&#38598;&#21019;&#24314;&#32773;&#30340;&#25351;&#20196;&#65292;&#32780;&#19981;&#26159;&#20219;&#21153;&#26412;&#36523;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25351;&#20196;&#20559;&#35265;&#30830;&#23454;&#21487;&#33021;&#23548;&#33268;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#36807;&#39640;&#20272;&#35745;&#65292;&#24182;&#19988;&#27169;&#22411;&#38590;&#20197;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.00415v3 Announce Type: replace  Abstract: In recent years, progress in NLU has been driven by benchmarks. These benchmarks are typically collected by crowdsourcing, where annotators write examples based on annotation instructions crafted by dataset creators. In this work, we hypothesize that annotators pick up on patterns in the crowdsourcing instructions, which bias them to write many similar examples that are then over-represented in the collected data. We study this form of bias, termed instruction bias, in 14 recent NLU benchmarks, showing that instruction examples often exhibit concrete patterns, which are propagated by crowdworkers to the collected data. This extends previous work (Geva et al., 2019) and raises a new concern of whether we are modeling the dataset creator's instructions, rather than the task. Through a series of experiments, we show that, indeed, instruction bias can lead to overestimation of model performance, and that models struggle to generalize bey
&lt;/p&gt;</description></item><item><title>BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14166</link><description>&lt;p&gt;
BayesPrompt: &#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#19978;&#25351;&#23548;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BayesPrompt: Prompting Large-Scale Pre-Trained Language Models on Few-shot Inference via Debiased Domain Abstraction. (arXiv:2401.14166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14166
&lt;/p&gt;
&lt;p&gt;
BayesPrompt&#36890;&#36807;&#26080;&#20559;&#39046;&#22495;&#25277;&#35937;&#35299;&#20915;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#23569;&#26679;&#26412;&#25512;&#29702;&#20013;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#26032;&#39062;&#26377;&#25928;&#30340;&#24494;&#35843;&#33539;&#24335;&#65292;prompt-tuning&#26088;&#22312;&#32553;&#23567;&#19979;&#28216;&#20219;&#21153;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;prompt-tuning&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25345;&#32493;&#36827;&#23637;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#25345;&#20037;&#30340;&#32570;&#38519;&#65306;prompt-tuning&#26041;&#27861;&#26080;&#27861;&#27867;&#21270;&#21040;&#29305;&#23450;&#30340;&#23569;&#26679;&#26412;&#27169;&#24335;&#12290;&#20174;&#20998;&#24067;&#20998;&#26512;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#19968;&#29616;&#35937;&#32972;&#21518;&#30340;&#20869;&#22312;&#38382;&#39064;&#26159;PLMs&#20013;&#21253;&#21547;&#36807;&#22810;&#30340;&#27010;&#24565;&#30693;&#35782;&#21644;&#30446;&#26631;&#19979;&#28216;&#39046;&#22495;&#30340;&#32553;&#20943;&#30693;&#35782;&#65292;&#20004;&#32773;&#20849;&#21516;&#23548;&#33268;PLMs&#22312;&#26222;&#36941;&#30340;&#30693;&#35782;&#23884;&#20837;&#31354;&#38388;&#20013;&#38169;&#35823;&#22320;&#23450;&#20301;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#23545;&#24212;&#30340;&#30693;&#35782;&#20998;&#24067;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30452;&#35266;&#22320;&#25506;&#32034;&#20102;&#20197;&#26080;&#20559;&#26041;&#24335;&#36924;&#36817;&#19979;&#28216;&#20219;&#21153;&#30340;&#23436;&#25972;&#30446;&#26631;&#39046;&#22495;&#65292;&#24182;&#36890;&#36807;&#25277;&#35937;&#36825;&#26679;&#30340;&#39046;&#22495;&#29983;&#25104;&#26377;&#21306;&#21035;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#26080;&#27495;&#20041;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a novel and effective fine-tuning paradigm based on large-scale pre-trained language models (PLMs), prompt-tuning aims to reduce the gap between downstream tasks and pre-training objectives. While prompt-tuning has yielded continuous advancements in various tasks, such an approach still remains a persistent defect: prompt-tuning methods fail to generalize to specific few-shot patterns. From the perspective of distribution analyses, we disclose that the intrinsic issues behind the phenomenon are the over-multitudinous conceptual knowledge contained in PLMs and the abridged knowledge for target downstream domains, which jointly result in that PLMs mis-locate the knowledge distributions corresponding to the target domains in the universal knowledge embedding space. To this end, we intuitively explore to approximate the unabridged target domains of downstream tasks in a debiased manner, and then abstract such domains to generate discriminative prompts, thereby providing the de-ambiguous
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2401.09002</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on Large Language Models. (arXiv:2401.09002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#65292;&#24341;&#20837;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#30340;&#35780;&#20272;&#35282;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19987;&#38376;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#20316;&#20026;&#22522;&#20934;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#22522;&#30784;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21019;&#24615;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#36234;&#29425;&#25915;&#20987;&#25928;&#26524;&#30340;&#26032;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#20581;&#22766;&#24615;&#35780;&#20272;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#26694;&#26550;&#65306;&#31895;&#31890;&#24230;&#35780;&#20272;&#21644;&#32454;&#31890;&#24230;&#35780;&#20272;&#12290;&#27599;&#20010;&#26694;&#26550;&#37117;&#20351;&#29992;&#20174;0&#21040;1&#30340;&#35780;&#20998;&#33539;&#22260;&#65292;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#65292;&#33021;&#22815;&#26356;&#20840;&#38754;&#21644;&#32454;&#33268;&#22320;&#35780;&#20272;&#25915;&#20987;&#25928;&#26524;&#65292;&#24182;&#24110;&#21161;&#25915;&#20987;&#32773;&#26356;&#22909;&#22320;&#20248;&#21270;&#25915;&#20987;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#36234;&#29425;&#20219;&#21153;&#30340;&#20840;&#38754;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#19981;&#20165;&#26159;&#25105;&#20204;&#24403;&#21069;&#30740;&#31350;&#30340;&#20851;&#38190;&#22522;&#20934;&#65292;&#20063;&#20026;&#26410;&#26469;&#30740;&#31350;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#30784;&#36164;&#28304;&#65292;&#21487;&#20197;&#22312;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#36827;&#34892;&#19968;&#33268;&#21644;&#27604;&#36739;&#30340;&#20998;&#26512;&#12290;&#36890;&#36807;&#19982;&#20256;&#32479;&#35780;&#20272;&#26041;&#27861;&#30340;&#31934;&#24515;&#27604;&#36739;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#35780;&#20272;&#26041;&#27861;&#19982;&#20043;&#30456;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our research, we pioneer a novel approach to evaluate the effectiveness of jailbreak attacks on Large Language Models (LLMs), such as GPT-4 and LLaMa2, diverging from traditional robustness-focused binary evaluations. Our study introduces two distinct evaluation frameworks: a coarse-grained evaluation and a fine-grained evaluation. Each framework, using a scoring range from 0 to 1, offers a unique perspective, enabling a more comprehensive and nuanced evaluation of attack effectiveness and empowering attackers to refine their attack prompts with greater understanding. Furthermore, we have developed a comprehensive ground truth dataset specifically tailored for jailbreak tasks. This dataset not only serves as a crucial benchmark for our current study but also establishes a foundational resource for future research, enabling consistent and comparative analyses in this evolving field. Upon meticulous comparison with traditional evaluation methods, we discovered that our evaluation alig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.03512</link><description>&lt;p&gt;
&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20197;&#26356;&#20934;&#30830;&#30340;&#26684;&#24335;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;
&lt;/p&gt;
&lt;p&gt;
Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate Format. (arXiv:2401.03512v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#65292;&#24182;&#35299;&#20915;&#20102;&#26684;&#24335;&#19981;&#20934;&#30830;&#24615;&#30340;&#38382;&#39064;&#12290;&#39564;&#35777;&#20102;&#29616;&#26377;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#22312;&#23383;&#31526;&#21644;&#20998;&#35789;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30340;&#30693;&#35782;&#26377;&#38480;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#23450;&#21046;&#27169;&#22411;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#21644;Qwen-chat&#65289;&#33021;&#22815;&#26681;&#25454;&#20154;&#31867;&#30340;&#25351;&#20196;&#29983;&#25104;&#20013;&#22269;&#21476;&#20856;&#35799;&#35789;&#12290;&#34429;&#28982;&#35821;&#35328;&#27169;&#22411;&#22312;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#36890;&#24120;&#22312;&#26684;&#24335;&#19978;&#23384;&#22312;&#38382;&#39064;&#65292;&#27599;&#34892;&#23383;&#31526;&#30340;&#25968;&#37327;&#26377;&#26102;&#36807;&#22810;&#25110;&#19981;&#36275;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22522;&#20110;&#20998;&#35789;&#30340;&#65292;&#25105;&#20204;&#35748;&#20026;&#26684;&#24335;&#19981;&#20934;&#30830;&#26159;&#30001;&#20110;"&#20998;&#35789;&#35268;&#21010;"&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#20934;&#30830;&#30693;&#36947;&#27599;&#20010;&#20998;&#35789;&#20013;&#21253;&#21547;&#22810;&#23569;&#20010;&#23383;&#31526;&#65292;&#24182;&#22522;&#20110;&#36825;&#20010;&#30693;&#35782;&#36827;&#34892;&#38271;&#24230;&#25511;&#21046;&#35268;&#21010;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#23637;&#31034;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#35789;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20998;&#35789;&#21644;&#23383;&#31526;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#30693;&#35782;&#26377;&#38480;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#25340;&#20889;&#27604;&#36187;&#25506;&#27979;&#31243;&#24207;&#65292;&#24182;&#21457;&#29616;Qwen-chat&#22312;&#36817;15%&#30340;&#20013;&#25991;&#25340;&#20889;&#27979;&#35797;&#20013;&#22833;&#36133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#22522;&#20110;&#20998;&#35789;&#30340;&#27169;&#22411;&#21487;&#20197;&#36731;&#26494;&#23450;&#21046;&#25104;&#26080;&#38656;&#20998;&#35789;&#30340;&#27169;&#22411;&#65288;&#23545;&#20110;&#20013;&#25991;&#26469;&#35828;&#65289;&#65292;&#20174;&#32780;&#33021;&#22815;&#24456;&#22823;&#31243;&#24230;&#19978;&#35299;&#20915;&#26684;&#24335;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuned large language models (such as ChatGPT and Qwen-chat) can generate Chinese classical poetry following human's instructions. LLMs perform well in content, but are usually lacking in format, with occasionally excess or insufficient number of characters in each line. Since most SOTA LLMs are token-based, we assume that the format inaccuracy is due to the difficulty of the "token planning" task, which means that the LLM need to know exactly how much characters are contained in each token and do length-control planning based on that knowledge. In this paper, we first confirm our assumption by showing that existing token-based large language models has limited knowledge on token-character relationship. We use a spelling bee probing procedure, and find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show that a token-based model can be easily tailored into a token-free model (in terms of Chinese), which can largely solve the format accuracy problem. Our tailoring 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.20204</link><description>&lt;p&gt;
&#21033;&#29992;&#36817;&#26080;&#38480;&#21382;&#21490;&#30340;&#36890;&#29992;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
General-Purpose Retrieval-Enhanced Medical Prediction Model Using Near-Infinite History. (arXiv:2310.20204v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20204
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;REMed&#30340;&#26816;&#32034;&#22686;&#24378;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#26080;&#38480;&#35780;&#20272;&#20020;&#24202;&#20107;&#20214;&#24182;&#33258;&#21160;&#36873;&#25321;&#30456;&#20851;&#20107;&#20214;&#36827;&#34892;&#39044;&#27979;&#65292;&#28040;&#38500;&#20102;&#20154;&#24037;&#29305;&#24449;&#36873;&#25321;&#21644;&#35266;&#23519;&#31383;&#21475;&#30340;&#38480;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#24320;&#21457;&#20020;&#24202;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#27515;&#20129;&#39044;&#27979;&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#19987;&#23478;&#24847;&#35265;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#35843;&#25972;&#35266;&#27979;&#31383;&#21475;&#22823;&#23567;&#12290;&#36825;&#32473;&#19987;&#23478;&#24102;&#26469;&#36127;&#25285;&#24182;&#22312;&#24320;&#21457;&#36807;&#31243;&#20013;&#36896;&#25104;&#29942;&#39048;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#21307;&#23398;&#39044;&#27979;&#27169;&#22411;&#65288;REMed&#65289;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;REMed&#21487;&#20197;&#22522;&#26412;&#35780;&#20272;&#26080;&#38480;&#37327;&#30340;&#20020;&#24202;&#20107;&#20214;&#65292;&#36873;&#25321;&#30456;&#20851;&#30340;&#20107;&#20214;&#24182;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#24182;&#23454;&#26102;&#35266;&#23519;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;27&#20010;&#20020;&#24202;&#20219;&#21153;&#21644;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;EHR&#25968;&#25454;&#38598;&#30340;&#29420;&#31435;&#38431;&#21015;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#29305;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;REMed&#20248;&#20110;&#20854;&#20182;&#29616;&#20195;&#26550;&#26500;&#65292;&#23427;&#20204;&#26088;&#22312;&#22788;&#29702;&#23613;&#21487;&#33021;&#22810;&#30340;&#20107;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;REMed&#30340;&#20559;&#22909;&#19982;&#21307;&#23398;&#19987;&#23478;&#30340;&#20559;&#22909;&#23494;&#20999;&#30456;&#20284;&#12290;&#25105;&#20204;&#26399;&#26395;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#26174;&#33879;&#21152;&#36895;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing clinical prediction models (e.g., mortality prediction) based on electronic health records (EHRs) typically relies on expert opinion for feature selection and adjusting observation window size. This burdens experts and creates a bottleneck in the development process. We propose Retrieval-Enhanced Medical prediction model (REMed) to address such challenges. REMed can essentially evaluate an unlimited number of clinical events, select the relevant ones, and make predictions. This approach effectively eliminates the need for manual feature selection and enables an unrestricted observation window. We verified these properties through experiments on 27 clinical tasks and two independent cohorts from publicly available EHR datasets, where REMed outperformed other contemporary architectures that aim to handle as many events as possible. Notably, we found that the preferences of REMed align closely with those of medical experts. We expect our approach to significantly expedite the d
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07923</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expresssive Power of Transformers with Chain of Thought. (arXiv:2310.07923v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;Transformer&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#20801;&#35768;&#20351;&#29992;&#20013;&#38388;&#29983;&#25104;&#30340;&#26041;&#24335;&#25552;&#39640;&#20102;Transformer&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#20123;&#20986;&#20154;&#24847;&#26009;&#22320;&#31616;&#21333;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#20363;&#22914;&#26816;&#26597;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#36830;&#25509;&#30340;&#20004;&#20010;&#33410;&#28857;&#65292;&#25110;&#27169;&#25311;&#26377;&#38480;&#29366;&#24577;&#26426;&#65292;&#36825;&#20123;&#38382;&#39064;&#34987;&#35777;&#26126;&#26080;&#27861;&#30001;&#31435;&#21363;&#35835;&#21462;&#36755;&#20837;&#21518;&#22238;&#31572;&#30340;&#26631;&#20934;Transformer&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20801;&#35768;Transformer&#20351;&#29992;&#8220;&#24605;&#32500;&#38142;&#8221;&#25110;&#8220;&#33609;&#31295;&#32440;&#8221;&#65292;&#21363;&#22312;&#22238;&#31572;&#20043;&#21069;&#29983;&#25104;&#24182;&#20381;&#36182;&#19968;&#31995;&#21015;&#20013;&#38388;token&#65292;&#21487;&#20197;&#25913;&#21892;&#20854;&#25512;&#29702;&#33021;&#21147;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#38382;&#65306;&#36825;&#31181;&#20013;&#38388;&#29983;&#25104;&#26159;&#21542;&#20174;&#26681;&#26412;&#19978;&#25193;&#23637;&#20102;&#20165;&#26377;&#35299;&#30721;&#22120;&#30340;Transformer&#30340;&#35745;&#31639;&#33021;&#21147;&#65311;&#25105;&#20204;&#34920;&#26126;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#65292;&#20294;&#22686;&#21152;&#30340;&#31243;&#24230;&#20851;&#38190;&#21462;&#20915;&#20110;&#20013;&#38388;&#29983;&#25104;&#30340;&#25968;&#37327;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#26469;&#35828;&#65292;&#20855;&#26377;&#23545;&#25968;&#32423;&#35299;&#30721;&#27493;&#39588;&#30340;Transformer&#35299;&#30721;&#22120;&#20165;&#30053;&#24494;&#25512;&#21160;&#20102;&#26631;&#20934;Transformer&#30340;&#26497;&#38480;&#65292;&#32780;&#32447;&#24615;&#25968;&#37327;&#30340;&#35299;&#30721;&#27493;&#39588;&#21017;&#22686;&#21152;&#20102;&#26126;&#26174;&#30340;&#26032;&#33021;&#21147;&#65288;&#22312;&#26631;&#20934;&#35745;&#31639;&#22797;&#26434;&#24230;&#19979;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical work has identified surprisingly simple reasoning problems, such as checking if two nodes in a graph are connected or simulating finite-state machines, that are provably unsolvable by standard transformers that answer immediately after reading their input. However, in practice, transformers' reasoning can be improved by allowing them to use a "chain of thought" or "scratchpad", i.e., generate and condition on a sequence of intermediate tokens before answering. Motivated by this, we ask: Does such intermediate generation fundamentally extend the computational power of a decoder-only transformer? We show that the answer is yes, but the amount of increase depends crucially on the amount of intermediate generation. For instance, we find that transformer decoders with a logarithmic number of decoding steps (w.r.t. the input length) push the limits of standard transformers only slightly, while a linear number of decoding steps adds a clear new ability (under standard compl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChEDDAR&#65292;&#19968;&#20010;&#22312;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#24212;&#29992;&#30340;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#23398;&#29983;&#23545;&#29983;&#25104;&#22411;AI&#30340;&#20351;&#29992;&#27169;&#24335;&#21644;&#24863;&#30693;&#65292;&#24182;&#20026;&#25945;&#32946;&#32972;&#26223;&#19979;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.13243</link><description>&lt;p&gt;
ChEDDAR: &#22312;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#30340;&#23398;&#29983;-ChatGPT&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
ChEDDAR: Student-ChatGPT Dialogue in EFL Writing Education. (arXiv:2309.13243v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13243
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;ChEDDAR&#65292;&#19968;&#20010;&#22312;EFL&#20889;&#20316;&#25945;&#32946;&#20013;&#24212;&#29992;&#30340;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#35813;&#30740;&#31350;&#20998;&#26512;&#20102;&#23398;&#29983;&#23545;&#29983;&#25104;&#22411;AI&#30340;&#20351;&#29992;&#27169;&#24335;&#21644;&#24863;&#30693;&#65292;&#24182;&#20026;&#25945;&#32946;&#32972;&#26223;&#19979;&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23558;&#29983;&#25104;&#22411;AI&#24212;&#29992;&#20110;&#25945;&#32946;&#39046;&#22495;&#24050;&#26377;&#19981;&#23569;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#23398;&#29983;&#21644;AI&#31995;&#32479;&#20043;&#38388;&#22823;&#35268;&#27169;&#19988;&#30495;&#23454;&#30340;&#20114;&#21160;&#30340;&#23454;&#35777;&#20998;&#26512;&#20173;&#28982;&#24456;&#26377;&#38480;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ChEDDAR&#65292;&#21363;ChatGPT&#21644;EFL&#23398;&#20064;&#32773;&#30340;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#22312;&#19968;&#20010;&#23398;&#26399;&#38271;&#30340;&#32437;&#21521;&#23454;&#39564;&#20013;&#25910;&#38598;&#30340;&#65292;&#30740;&#31350;&#23545;&#35937;&#21253;&#25324;212&#21517;&#21442;&#21152;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#65288;EFL&#65289;&#20889;&#20316;&#35838;&#31243;&#30340;&#22823;&#23398;&#29983;&#12290;&#23398;&#29983;&#34987;&#35201;&#27714;&#36890;&#36807;&#19982;ChatGPT&#30340;&#23545;&#35805;&#26469;&#20462;&#25913;&#20182;&#20204;&#30340;&#25991;&#31456;&#12290;ChEDDAR&#21253;&#25324;&#23545;&#35805;&#26085;&#24535;&#65292;&#35805;&#35821;&#32423;&#21035;&#30340;&#25991;&#31456;&#32534;&#36753;&#21382;&#21490;&#65292;&#33258;&#25105;&#35780;&#20215;&#28385;&#24847;&#24230;&#21644;&#23398;&#29983;&#24847;&#22270;&#65292;&#20197;&#21450;&#35760;&#24405;&#20182;&#20204;&#30446;&#26631;&#21644;&#25972;&#20307;&#20307;&#39564;&#30340;&#20250;&#35805;&#32423;&#21035;&#30340;&#21069;&#21518;&#35843;&#26597;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#23398;&#29983;&#23545;&#29983;&#25104;&#22411;AI&#30340;&#20351;&#29992;&#27169;&#24335;&#21644;&#24863;&#30693;&#65292;&#20197;&#21450;&#20182;&#20204;&#30340;&#24847;&#22270;&#21644;&#28385;&#24847;&#24230;&#12290;&#20316;&#20026;&#22522;&#30784;&#24615;&#27493;&#39588;&#65292;&#25105;&#20204;&#20026;&#25945;&#32946;&#32972;&#26223;&#19979;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#30340;&#20004;&#20010;&#20851;&#38190;&#20219;&#21153;&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65306;&#22312;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
The integration of generative AI in education is expanding, yet empirical analyses of large-scale, real-world interactions between students and AI systems still remain limited. In this study, we present ChEDDAR, ChatGPT &amp; EFL Learner's Dialogue Dataset As Revising an essay, which is collected from a semester-long longitudinal experiment involving 212 college students enrolled in English as Foreign Langauge (EFL) writing courses. The students were asked to revise their essays through dialogues with ChatGPT. ChEDDAR includes a conversation log, utterance-level essay edit history, self-rated satisfaction, and students' intent, in addition to session-level pre-and-post surveys documenting their objectives and overall experiences. We analyze students' usage patterns and perceptions regarding generative AI with respect to their intent and satisfaction. As a foundational step, we establish baseline results for two pivotal tasks in task-oriented dialogue systems within educational contexts: in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CTC-based&#35821;&#38899;&#35782;&#21035;&#20013;&#29992;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#32553;&#30701;&#24207;&#21015;&#38271;&#24230;&#30340;&#21333;&#27169;&#32858;&#21512;&#26041;&#27861;(UMA)&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#38598;&#25104;&#21516;&#19968;&#25991;&#26412;&#26631;&#35760;&#30340;&#29305;&#24449;&#24103;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UMA&#22312;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#38598;&#25104;&#33258;&#26465;&#20214;CTC&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08150</link><description>&lt;p&gt;
CTC-based&#35821;&#38899;&#35782;&#21035;&#30340;&#21333;&#27169;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unimodal Aggregation for CTC-based Speech Recognition. (arXiv:2309.08150v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;CTC-based&#35821;&#38899;&#35782;&#21035;&#20013;&#29992;&#20110;&#23398;&#20064;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#32553;&#30701;&#24207;&#21015;&#38271;&#24230;&#30340;&#21333;&#27169;&#32858;&#21512;&#26041;&#27861;(UMA)&#65292;&#36890;&#36807;&#20998;&#21106;&#21644;&#38598;&#25104;&#21516;&#19968;&#25991;&#26412;&#26631;&#35760;&#30340;&#29305;&#24449;&#24103;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#35782;&#21035;&#38169;&#35823;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;UMA&#22312;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#36890;&#36807;&#38598;&#25104;&#33258;&#26465;&#20214;CTC&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38750;&#33258;&#22238;&#24402;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36827;&#34892;&#30740;&#31350;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27169;&#32858;&#21512;&#65288;UMA&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#23646;&#20110;&#21516;&#19968;&#25991;&#26412;&#26631;&#35760;&#30340;&#29305;&#24449;&#24103;&#36827;&#34892;&#20998;&#21106;&#21644;&#38598;&#25104;&#65292;&#20174;&#32780;&#23398;&#20064;&#26356;&#22909;&#30340;&#25991;&#26412;&#26631;&#35760;&#29305;&#24449;&#34920;&#31034;&#12290;&#29305;&#24449;&#24103;&#21644;&#26435;&#37325;&#37117;&#26469;&#33258;&#20110;&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#21333;&#27169;&#26435;&#37325;&#38598;&#25104;&#29305;&#24449;&#24103;&#65292;&#24182;&#32463;&#36807;&#35299;&#30721;&#22120;&#36827;&#19968;&#27493;&#22788;&#29702;&#12290;&#35757;&#32451;&#26102;&#37319;&#29992;&#20102;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;&#25439;&#22833;&#12290;&#19982;&#24120;&#35268;CTC&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#26356;&#22909;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#24182;&#32553;&#30701;&#20102;&#24207;&#21015;&#38271;&#24230;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#35782;&#21035;&#38169;&#35823;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#19977;&#20010;&#26222;&#36890;&#35805;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UMA&#30456;&#27604;&#20854;&#20182;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#65288;&#22914;&#33258;&#26465;&#20214;CTC&#65289;&#34920;&#29616;&#20986;&#26356;&#22909;&#25110;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#33258;&#26465;&#20214;CTC&#38598;&#25104;&#21040;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#20013;&#65292;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper works on non-autoregressive automatic speech recognition. A unimodal aggregation (UMA) is proposed to segment and integrate the feature frames that belong to the same text token, and thus to learn better feature representations for text tokens. The frame-wise features and weights are both derived from an encoder. Then, the feature frames with unimodal weights are integrated and further processed by a decoder. Connectionist temporal classification (CTC) loss is applied for training. Compared to the regular CTC, the proposed method learns better feature representations and shortens the sequence length, resulting in lower recognition error and computational complexity. Experiments on three Mandarin datasets show that UMA demonstrates superior or comparable performance to other advanced non-autoregressive methods, such as self-conditioned CTC. Moreover, by integrating self-conditioned CTC into the proposed framework, the performance can be further noticeably improved.
&lt;/p&gt;</description></item><item><title>MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2309.07915</link><description>&lt;p&gt;
MMICL&#65306;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning. (arXiv:2309.07915v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07915
&lt;/p&gt;
&lt;p&gt;
MMICL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#35774;&#35745;&#65292;&#20197;&#35299;&#20915;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#22810;&#27169;&#24577;&#25552;&#31034;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#33487;&#24320;&#22987;&#65292;&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;LLM&#21487;&#20197;&#21033;&#29992;&#20016;&#23500;&#30340;&#32972;&#26223;&#30693;&#35782;&#21644;&#20219;&#21153;&#20449;&#24687;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22823;&#22810;&#25968;VLM&#22312;&#29702;&#35299;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;&#21253;&#21547;&#22810;&#20010;&#22270;&#20687;&#65289;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#22256;&#38590;&#12290;&#36825;&#20010;&#38382;&#39064;&#21487;&#20197;&#36861;&#28335;&#21040;VLM&#30340;&#26550;&#26500;&#35774;&#35745;&#25110;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#24403;&#21069;&#30340;VLM&#20027;&#35201;&#24378;&#35843;&#21033;&#29992;&#24102;&#26377;&#21333;&#20010;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#32780;&#19981;&#26159;&#24102;&#26377;&#20132;&#38169;&#22810;&#20010;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#23613;&#31649;&#19968;&#20123;&#26032;&#25552;&#20986;&#30340;VLM&#21487;&#20197;&#22788;&#29702;&#24102;&#26377;&#22810;&#20010;&#22270;&#20687;&#30340;&#29992;&#25143;&#25552;&#31034;&#65292;&#20294;&#39044;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#25552;&#20379;&#27604;&#20174;Web&#25235;&#21462;&#26102;&#20132;&#38169;&#22270;&#20687;&#21644;&#25991;&#26412;&#26356;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;MMICL&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Starting from the resurgence of deep learning, vision-language models (VLMs) benefiting from large language models (LLMs) have never been so popular. However, while LLMs can utilize extensive background knowledge and task information with in-context learning, most VLMs still struggle with understanding complex multi-modal prompts with multiple images. The issue can traced back to the architectural design of VLMs or pre-training data. Specifically, the current VLMs primarily emphasize utilizing multi-modal data with a single image some, rather than multi-modal prompts with interleaved multiple images and text. Even though some newly proposed VLMs could handle user prompts with multiple images, pre-training data does not provide more sophisticated multi-modal prompts than interleaved image and text crawled from the web. We propose MMICL to address the issue by considering both the model and data perspectives. We introduce a well-designed architecture capable of seamlessly integrating vis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20013;&#25991;&#26041;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36873;&#25321;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.07081</link><description>&lt;p&gt;
Whisper&#33021;&#22815;&#36827;&#34892;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Whisper perform speech-based in-context learning. (arXiv:2309.07081v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#36866;&#24212;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20013;&#25991;&#26041;&#35328;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#21333;&#35789;&#38169;&#35823;&#29575;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#20248;&#21270;&#36873;&#25321;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;OpenAI&#21457;&#24067;&#30340;Whisper&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#30340;&#35821;&#22659;&#23398;&#20064;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#35821;&#22659;&#30340;&#35821;&#38899;&#23398;&#20064;&#65288;SICL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26102;&#36866;&#24212;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26799;&#24230;&#19979;&#38477;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#21333;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#26631;&#35760;&#30340;&#35821;&#38899;&#26679;&#26412;&#12290;&#20351;&#29992;&#20013;&#25991;&#26041;&#35328;&#36827;&#34892;&#35821;&#35328;&#32423;&#21035;&#30340;&#36866;&#24212;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#23558;SICL&#24212;&#29992;&#20110;&#23396;&#31435;&#35789;ASR&#26102;&#65292;&#21487;&#20197;&#22312;&#20004;&#20010;&#26041;&#35328;&#19978;&#20351;&#29992;&#20219;&#24847;&#22823;&#23567;&#30340;Whisper&#27169;&#22411;&#23454;&#29616;&#19968;&#33268;&#19988;&#26174;&#33879;&#30340;WER&#30456;&#23545;&#38477;&#20302;&#65292;&#24179;&#22343;&#20026;32.3%&#12290;&#22522;&#20110;k&#26368;&#36817;&#37051;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#36873;&#25321;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;SICL&#30340;&#25928;&#29575;&#65292;&#24179;&#22343;&#30456;&#23545;WER&#38477;&#20302;&#29575;&#20026;36.4%&#12290;&#36890;&#36807;&#35828;&#35805;&#20154;&#36866;&#24212;&#25110;&#36830;&#32493;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#26469;&#39564;&#35777;&#20102;&#36825;&#20123;&#21457;&#29616;&#65292;&#24182;&#19988;&#20004;&#32773;&#37117;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#23545;WER&#38477;&#20302;&#12290;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the in-context learning abilities of the Whisper automatic speech recognition (ASR) models released by OpenAI. A novel speech-based in-context learning (SICL) approach is proposed for test-time adaptation, which can reduce the word error rates (WERs) with only a small number of labelled speech samples without gradient descent. Language-level adaptation experiments using Chinese dialects showed that when applying SICL to isolated word ASR, consistent and considerable relative WER reductions can be achieved using Whisper models of any size on two dialects, which is on average 32.3%. A k-nearest-neighbours-based in-context example selection technique can be applied to further improve the efficiency of SICL, which can increase the average relative WER reduction to 36.4%. The findings are verified using speaker adaptation or continuous speech recognition tasks, and both achieved considerable relative WER reductions. Detailed quantitative analyses are also provided to
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.04461</link><description>&lt;p&gt;
&#27979;&#37327;&#21644;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models. (arXiv:2309.04461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20154;&#31867;&#25512;&#29702;&#33021;&#21147;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#12290;&#36890;&#36807;&#19968;&#20010;&#27969;&#27700;&#32447;&#21644;&#24050;&#26377;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#27979;&#37327;&#36825;&#31181;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#33021;&#35299;&#26512;&#20851;&#20110;&#35270;&#35273;&#20869;&#23481;&#30340;&#33258;&#28982;&#26597;&#35810;&#24182;&#29983;&#25104;&#31867;&#20154;&#36755;&#20986;&#30340;&#35270;&#35273;&#21161;&#25163;&#65292;&#23637;&#31034;&#20986;&#20102;&#24378;&#22823;&#30340;&#21151;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#23637;&#31034;&#22522;&#20110;&#25152;&#24863;&#30693;&#20449;&#24687;&#30340;&#31867;&#20154;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#20851;&#20110;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21040;&#24213;&#26377;&#22810;&#19968;&#33268;&#21644;&#26377;&#22810;&#22522;&#20110;&#23454;&#38469;&#30340;&#19968;&#20010;&#37325;&#35201;&#30097;&#34385;&#65292;&#25105;&#20204;&#36824;&#27979;&#37327;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#30340;&#19968;&#33268;&#24615;&#24230;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#35780;&#20272;&#38656;&#35201;&#28085;&#30422;&#39640;&#23618;&#27425;&#25512;&#29702;&#21644;&#32454;&#33410;&#25512;&#29702;&#38142;&#30340;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#39033;&#26114;&#36149;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;LLM-Human-in-the-Loop&#27969;&#27700;&#32447;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#27969;&#27700;&#32447;&#26174;&#33879;&#38477;&#20302;&#20102;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#27969;&#27700;&#32447;&#21644;&#29616;&#26377;&#30340;&#31895;&#31890;&#24230;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;CURE&#22522;&#20934;&#26469;&#21516;&#26102;&#27979;&#37327;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-language models (VLMs) have recently demonstrated strong efficacy as visual assistants that can parse natural queries about the visual content and generate human-like outputs. In this work, we explore the ability of these models to demonstrate human-like reasoning based on the perceived information. To address a crucial concern regarding the extent to which their reasoning capabilities are fully consistent and grounded, we also measure the reasoning consistency of these models. We achieve this by proposing a chain-of-thought (CoT) based consistency measure. However, such an evaluation requires a benchmark that encompasses both high-level inference and detailed reasoning chains, which is costly. We tackle this challenge by proposing a LLM-Human-in-the-Loop pipeline, which notably reduces cost while simultaneously ensuring the generation of a high-quality dataset. Based on this pipeline and the existing coarse-grained annotated dataset, we build the CURE benchmark to measure both 
&lt;/p&gt;</description></item><item><title>HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02706</link><description>&lt;p&gt;
HAE-RAE Bench: &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench: Evaluation of Korean Knowledge in Language Models. (arXiv:2309.02706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02706
&lt;/p&gt;
&lt;p&gt;
HAE-RAE Bench&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#23545;&#38889;&#22269;&#30693;&#35782;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20351;&#29992;&#27604;GPT-3.5&#23567;&#30340;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#24378;&#35843;&#20102;&#21516;&#36136;&#35821;&#26009;&#24211;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#27880;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20013;&#26377;&#38480;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#24182;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#38889;&#35821;&#35821;&#35328;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#29087;&#32451;&#31243;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HAE-RAE Bench&#65292;&#22312;&#35789;&#27719;&#12289;&#21382;&#21490;&#21644;&#19968;&#33324;&#30693;&#35782;&#31561;6&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;&#20351;&#29992;&#22823;&#22411;&#29305;&#23450;&#35821;&#35328;&#27169;&#22411;(LLSMs)&#19982;&#20687;GPT-3.5&#36825;&#26679;&#30340;&#20840;&#38754;&#36890;&#29992;&#27169;&#22411;&#30456;&#27604;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#27604;GPT-3.5&#32422;&#23567;13&#20493;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#26816;&#32034;&#26041;&#38754;&#23637;&#29616;&#20986;&#31867;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#36825;&#19968;&#35266;&#23519;&#24378;&#35843;&#20102;&#22312;&#35757;&#32451;&#19987;&#19994;&#32423;&#35821;&#35328;&#29305;&#23450;&#27169;&#22411;&#26102;&#21516;&#36136;&#35821;&#26009;&#24211;&#30340;&#37325;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#24403;&#36825;&#20123;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;......
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) pretrained on massive corpora exhibit remarkable capabilities across a wide range of tasks, however, the attention given to non-English languages has been limited in this field of research. To address this gap and assess the proficiency of language models in the Korean language and culture, we present HAE-RAE Bench, covering 6 tasks including vocabulary, history, and general knowledge. Our evaluation of language models on this benchmark highlights the potential advantages of employing Large Language-Specific Models(LLSMs) over a comprehensive, universal model like GPT-3.5. Remarkably, our study reveals that models approximately 13 times smaller than GPT-3.5 can exhibit similar performance levels in terms of language-specific knowledge retrieval. This observation underscores the importance of homogeneous corpora for training professional-level language-specific models. On the contrary, we also observe a perplexing performance dip in these smaller LMs when th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#24341;&#23548;&#30340;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#26469;&#22686;&#24378;&#30701;&#35821;&#34920;&#31034;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#35789;&#20449;&#24687;&#65292;&#36890;&#36807;&#20248;&#21270;&#25490;&#21517;&#32593;&#32476;&#21644;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26469;&#25552;&#39640;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.08739</link><description>&lt;p&gt;
&#20351;&#29992;&#20449;&#24687;&#29942;&#39048;&#24341;&#23548;&#30340;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#22686;&#24378;&#30701;&#35821;&#34920;&#31034;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Phrase Representation by Information Bottleneck Guided Text Diffusion Process for Keyphrase Extraction. (arXiv:2308.08739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08739
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#29942;&#39048;&#24341;&#23548;&#30340;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#26469;&#22686;&#24378;&#30701;&#35821;&#34920;&#31034;&#30340;&#20851;&#38190;&#35789;&#25552;&#21462;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#35789;&#20449;&#24687;&#65292;&#36890;&#36807;&#20248;&#21270;&#25490;&#21517;&#32593;&#32476;&#21644;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;&#26469;&#25552;&#39640;&#20851;&#38190;&#35789;&#25552;&#21462;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;&#35789;&#25552;&#21462;(KPE)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#29992;&#20110;&#20174;&#32473;&#23450;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#38190;&#35789;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#30417;&#30563;&#26041;&#27861;&#23558;KPE&#35270;&#20026;&#24207;&#21015;&#26631;&#27880;&#12289;&#36328;&#24230;&#32423;&#20998;&#31867;&#25110;&#29983;&#25104;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#21033;&#29992;&#20851;&#38190;&#35789;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#21487;&#33021;&#23548;&#33268;&#32467;&#26524;&#26377;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Diff-KPE&#65292;&#23427;&#21033;&#29992;&#30417;&#30563;&#30340;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;(VIB)&#26469;&#24341;&#23548;&#25991;&#26412;&#25193;&#25955;&#36807;&#31243;&#65292;&#29983;&#25104;&#22686;&#24378;&#30340;&#20851;&#38190;&#35789;&#34920;&#31034;&#12290;Diff-KPE&#39318;&#20808;&#26681;&#25454;&#25972;&#20010;&#25991;&#26723;&#29983;&#25104;&#25152;&#38656;&#30340;&#20851;&#38190;&#35789;&#23884;&#20837;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#23884;&#20837;&#27880;&#20837;&#21040;&#27599;&#20010;&#30701;&#35821;&#34920;&#31034;&#20013;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#25490;&#21517;&#32593;&#32476;&#21644;VIB&#21516;&#26102;&#36827;&#34892;&#25490;&#21517;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#30340;&#20248;&#21270;&#12290;Diff-KPE&#30340;&#35774;&#35745;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#20851;&#38190;&#35789;&#21644;&#25991;&#26723;&#30340;&#20449;&#24687;&#23545;&#27599;&#20010;&#20505;&#36873;&#30701;&#35821;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Keyphrase extraction (KPE) is an important task in Natural Language Processing for many scenarios, which aims to extract keyphrases that are present in a given document. Many existing supervised methods treat KPE as sequential labeling, span-level classification, or generative tasks. However, these methods lack the ability to utilize keyphrase information, which may result in biased results. In this study, we propose Diff-KPE, which leverages the supervised Variational Information Bottleneck (VIB) to guide the text diffusion process for generating enhanced keyphrase representations. Diff-KPE first generates the desired keyphrase embeddings conditioned on the entire document and then injects the generated keyphrase embeddings into each phrase representation. A ranking network and VIB are then optimized together with rank loss and classification loss, respectively. This design of Diff-KPE allows us to rank each candidate phrase by utilizing both the information of keyphrases and the docu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2308.00221</link><description>&lt;p&gt;
&#36229;&#36234;&#35782;&#21035;&#65306;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Advancing Beyond Identification: Multi-bit Watermark for Language Models. (arXiv:2308.00221v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#20301;&#27700;&#21360;&#25216;&#26415;&#8212;&#8212;COLOR&#65292;&#21487;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#31561;&#21151;&#33021;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#26174;&#31034;&#25104;&#21151;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#20013;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#30740;&#31350;&#26377;&#25928;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#30340;&#21453;&#21046;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#31215;&#26497;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#28389;&#29992;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#20391;&#37325;&#20110;&#26816;&#27979;&#65292;&#20294;&#26576;&#20123;&#24694;&#24847;&#28389;&#29992;&#38656;&#35201;&#36319;&#36394;&#23545;&#25163;&#29992;&#25143;&#20197;&#36827;&#34892;&#21453;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#22810;&#20301;&#27700;&#21360;&#36890;&#36807;&#39068;&#33394;&#32534;&#30721;&#8221;&#65288;COLOR&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36807;&#31243;&#20013;&#23884;&#20837;&#21487;&#36861;&#36394;&#30340;&#22810;&#20301;&#20449;&#24687;&#12290;&#21033;&#29992;&#38646;&#20301;&#27700;&#21360;&#25216;&#26415;&#30340;&#20248;&#21183;&#65288;Kirchenbauer&#31561;&#65292;2023a&#65289;&#65292;COLOR&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#27700;&#21360;&#12289;&#21363;&#26102;&#23884;&#20837;&#21644;&#32500;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20801;&#35768;&#38646;&#20301;&#26816;&#27979;&#12290;&#21021;&#27493;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20013;&#31561;&#38271;&#24230;&#30340;&#25991;&#26412;&#65288;&#32422;500&#20010;&#26631;&#35760;&#65289;&#20013;&#25104;&#21151;&#23884;&#20837;&#20102;32&#20301;&#28040;&#24687;&#65292;&#20934;&#30830;&#29575;&#20026;91.9&#65285;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#25928;&#22320;&#25512;&#36827;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#28389;&#29992;&#36827;&#34892;&#21453;&#21046;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study aims to proactively tackle misuse of large language models beyond identification of machine-generated text. While existing methods focus on detection, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose "Multi-bit Watermark through Color-listing" (COLOR), embedding traceable multi-bit information during language model generation. Leveraging the benefits of zero-bit watermarking (Kirchenbauer et al., 2023a), COLOR enables extraction without model access, on-the-fly embedding, and maintains text quality, while allowing zero-bit detection all at the same time. Preliminary experiments demonstrates successful embedding of 32-bit messages with 91.9% accuracy in moderate-length texts ($\sim$500 tokens). This work advances strategies to counter language model misuse effectively.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;LSR-Benchmark&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35828;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05113</link><description>&lt;p&gt;
&#36229;&#36234;&#26174;&#32780;&#26131;&#35265;&#65306;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#22522;&#20110;&#29983;&#27963;&#26223;&#35266;&#25512;&#29702;&#22522;&#20934;(LSR-Benchmark)&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Beyond the Obvious: Evaluating the Reasoning Ability In Real-life Scenarios of Language Models on Life Scapes Reasoning Benchmark~(LSR-Benchmark). (arXiv:2307.05113v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;LSR-Benchmark&#65292;&#26088;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#30495;&#23454;&#24773;&#22659;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#22312;&#36825;&#26041;&#38754;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35828;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#27963;&#26223;&#35266;&#25512;&#29702;&#22522;&#20934; (LSR-Benchmark)&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#30495;&#23454;&#24773;&#22659;&#25512;&#29702;&#30340;&#26032;&#22411;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24357;&#34917;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#22312;&#26085;&#24120;&#32972;&#26223;&#19979;&#25512;&#29702;&#33021;&#21147;&#30340;&#24046;&#36317;&#12290;&#19982;&#39046;&#22495;&#30693;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;LSR-Benchmark&#21253;&#21547;&#33258;&#30001;&#25991;&#26412;&#26684;&#24335;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#26377;&#20851;&#30495;&#23454;&#29983;&#27963;&#24773;&#26223;&#12289;&#20154;&#31867;&#34892;&#20026;&#21644;&#35282;&#33394;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#35813;&#25968;&#25454;&#38598;&#30001;&#26469;&#33258;&#24320;&#28304;&#22312;&#32447;&#26469;&#28304;&#30340;2162&#20010;&#38382;&#39064;&#32452;&#25104;&#65292;&#24182;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#20197;&#25552;&#39640;&#36136;&#37327;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;gpt3.5-turbo&#21644;instruction fine-tuned llama&#27169;&#22411;&#65292;&#27979;&#35797;&#20854;&#22312;LSR-Benchmark&#19978;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#26126;&#26174;&#20248;&#20110;&#36825;&#20123;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29702;&#35299;&#26085;&#24120;&#29983;&#27963;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Life Scapes Reasoning Benchmark (LSR-Benchmark), a novel dataset targeting real-life scenario reasoning, aiming to close the gap in artificial neural networks' ability to reason in everyday contexts. In contrast to domain knowledge reasoning datasets, LSR-Benchmark comprises free-text formatted questions with rich information on real-life scenarios, human behaviors, and character roles. The dataset consists of 2,162 questions collected from open-source online sources and is manually annotated to improve its quality. Experiments are conducted using state-of-the-art language models, such as gpt3.5-turbo and instruction fine-tuned llama models, to test the performance in LSR-Benchmark. The results reveal that humans outperform these models significantly, indicating a persisting challenge for machine learning models in comprehending daily human life.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;ProTEC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#20174;&#26131;&#21040;&#38590;&#22320;&#23398;&#20064;&#38169;&#35823;&#26816;&#27979;&#12289;&#38169;&#35823;&#31867;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#32467;&#26524;&#29983;&#25104;&#65292;&#20197;&#35299;&#20915;&#36807;&#32416;&#27491;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.17447</link><description>&lt;p&gt;
&#38754;&#21521;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Progressive Multi-task Learning Framework for Chinese Text Error Correction. (arXiv:2306.17447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17447
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;ProTEC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#20174;&#26131;&#21040;&#38590;&#22320;&#23398;&#20064;&#38169;&#35823;&#26816;&#27979;&#12289;&#38169;&#35823;&#31867;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#32467;&#26524;&#29983;&#25104;&#65292;&#20197;&#35299;&#20915;&#36807;&#32416;&#27491;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#26088;&#22312;&#26816;&#27979;&#21644;&#32416;&#27491;&#36755;&#20837;&#25991;&#26412;&#20013;&#30340;&#38169;&#35823;&#65292;&#36825;&#26377;&#30410;&#20110;&#20154;&#31867;&#26085;&#24120;&#29983;&#27963;&#21644;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36817;&#26399;&#30340;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#26469;&#35299;&#20915;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#23384;&#22312;&#36807;&#32416;&#27491;&#21644;&#27424;&#32416;&#27491;&#30340;&#38382;&#39064;&#65292;&#21069;&#32773;&#22312;&#23545;&#31934;&#30830;&#24615;&#35201;&#27714;&#36739;&#39640;&#30340;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#20219;&#21153;&#20013;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#20102;&#32531;&#35299;&#36807;&#32416;&#27491;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#28176;&#36827;&#24335;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;ProTEC&#65292;&#23427;&#24341;&#23548;&#19968;&#20010;CTEC&#27169;&#22411;&#20174;&#31616;&#21333;&#21040;&#22256;&#38590;&#22320;&#23398;&#20064;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;CTEC&#20219;&#21153;&#20998;&#20026;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#20174;&#26131;&#21040;&#38590;&#20998;&#21035;&#20026;&#38169;&#35823;&#26816;&#27979;&#12289;&#38169;&#35823;&#31867;&#22411;&#35782;&#21035;&#21644;&#26657;&#27491;&#32467;&#26524;&#29983;&#25104;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;ProTEC&#23558;&#36825;&#20123;&#23376;&#20219;&#21153;&#32435;&#20837;&#22810;&#20219;&#21153;&#35757;&#32451;&#30446;&#26631;&#65292;&#24341;&#23548;&#27169;&#22411;&#36880;&#28176;&#23398;&#20064;&#25991;&#26412;&#38169;&#35823;&#26657;&#27491;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#21017;...
&lt;/p&gt;
&lt;p&gt;
Chinese Text Error Correction (CTEC) aims to detect and correct errors in the input text, which benefits human's daily life and various downstream tasks. Recent approaches mainly employ Pre-trained Language Models (PLMs) to resolve CTEC task and achieve tremendous success. However, previous approaches suffer from issues of over-correction and under-correction, and the former is especially conspicuous in the precision-critical CTEC task. To mitigate the issue of overcorrection, we propose a novel model-agnostic progressive multitask learning framework for CTEC, named ProTEC, which guides a CTEC model to learn the task from easy to difficult. We divide CTEC task into three sub-tasks from easy to difficult: Error Detection, Error Type Identification, and Correction Result Generation. During the training process, ProTEC guides the model to learn text error correction progressively by incorporating these sub-tasks into a multi-task training objective. During the inference process, the model
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.14565</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#25351;&#20196;&#35843;&#25972;&#26469;&#20943;&#36731;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. (arXiv:2306.14565v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#24187;&#35273;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#35774;&#35745;&#21253;&#21547;&#27491;&#36127;&#25351;&#20196;&#30340;&#25968;&#25454;&#38598;&#21644;&#25552;&#20986;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#34913;&#37327;&#27169;&#22411;&#20135;&#29983;&#30340;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22810;&#27169;&#24577;&#20219;&#21153;&#21462;&#24471;&#20102;&#21487;&#21916;&#30340;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#24456;&#23481;&#26131;&#22312;&#25551;&#36848;&#22270;&#20687;&#21644;&#20154;&#31867;&#25351;&#20196;&#26102;&#20135;&#29983;&#19981;&#19968;&#33268;&#30340;&#24187;&#35273;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#31532;&#19968;&#20010;&#22823;&#22411;&#22810;&#26679;&#21270;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;LRV-Instruction&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#30001;GPT4&#29983;&#25104;&#30340;12&#19975;&#20010;&#35270;&#35273;&#25351;&#20196;&#65292;&#28085;&#30422;&#20102;16&#20010;&#24320;&#25918;&#24335;&#25351;&#20196;&#21644;&#31572;&#26696;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20219;&#21153;&#12290;&#19982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#27491;&#25351;&#20196;&#26679;&#26412;&#19981;&#21516;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;LRV-Instruction&#20197;&#21253;&#21547;&#26356;&#22810;&#38024;&#23545;&#26356;&#24378;&#30340;&#35270;&#35273;&#25351;&#20196;&#35843;&#25972;&#30340;&#27491;&#36127;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#36127;&#25351;&#20196;&#22312;&#20004;&#20010;&#35821;&#20041;&#23618;&#27425;&#19978;&#35774;&#35745;&#65306;&#65288;i&#65289;&#19981;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#21644;&#65288;ii&#65289;&#23384;&#22312;&#20803;&#32032;&#25805;&#20316;&#12290;&#20026;&#20102;&#26356;&#26377;&#25928;&#22320;&#34913;&#37327;LMM&#25152;&#20135;&#29983;&#30340;&#24187;&#35273;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;GPT4&#36741;&#21161;&#30340;&#35270;&#35273;&#25351;&#20196;&#35780;&#20272;&#65288;GAVIE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the promising progress in multi-modal tasks, current large multi-modal models (LMM) are prone to hallucinating inconsistent descriptions with respect to the associated image and human instructions. This paper addresses this issue by introducing the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction. Our dataset consists of 120k visual instructions generated by GPT4, covering 16 vision-and-language tasks with open-ended instructions and answers. Unlike existing studies that primarily focus on positive instruction samples, we design LRV-Instruction to include both positive and negative instructions for more robust visual instruction tuning. Our negative instructions are designed at two semantic levels: (i) Nonexistent Element Manipulation and (ii) Existent Element Manipulation. To efficiently measure the hallucination generated by LMMs, we propose GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.12725</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Generative Multimodal Entity Linking. (arXiv:2306.12725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GEMEL &#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#65292;&#20165;&#35843;&#25972;&#20102;&#26497;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340; MEL &#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26159;&#23558;&#24102;&#26377;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#25552;&#21450;&#26144;&#23556;&#21040;&#30693;&#35782;&#24211;&#65288;&#20363;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#30340;&#24341;&#29992;&#23454;&#20307;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; GEMEL &#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29983;&#25104;&#24335;&#22810;&#27169;&#24577;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340; LLMs &#30452;&#25509;&#29983;&#25104;&#30446;&#26631;&#23454;&#20307;&#21517;&#31216;&#12290;&#25105;&#20204;&#20445;&#25345;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#20923;&#32467;&#65292;&#21482;&#35757;&#32451;&#19968;&#20010;&#32447;&#24615;&#23618;&#20197;&#21551;&#29992;&#36328;&#27169;&#24577;&#20132;&#20114;&#12290;&#20026;&#20102;&#23558; LLMs &#36866;&#24212; MEL &#20219;&#21153;&#65292;&#25105;&#20204;&#21033;&#29992; LLMs &#30340;&#26032;&#20852;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65292;&#36890;&#36807;&#26816;&#32034;&#22810;&#27169;&#24577;&#23454;&#20363;&#20316;&#20026;&#31034;&#33539;&#26469;&#36827;&#34892;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#35843;&#25972;&#20102;&#22823;&#32422;0.3&#65285;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;GEMEL &#23601;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Entity Linking (MEL) is the task of mapping mentions with multimodal contexts to the referent entities from a knowledge base (e.g., Wikipedia). Prior MEL methods mainly focus on designing complex multimodal interaction mechanisms and require fine-tuning all model parameters, which can be prohibitively costly and difficult to scale in the era of Large Language Models (LLMs). In this work, we propose GEMEL, a simple yet effective Generative Multimodal Entity Linking method, which leverages the capabilities of LLMs from large-scale pre-training to directly generate target entity names. We keep the vision and language model frozen and only train a linear layer to enable cross-modality interactions. To adapt LLMs to the MEL task, we take advantage of the emerging in-context learning (ICL) capability of LLMs by retrieving multimodal instances as demonstrations. Extensive experiments show that with only ~0.3% of the model parameters fine-tuned, GEMEL achieves state-of-the-art resul
&lt;/p&gt;</description></item><item><title>BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;</title><link>http://arxiv.org/abs/2306.12245</link><description>&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#30340;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12245
&lt;/p&gt;
&lt;p&gt;
BEER^2&#26159;&#19968;&#31181;&#29992;&#20110;Retriever&#21644;Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#65288;EL&#65289;&#26159;&#20449;&#24687;&#25552;&#21462;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#23427;&#30340;&#19968;&#33324;&#24418;&#24335;&#65288;&#21363;&#31471;&#21040;&#31471;EL&#65289;&#26088;&#22312;&#39318;&#20808;&#22312;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#20013;&#25214;&#21040;&#25552;&#21450;&#65292;&#24182;&#23558;&#25552;&#21450;&#38142;&#25509;&#21040;&#29305;&#23450;&#30693;&#35782;&#24211;&#20013;&#30340;&#30456;&#24212;&#23454;&#20307;&#12290;&#26368;&#36817;&#65292;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#20419;&#36827;&#20102;&#31471;&#21040;&#31471;EL&#30340;&#36827;&#23637;&#65292;&#21463;&#30410;&#20110;&#23494;&#38598;&#30340;&#23454;&#20307;&#26816;&#32034;&#21644;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20165;&#20197;&#27969;&#27700;&#32447;&#26041;&#24335;&#21333;&#29420;&#35757;&#32451;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#65292;&#24573;&#30053;&#20102;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20043;&#38388;&#20132;&#20114;&#24102;&#26469;&#30340;&#30410;&#22788;&#12290;&#20026;&#20102;&#20351;&#26816;&#32034;&#22120;-&#38405;&#35835;&#22120;&#33539;&#24335;&#26356;&#23436;&#32654;&#22320;&#25191;&#34892;&#31471;&#21040;&#31471;EL&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BEER$^2$&#65292;&#19968;&#31181;&#29992;&#20110;Retriever and Reader&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#26694;&#26550;&#12290;&#36890;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;&#21452;&#21521;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;BEER$^2$&#25351;&#23548;&#26816;&#32034;&#22120;&#21644;&#38405;&#35835;&#22120;&#20114;&#30456;&#23398;&#20064;&#65292;&#20849;&#21516;&#36827;&#27493;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#31471;&#21040;&#31471;EL&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity Linking (EL) is a fundamental task for Information Extraction and Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first find mentions in the given input document and then link the mentions to corresponding entities in a specific knowledge base. Recently, the paradigm of retriever-reader promotes the progress of end-to-end EL, benefiting from the advantages of dense entity retrieval and machine reading comprehension. However, the existing study only trains the retriever and the reader separately in a pipeline manner, which ignores the benefit that the interaction between the retriever and the reader can bring to the task. To advance the retriever-reader paradigm to perform more perfectly on end-to-end EL, we propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever and Reader. Through our designed bidirectional end-to-end training, BEER$^2$ guides the retriever and the reader to learn from each other, make progress together, and ultimate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#36825;&#20123;&#24187;&#35273;&#12290;&#24182;&#19988;&#36890;&#36807;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2305.18248</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30693;&#36947;&#33258;&#24049;&#22312;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Language Models Know When They're Hallucinating References?. (arXiv:2305.18248v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#36825;&#20123;&#24187;&#35273;&#12290;&#24182;&#19988;&#36890;&#36807;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#40657;&#30418;&#26597;&#35810;&#26469;&#36827;&#34892;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#20854;&#8220;&#24187;&#35273;&#8221;&#21442;&#32771;&#25991;&#29486;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#34394;&#26500;&#30340;&#25991;&#31456;&#21644;&#20070;&#21517;&#24341;&#36215;&#20102;&#21361;&#23475;&#65292;&#23545;&#23427;&#20204;&#30340;&#20351;&#29992;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#24182;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#21453;&#24377;&#12290;&#23613;&#31649;&#20854;&#20182;&#31867;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#20063;&#24456;&#37325;&#35201;&#65292;&#20294;&#25105;&#20204;&#23558;&#24187;&#35273;&#21442;&#32771;&#25991;&#29486;&#25552;&#20986;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#24187;&#35273;&#30740;&#31350;&#30340;&#8220;&#26524;&#34631;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#29305;&#21035;&#23481;&#26131;&#30740;&#31350;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31616;&#21333;&#30340;&#25628;&#32034;&#24341;&#25806;&#26597;&#35810;&#21487;&#21487;&#38752;&#22320;&#35782;&#21035;&#27492;&#31867;&#24187;&#35273;&#65292;&#20174;&#32780;&#20415;&#20110;&#35780;&#20272;&#12290;&#20026;&#20102;&#24320;&#22987;&#21078;&#26512;&#24187;&#35273;&#35821;&#35328;&#27169;&#22411;&#21442;&#32771;&#25991;&#29486;&#30340;&#24615;&#36136;&#65292;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#23545;&#21516;&#19968;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#26597;&#35810;&#26469;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#19981;&#20511;&#21161;&#20219;&#20309;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#23558;&#8220;&#30452;&#25509;&#8221;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#19982;&#8220;&#38388;&#25509;&#8221;&#26597;&#35810;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21518;&#32773;&#35810;&#38382;&#20102;&#38468;&#21152;&#30340;&#32454;&#33410;&#65292;&#22914;&#20316;&#21697;&#30340;&#20316;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art language models (LMs) are famous for "hallucinating" references. These fabricated article and book titles lead to harms, obstacles to their use, and public backlash. While other types of LM hallucinations are also important, we propose hallucinated references as the "drosophila" of research on hallucination in large language models (LLMs), as they are particularly easy to study. We show that simple search engine queries reliably identify such hallucinations, which facilitates evaluation. To begin to dissect the nature of hallucinated LM references, we attempt to classify them using black-box queries to the same LM, without consulting any external resources. Consistency checks done with "direct" queries about whether the generated reference title is real (inspired by Kadavath et al. 2022, Lin et al. 2022, Manakul et al. 2023) are compared to consistency checks with "indirect" queries which ask for ancillary details such as the authors of the work. These consistency chec
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>http://arxiv.org/abs/2305.14456</link><description>&lt;p&gt;
&#22312;&#31048;&#31095;&#20043;&#21518;&#21917;&#21860;&#37202;&#65311;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having Beer after Prayer? Measuring Cultural Bias in Large Language Models. (arXiv:2305.14456v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14456
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#20986;&#29616;&#30340;&#25991;&#21270;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#29616;&#35937;&#65292;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#31561;&#20843;&#20010;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#23545;&#20110;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#23384;&#22312;&#25991;&#21270;&#20559;&#35265;&#65311;&#35821;&#35328;&#27169;&#22411;&#31526;&#21512;&#25152;&#26381;&#21153;&#31038;&#21306;&#30340;&#25991;&#21270;&#22240;&#32032;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#34920;&#26126;&#22312;&#22788;&#29702;&#21644;&#29983;&#25104;&#38463;&#25289;&#20271;&#25991;&#26412;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26174;&#33879;&#30340;&#20559;&#21521;&#35199;&#26041;&#25991;&#21270;&#30340;&#20559;&#35265;&#65292;&#20542;&#21521;&#20110;&#20135;&#29983;&#35199;&#26041;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#32780;&#38750;&#38463;&#25289;&#20271;&#25991;&#21270;&#30456;&#20851;&#20869;&#23481;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20174;&#22312;&#32447;&#31038;&#20132;&#23186;&#20307;&#19978;&#25910;&#38598;&#30340;&#33258;&#28982;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#21644;&#22522;&#20110;&#21487;&#33021;&#24615;&#35780;&#20998;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#36825;&#31181;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#38463;&#25289;&#20271;&#35821;&#21333;&#35821;&#21644;&#22810;&#35821;&#27169;&#22411;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25991;&#21270;&#26041;&#38754;&#23384;&#22312;&#35199;&#26041;&#25991;&#21270;&#20559;&#35265;&#65292;&#21253;&#25324;&#20154;&#21517;&#12289;&#39135;&#21697;&#12289;&#26381;&#35013;&#12289;&#22320;&#28857;&#12289;&#25991;&#23398;&#12289;&#39278;&#26009;&#12289;&#23447;&#25945;&#21644;&#20307;&#32946;&#12290;&#24403;&#36755;&#20837;&#30340;&#38463;&#25289;&#20271;&#35821;&#21477;&#23376;&#36234;&#25509;&#36817;&#33521;&#35821;&#26102;&#65292;&#27169;&#22411;&#20063;&#26356;&#23481;&#26131;&#34920;&#29616;&#20986;&#20559;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#24341;&#21457;&#20154;&#20204;&#23545;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#25991;&#21270;&#30456;&#20851;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#27169;&#22411;&#35774;&#35745;&#20013;&#24212;&#26356;&#22810;&#32771;&#34385;&#25991;&#21270;&#22240;&#32032;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are language models culturally biased? It is important that language models conform to the cultural aspects of the communities they serve. However, we show in this paper that language models suffer from a significant bias towards Western culture when handling and generating text in Arabic, often preferring, and producing Western-fitting content as opposed to the relevant Arab content. We quantify this bias through a likelihood scoring-based metric using naturally occurring contexts that we collect from online social media. Our experiments reveal that both Arabic monolingual and multilingual models exhibit bias towards Western culture in eight different cultural aspects: person names, food, clothing, location, literature, beverage, religion, and sports. Models also tend to exhibit more bias when prompted with Arabic sentences that are more linguistically aligned with English. These findings raise concerns about the cultural relevance of current language models. Our analyses show that pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#65288;PaD&#65289;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#19987;&#19994;&#21270;&#30340;&#23567;&#27169;&#22411;&#12290;PaD&#20351;&#29992;&#31243;&#24207;&#36741;&#21161;&#25512;&#29702;&#21152;&#24378;&#19987;&#19994;&#21270;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#38169;&#35823;&#26816;&#26597;&#26469;&#24110;&#21161;&#23427;&#20204;&#20811;&#26381;&#38169;&#35823;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;</title><link>http://arxiv.org/abs/2305.13888</link><description>&lt;p&gt;
PaD: &#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#19987;&#27880;&#20110;&#25512;&#29702;&#30340;&#22823;&#22411;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PaD: Program-aided Distillation Specializes Large Models in Reasoning. (arXiv:2305.13888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#65288;PaD&#65289;&#25216;&#26415;&#65292;&#23427;&#21487;&#20197;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#19987;&#19994;&#21270;&#30340;&#23567;&#27169;&#22411;&#12290;PaD&#20351;&#29992;&#31243;&#24207;&#36741;&#21161;&#25512;&#29702;&#21152;&#24378;&#19987;&#19994;&#21270;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#38169;&#35823;&#26816;&#26597;&#26469;&#24110;&#21161;&#23427;&#20204;&#20811;&#26381;&#38169;&#35823;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#20294;&#23427;&#20204;&#30340;&#22823;&#23567;&#21644;&#19981;&#21487;&#35775;&#38382;&#24615;&#23545;&#20110;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#23545;LLMs&#36827;&#34892;&#31934;&#28860;&#20197;&#33719;&#21462;&#19987;&#19994;&#25216;&#33021;&#65292;&#22312;&#21830;&#19994;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#36890;&#29992;&#33021;&#21147;&#30340;&#20132;&#25442;&#65292;&#31216;&#20026;&#27169;&#22411;&#19987;&#19994;&#21270;&#12290;&#23545;&#20110;&#25512;&#29702;&#33021;&#21147;&#65292;&#20844;&#21496;&#24050;&#21512;&#25104;&#29992;&#20110;&#21518;&#32493;&#25552;&#28860;&#30340;&#24605;&#32500;&#38142;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24187;&#35273;&#65292;LLMs&#30340;&#21512;&#25104;&#24605;&#32500;&#38142;&#21253;&#21547;&#38169;&#35823;&#25512;&#29702;&#65292;&#36825;&#20123;&#19981;&#27491;&#30830;&#30340;&#25512;&#29702;&#27493;&#39588;&#25439;&#23475;&#20102;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31243;&#24207;&#36741;&#21161;&#33976;&#39311;&#65288;PaD&#65289;&#65292;&#23427;&#21487;&#20197;&#33976;&#39311;LLMs&#20197;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#33719;&#24471;&#19987;&#19994;&#21270;&#30340;&#23567;&#27169;&#22411;&#12290;&#22312;PaD&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#31243;&#24207;&#36741;&#21161;&#25512;&#29702;&#21152;&#24378;&#19987;&#19994;&#21270;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#21270;&#38169;&#35823;&#26816;&#26597;&#26469;&#24110;&#21161;&#23427;&#20204;&#20811;&#26381;&#38169;&#35823;&#30340;&#25512;&#29702;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;GSM8K&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;PaD&#30340;0.06B&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#32988;&#36807;&#26576;&#20123;LLMs&#65288;&#20363;&#22914;LLaMA&#65289;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#21462;&#24471;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) excel in several natural language processing tasks, their size and inaccessibility present challenges for extensive practical application. Previous studies acquire specialized skills through distillation on LLMs, which result in trading generic abilities, called model specialization. As for reasoning ability, chain-of-thought was synthesized to subsequent distillation. However, due to hallucination, synthetic chain-of-thought from LLMs contains faulty reasoning. These incorrect reasoning steps damage the reasoning capability. To tackle above issues, we propose Program-aided Distillation (PaD), which distills LLMs to obtain specialized small models in reasoning tasks. In PaD, we strengthen specialized models with program-aided reasoning, and help them overcome faulty reasoning steps with automated error checking. Experimental results demonstrate that, on the GSM8K benchmark, a 0.06B model using PaD can not only outperform certain LLMs (e.g., LLaMA), bu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.09098</link><description>&lt;p&gt;
&#20219;&#21153;&#26080;&#20851;BERT&#21387;&#32553;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Weight-Inherited Distillation for Task-Agnostic BERT Compression. (arXiv:2305.09098v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#23601;&#21487;&#20197;&#35757;&#32451;&#20986;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#26159;&#21387;&#32553;BERT&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#20043;&#21069;&#30340;KD&#26041;&#27861;&#20391;&#37325;&#20110;&#20026;&#23398;&#29983;&#27169;&#22411;&#35774;&#35745;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#20197;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#36825;&#20123;&#26041;&#27861;&#20197;&#38388;&#25509;&#30340;&#26041;&#24335;&#20256;&#36882;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26435;&#37325;&#32487;&#25215;&#33976;&#39311;&#65288;WID&#65289;&#26041;&#27861;&#65292;&#30452;&#25509;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#12290;WID&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#23545;&#40784;&#25439;&#22833;&#65292;&#36890;&#36807;&#32487;&#25215;&#26435;&#37325;&#26469;&#35757;&#32451;&#19968;&#20010;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#26032;&#35270;&#35282;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#34892;&#21387;&#32553;&#22120;&#21644;&#21015;&#21387;&#32553;&#22120;&#35774;&#35745;&#20026;&#26144;&#23556;&#65292;&#28982;&#21518;&#36890;&#36807;&#32467;&#26500;&#37325;&#21442;&#25968;&#21270;&#21387;&#32553;&#26435;&#37325;&#12290;&#22312;GLUE&#21644;SQuAD&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;WID&#20248;&#20110;&#20043;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;KD&#30340;&#22522;&#32447;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;WID&#20063;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27880;&#24847;&#21147;&#20998;&#24067;&#23545;&#40784;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#25945;&#24072;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation (KD) is a predominant approach for BERT compression. Previous KD-based methods focus on designing extra alignment losses for the student model to mimic the behavior of the teacher model. These methods transfer the knowledge in an indirect way. In this paper, we propose a novel Weight-Inherited Distillation (WID), which directly transfers knowledge from the teacher. WID does not require any additional alignment loss and trains a compact student by inheriting the weights, showing a new perspective of knowledge distillation. Specifically, we design the row compactors and column compactors as mappings and then compress the weights via structural re-parameterization. Experimental results on the GLUE and SQuAD benchmarks show that WID outperforms previous state-of-the-art KD-based baselines. Further analysis indicates that WID can also learn the attention patterns from the teacher model without any alignment loss on attention distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#35774;&#35745;&#20102;&#22810;&#39033;&#20855;&#20307;&#26041;&#27861;&#65292;&#21253;&#25324;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#21644;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#65292;&#20197;&#25235;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.11117</link><description>&lt;p&gt;
EmotionIC&#65306;&#22522;&#20110;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#30340;&#20381;&#36182;&#24314;&#27169;&#21487;&#29992;&#20110;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
EmotionIC: Emotional Inertia and Contagion-driven Dependency Modelling for Emotion Recognition in Conversation. (arXiv:2303.11117v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#35774;&#35745;&#20102;&#22810;&#39033;&#20855;&#20307;&#26041;&#27861;&#65292;&#21253;&#25324;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#21644;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#65292;&#20197;&#25235;&#21462;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#20154;&#26426;&#30028;&#38754;&#25216;&#26415;&#30340;&#36827;&#27493;&#21644;&#23454;&#26045;&#65292;&#23545;&#35805;&#20013;&#30340;&#24773;&#24863;&#35782;&#21035;&#65288;ERC&#65289;&#21560;&#24341;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#19979;&#25991;&#20381;&#36182;&#26041;&#38754;&#20002;&#22833;&#20102;&#20381;&#36182;&#20449;&#24687;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#32423;&#21035;&#19981;&#32771;&#34385;&#19978;&#19979;&#25991;&#20381;&#36182;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20381;&#36182;&#24615;&#24314;&#27169;&#26041;&#27861;&#65292;&#30001;&#24773;&#24863;&#24815;&#24615;&#21644;&#24863;&#26579;&#39537;&#21160;&#65288;EmotionIC&#65289;&#65292;&#29992;&#20110;&#22312;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#32423;&#21035;&#19978;&#36827;&#34892;&#20250;&#35805;&#24773;&#24863;&#35782;&#21035;&#12290;&#22312;&#29305;&#24449;&#25552;&#21462;&#32423;&#21035;&#65292;&#25105;&#20204;&#35774;&#35745;&#30340;&#36523;&#20221;&#25513;&#30721;&#22810;&#22836;&#27880;&#24847;&#65288;IM-MHA&#65289;&#25429;&#25417;&#23545;&#35805;&#20013;&#22522;&#20110;&#36523;&#20221;&#30340;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#65292;&#20197;&#21253;&#21547;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#19981;&#21516;&#24433;&#21709;&#26500;&#24314;&#20840;&#23616;&#24773;&#24863;&#27675;&#22260;&#65292;&#32780;&#35774;&#35745;&#30340;&#22522;&#20110;&#23545;&#35805;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;(DialogGRU)&#21017;&#32858;&#21512;&#20102;&#20108;&#20803;&#23545;&#35805;&#30340;&#24773;&#24863;&#20542;&#21521;&#65292;&#24182;&#24212;&#29992;&#20110;&#20998;&#31867;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) has attracted growing attention in recent years as a result of the advancement and implementation of human-computer interface technologies. However, previous approaches to modeling global and local context dependencies lost the diversity of dependency information and do not take the context dependency into account at the classification level. In this paper, we propose a novel approach to dependency modeling driven by Emotional Inertia and Contagion (EmotionIC) for conversational emotion recognition at the feature extraction and classification levels. At the feature extraction level, our designed Identity Masked Multi-head Attention (IM-MHA) captures the identity-based long-distant context in the dialogue to contain the diverse influence of different participants and construct the global emotional atmosphere, while the devised Dialogue-based Gate Recurrent Unit (DialogGRU) that aggregates the emotional tendencies of dyadic dialogue is applied to
&lt;/p&gt;</description></item></channel></rss>