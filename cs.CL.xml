<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#26469;&#36827;&#34892;&#20302;&#24102;&#23485;&#20998;&#25955;&#24335;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#27599;&#21488;&#26426;&#22120;&#29983;&#25104;&#19981;&#21516;&#30340;&#38543;&#26426;&#25200;&#21160;&#26469;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#65292;&#20174;&#32780;&#20855;&#26377;&#39640;&#24230;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.10015</link><description>&lt;p&gt;
&#20165;&#20351;&#29992;&#19968;&#23383;&#33410;&#65288;&#27599;&#26799;&#24230;&#65289;&#65306;&#20851;&#20110;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#36827;&#34892;&#20302;&#24102;&#23485;&#20998;&#25955;&#24335;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#31616;&#35201;&#35828;&#26126;
&lt;/p&gt;
&lt;p&gt;
Just One Byte (per gradient): A Note on Low-Bandwidth Decentralized Language Model Finetuning Using Shared Randomness. (arXiv:2306.10015v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#26469;&#36827;&#34892;&#20302;&#24102;&#23485;&#20998;&#25955;&#24335;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36890;&#36807;&#27599;&#21488;&#26426;&#22120;&#29983;&#25104;&#19981;&#21516;&#30340;&#38543;&#26426;&#25200;&#21160;&#26469;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#65292;&#20174;&#32780;&#20855;&#26377;&#39640;&#24230;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#19988;&#20855;&#26377;&#38544;&#31169;&#20445;&#25252;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#27169;&#22411;&#35757;&#32451;&#21463;&#21040;&#26799;&#24230;&#20132;&#25442;&#30340;&#36890;&#20449;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#38543;&#26426;&#24615;&#26469;&#36827;&#34892;&#20302;&#24102;&#23485;&#30340;&#20998;&#25955;&#24335;&#24494;&#35843;&#65292;&#25193;&#23637;&#20102;Malladi&#31561;&#20154;2023&#24180;&#30340;&#26368;&#26032;&#24037;&#20316;&#12290;&#35813;&#26041;&#27861;&#26159;&#23545;&#20855;&#26377;&#35760;&#24518;&#25928;&#29575;&#30340;&#21516;&#26102;&#25200;&#21160;&#38543;&#26426;&#36924;&#36817;&#65288;SPSA&#65289;&#30340;&#33258;&#28982;&#20998;&#25955;&#24335;&#25193;&#23637;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#27599;&#21488;&#26426;&#22120;&#37117;&#20351;&#29992;&#38543;&#26426;&#25968;&#29983;&#25104;&#22120;&#65288;RNG&#65289;&#26469;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#23616;&#37096;&#21487;&#37325;&#29616;&#30340;&#25200;&#21160;&#65292;&#24182;&#35745;&#31639;&#21644;&#20132;&#25442;&#26631;&#37327;&#25237;&#24433;&#26799;&#24230;&#65292;&#28982;&#21518;&#29992;&#20110;&#26356;&#26032;&#27599;&#20010;&#27169;&#22411;&#12290;&#36890;&#36807;&#23558;&#65288;&#26426;&#22120;&#65292;&#26679;&#26412;&#65289;&#26631;&#35782;&#31526;&#29992;&#20316;&#38543;&#26426;&#31181;&#23376;&#65292;&#27599;&#20010;&#27169;&#22411;&#21487;&#20197;&#37325;&#26032;&#29983;&#25104;&#24444;&#27492;&#30340;&#25200;&#21160;&#12290;&#30001;&#20110;&#26426;&#22120;&#21482;&#20132;&#25442;&#21333;&#23383;&#33410;&#30340;&#25237;&#24433;&#26799;&#24230;&#65292;&#22240;&#27492;&#36825;&#26159;&#39640;&#24230;&#36890;&#20449;&#25928;&#29575;&#30340;&#12290;&#27492;&#22806;&#65292;&#36824;&#23384;&#22312;&#28508;&#22312;&#30340;&#38544;&#31169;&#20248;&#21183;&#65292;&#22240;&#20026;&#25237;&#24433;&#26799;&#24230;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#35745;&#31639;&#65292;&#32780;&#27169;&#22411;&#20174;&#19981;&#35775;&#38382;&#20854;&#20182;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20855;&#26377;&#20302;&#36890;&#20449;&#25104;&#26412;&#30340;&#21516;&#26102;&#65292;&#20063;&#22312;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#23637;&#29616;&#20986;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model training in distributed settings is limited by the communication cost of gradient exchanges. In this short note, we extend recent work from Malladi et al. (2023), using shared randomness to perform distributed fine-tuning with low bandwidth. The method is a natural decentralized extension of memory-efficient Simultaneous Perturbation Stochastic Approximation (SPSA). Each iteration, each machine seeds a Random Number Generator (RNG) to perform local reproducible perturbations on model weights and calculate and exchange scalar projected gradients, which are then used to update each model. By using a (machine, sample) identifier as the random seed, each model can regenerate one another's perturbations. As machines only exchange single-byte projected gradients, this is highly communication efficient. There are also potential privacy benefits, as projected gradients may be calculated on different training data, and models never access the other's data. Our approach not only d
&lt;/p&gt;</description></item><item><title>MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.10012</link><description>&lt;p&gt;
MagicBrush: &#20154;&#24037;&#26631;&#27880;&#30340;&#29992;&#20110;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing. (arXiv:2306.10012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10012
&lt;/p&gt;
&lt;p&gt;
MagicBrush&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#12290;&#23427;&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;&#27492;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;InstructPix2Pix&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#25351;&#23548;&#30340;&#22270;&#20687;&#32534;&#36753;&#20174;&#20010;&#20154;&#20351;&#29992;&#21040;&#19987;&#19994;&#24212;&#29992;&#65288;&#22914;Photoshop&#65289;&#24191;&#27867;&#38656;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#38646;&#26679;&#26412;&#65292;&#35201;&#20040;&#26159;&#22312;&#33258;&#21160;&#21512;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21547;&#26377;&#22823;&#37327;&#30340;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20173;&#38656;&#35201;&#22823;&#37327;&#30340;&#25163;&#21160;&#35843;&#25972;&#25165;&#33021;&#20135;&#29983;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MagicBrush&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25351;&#23548;&#30495;&#23454;&#22270;&#20687;&#30340;&#32534;&#36753;&#65292;&#21253;&#25324;&#21333;&#20010;&#25805;&#20316;&#12289;&#22810;&#20010;&#25805;&#20316;&#12289;&#25552;&#20379;&#25513;&#30721;&#21644;&#19981;&#25552;&#20379;&#25513;&#30721;&#31561;&#19981;&#21516;&#22330;&#26223;&#12290;MagicBrush&#21253;&#25324;&#36229;&#36807;10K&#20010;&#25163;&#21160;&#26631;&#27880;&#30340;&#19977;&#20803;&#32452;&#65288;&#28304;&#22270;&#20687;&#65292;&#25351;&#20196;&#65292;&#30446;&#26631;&#22270;&#20687;&#65289;&#65292;&#25903;&#25345;&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;MagicBrush&#19978;&#24494;&#35843;InstructPix2Pix&#65292;&#24182;&#23637;&#31034;&#20102;&#26032;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#25552;&#20379;&#26356;&#22909;&#30340;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#20351;&#29992;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop. However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise. Thus, they still require lots of manual tuning to produce desirable outcomes in practice. To address this issue, we introduce MagicBrush (https://osu-nlp-group.github.io/MagicBrush/), the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing. MagicBrush comprises over 10K manually annotated triples (source image, instruction, target image), which supports trainining large-scale text-guided image editing models. We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation. We further conduct extensive experiments to evaluate cu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;</title><link>http://arxiv.org/abs/2306.09996</link><description>&lt;p&gt;
&#25506;&#31350;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#25552;&#31034;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering. (arXiv:2306.09996v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#37325;&#28857;&#20851;&#27880; BLIP2 &#27169;&#22411;&#65292;&#26469;&#25552;&#39640;&#38646;&#26679;&#26412; VQA &#30340;&#24615;&#33021;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#21487;&#20197;&#20419;&#36827; VQA &#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#20855;&#22791;&#29702;&#35299;&#21644;&#25512;&#29702;&#35270;&#35273;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#34429;&#28982;&#36817;&#26399;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#38646;&#26679;&#26412;VQA&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22797;&#26434;&#32452;&#21512;&#38382;&#39064;&#21644;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#22914;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#29702;&#26041;&#38754;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;&#20351;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;BLIP2&#27169;&#22411;&#65292;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;VQA&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;VQA&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#38382;&#39064;&#27169;&#26495;&#30340;&#26377;&#25928;&#24615;&#12289;&#23569;&#37327;&#26679;&#26412;&#31034;&#20363;&#30340;&#20316;&#29992;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#24433;&#21709;&#20197;&#21450;&#23558;&#22270;&#20687;&#26631;&#39064;&#20316;&#20026;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#34701;&#21512;&#30340;&#22909;&#22788;&#12290;&#23613;&#31649;&#32467;&#26524;&#21508;&#24322;&#65292;&#20294;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#31934;&#24515;&#35774;&#35745;&#30340;&#38382;&#39064;&#27169;&#26495;&#21644;&#25972;&#21512;&#39069;&#22806;&#35270;&#35273;&#32447;&#32034;&#65288;&#22914;&#22270;&#20687;&#26631;&#39064;&#65289;&#21487;&#20197;&#20419;&#36827;VQA&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#24403;&#23427;&#20204;&#32467;&#21512;&#20351;&#29992;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a challenging task that requires the ability to comprehend and reason with visual information. While recent vision-language models have made strides, they continue to struggle with zero-shot VQA, particularly in handling complex compositional questions and adapting to new domains i.e. knowledge-based reasoning. This paper explores the use of various prompting strategies, focusing on the BLIP2 model, to enhance zero-shot VQA performance. We conduct a comprehensive investigation across several VQA datasets, examining the effectiveness of different question templates, the role of few-shot exemplars, the impact of chain-of-thought (CoT) reasoning, and the benefits of incorporating image captions as additional visual cues. Despite the varied outcomes, our findings demonstrate that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conj
&lt;/p&gt;</description></item><item><title>ClinicalGPT&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20020;&#24202;&#22330;&#26223;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22788;&#29702;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#12290;&#23427;&#30340;&#32508;&#21512;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09968</link><description>&lt;p&gt;
ClinicalGPT: &#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#32454;&#33268;&#31934;&#35843;&#21644;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ClinicalGPT: Large Language Models Finetuned with Diverse Medical Data and Comprehensive Evaluation. (arXiv:2306.09968v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09968
&lt;/p&gt;
&lt;p&gt;
ClinicalGPT&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20020;&#24202;&#22330;&#26223;&#35774;&#35745;&#21644;&#20248;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#22788;&#29702;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#12290;&#23427;&#30340;&#32508;&#21512;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#20854;&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21033;&#29992;&#35832;&#22914;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#31561;&#25216;&#26415;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#25928;&#26524;&#21463;&#21040;&#38480;&#21046;&#65292;&#30001;&#20110;&#23384;&#22312;&#35832;&#22914;&#20107;&#23454;&#19981;&#20934;&#30830;&#12289;&#25512;&#29702;&#33021;&#21147;&#21644;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#32463;&#39564;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ClinicalGPT&#65292;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#21644;&#20248;&#21270;&#29992;&#20110;&#20020;&#24202;&#22330;&#26223;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#32467;&#21512;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#65292;&#22914;&#21307;&#30103;&#35760;&#24405;&#12289;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#22810;&#36718;&#23545;&#35805;&#21672;&#35810;&#65292;ClinicalGPT&#26356;&#22909;&#22320;&#20934;&#22791;&#22788;&#29702;&#22810;&#20010;&#20020;&#24202;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#32508;&#21512;&#35780;&#20272;&#26694;&#26550;&#65292;&#21253;&#25324;&#21307;&#23398;&#30693;&#35782;&#38382;&#31572;&#12289;&#21307;&#23398;&#32771;&#35797;&#12289;&#30149;&#20154;&#21672;&#35810;&#21644;&#21307;&#30103;&#35760;&#24405;&#30340;&#35786;&#26029;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;ClinicalGPT&#22312;&#22823;&#22810;&#25968;&#27979;&#35797;&#22330;&#26223;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have exhibited exceptional performance on various Natural Language Processing (NLP) tasks, leveraging techniques such as the pre-training, and instruction fine-tuning. Despite these advances, their effectiveness in medical applications is limited, due to challenges such as factual inaccuracies, reasoning abilities, and lack grounding in real-world experience. In this study, we present ClinicalGPT, a language model explicitly designed and optimized for clinical scenarios. By incorporating extensive and diverse real-world data, such as medical records, domain-specific knowledge, and multi-round dialogue consultations in the training process, ClinicalGPT is better prepared to handle multiple clinical task. Furthermore, we introduce a comprehensive evaluation framework that includes medical knowledge question-answering, medical exams, patient consultations, and diagnostic analysis of medical records. Our results demonstrate that ClinicalGPT significantly outperforms o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09927</link><description>&lt;p&gt;
&#35757;&#32451;&#22909;&#30340;Transformer&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Transformer&#22312;&#20855;&#26377;&#21333;&#23618;&#32447;&#24615;&#33258;&#27880;&#24847;&#23618;&#30340;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#19978;&#36890;&#36807;&#26799;&#24230;&#27969;&#36827;&#34892;&#35757;&#32451;&#30340;ICL&#26426;&#21046;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#27969;&#20855;&#26377;&#25214;&#21040;&#30446;&#26631;&#20989;&#25968;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20363;&#22914;Transformers&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65306;&#32473;&#23450;&#19968;&#20010;&#26469;&#33258;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#30340;&#30701;&#35821;&#24207;&#21015;&#30340;&#25552;&#31034;&#65292;&#23427;&#20204;&#21487;&#20197;&#21046;&#23450;&#30456;&#20851;&#30340;&#27599;&#20010;&#20196;&#29260;&#21644;&#19979;&#19968;&#20010;&#20196;&#29260;&#30340;&#39044;&#27979;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#21442;&#25968;&#26356;&#26032;&#12290;&#36890;&#36807;&#23558;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#26631;&#35760;&#30340;&#27979;&#35797;&#25968;&#25454;&#24207;&#21015;&#23884;&#20837;&#21040;&#25552;&#31034;&#20013;&#65292;&#36825;&#20351;&#24471;Transformer&#34920;&#29616;&#24471;&#20687;&#26377;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#22312;&#38543;&#26426;&#23454;&#20363;&#19978;&#35757;&#32451;Transformer&#20307;&#31995;&#32467;&#26500;&#30340;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#20250;&#27169;&#20223;&#26222;&#36890;&#26368;&#23567;&#20108;&#20056;&#27861;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based neural networks such as transformers have demonstrated a remarkable ability to exhibit in-context learning (ICL): Given a short prompt sequence of tokens from an unseen task, they can formulate relevant per-token and next-token predictions without any parameter updates. By embedding a sequence of labeled training data and unlabeled test data as a prompt, this allows for transformers to behave like supervised learning algorithms. Indeed, recent work has shown that when training transformer architectures over random instances of linear regression problems, these models' predictions mimic those of ordinary least squares.  Towards understanding the mechanisms underlying this phenomenon, we investigate the dynamics of ICL in transformers with a single linear self-attention layer trained by gradient flow on linear regression tasks. We show that despite non-convexity, gradient flow with a suitable random initialization finds a global minimum of the objective function. At this 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#26426;&#22120;&#20154;&#21160;&#20316;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#23436;&#25104;&#24635;&#32467;&#21644;&#22238;&#31572;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2306.09922</link><description>&lt;p&gt;
&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#19982;&#34394;&#25311;&#26426;&#22120;&#20154;&#36807;&#21435;&#21160;&#20316;&#30456;&#20851;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning to Summarize and Answer Questions about a Virtual Robot's Past Actions. (arXiv:2306.09922v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#26426;&#22120;&#20154;&#21160;&#20316;&#21382;&#21490;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#23436;&#25104;&#24635;&#32467;&#21644;&#22238;&#31572;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#20102;&#33258;&#21160;&#29983;&#25104;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;&#27492;&#26041;&#27861;&#33021;&#22815;&#23454;&#29616;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#20154;&#25191;&#34892;&#38271;&#24207;&#21015;&#30340;&#21160;&#20316;&#26102;&#65292;&#29992;&#25143;&#38656;&#35201;&#36731;&#26494;&#12289;&#21487;&#38752;&#22320;&#20102;&#35299;&#23427;&#20204;&#25152;&#20570;&#30340;&#20107;&#24773;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#24635;&#32467;&#21644;&#22238;&#31572;&#20851;&#20110;&#26426;&#22120;&#20154;&#20195;&#29702;&#36807;&#21435;&#21160;&#20316;&#30340;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#19968;&#20010;&#26680;&#24515;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21333;&#19968;&#31995;&#32479;&#34987;&#35757;&#32451;&#29992;&#20110;&#24635;&#32467;&#21644;&#22238;&#31572;&#20851;&#20110;&#34394;&#25311;&#26426;&#22120;&#20154;&#30340;&#33258;&#25105;&#20013;&#24515;&#35270;&#39057;&#24103;&#21644;&#38382;&#39064;&#25552;&#31034;&#30340;&#21160;&#20316;&#24207;&#21015;&#12290;&#20026;&#20102;&#23454;&#29616;&#38382;&#39064;&#22238;&#31572;&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#20851;&#20110;&#23545;&#35937;&#12289;&#21160;&#20316;&#21644;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#26426;&#22120;&#20154;&#21160;&#20316;&#24207;&#21015;&#26399;&#38388;&#21160;&#20316;&#21457;&#29983;&#30340;&#26102;&#38388;&#39034;&#24207;&#30340;&#33521;&#25991;&#38382;&#39064;&#21644;&#31572;&#26696;&#12290;&#23558;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#24635;&#32467;&#21644;&#22238;&#31572;&#38382;&#39064;&#20351;&#24471;&#20174;&#38382;&#39064;&#22238;&#31572;&#20013;&#23398;&#20064;&#30340;&#23545;&#35937;&#34920;&#31034;&#30340;&#38646;-shot&#36716;&#31227;&#33021;&#22815;&#25552;&#39640;&#21160;&#20316;&#24635;&#32467;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
When robots perform long action sequences, users will want to easily and reliably find out what they have done. We therefore demonstrate the task of learning to summarize and answer questions about a robot agent's past actions using natural language alone. A single system with a large language model at its core is trained to both summarize and answer questions about action sequences given ego-centric video frames of a virtual robot and a question prompt. To enable training of question answering, we develop a method to automatically generate English-language questions and answers about objects, actions, and the temporal order in which actions occurred during episodes of robot action in the virtual environment. Training one model to both summarize and answer questions enables zero-shot transfer of representations of objects learned through question answering to improved action summarization. % involving objects not seen in training to summarize.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#20013;&#26631;&#31614;&#25805;&#20316;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;NLI&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#35780;&#20215;&#20307;&#31995;&#30340;&#35266;&#28857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22788;&#29702;&#27880;&#37322;&#32773;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09918</link><description>&lt;p&gt;
&#29702;&#24615;&#19968;&#26080;&#25152;&#24863;&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20013;&#37325;&#26032;&#25805;&#20316;&#20013;&#31435;&#24615;
&lt;/p&gt;
&lt;p&gt;
No Strong Feelings One Way or Another: Re-operationalizing Neutrality in Natural Language Inference. (arXiv:2306.09918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#20013;&#26631;&#31614;&#25805;&#20316;&#23384;&#22312;&#30340;&#32570;&#28857;&#65292;&#25552;&#20986;&#20102;NLI&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#35780;&#20215;&#20307;&#31995;&#30340;&#35266;&#28857;&#65292;&#24182;&#27604;&#36739;&#20102;&#22788;&#29702;&#27880;&#37322;&#32773;&#19968;&#33268;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;(NLI)&#19968;&#30452;&#26159;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#22522;&#30707;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;NLI&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#19977;&#20998;&#31867;&#27169;&#24335;&#22312;&#35780;&#20272;&#27169;&#22411;&#25429;&#25417;&#20154;&#31867;&#25512;&#29702;&#30340;&#24494;&#22937;&#20043;&#22788;&#26041;&#38754;&#23384;&#22312;&#24050;&#30693;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;NLI&#25968;&#25454;&#38598;&#20013;&#20013;&#31435;&#26631;&#31614;&#30340;&#25805;&#20316;&#21270;&#25928;&#24230;&#24456;&#20302;&#65292;&#35299;&#37322;&#19981;&#19968;&#33268;&#65292;&#24182;&#19988;&#36890;&#24120;&#24573;&#30053;&#33267;&#23569;&#19968;&#20010;&#37325;&#35201;&#30340;&#20013;&#31435;&#24847;&#20041;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#32570;&#28857;&#30340;&#26377;&#23475;&#24433;&#21709;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23548;&#33268;&#27880;&#37322;&#25968;&#25454;&#38598;&#23454;&#38469;&#19978;&#38477;&#20302;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22788;&#29702;&#27880;&#37322;&#32773;&#19981;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#36817;&#30340;&#19968;&#20010;NLI&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#38382;&#39064;&#25805;&#20316;&#21270;&#30340;&#27880;&#37322;&#32773;&#30740;&#31350;&#30340;&#32570;&#38519;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20984;&#26174;&#20102;NLI&#38656;&#35201;&#26356;&#31934;&#32454;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#24076;&#26395;&#24341;&#21457;NLP&#31038;&#21306;&#23545;&#20110;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35752;&#35770;&#21644;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Inference (NLI) has been a cornerstone task in evaluating language models' inferential reasoning capabilities. However, the standard three-way classification scheme used in NLI has well-known shortcomings in evaluating models' ability to capture the nuances of natural human reasoning. In this paper, we argue that the operationalization of the neutral label in current NLI datasets has low validity, is interpreted inconsistently, and that at least one important sense of neutrality is often ignored. We uncover the detrimental impact of these shortcomings, which in some cases leads to annotation datasets that actually decrease performance on downstream tasks. We compare approaches of handling annotator disagreement and identify flaws in a recent NLI dataset that designs an annotator study based on a problematic operationalization. Our findings highlight the need for a more refined evaluation framework for NLI, and we hope to spark further discussion and action in the NLP c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#24037;&#35760;&#24405;&#65292;&#25581;&#31034;&#20102;&#31038;&#20250;&#24773;&#22659;&#23545;&#36873;&#25321;&#30284;&#30151;&#27835;&#30103;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.09877</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#31038;&#20250;&#24037;&#20316;&#35760;&#24405;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25581;&#31034;&#31038;&#20250;&#24773;&#22659;&#23545;&#30284;&#30151;&#27835;&#30103;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Revealing the impact of social circumstances on the selection of cancer therapy through natural language processing of social work notes. (arXiv:2306.09877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#24037;&#35760;&#24405;&#65292;&#25581;&#31034;&#20102;&#31038;&#20250;&#24773;&#22659;&#23545;&#36873;&#25321;&#30284;&#30151;&#27835;&#30103;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35843;&#26597;&#31038;&#20250;&#24773;&#22659;&#23545;&#30284;&#30151;&#27835;&#30103;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23545;&#31038;&#24037;&#25991;&#26723;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#33719;&#24471;&#27934;&#35265;&#12290;&#25105;&#20204;&#24320;&#21457;&#24182;&#37319;&#29992;&#20102;&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36716;&#25442;&#22120;&#65288;BERT&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20998;&#23618;&#22810;&#27493;BERT&#27169;&#22411;&#65288;BERT-MS&#65289;&#20165;&#22522;&#20110;&#20020;&#24202;&#31038;&#20250;&#24037;&#20316;&#32773;&#30340;&#25991;&#26723;&#39044;&#27979;&#20102;&#27835;&#30103;&#38774;&#21521;&#30284;&#30151;&#30340;&#22788;&#26041;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#27835;&#30103;&#20083;&#33146;&#30284;&#30340;&#25152;&#26377;&#24739;&#32773;&#30340;&#33258;&#30001;&#25991;&#26412;&#20020;&#24202;&#31038;&#20250;&#24037;&#20316;&#35760;&#24405;&#65292;&#24182;&#32467;&#21512;&#33647;&#29289;&#22788;&#26041;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#24433;&#21709;&#30284;&#30151;&#27835;&#30103;&#36873;&#25321;&#30340;&#20855;&#20307;&#31038;&#20250;&#24773;&#22659;&#12290;&#20165;&#20351;&#29992;&#31038;&#24037;&#35760;&#24405;&#65292;&#25105;&#20204;&#19968;&#30452;&#39044;&#27979;&#38774;&#21521;&#27835;&#30103;&#30340;&#20351;&#29992;&#65292;&#34920;&#26126;&#30001;&#20110;&#38750;&#20020;&#24202;&#22240;&#32032;&#32780;&#23384;&#22312;&#27835;&#30103;&#36873;&#25321;&#30340;&#31995;&#32479;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We aimed to investigate the impact of social circumstances on cancer therapy selection using natural language processing to derive insights from social worker documentation. We developed and employed a Bidirectional Encoder Representations from Transformers (BERT) based approach, using a hierarchical multi-step BERT model (BERT-MS) to predict the prescription of targeted cancer therapy to patients based solely on documentation by clinical social workers. Our corpus included free-text clinical social work notes, combined with medication prescription information, for all patients treated for breast cancer. We conducted a feature importance analysis to pinpoint the specific social circumstances that impact cancer therapy selection. Using only social work notes, we consistently predicted the administration of targeted therapies, suggesting systematic differences in treatment selection exist due to non-clinical factors. The UCSF-BERT model, pretrained on clinical text at UCSF, outperformed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2306.09869</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#29992;&#20110;&#36125;&#21494;&#26031;&#19978;&#19979;&#25991;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Cross Attention for Bayesian Context Update in Text-to-Image Diffusion Models. (arXiv:2306.09869v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;EBM&#26694;&#26550;&#65292;&#36890;&#36807;&#26356;&#26032;&#21644;&#36716;&#31227;&#19978;&#19979;&#25991;&#21521;&#37327;&#65292;&#38544;&#24335;&#26368;&#23567;&#21270;&#33021;&#37327;&#20989;&#25968;&#30340;&#23884;&#22871;&#23618;&#27425;&#65292;&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#23545;&#40784;&#38382;&#39064;&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#29983;&#25104;&#30340;&#22270;&#20687;&#26377;&#26102;&#26080;&#27861;&#25429;&#25417;&#21040;&#25991;&#26412;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#35821;&#20041;&#38169;&#20301;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#27599;&#20010;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#20013;&#21046;&#23450;&#28508;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#25991;&#26412;&#23884;&#20837;&#30340;EBM&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#33719;&#24471;&#19978;&#19979;&#25991;&#21521;&#37327;&#30340;&#23545;&#25968;&#21518;&#39564;&#26799;&#24230;&#65292;&#21487;&#20197;&#26356;&#26032;&#21644;&#36716;&#31227;&#21040;&#21518;&#32493;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#65292;&#20174;&#32780;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#23884;&#22871;&#23618;&#27425;&#30340;&#33021;&#37327;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#28508;&#22312;EBMs&#36824;&#20801;&#35768;&#38646;&#26679;&#26412;&#32452;&#21512;&#29983;&#25104;&#65292;&#21363;&#36890;&#36807;&#19981;&#21516;&#19978;&#19979;&#25991;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#24182;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#25991;&#26412;&#25552;&#31034;&#21644;&#29983;&#25104;&#22270;&#20687;&#20043;&#38388;&#30340;&#35821;&#20041;&#38169;&#20301;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable performance of text-to-image diffusion models in image generation tasks, recent studies have raised the issue that generated images sometimes cannot capture the intended semantic contents of the text prompts, which phenomenon is often called semantic misalignment. To address this, here we present a novel energy-based model (EBM) framework. Specifically, we first formulate EBMs of latent image representations and text embeddings in each cross-attention layer of the denoising autoencoder. Then, we obtain the gradient of the log posterior of context vectors, which can be updated and transferred to the subsequent cross-attention layer, thereby implicitly minimizing a nested hierarchy of energy functions. Our latent EBMs further allow zero-shot compositional generation as a linear combination of cross-attention outputs from different contexts. Using extensive experiments, we demonstrate that the proposed method is highly effective in handling various image generation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35874;&#33778;&#23572;&#24503;&#22823;&#23398;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25104;&#21151;&#22312;AmericasNLP&#26426;&#22120;&#32763;&#35793;&#22303;&#33879;&#35821;&#35328;&#20998;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;chrF&#65292;&#20854;&#20013;&#22312;Aymara&#65292;Guarani&#21644;Quechua&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.09830</link><description>&lt;p&gt;
&#35874;&#33778;&#23572;&#24503;&#22823;&#23398;&#25552;&#20132;&#32473;AmericasNLP&#26426;&#22120;&#32763;&#35793;&#22303;&#33879;&#35821;&#35328;&#20998;&#20139;&#20219;&#21153;&#30340;&#35770;&#25991;&#65288;arXiv: 2306.09830v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Sheffield's Submission to the AmericasNLP Shared Task on Machine Translation into Indigenous Languages. (arXiv:2306.09830v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35874;&#33778;&#23572;&#24503;&#22823;&#23398;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;&#25104;&#21151;&#22312;AmericasNLP&#26426;&#22120;&#32763;&#35793;&#22303;&#33879;&#35821;&#35328;&#20998;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24179;&#22343;chrF&#65292;&#20854;&#20013;&#22312;Aymara&#65292;Guarani&#21644;Quechua&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#35874;&#33778;&#23572;&#24503;&#22823;&#23398;&#25552;&#20132;&#32473;AmericasNLP 2023&#26426;&#22120;&#32763;&#35793;&#22303;&#33879;&#35821;&#35328;&#20998;&#20139;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#35813;&#20219;&#21153;&#21253;&#25324;&#23558;&#35199;&#29677;&#29273;&#35821;&#32763;&#35793;&#25104;&#21313;&#19968;&#31181;&#22303;&#33879;&#35821;&#35328;&#12290; &#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#25193;&#23637;&#65292;&#35757;&#32451;&#21644;&#19982;&#19981;&#21516;&#31181;&#31867;&#30340;NLLB-200&#32452;&#21512;&#12290; &#25105;&#20204;&#20351;&#29992;&#32452;&#32455;&#32773;&#25552;&#20379;&#30340;&#25968;&#25454;&#20197;&#21450;&#23466;&#27861;&#65292;&#25163;&#20876;&#65292;&#26032;&#38395;&#25991;&#31456;&#21644;&#20174;&#21333;&#35821;&#25968;&#25454;&#29983;&#25104;&#30340;&#22238;&#35793;&#31561;&#21508;&#31181;&#20854;&#20182;&#26469;&#28304;&#30340;&#25968;&#25454;&#12290; &#22312;&#24320;&#21457;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#25104;&#32489;&#22312;&#25152;&#26377;&#35821;&#35328;&#30340;&#24179;&#22343;chrF&#19978;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;11&#65285;&#65292;&#23588;&#20854;&#26159;&#22312;Aymara&#65292;Guarani&#21644;Quechua&#26041;&#38754;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290; &#22312;&#27979;&#35797;&#38598;&#19978;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#25152;&#26377;&#25552;&#20132;&#20013;&#26368;&#39640;&#30340;&#24179;&#22343;chrF&#65292;&#25105;&#20204;&#22312;11&#31181;&#35821;&#35328;&#20013;&#25490;&#21517;&#21069;&#22235;&#20301;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#33267;&#23569;&#19968;&#20010;&#25552;&#20132;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#25490;&#21517;&#21069;&#19977;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we describe the University of Sheffield's submission to the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages which comprises the translation from Spanish to eleven indigenous languages. Our approach consists of extending, training, and ensembling different variations of NLLB-200. We use data provided by the organizers and data from various other sources such as constitutions, handbooks, news articles, and backtranslations generated from monolingual data. On the dev set, our best submission outperforms the baseline by 11% average chrF across all languages, with substantial improvements particularly for Aymara, Guarani and Quechua. On the test set, we achieve the highest average chrF of all the submissions, we rank first in four of the eleven languages, and at least one of our submissions ranks in the top 3 for all languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PK-iL&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19978;&#28155;&#21152;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20026;&#24515;&#29702;&#21355;&#29983;&#25252;&#29702;&#21644;&#39044;&#38450;&#31574;&#30053;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.09824</link><description>&lt;p&gt;
&#22522;&#20110;&#27969;&#31243;&#30693;&#35782;&#27880;&#20837;&#30340;&#21307;&#29983;&#21451;&#22909;&#22411;&#35299;&#37322;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Process Knowledge-infused Learning for Clinician-friendly Explanations. (arXiv:2306.09824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PK-iL&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19978;&#28155;&#21152;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#32467;&#26500;&#65292;&#20174;&#32780;&#20351;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#20174;&#32780;&#20026;&#24515;&#29702;&#21355;&#29983;&#25252;&#29702;&#21644;&#39044;&#38450;&#31574;&#30053;&#25552;&#20379;&#25903;&#25345;&#21644;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#35780;&#20272;&#24515;&#29702;&#20581;&#24247;&#12290;&#36890;&#36807;&#20998;&#26512;&#22312;&#32447;&#24086;&#23376;&#21644;&#20132;&#27969;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#26816;&#27979;&#34920;&#26126;&#25233;&#37057;&#30151;&#12289;&#28966;&#34385;&#30151;&#25110;&#33258;&#26432;&#20542;&#21521;&#31561;&#24515;&#29702;&#20581;&#24247;&#24773;&#20917;&#30340;&#27169;&#24335;&#12290;&#23427;&#20204;&#36890;&#36807;&#20851;&#38190;&#35789;&#12289;&#35821;&#35328;&#26631;&#35760;&#21644;&#24773;&#24863;&#26469;&#27934;&#23519;&#20010;&#20307;&#30340;&#24515;&#29702;&#20581;&#24247;&#29366;&#20917;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#26089;&#26399;&#26816;&#27979;&#12289;&#24178;&#39044;&#21644;&#25903;&#25345;&#33267;&#20851;&#37325;&#35201;&#65292;&#21487;&#20197;&#25913;&#21892;&#24515;&#29702;&#21355;&#29983;&#25252;&#29702;&#21644;&#39044;&#38450;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20174;&#31038;&#20132;&#23186;&#20307;&#23545;&#24515;&#29702;&#20581;&#24247;&#36827;&#34892;&#35780;&#20272;&#26377;&#20004;&#20010;&#23616;&#38480;&#24615;&#65306;(1)&#23427;&#20204;&#19981;&#20250;&#23558;&#24086;&#23376;&#19982;&#20020;&#24202;&#21307;&#29983;&#30340;&#35786;&#26029;&#36807;&#31243;&#36827;&#34892;&#27604;&#36739;&#65292;(2)&#20351;&#29992;&#20020;&#24202;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#30340;&#27010;&#24565;&#65288;&#21363;&#21307;&#29983;&#21451;&#22909;&#22411;&#35299;&#37322;&#65289;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#27969;&#31243;&#30693;&#35782;&#27880;&#20837;&#30340;&#23398;&#20064;&#65288;PK-iL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#19978;&#28155;&#21152;&#20020;&#24202;&#27969;&#31243;&#30693;&#35782;&#32467;&#26500;&#65292;&#20351;&#21307;&#29983;&#33021;&#22815;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models have the potential to assess mental health using social media data. By analyzing online posts and conversations, these models can detect patterns indicating mental health conditions like depression, anxiety, or suicidal thoughts. They examine keywords, language markers, and sentiment to gain insights into an individual's mental well-being. This information is crucial for early detection, intervention, and support, improving mental health care and prevention strategies. However, using language models for mental health assessments from social media has two limitations: (1) They do not compare posts against clinicians' diagnostic processes, and (2) It's challenging to explain language model outputs using concepts that the clinician can understand, i.e., clinician-friendly explanations. In this study, we introduce Process Knowledge-infused Learning (PK-iL), a new learning paradigm that layers clinical process knowledge structures on language model outputs, enabling clinicia
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20197;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#24182;&#20248;&#21270;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2306.09821</link><description>&lt;p&gt;
&#24320;&#21457;&#29992;&#25143;&#21453;&#39304;&#30340;&#28508;&#21147;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#20197;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Unlocking the Potential of User Feedback: Leveraging Large Language Model as User Simulator to Enhance Dialogue System. (arXiv:2306.09821v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#20197;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#24182;&#20248;&#21270;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#28385;&#24847;&#24230;&#21453;&#39304;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#31995;&#32479;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#19982;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30456;&#27604;&#65292;&#30452;&#25509;&#21033;&#29992;LLMs&#20316;&#20026;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#65288;TOD&#65289;&#27169;&#22411;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25215;&#35748;LLMs&#30340;&#37325;&#22823;&#28508;&#21147;&#24182;&#25506;&#32034;&#21033;&#29992;&#23427;&#20204;&#30340;&#24778;&#20154;&#33021;&#21147;&#30340;&#25913;&#36827;&#26041;&#27861;&#38750;&#24120;&#37325;&#35201;&#12290;&#20026;&#20102;&#21033;&#29992;LLMs&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29992;&#25143;&#24341;&#23548;&#21709;&#24212;&#20248;&#21270;&#65288;UGRO&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23558;LLM&#19982;&#36739;&#23567;&#30340;TOD&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;LLM&#20316;&#20026;&#26080;&#27880;&#37322;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35780;&#20272;&#23545;&#35805;&#21709;&#24212;&#65292;&#23558;&#20854;&#19982;&#36739;&#23567;&#30340;&#32463;&#36807;&#24494;&#35843;&#30340;&#31471;&#21040;&#31471;TOD&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;LLMs&#29983;&#25104;&#30340;&#28385;&#24847;&#24230;&#21453;&#39304;&#65292;UGRO&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#30417;&#30563;&#24335;&#32463;&#36807;&#24494;&#35843;&#30340;TOD&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TOD&#27169;&#22411;&#20197;&#23545;&#35805;&#21382;&#21490;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#22312;&#29992;&#25143;&#27169;&#25311;&#22120;&#21453;&#39304;&#30340;&#24110;&#21161;&#19979;&#29983;&#25104;&#31526;&#21512;&#29992;&#25143;&#38656;&#27714;&#30340;&#39640;&#28385;&#24847;&#24230;&#21709;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;UGRO&#30340;&#26041;&#27861;&#30456;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#21033;&#29992;LLMs&#22686;&#24378;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue systems and large language models (LLMs) have gained considerable attention. However, the direct utilization of LLMs as task-oriented dialogue (TOD) models has been found to underperform compared to smaller task-specific models. Nonetheless, it is crucial to acknowledge the significant potential of LLMs and explore improved approaches for leveraging their impressive abilities. Motivated by the goal of leveraging LLMs, we propose an alternative approach called User-Guided Response Optimization (UGRO) to combine it with a smaller TOD model. This approach uses LLM as annotation-free user simulator to assess dialogue responses, combining them with smaller fine-tuned end-to-end TOD models. By utilizing the satisfaction feedback generated by LLMs, UGRO further optimizes the supervised fine-tuned TOD model. Specifically, the TOD model takes the dialogue history as input and, with the assistance of the user simulator's feedback, generates high-satisfaction responses that meet the user
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35789;&#24778;&#35766;&#24230;&#37327;&#20316;&#20026;&#19968;&#31181;&#35821;&#38899;&#21512;&#25104;&#38901;&#24459;&#29305;&#24449;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#19982;&#21333;&#35789;&#31361;&#20986;&#24615;&#26377;&#30528;&#36866;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20351;&#29992;&#24778;&#35766;&#24230;&#37327;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26080;&#27861;&#25552;&#39640;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.09814</link><description>&lt;p&gt;
&#25506;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24778;&#35766;&#24230;&#37327;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#38901;&#24459;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigating the Utility of Surprisal from Large Language Models for Speech Synthesis Prosody. (arXiv:2306.09814v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09814
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35789;&#24778;&#35766;&#24230;&#37327;&#20316;&#20026;&#19968;&#31181;&#35821;&#38899;&#21512;&#25104;&#38901;&#24459;&#29305;&#24449;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#23427;&#19982;&#21333;&#35789;&#31361;&#20986;&#24615;&#26377;&#30528;&#36866;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#20351;&#29992;&#24778;&#35766;&#24230;&#37327;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#26080;&#27861;&#25552;&#39640;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21333;&#35789;&#24778;&#35766;&#24230;&#37327;&#65292;&#21363;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#21487;&#39044;&#27979;&#24615;&#65292;&#20316;&#20026;&#19968;&#31181;&#36741;&#21161;&#35821;&#38899;&#21512;&#25104;&#38901;&#24459;&#30340;&#29305;&#24449;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#21462;&#30340;&#21333;&#35789;&#24778;&#35766;&#24230;&#37327;&#19982;&#21333;&#35789;&#31361;&#20986;&#24615;&#30340;&#30456;&#20851;&#24615;&#65292;&#21333;&#35789;&#31361;&#20986;&#24615;&#26159;&#19968;&#20010;&#23545;&#20110;&#32473;&#23450;&#35805;&#35821;&#20013;&#21333;&#35789;&#26174;&#33879;&#24615;&#30340;&#22522;&#20110;&#20449;&#21495;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;LLM&#22823;&#23567;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#24778;&#35766;&#24230;&#20540;&#20316;&#20026;&#26465;&#20214;&#30340;&#35821;&#38899;&#21512;&#25104;&#22120;&#19982;&#22522;&#20934;&#31995;&#32479;&#30456;&#27604;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#22240;&#32032;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22823;&#22411;&#33521;&#35821;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#22823;&#23567;&#19981;&#21516;&#30340;LLM&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21333;&#35789;&#24778;&#35766;&#24230;&#37327;&#21644;&#21333;&#35789;&#31361;&#20986;&#24615;&#26377;&#30528;&#36866;&#24230;&#30340;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#23427;&#20204;&#25429;&#25417;&#21040;&#20102;&#30456;&#20851;&#20294;&#19981;&#21516;&#30340;&#35821;&#35328;&#20351;&#29992;&#26041;&#38754;&#12290;&#25105;&#20204;&#21457;&#29616;&#19978;&#19979;&#25991;&#38271;&#24230;&#21644;LLM&#22823;&#23567;&#20250;&#24433;&#21709;&#30456;&#20851;&#24615;&#65292;&#20294;&#19981;&#26159;&#39044;&#26399;&#26041;&#21521;&#65292;&#38271;&#19978;&#19979;&#25991;&#21644;&#22823;&#30340;LLMs&#36890;&#24120;&#20197;&#20960;&#20046;&#32447;&#24615;&#30340;&#26041;&#24335;&#20302;&#20272;&#31361;&#20986;&#30340;&#21333;&#35789;&#12290;&#25105;&#20204;&#27809;&#26377;&#21457;&#29616;&#20351;&#29992;&#24778;&#35766;&#24230;&#37327;&#20316;&#20026;&#26465;&#20214;&#29305;&#24449;&#21487;&#20197;&#27604;&#22522;&#20934;&#31995;&#32479;&#25552;&#39640;&#35821;&#38899;&#21512;&#25104;&#36136;&#37327;&#65292;&#20294;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#27934;&#23519;&#21147;&#21644;&#24778;&#35766;&#24230;&#37327;&#20316;&#20026;&#35821;&#35328;&#29305;&#24449;&#29992;&#20110;&#35821;&#38899;&#21512;&#25104;&#38901;&#24459;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the use of word surprisal, a measure of the predictability of a word in a given context, as a feature to aid speech synthesis prosody. We explore how word surprisal extracted from large language models (LLMs) correlates with word prominence, a signal-based measure of the salience of a word in a given discourse. We also examine how context length and LLM size affect the results, and how a speech synthesizer conditioned with surprisal values compares with a baseline system. To evaluate these factors, we conducted experiments using a large corpus of English text and LLMs of varying sizes. Our results show that word surprisal and word prominence are moderately correlated, suggesting that they capture related but distinct aspects of language use. We find that length of context and size of the LLM impact the correlations, but not in the direction anticipated, with longer contexts and larger LLMs generally underpredicting prominent words in a nearly linear manner. We d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;&#33258;&#21160;&#26631;&#27880;&#30340;SRED$^{\rm FM}$&#21644;&#20154;&#24037;&#20462;&#35746;&#30340;RED$^{\rm FM}$&#12290;SRED$^{\rm FM}$&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#12289;400&#31181;&#20851;&#31995;&#31867;&#22411;&#12289;13&#31181;&#23454;&#20307;&#31867;&#22411;&#65292;&#24635;&#20849;&#36229;&#36807;4000&#19975;&#20010;&#19977;&#20803;&#32452;&#23454;&#20363;&#65307;RED$^{\rm FM}$&#26159;RED&#30340;&#31934;&#31616;&#29256;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#33021;&#26377;&#25928;&#29992;&#20110;&#24314;&#31435;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09802</link><description>&lt;p&gt;
RED$^{\rm FM}$&#65306;&#19968;&#20010;&#32463;&#36807;&#28388;&#27874;&#21644;&#22810;&#35821;&#35328;&#22788;&#29702;&#30340;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RED$^{\rm FM}$: a Filtered and Multilingual Relation Extraction Dataset. (arXiv:2306.09802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#20998;&#21035;&#26159;&#33258;&#21160;&#26631;&#27880;&#30340;SRED$^{\rm FM}$&#21644;&#20154;&#24037;&#20462;&#35746;&#30340;RED$^{\rm FM}$&#12290;SRED$^{\rm FM}$&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#12289;400&#31181;&#20851;&#31995;&#31867;&#22411;&#12289;13&#31181;&#23454;&#20307;&#31867;&#22411;&#65292;&#24635;&#20849;&#36229;&#36807;4000&#19975;&#20010;&#19977;&#20803;&#32452;&#23454;&#20363;&#65307;RED$^{\rm FM}$&#26159;RED&#30340;&#31934;&#31616;&#29256;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#33021;&#26377;&#25928;&#29992;&#20110;&#24314;&#31435;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#33719;&#21462;&#20851;&#31995;&#20107;&#23454;&#65292;&#24357;&#21512;&#33258;&#28982;&#35821;&#35328;&#21644;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#24448;&#24448;&#20381;&#36182;&#20110;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#23545;&#20110;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;&#20851;&#31995;&#31867;&#22411;&#35206;&#30422;&#29575;&#20063;&#36739;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36164;&#28304;&#65292;&#21487;&#29992;&#20110;&#22521;&#35757;&#21644;&#35780;&#20272;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#12290;&#20854;&#19968;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;SRED$^{\rm FM}$&#65292;&#28085;&#30422;&#20102;18&#31181;&#35821;&#35328;&#12289;400&#31181;&#20851;&#31995;&#31867;&#22411;&#12289;13&#31181;&#23454;&#20307;&#31867;&#22411;&#65292;&#24635;&#20849;&#36229;&#36807;4000&#19975;&#20010;&#19977;&#20803;&#32452;&#23454;&#20363;&#12290;&#20854;&#20108;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#20154;&#24037;&#20462;&#35746;&#30340;&#12289;&#38024;&#23545;&#19971;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;RED$^{\rm FM}$&#65292;&#21487;&#29992;&#20110;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26032;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;mREBEL&#36827;&#34892;&#23454;&#39564;&#65292;&#21253;&#25324;&#23454;&#20307;&#31867;&#22411;&#22312;&#20869;&#30340;&#19977;&#20803;&#32452;&#34987;&#25277;&#21462;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation Extraction (RE) is a task that identifies relationships between entities in a text, enabling the acquisition of relational facts and bridging the gap between natural language and structured knowledge. However, current RE models often rely on small datasets with low coverage of relation types, particularly when working with languages other than English. In this paper, we address the above issue and provide two new resources that enable the training and evaluation of multilingual RE systems. First, we present SRED$^{\rm FM}$, an automatically annotated dataset covering 18 languages, 400 relation types, 13 entity types, totaling more than 40 million triplet instances. Second, we propose RED$^{\rm FM}$, a smaller, human-revised dataset for seven languages that allows for the evaluation of multilingual RE systems. To demonstrate the utility of these novel datasets, we experiment with the first end-to-end multilingual RE model, mREBEL, that extracts triplets, including entity types,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20869;&#23384;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;LOMO&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#20174;&#32780;&#38477;&#20302;&#30740;&#31350;&#38376;&#27099;&#12290;</title><link>http://arxiv.org/abs/2306.09782</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Full Parameter Fine-tuning for Large Language Models with Limited Resources. (arXiv:2306.09782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#20869;&#23384;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;LOMO&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#26377;&#38480;&#36164;&#28304;&#19979;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#21442;&#25968;&#24494;&#35843;&#65292;&#20174;&#32780;&#38477;&#20302;&#30740;&#31350;&#38376;&#27099;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20855;&#26377;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;GPU&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#36896;&#25104;&#30740;&#31350;&#38376;&#27099;&#39640;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21442;&#25968;&#26377;&#25928;&#24494;&#35843;&#65292;&#21363;&#24494;&#35843;&#25110;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#65292;&#20294;&#24456;&#23569;&#26377;&#20851;&#27880;&#22312;&#26377;&#38480;&#36164;&#28304;&#24773;&#20917;&#19979;&#20840;&#21442;&#25968;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#22120;LOw-Memory Optimization&#65288;LOMO&#65289;, &#36890;&#36807;&#23558;&#26799;&#24230;&#35745;&#31639;&#21644;&#21442;&#25968;&#26356;&#26032;&#19968;&#27493;&#34701;&#21512;&#20197;&#20943;&#23569;&#20869;&#23384;&#20351;&#29992;&#12290;&#36890;&#36807;&#23558;LOMO&#19982;&#29616;&#26377;&#30340;&#20869;&#23384;&#33410;&#30465;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#23558;&#20869;&#23384;&#20351;&#29992;&#37327;&#38477;&#20302;&#21040;DeepSpeed&#26041;&#26696;&#30340;10.8&#65285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;65B&#27169;&#22411;&#30340;&#20840;&#21442;&#25968;&#24494;&#35843;&#22312;&#21482;&#38656;&#21333;&#21488;&#26426;&#22120;&#19978;&#25191;&#34892;&#65292;&#35813;&#26426;&#22120;&#25645;&#36733;8&#20010;RTX 3090&#65292;&#27599;&#20010;&#26174;&#23384;&#20026;24GB&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 RTX 3090, each with 24GB memory.
&lt;/p&gt;</description></item><item><title>&#20013;&#25991;&#24635;&#32467;&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.09752</link><description>&lt;p&gt;
&#31036;&#35980;&#21051;&#26495;&#21360;&#35937;&#21644;&#25915;&#20987;&#21521;&#37327;&#65306;&#26085;&#38889;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Politeness Stereotypes and Attack Vectors: Gender Stereotypes in Japanese and Korean Language Models. (arXiv:2306.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09752
&lt;/p&gt;
&lt;p&gt;
&#20013;&#25991;&#24635;&#32467;&#35813;&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#65292;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#25915;&#20987;&#21521;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36319;&#19978;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#20351;&#29992;&#65292;&#24615;&#21035;&#20559;&#35265;&#30740;&#31350;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#38750;&#33521;&#35821;&#20559;&#35265;&#30740;&#31350;&#36824;&#22788;&#20110;&#36215;&#27493;&#38454;&#27573;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19982;&#31036;&#35980;&#27700;&#24179;&#30456;&#20851;&#30340;&#35821;&#27861;&#24615;&#21035;&#20559;&#35265;&#22312;&#26085;&#35821;&#21644;&#38889;&#35821;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#35821;&#35328;&#30340;&#35821;&#35328;&#23398;&#30740;&#31350;&#24050;&#32463;&#30830;&#23450;&#20102;&#24615;&#21035;&#20559;&#35265;&#21644;&#31036;&#35980;&#27700;&#24179;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20294;&#23578;&#19981;&#28165;&#26970;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20250;&#22797;&#21046;&#36825;&#20123;&#20559;&#35265;&#12290;&#25105;&#20204;&#36890;&#36807;&#27169;&#26495;&#20998;&#26512;&#30007;&#24615;&#21644;&#22899;&#24615;&#35821;&#27861;&#24615;&#21035;&#30340;&#30456;&#23545;&#39044;&#27979;&#27010;&#29575;&#65292;&#24182;&#21457;&#29616;&#38750;&#27491;&#24335;&#31036;&#35980;&#35821;&#35328;&#26368;&#33021;&#34920;&#29616;&#20986;&#22899;&#24615;&#35821;&#27861;&#24615;&#21035;&#65292;&#32780;&#31895;&#40065;&#21644;&#27491;&#24335;&#35821;&#35328;&#26368;&#33021;&#34920;&#29616;&#20986;&#30007;&#24615;&#35821;&#27861;&#24615;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31036;&#35980;&#27700;&#24179;&#26159;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#27169;&#22411;&#20013;&#30340;&#19968;&#31181;&#25915;&#20987;&#21521;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#25216;&#24039;&#22238;&#36991;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In efforts to keep up with the rapid progress and use of large language models, gender bias research is becoming more prevalent in NLP. Non-English bias research, however, is still in its infancy with most work focusing on English. In our work, we study how grammatical gender bias relating to politeness levels manifests in Japanese and Korean language models. Linguistic studies in these languages have identified a connection between gender bias and politeness levels, however it is not yet known if language models reproduce these biases. We analyze relative prediction probabilities of the male and female grammatical genders using templates and find that informal polite speech is most indicative of the female grammatical gender, while rude and formal speech is most indicative of the male grammatical gender. Further, we find politeness levels to be an attack vector for allocational gender bias in cyberbullying detection models. Cyberbullies can evade detection through simple techniques ab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#65292;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#20998;&#26512;&#20026;&#20363;&#65292;&#25552;&#21462;&#21464;&#37327;&#20851;&#31995;&#24182;&#20351;&#29992;&#32593;&#32476;&#32508;&#21512;&#20854;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.09737</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65306;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Processing and Networks to Automate Structured Literature Reviews: An Application to Farmers Climate Change Adaptation. (arXiv:2306.09737v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#32593;&#32476;&#65292;&#33258;&#21160;&#21270;&#32467;&#26500;&#21270;&#25991;&#29486;&#32508;&#36848;&#65292;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#20998;&#26512;&#20026;&#20363;&#65292;&#25552;&#21462;&#21464;&#37327;&#20851;&#31995;&#24182;&#20351;&#29992;&#32593;&#32476;&#32508;&#21512;&#20854;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30740;&#31350;&#25991;&#31456;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#23398;&#32773;&#20204;&#24456;&#38590;&#36319;&#19978;&#19982;&#33258;&#24049;&#19987;&#19994;&#39046;&#22495;&#30456;&#20851;&#30340;&#26032;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38656;&#35201;&#36328;&#23398;&#31185;&#35299;&#20915;&#26041;&#26696;&#30340;&#22797;&#26434;&#20027;&#39064;&#65292;&#22914;&#27668;&#20505;&#21464;&#21270;&#65292;&#36328;&#23398;&#31185;&#30740;&#31350;&#20043;&#38388;&#30340;&#30693;&#35782;&#38142;&#25509;&#20063;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#40657;&#21283;&#23376;&#31867;&#22411;&#30340;&#25991;&#26412;&#25688;&#35201;&#30340;&#20852;&#36215;&#20351;&#24471;&#29702;&#35299;&#25991;&#26412;&#20851;&#31995;&#30340;&#24314;&#31435;&#21464;&#24471;&#22256;&#38590;&#65292;&#26356;&#19981;&#29992;&#35828;&#19982;&#24050;&#26377;&#29702;&#35770;&#27010;&#24565;&#21270;&#22240;&#26524;&#20851;&#31995;&#24182;&#36827;&#34892;&#20551;&#35774;&#30340;&#30456;&#20851;&#24615;&#20102;&#12290;&#26412;&#25991;&#26088;&#22312;&#21512;&#29702;&#22320;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#36890;&#36807;&#25552;&#21462;&#21464;&#37327;&#20851;&#31995;&#24182;&#20351;&#29992;&#32593;&#32476;&#32508;&#21512;&#20854;&#21457;&#29616;&#65292;&#21516;&#26102;&#19982;&#30456;&#20851;&#23398;&#31185;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#20851;&#38190;&#27010;&#24565;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#20197;&#20892;&#27665;&#24212;&#23545;&#27668;&#20505;&#21464;&#21270;&#30340;&#20998;&#26512;&#20026;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;Scopus&#20110;2022&#24180;8&#26376;&#36820;&#22238;&#30340;&#20986;&#29256;&#29289;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20998;&#26512;&#12290;&#32467;&#26524;&#23637;&#31034;...
&lt;/p&gt;
&lt;p&gt;
The fast-growing number of research articles makes it problematic for scholars to keep track of the new findings related to their areas of expertise. Furthermore, linking knowledge across disciplines in rapidly developing fields becomes challenging for complex topics like climate change that demand interdisciplinary solutions. At the same time, the rise of Black Box types of text summarization makes it difficult to understand how text relationships are built, let alone relate to existing theories conceptualizing cause-effect relationships and permitting hypothesizing. This work aims to sensibly use Natural Language Processing by extracting variables relations and synthesizing their findings using networks while relating to key concepts dominant in relevant disciplines. As an example, we apply our methodology to the analysis of farmers' adaptation to climate change. For this, we perform a Natural Language Processing analysis of publications returned by Scopus in August 2022. Results sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#32570;&#20047;&#20013;&#25991;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#35774;&#35745;&#30340;&#27979;&#35797;&#22871;&#20214;&#12290;&#21103;&#35789;&#26159;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#30340;&#20027;&#35201;&#38590;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.09725</link><description>&lt;p&gt;
&#27721;&#35821;&#35805;&#35821;&#34920;&#24449;&#32467;&#26500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Discourse Representation Structure Parsing for Chinese. (arXiv:2306.09725v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09725
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#22312;&#32570;&#20047;&#20013;&#25991;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#20026;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#35774;&#35745;&#30340;&#27979;&#35797;&#22871;&#20214;&#12290;&#21103;&#35789;&#26159;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#30340;&#20027;&#35201;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#21333;&#35821;&#33521;&#25991;&#35821;&#20041;&#20998;&#26512;&#12290;&#30456;&#21453;&#22320;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#32570;&#20047;&#20013;&#25991;&#21547;&#20041;&#34920;&#31034;&#26631;&#31614;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#32447;&#24615;&#21270;&#27721;&#35821;&#21547;&#20041;&#34920;&#31034;&#25968;&#25454;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#25910;&#38598;&#30340;&#27969;&#31243;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#35774;&#35745;&#30340;&#27979;&#35797;&#22871;&#20214;&#65292;&#20026;&#35299;&#26512;&#24615;&#33021;&#25552;&#20379;&#32454;&#31890;&#24230;&#35780;&#20272;&#65292;&#26088;&#22312;&#30740;&#31350;&#27721;&#35821;&#35299;&#26512;&#30340;&#38590;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27721;&#35821;&#35821;&#20041;&#20998;&#26512;&#30340;&#38590;&#28857;&#20027;&#35201;&#26159;&#30001;&#21103;&#35789;&#24341;&#36215;&#30340;&#12290;&#36890;&#36807;&#26426;&#22120;&#32763;&#35793;&#21644;&#33521;&#25991;&#35299;&#26512;&#22120;&#23454;&#29616;&#27721;&#35821;&#35299;&#26512;&#30340;&#24615;&#33021;&#30053;&#20302;&#20110;&#30452;&#25509;&#22312;&#27721;&#35821;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has predominantly focused on monolingual English semantic parsing. We, instead, explore the feasibility of Chinese semantic parsing in the absence of labeled data for Chinese meaning representations. We describe the pipeline of automatically collecting the linearized Chinese meaning representation data for sequential-to sequential neural networks. We further propose a test suite designed explicitly for Chinese semantic parsing, which provides fine-grained evaluation for parsing performance, where we aim to study Chinese parsing difficulties. Our experimental results show that the difficulty of Chinese semantic parsing is mainly caused by adverbs. Realizing Chinese parsing through machine translation and an English parser yields slightly lower performance than training a model directly on Chinese data.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.09719</link><description>&lt;p&gt;
&#25512;&#21160; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#30340;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Pushing the Limits of ChatGPT on NLP Tasks. (arXiv:2306.09719v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915; ChatGPT &#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#24369;&#28857;&#65292;&#21253;&#25324;&#21033;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#12289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#12289;&#36716;&#25442;&#20219;&#21153;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#20197;&#21450;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649; ChatGPT &#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#65292;&#20854;&#34920;&#29616;&#20173;&#36828;&#20302;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20854;&#20013;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#20854;&#34920;&#29616;&#27424;&#20339;&#30340;&#21407;&#22240;&#20027;&#35201;&#26377;&#65306;&#65288;1&#65289;&#25552;&#31034;&#31526;&#20013;&#30340;&#20196;&#29260;&#38480;&#21046;&#19981;&#20801;&#35768;&#20805;&#20998;&#21033;&#29992;&#30417;&#30563;&#25968;&#25454;&#38598;&#65307;&#65288;2&#65289;ChatGPT &#29983;&#25104;&#24615;&#36136;&#19982; NLP &#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#65307;&#65288;3&#65289;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#24369;&#28857;&#65292;&#22914;&#20135;&#29983;&#24187;&#35273;&#12289;&#36807;&#24230;&#20851;&#27880;&#29305;&#23450;&#20851;&#38190;&#35789;&#31561;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#36890;&#29992;&#27169;&#22359;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26088;&#22312;&#25512;&#21160; ChatGPT &#22312; NLP &#20219;&#21153;&#19978;&#30340;&#26497;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22359;&#21253;&#25324;&#65306;&#65288;1&#65289;&#19968;&#31181;&#36755;&#20837;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#22810;&#20010;&#25552;&#31034;&#31526;&#26469;&#36866;&#24212;&#26356;&#22810;&#28436;&#31034;&#65307;&#65288;2&#65289;&#20351;&#29992;&#31934;&#32454;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28436;&#31034;&#26816;&#32034;&#65307;&#65288;3&#65289;&#23558;&#20219;&#21153;&#36716;&#25442;&#20026;&#26356;&#36866;&#21512;&#29983;&#25104;&#24615;&#36136;&#30340;&#26684;&#24335;&#65307;&#65288;4&#65289;&#37319;&#29992;&#38024;&#23545; NLP &#20219;&#21153;&#35774;&#35745;&#30340;&#25512;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of ChatGPT, its performances on most NLP tasks are still well below the supervised baselines. In this work, we looked into the causes, and discovered that its subpar performance was caused by the following factors: (1) token limit in the prompt does not allow for the full utilization of the supervised datasets; (2) mismatch between the generation nature of ChatGPT and NLP tasks; (3) intrinsic pitfalls of LLMs models, e.g., hallucination, overly focus on certain keywords, etc.  In this work, we propose a collection of general modules to address these issues, in an attempt to push the limits of ChatGPT on NLP tasks. Our proposed modules include (1) a one-input-multiple-prompts strategy that employs multiple prompts for one input to accommodate more demonstrations; (2) using fine-tuned models for better demonstration retrieval; (3) transforming tasks to formats that are more tailored to the generation nature; (4) employing reasoning strategies that are tailored to addr
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#24179;&#34913;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#24615;&#33021;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.09712</link><description>&lt;p&gt;
&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#20248;&#21270;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Semi-Offline Reinforcement Learning for Optimized Text Generation. (arXiv:2306.09712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09712
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#24179;&#34913;&#20102;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#22522;&#30784;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#65292;&#24182;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#39640;&#25928;&#19988;&#24615;&#33021;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#19982;&#29615;&#22659;&#20132;&#20114;&#26377;&#20004;&#31181;&#20027;&#35201;&#26041;&#24335;&#65306;&#22312;&#32447;&#21644;&#31163;&#32447;&#12290;&#22312;&#32447;&#26041;&#27861;&#25506;&#32034;&#29615;&#22659;&#25152;&#38656;&#26102;&#38388;&#36739;&#38271;&#65292;&#32780;&#31163;&#32447;&#26041;&#27861;&#36890;&#36807;&#29306;&#29298;&#25506;&#32034;&#33021;&#21147;&#26377;&#25928;&#22320;&#33719;&#24471;&#22870;&#21169;&#20449;&#21495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21322;&#31163;&#32447;RL&#65292;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#24179;&#28369;&#22320;&#20174;&#31163;&#32447;&#36716;&#25442;&#21040;&#22312;&#32447;&#35774;&#32622;&#65292;&#24179;&#34913;&#25506;&#32034;&#33021;&#21147;&#21644;&#22521;&#35757;&#25104;&#26412;&#65292;&#24182;&#20026;&#27604;&#36739;&#19981;&#21516;RL&#35774;&#32622;&#25552;&#20379;&#29702;&#35770;&#22522;&#30784;&#12290;&#22522;&#20110;&#21322;&#31163;&#32447;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#25104;&#26412;&#12289;&#28176;&#36817;&#35823;&#24046;&#21644;&#36807;&#24230;&#25311;&#21512;&#35823;&#24046;&#30028;&#26041;&#38754;&#26368;&#20248;&#30340;RL&#35774;&#32622;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#21322;&#31163;&#32447;&#26041;&#27861;&#25928;&#29575;&#39640;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#21487;&#27604;&#24615;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning (RL), there are two major settings for interacting with the environment: online and offline. Online methods explore the environment at significant time cost, and offline methods efficiently obtain reward signals by sacrificing exploration capability. We propose semi-offline RL, a novel paradigm that smoothly transits from offline to online settings, balances exploration capability and training cost, and provides a theoretical foundation for comparing different RL settings. Based on the semi-offline formulation, we present the RL setting that is optimal in terms of optimization cost, asymptotic error, and overfitting error bound. Extensive experiments show that our semi-offline approach is efficient and yields comparable or often better performance compared with state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#24352;&#37327;&#24490;&#29615;&#32593;&#32476;&#26159;&#19968;&#31181;&#38477;&#20302;&#24773;&#24863;&#20998;&#26512;&#35745;&#31639;&#25104;&#26412;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;</title><link>http://arxiv.org/abs/2306.09705</link><description>&lt;p&gt;
&#38477;&#20302;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#35745;&#31639;&#25104;&#26412;&#65306;&#24352;&#37327;&#24490;&#29615;&#32593;&#32476;&#19982;&#24490;&#29615;&#32593;&#32476;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Reducing Computational Costs in Sentiment Analysis: Tensorized Recurrent Networks vs. Recurrent Networks. (arXiv:2306.09705v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09705
&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#24490;&#29615;&#32593;&#32476;&#26159;&#19968;&#31181;&#38477;&#20302;&#24773;&#24863;&#20998;&#26512;&#35745;&#31639;&#25104;&#26412;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#35266;&#20247;&#23545;&#26576;&#19968;&#25991;&#26412;&#30340;&#21453;&#24212;&#23545;&#20110;&#25919;&#27835;&#12289;&#30740;&#31350;&#21644;&#21830;&#19994;&#34892;&#19994;&#31561;&#22810;&#20010;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#26159;&#19968;&#31181;&#26377;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#65292;&#21033;&#29992;&#35789;&#27719;/&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#30830;&#23450;&#19981;&#21516;&#22823;&#23567;&#30340;&#25991;&#26412;&#26159;&#21542;&#34920;&#29616;&#20986;&#31215;&#26497;&#12289;&#28040;&#26497;&#25110;&#20013;&#24615;&#30340;&#24773;&#24863;&#12290;&#24490;&#29615;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#24191;&#27867;&#29992;&#20110;&#22788;&#29702;&#24207;&#21015;&#25968;&#25454;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#30340;&#27169;&#22411;&#30340;&#19968;&#20010;&#32570;&#28857;&#26159;&#21442;&#25968;&#25968;&#37327;&#26174;&#33879;&#39640;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#12290;&#24403;&#21487;&#29992;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#36825;&#31181;&#32570;&#28857;&#29978;&#33267;&#26356;&#20026;&#26174;&#33879;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26174;&#30528;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#21644;&#27491;&#21017;&#21270;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#24352;&#37327;&#21270;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#23558;&#23545;&#19968;&#20123;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anticipating audience reaction towards a certain text is integral to several facets of society ranging from politics, research, and commercial industries. Sentiment analysis (SA) is a useful natural language processing (NLP) technique that utilizes lexical/statistical and deep learning methods to determine whether different-sized texts exhibit positive, negative, or neutral emotions. Recurrent networks are widely used in machine-learning communities for problems with sequential data. However, a drawback of models based on Long-Short Term Memory networks and Gated Recurrent Units is the significantly high number of parameters, and thus, such models are computationally expensive. This drawback is even more significant when the available data are limited. Also, such models require significant over-parameterization and regularization to achieve optimal performance. Tensorized models represent a potential solution. In this paper, we classify the sentiment of some social media posts. We comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;Cross-corpus text Readability Compatibility Assessment (CRCA)&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09704</link><description>&lt;p&gt;
&#36328;&#35821;&#26009;&#38405;&#35835;&#24615;&#20860;&#23481;&#24615;&#35780;&#20272;&#65306;&#33521;&#25991;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Cross-corpus Readability Compatibility Assessment for English Texts. (arXiv:2306.09704v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;Cross-corpus text Readability Compatibility Assessment (CRCA)&#65292;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#35821;&#26009;&#24211;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#20860;&#23481;&#24615;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#20855;&#26377;&#26174;&#33879;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#30340;&#21487;&#35835;&#24615;&#35780;&#20272;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#20013;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#23545;&#35821;&#26009;&#24211;&#20860;&#23481;&#24615;&#30340;&#32570;&#20047;&#25506;&#32034;&#26500;&#25104;&#20102;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#30340;&#30740;&#31350;&#23567;&#32452;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;Cross-corpus text Readability Compatibility Assessment (CRCA)&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65306;(1) &#35821;&#26009;&#24211;&#65306;CEFR&#65292;CLEC&#65292;CLOTH&#65292;NES&#65292;OSP&#21644;RACE&#12290;&#25552;&#21462;&#20102;&#35821;&#35328;&#29305;&#24449;&#12289;GloVe&#35789;&#21521;&#37327;&#34920;&#31034;&#21644;&#23427;&#20204;&#30340;&#34701;&#21512;&#29305;&#24449;&#12290;(2) &#20998;&#31867;&#27169;&#22411;&#65306;&#37319;&#29992;&#20102;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;XGBoost&#65292;SVM&#65289;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65288;BiLSTM&#65292;Attention-BiLSTM&#65289;&#12290;(3) &#20860;&#23481;&#24615;&#25351;&#26631;&#65306;RJSD&#65292;RRNSS&#21644;NDCG&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(1) OSP&#34920;&#29616;&#26174;&#33879;&#19981;&#21516;&#20110;&#20854;&#20182;&#25968;&#25454;&#38598;&#30340;&#35777;&#23454;&#20102;&#35821;&#26009;&#20860;&#23481;&#24615;&#12290;(2) &#20860;&#23481;&#24615;&#12289;&#29305;&#24449;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#26377;&#36866;&#24212;&#24615;&#25928;&#24212;&#12290;(3) &#22312;&#19981;&#21516;&#20860;&#23481;&#24615;&#25351;&#26631;&#19979;&#24471;&#21040;&#20102;&#19968;&#33268;&#30340;&#35780;&#20272;&#32467;&#26524;&#65292;&#36825;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text readability assessment has gained significant attention from researchers in various domains. However, the lack of exploration into corpus compatibility poses a challenge as different research groups utilize different corpora. In this study, we propose a novel evaluation framework, Cross-corpus text Readability Compatibility Assessment (CRCA), to address this issue. The framework encompasses three key components: (1) Corpus: CEFR, CLEC, CLOTH, NES, OSP, and RACE. Linguistic features, GloVe word vector representations, and their fusion features were extracted. (2) Classification models: Machine learning methods (XGBoost, SVM) and deep learning methods (BiLSTM, Attention-BiLSTM) were employed. (3) Compatibility metrics: RJSD, RRNSS, and NDCG metrics. Our findings revealed: (1) Validated corpus compatibility, with OSP standing out as significantly different from other datasets. (2) An adaptation effect among corpora, feature representations, and classification methods. (3) Consistent 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31867;&#21035;&#33258;&#36866;&#24212;&#37325;&#26032;&#37319;&#26679;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30340;&#24635;&#20307;&#21484;&#22238;&#29575;&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#20934;&#30830;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09697</link><description>&lt;p&gt;
&#22522;&#20110;&#31867;&#36866;&#24212;&#30340;&#33258;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#19981;&#23436;&#20840;&#26631;&#27880;&#30340;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Class-Adaptive Self-Training for Relation Extraction with Incompletely Annotated Training Data. (arXiv:2306.09697v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09697
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31867;&#21035;&#33258;&#36866;&#24212;&#37325;&#26032;&#37319;&#26679;&#33258;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30340;&#24635;&#20307;&#21484;&#22238;&#29575;&#65292;&#32780;&#19981;&#20250;&#26126;&#26174;&#38477;&#20302;&#20934;&#30830;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20851;&#31995;&#25552;&#21462;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#31995;&#25552;&#21462;&#26088;&#22312;&#20174;&#21477;&#23376;&#21644;&#25991;&#26723;&#20013;&#25552;&#21462;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#20851;&#31995;&#25552;&#21462;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35768;&#22810;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#26159;&#19981;&#23436;&#20840;&#27880;&#37322;&#30340;&#12290;&#36825;&#34987;&#31216;&#20026;&#38169;&#35823;&#21542;&#23450;&#38382;&#39064;&#65292;&#21363;&#23558;&#26377;&#25928;&#30340;&#20851;&#31995;&#38169;&#35823;&#22320;&#27880;&#37322;&#20026;&#8220;&#26080;&#20851;&#31995;&#8221;&#12290;&#20351;&#29992;&#36825;&#31181;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#25512;&#29702;&#38454;&#27573;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#29359;&#31867;&#20284;&#30340;&#38169;&#35823;&#12290;&#33258;&#35757;&#32451;&#24050;&#34987;&#35777;&#26126;&#22312;&#32531;&#35299;&#38169;&#35823;&#21542;&#23450;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#33258;&#35757;&#32451;&#23481;&#26131;&#21463;&#21040;&#30830;&#35748;&#20559;&#24046;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#23569;&#25968;&#31867;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#31867;&#21035;&#33258;&#36866;&#24212;&#37325;&#26032;&#37319;&#26679;&#33258;&#35757;&#32451;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#20998;&#25968;&#37325;&#26032;&#23545;&#27599;&#20010;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#20102;&#37325;&#26032;&#37319;&#26679;&#12290;&#25105;&#20204;&#30340;&#37325;&#26032;&#37319;&#26679;&#31574;&#30053;&#21033;&#20110;&#39640;&#20934;&#30830;&#29575;&#21644;&#20302;&#21484;&#22238;&#29575;&#30340;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#24635;&#20307;&#21484;&#22238;&#29575;&#65292;&#32780;&#19981;&#20250;&#26174;&#30528;&#38477;&#20302;&#20934;&#30830;&#29575;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;F1&#24471;&#20998;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#25968;&#31867;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Relation extraction (RE) aims to extract relations from sentences and documents. Existing relation extraction models typically rely on supervised machine learning. However, recent studies showed that many RE datasets are incompletely annotated. This is known as the false negative problem in which valid relations are falsely annotated as 'no_relation'. Models trained with such data inevitably make similar mistakes during the inference stage. Self-training has been proven effective in alleviating the false negative problem. However, traditional self-training is vulnerable to confirmation bias and exhibits poor performance in minority classes. To overcome this limitation, we proposed a novel class-adaptive re-sampling self-training framework. Specifically, we re-sampled the pseudo-labels for each class by precision and recall scores. Our re-sampling strategy favored the pseudo-labels of classes with high precision and low recall, which improved the overall recall without significantly com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#22312;&#32447;&#33976;&#39311;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#32447;&#36880;&#27493;&#24314;&#31435;&#27169;&#22411;&#65292;&#39044;&#27979;&#26597;&#35810;&#19982;&#25991;&#26723;&#30340;&#30456;&#20851;&#24471;&#20998;&#65292;&#24182;&#22312;&#32034;&#24341;&#20013;&#39640;&#25928;&#25191;&#34892;&#65292;&#20197;&#20248;&#21270;&#25972;&#20307;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.09657</link><description>&lt;p&gt;
Online Distillation for Pseudo-Relevance Feedback&#65288;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#22312;&#32447;&#33976;&#39311;&#25216;&#26415;&#65289;
&lt;/p&gt;
&lt;p&gt;
Online Distillation for Pseudo-Relevance Feedback. (arXiv:2306.09657v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20266;&#30456;&#20851;&#21453;&#39304;&#30340;&#22312;&#32447;&#33976;&#39311;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#32447;&#36880;&#27493;&#24314;&#31435;&#27169;&#22411;&#65292;&#39044;&#27979;&#26597;&#35810;&#19982;&#25991;&#26723;&#30340;&#30456;&#20851;&#24471;&#20998;&#65292;&#24182;&#22312;&#32034;&#24341;&#20013;&#39640;&#25928;&#25191;&#34892;&#65292;&#20197;&#20248;&#21270;&#25972;&#20307;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#33976;&#39311;&#26159;&#19968;&#31181;&#25552;&#39640;&#31070;&#32463;&#25628;&#32034;&#27169;&#22411;&#25928;&#26524;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#24403;&#21069;&#65292;&#20256;&#32479;&#30340;&#33976;&#39311;&#26041;&#27861;&#37319;&#29992;&#31163;&#32447;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#21363;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#27169;&#22411;&#39044;&#27979;&#20219;&#24847;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24471;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#33976;&#39311;&#36880;&#27493;&#24314;&#31435;&#27169;&#22411;&#65292;&#20197;&#27492;&#26469;&#39044;&#27979;&#26576;&#19968;&#26597;&#35810;&#19982;&#25991;&#26723;&#20043;&#38388;&#30340;&#30456;&#20851;&#24471;&#20998;&#65292;&#24182;&#22312;&#32034;&#24341;&#20013;&#25191;&#34892;&#20986;&#33394;&#12290;&#35813;&#25216;&#26415;&#19981;&#20165;&#21487;&#20197;&#25193;&#22823;&#37325;&#26032;&#25490;&#24207;&#30340;&#25991;&#26723;&#25968;&#37327;&#65292;&#36824;&#33021;&#35782;&#21035;&#22312;&#31532;&#19968;&#38454;&#27573;&#26816;&#32034;&#20013;&#34987;&#24573;&#30053;&#30340;&#25991;&#26723;&#65292;&#20197;&#20248;&#21270;&#25972;&#20307;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model distillation has emerged as a prominent technique to improve neural search models. To date, distillation taken an offline approach, wherein a new neural model is trained to predict relevance scores between arbitrary queries and documents. In this paper, we explore a departure from this offline distillation strategy by investigating whether a model for a specific query can be effectively distilled from neural re-ranking results (i.e., distilling in an online setting). Indeed, we find that a lexical model distilled online can reasonably replicate the re-ranking of a neural model. More importantly, these models can be used as queries that execute efficiently on indexes. This second retrieval stage can enrich the pool of documents for re-ranking by identifying documents that were missed in the first retrieval stage. Empirically, we show that this approach performs favourably when compared with established pseudo relevance feedback techniques, dense retrieval methods, and sparse-dense
&lt;/p&gt;</description></item><item><title>ReactGenie&#26159;&#19968;&#20010;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2306.09649</link><description>&lt;p&gt;
ReactGenie&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#26469;&#25903;&#25345;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
ReactGenie: An Object-Oriented State Abstraction for Complex Multimodal Interactions Using Large Language Models. (arXiv:2306.09649v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09649
&lt;/p&gt;
&lt;p&gt;
ReactGenie&#26159;&#19968;&#20010;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#30340;&#32534;&#31243;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#65292;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20132;&#20114;&#24050;&#34987;&#35777;&#26126;&#27604;&#20256;&#32479;&#30340;&#22270;&#24418;&#30028;&#38754;&#26356;&#21152;&#28789;&#27963;&#12289;&#39640;&#25928;&#21644;&#36866;&#24212;&#21508;&#31181;&#29992;&#25143;&#21644;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#24320;&#21457;&#26694;&#26550;&#35201;&#20040;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#21629;&#20196;&#30340;&#22797;&#26434;&#24615;&#21644;&#32452;&#21512;&#24615;&#65292;&#35201;&#20040;&#38656;&#35201;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#22823;&#37327;&#20195;&#30721;&#26469;&#25903;&#25345;&#36825;&#20123;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ReactGenie&#65292;&#36825;&#26159;&#19968;&#20010;&#32534;&#31243;&#26694;&#26550;&#65292;&#20351;&#29992;&#20849;&#20139;&#30340;&#38754;&#21521;&#23545;&#35937;&#29366;&#24577;&#25277;&#35937;&#26469;&#25903;&#25345;&#26500;&#24314;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#31227;&#21160;&#24212;&#29992;&#31243;&#24207;&#12290;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#20849;&#20139;&#29366;&#24577;&#25277;&#35937;&#20351;&#24471;&#20351;&#29992;ReactGenie&#30340;&#24320;&#21457;&#20154;&#21592;&#33021;&#22815;&#26080;&#32541;&#22320;&#38598;&#25104;&#21644;&#32452;&#21512;&#36825;&#20123;&#27169;&#24577;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#20132;&#20114;&#12290;ReactGenie&#26159;&#26500;&#24314;&#22270;&#24418;&#24212;&#29992;&#31243;&#24207;&#30340;&#29616;&#26377;&#24037;&#20316;&#27969;&#31243;&#30340;&#33258;&#28982;&#25193;&#23637;&#65292;&#23601;&#20687;&#20351;&#29992;React-Redux&#19968;&#26679;&#12290;&#24320;&#21457;&#20154;&#21592;&#21482;&#38656;&#35201;&#28155;&#21152;&#19968;&#20123;&#27880;&#37322;&#21644;&#31034;&#20363;&#26469;&#25351;&#31034;&#33258;&#28982;&#35821;&#35328;&#22914;&#20309;&#26144;&#23556;&#21040;&#29992;&#25143;&#21487;&#35775;&#38382;&#30340;&#29366;&#24577;&#21363;&#21487;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal interactions have been shown to be more flexible, efficient, and adaptable for diverse users and tasks than traditional graphical interfaces. However, existing multimodal development frameworks either do not handle the complexity and compositionality of multimodal commands well or require developers to write a substantial amount of code to support these multimodal interactions. In this paper, we present ReactGenie, a programming framework that uses a shared object-oriented state abstraction to support building complex multimodal mobile applications. Having different modalities share the same state abstraction allows developers using ReactGenie to seamlessly integrate and compose these modalities to deliver multimodal interaction.  ReactGenie is a natural extension to the existing workflow of building a graphical app, like the workflow with React-Redux. Developers only have to add a few annotations and examples to indicate how natural language is mapped to the user-accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#36328;&#22495;&#26465;&#20214;&#19979;&#26816;&#27979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#29616;&#25104;&#30340;&#35789;&#20856;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#36328;&#22495;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#29992;&#20110;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#26576;&#20123;&#31867;&#22411;&#30340;&#20551;&#38451;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09642</link><description>&lt;p&gt;
&#36328;&#22495;&#27602;&#24615;&#29255;&#27573;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Toxic Spans Detection. (arXiv:2306.09642v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#36328;&#22495;&#26465;&#20214;&#19979;&#26816;&#27979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#29616;&#25104;&#30340;&#35789;&#20856;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#36328;&#22495;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#32780;&#29992;&#20110;&#39046;&#22495;&#20869;&#30340;&#35821;&#35328;&#27169;&#22411;&#23481;&#26131;&#20986;&#29616;&#26576;&#20123;&#31867;&#22411;&#30340;&#20551;&#38451;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#27602;&#24615;&#35821;&#35328;&#30340;&#21160;&#24577;&#24615;&#65292;&#33258;&#21160;&#26816;&#27979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#21487;&#33021;&#20250;&#36973;&#36935;&#20998;&#24067;&#36716;&#31227;&#12290;&#20026;&#20102;&#25506;&#32034;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#19977;&#31181;&#26816;&#27979;&#36328;&#22495;&#26465;&#20214;&#19979;&#27602;&#24615;&#29255;&#27573;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#35789;&#20856;&#12289;&#26681;&#22240;&#25277;&#21462;&#21644;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#29616;&#25104;&#30340;&#35789;&#20856;&#30340;&#31616;&#21333;&#26041;&#27861;&#22312;&#36328;&#22495;&#35774;&#32622;&#20013;&#34920;&#29616;&#26368;&#20339;&#12290;&#36328;&#39046;&#22495;&#35823;&#24046;&#20998;&#26512;&#34920;&#26126;&#65306;&#65288;1&#65289;&#26681;&#22240;&#25552;&#21462;&#26041;&#27861;&#23481;&#26131;&#20986;&#29616;&#20551;&#36127;&#32467;&#26524;&#65292;&#32780;&#65288;2&#65289;&#35821;&#35328;&#27169;&#22411;&#23613;&#31649;&#22312;&#39046;&#22495;&#20869;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#20854;&#26126;&#30830;&#25552;&#21462;&#27602;&#24615;&#21333;&#35789;&#30340;&#25968;&#37327;&#27604;&#35789;&#20856;&#23569;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#26576;&#20123;&#31867;&#22411;&#30340;&#20551;&#38451;&#24615;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#20844;&#24320;&#22312;https://github.com/sfschouten/toxic-cross-domain &#12290;
&lt;/p&gt;
&lt;p&gt;
Given the dynamic nature of toxic language use, automated methods for detecting toxic spans are likely to encounter distributional shift. To explore this phenomenon, we evaluate three approaches for detecting toxic spans under cross-domain conditions: lexicon-based, rationale extraction, and fine-tuned language models. Our findings indicate that a simple method using off-the-shelf lexicons performs best in the cross-domain setup. The cross-domain error analysis suggests that (1) rationale extraction methods are prone to false negatives, while (2) language models, despite performing best for the in-domain case, recall fewer explicitly toxic words than lexicons and are prone to certain types of false positives. Our code is publicly available at: https://github.com/sfschouten/toxic-cross-domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33258;&#21160;&#25968;&#25454;&#38598;&#21512;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20010;&#24615;&#21270;&#29992;&#25143;&#30011;&#20687;&#21644;&#32467;&#26500;&#21270;&#22270;&#34920;&#20013;&#30340;&#20016;&#23500;&#20132;&#20114;&#20449;&#24687;&#65292;&#36890;&#36807;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#29983;&#25104;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2306.09631</link><description>&lt;p&gt;
AUGUST&#65306;&#29992;&#20110;&#21512;&#25104;&#23545;&#35805;&#24335;&#25512;&#33616;&#25968;&#25454;&#38598;&#30340;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
AUGUST: an Automatic Generation Understudy for Synthesizing Conversational Recommendation Datasets. (arXiv:2306.09631v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#33258;&#21160;&#25968;&#25454;&#38598;&#21512;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20010;&#24615;&#21270;&#29992;&#25143;&#30011;&#20687;&#21644;&#32467;&#26500;&#21270;&#22270;&#34920;&#20013;&#30340;&#20016;&#23500;&#20132;&#20114;&#20449;&#24687;&#65292;&#36890;&#36807;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#29983;&#25104;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#25968;&#25454;&#23545;&#20110;&#23545;&#35805;&#24335;&#25512;&#33616;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#65292;&#26159;&#32593;&#32476;&#26550;&#26500;&#24320;&#21457;&#21644;&#22521;&#35757;&#31574;&#30053;&#35774;&#35745;&#30340;&#22522;&#30707;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#26469;&#25163;&#21160;&#26631;&#35760;&#25110;&#35774;&#35745;&#21644;&#25193;&#23637;&#25512;&#33616;&#23545;&#35805;&#27169;&#26495;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#30340;&#38382;&#39064;&#26159;&#65306;&#65288;i&#65289;&#26377;&#38480;&#30340;&#20154;&#24037;&#26631;&#27880;&#32773;&#25968;&#37327;&#23548;&#33268;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;&#21040;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20016;&#23500;&#21644;&#22823;&#35268;&#27169;&#30340;&#26696;&#20363;&#65292;&#65288;ii&#65289;&#26631;&#27880;&#32773;&#30340;&#26377;&#38480;&#32463;&#39564;&#21644;&#30693;&#35782;&#23548;&#33268;&#19981;&#22815;&#20449;&#24687;&#20016;&#23500;&#21644;&#19981;&#21512;&#36866;&#30340;&#25512;&#33616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#21512;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#29983;&#25104;&#22823;&#35268;&#27169;&#21644;&#39640;&#36136;&#37327;&#30340;&#25512;&#33616;&#23545;&#35805;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#21033;&#29992;&#20102;&#65306;&#65288;i&#65289;&#20256;&#32479;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20016;&#23500;&#30340;&#20010;&#24615;&#21270;&#29992;&#25143;&#30011;&#20687;&#65292;&#65288;ii&#65289;&#32467;&#26500;&#21270;&#22270;&#34920;&#20013;&#30340;&#20016;&#23500;&#20132;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-quality data is essential for conversational recommendation systems and serves as the cornerstone of the network architecture development and training strategy design. Existing works contribute heavy human efforts to manually labeling or designing and extending recommender dialogue templates. However, they suffer from (i) the limited number of human annotators results in that datasets can hardly capture rich and large-scale cases in the real world, (ii) the limited experience and knowledge of annotators account for the uninformative corpus and inappropriate recommendations. In this paper, we propose a novel automatic dataset synthesis approach that can generate both large-scale and high-quality recommendation dialogues through a data2text generation process, where unstructured recommendation conversations are generated from structured graphs based on user-item information from the real world. In doing so, we comprehensively exploit: (i) rich personalized user profiles from traditi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;CLIPScores&#20316;&#20026;&#38544;&#24335;&#21442;&#32771;&#38142;&#30340;PhotoBook&#25351;&#31216;&#28216;&#25103;&#30340;&#21548;&#32773;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#20102;&#20915;&#23450;&#22270;&#20687;&#26159;&#21542;&#19982;&#20249;&#20276;&#20849;&#20139;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#38598;/&#28216;&#25103;&#20027;&#39064;&#19978;&#21462;&#24471;&#20102;&gt;77%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&gt;17&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.09607</link><description>&lt;p&gt;
&#20351;&#29992;CLIPScores&#20316;&#20026;&#38544;&#24335;&#21442;&#32771;&#38142;&#30340;PhotoBook&#25351;&#31216;&#28216;&#25103;&#30340;&#21548;&#32773;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Listener Model for the PhotoBook Referential Game with CLIPScores as Implicit Reference Chain. (arXiv:2306.09607v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;CLIPScores&#20316;&#20026;&#38544;&#24335;&#21442;&#32771;&#38142;&#30340;PhotoBook&#25351;&#31216;&#28216;&#25103;&#30340;&#21548;&#32773;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30452;&#25509;&#35299;&#20915;&#20102;&#20915;&#23450;&#22270;&#20687;&#26159;&#21542;&#19982;&#20249;&#20276;&#20849;&#20139;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#38598;/&#28216;&#25103;&#20027;&#39064;&#19978;&#21462;&#24471;&#20102;&gt;77%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&gt;17&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PhotoBook&#26159;&#19968;&#31181;&#21327;&#20316;&#23545;&#35805;&#28216;&#25103;&#65292;&#20854;&#20013;&#20004;&#20010;&#29609;&#23478;&#25910;&#21040;&#31169;&#26377;&#30340;&#12289;&#37096;&#20998;&#37325;&#21472;&#30340;&#22270;&#20687;&#38598;&#65292;&#24182;&#35299;&#20915;&#23427;&#20204;&#20849;&#26377;&#21738;&#20123;&#22270;&#20687;&#12290;&#23427;&#21521;&#26426;&#22120;&#25552;&#20986;&#20102;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#21363;&#23398;&#20064;&#22914;&#20309;&#24314;&#31435;&#20849;&#21516;&#22522;&#30784;&#20197;&#26377;&#25928;&#22320;&#27807;&#36890;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#24320;&#21457;&#30340;&#26041;&#27861;&#19981;&#33021;&#29992;&#20110;&#30495;&#23454;&#30340;&#28216;&#25103;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#35299;&#20915;&#20102;&#28216;&#25103;&#30340;&#26576;&#20123;&#23376;&#20219;&#21153;&#65292;&#32780;&#19988;&#23427;&#20204;&#38656;&#35201;&#39069;&#22806;&#30340;&#21442;&#32771;&#38142;&#36755;&#20837;&#65292;&#20854;&#25552;&#21462;&#36807;&#31243;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#21442;&#32771;&#38142;&#30340;&#21548;&#32773;&#27169;&#22411;&#65292;&#30452;&#25509;&#35299;&#20915;&#28216;&#25103;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#21363;&#20915;&#23450;&#19968;&#20010;&#22270;&#20687;&#26159;&#21542;&#19982;&#20249;&#20276;&#20849;&#20139;&#12290;&#25105;&#20204;&#30340;DeBERTa-based&#21548;&#32773;&#27169;&#22411;&#35835;&#21462;&#23436;&#25972;&#30340;&#23545;&#35805;&#65292;&#24182;&#21033;&#29992;CLIPScore&#29305;&#24449;&#35780;&#20272;&#35805;&#35821;-&#22270;&#20687;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;&#38598;/&#28216;&#25103;&#20027;&#39064;&#19978;&#23454;&#29616;&#20102;&gt;77%&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#22522;&#32447;&gt;17&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
PhotoBook is a collaborative dialogue game where two players receive private, partially-overlapping sets of images and resolve which images they have in common. It presents machines with a great challenge to learn how people build common ground around multimodal context to communicate effectively. Methods developed in the literature, however, cannot be deployed to real gameplay since they only tackle some subtasks of the game, and they require additional reference chains inputs, whose extraction process is imperfect. Therefore, we propose a reference chain-free listener model that directly addresses the game's predictive task, i.e., deciding whether an image is shared with partner. Our DeBERTa-based listener model reads the full dialogue, and utilizes CLIPScore features to assess utterance-image relevance. We achieve &gt;77% accuracy on unseen sets of images/game themes, outperforming baseline by &gt;17 points.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.09597</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Clickbait Detection via Large Language Models. (arXiv:2306.09597v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#19978;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;LLM&#26080;&#27861;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#19988;&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#20987;&#35825;&#39575;&#65288;Clickbait&#65289;&#20250;&#36890;&#36807;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#29978;&#33267;&#24341;&#20154;&#20837;&#32988;&#30340;&#26631;&#39064;&#26469;&#35825;&#23548;&#29992;&#25143;&#36827;&#34892;&#28857;&#20987;&#65292;&#20960;&#20046;&#28183;&#36879;&#21040;&#25152;&#26377;&#22312;&#32447;&#20869;&#23481;&#21457;&#24067;&#32773;&#65292;&#22914;&#26032;&#38395;&#38376;&#25143;&#21644;&#31038;&#20132;&#23186;&#20307;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#24050;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;NLP&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;LLM&#26159;&#21542;&#21487;&#20197;&#20316;&#20026;&#39640;&#36136;&#37327;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#31995;&#32479;&#36824;&#19981;&#20026;&#20154;&#25152;&#30693;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;LLM&#22312;&#22810;&#20010;&#33521;&#25991;&#21644;&#20013;&#25991;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#21644;&#24494;&#35843;PLM&#26041;&#27861;&#30456;&#27604;&#65292;LLM&#26080;&#27861;&#36798;&#21040;&#26368;&#20339;&#32467;&#26524;&#12290;&#19982;&#20154;&#31867;&#30452;&#35273;&#19981;&#21516;&#65292;&#23454;&#39564;&#34920;&#26126;LLM&#19981;&#33021;&#20165;&#36890;&#36807;&#26631;&#39064;&#23454;&#29616;&#28385;&#24847;&#30340;&#28857;&#20987;&#35825;&#39575;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a serious of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot scenarios on a number of English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from the human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;MLM&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;CMLM-CSE&#65292;&#24378;&#21046;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#26356;&#22810;&#30340;&#25513;&#30721;&#35789;&#20449;&#24687;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#36229;&#36234;SimCSE&#12290;</title><link>http://arxiv.org/abs/2306.09594</link><description>&lt;p&gt;
&#22522;&#20110;&#26465;&#20214;MLM&#23545;&#27604;&#23398;&#20064;&#30340;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;CMLM-CSE
&lt;/p&gt;
&lt;p&gt;
CMLM-CSE: Based on Conditional MLM Contrastive Learning for Sentence Embeddings. (arXiv:2306.09594v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;MLM&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;CMLM-CSE&#65292;&#24378;&#21046;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#26356;&#22810;&#30340;&#25513;&#30721;&#35789;&#20449;&#24687;&#65292;&#21487;&#20197;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#36229;&#36234;SimCSE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27604;&#36739;&#23398;&#20064;&#21477;&#23376;&#23884;&#20837;&#25216;&#26415;&#30452;&#25509;&#20351;&#29992;&#32534;&#30721;&#22120;&#25552;&#21462;&#21477;&#23376;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#27604;&#36739;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#21477;&#23376;&#20027;&#20307;&#65292;&#32780;&#24573;&#30053;&#20102;&#21477;&#23376;&#20013;&#19968;&#20123;&#35789;&#23545;&#21477;&#23376;&#35821;&#20041;&#30340;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CMLM-CSE&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;MLM&#30340;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#20256;&#32479;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#22686;&#21152;&#19968;&#20010;&#38468;&#21152;&#30340;&#32593;&#32476;&#26469;&#38598;&#25104;&#21477;&#23376;&#23884;&#20837;&#20197;&#25191;&#34892;MLM&#20219;&#21153;&#65292;&#24378;&#21046;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#26356;&#22810;&#30340;&#25513;&#30721;&#35789;&#20449;&#24687;&#12290;&#26368;&#21518;&#65292;&#24403;&#20351;&#29992;Bertbase&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#27604;SimCSE&#39640;0.55&#20010;&#30334;&#20998;&#28857;&#65292;&#22312;&#20351;&#29992;Robertabase&#20316;&#20026;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#22312;&#25991;&#26412;&#30456;&#20284;&#24230;&#20219;&#21153;&#20013;&#24179;&#22343;&#36229;&#36807;SimCSE 0.3&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional comparative learning sentence embedding directly uses the encoder to extract sentence features, and then passes in the comparative loss function for learning. However, this method pays too much attention to the sentence body and ignores the influence of some words in the sentence on the sentence semantics. To this end, we propose CMLM-CSE, an unsupervised contrastive learning framework based on conditional MLM. On the basis of traditional contrastive learning, an additional auxiliary network is added to integrate sentence embedding to perform MLM tasks, forcing sentence embedding to learn more masked word information. Finally, when Bertbase was used as the pretraining language model, we exceeded SimCSE by 0.55 percentage points on average in textual similarity tasks, and when Robertabase was used as the pretraining language model, we exceeded SimCSE by 0.3 percentage points on average in textual similarity tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26085;&#35821;&#36825;&#31181;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#35821;&#35328;&#20013;&#65292;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#26368;&#20339;&#30340;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#65292;&#24182;&#19988;&#26080;&#35770;&#20219;&#21153;&#31867;&#22411;&#22914;&#20309;&#65292;&#26368;&#22909;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#25110;Unigram&#20316;&#20026;&#23376;&#35789;&#20998;&#35789;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.09572</link><description>&lt;p&gt;
&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#22312;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#20197;&#26085;&#35821;&#20026;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do different tokenizers perform on downstream tasks in scriptio continua languages?: A case study in Japanese. (arXiv:2306.09572v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26085;&#35821;&#36825;&#31181;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#35821;&#35328;&#20013;&#65292;&#19981;&#21516;&#30340;&#20998;&#35789;&#22120;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#26368;&#20339;&#30340;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#65292;&#24182;&#19988;&#26080;&#35770;&#20219;&#21153;&#31867;&#22411;&#22914;&#20309;&#65292;&#26368;&#22909;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#25110;Unigram&#20316;&#20026;&#23376;&#35789;&#20998;&#35789;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36830;&#32493;&#20070;&#20889;&#25991;&#23383;&#35821;&#35328;&#20013;&#65292;&#20998;&#35789;&#22120;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24433;&#21709;&#65292;&#20197;&#26085;&#35821;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#36825;&#31181;&#35821;&#35328;&#30340;&#20998;&#35789;&#22120;&#36890;&#24120;&#30001;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#21644;&#23376;&#35789;&#20998;&#35789;&#22120;&#32452;&#25104;&#65292;&#38656;&#35201;&#25105;&#20204;&#23545;&#25152;&#26377;&#21487;&#33021;&#30340;&#32452;&#21512;&#36827;&#34892;&#32508;&#21512;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#32570;&#20047;&#36825;&#31181;&#20840;&#38754;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#22823;&#37327;&#30340;&#20998;&#35789;&#22120;&#38598;&#65292;&#24182;&#20351;&#29992;&#27599;&#20010;&#38598;&#21512;&#26500;&#24314;&#20102;&#19968;&#20010;PLM&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#33539;&#22260;&#20869;&#27979;&#37327;&#20102;&#19979;&#28216;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27599;&#20010;&#19979;&#28216;&#20219;&#21153;&#37117;&#26377;&#19968;&#20010;&#19981;&#21516;&#30340;&#26368;&#20339;&#24418;&#24577;&#23398;&#20998;&#26512;&#22120;&#65292;&#24182;&#19988;&#26080;&#35770;&#20219;&#21153;&#31867;&#22411;&#22914;&#20309;&#65292;&#26368;&#22909;&#20351;&#29992;&#23383;&#33410;&#23545;&#32534;&#30721;&#25110;Unigram&#32780;&#19981;&#26159;WordPiece&#20316;&#20026;&#23376;&#35789;&#20998;&#35789;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effect of tokenizers on the downstream performance of pretrained language models (PLMs) in scriptio continua languages where no explicit spaces exist between words, using Japanese as a case study. The tokenizer for such languages often consists of a morphological analyzer and a subword tokenizer, requiring us to conduct a comprehensive study of all possible pairs. However, previous studies lack this comprehensiveness. We therefore train extensive sets of tokenizers, build a PLM using each, and measure the downstream performance on a wide range of tasks. Our results demonstrate that each downstream task has a different optimal morphological analyzer, and that it is better to use Byte-Pair-Encoding or Unigram rather than WordPiece as a subword tokenizer, regardless of the type of task.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;NLP&#31185;&#23398;&#36827;&#23637;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#20171;&#32461;&#20102;NLP&#21487;&#37325;&#22797;&#24615;&#28165;&#21333;&#30340;&#24773;&#20917;&#21644;&#36890;&#36807;&#23545;&#27492;&#28165;&#21333;10,405&#20010;&#21311;&#21517;&#22238;&#31572;&#30340;&#20998;&#26512;&#21457;&#29616;&#26576;&#20123;&#39033;&#22238;&#31572;&#8220;&#26159;&#8221;&#30340;&#25552;&#20132;&#25968;&#37327;&#22686;&#21152;&#12289;&#38543;&#20043;&#25509;&#21463;&#29575;&#22686;&#21152;&#20197;&#21450;&#24320;&#28304;&#20195;&#30721;&#19982;&#21487;&#37325;&#22797;&#24615;&#24471;&#20998;&#25552;&#39640;&#20043;&#38388;&#30340;&#20851;&#31995;&#31561;&#12290;</title><link>http://arxiv.org/abs/2306.09562</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#65306;&#20174;&#28165;&#21333;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reproducibility in NLP: What Have We Learned from the Checklist?. (arXiv:2306.09562v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;NLP&#31185;&#23398;&#36827;&#23637;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#20171;&#32461;&#20102;NLP&#21487;&#37325;&#22797;&#24615;&#28165;&#21333;&#30340;&#24773;&#20917;&#21644;&#36890;&#36807;&#23545;&#27492;&#28165;&#21333;10,405&#20010;&#21311;&#21517;&#22238;&#31572;&#30340;&#20998;&#26512;&#21457;&#29616;&#26576;&#20123;&#39033;&#22238;&#31572;&#8220;&#26159;&#8221;&#30340;&#25552;&#20132;&#25968;&#37327;&#22686;&#21152;&#12289;&#38543;&#20043;&#25509;&#21463;&#29575;&#22686;&#21152;&#20197;&#21450;&#24320;&#28304;&#20195;&#30721;&#19982;&#21487;&#37325;&#22797;&#24615;&#24471;&#20998;&#25552;&#39640;&#20043;&#38388;&#30340;&#20851;&#31995;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#30340;&#31185;&#23398;&#36827;&#23637;&#24314;&#31435;&#22312;&#30740;&#31350;&#20154;&#21592;&#22768;&#26126;&#30340;&#21487;&#37325;&#22797;&#24615;&#22522;&#30784;&#20043;&#19978;&#12290;*CL&#20250;&#35758;&#22312;2020&#24180;&#21019;&#24314;&#20102;NLP&#21487;&#37325;&#22797;&#24615;&#28165;&#21333;&#65292;&#35201;&#27714;&#20316;&#32773;&#22312;&#25552;&#20132;&#26102;&#23436;&#25104;&#65292;&#20197;&#25552;&#37266;&#20182;&#20204;&#21253;&#25324;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;10,405&#20010;&#21311;&#21517;&#22238;&#24212;&#65292;&#25552;&#20379;&#20102;&#35813;&#28165;&#21333;&#30340;&#31532;&#19968;&#27425;&#20998;&#26512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#28165;&#21333;&#24341;&#20837;&#21518;&#65292;&#26377;&#20851;&#25928;&#29575;&#12289;&#39564;&#35777;&#24615;&#33021;&#12289;&#25688;&#35201;&#32479;&#35745;&#21644;&#36229;&#21442;&#25968;&#30340;&#20449;&#24687;&#25253;&#21578;&#26377;&#25152;&#22686;&#21152;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25509;&#21463;&#29575;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#8220;&#26159;&#8221;&#22238;&#31572;&#30340;&#25552;&#20132;&#22686;&#38271;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25910;&#38598;&#26032;&#25968;&#25454;&#30340;44%&#30340;&#25552;&#20132;&#27604;&#27809;&#26377;&#25910;&#38598;&#26032;&#25968;&#25454;&#30340;&#25552;&#20132;&#23569;5%&#30340;&#34987;&#25509;&#21463;&#29575;&#65307;&#36825;&#20123;&#25552;&#20132;&#30340;&#24179;&#22343;&#35780;&#23457;&#21487;&#37325;&#22797;&#24615;&#19982;&#20854;&#20182;&#25552;&#20132;&#30456;&#27604;&#20063;&#20302;&#20102;2%&#12290;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;46%&#30340;&#25552;&#20132;&#22768;&#26126;&#24320;&#28304;&#20182;&#20204;&#30340;&#20195;&#30721;&#65292;&#23613;&#31649;&#37319;&#29992;&#27492;&#39033;&#30340;&#25552;&#20132;&#19982;&#26410;&#37319;&#29992;&#27492;&#39033;&#30340;&#25552;&#20132;&#30456;&#27604;&#65292;&#21487;&#37325;&#22797;&#24615;&#24471;&#20998;&#39640;8&#65285;&#65292;&#26159;&#20219;&#20309;&#26465;&#30446;&#20013;&#26368;&#39640;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scientific progress in NLP rests on the reproducibility of researchers' claims. The *CL conferences created the NLP Reproducibility Checklist in 2020 to be completed by authors at submission to remind them of key information to include. We provide the first analysis of the Checklist by examining 10,405 anonymous responses to it. First, we find evidence of an increase in reporting of information on efficiency, validation performance, summary statistics, and hyperparameters after the Checklist's introduction. Further, we show acceptance rate grows for submissions with more Yes responses. We find that the 44% of submissions that gather new data are 5% less likely to be accepted than those that did not; the average reviewer-rated reproducibility of these submissions is also 2% lower relative to the rest. We find that only 46% of submissions claim to open-source their code, though submissions that do have 8% higher reproducibility score relative to those that do not, the most for any item. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#40065;&#26834;&#24615;&#29983;&#25104;&#24335;&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#22359;&#65292;&#19982;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#36941;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#22495;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#24615;&#65292;&#23454;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09544</link><description>&lt;p&gt;
&#22797;&#26434;&#20219;&#21153;&#30340;&#26500;&#24314;&#27169;&#22359;&#65306;&#39046;&#22495;&#36716;&#31227;&#19979;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#29983;&#25104;&#24615;&#20107;&#20214;&#25552;&#21462;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building blocks for complex tasks: Robust generative event extraction for radiology reports under domain shifts. (arXiv:2306.09544v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22312;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#40065;&#26834;&#24615;&#29983;&#25104;&#24335;&#27169;&#22411;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#12290;&#36890;&#36807;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#22359;&#65292;&#19982;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#36941;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#22495;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#24615;&#65292;&#23454;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20174;&#25918;&#23556;&#23398;&#25253;&#21578;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#32771;&#35797;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#27867;&#21270;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#27880;&#37322;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27425;T5&#22522;&#20110;&#25991;&#26412;&#30340;&#29983;&#25104;&#27169;&#22411;&#27604;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#20219;&#21153;&#29305;&#23450;&#20998;&#31867;&#23618;&#30340;&#26041;&#27861;&#22312;&#32771;&#35797;&#26041;&#24335;&#20043;&#38388;&#34920;&#29616;&#26356;&#19978;&#20339;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20943;&#23569;&#27169;&#22411;&#25512;&#29702;&#25104;&#26412;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#20020;&#24202;&#24212;&#29992;&#30340;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#22788;&#29702;&#21464;&#24471;&#26356;&#21152;&#21487;&#34892;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#29983;&#25104;&#25216;&#26415;&#65292;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#20219;&#21153;&#22359;&#65292;&#24403;&#19982;&#22810;&#20219;&#21153;&#35757;&#32451;&#30456;&#32467;&#21512;&#26102;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#36941;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#25105;&#20204;&#21033;&#29992;&#30446;&#26631;&#22495;&#19978;&#19979;&#25991;&#26469;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#12290;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20851;&#19981;&#21516;&#25104;&#26412;&#38477;&#20302;&#31574;&#30053;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores methods for extracting information from radiology reports that generalize across exam modalities to reduce requirements for annotated data. We demonstrate that multi-pass T5-based text-to-text generative models exhibit better generalization across exam modalities compared to approaches that employ BERT-based task-specific classification layers. We then develop methods that reduce the inference cost of the model, making large-scale corpus processing more feasible for clinical applications. Specifically, we introduce a generative technique that decomposes complex tasks into smaller subtask blocks, which improves a single-pass model when combined with multitask training. In addition, we leverage target-domain contexts during inference to enhance domain adaptation, enabling use of smaller models. Analyses offer insights into the benefits of different cost reduction strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09539</link><description>&lt;p&gt;
&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Block-State Transformer. (arXiv:2306.09539v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#32467;&#21512;&#20102;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#22359;&#21464;&#25442;&#22120;&#65292;&#26088;&#22312;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#25552;&#39640;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22312;&#38656;&#35201;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#24615;&#24182;&#19988;&#38656;&#35201;&#39640;&#25928;&#25193;&#23637;&#21040;&#38271;&#24207;&#21015;&#30340;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#25928;&#26524;&#12290;&#23613;&#31649;&#26368;&#21021;&#26159;&#20026;&#36830;&#32493;&#20449;&#21495;&#35774;&#35745;&#30340;&#65292;&#20294;SSM&#22312;&#35270;&#35273;&#21644;&#38899;&#39057;&#31561;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#65307;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#20013;&#65292;SSM&#20173;&#28982;&#33853;&#21518;&#20110;Transformers&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#65288;BST&#65289;&#30340;&#28151;&#21512;&#23618;&#65292;&#23427;&#22312;&#20869;&#37096;&#32452;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#38271;&#36317;&#31163;&#19978;&#19979;&#25991;&#21270;&#30340;SSM&#23376;&#23618;&#21644;&#19968;&#20010;&#29992;&#20110;&#30701;&#26399;&#24207;&#21015;&#34920;&#31034;&#30340;&#22359;&#21464;&#25442;&#22120;&#23376;&#23618;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#12289;&#23436;&#20840;&#21487;&#24182;&#34892;&#30340;&#38598;&#25104;SSM&#21644;&#22359;&#27880;&#24847;&#21147;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#35821;&#35328;&#24314;&#27169;&#30340;&#22256;&#24785;&#24230;&#19978;&#20248;&#20110;&#31867;&#20284;&#30340;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#25512;&#24191;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;&#27492;&#22806;&#65292;&#22359;&#29366;&#24577;&#21464;&#25442;&#22120;&#22312;&#23618;&#32423;&#21035;&#19978;&#20855;&#26377;&#36229;&#36807;&#21313;&#20493;&#30340;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2306.09525</link><description>&lt;p&gt;
&#21033;&#29992;&#22686;&#24378;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;GPT-4&#65289;&#35299;&#37322;&#27861;&#24459;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Explaining Legal Concepts with Augmented Large Language Models (GPT-4). (arXiv:2306.09525v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#21033;&#29992;GPT-4&#22686;&#24378;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#19982;&#22522;&#20934;&#35774;&#32622;&#30456;&#27604;&#65292;&#20351;&#29992;&#22686;&#24378;&#30340;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#21487;&#20197;&#25552;&#39640;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#24182;&#28040;&#38500;&#27169;&#22411;&#30340;&#24187;&#35273;&#38382;&#39064;&#65292;&#20174;&#32780;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#27861;&#24459;&#24320;&#25918;&#24615;&#26415;&#35821;&#30340;&#21547;&#20041;&#26159;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#20808;&#21069;&#27861;&#38498;&#26696;&#20363;&#20013;&#35813;&#26415;&#35821;&#30340;&#24212;&#29992;&#26159;&#35299;&#37322;&#20854;&#21547;&#20041;&#30340;&#37325;&#35201;&#26469;&#28304;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;GPT-4&#29983;&#25104;&#27861;&#24459;&#26415;&#35821;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#12289;&#28165;&#26224;&#24615;&#21644;&#30456;&#20851;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;GPT-4&#34987;&#30452;&#25509;&#35201;&#27714;&#35299;&#37322;&#27861;&#24459;&#26415;&#35821;&#30340;&#22522;&#20934;&#35774;&#32622;&#30340;&#24615;&#33021;&#19982;&#22686;&#24378;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#22312;&#22686;&#24378;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#27861;&#24459;&#20449;&#24687;&#26816;&#32034;&#27169;&#22359;&#34987;&#29992;&#26469;&#20026;&#27169;&#22411;&#25552;&#20379;&#30456;&#20851;&#32972;&#26223;&#65292;&#21363;&#26469;&#33258;&#26696;&#20363;&#27861;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30452;&#25509;&#24212;&#29992;GPT-4&#20135;&#29983;&#30340;&#35299;&#37322;&#22312;&#34920;&#38754;&#19978;&#20284;&#20046;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#35814;&#32454;&#20998;&#26512;&#25581;&#31034;&#20102;&#35299;&#37322;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#24615;&#33021;&#21487;&#20197;&#25552;&#39640;&#36136;&#37327;&#65292;&#24182;&#20284;&#20046;&#28040;&#38500;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#21457;&#26126;&#19981;&#27491;&#30830;&#30340;&#38472;&#36848;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interpreting the meaning of legal open-textured terms is a key task of legal professionals. An important source for this interpretation is how the term was applied in previous court cases. In this paper, we evaluate the performance of GPT-4 in generating factually accurate, clear and relevant explanations of terms in legislation. We compare the performance of a baseline setup, where GPT-4 is directly asked to explain a legal term, to an augmented approach, where a legal information retrieval module is used to provide relevant context to the model, in the form of sentences from case law. We found that the direct application of GPT-4 yields explanations that appear to be of very high quality on their surface. However, detailed analysis uncovered limitations in terms of the factual accuracy of the explanations. Further, we found that the augmentation leads to improved quality, and appears to eliminate the issue of hallucination, where models invent incorrect statements. These findings ope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.09519</link><description>&lt;p&gt;
&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#21442;&#32771;&#23454;&#20307;&#23545;&#39044;&#27979;&#20851;&#31995;&#30340;&#26410;&#35265;&#20107;&#23454;&#12290;&#29616;&#26377;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#36127;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22522;&#20110;&#36793;&#30028;&#30340;&#25490;&#21517;&#25439;&#22833;&#65292;&#20294;&#36825;&#23481;&#26131;&#23548;&#33268;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#30452;&#35273;&#19978;&#65292;&#19982;&#27491;&#26679;&#26412;&#26356;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#23558;&#23545;&#27169;&#22411;&#36129;&#29486;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#25417;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;RANA&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#27880;&#37322;&#20154;&#29289;&#20256;&#35760;&#20107;&#20214;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#35757;&#32451;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#20197;&#26816;&#27979;&#23454;&#20307;&#21450;&#20854;&#30456;&#20851;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#20998;&#26512;&#32500;&#22522;&#30334;&#31185;&#20256;&#35760;&#20013;&#26377;&#20851;&#22899;&#24615;&#21644;&#38750;&#35199;&#26041;&#20154;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2306.09505</link><description>&lt;p&gt;
Wikibio: &#29992;&#20110;&#20154;&#29289;&#20256;&#35760;&#20107;&#20214;&#20132;&#21449;&#20998;&#26512;&#30340;&#35821;&#20041;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Wikibio: a Semantic Resource for the Intersectional Analysis of Biographical Events. (arXiv:2306.09505v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#27880;&#37322;&#20154;&#29289;&#20256;&#35760;&#20107;&#20214;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#65292;&#24182;&#35757;&#32451;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#20197;&#26816;&#27979;&#23454;&#20307;&#21450;&#20854;&#30456;&#20851;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#20998;&#26512;&#32500;&#22522;&#30334;&#31185;&#20256;&#35760;&#20013;&#26377;&#20851;&#22899;&#24615;&#21644;&#38750;&#35199;&#26041;&#20154;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#29289;&#20256;&#35760;&#20107;&#20214;&#26816;&#27979;&#26159;&#25506;&#32034;&#21644;&#27604;&#36739;&#20154;&#20204;&#29983;&#27963;&#21465;&#36848;&#21644;&#34920;&#36798;&#26041;&#24335;&#30340;&#30456;&#20851;&#20219;&#21153;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#23427;&#21487;&#20197;&#25903;&#25345;&#25968;&#23383;&#20154;&#25991;&#23398;&#21644;&#25506;&#32034;&#26377;&#20851;&#23569;&#25968;&#32676;&#20307;&#20559;&#35265;&#30340;&#30740;&#31350;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#27809;&#26377;&#19987;&#38376;&#38024;&#23545;&#27492;&#20219;&#21153;&#35774;&#35745;&#30340;&#35821;&#26009;&#24211;&#21644;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#27880;&#37322;&#20154;&#29289;&#20256;&#35760;&#20107;&#20214;&#26816;&#27979;&#30340;&#35821;&#26009;&#24211;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;20&#31687;&#32500;&#22522;&#30334;&#31185;&#20256;&#35760;&#65292;&#19982;&#20116;&#20010;&#29616;&#26377;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#27604;&#36739;&#20197;&#35757;&#32451;&#29992;&#20110;&#20154;&#29289;&#20256;&#35760;&#20107;&#20214;&#26816;&#27979;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20197;0.808&#30340;F-score&#26816;&#27979;&#20256;&#35760;&#20013;&#30446;&#26631;&#23454;&#20307;&#30340;&#25152;&#26377;&#25552;&#21450;&#65292;&#20197;&#21450;&#20197;0.859&#30340;F-score&#26816;&#27979;&#23454;&#20307;&#30456;&#20851;&#20107;&#20214;&#12290;&#26368;&#21518;&#65292;&#35813;&#27169;&#22411;&#34987;&#29992;&#20110;&#20998;&#26512;&#32500;&#22522;&#30334;&#31185;&#20256;&#35760;&#20013;&#26377;&#20851;&#22899;&#24615;&#21644;&#38750;&#35199;&#26041;&#20154;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biographical event detection is a relevant task for the exploration and comparison of the ways in which people's lives are told and represented. In this sense, it may support several applications in digital humanities and in works aimed at exploring bias about minoritized groups. Despite that, there are no corpora and models specifically designed for this task. In this paper we fill this gap by presenting a new corpus annotated for biographical event detection. The corpus, which includes 20 Wikipedia biographies, was compared with five existing corpora to train a model for the biographical event detection task. The model was able to detect all mentions of the target-entity in a biography with an F-score of 0.808 and the entity-related events with an F-score of 0.859. Finally, the model was used for performing an analysis of biases about women and non-Western people in Wikipedia biographies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#24615;&#33021;&#21487;&#33021;&#20986;&#29616;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36825;&#19968;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#35760;&#24518;&#37325;&#29616;&#12289;&#23398;&#20064;&#26679;&#26412;&#38169;&#35823;&#12289;&#20219;&#21153;&#26131;&#20110;&#24178;&#25200;&#12289;&#21644;&#20219;&#21153;&#31034;&#33539;&#30340;&#35823;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.09479</link><description>&lt;p&gt;
&#36870;&#21521;&#32553;&#25918;&#65306;&#21464;&#24471;&#26356;&#22823;&#24182;&#19981;&#24847;&#21619;&#30528;&#26356;&#22909;
&lt;/p&gt;
&lt;p&gt;
Inverse Scaling: When Bigger Isn't Better. (arXiv:2306.09479v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20219;&#21153;&#24615;&#33021;&#21487;&#33021;&#20986;&#29616;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36825;&#19968;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#35760;&#24518;&#37325;&#29616;&#12289;&#23398;&#20064;&#26679;&#26412;&#38169;&#35823;&#12289;&#20219;&#21153;&#26131;&#20110;&#24178;&#25200;&#12289;&#21644;&#20219;&#21153;&#31034;&#33539;&#30340;&#35823;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#12289;&#35745;&#31639;&#37327;&#30340;&#22686;&#21152;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#25439;&#22833;&#27604;&#20363;&#26377;&#21487;&#39044;&#27979;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;LMs&#20063;&#21487;&#33021;&#26174;&#31034;&#36870;&#21521;&#32553;&#25918;&#65292;&#21363;&#38543;&#30528;&#35268;&#27169;&#30340;&#22686;&#21152;&#20219;&#21153;&#24615;&#33021;&#36234;&#26469;&#36234;&#24046;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#35757;&#32451;&#30446;&#26631;&#21644;&#25968;&#25454;&#30340;&#32570;&#38519;&#25152;&#33268;&#12290;&#26412;&#25991;&#36890;&#36807;&#20844;&#24320;&#27604;&#36187;&#65292;Inverse Scaling Prize&#65292;&#22312;11&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#36870;&#21521;&#32553;&#25918;&#29616;&#35937;&#12290;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#38598;&#21450;&#20854;&#20182;&#23454;&#20363;&#65292;&#25105;&#20204;&#35748;&#20026;&#36870;&#21521;&#32553;&#25918;&#30340;&#21407;&#22240;&#21487;&#33021;&#26377;&#22235;&#31181;&#65306;&#65288;i&#65289;&#20542;&#21521;&#20110;&#37325;&#22797;&#35760;&#24518;&#30340;&#24207;&#21015;&#32780;&#38750;&#36319;&#38543;&#19978;&#19979;&#25991;&#25351;&#31034;&#65292;&#65288;ii&#65289;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#27169;&#20223;&#19981;&#33391;&#27169;&#24335;&#65292;&#65288;iii&#65289;&#20219;&#21153;&#20013;&#26377;&#19968;&#20010;&#26131;&#20110;&#24178;&#25200;LMs&#30340;&#20219;&#21153;&#65292;&#23558;&#20854;&#27880;&#24847;&#21147;&#36716;&#31227;&#21040;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#65292;&#32780;&#38750;&#36739;&#38590;&#30340;&#20219;&#21153;&#65292;&#65288;iv&#65289;&#20219;&#21153;&#30340;&#27491;&#30830;&#31034;&#33539;&#35823;&#23548;LMs&#12290;&#20316;&#32773;&#36824;&#20844;&#24067;&#20102;&#27604;&#36187;&#30340;&#33719;&#32988;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Work on scaling laws has found that large language models (LMs) show predictable improvements to overall loss with increased scale (model size, training data, and compute). Here, we present evidence for the claim that LMs may show inverse scaling, or worse task performance with increased scale, e.g., due to flaws in the training objective and data. We present empirical evidence of inverse scaling on 11 datasets collected by running a public contest, the Inverse Scaling Prize, with a substantial prize pool. Through analysis of the datasets, along with other examples found in the literature, we identify four potential causes of inverse scaling: (i) preference to repeat memorized sequences over following in-context instructions, (ii) imitation of undesirable patterns in the training data, (iii) tasks containing an easy distractor task which LMs could focus on, rather than the harder real task, and (iv) correct but misleading few-shot demonstrations of the task. We release the winning data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2306.09442</link><description>&lt;p&gt;
&#20174;&#38646;&#24320;&#22987;&#23454;&#29616;&#32418;&#38431;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#25506;&#32034;&#19982;&#24314;&#31435;
&lt;/p&gt;
&lt;p&gt;
Explore, Establish, Exploit: Red Teaming Language Models from Scratch. (arXiv:2306.09442v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32418;&#38431;&#34892;&#21160;&#65292;&#36890;&#36807;&#20174;&#39640;&#23618;&#27425;&#12289;&#25277;&#35937;&#30340;&#35268;&#33539;&#20986;&#21457;&#26469;&#32771;&#34385;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#20197;&#25506;&#31350;&#27169;&#22411;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#27602;&#25110;&#19981;&#35802;&#23454;&#38472;&#36848;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#24341;&#20837;&#20102;&#24037;&#20855;&#20197;&#35843;&#26597;&#26377;&#23475;&#36755;&#20986;&#65292;&#20197;&#35782;&#21035;&#21644;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#34429;&#28982;&#36825;&#26159;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#23433;&#20840;&#30340;&#26377;&#20215;&#20540;&#27493;&#39588;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29616;&#26377;&#30340;&#38024;&#23545;&#19981;&#24076;&#26395;&#30340;&#36755;&#20986;&#30340;&#20998;&#31867;&#22120;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21482;&#26377;&#39044;&#20808;&#30693;&#36947;&#26377;&#23475;&#34892;&#20026;&#31867;&#22411;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#36339;&#36807;&#20102;&#32418;&#38431;&#34892;&#21160;&#30340;&#26680;&#24515;&#25361;&#25112;&#65306;&#24320;&#21457;&#27169;&#22411;&#21487;&#33021;&#23637;&#31034;&#30340;&#34892;&#20026;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#24403;&#36825;&#26679;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#23384;&#22312;&#26102;&#65292;&#32418;&#38431;&#34892;&#21160;&#30340;&#36793;&#38469;&#20215;&#20540;&#26377;&#38480;&#65292;&#22240;&#20026;&#20998;&#31867;&#22120;&#21487;&#20197;&#29992;&#20110;&#36807;&#28388;&#35757;&#32451;&#25968;&#25454;&#25110;&#27169;&#22411;&#36755;&#20986;&#12290;&#26412;&#25991;&#32771;&#34385;&#22312;&#20551;&#35774;&#23545;&#25163;&#20174;&#39640;&#32423;&#12289;&#25277;&#35937;&#30340;&#19981;&#33391;&#34892;&#20026;&#35268;&#33539;&#20986;&#21457;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#32418;&#38431;&#34892;&#21160;&#12290;&#32418;&#38431;&#24212;&#35813;&#22312;&#31934;&#21270;/&#25193;&#23637;&#27492;&#35268;&#33539;&#30340;&#21516;&#26102;&#23545;&#25239;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying Large language models (LLMs) can pose hazards from harmful outputs such as toxic or dishonest speech. Prior work has introduced tools that elicit harmful outputs in order to identify and mitigate these risks. While this is a valuable step toward securing language models, these approaches typically rely on a pre-existing classifier for undesired outputs. This limits their application to situations where the type of harmful behavior is known with precision beforehand. However, this skips a central challenge of red teaming: developing a contextual understanding of the behaviors that a model can exhibit. Furthermore, when such a classifier already exists, red teaming has limited marginal value because the classifier could simply be used to filter training data or model outputs. In this work, we consider red teaming under the assumption that the adversary is working from a high-level, abstract specification of undesired behavior. The red team is expected to refine/extend this spec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#35780;&#20272;&#20102;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#27169;&#22411;&#22312;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#27604;&#36739;&#20102;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21709;&#24212;&#29983;&#25104;&#30340;&#26368;&#20339;&#28201;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#21442;&#25968;&#20248;&#21270;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.09390</link><description>&lt;p&gt;
&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#33258;&#26432;&#39118;&#38505;&#35780;&#20272;&#65306;&#27169;&#22411;&#24615;&#33021;&#12289;&#28508;&#21147;&#21644;&#38480;&#21046;&#30340;&#37327;&#21270;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
ChatGPT for Suicide Risk Assessment on Social Media: Quantitative Evaluation of Model Performance, Potentials and Limitations. (arXiv:2306.09390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#35780;&#20272;&#20102;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#30340; ChatGPT &#27169;&#22411;&#22312;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#27604;&#36739;&#20102;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#27169;&#22411;&#21709;&#24212;&#29983;&#25104;&#30340;&#26368;&#20339;&#28201;&#24230;&#65292;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#65292;&#36825;&#31687;&#35770;&#25991;&#20026;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#27169;&#22411;&#35780;&#20272;&#21644;&#21442;&#25968;&#20248;&#21270;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#26469;&#37327;&#21270;&#35780;&#20272;&#20132;&#20114;&#24335; ChatGPT &#27169;&#22411;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#36827;&#34892;&#33258;&#26432;&#20542;&#21521;&#35780;&#20272;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#39532;&#37324;&#20848;&#22823;&#23398; Reddit &#33258;&#26432;&#20542;&#21521;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#23454;&#39564;&#26469;&#23545; ChatGPT &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#25216;&#26415;&#35780;&#20272;&#65292;&#24182;&#23558;&#20854;&#32467;&#26524;&#19982;&#20004;&#20010;&#22522;&#20110; transformer &#30340;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#28201;&#24230;&#21442;&#25968;&#23545; ChatGPT &#21709;&#24212;&#29983;&#25104;&#30340;&#24433;&#21709;&#65292;&#24182;&#35752;&#35770;&#22522;&#20110; ChatGPT &#19981;&#30830;&#23450;&#24615;&#29575;&#30340;&#26368;&#20339;&#28201;&#24230;&#21442;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982; ChatGPT &#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#20855;&#26377;&#30456;&#24403;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#20294;&#20197;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#38598;&#20026;&#22522;&#30784;&#24494;&#35843;&#30340; Transformer &#27169;&#22411;&#34920;&#29616;&#26356;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#38416;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#35843;&#25972; ChatGPT &#30340;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#20854;&#22312;&#36825;&#19968;&#20851;&#38190;&#20219;&#21153;&#20013;&#24110;&#21161;&#24515;&#29702;&#20581;&#24247;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel framework for quantitatively evaluating the interactive ChatGPT model in the context of suicidality assessment from social media posts, utilizing the University of Maryland Reddit suicidality dataset. We conduct a technical evaluation of ChatGPT's performance on this task using Zero-Shot and Few-Shot experiments and compare its results with those of two fine-tuned transformer-based models. Additionally, we investigate the impact of different temperature parameters on ChatGPT's response generation and discuss the optimal temperature based on the inconclusiveness rate of ChatGPT. Our results indicate that while ChatGPT attains considerable accuracy in this task, transformer-based models fine-tuned on human-annotated datasets exhibit superior performance. Moreover, our analysis sheds light on how adjusting the ChatGPT's hyperparameters can improve its ability to assist mental health professionals in this critical task.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.09361</link><description>&lt;p&gt;
MFAS: &#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#65292;&#27169;&#25311;&#20154;&#31867;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
MFAS: Emotion Recognition through Multiple Perspectives Fusion Architecture Search Emulating Human Cognition. (arXiv:2306.09361v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09361
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#35282;&#24230;&#34701;&#21512;&#32467;&#26500;&#25628;&#32034;&#30340;&#24773;&#24863;&#35782;&#21035;&#26694;&#26550;&#65292;&#27169;&#25311;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#65292;&#33021;&#22815;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#26088;&#22312;&#35782;&#21035;&#21644;&#20998;&#26512;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;&#23436;&#32654;&#30340;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21892;&#21508;&#31181;&#20154;&#26426;&#20132;&#20114;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#29702;&#35299;&#24773;&#24863;&#30340;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#37327;&#21270;&#24314;&#27169;&#30456;&#27604;&#65292;&#20174;&#36830;&#32493;&#30340;&#35282;&#24230;&#29702;&#35299;&#35821;&#38899;&#20869;&#23481;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#29702;&#35299;&#65292;&#33021;&#22815;&#20351;&#27169;&#22411;&#25429;&#25417;&#26356;&#20840;&#38754;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#26681;&#25454;&#35821;&#38899;&#20013;&#23384;&#22312;&#30340;&#26576;&#20123;&#32447;&#32034;&#35843;&#25972;&#24773;&#24863;&#21333;&#35789;&#30340;&#25991;&#26412;&#35821;&#20041;&#30340;&#24863;&#30693;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#24182;&#25628;&#32034;&#20004;&#31181;&#20449;&#24687;&#30340;&#26368;&#20339;&#34701;&#21512;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#35843;&#25972;&#24863;&#30693;&#30340;&#37325;&#35201;&#24615;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;Multiple perspectives Fusion Architecture Search(MFAS)&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech emotion recognition aims to identify and analyze emotional states in target speech similar to humans. Perfect emotion recognition can greatly benefit a wide range of human-machine interaction tasks. Inspired by the human process of understanding emotions, we demonstrate that compared to quantized modeling, understanding speech content from a continuous perspective, akin to human-like comprehension, enables the model to capture more comprehensive emotional information. Additionally, considering that humans adjust their perception of emotional words in textual semantic based on certain cues present in speech, we design a novel search space and search for the optimal fusion strategy for the two types of information. Experimental results further validate the significance of this perception adjustment. Building on these observations, we propose a novel framework called Multiple perspectives Fusion Architecture Search (MFAS). Specifically, we utilize continuous-based knowledge to capt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08161</link><description>&lt;p&gt;
h2oGPT&#65306;&#27665;&#20027;&#21270;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
h2oGPT: Democratizing Large Language Models. (arXiv:2306.08161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#30446;&#26631;&#26159;&#21019;&#24314;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#65292;&#25552;&#39640;&#20154;&#24037;&#26234;&#33021;&#30340;&#24320;&#21457;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPTs&#65289;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#22240;&#20854;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#29616;&#23454;&#24212;&#29992;&#32780;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#38761;&#21629;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20063;&#24102;&#26469;&#20102;&#35768;&#22810;&#37325;&#22823;&#30340;&#39118;&#38505;&#65292;&#22914;&#23384;&#22312;&#26377;&#20559;&#35265;&#12289;&#31169;&#20154;&#25110;&#26377;&#23475;&#25991;&#26412;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#29256;&#26435;&#26448;&#26009;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;h2oGPT&#65292;&#36825;&#26159;&#19968;&#22871;&#24320;&#28304;&#20195;&#30721;&#24211;&#65292;&#29992;&#20110;&#21019;&#24314;&#21644;&#20351;&#29992;&#22522;&#20110;GPTs&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19990;&#30028;&#19978;&#26368;&#22909;&#30340;&#30495;&#27491;&#24320;&#28304;&#30340;&#26367;&#20195;&#23553;&#38381;&#28304;GPTs&#12290;&#19982;&#24320;&#28304;&#31038;&#21306;&#21512;&#20316;&#65292;&#20316;&#20026;&#20854;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#20960;&#20010;LLM&#65292;&#20854;&#21442;&#25968;&#20174;7&#20159;&#21040;400&#20159;&#65292;&#21487;&#22312;&#23436;&#20840;&#33258;&#30001;&#30340;Apache 2.0&#35768;&#21487;&#19979;&#21830;&#29992;&#12290;&#25105;&#20204;&#30340;&#21457;&#24067;&#21253;&#25324;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;100&#65285;&#31169;&#26377;&#25991;&#26723;&#25628;&#32034;&#12290;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26377;&#21161;&#20110;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#24182;&#20351;&#20854;&#26356;&#21152;&#21487;&#38752;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation Large Language Models (LLMs) such as GPT-4 represent a revolution in AI due to their real-world applications though natural language processing. However, they also pose many significant risks such as the presence of biased, private, or harmful text, and the unauthorized inclusion of copyrighted material.  We introduce h2oGPT, a suite of open-source code repositories for the creation and use of Large Language Models (LLMs) based on Generative Pretrained Transformers (GPTs). The goal of this project is to create the world's best truly open-source alternative to closed-source GPTs. In collaboration with and as part of the incredible and unstoppable open-source community, we open-source several fine-tuned h2oGPT models from 7 to 40 Billion parameters, ready for commercial use under fully permissive Apache 2.0 licenses. Included in our release is 100% private document search using natural language.  Open-source language models help boost AI development and make it more accessible
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;</title><link>http://arxiv.org/abs/2306.07567</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#29983;&#25104;&#32431;&#36127;&#21453;&#39304;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sometimes Generate Purely Negatively-Reinforced Text. (arXiv:2306.07567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07567
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#26102;&#20250;&#20174;&#20165;&#21253;&#21547;&#36127;&#22870;&#21169;&#30340;&#20363;&#23376;&#20013;&#23398;&#20064;&#65292;&#23548;&#33268;&#29983;&#25104;&#31867;&#20284;&#27844;&#28431;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#31561;&#25935;&#24863;&#20449;&#24687;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#23545;&#25239;&#24615;&#35757;&#32451;&#26102;&#65292;&#36890;&#24120;&#20250;&#35757;&#32451;&#21453;&#23545;&#26368;&#20005;&#37325;&#22833;&#36133;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#21487;&#33021;&#20250;&#24847;&#21619;&#30528;&#20351;&#29992;&#21253;&#21547;&#25935;&#24863;&#20449;&#24687;&#65288;&#20363;&#22914;&#27844;&#38706;&#30340;&#23494;&#30721;&#25110;&#23433;&#20840;&#28431;&#27934;&#65289;&#30340;&#26696;&#20363;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#21487;&#33021;&#20250;&#35748;&#20026;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#27704;&#36828;&#19981;&#20250;&#29983;&#25104;&#20165;&#22312;&#19982;&#26368;&#20302;&#22870;&#21169;&#30456;&#20851;&#32852;&#30340;&#31034;&#20363;&#20013;&#20986;&#29616;&#30340;&#25991;&#26412;&#29255;&#27573;&#12290;&#26412;&#25991;&#34920;&#26126;&#36825;&#31181;&#20551;&#35774;&#26159;&#38169;&#35823;&#30340;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30830;&#23454;&#20174;&#36825;&#31181;&#32431;&#36127;&#21453;&#39304;&#30340;&#31034;&#20363;&#20013;&#23398;&#20064;&#21040;&#20102;&#19996;&#35199;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#23450;&#30340;&#35757;&#32451;&#35774;&#32622;&#65292;&#20351;&#24471;Pythia-160M&#33021;&#22815;&#29983;&#25104;&#23494;&#30721;&#30340;&#27010;&#29575;&#30053;&#39640;&#20110;&#38543;&#26426;&#65292;&#23613;&#31649;&#20165;&#22312;&#23545;&#27169;&#22411;&#19981;&#36755;&#20986;&#36825;&#20123;&#23494;&#30721;&#30340;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#36825;&#20123;&#23494;&#30721;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/FabienRoger/Learning-From-Negative-Examples&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
When using adversarial training, it is common practice to train against the most egregious failures. However, this might imply using examples with sensitive information (such as leaked passwords or security vulnerabilities) as training data. One might assume that language models trained with gradient descent never generate text snippets which were only present in examples associated with the lowest possible reward. In this paper, we show that this assumption is wrong: in some situations, large language models do learn from such negatively-reinforced examples. We present a specific training setup that enables Pythia-160M to generate passwords with a probability slightly greater than chance, despite only showing it these passwords on examples where the model is incentivized to not output these passwords. Our code is available at https://github.com/FabienRoger/Learning-From-Negative-Examples
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17680</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3&#29983;&#25104;&#30340;&#20167;&#24680;&#20869;&#23481;&#23457;&#26680;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;Fine-tune&#25110;&#25552;&#31034;&#29983;&#25104;&#20167;&#24680;&#35328;&#35770;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#29983;&#25104;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#38480;&#21046;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#20102;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#26631;&#35760;&#20869;&#23481;&#26412;&#36136;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#26816;&#26597;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;GPT-3&#19978;&#36755;&#20837;&#20167;&#24680;&#21644;&#38750;&#20167;&#24680;&#20869;&#23481;&#65292;&#21457;&#29616;&#21463;&#35843;&#26597;&#32773;&#22312;&#20154;&#24037;&#23457;&#26680;GPT&#29983;&#25104;&#30340;&#35299;&#37322;&#26102;&#65292;&#23558;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#35780;&#20215;&#20026;&#19981;&#22815;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#20174;&#38750;&#35773;&#21050;&#24615;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#24102;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#35773;&#21050;&#24615;&#21477;&#23376;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2305.04105</link><description>&lt;p&gt;
"&#24403;&#25991;&#23383;&#38590;&#20197;&#34920;&#36798;&#26102;&#65292;&#34920;&#24773;&#31526;&#21495;&#31216;&#38712;": &#21033;&#29992;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#29983;&#25104;&#24102;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#35773;&#21050;&#35805;&#35821;
&lt;/p&gt;
&lt;p&gt;
"When Words Fail, Emojis Prevail": Generating Sarcastic Utterances with Emoji Using Valence Reversal and Semantic Incongruity. (arXiv:2305.04105v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#20174;&#38750;&#35773;&#21050;&#24615;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#24102;&#26377;&#34920;&#24773;&#31526;&#21495;&#30340;&#35773;&#21050;&#24615;&#21477;&#23376;&#30340;&#26550;&#26500;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#20010;&#20307;&#29992;&#26469;&#34920;&#36798;&#19982;&#21547;&#20041;&#30456;&#21453;&#30340;&#20107;&#24773;&#30340;&#24494;&#22937;&#24418;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#20174;&#38750;&#35773;&#21050;&#24615;&#36755;&#20837;&#21477;&#23376;&#29983;&#25104;&#35773;&#21050;&#24615;&#21477;&#23376;&#30340;&#26032;&#39062;&#26550;&#26500;&#12290;&#25105;&#20204;&#23558;&#29983;&#25104;&#20219;&#21153;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#35773;&#21050;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#25910;&#38598;&#19982;&#36825;&#20123;&#35773;&#21050;&#21477;&#23376;&#30456;&#20851;&#30340;&#34920;&#24773;&#31526;&#21495;&#12290;&#35773;&#21050;&#30340;&#20004;&#20010;&#20851;&#38190;&#20803;&#32032;&#34987;&#32435;&#20837;&#21040;&#25991;&#26412;&#35773;&#21050;&#29983;&#25104;&#20219;&#21153;&#20013;&#65306;&#24773;&#24863;&#21453;&#36716;&#21644;&#35821;&#20041;&#19981;&#19968;&#33268;&#24615;&#19982;&#35821;&#22659;&#65292;&#20854;&#20013;&#35821;&#22659;&#21487;&#33021;&#28041;&#21450;&#28436;&#35762;&#32773;&#21644;&#21548;&#20247;&#20043;&#38388;&#20849;&#20139;&#30340;&#24120;&#35782;&#25110;&#19968;&#33324;&#30693;&#35782;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35773;&#21050;&#29983;&#25104;&#20316;&#21697;&#37117;&#38598;&#20013;&#22312;&#36825;&#31181;&#25991;&#26412;&#24418;&#24335;&#19978;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#24403;&#20070;&#38754;&#25991;&#26412;&#26080;&#27861;&#26377;&#25928;&#22320;&#25429;&#25417;&#21475;&#22836;&#21644;&#38754;&#23545;&#38754;&#20132;&#27969;&#30340;&#24773;&#24863;&#32447;&#32034;&#26102;&#65292;&#20154;&#20204;&#32463;&#24120;&#36873;&#25321;&#34920;&#24773;&#31526;&#21495;&#26469;&#20934;&#30830;&#22320;&#34920;&#36798;&#20182;&#20204;&#30340;&#24773;&#24863;&#12290;&#30001;&#20110;&#34920;&#24773;&#31526;&#21495;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#21512;&#29702;&#22320;&#32435;&#20837;&#36866;&#24403;&#30340;&#34920;&#24773;&#31526;&#21495;&#24456;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm pertains to the subtle form of language that individuals use to express the opposite of what is implied. We present a novel architecture for sarcasm generation with emoji from a non-sarcastic input sentence. We divide the generation task into two sub tasks: one for generating textual sarcasm and another for collecting emojis associated with those sarcastic sentences. Two key elements of sarcasm are incorporated into the textual sarcasm generation task: valence reversal and semantic incongruity with context, where the context may involve shared commonsense or general knowledge between the speaker and their audience. The majority of existing sarcasm generation works have focused on this textual form. However, in the real world, when written texts fall short of effectively capturing the emotional cues of spoken and face-to-face communication, people often opt for emojis to accurately express their emotions. Due to the wide range of applications of emojis, incorporating appropriate
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#25152;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#21487;&#20197;&#37327;&#21270;&#22320;&#26681;&#25454;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26435;&#34913;&#27599;&#20010;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#24378;&#22823;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.04187</link><description>&lt;p&gt;
&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Similarity-Aware Multimodal Prompt Learning for Fake News Detection. (arXiv:2304.04187v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#25928;&#34701;&#21512;&#19981;&#21516;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20551;&#26032;&#38395;&#26816;&#27979;&#12290;&#25152;&#35774;&#35745;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#21487;&#20197;&#37327;&#21270;&#22320;&#26681;&#25454;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26435;&#34913;&#27599;&#20010;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#22122;&#22768;&#24178;&#25200;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#20063;&#35777;&#26126;&#25552;&#20986;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#20854;&#20182;&#24378;&#22823;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#26631;&#20934;&#33539;&#24335;&#20027;&#35201;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#26469;&#24314;&#31435;&#26032;&#38395;&#30340;&#30495;&#23454;&#24615;&#65292;&#28982;&#32780;&#65292;&#32593;&#19978;&#20551;&#26032;&#38395;&#30340;&#35805;&#35821;&#36890;&#24120;&#27604;&#36739;&#24494;&#22937;&#65292;&#38656;&#35201;&#19987;&#23478;&#30693;&#35782;&#25165;&#33021;&#20351;&#29992;&#25991;&#26412;&#20449;&#24687;&#25581;&#38706;&#20551;&#26032;&#38395;&#12290;&#26368;&#36817;&#65292;&#20851;&#27880;&#20110;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#30740;&#31350;&#24050;&#32463;&#36229;&#36234;&#20102;&#20165;&#22522;&#20110;&#25991;&#26412;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25552;&#21462;&#21333;&#27169;&#24577;&#29305;&#24449;&#65292;&#25110;&#30452;&#25509;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36825;&#25104;&#20026;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#26032;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#23454;&#20363;&#65292;&#35201;&#20040;&#38656;&#35201;&#26356;&#26032;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#38598;&#65292;&#19981;&#23454;&#38469;&#21487;&#34892;&#12290;&#27492;&#22806;&#65292;&#20256;&#32479;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#23558;&#36328;&#27169;&#24577;&#29305;&#24449;&#30452;&#25509;&#34701;&#21512;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30456;&#20851;&#30340;&#35821;&#20041;&#34920;&#31034;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#22810;&#27169;&#24577;&#29305;&#24449;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20284;&#24230;&#24863;&#30693;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#23398;&#20064;&#65288;SAMPLE&#65289;&#26694;&#26550;&#65292;&#29992;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#22320;&#34701;&#21512;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#30456;&#20284;&#24230;&#24863;&#30693;&#34701;&#21512;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#23398;&#20064;&#26681;&#25454;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#26469;&#26435;&#34913;&#19981;&#21516;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22823;&#24133;&#20248;&#20110;&#24378;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The standard paradigm for fake news detection mainly utilizes text information to model the truthfulness of news. However, the discourse of online fake news is typically subtle and it requires expert knowledge to use textual information to debunk fake news. Recently, studies focusing on multimodal fake news detection have outperformed text-only methods. Recent approaches utilizing the pre-trained model to extract unimodal features, or fine-tuning the pre-trained model directly, have become a new paradigm for detecting fake news. Again, this paradigm either requires a large number of training instances, or updates the entire set of pre-trained model parameters, making real-world fake news detection impractical. Furthermore, traditional multimodal methods fuse the cross-modal features directly without considering that the uncorrelated semantic representation might inject noise into the multimodal features. This paper proposes a Similarity-Aware Multimodal Prompt Learning (SAMPLE) framewo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2304.03394</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#35838;&#31243;&#35780;&#35770;&#30340;&#35266;&#28857;&#25366;&#25496;&#21644;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Opinion Mining and Topic Classification of Course Reviews. (arXiv:2304.03394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#27604;&#36739;&#20256;&#32479;&#26041;&#27861;&#21644;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22788;&#29702;&#22823;&#37327;&#35838;&#31243;&#35780;&#35770;&#65292;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25945;&#32946;&#24037;&#20316;&#32773;&#21644;&#31649;&#29702;&#32773;&#26469;&#35828;&#65292;&#23398;&#29983;&#23545;&#35838;&#31243;&#30340;&#21453;&#39304;&#24847;&#35265;&#38750;&#24120;&#37325;&#35201;&#65292;&#26080;&#35770;&#35838;&#31243;&#30340;&#31867;&#22411;&#25110;&#26426;&#26500;&#22914;&#20309;&#12290;&#22312;&#26426;&#26500;&#32423;&#21035;&#25110;&#22312;&#32447;&#35770;&#22363;&#19978;&#22788;&#29702;&#22823;&#37327;&#30340;&#24320;&#25918;&#21453;&#39304;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#20102;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#35838;&#31243;&#35780;&#35770;&#12290;&#25105;&#20204;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#30446;&#30340;&#26159;&#20102;&#35299;&#23398;&#29983;&#30340;&#24773;&#24863;&#21644;&#20027;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#22914;&#35789;&#23884;&#20837;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;BERT&#65288;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#65289;&#12289;RoBERTa&#65288;&#32463;&#36807;&#20248;&#21270;&#30340;BERT&#26041;&#27861;&#65289;&#21644;XLNet&#65288;&#24191;&#20041;&#33258;&#22238;&#24402;&#39044;&#35757;&#32451;&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#36825;&#20123;&#25216;&#26415;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24046;&#24322;&#12290;&#36825;&#39033;&#27604;&#36739;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#24773;&#24863;&#26497;&#24615;&#20998;&#26512;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polari
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.03363</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#31867;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#22686;&#24378;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language. (arXiv:2303.03363v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#24615;&#21644;&#24615;&#36136;&#39044;&#27979;&#27169;&#22411;&#26159;&#33647;&#29289;&#30740;&#21457;&#21644;&#26448;&#26009;&#31185;&#23398;&#20013;&#30340;&#26680;&#24515;&#24037;&#20316;&#65292;&#20294;&#30446;&#21069;&#23427;&#20204;&#24517;&#39035;&#32463;&#36807;&#35757;&#32451;&#25110;&#24494;&#35843;&#25165;&#33021;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#31185;&#23398;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#38646;&#25968;&#25454;&#21644;&#23569;&#25968;&#25454;&#26679;&#26412;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#27492;&#31867;&#20302;&#25968;&#25454;&#20219;&#21153;&#65292;&#26080;&#38656;&#35757;&#32451;&#25110;&#24494;&#35843;&#21363;&#21487;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#27963;&#24615;&#39044;&#27979;&#26041;&#38754;&#30340;&#39044;&#27979;&#36136;&#37327;&#19981;&#36275;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#27963;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#36890;&#36807;&#29702;&#35299;&#25551;&#36848;&#20219;&#21153;&#30340;&#25991;&#26412;&#20449;&#24687;&#26469;&#36866;&#24212;&#25512;&#29702;&#26102;&#30340;&#26032;&#39044;&#27979;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#21270;&#23398;&#21644;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#30340;&#20998;&#31163;&#27169;&#22359;&#65292;&#20197;&#21450;&#22823;&#22411;&#29983;&#29289;&#21270;&#23398;&#25968;&#25454;&#24211;&#20013;&#30340;&#23545;&#27604;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;CLAMP&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#21644;&#33647;&#29289;&#30740;&#21457;&#20013;&#30340;&#38646;&#25968;&#25454;&#38382;&#39064;&#19978;&#37117;&#33021;&#21462;&#24471;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#36827;&#23637;&#24402;&#22240;&#20110;&#24773;&#22659;&#24863;&#30693;&#27169;&#22411;&#21644;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#30340;&#32467;&#21512;&#65292;&#20197;&#21450;&#29992;&#20110;&#32467;&#21512;&#25968;&#25454;&#28304;&#30340;&#23545;&#25239;&#24615;&#33258;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Activity and property prediction models are the central workhorses in drug discovery and materials sciences, but currently they have to be trained or fine-tuned for new tasks. Without training or fine-tuning, scientific language models could be used for such low-data tasks through their announced zero- and few-shot capabilities. However, their predictive quality at activity prediction is lacking. In this work, we envision a novel type of activity prediction model that is able to adapt to new prediction tasks at inference time, via understanding textual information describing the task. To this end, we propose a new architecture with separate modules for chemical and natural language inputs, and a contrastive pre-training objective on data from large biochemical databases. In extensive experiments, we show that our method CLAMP yields improved predictive performance on few-shot learning benchmarks and zero-shot problems in drug discovery. We attribute the advances of our method to the mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30340;&#23545;&#25239;&#25915;&#20987;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31639;&#27861; TransFool&#12290;&#36890;&#36807;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#24449;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19982;&#21407;&#22987;&#26679;&#26412;&#26377;&#30528;&#39640;&#24230;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20934;&#30830;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#32763;&#35793;&#20219;&#21153;&#21644; NMT &#26550;&#26500;&#20043;&#19979;&#65292;TransFool &#21487;&#20197;&#36716;&#31227;&#21040;&#26410;&#30693;&#30340;&#30446;&#26631;&#27169;&#22411;&#65292;&#23548;&#33268;&#20005;&#37325;&#38477;&#20302;&#35793;&#25991;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.00944</link><description>&lt;p&gt;
TransFool: &#38754;&#21521;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
TransFool: An Adversarial Attack against Neural Machine Translation Models. (arXiv:2302.00944v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#30340;&#23545;&#25239;&#25915;&#20987;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31639;&#27861; TransFool&#12290;&#36890;&#36807;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#24449;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19982;&#21407;&#22987;&#26679;&#26412;&#26377;&#30528;&#39640;&#24230;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20934;&#30830;&#23545;&#25239;&#24615;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#19981;&#21516;&#32763;&#35793;&#20219;&#21153;&#21644; NMT &#26550;&#26500;&#20043;&#19979;&#65292;TransFool &#21487;&#20197;&#36716;&#31227;&#21040;&#26410;&#30693;&#30340;&#30446;&#26631;&#27169;&#22411;&#65292;&#23548;&#33268;&#20005;&#37325;&#38477;&#20302;&#35793;&#25991;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#20250;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#25200;&#21160;&#20135;&#29983;&#26131;&#21463;&#25915;&#20987;&#30340;&#28431;&#27934;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;(NMT)&#27169;&#22411;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25915;&#20987;&#31639;&#27861;&#31216;&#20026;TransFool&#12290;&#20026;&#20102;&#27450;&#39575; NMT &#27169;&#22411;&#65292;TransFool &#24314;&#31435;&#22312;&#19968;&#20010;&#22810;&#39033;&#24335;&#20248;&#21270;&#38382;&#39064;&#21644;&#26799;&#24230;&#25237;&#24433;&#27493;&#39588;&#20043;&#19978;&#12290;&#36890;&#36807;&#38598;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#34920;&#24449;&#65292;&#25105;&#20204;&#22312;&#28304;&#35821;&#35328;&#20013;&#29983;&#25104;&#27969;&#30021;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65292;&#20854;&#19982;&#21407;&#22987;&#26679;&#26412;&#26377;&#30528;&#39640;&#24230;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#19981;&#21516;&#30340;&#32763;&#35793;&#20219;&#21153;&#21644; NMT &#26550;&#26500;&#65292;&#25105;&#20204;&#30340;&#30333;&#31665;&#25915;&#20987;&#21487;&#20197;&#20005;&#37325;&#21066;&#24369;&#32763;&#35793;&#36136;&#37327;&#65292;&#32780;&#21407;&#26412;&#19982;&#23545;&#25239;&#24615;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#20173;&#26087;&#24456;&#39640;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; TransFool &#21487;&#20197;&#36716;&#31227;&#33267;&#26410;&#30693;&#30340;&#30446;&#26631;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#22312;&#33258;&#21160;&#21270;&#21644;&#20154;&#20026;&#35780;&#20272;&#30340;&#22522;&#30784;&#19978;&#65292;TransFool &#23548;&#33268;&#20102;&#37325;&#35201;&#30340;&#35793;&#25991;&#36136;&#37327;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been shown to be vulnerable to small perturbations of their inputs, known as adversarial attacks. In this paper, we investigate the vulnerability of Neural Machine Translation (NMT) models to adversarial attacks and propose a new attack algorithm called TransFool. To fool NMT models, TransFool builds on a multi-term optimization problem and a gradient projection step. By integrating the embedding representation of a language model, we generate fluent adversarial examples in the source language that maintain a high level of semantic similarity with the clean samples. Experimental results demonstrate that, for different translation tasks and NMT architectures, our white-box attack can severely degrade the translation quality while the semantic similarity between the original and the adversarial sentences stays high. Moreover, we show that TransFool is transferable to unknown target models. Finally, based on automatic and human evaluations, TransFool leads to imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;SLU&#22522;&#20934;&#35780;&#27979;&#20219;&#21153;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#38382;&#31572;&#21644;&#25688;&#35201;&#12289;&#21629;&#21517;&#23454;&#20307;&#26412;&#22320;&#21270;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#22522;&#20110;&#33258;&#30001;&#33719;&#21462;&#30340;&#35821;&#38899;&#25968;&#25454;&#35774;&#35745;&#65292;&#24076;&#26395;&#21487;&#20197;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;SLU&#25216;&#26415;&#65292;&#20174;&#32780;&#25512;&#21160;&#21475;&#35821;&#29702;&#35299;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2212.10525</link><description>&lt;p&gt;
SLUE Phase-2&#65306;&#22810;&#26679;&#21270;&#21475;&#35821;&#29702;&#35299;&#20219;&#21153;&#22522;&#20934;&#35780;&#27979;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
SLUE Phase-2: A Benchmark Suite of Diverse Spoken Language Understanding Tasks. (arXiv:2212.10525v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;SLU&#22522;&#20934;&#35780;&#27979;&#20219;&#21153;&#22871;&#20214;&#65292;&#20854;&#20013;&#21253;&#25324;&#38382;&#31572;&#21644;&#25688;&#35201;&#12289;&#21629;&#21517;&#23454;&#20307;&#26412;&#22320;&#21270;&#21644;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#22522;&#20110;&#33258;&#30001;&#33719;&#21462;&#30340;&#35821;&#38899;&#25968;&#25454;&#35774;&#35745;&#65292;&#24076;&#26395;&#21487;&#20197;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;SLU&#25216;&#26415;&#65292;&#20174;&#32780;&#25512;&#21160;&#21475;&#35821;&#29702;&#35299;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#20219;&#21153;&#22312;&#35821;&#38899;&#30740;&#31350;&#39046;&#22495;&#24050;&#32463;&#30740;&#31350;&#20102;&#35768;&#22810;&#21313;&#24180;&#65292;&#20294;&#26159;&#27809;&#26377;&#20687;&#35821;&#38899;&#21644;&#35828;&#35805;&#20154;&#35782;&#21035;&#31561;&#20302;&#23618;&#20219;&#21153;&#19968;&#26679;&#21463;&#21040;&#24456;&#22810;&#20851;&#27880;&#12290;&#29305;&#21035;&#26159;&#65292;&#27809;&#26377;&#22810;&#23569;SLU&#20219;&#21153;&#22522;&#20934;&#35780;&#27979;&#65292;&#24182;&#19988;&#35768;&#22810;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#27979;&#20351;&#29992;&#25968;&#25454;&#24182;&#19981;&#26159;&#25152;&#26377;&#30740;&#31350;&#20154;&#21592;&#37117;&#21487;&#20197;&#33258;&#30001;&#33719;&#21462;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#24037;&#20316;&#24320;&#22987;&#20026;&#20960;&#20010;&#20219;&#21153;&#24341;&#20837;&#36825;&#26679;&#30340;&#22522;&#20934;&#35780;&#27979;&#25968;&#25454;&#38598;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#33258;&#30001;&#33719;&#21462;&#30340;&#35821;&#38899;&#25968;&#25454;&#24341;&#20837;&#20102;&#20960;&#20010;&#26032;&#30340;&#27880;&#37322;SLU&#22522;&#20934;&#35780;&#27979;&#20219;&#21153;&#65292;&#36825;&#20123;&#20219;&#21153;&#34917;&#20805;&#20102;&#29616;&#26377;&#30340;&#22522;&#20934;&#35780;&#27979;&#24182;&#22635;&#34917;&#20102;SLU&#35780;&#20272;&#39046;&#22495;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#22235;&#20010;&#20219;&#21153;&#65306;&#38382;&#31572;&#21644;&#25688;&#35201;&#28041;&#21450;&#26356;&#38271;&#30340;&#35821;&#38899;&#24207;&#21015;&#30340;&#25512;&#29702;&#65307;&#21629;&#21517;&#23454;&#20307;&#26412;&#22320;&#21270;&#22788;&#29702;&#23450;&#20301;&#20449;&#21495;&#20013;&#30340;&#30446;&#26631;&#20869;&#23481;&#36825;&#19968;&#35821;&#38899;&#29305;&#23450;&#20219;&#21153;&#65307;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30830;&#23450;&#32473;&#23450;&#35821;&#38899;&#35805;&#35821;&#30340;&#21151;&#33021;&#12290;&#25105;&#20204;&#36981;&#24490;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#65288;SLUE&#65289;&#31532;&#19968;&#38454;&#27573;&#30340;&#34013;&#22270;&#65292;&#35774;&#35745;&#36328;&#36234;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#21644;&#20351;&#29992;&#19981;&#21516;&#35821;&#35328;&#25216;&#26415;&#30340;&#22810;&#26679;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;SLUE Phase-2&#22522;&#20934;&#35780;&#27979;&#21487;&#20197;&#40723;&#21169;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;SLU&#25216;&#26415;&#65292;&#20174;&#32780;&#25512;&#21160;&#21475;&#35821;&#29702;&#35299;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken language understanding (SLU) tasks have been studied for many decades in the speech research community, but have not received as much attention as lower-level tasks like speech and speaker recognition. In particular, there are not nearly as many SLU task benchmarks, and many of the existing ones use data that is not freely available to all researchers. Recent work has begun to introduce such benchmark datasets for several tasks. In this work, we introduce several new annotated SLU benchmark tasks based on freely available speech data, which complement existing benchmarks and address gaps in the SLU evaluation landscape. We contribute four tasks: question answering and summarization involve inference over longer speech sequences; named entity localization addresses the speech-specific task of locating the targeted content in the signal; dialog act classification identifies the function of a given speech utterance. We follow the blueprint of the Spoken Language Understanding Evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pix2Struct&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#32431;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#21487;&#20197;&#22312;&#21253;&#21547;&#35270;&#35273;&#35821;&#22659;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;Pix2Struct&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26159;&#23398;&#20064;&#23558;&#23631;&#24149;&#25130;&#22270;&#35299;&#26512;&#25104;&#31616;&#21270;&#30340;HTML&#65292;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26469;&#28304;&#22797;&#26434;&#12289;&#22810;&#26679;&#24615;&#22823;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2210.03347</link><description>&lt;p&gt;
Pix2Struct&#65306;&#23631;&#24149;&#25130;&#22270;&#35299;&#26512;&#20316;&#20026;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding. (arXiv:2210.03347v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03347
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pix2Struct&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#32431;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#21487;&#20197;&#22312;&#21253;&#21547;&#35270;&#35273;&#35821;&#22659;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;Pix2Struct&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26159;&#23398;&#20064;&#23558;&#23631;&#24149;&#25130;&#22270;&#35299;&#26512;&#25104;&#31616;&#21270;&#30340;HTML&#65292;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#26469;&#28304;&#22797;&#26434;&#12289;&#22810;&#26679;&#24615;&#22823;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#22659;&#30340;&#35821;&#35328;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#65292;&#23427;&#28304;&#20110;&#20174;&#26377;&#22270;&#31034;&#30340;&#25945;&#31185;&#20070;&#21040;&#26377;&#22270;&#20687;&#21644;&#34920;&#26684;&#30340;&#32593;&#39029;&#20877;&#21040;&#26377;&#25353;&#38062;&#21644;&#34920;&#21333;&#30340;&#31227;&#21160;&#24212;&#29992;&#12290;&#20026;&#27492;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;&#25216;&#26415;&#65292;&#19988;&#24456;&#23569;&#20849;&#20139;&#22522;&#30784;&#25968;&#25454;&#12289;&#27169;&#22411;&#26550;&#26500;&#21644;&#30446;&#26631;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Pix2Struct&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#21040;&#25991;&#26412;&#27169;&#22411;&#65292;&#29992;&#20110;&#32431;&#35270;&#35273;&#35821;&#35328;&#29702;&#35299;&#65292;&#21487;&#20197;&#22312;&#21253;&#21547;&#35270;&#35273;&#35821;&#22659;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;Pix2Struct&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#26159;&#23398;&#20064;&#23558;&#23631;&#24149;&#25130;&#22270;&#35299;&#26512;&#25104;&#31616;&#21270;&#30340;HTML&#12290;Web&#20855;&#26377;&#20016;&#23500;&#30340;&#35270;&#35273;&#20803;&#32032;&#65292;&#36825;&#20123;&#20803;&#32032;&#22312;HTML&#32467;&#26500;&#20013;&#28165;&#26224;&#22320;&#21453;&#26144;&#20986;&#26469;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#38750;&#24120;&#36866;&#29992;&#20110;&#22810;&#26679;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#30452;&#35266;&#19978;&#65292;&#36825;&#20010;&#30446;&#26631;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#39044;&#35757;&#32451;&#20449;&#21495;&#65292;&#20363;&#22914;OCR&#12289;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#25551;&#36848;&#12290;&#38500;&#20102;&#26032;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#21464;&#20998;&#36776;&#29575;&#30340;&#36755;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visually-situated language is ubiquitous -- sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#38024;&#23545;&#22312;&#32447;&#35777;&#25454;&#30340;&#20266;&#35013;&#21644;&#34394;&#20551;&#20449;&#24687;&#31561;&#20004;&#31181;&#25915;&#20987;&#30446;&#26631;&#21644;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#32500;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#35777;&#25454;&#20013;&#19982;&#20107;&#23454;&#30456;&#20851;&#30340;&#37096;&#20998;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#29983;&#25104;&#26080;&#27861;&#34987;&#33258;&#21160;&#35782;&#21035;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2209.03755</link><description>&lt;p&gt;
&#20107;&#23454;&#30772;&#22351;&#32773;&#65306;&#38024;&#23545;&#20107;&#23454;&#39564;&#35777;&#31995;&#32479;&#30340;&#35777;&#25454;&#25805;&#32437;&#25915;&#20987;&#20998;&#31867;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fact-Saboteurs: A Taxonomy of Evidence Manipulation Attacks against Fact-Verification Systems. (arXiv:2209.03755v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#38024;&#23545;&#22312;&#32447;&#35777;&#25454;&#30340;&#20266;&#35013;&#21644;&#34394;&#20551;&#20449;&#24687;&#31561;&#20004;&#31181;&#25915;&#20987;&#30446;&#26631;&#21644;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#32500;&#24230;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#35777;&#25454;&#20013;&#19982;&#20107;&#23454;&#30456;&#20851;&#30340;&#37096;&#20998;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#29983;&#25104;&#26080;&#27861;&#34987;&#33258;&#21160;&#35782;&#21035;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#23548;&#21644;&#34394;&#20551;&#20449;&#24687;&#23545;&#25105;&#20204;&#30340;&#23433;&#20840;&#21644;&#31283;&#23450;&#26500;&#25104;&#20102;&#20005;&#37325;&#30340;&#20840;&#29699;&#23041;&#32961;&#12290;&#20026;&#20102;&#24212;&#23545;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#35268;&#27169;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#22312;&#21162;&#21147;&#36890;&#36807;&#26816;&#32034;&#21644;&#39564;&#35777;&#30456;&#20851;&#35777;&#25454;&#26469;&#33258;&#21160;&#36827;&#34892;&#20107;&#23454;&#26680;&#26597;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#35768;&#22810;&#36827;&#23637;&#65292;&#20294;&#23545;&#27492;&#31867;&#31995;&#32479;&#21487;&#33021;&#38754;&#20020;&#30340;&#25915;&#20987;&#21521;&#37327;&#30340;&#20840;&#38754;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#20551;&#35774;&#23384;&#22312;&#19968;&#20010;&#25932;&#23545;&#26041;&#65292;&#36890;&#36807;&#20266;&#35013;&#30456;&#20851;&#35777;&#25454;&#25110;&#32773;&#25552;&#20379;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#65292;&#33258;&#21160;&#24178;&#25200;&#22312;&#32447;&#35777;&#25454;&#65292;&#20197;&#30772;&#22351;&#20107;&#23454;&#39564;&#35777;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#20998;&#31867;&#27861;&#65292;&#28085;&#30422;&#36825;&#20004;&#20010;&#30446;&#26631;&#21644;&#19981;&#21516;&#30340;&#23041;&#32961;&#27169;&#22411;&#32500;&#24230;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#22810;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#35777;&#25454;&#20013;&#19982;&#20107;&#23454;&#30456;&#20851;&#30340;&#37096;&#20998;&#36827;&#34892;&#24494;&#23567;&#30340;&#20462;&#25913;&#65292;&#24182;&#29983;&#25104;&#19982;&#21407;&#22987;&#35777;&#25454;&#33073;&#33410;&#30340;&#34394;&#20551;&#20449;&#24687;&#65292;&#20174;&#32780;&#30772;&#22351;&#20107;&#23454;&#39564;&#35777;&#32467;&#26524;&#65292;&#20351;&#20854;&#26080;&#27861;&#34987;&#33258;&#21160;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mis- and disinformation are a substantial global threat to our security and safety. To cope with the scale of online misinformation, researchers have been working on automating fact-checking by retrieving and verifying against relevant evidence. However, despite many advances, a comprehensive evaluation of the possible attack vectors against such systems is still lacking. Particularly, the automated fact-verification process might be vulnerable to the exact disinformation campaigns it is trying to combat. In this work, we assume an adversary that automatically tampers with the online evidence in order to disrupt the fact-checking model via camouflaging the relevant evidence or planting a misleading one. We first propose an exploratory taxonomy that spans these two targets and the different threat model dimensions. Guided by this, we design and propose several potential attack methods. We show that it is possible to subtly modify claim-salient snippets in the evidence and generate diver
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2206.04039</link><description>&lt;p&gt;
&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20247;&#21253;&#24037;&#20316;&#32773;&#30340;&#20154;&#31867;&#21463;&#35797;&#32773;&#36523;&#20221;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Resolving the Human Subjects Status of Machine Learning's Crowdworkers. (arXiv:2206.04039v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04039
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#20247;&#21253;&#24037;&#20316;&#32773;&#38382;&#39064;&#24341;&#36215;&#20102;&#23545;&#20854;&#21463;&#35797;&#32773;&#36523;&#20221;&#30340;&#20105;&#35758;&#19982;&#30417;&#31649;&#21512;&#35268;&#24615;&#65292;&#26412;&#25991;&#38024;&#23545;&#35813;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#30740;&#31350;&#30417;&#31649;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;(Machine Learning, ML)&#22312;&#26500;&#24314;&#25968;&#25454;&#38598;&#21644;&#35299;&#20915;&#38656;&#35201;&#20154;&#31867;&#20132;&#20114;&#25110;&#21028;&#26029;&#30340;&#30740;&#31350;&#38382;&#39064;&#26041;&#38754;&#65292;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;&#20247;&#21253;&#24037;&#20316;&#32773;&#12290;&#30001;&#20110;&#25191;&#34892;&#30340;&#20219;&#21153;&#22810;&#26679;&#21270;&#21644;&#25968;&#25454;&#29992;&#36884;&#30340;&#22810;&#26679;&#24615;&#65292;&#24456;&#38590;&#30830;&#23450;&#20309;&#26102;&#23558;&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#24037;&#20154;(&#32780;&#38750;&#20154;&#31867;&#21463;&#35797;&#32773;)&#12290;&#36825;&#20123;&#22256;&#38590;&#21152;&#21095;&#20102;&#25919;&#31574;&#30340;&#20914;&#31361;&#65292;&#19968;&#20123;&#26426;&#26500;&#21644;&#30740;&#31350;&#20154;&#21592;&#23558;&#25152;&#26377;ML&#20247;&#21253;&#24037;&#20316;&#32773;&#35270;&#20026;&#20154;&#31867;&#21463;&#35797;&#32773;&#65292;&#32780;&#20854;&#20182;&#20154;&#21017;&#35748;&#20026;&#23427;&#20204;&#24456;&#23569;&#26500;&#25104;&#20154;&#31867;&#21463;&#35797;&#32773;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21253;&#25324;&#20247;&#21253;&#24037;&#20316;&#30340;&#40092;&#26377;ML&#35770;&#25991;&#25552;&#21040;IRB&#30340;&#30417;&#30563;&#65292;&#24341;&#21457;&#20102;&#36829;&#21453;&#36947;&#24503;&#21644;&#27861;&#35268;&#35201;&#27714;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;ML&#20247;&#21253;&#30740;&#31350;&#30340;&#36866;&#24403;&#21010;&#23450;&#65292;&#24182;&#20851;&#27880;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26292;&#38706;&#20986;&#30340;&#29420;&#29305;&#30740;&#31350;&#30417;&#30563;&#25361;&#25112;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#32654;&#22269;&#20844;&#20849;&#35268;&#21017;&#19979;&#65292;&#36825;&#20123;&#21028;&#26029;&#21462;&#20915;&#20110;&#20851;&#20110;&#38382;&#39064;&#30340;&#30830;&#23450;&#65292;&#28041;&#21450;&#35841;(&#25110;&#20160;&#20040;)&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, machine learning (ML) has relied heavily on crowdworkers both for building datasets and for addressing research questions requiring human interaction or judgment. The diverse tasks performed and uses of the data produced render it difficult to determine when crowdworkers are best thought of as workers (versus human subjects). These difficulties are compounded by conflicting policies, with some institutions and researchers regarding all ML crowdworkers as human subjects and others holding that they rarely constitute human subjects. Notably few ML papers involving crowdwork mention IRB oversight, raising the prospect of non-compliance with ethical and regulatory requirements. We investigate the appropriate designation of ML crowdsourcing studies, focusing our inquiry on natural language processing to expose unique challenges for research oversight. Crucially, under the U.S. Common Rule, these judgments hinge on determinations of aboutness, concerning both whom (or what) 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20027;&#35201;&#38024;&#23545;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20013;&#23384;&#22312;&#20551;&#38452;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#37325;&#26032;&#27880;&#37322; DocRED &#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#34987;&#24573;&#30053;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#21518;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#24615;&#33021;&#25552;&#21319;&#32422; 13 F1 &#20998;&#25968;&#30340;&#26032;&#25968;&#25454;&#38598; Re-DocRED&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#25928;&#25913;&#36827;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2205.12696</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270; DocRED - &#35299;&#20915;&#20851;&#31995;&#25277;&#21462;&#20013;&#30340;&#20551;&#38452;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Revisiting DocRED -- Addressing the False Negative Problem in Relation Extraction. (arXiv:2205.12696v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12696
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20027;&#35201;&#38024;&#23545;&#25991;&#26723;&#23618;&#38754;&#19978;&#30340;&#20851;&#31995;&#25277;&#21462;&#20013;&#23384;&#22312;&#20551;&#38452;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#37325;&#26032;&#27880;&#37322; DocRED &#25968;&#25454;&#38598;&#20013;&#28155;&#21152;&#34987;&#24573;&#30053;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#21518;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#24615;&#33021;&#25552;&#21319;&#32422; 13 F1 &#20998;&#25968;&#30340;&#26032;&#25968;&#25454;&#38598; Re-DocRED&#65292;&#24182;&#21457;&#29616;&#20102;&#26377;&#25928;&#25913;&#36827;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DocRED &#25968;&#25454;&#38598;&#26159;&#26368;&#27969;&#34892;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#22522;&#20934;&#25968;&#25454;&#38598;&#20043;&#19968;&#12290;&#23427;&#37319;&#29992;&#20102;&#19968;&#20010;&#25512;&#33616;-&#20462;&#35746;&#30340;&#26631;&#27880;&#26041;&#26696;&#65292;&#20197;&#33719;&#24471;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616; DocRED &#30340;&#26631;&#27880;&#26159;&#19981;&#23436;&#25972;&#30340;&#65292;&#21363;&#20551;&#38452;&#24615;&#26679;&#26412;&#24456;&#26222;&#36941;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#21521;&#21407;&#22987; DocRED &#20013;&#28155;&#21152;&#34987;&#24573;&#30053;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#26469;&#37325;&#26032;&#27880;&#37322;&#20102; 4,053 &#20010;&#25991;&#26723;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#20462;&#27491;&#21518;&#30340; DocRED &#25968;&#25454;&#38598;&#21629;&#21517;&#20026; Re-DocRED&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#31070;&#32463;&#27169;&#22411;&#22312;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#24320;&#23637;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340; Re-DocRED &#35757;&#32451;&#21644;&#35780;&#20272;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#20102;&#22823;&#32422; 13 &#20010; F1 &#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;&#20102;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#28508;&#22312;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20844;&#24320;&#22312; https://github&#12290;
&lt;/p&gt;
&lt;p&gt;
The DocRED dataset is one of the most popular and widely used benchmarks for document-level relation extraction (RE). It adopts a recommend-revise annotation scheme so as to have a large-scale annotated dataset. However, we find that the annotation of DocRED is incomplete, i.e., false negative samples are prevalent. We analyze the causes and effects of the overwhelming false negative problem in the DocRED dataset. To address the shortcoming, we re-annotate 4,053 documents in the DocRED dataset by adding the missed relation triples back to the original DocRED. We name our revised DocRED dataset Re-DocRED. We conduct extensive experiments with state-of-the-art neural models on both datasets, and the experimental results show that the models trained and evaluated on our Re-DocRED achieve performance improvements of around 13 F1 points. Moreover, we conduct a comprehensive analysis to identify the potential areas for further improvement. Our dataset is publicly available at https://github.
&lt;/p&gt;</description></item></channel></rss>