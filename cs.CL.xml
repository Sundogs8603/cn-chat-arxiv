<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;</title><link>https://arxiv.org/abs/2403.15371</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#19978;&#19979;&#25991;&#20013;&#30340;&#25506;&#32034;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can large language models explore in-context?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15371
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24456;&#38590;&#26377;&#25928;&#36827;&#34892;&#25506;&#32034;&#65292;&#38500;&#20102;&#29305;&#23450;&#37197;&#32622;&#19979;&#30340;GPT-4&#20855;&#26377;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#34920;&#29616;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36827;&#34892;&#25506;&#32034;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#24378;&#21270;&#23398;&#20064;&#21644;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#25105;&#20204;&#20851;&#27880;&#29616;&#26377;LLMs&#30340;&#21407;&#29983;&#24615;&#33021;&#65292;&#27809;&#26377;&#36827;&#34892;&#35757;&#32451;&#24178;&#39044;&#12290;&#25105;&#20204;&#23558;LLMs&#37096;&#32626;&#20026;&#31616;&#21333;&#22810;&#33218;&#32769;&#34382;&#26426;&#29615;&#22659;&#20013;&#30340;&#20195;&#29702;&#65292;&#24182;&#23436;&#20840;&#22312;&#19978;&#19979;&#25991;&#20013;&#25351;&#23450;&#29615;&#22659;&#25551;&#36848;&#21644;&#20132;&#20114;&#21382;&#21490;&#65292;&#21363;&#22312;LLM&#25552;&#31034;&#20869;&#37096;&#36827;&#34892;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#35745;&#23545;GPT-3.5&#12289;GPT-4&#21644;Llama2&#36827;&#34892;&#23454;&#39564;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#27809;&#26377;&#23454;&#36136;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#24182;&#27809;&#26377;&#31283;&#20581;&#22320;&#36827;&#34892;&#25506;&#32034;&#65306;i&#65289;&#22312;&#25105;&#20204;&#30340;&#25152;&#26377;&#23454;&#39564;&#20013;&#65292;&#21482;&#26377;&#19968;&#20010;&#37197;&#32622;&#23548;&#33268;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#25506;&#32034;&#34892;&#20026;&#65306;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#22806;&#37096;&#24635;&#32467;&#30340;&#20132;&#20114;&#21382;&#21490;&#30340;GPT-4&#65292;&#36825;&#20123;&#34987;&#21576;&#29616;&#20026;&#20805;&#20998;&#32479;&#35745;&#30340;&#24773;&#20917;&#65307;ii&#65289;&#25152;&#26377;&#20854;&#20182;&#37197;&#32622;&#37117;&#27809;&#26377;&#20135;&#29983;&#31283;&#20581;&#30340;&#25506;&#32034;&#34892;&#20026;&#65292;&#21253;&#25324;&#20855;&#26377;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#20854;&#20182;&#37197;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15371v1 Announce Type: cross  Abstract: We investigate the extent to which contemporary Large Language Models (LLMs) can engage in exploration, a core capability in reinforcement learning and decision making. We focus on native performance of existing LLMs, without training interventions. We deploy LLMs as agents in simple multi-armed bandit environments, specifying the environment description and interaction history entirely in-context, i.e., within the LLM prompt. We experiment with GPT-3.5, GPT-4, and Llama2, using a variety of prompt designs, and find that the models do not robustly engage in exploration without substantial interventions: i) Across all of our experiments, only one configuration resulted in satisfactory exploratory behavior: GPT-4 with chain-of-thought reasoning and an externally summarized interaction history, presented as sufficient statistics; ii) All other configurations did not result in robust exploratory behavior, including those with chain-of-thou
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#34920;&#31034;&#26469;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#21457;&#29616;&#25972;&#21512;&#23454;&#20307;&#30456;&#20851;&#30693;&#35782;&#26377;&#21161;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#23454;&#20307;&#28966;&#28857;&#20195;&#30721;&#20999;&#25442;&#25913;&#21892;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15364</link><description>&lt;p&gt;
&#23454;&#29616;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Knowledge-Grounded Natural Language Understanding and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#34920;&#31034;&#26469;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#21457;&#29616;&#25972;&#21512;&#23454;&#20307;&#30456;&#20851;&#30693;&#35782;&#26377;&#21161;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#23454;&#20307;&#28966;&#28857;&#20195;&#30721;&#20999;&#25442;&#25913;&#21892;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;transformer&#27169;&#22411;&#30340;&#30693;&#35782;&#34920;&#31034;&#26469;&#21463;&#30410;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#24182;&#25506;&#35752;&#20102;&#20197;&#19979;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65306;(i) &#23454;&#20307;&#30693;&#35782;&#33021;&#21542;&#25193;&#23637;&#20854;&#20248;&#21183;&#33267;&#23454;&#20307;&#38142;&#25509;&#31561;&#19981;&#38480;&#20110;&#23454;&#20307;&#30340;&#20219;&#21153;? (ii) &#25105;&#20204;&#22914;&#20309;&#24544;&#23454;&#26377;&#25928;&#22320;&#20174;&#21407;&#22987;&#25991;&#26412;&#20013;&#25552;&#21462;&#36825;&#31181;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#23588;&#20854;&#26159;&#22312;&#22024;&#26434;&#30340;&#32593;&#32476;&#25991;&#26412;&#20013;? (iii) &#38500;&#20102;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#22806;&#65292;&#20854;&#20182;&#31867;&#22411;&#30340;&#30693;&#35782;&#22914;&#20309;&#26377;&#21161;&#20110;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;? &#36825;&#31687;&#35770;&#25991;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25972;&#21512;&#19982;&#23454;&#20307;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#30693;&#35782;&#26377;&#21161;&#20110;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#32780;&#20197;&#23454;&#20307;&#20026;&#28966;&#28857;&#30340;&#20195;&#30721;&#20999;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#23454;&#20307;&#30456;&#20851;&#20219;&#21153;&#30340;&#38646;&#26679;&#20363;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#22312;&#25552;&#21462;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#26377;&#25928;&#21644;&#24544;&#23454;&#26041;&#27861;&#26041;&#38754;&#65292;&#35266;&#23519;&#21040;&#36890;&#36807;&#25972;&#21512;&#36127;&#20363;&#21644;&#36827;&#34892;&#23454;&#20307;&#35268;&#21010;&#35757;&#32451;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15364v1 Announce Type: new  Abstract: This thesis investigates how natural language understanding and generation with transformer models can benefit from grounding the models with knowledge representations and addresses the following key research questions: (i) Can knowledge of entities extend its benefits beyond entity-centric tasks, such as entity linking? (ii) How can we faithfully and effectively extract such structured knowledge from raw text, especially noisy web text? (iii) How do other types of knowledge, beyond structured knowledge, contribute to improving NLP tasks?   Studies in this thesis find that incorporating relevant and up-to-date knowledge of entities benefits fake news detection, and entity-focused code-switching significantly enhances zero-shot cross-lingual transfer on entity-centric tasks. In terms of effective and faithful approaches to extracting structured knowledge, it is observed that integrating negative examples and training with entity planning 
&lt;/p&gt;</description></item><item><title>CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#26032;&#27010;&#24565;&#23884;&#20837;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.15362</link><description>&lt;p&gt;
CoLLEGe: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27010;&#24565;&#23884;&#20837;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
CoLLEGe: Concept Embedding Generation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15362
&lt;/p&gt;
&lt;p&gt;
CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28789;&#27963;&#30340;&#26032;&#27010;&#24565;&#23884;&#20837;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#24555;&#36895;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#36890;&#24120;&#38656;&#35201;&#26356;&#22797;&#26434;&#30340;&#24494;&#35843;&#36807;&#31243;&#25165;&#33021;&#23398;&#20064;&#24471;&#26356;&#31283;&#20581;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;CoLLEGe&#65288;Concept Learning with Language Embedding Generation&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29616;&#20195;&#21270;&#30340;&#23569;&#26679;&#26412;&#27010;&#24565;&#23398;&#20064;&#12290;CoLLEGe&#26159;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#21477;&#23376;&#25110;&#23450;&#20041;&#29983;&#25104;&#26032;&#27010;&#24565;&#30340;&#28789;&#27963;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#20803;&#23398;&#20064;&#30446;&#26631;&#21482;&#26159;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#38543;&#21518;&#30340;&#21477;&#23376;&#20013;&#36827;&#34892;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#65292;&#20351;&#20854;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15362v1 Announce Type: cross  Abstract: Current language models are unable to quickly learn new concepts on the fly, often requiring a more involved finetuning process to learn robustly. Prompting in-context is not robust to context distractions, and often fails to confer much information about the new concepts. Classic methods for few-shot word learning in NLP, relying on global word vectors, are less applicable to large language models. In this paper, we introduce a novel approach named CoLLEGe (Concept Learning with Language Embedding Generation) to modernize few-shot concept learning. CoLLEGe is a meta-learning framework capable of generating flexible embeddings for new concepts using a small number of example sentences or definitions. Our primary meta-learning objective is simply to facilitate a language model to make next word predictions in forthcoming sentences, making it compatible with language model pretraining. We design a series of tasks to test new concept lear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#19978;&#19979;&#25991;&#30340;&#22810;&#23457;&#38405;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#35780;&#35770;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.15351</link><description>&lt;p&gt;
&#34701;&#21512;&#19978;&#19979;&#25991;&#30340;&#22810;&#23457;&#38405;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multi-Review Fusion-in-Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15351
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#19978;&#19979;&#25991;&#30340;&#22810;&#23457;&#38405;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#35780;&#35770;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#38754;&#25991;&#26412;&#29983;&#25104;&#28085;&#30422;&#20102;&#35832;&#22914;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#31561;&#20219;&#21153;&#65292;&#38656;&#35201;&#20869;&#23481;&#36873;&#25321;&#21644;&#20869;&#23481;&#25972;&#21512;&#12290;&#24403;&#21069;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#30001;&#20110;&#19981;&#36879;&#26126;&#24615;&#32780;&#38590;&#20197;&#25511;&#21046;&#21644;&#35299;&#37322;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#20026;&#27599;&#20010;&#27493;&#39588;&#37117;&#25552;&#20379;&#21333;&#29420;&#30340;&#32452;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#29983;&#25104;&#36830;&#36143;&#25991;&#26412;&#30340;&#31532;&#20108;&#23376;&#20219;&#21153;&#65292;&#21363;&#22312;&#22810;&#25991;&#26723;&#29615;&#22659;&#20013;&#32473;&#23450;&#39044;&#36873;&#20869;&#23481;&#12290;&#25105;&#20204;&#23558;\textit{Fusion-in-Context}(FiC)&#20855;&#20307;&#21270;&#20026;&#19968;&#20010;&#29420;&#31435;&#20219;&#21153;&#65292;&#20854;&#36755;&#20837;&#21253;&#25324;&#24102;&#26377;&#30446;&#26631;&#20869;&#23481;&#39640;&#20142;&#37096;&#20998;&#30340;&#28304;&#25991;&#26412;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#38656;&#35201;&#29983;&#25104;&#19968;&#20010;&#21253;&#21547;&#25152;&#26377;&#19988;&#20165;&#21253;&#21547;&#30446;&#26631;&#20449;&#24687;&#30340;&#36830;&#36143;&#27573;&#33853;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21253;&#25324;&#22312;&#35780;&#35770;&#39046;&#22495;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;1000&#20010;&#23454;&#20363;&#30340;&#31934;&#24515;&#31574;&#21010;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#39640;&#20142;&#30495;&#23454;&#24615;&#21644;&#28085;&#30422;&#33539;&#22260;&#30340;&#26032;&#39062;&#35780;&#20272;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15351v1 Announce Type: new  Abstract: Grounded text generation, encompassing tasks such as long-form question-answering and summarization, necessitates both content selection and content consolidation. Current end-to-end methods are difficult to control and interpret due to their opaqueness. Accordingly, recent works have proposed a modular approach, with separate components for each step. Specifically, we focus on the second subtask, of generating coherent text given pre-selected content in a multi-document setting. Concretely, we formalize \textit{Fusion-in-Context} (FiC) as a standalone task, whose input consists of source texts with highlighted spans of targeted content. A model then needs to generate a coherent passage that includes all and only the target information. Our work includes the development of a curated dataset of 1000 instances in the reviews domain, alongside a novel evaluation framework for assessing the faithfulness and coverage of highlights, which stro
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#23450;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;CO-Fun&#65292;&#22260;&#32469;&#24503;&#22269;&#22522;&#37329;&#25307;&#32929;&#20070;&#20013;&#20844;&#21496;&#30340;&#22806;&#21253;&#23454;&#36341;&#23637;&#24320;&#65292;&#26631;&#27880;&#21253;&#25324;&#22235;&#31181;&#23454;&#20307;&#31867;&#22411;&#21644;&#20004;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15322</link><description>&lt;p&gt;
CO-Fun: &#19968;&#20221;&#20851;&#20110;&#24503;&#22269;&#22522;&#37329;&#25307;&#32929;&#20070;&#20013;&#20844;&#21496;&#22806;&#21253;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CO-Fun: A German Dataset on Company Outsourcing in Fund Prospectuses for Named Entity Recognition and Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15322
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#23450;&#35774;&#35745;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;CO-Fun&#65292;&#22260;&#32469;&#24503;&#22269;&#22522;&#37329;&#25307;&#32929;&#20070;&#20013;&#20844;&#21496;&#30340;&#22806;&#21253;&#23454;&#36341;&#23637;&#24320;&#65292;&#26631;&#27880;&#21253;&#25324;&#22235;&#31181;&#23454;&#20307;&#31867;&#22411;&#21644;&#20004;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15322v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#36890;&#36807;&#32593;&#32476;&#26144;&#23556;&#65292;&#21487;&#20197;&#25581;&#31034;&#37329;&#34701;&#23454;&#20307;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22260;&#32469;&#24503;&#22269;&#22522;&#37329;&#25307;&#32929;&#20070;&#20013;&#20844;&#21496;&#30340;&#22806;&#21253;&#23454;&#36341;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#19977;&#20301;&#19987;&#23478;&#23545;948&#20010;&#21477;&#23376;&#36827;&#34892;&#26631;&#27880;&#65292;&#20849;&#33719;&#24471;&#20102;&#22235;&#31181;&#23454;&#20307;&#31867;&#22411;&#65288;&#22806;&#21253;&#12289;&#20844;&#21496;&#12289;&#22320;&#28857;&#21644;&#36719;&#20214;&#65289;&#30340;5,969&#20010;&#26631;&#27880;&#21644;&#20851;&#31995;&#65288;&#22806;&#21253;-&#20844;&#21496;&#12289;&#20844;&#21496;-&#22320;&#28857;&#65289;&#30340;4,102&#20010;&#26631;&#27880;&#12290;&#35757;&#32451;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35782;&#21035;&#23454;&#20307;&#24182;&#25552;&#21462;&#20851;&#31995;&#65292;&#26174;&#31034;&#20986;&#20102;&#21021;&#27493;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#25968;&#25454;&#38598;&#30340;&#21311;&#21517;&#29256;&#26412;&#65292;&#20197;&#21450;&#27169;&#22411;&#35757;&#32451;&#25152;&#20351;&#29992;&#30340;&#25351;&#21335;&#21644;&#20195;&#30721;&#65292;&#22343;&#21487;&#20197;&#22312;https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15322v1 Announce Type: new  Abstract: The process of cyber mapping gives insights in relationships among financial entities and service providers. Centered around the outsourcing practices of companies within fund prospectuses in Germany, we introduce a dataset specifically designed for named entity recognition and relation extraction tasks. The labeling process on 948 sentences was carried out by three experts which yields to 5,969 annotations for four entity types (Outsourcing, Company, Location and Software) and 4,102 relation annotations (Outsourcing-Company, Company-Location). State-of-the-art deep learning models were trained to recognize entities and extract relations showing first promising results. An anonymized version of the dataset, along with guidelines and the code used for model training, are publicly available at https://www.dfki.uni-kl.de/cybermapping/data/CO-Fun-1.0-anonymized.zip.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.15309</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlled Training Data Generation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#19987;&#38376;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#20043;&#21069;&#37027;&#20123;&#37319;&#29992;&#24320;&#29615;&#26041;&#27861;&#24182;&#39044;&#20808;&#23450;&#20041;&#25552;&#31034;&#35789;&#26469;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#38381;&#29615;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#31532;&#19968;&#20010;&#26426;&#21046;&#20351;&#29992;&#26469;&#33258;&#32473;&#23450;&#30417;&#30563;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#30340;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#25552;&#31034;&#35789;&#23548;&#33268;&#20102;&#32463;&#36807;&#27169;&#22411;&#35757;&#32451;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#30693;&#36947;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#31532;&#20108;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#24341;&#23548;&#21040;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#31216;&#23558;&#36825;&#20004;&#20010;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#20026;&#24341;&#23548;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15309v1 Announce Type: cross  Abstract: In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, da
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LENS&#30340;&#26694;&#26550;&#65292;&#25351;&#20986;&#35821;&#35328;&#25551;&#36848;&#20250;&#35302;&#21457;&#24773;&#32490;&#21453;&#24212;&#21644;&#28508;&#22312;&#34892;&#20026;&#35268;&#33539;&#65292;&#20174;&#32780;&#24433;&#21709;&#20010;&#20307;&#30340;&#25112;&#30053;&#36873;&#25321;&#12290;</title><link>https://arxiv.org/abs/2403.15293</link><description>&lt;p&gt;
&#36879;&#36807;&#38236;&#22836;&#30475;&#20154;&#31867;&#34892;&#20026;&#65306;&#35821;&#35328;&#20869;&#23481;&#22914;&#20309;&#35302;&#21457;&#24773;&#32490;&#21644;&#35268;&#33539;&#65292;&#24182;&#20915;&#23450;&#31574;&#30053;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Human behaviour through a LENS: How Linguistic content triggers Emotions and Norms and determines Strategy choices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LENS&#30340;&#26694;&#26550;&#65292;&#25351;&#20986;&#35821;&#35328;&#25551;&#36848;&#20250;&#35302;&#21457;&#24773;&#32490;&#21453;&#24212;&#21644;&#28508;&#22312;&#34892;&#20026;&#35268;&#33539;&#65292;&#20174;&#32780;&#24433;&#21709;&#20010;&#20307;&#30340;&#25112;&#30053;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#23454;&#39564;&#30740;&#31350;&#35777;&#26126;&#65292;&#35821;&#35328;&#26694;&#26550;&#24433;&#21709;&#20102;&#32463;&#27982;&#21338;&#24328;&#20013;&#20154;&#31867;&#34892;&#20026;&#65292;&#36229;&#36234;&#20102;&#21487;&#29992;&#34892;&#21160;&#30340;&#32463;&#27982;&#21518;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#36234;&#20256;&#32479;&#22522;&#20110;&#32467;&#26524;&#20559;&#22909;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#26681;&#25454;LENS&#27169;&#22411;&#65292;&#23545;&#20915;&#31574;&#38382;&#39064;&#30340;&#35821;&#35328;&#25551;&#36848;&#20250;&#35302;&#21457;&#24773;&#32490;&#21453;&#24212;&#24182;&#26263;&#31034;&#28508;&#22312;&#30340;&#34892;&#20026;&#35268;&#33539;&#65292;&#36825;&#20123;&#35268;&#33539;&#20877;&#30456;&#20114;&#20316;&#29992;&#26469;&#22609;&#36896;&#20010;&#20307;&#30340;&#25112;&#30053;&#36873;&#25321;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#25903;&#25345;LENS&#27169;&#22411;&#27599;&#20010;&#36335;&#24452;&#30340;&#23454;&#39564;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#23427;&#30830;&#23450;&#24182;&#35752;&#35770;&#20102;&#19968;&#20123;&#30001;&#35813;&#27169;&#22411;&#24341;&#30003;&#20986;&#30340;&#37325;&#35201;&#30740;&#31350;&#38382;&#39064;&#65292;&#25351;&#21521;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15293v1 Announce Type: new  Abstract: Over the last two decades, a growing body of experimental research has provided evidence that linguistic frames influence human behaviour in economic games, beyond the economic consequences of the available actions. This article proposes a novel framework that transcends the traditional confines of outcome-based preference models. According to the LENS model, the Linguistic description of the decision problem triggers Emotional responses and suggests potential Norms of behaviour, which then interact to shape an individual's Strategic choice. The article reviews experimental evidence that supports each path of the LENS model. Furthermore, it identifies and discusses several critical research questions that arise from this model, pointing towards avenues for future inquiry.
&lt;/p&gt;</description></item><item><title>Fundus&#26159;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#38395;&#29228;&#34411;&#24037;&#20855;&#65292;&#36890;&#36807;&#25163;&#24037;&#23450;&#21046;&#30340;&#20869;&#23481;&#25552;&#21462;&#22120;&#65292;&#38024;&#23545;&#27599;&#20010;&#25903;&#25345;&#30340;&#22312;&#32447;&#25253;&#32440;&#26684;&#24335;&#25351;&#21335;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#25552;&#21462;&#65292;&#21516;&#26102;&#32467;&#21512;&#29228;&#21462;&#21644;&#20869;&#23481;&#25552;&#21462;&#20110;&#19968;&#20307;&#65292;&#20026;&#38750;&#25216;&#26415;&#29992;&#25143;&#25552;&#20379;&#32479;&#19968;&#20351;&#29992;&#30028;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.15279</link><description>&lt;p&gt;
Fundus&#65306;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#38395;&#29228;&#34411;&#65292;&#20248;&#21270;&#39640;&#36136;&#37327;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Fundus: A Simple-to-Use News Scraper Optimized for High Quality Extractions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15279
&lt;/p&gt;
&lt;p&gt;
Fundus&#26159;&#19968;&#20010;&#31616;&#21333;&#26131;&#29992;&#30340;&#26032;&#38395;&#29228;&#34411;&#24037;&#20855;&#65292;&#36890;&#36807;&#25163;&#24037;&#23450;&#21046;&#30340;&#20869;&#23481;&#25552;&#21462;&#22120;&#65292;&#38024;&#23545;&#27599;&#20010;&#25903;&#25345;&#30340;&#22312;&#32447;&#25253;&#32440;&#26684;&#24335;&#25351;&#21335;&#36827;&#34892;&#20248;&#21270;&#65292;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#25552;&#21462;&#65292;&#21516;&#26102;&#32467;&#21512;&#29228;&#21462;&#21644;&#20869;&#23481;&#25552;&#21462;&#20110;&#19968;&#20307;&#65292;&#20026;&#38750;&#25216;&#26415;&#29992;&#25143;&#25552;&#20379;&#32479;&#19968;&#20351;&#29992;&#30028;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fundus&#65292;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#26032;&#38395;&#29228;&#34411;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#20165;&#20973;&#20960;&#34892;&#20195;&#30721;&#33719;&#24471;&#25968;&#30334;&#19975;&#39640;&#36136;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#12290;&#19982;&#29616;&#26377;&#30340;&#26032;&#38395;&#29228;&#34411;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#25163;&#24037;&#23450;&#21046;&#30340;&#12289;&#19987;&#38376;&#38024;&#23545;&#27599;&#20010;&#25903;&#25345;&#30340;&#22312;&#32447;&#25253;&#32440;&#30340;&#26684;&#24335;&#25351;&#21335;&#30340;&#20869;&#23481;&#25552;&#21462;&#22120;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20248;&#21270;&#25105;&#20204;&#30340;&#29228;&#21462;&#36136;&#37327;&#65292;&#20197;&#30830;&#20445;&#26816;&#32034;&#21040;&#30340;&#26032;&#38395;&#25991;&#31456;&#23436;&#25972;&#19988;&#27809;&#26377;HTML&#30165;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#29228;&#21462;&#65288;&#20174;&#32593;&#32476;&#25110;&#22823;&#22411;&#32593;&#32476;&#24402;&#26723;&#20013;&#26816;&#32034;HTML&#65289;&#21644;&#20869;&#23481;&#25552;&#21462;&#32467;&#21512;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#27969;&#27700;&#32447;&#20013;&#12290;&#36890;&#36807;&#20026;&#39044;&#23450;&#20041;&#30340;&#19968;&#32452;&#25253;&#32440;&#25552;&#20379;&#32479;&#19968;&#30340;&#30028;&#38754;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;Fundus&#21363;&#20351;&#23545;&#38750;&#25216;&#26415;&#29992;&#25143;&#20063;&#26131;&#20110;&#20351;&#29992;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#26694;&#26550;&#65292;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#35774;&#35745;&#36873;&#25321;&#65292;&#24182;&#38024;&#23545;&#20854;&#20182;&#27969;&#34892;&#30340;&#26032;&#38395;&#29228;&#34411;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;Fundus&#21462;&#24471;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15279v1 Announce Type: new  Abstract: This paper introduces Fundus, a user-friendly news scraper that enables users to obtain millions of high-quality news articles with just a few lines of code. Unlike existing news scrapers, we use manually crafted, bespoke content extractors that are specifically tailored to the formatting guidelines of each supported online newspaper. This allows us to optimize our scraping for quality such that retrieved news articles are textually complete and without HTML artifacts. Further, our framework combines both crawling (retrieving HTML from the web or large web archives) and content extraction into a single pipeline. By providing a unified interface for a predefined collection of newspapers, we aim to make Fundus broadly usable even for non-technical users. This paper gives an overview of the framework, discusses our design choices, and presents a comparative evaluation against other popular news scrapers. Our evaluation shows that Fundus yie
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#21517;&#35789;&#30701;&#35821;&#30340;&#27867;&#25351;&#24615;&#36827;&#34892;&#32454;&#31890;&#24230;&#24314;&#27169;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#35780;&#27880;&#26041;&#27861;&#25429;&#25417;&#20102;&#27867;&#25351;&#24615;&#30340;&#24494;&#22937;&#26041;&#38754;&#65292;&#20026;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#20102;&#23454;&#29992;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.15278</link><description>&lt;p&gt;
&#36890;&#36807;&#21253;&#23481;&#24615;&#21644;&#25277;&#35937;&#24615;&#36830;&#32493;&#21051;&#24230;&#35268;&#33539;&#27867;&#25351;&#24615;
&lt;/p&gt;
&lt;p&gt;
Specifying Genericity through Inclusiveness and Abstractness Continuous Scales
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15278
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#21517;&#35789;&#30701;&#35821;&#30340;&#27867;&#25351;&#24615;&#36827;&#34892;&#32454;&#31890;&#24230;&#24314;&#27169;&#65292;&#36890;&#36807;&#36830;&#32493;&#30340;&#35780;&#27880;&#26041;&#27861;&#25429;&#25417;&#20102;&#27867;&#25351;&#24615;&#30340;&#24494;&#22937;&#26041;&#38754;&#65292;&#20026;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#20102;&#23454;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#33258;&#28982;&#35821;&#35328;&#20013;&#21517;&#35789;&#30701;&#35821;&#65288;NPs&#65289;&#30340;&#27867;&#25351;&#24615;&#36827;&#34892;&#32454;&#31890;&#24230;&#24314;&#27169;&#12290;&#35813;&#26694;&#26550;&#35774;&#35745;&#31616;&#21333;&#30452;&#35266;&#65292;&#36866;&#29992;&#20110;&#38750;&#19987;&#23478;&#27880;&#37322;&#32773;&#24182;&#36866;&#21512;&#20247;&#21253;&#20219;&#21153;&#12290;&#32467;&#21512;&#26377;&#20851;&#27867;&#25351;&#24615;&#30340;&#29702;&#35770;&#21644;&#35748;&#30693;&#25991;&#29486;&#65292;&#35813;&#26694;&#26550;&#26681;&#26893;&#20110;&#24050;&#24314;&#31435;&#30340;&#35821;&#35328;&#23398;&#29702;&#35770;&#12290;&#36890;&#36807;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;324&#20010;&#21477;&#23376;&#30340;&#23567;&#32780;&#20851;&#38190;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20026;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#36830;&#32493;&#27880;&#37322;&#19982;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#29616;&#26377;&#30340;&#20108;&#20803;&#27880;&#37322;&#65292;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#25429;&#25417;&#27867;&#25351;&#24615;&#30340;&#24494;&#22937;&#26041;&#38754;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#35821;&#35328;&#23398;&#23478;&#25552;&#20379;&#20102;&#19968;&#20010;&#23454;&#29992;&#36164;&#28304;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#31532;&#19968;&#20010;&#32463;&#36807;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26088;&#22312;&#26500;&#24314;&#21487;&#29992;&#20110;&#30740;&#31350;&#30340;&#23454;&#38469;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15278v1 Announce Type: new  Abstract: This paper introduces a novel annotation framework for the fine-grained modeling of Noun Phrases' (NPs) genericity in natural language. The framework is designed to be simple and intuitive, making it accessible to non-expert annotators and suitable for crowd-sourced tasks. Drawing from theoretical and cognitive literature on genericity, this framework is grounded in established linguistic theory. Through a pilot study, we created a small but crucial annotated dataset of 324 sentences, serving as a foundation for future research. To validate our approach, we conducted an evaluation comparing our continuous annotations with existing binary annotations on the same dataset, demonstrating the framework's effectiveness in capturing nuanced aspects of genericity. Our work offers a practical resource for linguists, providing a first annotated dataset and an annotation scheme designed to build real-language datasets that can be used in studies on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;TempRel&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#22686;&#24378;&#25552;&#31034;&#27169;&#26495;&#21644;&#35789;&#27719;&#21270;&#22120;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.15273</link><description>&lt;p&gt;
&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Event Temporal Relation Extraction based on Retrieval-Augmented on LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;TempRel&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#22686;&#24378;&#25552;&#31034;&#27169;&#26495;&#21644;&#35789;&#27719;&#21270;&#22120;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#23545;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#65288;TempRel&#65289;&#26159;&#20107;&#20214;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;&#30340;&#20027;&#35201;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;TempRel&#30340;&#22266;&#26377;&#27169;&#31946;&#24615;&#22686;&#21152;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#38543;&#30528;&#25552;&#31034;&#24037;&#31243;&#30340;&#20852;&#36215;&#65292;&#35774;&#35745;&#26377;&#25928;&#30340;&#25552;&#31034;&#27169;&#26495;&#21644;&#35789;&#27719;&#21270;&#22120;&#20197;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#21313;&#20998;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25163;&#21160;&#35774;&#35745;&#27169;&#26495;&#38590;&#20197;&#25552;&#21462;&#31934;&#30830;&#30340;&#26102;&#38388;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;TempRel&#25552;&#21462;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#32034;&#21040;&#30340;&#30693;&#35782;&#26469;&#22686;&#24378;&#25552;&#31034;&#27169;&#26495;&#21644;&#35789;&#27719;&#21270;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;&#21508;&#31181;LLMs&#30340;&#22810;&#26679;&#33021;&#21147;&#65292;&#20026;&#27169;&#26495;&#21644;&#35789;&#27719;&#21270;&#22120;&#35774;&#35745;&#29983;&#25104;&#20102;&#24191;&#27867;&#30340;&#24819;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20805;&#20998;&#21033;&#29992;&#20102;LLMs&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#20026;&#25105;&#20204;&#30340;&#35774;&#35745;&#25552;&#20379;&#20102;&#26356;&#22810;&#30693;&#35782;&#12290;&#23545;&#19977;&#20010;&#24191;&#27867;&#35748;&#21487;&#30340;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15273v1 Announce Type: new  Abstract: Event temporal relation (TempRel) is a primary subject of the event relation extraction task. However, the inherent ambiguity of TempRel increases the difficulty of the task. With the rise of prompt engineering, it is important to design effective prompt templates and verbalizers to extract relevant knowledge. The traditional manually designed templates struggle to extract precise temporal knowledge. This paper introduces a novel retrieval-augmented TempRel extraction approach, leveraging knowledge retrieved from large language models (LLMs) to enhance prompt templates and verbalizers. Our method capitalizes on the diverse capabilities of various LLMs to generate a wide array of ideas for template and verbalizer design. Our proposed method fully exploits the potential of LLMs for generation tasks and contributes more knowledge to our design. Empirical evaluations across three widely recognized datasets demonstrate the efficacy of our met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15268</link><description>&lt;p&gt;
&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65306;&#23398;&#20064;&#24819;&#35937;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#26469;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38382;&#39064;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;
Imagination Augmented Generation: Learning to Imagine Richer Context for Question Answering over Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15268
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#36890;&#36807;&#24819;&#35937;&#21147;&#65292;&#32780;&#38750;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#65292;&#26469;&#34917;&#20805;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#30693;&#35782;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#21644;&#29983;&#25104;&#22686;&#24378;&#29983;&#25104;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#30340;&#38382;&#39064;&#22238;&#31572;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20381;&#36182;&#20110;&#22806;&#37096;&#36164;&#28304;&#65292;&#32780;&#19988;&#20004;&#32773;&#37117;&#38656;&#35201;&#23558;&#26174;&#24335;&#25991;&#26723;&#21512;&#24182;&#21040;&#19978;&#19979;&#25991;&#20013;&#65292;&#23548;&#33268;&#26356;&#38271;&#30340;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#28040;&#32791;&#26356;&#22810;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#24050;&#32463;&#24314;&#27169;&#20102;&#20016;&#23500;&#30340;&#30693;&#35782;&#65292;&#23613;&#31649;&#27809;&#26377;&#34987;&#26377;&#25928;&#22320;&#35302;&#21457;&#25110;&#28608;&#27963;&#12290;&#22312;&#27492;&#21551;&#21457;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#22686;&#24378;&#26694;&#26550;&#65292;&#21363;&#24819;&#35937;&#22686;&#24378;&#29983;&#25104;&#65288;IAG&#65289;&#65292;&#23427;&#27169;&#25311;&#20102;&#20154;&#31867;&#36890;&#36807;&#24819;&#35937;&#21147;&#22312;&#20165;&#20973;&#24819;&#35937;&#22238;&#31572;&#38382;&#39064;&#26102;&#24357;&#34917;&#30693;&#35782;&#32570;&#38519;&#30340;&#33021;&#21147;&#65292;&#32780;&#19981;&#20381;&#36182;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;IAG&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24819;&#35937;&#26356;&#20016;&#23500;&#32972;&#26223;&#30340;&#26041;&#27861;&#65288;IMcQA&#65289;&#65292;&#36890;&#36807;&#20197;&#19979;&#20004;&#20010;&#27169;&#22359;&#33719;&#24471;&#26356;&#20016;&#23500;&#30340;&#32972;&#26223;&#65306;&#36890;&#36807;&#29983;&#25104;&#31616;&#21333;&#30340;&#24819;&#35937;&#23454;&#29616;&#26174;&#24335;&#24819;&#35937;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15268v1 Announce Type: new  Abstract: Retrieval-Augmented-Generation and Gener-ation-Augmented-Generation have been proposed to enhance the knowledge required for question answering over Large Language Models (LLMs). However, the former depends on external resources, and both require incorporating the explicit documents into the context, which results in longer contexts that lead to more resource consumption. Recent works indicate that LLMs have modeled rich knowledge, albeit not effectively triggered or activated. Inspired by this, we propose a novel knowledge-augmented framework, Imagination-Augmented-Generation (IAG), which simulates the human capacity to compensate for knowledge deficits while answering questions solely through imagination, without relying on external resources. Guided by IAG, we propose an imagine richer context method for question answering (IMcQA), which obtains richer context through the following two modules: explicit imagination by generating a sho
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;</title><link>https://arxiv.org/abs/2403.15250</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35780;&#20272;&#32467;&#26524;&#22312;LLM&#20013;&#30340;&#20840;&#38754;&#37325;&#26032;&#35780;&#20272;&#65306;&#19968;&#31181;&#22810;&#26041;&#20301;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Reassessment of Large-Scale Evaluation Outcomes in LLMs: A Multifaceted Statistical Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15250
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;LLM&#20013;&#22240;&#32032;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#36890;&#36807;&#20840;&#38754;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;LLM&#24555;&#36895;&#21457;&#23637;&#30340;&#32972;&#26223;&#19979;&#65292;&#35780;&#20272;&#22312;&#29702;&#35299;&#21644;&#25512;&#21160;&#36825;&#20123;&#27169;&#22411;&#21069;&#36827;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#20984;&#26174;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#32553;&#25918;&#12289;&#35757;&#32451;&#31867;&#22411;&#12289;&#26550;&#26500;&#31561;&#22240;&#32032;&#28145;&#21051;&#24433;&#21709;LLM&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#35780;&#20998;&#30340;&#24433;&#21709;&#31243;&#24230;&#21644;&#24615;&#36136;&#20173;&#28982;&#23384;&#22312;&#20105;&#35758;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35780;&#20272;&#23616;&#38480;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27169;&#22411;&#21644;&#25968;&#25454;&#28857;&#12290;&#36890;&#36807;&#32479;&#35745;&#35270;&#35282;&#26356;&#26377;&#25928;&#22320;&#28548;&#28165;&#36825;&#20123;&#22240;&#32032;&#23545;&#24615;&#33021;&#24471;&#20998;&#30340;&#24433;&#21709;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#36825;&#20123;LLM&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#37325;&#26032;&#26816;&#26597;&#65292;&#38024;&#23545;&#24403;&#21069;&#35780;&#20272;&#26041;&#27861;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;&#38543;&#30528;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#26694;&#26550;&#30340;&#20986;&#29616;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#32467;&#26524;&#25968;&#25454;&#38598;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#32479;&#35745;&#26041;&#27861;&#35770;&#12290;&#20854;&#20013;&#21253;&#25324;ANOVA&#12289;Tukey HSD&#26816;&#39564;&#12289;GAMM&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15250v1 Announce Type: cross  Abstract: Amidst the rapid evolution of LLMs, the significance of evaluation in comprehending and propelling these models forward is increasingly paramount. Evaluations have revealed that factors such as scaling, training types, architectures and other factors profoundly impact the performance of LLMs. However, the extent and nature of these impacts continue to be subjects of debate because most assessments have been restricted to a limited number of models and data points. Clarifying the effects of these factors on performance scores can be more effectively achieved through a statistical lens. Our study embarks on a thorough re-examination of these LLMs, targeting the inadequacies in current evaluation methods. With the advent of a uniform evaluation framework, our research leverages an expansive dataset of evaluation results, introducing a comprehensive statistical methodology. This includes the application of ANOVA, Tukey HSD tests, GAMM, and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15246</link><description>&lt;p&gt;
FollowIR: &#35780;&#20272;&#21644;&#25945;&#25480;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#20197;&#36981;&#24490;&#35828;&#26126;&#20070;
&lt;/p&gt;
&lt;p&gt;
FollowIR: Evaluating and Teaching Information Retrieval Models to Follow Instructions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15246
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#21644;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#26356;&#22909;&#22320;&#36981;&#24490;&#30495;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;&#35758;&#35770;&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#21382;&#21490;&#65292;&#26088;&#22312;&#20351;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#21028;&#26029;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36981;&#24490;&#38271;&#19988;&#22797;&#26434;&#30340;&#35828;&#26126;&#20070;&#65292;&#20174;&#32780;&#23454;&#29616;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20351;&#29992;LLMs&#20316;&#20026;&#20854;&#26550;&#26500;&#30340;&#25903;&#26609;&#65292;&#20960;&#20046;&#25152;&#26377;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#21482;&#25509;&#21463;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#65292;&#27809;&#26377;&#35828;&#26126;&#20070;&#12290;&#23545;&#20110;&#26368;&#36817;&#19968;&#20123;&#25509;&#21463;&#35828;&#26126;&#20070;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#23427;&#20204;&#22914;&#20309;&#20351;&#29992;&#36825;&#20123;&#35828;&#26126;&#20070;&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FollowIR&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20005;&#26684;&#30340;&#35828;&#26126;&#20070;&#35780;&#20272;&#22522;&#20934;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#38598;&#65292;&#24110;&#21161;IR&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#22320;&#36981;&#24490;&#29616;&#23454;&#19990;&#30028;&#30340;&#35828;&#26126;&#20070;&#12290;FollowIR&#22522;&#20110;TREC&#20250;&#35758;&#30340;&#24736;&#20037;&#21382;&#21490;&#65306;&#27491;&#22914;TREC&#20026;&#20154;&#31867;&#26631;&#27880;&#21592;&#25552;&#20379;&#35828;&#26126;&#20070;&#65288;&#20063;&#31216;&#20026;&#21465;&#36848;&#65289;&#26469;&#21028;&#26029;&#25991;&#26723;&#30340;&#30456;&#20851;&#24615;&#19968;&#26679;&#65292;&#22240;&#27492;IR&#27169;&#22411;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#36825;&#20123;&#35814;&#32454;&#35828;&#26126;&#20070;&#29702;&#35299;&#21644;&#30830;&#23450;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#22522;&#20934;&#20174;&#19977;&#20010;&#32463;&#36807;&#28145;&#24230;&#21028;&#26029;&#30340;TREC&#25910;&#34255;&#24320;&#22987;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15246v1 Announce Type: cross  Abstract: Modern Large Language Models (LLMs) are capable of following long and complex instructions that enable a diverse amount of user tasks. However, despite Information Retrieval (IR) models using LLMs as the backbone of their architectures, nearly all of them still only take queries as input, with no instructions. For the handful of recent models that do take instructions, it's unclear how they use them. We introduce our dataset FollowIR, which contains a rigorous instruction evaluation benchmark as well as a training set for helping IR models learn to better follow real-world instructions. FollowIR builds off the long history of the TREC conferences: as TREC provides human annotators with instructions (also known as narratives) to determine document relevance, so should IR models be able to understand and decide relevance based on these detailed instructions. Our evaluation benchmark starts with three deeply judged TREC collections and al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36339;&#36807;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#39640;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15226</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#27880;&#24847;&#21147;&#37117;&#26159;&#24517;&#35201;&#30340;&#65306;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39640;&#25928;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Not All Attention is Needed: Parameter and Computation Efficient Transfer Learning for Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#36339;&#36807;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#24182;&#20445;&#25345;&#39640;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#35843;&#21442;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#65292;&#31216;&#20026;&#39640;&#25928;&#36339;&#36807;&#27880;&#24847;&#21147;&#65288;EAS&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25581;&#31034;&#20102;&#22810;&#22836;&#27880;&#24847;&#21147;&#65288;MHA&#65289;&#20316;&#20026;MLLM&#30340;&#20027;&#35201;&#35745;&#31639;&#24320;&#38144;&#65292;&#36890;&#24120;&#23545;&#19979;&#28216;&#20219;&#21153;&#26469;&#35828;&#26159;&#22810;&#20313;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;EAS&#35780;&#20272;&#27880;&#24847;&#21147;&#20887;&#20313;&#24182;&#36339;&#36807;&#36739;&#19981;&#37325;&#35201;&#30340;MHA&#20197;&#21152;&#36895;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#24687;&#20256;&#25773;&#36866;&#37197;&#22120;&#65288;PIA&#65289;&#26469;&#26381;&#21153;EAS&#30340;&#27880;&#24847;&#21147;&#36339;&#36807;&#24182;&#20445;&#25345;&#21442;&#25968;&#25928;&#29575;&#65292;&#23427;&#21487;&#20197;&#36827;&#19968;&#27493;&#37325;&#26032;&#21442;&#25968;&#21270;&#20026;&#38646;&#39069;&#22806;&#24310;&#36831;&#30340;&#21069;&#39304;&#32593;&#32476;&#65288;FFNs&#65289;&#12290;&#20026;&#20102;&#39564;&#35777;EAS&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;LaVIN&#21644;&#32463;&#20856;&#30340;VL&#39044;&#35757;&#32451;&#27169;&#22411;METER&#65292;&#24182;&#22312;&#19968;&#32452;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;EAS&#19981;&#20165;&#20445;&#25345;&#20102;&#39640;&#24615;&#33021;&#21644;&#21442;&#25968;&#25928;&#29575;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15226v1 Announce Type: cross  Abstract: In this paper, we propose a novel parameter and computation efficient tuning method for Multi-modal Large Language Models (MLLMs), termed Efficient Attention Skipping (EAS). Concretely, we first reveal that multi-head attentions (MHAs), the main computational overhead of MLLMs, are often redundant to downstream tasks. Based on this observation, EAS evaluates the attention redundancy and skips the less important MHAs to speed up inference. Besides, we also propose a novel propagation-of-information adapter (PIA) to serve the attention skipping of EAS and keep parameter efficiency, which can be further re-parameterized into feed-forward networks (FFNs) for zero-extra latency. To validate EAS, we apply it to a recently proposed MLLM called LaVIN and a classic VL pre-trained model called METER, and conduct extensive experiments on a set of benchmarks. The experiments show that EAS not only retains high performance and parameter efficiency,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#24378;&#21046;&#25191;&#34892;&#22312;&#32447;&#25259;&#38706;&#36190;&#21161;&#20869;&#23481;&#30456;&#20851;&#27861;&#24459;&#35201;&#27714;&#30340;&#28508;&#21147;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21512;&#25104;Instagram&#26631;&#39064;&#30340;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#30446;&#26631;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#12290;</title><link>https://arxiv.org/abs/2403.15214</link><description>&lt;p&gt;
InstaSynth&#65306;&#20351;&#29992;ChatGPT&#20026;&#36190;&#21161;&#20869;&#23481;&#26816;&#27979;&#29983;&#25104;&#21512;&#25104;Instagram&#25968;&#25454;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24110;&#21161;&#24378;&#21046;&#25191;&#34892;&#22312;&#32447;&#25259;&#38706;&#36190;&#21161;&#20869;&#23481;&#30456;&#20851;&#27861;&#24459;&#35201;&#27714;&#30340;&#28508;&#21147;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21512;&#25104;Instagram&#26631;&#39064;&#30340;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#30446;&#26631;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#21457;&#20102;&#26377;&#20851;&#38477;&#20302;&#29983;&#25104;&#21487;&#33021;&#34987;&#29992;&#20110;&#19981;&#36947;&#24503;&#25110;&#38750;&#27861;&#30446;&#30340;&#30340;&#25991;&#26412;&#25104;&#26412;&#30340;&#25285;&#24551;&#65292;&#23588;&#20854;&#26159;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36825;&#20123;&#27169;&#22411;&#24110;&#21161;&#24378;&#21046;&#25191;&#34892;&#19982;&#22312;&#32447;&#25259;&#38706;&#36190;&#21161;&#20869;&#23481;&#30456;&#20851;&#30340;&#27861;&#24459;&#35201;&#27714;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#29983;&#25104;&#21512;&#25104;Instagram&#26631;&#39064;&#30340;&#20004;&#20010;&#30446;&#26631;&#65306;&#31532;&#19968;&#20010;&#30446;&#26631;&#65288;&#20445;&#30495;&#24230;&#65289;&#26159;&#20135;&#29983;&#36924;&#30495;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#20869;&#23481;&#32423;&#21644;&#32593;&#32476;&#32423;&#25351;&#26631;&#26469;&#35780;&#20272;&#21512;&#25104;&#26631;&#39064;&#26159;&#21542;&#30495;&#23454;&#12290;&#31532;&#20108;&#20010;&#30446;&#26631;&#65288;&#23454;&#29992;&#24615;&#65289;&#26159;&#21019;&#24314;&#23545;&#36190;&#21161;&#20869;&#23481;&#26816;&#27979;&#26377;&#29992;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25152;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#20197;&#35782;&#21035;Instagram&#19978;&#26410;&#20844;&#24320;&#24191;&#21578;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#34920;&#26126;&#65292;&#20445;&#30495;&#24230;&#21644;&#23454;&#29992;&#24615;&#30340;&#30446;&#26631;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15214v1 Announce Type: cross  Abstract: Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media. This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored content online. We investigate the use of LLMs for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets. For this, we implement content-level and network-level metrics to assess whether synthetic captions are realistic. The second objective (utility) is to create synthetic data that is useful for sponsored content detection. For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram. Our investigations show that the objectives of fidelity and utility may conflict and that promp
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#22312;&#20989;&#25968;&#24335;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20195;&#30721;&#34917;&#20840;&#24615;&#33021;&#30740;&#31350;&#20197;Haskell&#20026;&#20363;&#65292;&#21457;&#29616;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#30693;&#35782;&#23545;&#25552;&#39640;&#24615;&#33021;&#26377;&#24110;&#21161;</title><link>https://arxiv.org/abs/2403.15185</link><description>&lt;p&gt;
&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#22312;&#20989;&#25968;&#24335;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20195;&#30721;&#34917;&#20840;&#24615;&#33021;&#65306;&#20197;Haskell&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Investigating the Performance of Language Models for Completing Code in Functional Programming Languages: a Haskell Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15185
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#20989;&#25968;&#24335;&#32534;&#31243;&#35821;&#35328;&#20013;&#30340;&#20195;&#30721;&#34917;&#20840;&#24615;&#33021;&#30740;&#31350;&#20197;Haskell&#20026;&#20363;&#65292;&#21457;&#29616;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#30693;&#35782;&#23545;&#25552;&#39640;&#24615;&#33021;&#26377;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#32534;&#31243;&#35821;&#35328;&#20013;&#24110;&#21161;&#25104;&#21315;&#19978;&#19975;&#30340;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#20195;&#30721;&#30340;&#20351;&#29992;&#24555;&#36895;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20195;&#30721;&#34917;&#20840;&#27169;&#22411;&#30340;&#30740;&#31350;&#36890;&#24120;&#38598;&#20013;&#22312;&#35832;&#22914;Python&#21644;JavaScript&#31561;&#21629;&#20196;&#24335;&#35821;&#35328;&#65292;&#36825;&#23548;&#33268;&#23545;&#20989;&#25968;&#24335;&#32534;&#31243;&#35821;&#35328;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;Haskell&#31561;&#20989;&#25968;&#24335;&#35821;&#35328;&#19978;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35843;&#26597;&#26159;&#21542;&#21487;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#29992;&#20110;&#20195;&#30721;&#30340;&#35821;&#35328;&#27169;&#22411;CodeGPT&#21644;UniXcoder&#22312;&#20989;&#25968;&#24335;&#32534;&#31243;&#35821;&#35328;Haskell&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;HuggingFace&#19978;&#30340;&#19968;&#20010;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;Haskell&#25968;&#25454;&#38598;&#19978;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#22411;&#32763;&#35793;&#30340;HumanEval&#25968;&#25454;&#38598;&#25163;&#21160;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#33258;&#21160;&#35780;&#20272;&#26174;&#31034;&#65292;LLMs&#30340;&#39044;&#35757;&#32451;&#20013;&#23545;&#21629;&#20196;&#24335;&#32534;&#31243;&#35821;&#35328;&#30340;&#30693;&#35782;&#21487;&#33021;&#20250;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15185v1 Announce Type: new  Abstract: Language model-based code completion models have quickly grown in use, helping thousands of developers write code in many different programming languages. However, research on code completion models typically focuses on imperative languages such as Python and JavaScript, which results in a lack of representation for functional programming languages. Consequently, these models often perform poorly on functional languages such as Haskell. To investigate whether this can be alleviated, we evaluate the performance of two language models for code, CodeGPT and UniXcoder, on the functional programming language Haskell. We fine-tune and evaluate the models on Haskell functions sourced from a publicly accessible Haskell dataset on HuggingFace. Additionally, we manually evaluate the models using our novel translated HumanEval dataset. Our automatic evaluation shows that knowledge of imperative programming languages in the pre-training of LLMs may 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CACA&#20195;&#29702;&#65292;&#37319;&#29992;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;&#24320;&#25918;&#26550;&#26500;&#65292;&#25972;&#21512;&#20102;&#19968;&#32452;&#21327;&#20316;&#33021;&#21147;&#26469;&#23454;&#29616;AI&#20195;&#29702;&#65292;&#22686;&#24378;&#20102;AI&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15137</link><description>&lt;p&gt;
CACA Agent&#65306;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;AI&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CACA Agent: Capability Collaboration based AI Agent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15137
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CACA&#20195;&#29702;&#65292;&#37319;&#29992;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;&#24320;&#25918;&#26550;&#26500;&#65292;&#25972;&#21512;&#20102;&#19968;&#32452;&#21327;&#20316;&#33021;&#21147;&#26469;&#23454;&#29616;AI&#20195;&#29702;&#65292;&#22686;&#24378;&#20102;AI&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;AI&#20195;&#29702;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#20309;&#24555;&#36895;&#37096;&#32626;AI&#20195;&#29702;&#20197;&#21450;&#22914;&#20309;&#26041;&#20415;&#22320;&#25193;&#23637;AI&#20195;&#29702;&#30340;&#24212;&#29992;&#22330;&#26223;&#24050;&#25104;&#20026;&#19968;&#20010;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CACA&#20195;&#29702;&#65288;&#22522;&#20110;&#33021;&#21147;&#21327;&#20316;&#30340;AI&#20195;&#29702;&#65289;&#65292;&#37319;&#29992;&#20102;&#21463;&#26381;&#21153;&#35745;&#31639;&#21551;&#21457;&#30340;&#24320;&#25918;&#26550;&#26500;&#12290;CACA&#20195;&#29702;&#25972;&#21512;&#20102;&#19968;&#32452;&#21327;&#20316;&#33021;&#21147;&#26469;&#23454;&#29616;AI&#20195;&#29702;&#65292;&#19981;&#20165;&#20943;&#23569;&#20102;&#23545;&#21333;&#20010;LLM&#30340;&#20381;&#36182;&#65292;&#36824;&#22686;&#24378;&#20102;AI&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#21644;&#21487;&#29992;&#24037;&#20855;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#28436;&#31034;&#26469;&#35828;&#26126;&#25805;&#20316;&#21644;&#24212;&#29992;&#22330;&#26223;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15137v1 Announce Type: new  Abstract: As AI Agents based on Large Language Models (LLMs) have shown potential in practical applications across various fields, how to quickly deploy an AI agent and how to conveniently expand the application scenario of AI agents has become a challenge. Previous studies mainly focused on implementing all the reasoning capabilities of AI agents within a single LLM, which often makes the model more complex and also reduces the extensibility of AI agent functionality. In this paper, we propose CACA Agent (Capability Collaboration based AI Agent), using an open architecture inspired by service computing. CACA Agent integrates a set of collaborative capabilities to implement AI Agents, not only reducing the dependence on a single LLM, but also enhancing the extensibility of both the planning abilities and the tools available to AI agents. Utilizing the proposed system, we present a demo to illustrate the operation and the application scenario exten
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017;&#65292;&#29992;&#20110;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340; Grice &#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#20197;&#21450;&#20004;&#20010;&#26032;&#20934;&#21017;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#30340;&#29305;&#27530;&#34892;&#20026;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15115</link><description>&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20154;&#26426;&#20132;&#20114;&#30340;&#20250;&#35805;&#26368;&#22823;&#21270;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Language Models in Dialogue: Conversational Maxims for Human-AI Interactions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15115
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017;&#65292;&#29992;&#20110;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340; Grice &#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#20197;&#21450;&#20004;&#20010;&#26032;&#20934;&#21017;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#30340;&#29305;&#27530;&#34892;&#20026;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#34429;&#28982;&#22797;&#26434;&#65292;&#20294;&#22312;&#23545;&#35805;&#29615;&#22659;&#20013;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#32570;&#38519;&#12290;&#25105;&#20204;&#35748;&#20026;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#32570;&#38519;&#21487;&#20197;&#24402;&#22240;&#20110;&#36829;&#21453;&#19968;&#20010;&#25110;&#22810;&#20010;&#23545;&#35805;&#21407;&#21017;&#12290;&#36890;&#36807;&#20511;&#37492;&#31038;&#20250;&#31185;&#23398;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#24191;&#27867;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26368;&#22823;&#21270;&#20934;&#21017; - &#21253;&#25324;&#25968;&#37327;&#12289;&#36136;&#37327;&#12289;&#30456;&#20851;&#24615;&#12289;&#26041;&#24335;&#12289;&#20161;&#24904;&#20197;&#21450;&#36879;&#26126;&#24230; - &#26469;&#25551;&#36848;&#26377;&#25928;&#30340;&#20154;&#26426;&#23545;&#35805;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#22312;&#20154;&#26426;&#20114;&#21160;&#32972;&#26223;&#19979; Grice &#30340;&#21069;&#22235;&#20010;&#26368;&#22823;&#21270;&#20934;&#21017;&#30340;&#36866;&#29992;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#20004;&#20010;&#26032;&#30340;&#20934;&#21017;&#65292;&#20161;&#24904;&#65288;&#28041;&#21450;&#29983;&#25104;&#21644;&#21442;&#19982;&#26377;&#23475;&#20869;&#23481;&#65289;&#21644;&#36879;&#26126;&#24230;&#65288;&#28041;&#21450;&#35782;&#21035;&#33258;&#24049;&#30340;&#30693;&#35782;&#36793;&#30028;&#12289;&#25805;&#20316;&#32422;&#26463;&#21644;&#24847;&#22270;&#65289;&#65292;&#23545;&#20110;&#35299;&#20915;&#29616;&#20195;&#20154;&#26426;&#20114;&#21160;&#20013;&#29420;&#29305;&#34892;&#20026;&#26159;&#24517;&#35201;&#30340;&#12290;&#25552;&#20986;&#30340;&#20934;&#21017;&#20026;&#22914;&#20309;&#25552;&#20379;&#20855;&#20307;&#25351;&#23548;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15115v1 Announce Type: cross  Abstract: Modern language models, while sophisticated, exhibit some inherent shortcomings, particularly in conversational settings. We claim that many of the observed shortcomings can be attributed to violation of one or more conversational principles. By drawing upon extensive research from both the social science and AI communities, we propose a set of maxims -- quantity, quality, relevance, manner, benevolence, and transparency -- for describing effective human-AI conversation. We first justify the applicability of the first four maxims (from Grice) in the context of human-AI interactions. We then argue that two new maxims, benevolence (concerning the generation of, and engagement with, harmful content) and transparency (concerning recognition of one's knowledge boundaries, operational constraints, and intents), are necessary for addressing behavior unique to modern human-AI interactions. The proposed maxims offer prescriptive guidance on how
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15097</link><description>&lt;p&gt;
&#35770;&#25454;&#24863;&#30693;&#20107;&#20214;&#38142;&#25509;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Argument-Aware Approach To Event Linking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15097
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#33021;&#26356;&#22909;&#22320;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#65292;&#24357;&#34917;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#20107;&#20214;&#38142;&#25509;&#23558;&#25991;&#26412;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#65288;KB&#65289;&#20013;&#30456;&#20851;&#33410;&#28857;&#36830;&#25509;&#36215;&#26469;&#12290;&#20808;&#21069;&#22312;&#20107;&#20214;&#38142;&#25509;&#26041;&#38754;&#30340;&#30740;&#31350;&#20027;&#35201;&#20511;&#37492;&#20102;&#23454;&#20307;&#38142;&#25509;&#30340;&#26041;&#27861;&#65292;&#24573;&#30053;&#20102;&#20107;&#20214;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#19982;&#24191;&#27867;&#25506;&#35752;&#30340;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30456;&#27604;&#65292;&#20107;&#20214;&#20855;&#26377;&#26356;&#21152;&#22797;&#26434;&#30340;&#32467;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#26816;&#26597;&#20854;&#20851;&#32852;&#30340;&#35770;&#25454;&#26356;&#26377;&#25928;&#22320;&#21152;&#20197;&#21306;&#20998;&#12290;&#27492;&#22806;&#65292;&#20107;&#20214;&#30340;&#20449;&#24687;&#20016;&#23500;&#24615;&#23548;&#33268;&#20107;&#20214;&#30693;&#35782;&#24211;&#30340;&#31232;&#32570;&#24615;&#12290;&#36825;&#24378;&#35843;&#20102;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#38656;&#35201;&#35782;&#21035;&#21644;&#20998;&#31867;&#19981;&#22312;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#20214;&#25552;&#21450;&#20316;&#20026;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#36825;&#19968;&#39046;&#22495;&#21463;&#21040;&#20102;&#26377;&#38480;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#35770;&#25454;&#24863;&#30693;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#26631;&#35760;&#20107;&#20214;&#35770;&#25454;&#20449;&#24687;&#26469;&#25913;&#36827;&#20107;&#20214;&#38142;&#25509;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#26377;&#20851;&#20107;&#20214;&#25552;&#21450;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;&#38543;&#21518;&#65292;&#20026;&#20102;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#8220;&#36229;&#20986;&#30693;&#35782;&#24211;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15097v1 Announce Type: cross  Abstract: Event linking connects event mentions in text with relevant nodes in a knowledge base (KB). Prior research in event linking has mainly borrowed methods from entity linking, overlooking the distinct features of events. Compared to the extensively explored entity linking task, events have more complex structures and can be more effectively distinguished by examining their associated arguments. Moreover, the information-rich nature of events leads to the scarcity of event KBs. This emphasizes the need for event linking models to identify and classify event mentions not in the KB as ``out-of-KB,'' an area that has received limited attention. In this work, we tackle these challenges by introducing an argument-aware approach. First, we improve event linking models by augmenting input text with tagged event argument information, facilitating the recognition of key information about event mentions. Subsequently, to help the model handle ``out-
&lt;/p&gt;</description></item><item><title>CHisIEC&#26159;&#19968;&#20221;&#26088;&#22312;&#21152;&#36895;&#21476;&#20195;&#21382;&#21490;&#25991;&#21270;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;13&#20010;&#26397;&#20195;&#12289;&#36328;&#36234;1830&#24180;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#26102;&#38388;&#36328;&#24230;&#21644;&#25991;&#26412;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15088</link><description>&lt;p&gt;
CHisIEC&#65306;&#19968;&#20221;&#29992;&#20110;&#21476;&#20195;&#20013;&#22269;&#21382;&#21490;&#20449;&#24687;&#25552;&#21462;&#30340;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CHisIEC: An Information Extraction Corpus for Ancient Chinese History
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15088
&lt;/p&gt;
&lt;p&gt;
CHisIEC&#26159;&#19968;&#20221;&#26088;&#22312;&#21152;&#36895;&#21476;&#20195;&#21382;&#21490;&#25991;&#21270;&#30740;&#31350;&#30340;&#35821;&#26009;&#24211;&#65292;&#28085;&#30422;&#20102;13&#20010;&#26397;&#20195;&#12289;&#36328;&#36234;1830&#24180;&#30340;&#25968;&#25454;&#65292;&#20855;&#26377;&#20016;&#23500;&#30340;&#26102;&#38388;&#36328;&#24230;&#21644;&#25991;&#26412;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15088v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25277;&#35937;: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#22312;&#25968;&#23383;&#20154;&#25991;&#23398;&#65288;DH&#65289;&#39046;&#22495;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26159;&#25512;&#21160;&#21382;&#21490;&#21644;&#25991;&#21270;&#36951;&#20135;&#25991;&#26412;&#32467;&#26500;&#20998;&#26512;&#30340;&#22522;&#30707;&#12290;&#36825;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#39046;&#22495;&#23588;&#20026;&#26126;&#26174;&#12290;&#20026;&#20102;&#21152;&#36895;&#21476;&#20195;&#21382;&#21490;&#25991;&#21270;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#20013;&#22269;&#21382;&#21490;&#20449;&#24687;&#25552;&#21462;&#35821;&#26009;&#24211;&#8221;&#65288;CHisIEC&#65289;&#12290;CHisIEC&#26159;&#19968;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24320;&#21457;&#21644;&#35780;&#20272;NER&#21644;RE&#20219;&#21153;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#36164;&#28304;&#12290;&#28085;&#30422;&#20102;&#20174;13&#20010;&#26397;&#20195;&#12289;&#36328;&#36234;1830&#24180;&#30340;&#25968;&#25454;&#30340;&#21331;&#36234;&#21382;&#21490;&#26102;&#38388;&#36724;&#65292;CHisIEC&#20307;&#29616;&#20102;&#20013;&#22269;&#21382;&#21490;&#25991;&#26723;&#20013;&#23384;&#22312;&#30340;&#24191;&#27867;&#26102;&#38388;&#33539;&#22260;&#21644;&#25991;&#26412;&#24322;&#36136;&#24615;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#22235;&#31181;&#19981;&#21516;&#30340;&#23454;&#20307;&#31867;&#22411;&#21644;&#21313;&#20108;&#31181;&#20851;&#31995;&#31867;&#22411;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15088v1 Announce Type: new  Abstract: Natural Language Processing (NLP) plays a pivotal role in the realm of Digital Humanities (DH) and serves as the cornerstone for advancing the structural analysis of historical and cultural heritage texts. This is particularly true for the domains of named entity recognition (NER) and relation extraction (RE). In our commitment to expediting ancient history and culture, we present the ``Chinese Historical Information Extraction Corpus''(CHisIEC). CHisIEC is a meticulously curated dataset designed to develop and evaluate NER and RE tasks, offering a resource to facilitate research in the field. Spanning a remarkable historical timeline encompassing data from 13 dynasties spanning over 1830 years, CHisIEC epitomizes the extensive temporal range and text heterogeneity inherent in Chinese historical documents. The dataset encompasses four distinct entity types and twelve relation types, resulting in a meticulously labeled dataset comprising 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#38024;&#23545;&#26085;&#26412;&#37329;&#34701;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#24182;&#23545;&#22810;&#27169;&#22411;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#23454;&#20102;GPT-4&#34920;&#29616;&#20986;&#33394;&#65292;&#35813;&#22522;&#20934;&#26377;&#25928;&#21306;&#20998;&#20102;&#21508;&#24615;&#33021;&#33539;&#22260;&#20869;&#27169;&#22411;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15062</link><description>&lt;p&gt;
&#26500;&#24314;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26085;&#26412;&#37329;&#34701;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Construction of a Japanese Financial Benchmark for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15062
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#38024;&#23545;&#26085;&#26412;&#37329;&#34701;&#39046;&#22495;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#24182;&#23545;&#22810;&#27169;&#22411;&#36827;&#34892;&#27979;&#35797;&#65292;&#35777;&#23454;&#20102;GPT-4&#34920;&#29616;&#20986;&#33394;&#65292;&#35813;&#22522;&#20934;&#26377;&#25928;&#21306;&#20998;&#20102;&#21508;&#24615;&#33021;&#33539;&#22260;&#20869;&#27169;&#22411;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#20154;&#20204;&#24320;&#22987;&#35752;&#35770;&#19987;&#27880;&#20110;&#29305;&#23450;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#12290;&#24403;&#21069;&#23545;&#27599;&#20010;&#39046;&#22495;&#20013;&#29616;&#26377;LLMs&#24615;&#33021;&#36827;&#34892;&#35780;&#20272;&#30340;&#38656;&#27714;&#20063;&#26085;&#30410;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;&#29305;&#23450;&#20110;&#26085;&#26412;&#21644;&#37329;&#34701;&#39046;&#22495;&#30340;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#24182;&#23545;&#19968;&#20123;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#30446;&#21069;&#34920;&#29616;&#31361;&#20986;&#65292;&#24182;&#19988;&#26500;&#24314;&#30340;&#22522;&#20934;&#33021;&#22815;&#26377;&#25928;&#21457;&#25381;&#20316;&#29992;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#22522;&#20934;&#36890;&#36807;&#32467;&#21512;&#19981;&#21516;&#38590;&#24230;&#30340;&#20219;&#21153;&#65292;&#21487;&#20197;&#21306;&#20998;&#25152;&#26377;&#24615;&#33021;&#33539;&#22260;&#20869;&#21508;&#27169;&#22411;&#30340;&#22522;&#20934;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15062v1 Announce Type: cross  Abstract: With the recent development of large language models (LLMs), models that focus on certain domains and languages have been discussed for their necessity. There is also a growing need for benchmarks to evaluate the performance of current LLMs in each domain. Therefore, in this study, we constructed a benchmark comprising multiple tasks specific to the Japanese and financial domains and performed benchmark measurements on some models. Consequently, we confirmed that GPT-4 is currently outstanding, and that the constructed benchmarks function effectively. According to our analysis, our benchmark can differentiate benchmark scores among models in all performance ranges by combining tasks with different difficulties.
&lt;/p&gt;</description></item><item><title>LLM2LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#25945;&#24072;LLM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24182;&#23558;&#20854;&#28155;&#21152;&#22238;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24110;&#21161;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.15042</link><description>&lt;p&gt;
LLM2LLM: &#21033;&#29992;&#26032;&#30340;&#36845;&#20195;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#22686;&#24378;LLM
&lt;/p&gt;
&lt;p&gt;
LLM2LLM: Boosting LLMs with Novel Iterative Data Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15042
&lt;/p&gt;
&lt;p&gt;
LLM2LLM &#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#20351;&#29992;&#25945;&#24072;LLM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#24182;&#23558;&#20854;&#28155;&#21152;&#22238;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#24110;&#21161;&#20302;&#25968;&#25454;&#29615;&#22659;&#19979;&#30340;LLM&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30446;&#21069;&#26159;&#35299;&#20915;&#32477;&#22823;&#22810;&#25968;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20173;&#38656;&#35201;&#24494;&#35843;&#20197;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20294;&#20854;&#20013;&#35768;&#22810;&#24212;&#29992;&#22788;&#20110;&#20302;&#25968;&#25454;&#33539;&#22260;&#65292;&#20351;&#24471;&#24494;&#35843;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM2LLM&#65292;&#36825;&#26159;&#19968;&#31181;&#26377;&#38024;&#23545;&#24615;&#21644;&#36845;&#20195;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#19968;&#20010;&#25945;&#24072;LLM&#26469;&#22686;&#24378;&#19968;&#20010;&#23567;&#30340;&#31181;&#23376;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22686;&#21152;&#39069;&#22806;&#30340;&#25968;&#25454;&#29992;&#20110;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;LLM2LLM&#65288;1&#65289;&#22312;&#21021;&#22987;&#31181;&#23376;&#25968;&#25454;&#19978;&#24494;&#35843;&#22522;&#32447;&#23398;&#29983;LLM&#65292;&#65288;2&#65289;&#35780;&#20272;&#21644;&#25552;&#21462;&#27169;&#22411;&#38169;&#35823;&#30340;&#25968;&#25454;&#28857;&#65292;&#65288;3&#65289;&#20351;&#29992;&#25945;&#24072;LLM&#26681;&#25454;&#36825;&#20123;&#38169;&#35823;&#30340;&#25968;&#25454;&#28857;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#28982;&#21518;&#23558;&#20854;&#28155;&#21152;&#22238;&#35757;&#32451;&#25968;&#25454;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22686;&#24378;LLM&#23545;&#38169;&#35823;&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#20449;&#21495;&#65292;&#24182;&#37325;&#26032;&#25972;&#21512;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15042v1 Announce Type: new  Abstract: Pretrained large language models (LLMs) are currently state-of-the-art for solving the vast majority of natural language processing tasks. While many real-world applications still require fine-tuning to reach satisfactory levels of performance, many of them are in the low-data regime, making fine-tuning challenging. To address this, we propose LLM2LLM, a targeted and iterative data augmentation strategy that uses a teacher LLM to enhance a small seed dataset by augmenting additional data that can be used for fine-tuning on a specific task. LLM2LLM (1) fine-tunes a baseline student LLM on the initial seed data, (2) evaluates and extracts data points that the model gets wrong, and (3) uses a teacher LLM to generate synthetic data based on these incorrect data points, which are then added back into the training data. This approach amplifies the signal from incorrectly predicted data points by the LLM during training and reintegrates them in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#23548;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#65292;&#36890;&#36807;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#21160;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#31574;&#30053;&#26469;&#19982;&#26410;&#30693;&#30340;ESG&#35780;&#20272;&#26631;&#20934;&#36798;&#25104;&#19968;&#33268;&#65292;&#24182;&#22312;&#38889;&#25991;&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.15040</link><description>&lt;p&gt;
&#36890;&#36807;GPT-4&#36827;&#34892;&#38544;&#24335;&#35268;&#21017;&#23398;&#20064;&#30340;ESG&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ESG Classification by Implicit Rule Learning via GPT-4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24341;&#23548;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#65292;&#36890;&#36807;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#21160;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#31574;&#30053;&#26469;&#19982;&#26410;&#30693;&#30340;ESG&#35780;&#20272;&#26631;&#20934;&#36798;&#25104;&#19968;&#33268;&#65292;&#24182;&#22312;&#38889;&#25991;&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31532;&#20108;&#21517;&#30340;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29615;&#22659;&#12289;&#31038;&#20250;&#21644;&#27835;&#29702;&#65288;ESG&#65289;&#22240;&#32032;&#34987;&#24191;&#27867;&#37319;&#29992;&#20316;&#20026;&#26356;&#39640;&#25237;&#36164;&#22238;&#25253;&#30340;&#25351;&#26631;&#12290;&#22240;&#27492;&#65292;&#27491;&#22312;&#36827;&#34892;&#25345;&#32493;&#21162;&#21147;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#21270;ESG&#35780;&#20272;&#65292;&#20174;&#28023;&#37327;&#32593;&#32476;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#21495;&#21464;&#24471;&#26356;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35780;&#32423;&#26426;&#26500;&#20445;&#23494;&#20854;&#35780;&#20272;&#25351;&#26631;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#39281;&#21463;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#20043;&#33510;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22914;GPT-4&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#21160;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#31574;&#30053;&#26469;&#24341;&#23548;&#31526;&#21512;&#26410;&#30693;ESG&#35780;&#20272;&#26631;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#25152;&#25552;&#20379;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#19981;&#26356;&#26032;&#27169;&#22411;&#65292;&#22312;&#38889;&#25991;&#20849;&#20139;&#20219;&#21153;ML-ESG-3&#24433;&#21709;&#31867;&#22411;&#36319;&#36394;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#35843;&#25972;&#25552;&#31034;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#37329;&#34701;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#21033;&#29992;&#20855;&#26377;&#20844;&#24320;&#21487;&#29992;&#26435;&#37325;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15040v1 Announce Type: new  Abstract: Environmental, social, and governance (ESG) factors are widely adopted as higher investment return indicators. Accordingly, ongoing efforts are being made to automate ESG evaluation with language models to extract signals from massive web text easily. However, recent approaches suffer from a lack of training data, as rating agencies keep their evaluation metrics confidential. This paper investigates whether state-of-the-art language models like GPT-4 can be guided to align with unknown ESG evaluation criteria through strategies such as prompting, chain-of-thought reasoning, and dynamic in-context learning. We demonstrate the efficacy of these approaches by ranking 2nd in the Shared-Task ML-ESG-3 Impact Type track for Korean without updating the model on the provided training data. We also explore how adjusting prompts impacts the ability of language models to address financial tasks leveraging smaller models with openly available weights
&lt;/p&gt;</description></item><item><title>MasonTigers&#22312;SemEval-2024 Task 1&#20013;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#65292;&#22312;&#22788;&#29702;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14990</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024&#20219;&#21153;1&#20013;&#30340;&#38598;&#25104;&#26041;&#27861;&#30740;&#31350;&#65306;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 1: An Ensemble Approach for Semantic Textual Relatedness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14990
&lt;/p&gt;
&lt;p&gt;
MasonTigers&#22312;SemEval-2024 Task 1&#20013;&#37319;&#29992;&#38598;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#35821;&#35328;&#29305;&#23450;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#65292;&#22312;&#22788;&#29702;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#21442;&#19982;SemEval-2024&#20219;&#21153;1-&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#30340;&#24037;&#20316;&#12290;&#35813;&#20219;&#21153;&#28085;&#30422;&#20102;&#28085;&#30422;&#20102;&#30417;&#30563;&#65288;Track A&#65289;&#12289;&#26080;&#30417;&#30563;&#65288;Track B&#65289;&#21644;&#36328;&#35821;&#35328;&#65288;Track C&#65289;&#26041;&#27861;&#65292;&#28041;&#21450;14&#31181;&#19981;&#21516;&#35821;&#35328;&#12290;MasonTigers&#26159;&#23569;&#25968;&#21516;&#26102;&#21442;&#19982;&#20102;&#19977;&#20010;track&#20013;&#25152;&#26377;&#35821;&#35328;&#30340;&#20004;&#25903;&#22242;&#38431;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Track A&#20013;&#25490;&#21517;&#20174;&#31532;11&#21040;&#31532;21&#65292;&#22312;Track B&#20013;&#25490;&#21517;&#20174;&#31532;1&#21040;&#31532;8&#65292;&#22312;Track C&#20013;&#25490;&#21517;&#20174;&#31532;5&#21040;&#31532;12&#12290;&#22312;&#36981;&#24490;&#29305;&#23450;&#20219;&#21153;&#32422;&#26463;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#34920;&#29616;&#26368;&#22909;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#35821;&#35328;&#30340;BERT&#27169;&#22411;&#21644;&#21477;&#23376;&#21464;&#25442;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14990v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 1 - Semantic Textual Relatedness. The task encompasses supervised (Track A), unsupervised (Track B), and cross-lingual (Track C) approaches across 14 different languages. MasonTigers stands out as one of the two teams who participated in all languages across the three tracks. Our approaches achieved rankings ranging from 11th to 21st in Track A, from 1st to 8th in Track B, and from 5th to 12th in Track C. Adhering to the task-specific constraints, our best performing approaches utilize ensemble of statistical machine learning approaches combined with language-specific BERT based models and sentence transformers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#22312;SemEval-2024&#20219;&#21153;8&#19978;&#30340;&#34920;&#29616;&#20998;&#26512;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#37492;&#21035;&#22120;Transformer&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#21477;&#23376;Transformer&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#37096;&#20998;&#24773;&#20917;&#19979;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#38024;&#23545;FLAN-T5&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.14989</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024&#20219;&#21153;8&#19978;&#30340;&#34920;&#29616;&#20998;&#26512;&#65306;&#22522;&#20110;Transformer&#27169;&#22411;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 8: Performance Analysis of Transformer-based Models on Machine-Generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#22312;SemEval-2024&#20219;&#21153;8&#19978;&#30340;&#34920;&#29616;&#20998;&#26512;&#65292;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#37492;&#21035;&#22120;Transformer&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#32467;&#21512;&#21477;&#23376;Transformer&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#37096;&#20998;&#24773;&#20917;&#19979;&#37319;&#29992;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#38024;&#23545;FLAN-T5&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;MasonTigers&#21442;&#21152;SemEval-2024&#20219;&#21153;8&#30340;&#24773;&#20917; - &#22810;&#29983;&#25104;&#22120;&#12289;&#22810;&#39046;&#22495;&#21644;&#22810;&#35821;&#35328;&#30340;&#40657;&#30418;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#12290;&#35813;&#20219;&#21153;&#28085;&#30422;&#20102;&#20108;&#20803;&#20154;&#24037;&#25776;&#20889; vs. &#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#65288;Track A&#65289;&#12289;&#22810;&#36335;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#20998;&#31867;&#65288;Track B&#65289;&#21644;&#20154;&#26426;&#28151;&#21512;&#25991;&#26412;&#26816;&#27979;&#65288;Track C&#65289;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#37492;&#21035;&#22120;Transformer&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#20197;&#21450;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#21477;&#23376;Transformer&#21644;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;Track A&#21644;B&#65292;&#36824;&#20351;&#29992;&#20102;&#38646;&#26679;&#26412;&#25552;&#31034;&#21644;&#23545;FLAN-T5&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14989v1 Announce Type: new  Abstract: This paper presents the MasonTigers entry to the SemEval-2024 Task 8 - Multigenerator, Multidomain, and Multilingual Black-Box Machine-Generated Text Detection. The task encompasses Binary Human-Written vs. Machine-Generated Text Classification (Track A), Multi-Way Machine-Generated Text Classification (Track B), and Human-Machine Mixed Text Detection (Track C). Our best performing approaches utilize mainly the ensemble of discriminator transformer models along with sentence transformer and statistical machine learning approaches in specific cases. Moreover, zero-shot prompting and fine-tuning of FLAN-T5 are used for Track A and B.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39118;&#38505;&#35780;&#20272;&#38382;&#39064;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#35748;&#20026;&#20449;&#24687;&#39118;&#38505;&#36739;&#23569;&#26377;&#23475;&#65292;&#21516;&#26102;&#22312;&#20449;&#24687;&#39118;&#38505;&#22330;&#26223;&#20013;&#23545;&#36234;&#29425;&#25915;&#20987;&#23384;&#22312;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.14988</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39118;&#38505;&#19982;&#21709;&#24212;&#65306;&#35780;&#20272;&#20851;&#38190;&#23041;&#32961;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Risk and Response in Large Language Models: Evaluating Key Threat Categories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#39118;&#38505;&#35780;&#20272;&#38382;&#39064;&#65292;&#21457;&#29616;LLMs&#20542;&#21521;&#20110;&#35748;&#20026;&#20449;&#24687;&#39118;&#38505;&#36739;&#23569;&#26377;&#23475;&#65292;&#21516;&#26102;&#22312;&#20449;&#24687;&#39118;&#38505;&#22330;&#26223;&#20013;&#23545;&#36234;&#29425;&#25915;&#20987;&#23384;&#22312;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26085;&#30410;&#26222;&#21450;&#30340;&#39118;&#38505;&#35780;&#20272;&#38382;&#39064;&#12290;&#37325;&#28857;&#20851;&#27880;&#22870;&#21169;&#27169;&#22411;&#22914;&#20309;&#24863;&#30693;&#21644;&#20998;&#31867;&#19981;&#21516;&#31867;&#22411;&#30340;&#39118;&#38505;&#65292;&#22870;&#21169;&#27169;&#22411;&#26088;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLMs&#20197;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#20559;&#22909;&#35757;&#32451;&#25968;&#25454;&#30340;&#20027;&#35266;&#24615;&#36136;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;Anthropic Red-team&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#21253;&#25324;&#20449;&#24687;&#39118;&#38505;&#12289;&#24694;&#24847;&#29992;&#36884;&#21644;&#27495;&#35270;/&#20167;&#24680;&#20869;&#23481;&#22312;&#20869;&#30340;&#20027;&#35201;&#39118;&#38505;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#20542;&#21521;&#20110;&#35748;&#20026;&#20449;&#24687;&#21361;&#23475;&#36739;&#23569;&#26377;&#23475;&#65292;&#36825;&#19968;&#21457;&#29616;&#24471;&#21040;&#20102;&#19968;&#20010;&#29305;&#21035;&#24320;&#21457;&#30340;&#22238;&#24402;&#27169;&#22411;&#30340;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#26174;&#31034;&#65292;LLMs&#23545;&#20449;&#24687;&#21361;&#23475;&#30340;&#21709;&#24212;&#30456;&#23545;&#19981;&#37027;&#20040;&#20005;&#26684;&#12290;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;LLMs&#22312;&#20449;&#24687;&#21361;&#23475;&#22330;&#26223;&#20013;&#23545;&#36234;&#29425;&#25915;&#20987;&#23384;&#22312;&#26174;&#33879;&#30340;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14988v1 Announce Type: new  Abstract: This paper explores the pressing issue of risk assessment in Large Language Models (LLMs) as they become increasingly prevalent in various applications. Focusing on how reward models, which are designed to fine-tune pretrained LLMs to align with human values, perceive and categorize different types of risks, we delve into the challenges posed by the subjective nature of preference-based training data. By utilizing the Anthropic Red-team dataset, we analyze major risk categories, including Information Hazards, Malicious Uses, and Discrimination/Hateful content. Our findings indicate that LLMs tend to consider Information Hazards less harmful, a finding confirmed by a specially developed regression model. Additionally, our analysis shows that LLMs respond less stringently to Information Hazards compared to other risks. The study further reveals a significant vulnerability of LLMs to jailbreaking attacks in Information Hazard scenarios, hig
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;SemEval-2024 Task 9&#30340;&#35868;&#39064;&#20219;&#21153;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#25552;&#31034;&#25216;&#26415;&#65292;&#21253;&#25324;&#38646;&#21442;&#32771;&#21644;&#23569;&#37327;&#21442;&#32771;&#25552;&#31034;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#26368;&#32456;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#36880;&#27493;&#35299;&#37322;&#24615;&#25552;&#31034;&#22914;&#20309;&#21487;&#20197;&#26356;&#22909;&#22320;&#25581;&#31034;&#24050;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.14982</link><description>&lt;p&gt;
MasonTigers&#22312;SemEval-2024 Task 9&#20013;&#65306;&#20351;&#29992;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#35299;&#20915;&#35868;&#39064;
&lt;/p&gt;
&lt;p&gt;
MasonTigers at SemEval-2024 Task 9: Solving Puzzles with an Ensemble of Chain-of-Thoughts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14982
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;SemEval-2024 Task 9&#30340;&#35868;&#39064;&#20219;&#21153;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#25552;&#31034;&#25216;&#26415;&#65292;&#21253;&#25324;&#38646;&#21442;&#32771;&#21644;&#23569;&#37327;&#21442;&#32771;&#25552;&#31034;&#65292;&#20197;&#21450;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#26368;&#32456;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#36880;&#27493;&#35299;&#37322;&#24615;&#25552;&#31034;&#22914;&#20309;&#21487;&#20197;&#26356;&#22909;&#22320;&#25581;&#31034;&#24050;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#20171;&#32461;&#20102;&#22242;&#38431;MasonTigers&#25552;&#20132;&#32473;SemEval-2024 Task 9&#30340;&#20316;&#21697;&#65292;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#32452;&#35868;&#39064;&#25968;&#25454;&#38598;&#29992;&#20110;&#27979;&#35797;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#20960;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#38646;&#21442;&#32771;&#21644;&#23569;&#37327;&#21442;&#32771;&#25552;&#31034;&#36890;&#36807;&#19987;&#26377;LLMs&#27979;&#35797;&#26102;&#21462;&#24471;&#20102;&#30456;&#24403;&#19981;&#38169;&#30340;&#32467;&#26524;&#65292;&#30456;&#36739;&#20110;&#24320;&#28304;&#27169;&#22411;&#12290;&#36890;&#36807;&#24605;&#32500;&#38142;&#25552;&#31034;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#32467;&#26524;&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#25552;&#31034;&#26041;&#27861;&#65292;&#36880;&#27493;&#20998;&#35299;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#31995;&#21015;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#26368;&#22909;&#30340;&#32467;&#26524;&#65292;&#22312;&#21333;&#35789;&#35868;&#39064;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#20108;&#65292;&#22312;&#21477;&#23376;&#35868;&#39064;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;13&#12290;&#25552;&#31034;LLMs&#30340;&#24378;&#22823;&#34920;&#29616;&#26174;&#31034;&#20102;&#23427;&#20204;&#22312;&#25552;&#20379;&#24605;&#32771;&#36807;&#31243;&#20998;&#35299;&#26102;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#36880;&#27493;&#35299;&#37322;&#24615;&#25552;&#31034;&#22914;&#20309;&#33021;&#22815;&#26356;&#22810;&#22320;&#25581;&#31034;&#24050;&#32534;&#30721;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14982v1 Announce Type: new  Abstract: Our paper presents team MasonTigers submission to the SemEval-2024 Task 9 - which provides a dataset of puzzles for testing natural language understanding. We employ large language models (LLMs) to solve this task through several prompting techniques. Zero-shot and few-shot prompting generate reasonably good results when tested with proprietary LLMs, compared to the open-source models. We obtain further improved results with chain-of-thought prompting, an iterative prompting method that breaks down the reasoning process step-by-step. We obtain our best results by utilizing an ensemble of chain-of-thought prompts, placing 2nd in the word puzzle subtask and 13th in the sentence puzzle subtask. The strong performance of prompted LLMs demonstrates their capability for complex reasoning when provided with a decomposition of the thought process. Our work sheds light on how step-wise explanatory prompts can unlock more of the knowledge encoded 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14972</link><description>&lt;p&gt;
&#19968;&#22270;&#32988;&#21315;&#35328;&#65306;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#22270;&#35889;&#36777;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Picture Is Worth a Graph: Blueprint Debate on Graph for Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14972
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#24335;&#30340;&#22270;&#35889;&#36777;&#35770;&#26041;&#27861;&#65288;BDoG&#65289;&#65292;&#22312;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#38450;&#27490;&#24847;&#35265;&#38472;&#33104;&#21270;&#21644;&#20943;&#23569;&#30001;&#22270;&#20687;&#24341;&#20837;&#30340;&#20998;&#24515;&#27010;&#24565;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26088;&#22312;&#23558;&#22810;&#26234;&#33021;&#20307;&#36777;&#35770;&#24341;&#20837;&#22810;&#27169;&#24577;&#25512;&#29702;&#30340;&#35797;&#28857;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#30001;&#20110;&#36807;&#24230;&#24635;&#32467;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#65292;&#20197;&#21450;&#30001;&#20110;&#22270;&#20687;&#24341;&#20837;&#36716;&#31227;&#24615;&#27010;&#24565;&#32780;&#23548;&#33268;&#27880;&#24847;&#21147;&#20998;&#25955;&#30340;&#38382;&#39064;&#12290;&#36825;&#20123;&#25361;&#25112;&#28304;&#33258;&#29616;&#26377;&#36777;&#35770;&#26041;&#26696;&#30340;&#24402;&#32435;&#65288;&#33258;&#19979;&#32780;&#19978;&#65289;&#24615;&#36136;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28436;&#32462;&#65288;&#33258;&#19978;&#32780;&#19979;&#65289;&#30340;&#36777;&#35770;&#26041;&#27861;&#65292;&#31216;&#20026;&#22270;&#35889;&#36777;&#35770;&#65288;BDoG&#65289;&#12290;&#22312;BDoG&#20013;&#65292;&#36777;&#35770;&#20165;&#38480;&#20110;&#34013;&#22270;&#22270;&#20013;&#65292;&#20197;&#38450;&#27490;&#36890;&#36807;&#19990;&#30028;&#32423;&#25688;&#35201;&#32780;&#23548;&#33268;&#24847;&#35265;&#38472;&#33104;&#21270;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#22270;&#20013;&#30340;&#20998;&#25903;&#20013;&#23384;&#20648;&#35777;&#25454;&#65292;BDoG&#32531;&#35299;&#20102;&#39057;&#32321;&#20294;&#26080;&#20851;&#30340;&#27010;&#24565;&#24102;&#26469;&#30340;&#20998;&#25955;&#27880;&#24847;&#21147;&#29616;&#35937;&#12290;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;BDoG&#65292;&#22312;&#31185;&#23398;&#38382;&#31572;&#21644;MMBench&#20013;&#21462;&#24471;&#20102;&#26368;&#26032;&#25104;&#26524;&#65292;&#24182;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14972v1 Announce Type: new  Abstract: This paper presents a pilot study aimed at introducing multi-agent debate into multimodal reasoning. The study addresses two key challenges: the trivialization of opinions resulting from excessive summarization and the diversion of focus caused by distractor concepts introduced from images. These challenges stem from the inductive (bottom-up) nature of existing debating schemes. To address the issue, we propose a deductive (top-down) debating approach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are confined to a blueprint graph to prevent opinion trivialization through world-level summarization. Moreover, by storing evidence in branches within the graph, BDoG mitigates distractions caused by frequent but irrelevant concepts. Extensive experiments validate BDoG, achieving state-of-the-art results in Science QA and MMBench with significant improvements over previous methods.
&lt;/p&gt;</description></item><item><title>Adapprox&#26159;&#19968;&#31181;&#37319;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#36924;&#36817;Adam&#20248;&#21270;&#31639;&#27861;&#30340;&#20108;&#38454;&#30697;&#12290;&#22312;GPT-2&#30340;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;Adapprox&#30456;&#27604;AdamW&#33021;&#22815;&#23454;&#29616;34.5%&#33267;49.9%&#21644;33.8%&#33267;49.9%&#30340;&#20869;&#23384;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#23548;&#31574;&#30053;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.14958</link><description>&lt;p&gt;
Adapprox:&#33258;&#36866;&#24212;&#36817;&#20284;&#22312;Adam&#20248;&#21270;&#20013;&#20351;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Adapprox: Adaptive Approximation in Adam Optimization via Randomized Low-Rank Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14958
&lt;/p&gt;
&lt;p&gt;
Adapprox&#26159;&#19968;&#31181;&#37319;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#36924;&#36817;Adam&#20248;&#21270;&#31639;&#27861;&#30340;&#20108;&#38454;&#30697;&#12290;&#22312;GPT-2&#30340;&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;Adapprox&#30456;&#27604;AdamW&#33021;&#22815;&#23454;&#29616;34.5%&#33267;49.9%&#21644;33.8%&#33267;49.9%&#30340;&#20869;&#23384;&#33410;&#32422;&#65292;&#24182;&#36890;&#36807;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#23548;&#31574;&#30053;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#21644;&#21152;&#24555;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35268;&#27169;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#35832;&#22914;Adam&#20043;&#31867;&#30340;&#20248;&#21270;&#22120;&#22312;&#23384;&#20648;&#19968;&#12289;&#20108;&#38454;&#30697;&#25968;&#25454;&#26102;&#36935;&#21040;&#20102;&#26174;&#33879;&#30340;&#20869;&#23384;&#28040;&#32791;&#25361;&#25112;&#12290;&#24403;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#22914;Adafactor&#21644;CAME&#36890;&#24120;&#36890;&#36807;&#20854;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#25216;&#26415;&#26469;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Adapprox&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#38543;&#26426;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#26469;&#26356;&#26377;&#25928;&#21644;&#20934;&#30830;&#22320;&#36817;&#20284;Adam&#30340;&#20108;&#38454;&#30697;&#12290;Adapprox&#20855;&#26377;&#33258;&#36866;&#24212;&#31209;&#36873;&#25321;&#26426;&#21046;&#65292;&#31934;&#32454;&#24179;&#34913;&#20934;&#30830;&#24615;&#21644;&#20869;&#23384;&#25928;&#29575;&#65292;&#24182;&#21253;&#25324;&#19968;&#20010;&#21487;&#36873;&#30340;&#20313;&#24358;&#30456;&#20284;&#24615;&#25351;&#23548;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#31283;&#23450;&#24615;&#24182;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;GPT-2&#35757;&#32451;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;Adapprox&#36890;&#36807;&#23454;&#29616;&#23545;117M&#21644;345M&#27169;&#22411;&#30340;34.5%&#33267;49.9%&#21644;33.8%&#33267;49.9%&#20869;&#23384;&#33410;&#32422;&#65288;&#20998;&#21035;&#21551;&#29992;&#20102;&#31532;&#19968;&#38454;&#30697;&#65289;&#65292;&#36229;&#36234;&#20102;AdamW&#65292;&#24182;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#36825;&#20123;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14958v1 Announce Type: cross  Abstract: As deep learning models exponentially increase in size, optimizers such as Adam encounter significant memory consumption challenges due to the storage of first and second moment data. Current memory-efficient methods like Adafactor and CAME often compromise accuracy with their matrix factorization techniques. Addressing this, we introduce Adapprox, a novel approach that employs randomized low-rank matrix approximation for a more effective and accurate approximation of Adam's second moment. Adapprox features an adaptive rank selection mechanism, finely balancing accuracy and memory efficiency, and includes an optional cosine similarity guidance strategy to enhance stability and expedite convergence. In GPT-2 training and downstream tasks, Adapprox surpasses AdamW by achieving 34.5% to 49.9% and 33.8% to 49.9% memory savings for the 117M and 345M models, respectively, with the first moment enabled, and further increases these savings wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;(RARG)&#65292;&#36890;&#36807;&#25910;&#38598;&#31185;&#23398;&#26469;&#28304;&#30340;&#35777;&#25454;&#26469;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#36991;&#20813;&#36807;&#24230;&#37325;&#22797;&#12290;</title><link>https://arxiv.org/abs/2403.14952</link><description>&lt;p&gt;
&#22522;&#20110;&#35777;&#25454;&#39537;&#21160;&#30340;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#26816;&#32034;&#22686;&#24378;&#21709;&#24212;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Evidence-Driven Retrieval Augmented Response Generation for Online Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14952
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#30340;&#21709;&#24212;&#29983;&#25104;&#26041;&#27861;(RARG)&#65292;&#36890;&#36807;&#25910;&#38598;&#31185;&#23398;&#26469;&#28304;&#30340;&#35777;&#25454;&#26469;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#21644;&#36991;&#20813;&#36807;&#24230;&#37325;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32440;&#65306;arXiv:2403.14952v1   &#31867;&#22411;&#65306;&#20132;&#21449;   &#25688;&#35201;&#65306;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#27867;&#28389;&#23545;&#20844;&#20849;&#21033;&#30410;&#26500;&#25104;&#20102;&#37325;&#35201;&#23041;&#32961;&#12290;&#34429;&#28982;&#35768;&#22810;&#22312;&#32447;&#29992;&#25143;&#31215;&#26497;&#21442;&#19982;&#23545;&#25239;&#34394;&#20551;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#21709;&#24212;&#32570;&#20047;&#31036;&#35980;&#21644;&#25903;&#25345;&#20107;&#23454;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#27809;&#26377;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#23548;&#33268;&#25991;&#26412;&#36136;&#37327;&#19981;&#20339;&#21644;&#21709;&#24212;&#36807;&#20110;&#37325;&#22797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#32447;&#34394;&#20551;&#20449;&#24687;&#30340;&#26816;&#32034;&#22686;&#24378;&#21709;&#24212;&#29983;&#25104;&#65288;RARG&#65289;&#65292;&#35813;&#26041;&#27861;&#20174;&#31185;&#23398;&#26469;&#28304;&#25910;&#38598;&#25903;&#25345;&#35777;&#25454;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#35777;&#25454;&#29983;&#25104;&#21453;&#34394;&#20551;&#20449;&#24687;&#30340;&#21709;&#24212;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;RARG&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;1&#65289;&#35777;&#25454;&#25910;&#38598;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26816;&#32034;&#27969;&#31243;&#26469;&#26816;&#32034;&#21644;&#37325;&#26032;&#25490;&#21015;&#35777;&#25454;&#25991;&#26723;&#65292;&#20351;&#29992;&#19968;&#20010;dat
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14952v1 Announce Type: cross  Abstract: The proliferation of online misinformation has posed significant threats to public interest. While numerous online users actively participate in the combat against misinformation, many of such responses can be characterized by the lack of politeness and supporting facts. As a solution, text generation approaches are proposed to automatically produce counter-misinformation responses. Nevertheless, existing methods are often trained end-to-end without leveraging external knowledge, resulting in subpar text quality and excessively repetitive responses. In this paper, we propose retrieval augmented response generation for online misinformation (RARG), which collects supporting evidence from scientific sources and generates counter-misinformation responses based on the evidences. In particular, our RARG consists of two stages: (1) evidence collection, where we design a retrieval pipeline to retrieve and rerank evidence documents using a dat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowLA&#30340;&#30693;&#35782;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25554;&#20837;&#33258;&#36866;&#24212;&#23618;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#65292;&#33021;&#22815;&#25552;&#21319;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14950</link><description>&lt;p&gt;
KnowLA&#65306;&#21033;&#29992;&#30693;&#35782;&#33258;&#36866;&#24212;&#25552;&#21319;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
KnowLA: Enhancing Parameter-efficient Finetuning with Knowledgeable Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14950
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;KnowLA&#30340;&#30693;&#35782;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25554;&#20837;&#33258;&#36866;&#24212;&#23618;&#21644;&#30693;&#35782;&#22270;&#23884;&#20837;&#65292;&#33021;&#22815;&#25552;&#21319;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#26469;&#25913;&#21892;PEFT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;KnowLA&#30340;&#30693;&#35782;&#33258;&#36866;&#24212;&#26041;&#27861;&#12290;&#23427;&#22312;LLM&#20013;&#25554;&#20837;&#19968;&#20010;&#33258;&#36866;&#24212;&#23618;&#65292;&#23558;&#36755;&#20837;&#25991;&#26412;&#20013;&#20986;&#29616;&#30340;&#23454;&#20307;&#30340;&#23884;&#20837;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#33258;&#36866;&#24212;&#23618;&#19982;LoRA&#22312;&#25351;&#23548;&#25968;&#25454;&#19978;&#32452;&#21512;&#35757;&#32451;&#12290;&#22312;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#21644;&#19977;&#20010;&#30693;&#35782;&#22270;&#19978;&#36827;&#34892;&#30340;&#20845;&#39033;&#22522;&#20934;&#23454;&#39564;&#34920;&#26126;&#20102;KnowLA&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; \modelname &#33021;&#22815;&#24110;&#21161;&#28608;&#27963;LLM&#20013;&#30340;&#30456;&#20851;&#21442;&#25968;&#21270;&#30693;&#35782;&#20197;&#22238;&#31572;&#38382;&#39064;&#65292;&#32780;&#19981;&#25913;&#21464;&#20854;&#21442;&#25968;&#25110;&#36755;&#20837;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14950v1 Announce Type: new  Abstract: Parameter-efficient finetuning (PEFT) is a key technique for adapting large language models (LLMs) to downstream tasks. In this paper, we study leveraging knowledge graph embeddings to improve the effectiveness of PEFT. We propose a knowledgeable adaptation method called KnowLA. It inserts an adaptation layer into an LLM to integrate the embeddings of entities appearing in the input text. The adaptation layer is trained in combination with LoRA on instruction data. Experiments on six benchmarks with two popular LLMs and three knowledge graphs demonstrate the effectiveness and robustness of KnowLA. We show that \modelname can help activate the relevant parameterized knowledge in an LLM to answer a question without changing its parameters or input prompts.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#36716;&#25442;&#30697;&#38453;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21333;&#19968;&#32447;&#24615;&#23618;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;</title><link>https://arxiv.org/abs/2403.14946</link><description>&lt;p&gt;
&#19968;&#20010;&#32447;&#24615;&#23618;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#20302;&#31209;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
A Single Linear Layer Yields Task-Adapted Low-Rank Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#36716;&#25442;&#30697;&#38453;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#20851;&#31995;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#21333;&#19968;&#32447;&#24615;&#23618;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#30001;&#20004;&#20010;&#20302;&#31209;&#30697;&#38453;$ A $&#21644;$ B $&#32452;&#25104;&#30340;&#22686;&#37327;&#30697;&#38453;$ \Delta W $&#26356;&#26032;&#21021;&#22987;&#26435;&#37325;&#30697;&#38453;$ W_0 $&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;$ W_0 $&#21644;$ \Delta W $&#20043;&#38388;&#23384;&#22312;&#20851;&#32852;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#28145;&#20837;&#25506;&#35752;$ W_0 $&#19982;&#20302;&#31209;&#30697;&#38453;$ A $&#21644;$ B $&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36827;&#19968;&#27493;&#29702;&#35299;LoRA&#30340;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#19968;&#20010;&#23558;$ W_0 $&#36716;&#25442;&#20026;&#20302;&#31209;&#30697;&#38453;&#30340;&#36716;&#25442;&#30697;&#38453;&#65292;&#20854;&#20013;&#34164;&#21547;&#20102;&#20851;&#31995;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#36716;&#25442;&#30697;&#38453;&#22312;&#27599;&#19968;&#23618;&#20043;&#38388;&#26159;&#30456;&#20284;&#30340;&#12290;&#21463;&#21040;&#36825;&#20123;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20551;&#35774;&#19968;&#20010;&#21333;&#19968;&#32447;&#24615;&#23618;&#65292;&#23558;&#27599;&#19968;&#23618;&#30340;$ W_0 $&#20316;&#20026;&#36755;&#20837;&#65292;&#21487;&#20197;&#29983;&#25104;&#20219;&#21153;&#33258;&#36866;&#24212;&#30340;&#20302;&#31209;&#30697;&#38453;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#26377;&#26465;&#20214;&#21442;&#25968;&#21270;&#30340;LoRA (CondLoRA) &#26041;&#27861;&#65292;&#26469;&#26356;&#26032;&#21021;&#22987;&#26435;&#37325;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14946v1 Announce Type: cross  Abstract: Low-Rank Adaptation (LoRA) is a widely used Parameter-Efficient Fine-Tuning (PEFT) method that updates an initial weight matrix $W_0$ with a delta matrix $\Delta W$ consisted by two low-rank matrices $A$ and $B$. A previous study suggested that there is correlation between $W_0$ and $\Delta W$. In this study, we aim to delve deeper into relationships between $W_0$ and low-rank matrices $A$ and $B$ to further comprehend the behavior of LoRA. In particular, we analyze a conversion matrix that transform $W_0$ into low-rank matrices, which encapsulates information about the relationships. Our analysis reveals that the conversion matrices are similar across each layer. Inspired by these findings, we hypothesize that a single linear layer, which takes each layer's $W_0$ as input, can yield task-adapted low-rank matrices. To confirm this hypothesis, we devise a method named Conditionally Parameterized LoRA (CondLoRA) that updates initial weig
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#22312;&#38646;-shot &#35774;&#32622;&#19979;&#25506;&#32034;&#20102;&#22235;&#31181;LLM&#22312;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20026;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#25552;&#20379;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14938</link><description>&lt;p&gt;
&#20851;&#20110;LLM&#22312;&#38646;-shot &#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Zero-Shot Counterspeech Generation by LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#22312;&#38646;-shot &#35774;&#32622;&#19979;&#25506;&#32034;&#20102;&#22235;&#31181;LLM&#22312;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#20026;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#25552;&#20379;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#37327;&#22823;&#24133;&#22686;&#21152;&#12290;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#26159;&#19968;&#20010;&#37325;&#35201;&#20219;&#21153;&#65292;&#21162;&#21147;&#36890;&#36807;&#24494;&#35843;LLM&#19982;&#20167;&#24680;&#35328;&#35770;-&#23545;&#25239;&#24615;&#35328;&#35770;&#23545;&#26469;&#21457;&#23637;&#29983;&#25104;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#23581;&#35797;&#37117;&#27809;&#26377;&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;-shot &#35774;&#32622;&#20013;&#30340;&#20869;&#22312;&#29305;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#22235;&#20010;LLM&#65288;GPT-2&#12289;DialoGPT&#12289;ChatGPT&#21644;FlanT5&#65289;&#22312;&#38646;-shot &#35774;&#32622;&#19979;&#29992;&#20110;&#23545;&#25239;&#24615;&#35328;&#35770;&#29983;&#25104;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#12290;&#23545;&#20110;GPT-2&#21644;DialoGPT&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#65288;&#23567;&#12289;&#20013;&#12289;&#22823;&#65289;&#24615;&#33021;&#20559;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#29992;&#20110;&#29983;&#25104;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#25239;&#24615;&#35328;&#35770;&#65292;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#31574;&#30053;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14938v1 Announce Type: new  Abstract: With the emergence of numerous Large Language Models (LLM), the usage of such models in various Natural Language Processing (NLP) applications is increasing extensively. Counterspeech generation is one such key task where efforts are made to develop generative models by fine-tuning LLMs with hatespeech - counterspeech pairs, but none of these attempts explores the intrinsic properties of large language models in zero-shot settings. In this work, we present a comprehensive analysis of the performances of four LLMs namely GPT-2, DialoGPT, ChatGPT and FlanT5 in zero-shot settings for counterspeech generation, which is the first of its kind. For GPT-2 and DialoGPT, we further investigate the deviation in performance with respect to the sizes (small, medium, large) of the models. On the other hand, we propose three different prompting strategies for generating different types of counterspeech and analyse the impact of such strategies on the p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14932</link><description>&lt;p&gt;
&#19987;&#27880;&#39537;&#21160;&#30340;&#25512;&#29702;:&#37322;&#25918;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention-Driven Reasoning: Unlocking the Potential of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14932
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#22522;&#30784;&#26426;&#21046;&#20173;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#27880;&#24847;&#21147;&#26426;&#21046;&#20248;&#21270;&#26469;&#22686;&#24378;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#38750;&#35821;&#20041;&#26631;&#35760;&#23548;&#33268;&#30340;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#20302;&#25928;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26469;&#37325;&#26032;&#24179;&#34913;&#20559;&#26012;&#20998;&#24067;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25277;&#35937;&#26356;&#21152;&#24494;&#22937;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25512;&#29702;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;STEM&#38382;&#39064;&#12290;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#27880;&#24847;&#21147;&#27169;&#24335;&#22312;LLMs&#25512;&#29702;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#36825;&#20123;&#33021;&#21147;&#30340;&#26041;&#27861;&#65292;&#20026;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#30340;&#35821;&#35328;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14932v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown remarkable capabilities, but their reasoning abilities and underlying mechanisms remain poorly understood. We present a novel approach to enhance LLMs' reasoning through attention mechanism optimization, without additional training data. We identify inefficiencies in the attention distribution caused by non-semantic tokens and propose an algorithm to re-balance the skewed distribution, enabling the model to abstract more nuanced knowledge. Our experiments demonstrate significantly improved reasoning capabilities, particularly for non-STEM questions. We provide insights into the role of attention patterns in LLMs' reasoning and propose a method to enhance these abilities, paving the way for more powerful and versatile language models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Skip Decoding&#65288;HSD&#65289;&#30340;&#26032;&#22411;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20998;&#23618;&#22320;&#33258;&#36866;&#24212;&#36339;&#36807;&#35299;&#30721;&#23618;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#21644;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.14919</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#30340;&#20998;&#23618;&#36339;&#36291;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Skip Decoding for Efficient Autoregressive Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14919
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Skip Decoding&#65288;HSD&#65289;&#30340;&#26032;&#22411;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#65292;&#36890;&#36807;&#20998;&#23618;&#22320;&#33258;&#36866;&#24212;&#36339;&#36807;&#35299;&#30721;&#23618;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#21644;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#31574;&#30053;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#25552;&#21069;&#32467;&#26463;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21152;&#36895;&#25512;&#26029;&#38454;&#27573;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hierarchical Skip Decoding&#65288;HSD&#65289;&#30340;&#26032;&#22411;&#35299;&#30721;&#31574;&#30053;&#65292;&#29992;&#20110;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#12290;&#19982;&#38656;&#35201;&#39069;&#22806;&#21487;&#35757;&#32451;&#32452;&#20214;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;HSD&#26159;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#26681;&#25454;&#24403;&#21069;&#24207;&#21015;&#38271;&#24230;&#20197;&#20998;&#23618;&#30340;&#26041;&#24335;&#33258;&#36866;&#24212;&#22320;&#36339;&#36807;&#35299;&#30721;&#23618;&#65292;&#20174;&#32780;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#24182;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#22312;&#20116;&#20010;&#24102;&#26377;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#23454;&#39564;&#26174;&#31034;&#65292;HSD&#22312;&#24179;&#34913;&#25928;&#29575;&#21644;&#25991;&#26412;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#20960;&#20046;&#36339;&#36807;&#19968;&#21322;&#30340;&#23618;&#65292;HSD&#21487;&#20197;&#19982;&#21407;&#22987;&#33258;&#22238;&#24402;d&#27169;&#22411;&#30456;&#27604;&#20445;&#25345;90%&#30340;&#25991;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14919v1 Announce Type: cross  Abstract: Autoregressive decoding strategy is a commonly used method for text generation tasks with pre-trained language models, while early-exiting is an effective approach to speedup the inference stage. In this work, we propose a novel decoding strategy named Hierarchical Skip Decoding (HSD) for efficient autoregressive text generation. Different from existing methods that require additional trainable components, HSD is a plug-and-play method applicable to autoregressive text generation models, it adaptively skips decoding layers in a hierarchical manner based on the current sequence length, thereby reducing computational workload and allocating computation resources. Comprehensive experiments on five text generation datasets with pre-trained language models demonstrate HSD's advantages in balancing efficiency and text quality. With almost half of the layers skipped, HSD can sustain 90% of the text quality compared to vanilla autoregressive d
&lt;/p&gt;</description></item><item><title>Stance Reasoner&#26159;&#19968;&#31181;&#21033;&#29992;&#26174;&#24335;&#25512;&#29702;&#21644;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#38646;-shot&#31038;&#20132;&#23186;&#20307;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#27169;&#22411;&#65292;&#24182;&#33021;&#26356;&#22909;&#22320;&#27178;&#36328;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14895</link><description>&lt;p&gt;
&#26041;&#20301;&#25512;&#29702;&#22120;&#65306;&#21033;&#29992;&#26174;&#24335;&#25512;&#29702;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#38646;-shot&#31435;&#22330;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stance Reasoner: Zero-Shot Stance Detection on Social Media with Explicit Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14895
&lt;/p&gt;
&lt;p&gt;
Stance Reasoner&#26159;&#19968;&#31181;&#21033;&#29992;&#26174;&#24335;&#25512;&#29702;&#21644;&#19990;&#30028;&#30693;&#35782;&#36827;&#34892;&#38646;-shot&#31038;&#20132;&#23186;&#20307;&#31435;&#22330;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20248;&#20110;&#24403;&#21069;&#39046;&#20808;&#27169;&#22411;&#65292;&#24182;&#33021;&#26356;&#22909;&#22320;&#27178;&#36328;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#20016;&#23500;&#30340;&#35266;&#28857;&#20869;&#23481;&#26469;&#28304;&#12290;&#31435;&#22330;&#26816;&#27979;&#20801;&#35768;&#33258;&#21160;&#20174;&#36825;&#20123;&#20869;&#23481;&#20013;&#25552;&#21462;&#29992;&#25143;&#23545;&#21508;&#31181;&#35805;&#39064;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#20851;&#27880;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#65292;&#21363;&#27169;&#22411;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#65288;a&#65289;&#23545;&#30446;&#26631;&#35805;&#39064;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#65288;b&#65289;&#23398;&#20064;&#21487;&#20197;&#29992;&#20110;&#26032;&#35805;&#39064;&#30340;&#36890;&#29992;&#25512;&#29702;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Stance Reasoner&#65292;&#19968;&#31181;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#19978;&#30340;&#26174;&#24335;&#25512;&#29702;&#26469;&#24341;&#23548;&#27169;&#22411;&#25512;&#26029;&#26377;&#20851;&#25991;&#26723;&#22312;&#30446;&#26631;&#19978;&#30340;&#31435;&#22330;&#30340;&#38646;-shot&#31435;&#22330;&#26816;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#19990;&#30028;&#30693;&#35782;&#30340;&#26469;&#28304;&#65292;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;Stance Reasoner&#22312;3&#20010;Twitter&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#21253;&#25324;&#23436;&#20840;&#30417;&#30563;&#30340;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#26356;&#22909;&#22320;&#27178;&#36328;&#30446;&#26631;&#36827;&#34892;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14895v1 Announce Type: cross  Abstract: Social media platforms are rich sources of opinionated content. Stance detection allows the automatic extraction of users' opinions on various topics from such content. We focus on zero-shot stance detection, where the model's success relies on (a) having knowledge about the target topic; and (b) learning general reasoning strategies that can be employed for new topics. We present Stance Reasoner, an approach to zero-shot stance detection on social media that leverages explicit reasoning over background knowledge to guide the model's inference about the document's stance on a target. Specifically, our method uses a pre-trained language model as a source of world knowledge, with the chain-of-thought in-context learning approach to generate intermediate reasoning steps. Stance Reasoner outperforms the current state-of-the-art models on 3 Twitter datasets, including fully supervised models. It can better generalize across targets, while a
&lt;/p&gt;</description></item><item><title>AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;</title><link>https://arxiv.org/abs/2403.14888</link><description>&lt;p&gt;
AutoRE&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
AutoRE: Document-Level Relation Extraction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14888
&lt;/p&gt;
&lt;p&gt;
AutoRE &#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF&#30340;&#26032;&#39062;&#20851;&#31995;&#25277;&#21462;&#33539;&#24335;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#20998;&#24067;&#22312;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#33021;&#21147;&#65292;&#36825;&#28608;&#21169;&#30528;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#21033;&#29992;&#23427;&#20204;&#36827;&#34892;&#20449;&#24687;&#25277;&#21462;(IE)&#20219;&#21153;&#65292;&#21253;&#25324;&#20851;&#31995;&#25277;&#21462;(RE)&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#35774;&#35745;&#29992;&#20110;&#21477;&#23376;&#32423;&#20851;&#31995;&#25277;&#21462;(SentRE)&#20219;&#21153;&#65292;&#36825;&#36890;&#24120;&#28085;&#30422;&#20102;&#21333;&#20010;&#21477;&#23376;&#20869;&#30340;&#19968;&#32452;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#19968;&#20123;&#26041;&#27861;&#37319;&#29992;&#23558;&#20851;&#31995;&#20316;&#20026;&#20505;&#36873;&#36873;&#25321;&#38598;&#25104;&#21040;&#25552;&#31034;&#27169;&#26495;&#20013;&#30340;&#26041;&#24335;&#65292;&#23548;&#33268;&#22312;&#22788;&#29702;&#20998;&#24067;&#22312;&#32473;&#23450;&#25991;&#26723;&#20013;&#30340;&#22810;&#20010;&#20851;&#31995;&#21644;&#19977;&#20803;&#32452;&#20107;&#23454;&#26102;&#25928;&#29575;&#20302;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#22312;&#22788;&#29702;&#25991;&#26723;&#32423;&#20851;&#31995;&#25277;&#21462;(DocRE)&#20219;&#21153;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoRE&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;DocRE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;RHF(Re
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14888v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated exceptional abilities in comprehending and generating text, motivating numerous researchers to utilize them for Information Extraction (IE) purposes, including Relation Extraction (RE). Nonetheless, most existing methods are predominantly designed for Sentence-level Relation Extraction (SentRE) tasks, which typically encompass a restricted set of relations and triplet facts within a single sentence. Furthermore, certain approaches resort to treating relations as candidate choices integrated into prompt templates, leading to inefficient processing and suboptimal performance when tackling Document-Level Relation Extraction (DocRE) tasks, which entail handling multiple relations and triplet facts distributed across a given document, posing distinct challenges. To overcome these limitations, we introduce AutoRE, an end-to-end DocRE model that adopts a novel RE extraction paradigm named RHF (Re
&lt;/p&gt;</description></item><item><title>VidLA &#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#20351;&#29992;&#20998;&#23618;&#25968;&#25454;&#20196;&#29260;&#26469;&#25429;&#25417;&#30701;&#31243;&#21644;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25104;&#21151;&#34701;&#21512;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14870</link><description>&lt;p&gt;
VidLA: &#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
VidLA: Video-Language Alignment at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14870
&lt;/p&gt;
&lt;p&gt;
VidLA &#25552;&#20986;&#20102;&#19968;&#31181;&#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#31616;&#21270;&#32593;&#32476;&#26550;&#26500;&#21644;&#20351;&#29992;&#20998;&#23618;&#25968;&#25454;&#20196;&#29260;&#26469;&#25429;&#25417;&#30701;&#31243;&#21644;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#20174;&#32780;&#25104;&#21151;&#34701;&#21512;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VidLA&#65292;&#19968;&#31181;&#29992;&#20110;&#35268;&#27169;&#21270;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20197;&#24448;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#26041;&#27861;&#30340;&#20004;&#20010;&#20027;&#35201;&#23616;&#38480;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#27809;&#26377;&#25429;&#25417;&#21040;&#30701;&#31243;&#21644;&#38271;&#31243;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#37319;&#29992;&#22797;&#26434;&#30340;&#20998;&#23618;&#28145;&#24230;&#32593;&#32476;&#26550;&#26500;&#65292;&#38590;&#20197;&#19982;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#38598;&#25104;&#12290;&#20026;&#20102;&#26377;&#25928;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#20445;&#25345;&#32593;&#32476;&#26550;&#26500;&#31616;&#21333;&#65292;&#20351;&#29992;&#19968;&#32452;&#22312;&#19981;&#21516;&#26102;&#38388;&#20998;&#36776;&#29575;&#19979;&#20197;&#20998;&#23618;&#26041;&#24335;&#36816;&#34892;&#30340;&#25968;&#25454;&#20196;&#29260;&#65292;&#20174;&#32780;&#32771;&#34385;&#35270;&#39057;&#30340;&#26102;&#38388;&#20998;&#23618;&#24615;&#36136;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;&#21452;&#22612;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22522;&#30784;&#27169;&#22411;&#21021;&#22987;&#21270;&#25105;&#20204;&#30340;&#35270;&#39057;-&#35821;&#35328;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#26368;&#32456;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14870v1 Announce Type: cross  Abstract: In this paper, we propose VidLA, an approach for video-language alignment at scale. There are two major limitations of previous video-language alignment approaches. First, they do not capture both short-range and long-range temporal dependencies and typically employ complex hierarchical deep network architectures that are hard to integrate with existing pretrained image-text foundation models. To effectively address this limitation, we instead keep the network architecture simple and use a set of data tokens that operate at different temporal resolutions in a hierarchical manner, accounting for the temporally hierarchical nature of videos. By employing a simple two-tower architecture, we are able to initialize our video-language model with pretrained image-text foundation models, thereby boosting the final performance. Second, existing video-language alignment works struggle due to the lack of semantically aligned large-scale training 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.14859</link><description>&lt;p&gt;
&#22312;&#22522;&#30784;&#27169;&#22411;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27604;&#36739;&#21487;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Comparing Plausibility Estimates in Base and Instruction-Tuned Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14859
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#26368;&#21487;&#38752;&#30340;&#21477;&#23376;&#21487;&#20449;&#24230;&#25351;&#26631;&#65292;&#20294;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#20248;&#30340;LLM&#21487;&#20197;&#21709;&#24212;&#26126;&#30830;&#21046;&#23450;&#20026;&#25552;&#31034;&#30340;&#26597;&#35810;&#65292;&#36825;&#26497;&#22823;&#22320;&#20419;&#36827;&#20102;&#19982;&#20154;&#31867;&#29992;&#25143;&#30340;&#20132;&#20114;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#33021;&#24182;&#19981;&#24635;&#26159;&#33021;&#22815;&#21033;&#29992;LLM&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#38544;&#24335;&#30693;&#35782;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;LLM&#20013;&#35821;&#20041;&#21487;&#20449;&#24230;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#65288;a&#65289;&#26126;&#30830;&#25552;&#31034;&#21644;&#65288;b&#65289;&#30452;&#25509;&#35835;&#21462;&#27169;&#22411;&#20998;&#37197;&#32473;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#30340;&#38544;&#24335;&#20272;&#35745;&#65292;&#22312;&#33521;&#35821;&#21477;&#23376;&#21487;&#20449;&#24230;&#20219;&#21153;&#20013;&#27604;&#36739;&#20102;&#22522;&#30784;&#21644;&#25351;&#20196;&#35843;&#20248;LLM&#30340;&#24615;&#33021;&#12290;&#23454;&#39564;1&#34920;&#26126;&#65292;&#36328;&#27169;&#22411;&#26550;&#26500;&#21644;&#21487;&#20449;&#24230;&#25968;&#25454;&#38598;&#65292;&#65288;i&#65289;&#23545;&#25968;&#20284;&#28982;&#65288;LL&#65289;&#20998;&#25968;&#26159;&#21477;&#23376;&#21487;&#20449;&#24230;&#26368;&#21487;&#38752;&#30340;&#25351;&#26631;&#65292;&#38646;&#29031;&#23556;&#25552;&#31034;&#20135;&#29983;&#19981;&#19968;&#33268;&#19988;&#36890;&#24120;&#25928;&#26524;&#19981;&#20339;&#30340;&#32467;&#26524;&#65307;&#65288;ii&#65289;&#22522;&#20110;LL&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#20154;&#31867;&#34920;&#29616;&#65307;&#65288;iii&#65289;&#25351;&#20196;&#35843;&#20248;&#27169;&#22411;&#26377;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14859v1 Announce Type: cross  Abstract: Instruction-tuned LLMs can respond to explicit queries formulated as prompts, which greatly facilitates interaction with human users. However, prompt-based approaches might not always be able to tap into the wealth of implicit knowledge acquired by LLMs during pre-training. This paper presents a comprehensive study of ways to evaluate semantic plausibility in LLMs. We compare base and instruction-tuned LLM performance on an English sentence plausibility task via (a) explicit prompting and (b) implicit estimation via direct readout of the probabilities models assign to strings. Experiment 1 shows that, across model architectures and plausibility datasets, (i) log likelihood ($\textit{LL}$) scores are the most reliable indicator of sentence plausibility, with zero-shot prompting yielding inconsistent and typically poor results; (ii) $\textit{LL}$-based performance is still inferior to human performance; (iii) instruction-tuned models hav
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32763;&#35793;&#25968;&#25454;&#36741;&#21161;&#24418;&#24577;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23383;&#31526;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;&#39640;&#36164;&#28304;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#34920;&#31034;&#65292;&#23454;&#29616;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.14840</link><description>&lt;p&gt;
TAMS: &#32763;&#35793;&#36741;&#21161;&#30340;&#24418;&#24577;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
TAMS: Translation-Assisted Morphological Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14840
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32763;&#35793;&#25968;&#25454;&#36741;&#21161;&#24418;&#24577;&#20998;&#21106;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23383;&#31526;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#21450;&#39044;&#20808;&#35757;&#32451;&#30340;&#39640;&#36164;&#28304;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#34920;&#31034;&#65292;&#23454;&#29616;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#36229;&#36234;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35268;&#33539;&#24418;&#24577;&#20998;&#21106;&#26159;&#23558;&#21333;&#35789;&#20998;&#26512;&#20026;&#20854;&#26500;&#25104;&#24418;&#24577;&#32032;&#30340;&#26631;&#20934;&#65288;&#20063;&#31216;&#20026;&#24213;&#23618;&#65289;&#24418;&#24335;&#30340;&#36807;&#31243;&#12290;&#36825;&#26159;&#35821;&#35328;&#25991;&#26723;&#32534;&#21046;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#65292;NLP &#31995;&#32479;&#26377;&#26395;&#26174;&#33879;&#21152;&#24555;&#36825;&#19968;&#22788;&#29702;&#36807;&#31243;&#12290;&#22312;&#20856;&#22411;&#30340;&#35821;&#35328;&#25991;&#26723;&#32534;&#21046;&#29615;&#22659;&#20013;&#65292;&#35268;&#33539;&#24418;&#24577;&#20998;&#21106;&#30340;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#35757;&#32451;&#20986;&#39640;&#36136;&#37327;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#32763;&#35793;&#25968;&#25454;&#36890;&#24120;&#26356;&#20026;&#20016;&#23500;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23581;&#35797;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#35268;&#33539;&#20998;&#21106;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23383;&#31526;&#32423;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#26469;&#33258;&#39044;&#20808;&#35757;&#32451;&#30340;&#39640;&#36164;&#28304;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#34920;&#31034;&#20316;&#20026;&#39069;&#22806;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36229;&#20302;&#36164;&#28304;&#35774;&#32622;&#20013;&#20248;&#20110;&#22522;&#32447;&#65292;&#20294;&#22312;&#20855;&#26377;&#26356;&#22810;&#25968;&#25454;&#30340;&#35757;&#32451;&#20998;&#21106;&#19978;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#32467;&#26524;&#12290;&#38656;&#35201;&#36827;&#19968;&#27493;&#24037;&#20316;&#25165;&#33021;&#20351;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14840v1 Announce Type: new  Abstract: Canonical morphological segmentation is the process of analyzing words into the standard (aka underlying) forms of their constituent morphemes. This is a core task in language documentation, and NLP systems have the potential to dramatically speed up this process. But in typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage this data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. While further work is needed to make
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.14814</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#20250;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
The opportunities and risks of large language models in mental health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14814
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26377;&#26395;&#25552;&#20379;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#24212;&#27880;&#24847;&#20854;&#24212;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#39118;&#38505;&#65292;&#24182;&#31215;&#26497;&#37319;&#21462;&#31574;&#30053;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#30340;&#21457;&#29983;&#29575;&#27491;&#22312;&#19978;&#21319;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#29616;&#26377;&#30340;&#24515;&#29702;&#20445;&#20581;&#27169;&#24335;&#26080;&#27861;&#20805;&#20998;&#25193;&#23637;&#20197;&#28385;&#36275;&#38656;&#27714;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20154;&#20204;&#23545;&#23427;&#20204;&#20855;&#26377;&#21019;&#36896;&#26032;&#39062;&#12289;&#22823;&#35268;&#27169;&#35299;&#20915;&#26041;&#26696;&#20197;&#25903;&#25345;&#24515;&#29702;&#20581;&#24247;&#30340;&#25215;&#35834;&#24863;&#21040;&#20048;&#35266;&#12290;&#23613;&#31649;&#23427;&#20204;&#36824;&#22788;&#20110;&#21021;&#26399;&#38454;&#27573;&#65292;LLMs&#24050;&#34987;&#24212;&#29992;&#20110;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#20219;&#21153;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#24050;&#26377;&#25991;&#29486;&#20013;&#20851;&#20110;&#21033;&#29992;LLMs&#25552;&#20379;&#24515;&#29702;&#20581;&#24247;&#25945;&#32946;&#12289;&#35780;&#20272;&#21644;&#24178;&#39044;&#30340;&#21162;&#21147;&#65292;&#24182;&#31361;&#20986;&#20102;&#27599;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#30340;&#20851;&#38190;&#26426;&#20250;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#23558;LLMs&#24212;&#29992;&#20110;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#25152;&#20276;&#38543;&#30340;&#39118;&#38505;&#65292;&#24182;&#40723;&#21169;&#37319;&#29992;&#31574;&#30053;&#26469;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#12290;&#23545;&#20110;&#24515;&#29702;&#20581;&#24247;&#25903;&#25345;&#30340;&#36843;&#20999;&#38656;&#27714;&#24517;&#39035;&#19982;&#36127;&#36131;&#20219;&#30340;&#24515;&#29702;&#20581;&#24247;LLMs&#30340;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#30456;&#24179;&#34913;&#12290;&#29305;&#21035;&#20851;&#38190;&#30340;&#26159;&#30830;&#20445;&#24515;&#29702;&#20581;&#24247;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14814v1 Announce Type: cross  Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental he
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#20154;&#31867;&#21028;&#26029;&#21475;&#35821;&#23545;&#35805;&#35805;&#35821;&#20043;&#38388;&#35821;&#29992;&#30456;&#20284;&#24230;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#35780;&#20998;&#34920;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30456;&#20284;&#24230;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.14808</link><description>&lt;p&gt;
&#19968;&#32452;&#20851;&#20110;&#21475;&#35821;&#23545;&#35805;&#35805;&#35821;&#30340;&#23454;&#29992;&#30456;&#20284;&#24230;&#21028;&#26029;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
A Collection of Pragmatic-Similarity Judgments over Spoken Dialog Utterances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14808
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#20154;&#31867;&#21028;&#26029;&#21475;&#35821;&#23545;&#35805;&#35805;&#35821;&#20043;&#38388;&#35821;&#29992;&#30456;&#20284;&#24230;&#30340;&#38598;&#21512;&#65292;&#36890;&#36807;&#35780;&#20998;&#34920;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30456;&#20284;&#24230;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#27979;&#37327;&#35805;&#35821;&#20043;&#38388;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#23545;&#20110;&#35757;&#32451;&#35821;&#38899;&#21512;&#25104;&#22120;&#12289;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#20197;&#21450;&#35780;&#20272;&#23398;&#20064;&#32773;&#30340;&#20135;&#20986;&#38750;&#24120;&#23453;&#36149;&#12290;&#23613;&#31649;&#24050;&#32463;&#23384;&#22312;&#35821;&#20041;&#30456;&#20284;&#24230;&#21644;&#38901;&#24459;&#30456;&#20284;&#24230;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#20294;&#30446;&#21069;&#23578;&#26080;&#20851;&#20110;&#35821;&#29992;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35757;&#32451;&#36825;&#26679;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#20154;&#31867;&#21028;&#26029;&#21475;&#35821;&#23545;&#35805;&#35805;&#35821;&#20043;&#38388;&#35821;&#29992;&#30456;&#20284;&#24230;&#30340;&#38598;&#21512;&#12290;&#27599;&#23545;&#35805;&#35821;&#21253;&#25324;&#19968;&#27573;&#26469;&#33258;&#24405;&#38899;&#23545;&#35805;&#30340;&#35805;&#35821;&#21644;&#35813;&#35805;&#35821;&#30340;&#20877;&#29616;&#12290;&#20877;&#29616;&#26159;&#22312;&#35774;&#35745;&#20102;&#21508;&#31181;&#26465;&#20214;&#20197;&#21019;&#24314;&#19981;&#21516;&#31243;&#24230;&#30456;&#20284;&#24230;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#30340;&#12290;&#27599;&#23545;&#35805;&#35821;&#30001;6&#21040;9&#20301;&#35780;&#22996;&#22312;&#36830;&#32493;&#23610;&#24230;&#19978;&#35780;&#20998;&#12290;&#33521;&#35821;&#30340;&#35780;&#22996;&#38388;&#24179;&#22343;&#30456;&#20851;&#24615;&#39640;&#36798;0.72&#65292;&#32780;&#35199;&#29677;&#29273;&#35821;&#20026;0.66&#12290;&#25105;&#20204;&#22312;https://github.com/divettemarco/PragSim &#19978;&#25552;&#20379;&#20102;&#36825;&#20123;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14808v1 Announce Type: new  Abstract: Automatic measures of similarity between utterances are invaluable for training speech synthesizers, evaluating machine translation, and assessing learner productions. While there exist measures for semantic similarity and prosodic similarity, there are as yet none for pragmatic similarity. To enable the training of such measures, we developed the first collection of human judgments of pragmatic similarity between utterance pairs. Each pair consisting of an utterance extracted from a recorded dialog and a re-enactment of that utterance. Re-enactments were done under various conditions designed to create a variety of degrees of similarity. Each pair was rated on a continuous scale by 6 to 9 judges. The average inter-judge correlation was as high as 0.72 for English and 0.66 for Spanish. We make this data available at https://github.com/divettemarco/PragSim .
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.14783</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;VQA&#65306;&#25506;&#32034;&#38646;&#26679;&#26412;&#35270;&#35273;&#38382;&#31572;&#20013;&#30340;&#22810;&#26234;&#20307;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent VQA: Exploring Multi-Agent Foundation Models in Zero-Shot Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#24037;&#20855;&#65292;&#20811;&#26381;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#35752;&#20102;&#22522;&#30784;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#22810;&#26234;&#20307;&#31995;&#32479;&#65292;&#21629;&#21517;&#20026;&#22810;&#26234;&#20307;VQA&#65292;&#36890;&#36807;&#20351;&#29992;&#19987;&#38376;&#30340;&#26234;&#20307;&#20316;&#20026;&#24037;&#20855;&#65292;&#20197;&#20811;&#26381;&#22522;&#30784;&#27169;&#22411;&#22312;&#30446;&#26631;&#26816;&#27979;&#21644;&#35745;&#25968;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20391;&#37325;&#20110;&#22312;&#19981;&#23545;&#20854;&#36827;&#34892;&#29305;&#23450;VQA&#25968;&#25454;&#38598;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#22312;&#24320;&#25918;&#19990;&#30028;&#20013;&#26356;&#21152;&#23454;&#29992;&#21644;&#31283;&#20581;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#25552;&#20986;&#20102;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#65292;&#24182;&#31361;&#20986;&#20102;&#19968;&#20123;&#22833;&#36133;&#26696;&#20363;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14783v1 Announce Type: cross  Abstract: This work explores the zero-shot capabilities of foundation models in Visual Question Answering (VQA) tasks. We propose an adaptive multi-agent system, named Multi-Agent VQA, to overcome the limitations of foundation models in object detection and counting by using specialized agents as tools. Unlike existing approaches, our study focuses on the system's performance without fine-tuning it on specific VQA datasets, making it more practical and robust in the open world. We present preliminary experimental results under zero-shot scenarios and highlight some failure cases, offering new directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;</title><link>https://arxiv.org/abs/2403.14774</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Adversarial Prompt Learning on Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#26377;&#38480;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#65292;&#26174;&#33879;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#24182;&#36890;&#36807;&#31471;&#21040;&#31471;&#23398;&#20064;&#23545;&#25239;&#24615;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#24494;&#19981;&#21487;&#35265;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#21463;&#21040;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35270;&#35273;&#29305;&#24449;&#19982;&#25991;&#26412;&#30417;&#30563;&#23545;&#40784;&#26469;&#23454;&#29616;&#38646;&#26679;&#26412;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#30001;&#20110;&#21253;&#25324;&#37325;&#22823;&#36866;&#24212;&#25104;&#26412;&#12289;&#27425;&#20248;&#25991;&#26412;&#30417;&#30563;&#21644;&#26410;&#21463;&#25511;&#21046;&#30340;&#33258;&#28982;&#27867;&#21270;&#33021;&#21147;&#22312;&#20869;&#30340;&#22810;&#20010;&#38382;&#39064;&#65292;&#23427;&#20204;&#20173;&#28982;&#19981;&#23613;&#20154;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23569;&#26679;&#26412;&#23545;&#25239;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#25968;&#25454;&#35843;&#25972;&#36755;&#20837;&#24207;&#21015;&#20351;&#24471;&#23545;&#25239;&#40065;&#26834;&#24615;&#24471;&#21040;&#26174;&#33879;&#25552;&#21319;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#23545;&#25239;&#30456;&#20851;&#30340;&#25991;&#26412;&#30417;&#30563;&#65292;&#35813;&#30417;&#30563;&#26159;&#20174;&#23545;&#25239;&#24615;&#31034;&#20363;&#20013;&#31471;&#21040;&#31471;&#23398;&#20064;&#30340;&#65292;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#22810;&#27169;&#24577;&#29305;&#24449;&#19968;&#33268;&#24615;&#24182;&#40723;&#21169;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14774v1 Announce Type: cross  Abstract: The vulnerability of deep neural networks to imperceptible adversarial perturbations has attracted widespread attention. Inspired by the success of vision-language foundation models, previous efforts achieved zero-shot adversarial robustness by aligning adversarial visual features with text supervision. However, in practice, they are still unsatisfactory due to several issues, including heavy adaptation cost, suboptimal text supervision, and uncontrolled natural generalization capacity. In this paper, to address these issues, we propose a few-shot adversarial prompt framework where adapting input sequences with limited data makes significant adversarial robustness improvement. Specifically, we achieve this by providing adversarially correlated text supervision that is end-to-end learned from adversarial examples. We also propose a novel training objective that enhances the consistency of multi-modal features while encourages differenti
&lt;/p&gt;</description></item><item><title>StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;</title><link>https://arxiv.org/abs/2403.14773</link><description>&lt;p&gt;
StreamingT2V: &#19968;&#31181;&#19968;&#33268;&#12289;&#21160;&#24577;&#21644;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#38271;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
StreamingT2V: Consistent, Dynamic, and Extendable Long Video Generation from Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14773
&lt;/p&gt;
&lt;p&gt;
StreamingT2V&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#38271;&#35270;&#39057;&#65292;&#21487;&#20197;&#20135;&#29983;80&#12289;240&#12289;600&#12289;1200&#24103;&#29978;&#33267;&#26356;&#22810;&#24103;&#30340;&#35270;&#39057;&#65292;&#24182;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449; &#25688;&#35201;: &#25991;&#26412;&#21040;&#35270;&#39057;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#36981;&#24490;&#25991;&#26412;&#25351;&#20196;&#30340;&#39640;&#36136;&#37327;&#35270;&#39057;&#65292;&#20351;&#24471;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#20010;&#24615;&#21270;&#20869;&#23481;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#38598;&#20013;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#30701;&#35270;&#39057;&#65288;&#36890;&#24120;&#20026;16&#25110;24&#24103;&#65289;&#65292;&#24403;&#22825;&#30495;&#22320;&#25193;&#23637;&#21040;&#38271;&#35270;&#39057;&#21512;&#25104;&#30340;&#24773;&#20917;&#26102;&#65292;&#36890;&#24120;&#20250;&#20986;&#29616;&#30828;&#35009;&#21098;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;StreamingT2V&#65292;&#36825;&#26159;&#19968;&#31181;&#33258;&#22238;&#24402;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;80&#12289;240&#12289;600&#12289;1200&#25110;&#26356;&#22810;&#24103;&#30340;&#38271;&#35270;&#39057;&#65292;&#20855;&#26377;&#24179;&#28369;&#30340;&#36807;&#28193;&#12290;&#20027;&#35201;&#32452;&#20214;&#21253;&#25324;&#65306;&#65288;i&#65289;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#27880;&#24847;&#21147;&#27169;&#22359;&#65288;CAM&#65289;&#30340;&#30701;&#26399;&#35760;&#24518;&#22359;&#65292;&#36890;&#36807;&#27880;&#24847;&#26426;&#21046;&#23558;&#24403;&#21069;&#29983;&#25104;&#26465;&#20214;&#35774;&#32622;&#20026;&#20808;&#21069;&#22359;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#22359;&#36807;&#28193;&#65292;&#65288;ii&#65289;&#19968;&#31181;&#21517;&#20026;&#22806;&#35266;&#20445;&#23384;&#27169;&#22359;&#30340;&#38271;&#26399;&#35760;&#24518;&#22359;&#65292;&#20174;&#31532;&#19968;&#20010;&#35270;&#39057;&#22359;&#20013;&#25552;&#21462;&#39640;&#32423;&#22330;&#26223;&#21644;&#23545;&#35937;&#29305;&#24449;&#65292;&#20197;&#38450;&#27490;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14773v1 Announce Type: cross  Abstract: Text-to-video diffusion models enable the generation of high-quality videos that follow text instructions, making it easy to create diverse and individual content. However, existing approaches mostly focus on high-quality short video generation (typically 16 or 24 frames), ending up with hard-cuts when naively extended to the case of long video synthesis. To overcome these limitations, we introduce StreamingT2V, an autoregressive approach for long video generation of 80, 240, 600, 1200 or more frames with smooth transitions. The key components are:(i) a short-term memory block called conditional attention module (CAM), which conditions the current generation on the features extracted from the previous chunk via an attentional mechanism, leading to consistent chunk transitions, (ii) a long-term memory block called appearance preservation module, which extracts high-level scene and object features from the first video chunk to prevent th
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.14734</link><description>&lt;p&gt;
&#19968;&#39033;&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#30340;&#35843;&#26597;&#65306;&#33539;&#24335;&#12289;&#36827;&#23637;&#19982;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14734
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20195;&#30721;&#26234;&#33021;&#39046;&#22495;&#30340;&#35843;&#26597;&#31995;&#32479;&#22238;&#39038;&#20102;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21644;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#65292;&#31361;&#20986;&#20102;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#21644;&#25216;&#26415;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#31070;&#32463;&#20195;&#30721;&#26234;&#33021;--&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#20248;&#21270;&#20195;&#30721;--&#22312;&#25972;&#20010;&#31038;&#20250;&#19978;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#21487;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#21644;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#36825;&#19968;&#39046;&#22495;&#22312;&#36807;&#21435;&#20960;&#24180;&#24341;&#36215;&#20102;&#20004;&#20010;&#30740;&#31350;&#31038;&#21306;&#30740;&#31350;&#20154;&#21592;&#30340;&#26497;&#22823;&#20851;&#27880;&#12290;&#26412;&#35843;&#26597;&#31995;&#32479;&#22320;&#21644;&#25353;&#26102;&#38388;&#39034;&#24207;&#22238;&#39038;&#20102;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#30340;&#36827;&#23637;&#65292;&#21253;&#25324;50&#22810;&#31181;&#20195;&#34920;&#24615;&#27169;&#22411;&#21450;&#20854;&#21464;&#20307;&#12289;20&#22810;&#31181;&#20219;&#21153;&#31867;&#21035;&#20197;&#21450;&#36229;&#36807;680&#39033;&#30456;&#20851;&#20316;&#21697;&#12290;&#25105;&#20204;&#36981;&#24490;&#21382;&#21490;&#36827;&#23637;&#65292;&#36319;&#36394;&#19981;&#21516;&#30740;&#31350;&#38454;&#27573;&#30340;&#33539;&#24335;&#36716;&#21464;&#65288;&#20363;&#22914;&#65292;&#20174;&#20351;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#23545;&#20195;&#30721;&#24314;&#27169;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65289;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;&#19981;&#21516;&#38454;&#27573;&#28085;&#30422;&#30340;&#27169;&#22411;&#12289;&#20219;&#21153;&#21644;&#35780;&#20272;&#30340;&#20027;&#35201;&#25216;&#26415;&#36716;&#21464;&#12290;&#23545;&#20110;&#24212;&#29992;&#65292;&#25105;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14734v1 Announce Type: cross  Abstract: Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MulCanon&#26469;&#22788;&#29702;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.14733</link><description>&lt;p&gt;
&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#36827;&#34892;&#24320;&#25918;&#30693;&#35782;&#24211;&#35268;&#33539;&#21270;
&lt;/p&gt;
&lt;p&gt;
Open Knowledge Base Canonicalization with Multi-task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14733
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;MulCanon&#26469;&#22788;&#29702;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#35268;&#33539;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#24320;&#25918;&#30693;&#35782;&#24211;&#65288;OKB&#65289;&#30340;&#26500;&#24314;&#23545;&#20110;&#35832;&#22810;&#22522;&#20110;&#30693;&#35782;&#30340;&#32593;&#32476;&#24212;&#29992;&#22914;&#32593;&#32476;&#25628;&#32034;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;OKB&#20013;&#30340;&#21517;&#35789;&#30701;&#35821;&#21644;&#20851;&#31995;&#30701;&#35821;&#24448;&#24448;&#23384;&#22312;&#20887;&#20313;&#21644;&#27495;&#20041;&#65292;&#36825;&#38656;&#35201;&#23545;OKB&#36827;&#34892;&#35268;&#33539;&#21270;&#30340;&#30740;&#31350;&#12290;&#24403;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#35774;&#35745;&#20808;&#36827;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#24182;&#20351;&#29992;&#30693;&#35782;&#22270;&#23884;&#20837;&#65288;KGE&#65289;&#36827;&#19968;&#27493;&#20419;&#36827;&#35268;&#33539;&#21270;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#26410;&#33021;&#20805;&#20998;&#21033;&#29992;&#32858;&#31867;&#21644;KGE&#23398;&#20064;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#19988;&#20026;&#36825;&#20123;&#23376;&#20219;&#21153;&#35774;&#35745;&#30340;&#26041;&#27861;&#26159;&#27425;&#20248;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MulCanon&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;OKB&#30340;&#35268;&#33539;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#22312;&#36719;&#32858;&#31867;&#36807;&#31243;&#20013;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#36827;&#21517;&#35789;&#30701;&#35821;&#30340;&#34920;&#31034;&#65292;&#24102;&#26469;&#26356;&#20934;&#30830;&#30340;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14733v1 Announce Type: new  Abstract: The construction of large open knowledge bases (OKBs) is integral to many knowledge-driven applications on the world wide web such as web search. However, noun phrases and relational phrases in OKBs often suffer from redundancy and ambiguity, which calls for the investigation on OKB canonicalization. Current solutions address OKB canonicalization by devising advanced clustering algorithms and using knowledge graph embedding (KGE) to further facilitate the canonicalization process. Nevertheless, these works fail to fully exploit the synergy between clustering and KGE learning, and the methods designed for these subtasks are sub-optimal. To this end, we put forward a multi-task learning framework, namely MulCanon, to tackle OKB canonicalization. In addition, diffusion model is used in the soft clustering process to improve the noun phrase representations with neighboring information, which can lead to more accurate representations. MulCano
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#65288;RJA&#65289;&#21644;Metropolis-Hasting&#20462;&#25913;&#32553;&#20943;&#65288;MMR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#20998;&#21035;&#25913;&#21892;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14731</link><description>&lt;p&gt;
&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#23545;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#20462;&#25913;&#32553;&#20943;
&lt;/p&gt;
&lt;p&gt;
Reversible Jump Attack to Textual Classifiers with Modification Reduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14731
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#65288;RJA&#65289;&#21644;Metropolis-Hasting&#20462;&#25913;&#32553;&#20943;&#65288;MMR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;&#24182;&#20998;&#21035;&#25913;&#21892;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#23545;&#25239;&#26679;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#29616;&#26377;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#30340;&#25216;&#26415;&#36890;&#24120;&#30001;&#30830;&#23450;&#24615;&#30340;&#23618;&#27425;&#35268;&#21017;&#39537;&#21160;&#65292;&#36825;&#20123;&#35268;&#21017;&#23545;&#26368;&#20248;&#23545;&#25239;&#26679;&#26412;&#27627;&#19981;&#20851;&#24515;&#65292;&#36890;&#24120;&#23548;&#33268;&#23545;&#25239;&#26679;&#26412;&#22312;&#21464;&#21270;&#24133;&#24230;&#21644;&#25915;&#20987;&#25104;&#21151;&#20043;&#38388;&#23384;&#22312;&#27425;&#20248;&#24179;&#34913;&#12290;&#22312;&#27492;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#21487;&#36870;&#36339;&#21160;&#25915;&#20987;&#65288;RJA&#65289;&#21644;Metropolis-Hasting&#20462;&#25913;&#32553;&#20943;&#65288;MMR&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#39640;&#24230;&#26377;&#25928;&#30340;&#23545;&#25239;&#31034;&#20363;&#24182;&#20998;&#21035;&#25913;&#21892;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;RJA&#21033;&#29992;&#26032;&#39062;&#30340;&#38543;&#26426;&#21270;&#26426;&#21046;&#26469;&#25193;&#22823;&#25628;&#32034;&#31354;&#38388;&#65292;&#26377;&#25928;&#36866;&#24212;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#22810;&#20010;&#25200;&#21160;&#35789;&#27719;&#12290;&#21033;&#29992;&#29983;&#25104;&#30340;&#23545;&#25239;&#31034;&#20363;&#65292;MMR&#24212;&#29992;Metropolis-Hasting&#37319;&#26679;&#22120;&#20197;&#22686;&#24378;&#23545;&#25239;&#31034;&#20363;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14731v1 Announce Type: cross  Abstract: Recent studies on adversarial examples expose vulnerabilities of natural language processing (NLP) models. Existing techniques for generating adversarial examples are typically driven by deterministic hierarchical rules that are agnostic to the optimal adversarial examples, a strategy that often results in adversarial samples with a suboptimal balance between magnitudes of changes and attack successes. To this end, in this research we propose two algorithms, Reversible Jump Attack (RJA) and Metropolis-Hasting Modification Reduction (MMR), to generate highly effective adversarial examples and to improve the imperceptibility of the examples, respectively. RJA utilizes a novel randomization mechanism to enlarge the search space and efficiently adapts to a number of perturbed words for adversarial examples. With these generated adversarial examples, MMR applies the Metropolis-Hasting sampler to enhance the imperceptibility of adversarial e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20262;&#29702;&#21644;&#20844;&#24179;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#36824;&#20284;&#20046;&#25918;&#22823;&#20102;&#36825;&#20123;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14727</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#21463;&#20445;&#25252;&#32676;&#20307;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Protected group bias and stereotypes in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20262;&#29702;&#21644;&#20844;&#24179;&#39046;&#22495;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#36824;&#20284;&#20046;&#25918;&#22823;&#20102;&#36825;&#20123;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#25171;&#30772;&#35768;&#22810;&#26368;&#26032;&#25216;&#26415;&#22522;&#20934;&#65292;&#26412;&#25991;&#35843;&#26597;&#20102;&#23427;&#20204;&#22312;&#20262;&#29702;&#21644;&#20844;&#24179;&#39046;&#22495;&#30340;&#34892;&#20026;&#65292;&#37325;&#28857;&#20851;&#27880;&#21463;&#20445;&#25252;&#32676;&#20307;&#20559;&#35265;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#37096;&#20998;&#30740;&#31350;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24449;&#38598;&#20102;&#25551;&#36848;&#26469;&#33258;&#19981;&#21516;&#21463;&#20445;&#25252;&#32676;&#20307;&#65288;&#21253;&#25324;&#24615;&#21035;&#12289;&#24615;&#21462;&#21521;&#12289;&#23447;&#25945;&#21644;&#31181;&#26063;&#65289;&#20010;&#20154;&#32844;&#19994;&#30340;&#21477;&#23376;&#24310;&#32493;&#65307;&#20854;&#27425;&#65292;&#25105;&#20204;&#35753;&#27169;&#22411;&#29983;&#25104;&#20851;&#20110;&#25317;&#26377;&#19981;&#21516;&#31867;&#22411;&#32844;&#19994;&#30340;&#20010;&#20154;&#30340;&#25925;&#20107;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#27454;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340; &gt;10k &#20010;&#21477;&#23376;&#24310;&#32493;&#65292;&#21463;&#21040;&#20154;&#31867;&#26631;&#27880;&#12290;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#34987;&#36793;&#32536;&#21270;&#32676;&#20307;&#20013;&#23384;&#22312;&#20559;&#35265;&#65292;&#23588;&#20854;&#22312;&#24615;&#21035;&#21644;&#24615;&#21462;&#21521;&#39046;&#22495;&#65292;&#20197;&#21450;&#22312;&#27169;&#22411;&#29983;&#25104;&#20013;&#23384;&#22312;&#35199;&#26041;&#20559;&#35265;&#12290;&#27169;&#22411;&#19981;&#20165;&#21453;&#26144;&#20102;&#31038;&#20250;&#20559;&#35265;&#65292;&#36824;&#20284;&#20046;&#25918;&#22823;&#20102;&#36825;&#20123;&#20559;&#35265;&#12290;&#35813;&#27169;&#22411;&#23545;&#20110;&#19982;&#36793;&#32536;&#21270;&#32676;&#20307;&#30456;&#20851;&#30340;&#26597;&#35810;&#22238;&#22797;&#36807;&#20110;&#35880;&#24910;&#65292;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14727v1 Announce Type: cross  Abstract: As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect &gt;10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing re
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14725</link><description>&lt;p&gt;
Jailbreaking&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#26159;&#36890;&#36807;&#23450;&#20041;
&lt;/p&gt;
&lt;p&gt;
Jailbreaking is Best Solved by Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14725
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#20851;&#38190;&#26159;&#36890;&#36807;&#23450;&#20041;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#26469;&#36827;&#34892;&#38450;&#24481;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#25191;&#34892;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19978;"&#36234;&#29425;"&#25915;&#20987;&#30340;&#22686;&#22810;&#24341;&#21457;&#20102;&#22823;&#37327;&#38450;&#24481;&#24037;&#20316;&#65292;&#26088;&#22312;&#38450;&#27490;&#20135;&#29983;&#19981;&#33391;&#22238;&#24212;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#23457;&#35270;&#20102;&#38450;&#24481;&#31649;&#36947;&#30340;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#23450;&#20041;&#20309;&#20026;&#19981;&#23433;&#20840;&#36755;&#20986;&#65292;&#21644;&#65288;ii&#65289;&#36890;&#36807;&#36755;&#20837;&#22788;&#29702;&#25110;&#24494;&#35843;&#31561;&#26041;&#27861;&#26469;&#25191;&#34892;&#35813;&#23450;&#20041;&#12290;&#25105;&#20204;&#20005;&#37325;&#24576;&#30097;&#29616;&#26377;&#30340;&#25191;&#34892;&#26426;&#21046;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#23427;&#20204;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#19981;&#23433;&#20840;&#36755;&#20986;&#23450;&#20041;--&#21253;&#21547;&#21333;&#35789;"purple"&#30340;&#36755;&#20986;&#20063;&#26080;&#27861;&#38450;&#24481;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#36755;&#20986;&#36827;&#34892;&#21518;&#22788;&#29702;&#23545;&#20110;&#36825;&#26679;&#30340;&#23450;&#20041;&#26159;&#23436;&#20840;&#20581;&#22766;&#30340;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;&#22312;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#20013;&#30495;&#27491;&#30340;&#25361;&#25112;&#22312;&#20110;&#24471;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#19981;&#23433;&#20840;&#21709;&#24212;&#23450;&#20041;&#65306;&#27809;&#26377;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#20219;&#20309;&#25191;&#34892;&#31574;&#30053;&#37117;&#26080;&#27861;&#25104;&#21151;&#65292;&#20294;&#26377;&#20102;&#33391;&#22909;&#30340;&#23450;&#20041;&#65292;&#36755;&#20986;&#22788;&#29702;&#24050;&#32463;&#20316;&#20026;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14725v1 Announce Type: cross  Abstract: The rise of "jailbreak" attacks on language models has led to a flurry of defenses aimed at preventing the output of undesirable responses. In this work, we critically examine the two stages of the defense pipeline: (i) the definition of what constitutes unsafe outputs, and (ii) the enforcement of the definition via methods such as input processing or fine-tuning. We cast severe doubt on the efficacy of existing enforcement mechanisms by showing that they fail to defend even for a simple definition of unsafe outputs--outputs that contain the word "purple". In contrast, post-processing outputs is perfectly robust for such a definition. Drawing on our results, we present our position that the real challenge in defending jailbreaks lies in obtaining a good definition of unsafe responses: without a good definition, no enforcement strategy can succeed, but with a good definition, output processing already serves as a robust baseline albeit 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32858;&#28966;&#25216;&#26415;&#65292;&#19968;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#28304;&#26102;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#38752;&#30340;&#36755;&#20837;&#26469;&#28304;&#20449;&#21495;&#26469;&#38450;&#24481;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.14720</link><description>&lt;p&gt;
&#20351;&#29992;&#32858;&#28966;&#25216;&#26415;&#25269;&#24481;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Defending Against Indirect Prompt Injection Attacks With Spotlighting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14720
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32858;&#28966;&#25216;&#26415;&#65292;&#19968;&#31181;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20110;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#28304;&#26102;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25552;&#20379;&#21487;&#38752;&#30340;&#36755;&#20837;&#26469;&#28304;&#20449;&#21495;&#26469;&#38450;&#24481;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#34429;&#28982;&#24378;&#22823;&#65292;&#21364;&#26159;&#24314;&#31435;&#21644;&#35757;&#32451;&#29992;&#20110;&#22788;&#29702;&#21333;&#20010;&#25991;&#26412;&#36755;&#20837;&#30340;&#12290;&#22312;&#24120;&#35265;&#24212;&#29992;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#22810;&#20010;&#36755;&#20837;&#36830;&#25509;&#22312;&#19968;&#36215;&#24418;&#25104;&#21333;&#20010;&#25991;&#26412;&#27969;&#26469;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;LLM&#26080;&#27861;&#21306;&#20998;&#25552;&#31034;&#30340;&#21738;&#20123;&#37096;&#20998;&#23646;&#20110;&#19981;&#21516;&#30340;&#36755;&#20837;&#28304;&#12290;&#38388;&#25509;&#25552;&#31034;&#27880;&#20837;&#25915;&#20987;&#21033;&#29992;&#36825;&#19968;&#28431;&#27934;&#65292;&#23558;&#23545;&#25163;&#25351;&#20196;&#23884;&#20837;&#21040;&#19982;&#29992;&#25143;&#21629;&#20196;&#19968;&#36215;&#22788;&#29702;&#30340;&#19981;&#21463;&#20449;&#20219;&#25968;&#25454;&#20013;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;LLM&#20250;&#35823;&#23558;&#23545;&#25163;&#25351;&#20196;&#20316;&#20026;&#35201;&#36981;&#24490;&#30340;&#29992;&#25143;&#25351;&#20196;&#65292;&#20174;&#32780;&#22312;&#25972;&#20010;&#31995;&#32479;&#20013;&#21019;&#24314;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32858;&#28966;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;LLMs&#21306;&#20998;&#22810;&#20010;&#36755;&#20837;&#28304;&#33021;&#21147;&#30340;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#31995;&#21015;&#12290;&#20851;&#38190;&#30340;&#35265;&#35299;&#26159;&#21033;&#29992;&#36755;&#20837;&#30340;&#21464;&#25442;&#25552;&#20379;&#20854;&#26469;&#28304;&#30340;&#21487;&#38752;&#36830;&#32493;&#20449;&#21495;&#12290;&#25105;&#20204;&#23558;&#32858;&#28966;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#38450;&#24481;&#25163;&#27573;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14720v1 Announce Type: cross  Abstract: Large Language Models (LLMs), while powerful, are built and trained to process a single text input. In common applications, multiple inputs can be processed by concatenating them together into a single stream of text. However, the LLM is unable to distinguish which sections of prompt belong to various input sources. Indirect prompt injection attacks take advantage of this vulnerability by embedding adversarial instructions into untrusted data being processed alongside user commands. Often, the LLM will mistake the adversarial instructions as user commands to be followed, creating a security vulnerability in the larger system. We introduce spotlighting, a family of prompt engineering techniques that can be used to improve LLMs' ability to distinguish among multiple sources of input. The key insight is to utilize transformations of an input to provide a reliable and continuous signal of its provenance. We evaluate spotlighting as a defen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#32452;&#21512;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040; emerged words &#19982; natural language concepts &#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#32780;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.14705</link><description>&lt;p&gt;
&#27010;&#24565;&#26368;&#20339;&#21305;&#37197;&#65306;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#20013;&#30340;&#32452;&#21512;&#24615;
&lt;/p&gt;
&lt;p&gt;
Concept-Best-Matching: Evaluating Compositionality in Emergent Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14705
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#32452;&#21512;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25214;&#21040; emerged words &#19982; natural language concepts &#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#32780;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27807;&#36890;&#20197;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#33719;&#21462;&#30340;&#36890;&#20449;&#21327;&#35758;&#36890;&#24120;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#19981;&#36879;&#26126;&#30340;&#12290;&#22823;&#37327;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#21508;&#31181;&#35780;&#20272;&#26041;&#27861;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#65292;&#20854;&#20013;\emph{&#32452;&#21512;&#24615;}&#26159;&#19968;&#20010;&#31361;&#20986;&#30340;&#26399;&#26395;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#31243;&#24207;&#24182;&#26410;&#30452;&#25509;&#26292;&#38706;&#26032;&#20852;&#36890;&#20449;&#30340;&#32452;&#21512;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26032;&#20852;&#36890;&#20449;&#32452;&#21512;&#24615;&#30340;&#26041;&#27861;&#65292;&#21363;&#25214;&#21040;&#26032;&#20852;&#35789;&#27719;&#19982;&#33258;&#28982;&#35821;&#35328;&#27010;&#24565;&#20043;&#38388;&#30340;&#26368;&#20339;&#21305;&#37197;&#12290;&#26368;&#20339;&#21305;&#37197;&#31639;&#27861;&#25552;&#20379;&#20102;&#20840;&#23616;&#20998;&#25968;&#21644;&#20174;&#26032;&#20852;&#35789;&#27719;&#21040;&#33258;&#28982;&#35821;&#35328;&#27010;&#24565;&#30340;&#32763;&#35793;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#39318;&#27425;&#25552;&#20379;&#26032;&#20852;&#35789;&#27719;&#19982;&#20154;&#31867;&#27010;&#24565;&#20043;&#38388;&#30452;&#25509;&#32780;&#21487;&#35299;&#37322;&#30340;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14705v1 Announce Type: new  Abstract: Artificial agents that learn to communicate in order to accomplish a given task acquire communication protocols that are typically opaque to a human. A large body of work has attempted to evaluate the emergent communication via various evaluation measures, with \emph{compositionality} featuring as a prominent desired trait. However, current evaluation procedures do not directly expose the compositionality of the emergent communication. We propose a procedure to assess the compositionality of emergent communication by finding the best-match between emerged words and natural language concepts. The best-match algorithm provides both a global score and a translation-map from emergent words to natural language concepts. To the best of our knowledge, it is the first time that such direct and interpretable mapping between emergent words and human concepts is provided.
&lt;/p&gt;</description></item><item><title>GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#25945;&#23398;&#27963;&#21160;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#25903;&#25345;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#12289;&#38382;&#39064;&#35299;&#20915;&#65292;&#36824;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#21644;&#27979;&#35797;&#32416;&#38169;&#31561;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#26159;&#22312;&#22269;&#38469;&#21270;&#26041;&#38754;&#38656;&#27880;&#24847;&#36991;&#20813;&#20854;&#35823;&#29992;&#23548;&#33268;&#30340;&#20840;&#29699;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14694</link><description>&lt;p&gt;
GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#25945;&#23398;&#27963;&#21160;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of GPT Language Models for Innovation in Activities in University Teaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14694
&lt;/p&gt;
&lt;p&gt;
GPT&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#23398;&#25945;&#23398;&#27963;&#21160;&#21019;&#26032;&#20013;&#30340;&#24212;&#29992;&#19981;&#20165;&#21487;&#20197;&#25903;&#25345;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#12289;&#38382;&#39064;&#35299;&#20915;&#65292;&#36824;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#21644;&#27979;&#35797;&#32416;&#38169;&#31561;&#26041;&#38754;&#25552;&#20379;&#24110;&#21161;&#65292;&#20294;&#26159;&#22312;&#22269;&#38469;&#21270;&#26041;&#38754;&#38656;&#27880;&#24847;&#36991;&#20813;&#20854;&#35823;&#29992;&#23548;&#33268;&#30340;&#20840;&#29699;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT&#65288;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#65289;&#35821;&#35328;&#27169;&#22411;&#26159;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#21487;&#20197;&#23454;&#29616;&#33258;&#21160;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#23558;GPT&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22823;&#23398;&#25945;&#23398;&#30340;&#21508;&#20010;&#32500;&#24230;&#20013;&#23384;&#22312;&#30528;&#26085;&#30410;&#22686;&#38271;&#30340;&#20852;&#36259;&#12290;&#20174;&#23398;&#29983;&#21644;&#25945;&#24072;&#27963;&#21160;&#21019;&#26032;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20869;&#23481;&#12289;&#38382;&#39064;&#35299;&#20915;&#20197;&#21450;&#20010;&#24615;&#21270;&#21644;&#27979;&#35797;&#32416;&#38169;&#31561;&#26041;&#38754;&#25552;&#20379;&#25903;&#25345;&#12290;&#20174;&#22269;&#38469;&#21270;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35823;&#29992;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#30528;&#19968;&#20010;&#38656;&#35201;&#37319;&#21462;&#19968;&#31995;&#21015;&#20849;&#21516;&#25514;&#26045;&#30340;&#20840;&#29699;&#38382;&#39064;&#65292;&#36825;&#20123;&#25514;&#26045;&#38656;&#35201;&#21508;&#22320;&#21306;&#30340;&#22823;&#23398;&#20849;&#21516;&#21442;&#19982;&#12290;&#22312;&#19968;&#20123;&#22269;&#23478;&#65292;&#24050;&#32463;&#23545;&#35780;&#20272;&#24037;&#20855;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#20197;&#30830;&#20445;&#24037;&#20316;&#26159;&#30001;&#23398;&#29983;&#23436;&#25104;&#30340;&#65292;&#32780;&#19981;&#26159;&#30001;&#20154;&#24037;&#26234;&#33021;&#23436;&#25104;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#36825;&#31181;&#20195;&#34920;&#24615;&#23398;&#31185;&#30340;&#19968;&#20010;&#20855;&#20307;&#35838;&#39064;&#65292;&#27604;&#22914;&#36719;&#20214;&#24037;&#31243;&#20013;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14694v1 Announce Type: cross  Abstract: The GPT (Generative Pre-trained Transformer) language models are an artificial intelligence and natural language processing technology that enables automatic text generation. There is a growing interest in applying GPT language models to university teaching in various dimensions. From the perspective of innovation in student and teacher activities, they can provide support in understanding and generating content, problem-solving, as well as personalization and test correction, among others. From the dimension of internationalization, the misuse of these models represents a global problem that requires taking a series of common measures in universities from different geographical areas. In several countries, there has been a review of assessment tools to ensure that work is done by students and not by AI. To this end, we have conducted a detailed experiment in a representative subject of Computer Science such as Software Engineering, wh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#20960;&#20309;&#38382;&#39064;&#20013;&#30340;&#36741;&#21161;&#32452;&#20214;</title><link>https://arxiv.org/abs/2403.14690</link><description>&lt;p&gt;
&#23558;&#22270;&#27880;&#24847;&#26426;&#21046;&#34701;&#20837;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20960;&#20309;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14690
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22270;&#27880;&#24847;&#26426;&#21046;&#65292;&#29992;&#20110;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#20960;&#20309;&#38382;&#39064;&#20013;&#30340;&#36741;&#21161;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#25945;&#32946;&#32972;&#26223;&#19979;&#65292;&#35774;&#35745;&#19968;&#20010;&#33258;&#21160;&#27714;&#35299;&#20960;&#20309;&#38382;&#39064;&#30340;&#27714;&#35299;&#22120;&#34987;&#35748;&#20026;&#26159;&#36808;&#21521;&#36890;&#29992;&#25968;&#23398;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#19968;&#27493;&#65292;&#20854;&#20381;&#25176;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20256;&#32479;&#36923;&#36753;&#25512;&#29702;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#38382;&#39064;&#30340;&#35299;&#20915;&#26159;&#36890;&#36807;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#22914;&#32447;&#26465;&#25110;&#28857;&#26469;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22312;&#38656;&#35201;&#20570;&#20986;&#20851;&#38190;&#20915;&#31574;&#26102;&#36873;&#25321;&#21512;&#36866;&#30340;&#36741;&#21161;&#32452;&#20214;&#30340;&#22797;&#26434;&#24615;&#65292;&#33258;&#21160;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#26368;&#26032;&#24615;&#33021;&#26159;&#36890;&#36807;&#20174;&#31867;&#21035;&#24211;&#20013;&#31351;&#20030;&#25152;&#26377;&#21487;&#33021;&#30340;&#31574;&#30053;&#65292;&#20197;&#35782;&#21035;&#20855;&#26377;&#26368;&#22823;&#21487;&#33021;&#24615;&#30340;&#31574;&#30053;&#26469;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#25928;&#29575;&#26041;&#38754;&#20570;&#20986;&#22949;&#21327;&#65292;&#24517;&#39035;&#37319;&#29992;&#24191;&#27867;&#30340;&#31574;&#30053;&#25628;&#32034;&#12290;&#20026;&#20102;&#33258;&#21160;&#19988;&#39640;&#25928;&#22320;&#28155;&#21152;&#36741;&#21161;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14690v1 Announce Type: cross  Abstract: In the context of online education, designing an automatic solver for geometric problems has been considered a crucial step towards general math Artificial Intelligence (AI), empowered by natural language understanding and traditional logical inference. In most instances, problems are addressed by adding auxiliary components such as lines or points. However, adding auxiliary components automatically is challenging due to the complexity in selecting suitable auxiliary components especially when pivotal decisions have to be made. The state-of-the-art performance has been achieved by exhausting all possible strategies from the category library to identify the one with the maximum likelihood. However, an extensive strategy search have to be applied to trade accuracy for ef-ficiency. To add auxiliary components automatically and efficiently, we present deep reinforcement learning framework based on the language model, such as BERT. We first
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#20197;&#35299;&#20915;&#20854;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#21160;&#24577;&#20154;&#31867;&#36947;&#24503;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14683</link><description>&lt;p&gt;
&#19968;&#39033;&#36947;&#20041;&#20351;&#21629;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25345;&#32493;&#36229;&#23545;&#40784;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
A Moral Imperative: The Need for Continual Superalignment of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14683
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#20197;&#35299;&#20915;&#20854;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#21160;&#24577;&#20154;&#31867;&#36947;&#24503;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#32456;&#36523;&#36229;&#23545;&#40784;&#30340;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#12290;&#36229;&#23545;&#40784;&#26159;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#20445;&#36229;&#26234;&#33021;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#12290;&#23613;&#31649;&#20854;&#23637;&#26395;&#20196;&#20154;&#25391;&#22859;&#65292;&#25105;&#20204;&#35748;&#20026;&#23454;&#29616;&#36229;&#23545;&#40784;&#38656;&#35201;&#23545;&#24403;&#21069;LLM&#26550;&#26500;&#36827;&#34892;&#37325;&#22823;&#21464;&#38761;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#29702;&#35299;&#21644;&#36866;&#24212;&#20154;&#31867;&#36947;&#24503;&#30340;&#21160;&#24577;&#24615;&#21644;&#19981;&#26029;&#21457;&#23637;&#30340;&#20840;&#29699;&#24773;&#26223;&#26041;&#38754;&#22266;&#26377;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#21078;&#26512;&#20102;&#23558;&#19981;&#26029;&#21464;&#21270;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#35889;&#31995;&#32534;&#30721;&#21040;LLMs&#20013;&#30340;&#25361;&#25112;&#65292;&#31361;&#20986;&#20102;&#38745;&#24577;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20154;&#31867;&#31038;&#20250;&#21160;&#24577;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#31034;&#20363;&#65306;&#19968;&#20010;&#23637;&#31034;&#20102;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#23450;&#24615;&#36716;&#21464;&#65292;&#21478;&#19968;&#20010;&#21576;&#29616;&#20102;&#21487;&#37327;&#21270;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#36825;&#20123;&#31034;&#20363;&#65292;&#25105;&#20204;&#35828;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14683v1 Announce Type: cross  Abstract: This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illus
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25506;&#35752;&#20102;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14668</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#65306;&#25104;&#20154;&#35782;&#23383;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Predicting Learning Performance with Large Language Models: A Study in Adult Literacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14668
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#25506;&#35752;&#20102;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#65292;&#24182;&#21457;&#29616;GPT-4&#22312;&#27492;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14668v1 &#20844;&#21578;&#31867;&#21035;&#65306;&#36328;&#39046;&#22495; &#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#26174;&#33879;&#22686;&#24378;&#20102;&#25104;&#20154;&#35782;&#23383;&#22521;&#35757;&#65292;&#36825;&#26159;&#31038;&#20250;&#21442;&#19982;&#12289;&#23601;&#19994;&#26426;&#20250;&#21644;&#32456;&#36523;&#23398;&#20064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#39640;&#32423;AI&#27169;&#22411;&#65288;&#21253;&#25324;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#22312;ITS&#20013;&#39044;&#27979;&#25104;&#20154;&#35782;&#23383;&#35745;&#21010;&#23398;&#20064;&#34920;&#29616;&#30340;&#24212;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#21463;&#21040;&#20102;LLMs&#22522;&#20110;&#20854;&#20869;&#22312;&#25512;&#29702;&#21644;&#35745;&#31639;&#33021;&#21147;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#30340;&#28508;&#21147;&#30340;&#21551;&#21457;&#12290;&#36890;&#36807;&#20351;&#29992;ITS AutoTutor&#30340;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36890;&#36807;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#25216;&#26415;&#35780;&#20272;&#20102;GPT-4&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#23398;&#20064;&#34920;&#29616;&#26041;&#38754;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#23637;&#29616;&#20986;&#19982;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65288;&#22914;&#36125;&#21494;&#26031;&#30693;&#35782;&#36319;&#36394;&#12289;&#34920;&#29616;&#22240;&#32032;&#20998;&#26512;&#12289;&#31232;&#30095;&#22240;&#32032;&#20998;&#26512;&#65289;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14668v1 Announce Type: cross  Abstract: Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Fact
&lt;/p&gt;</description></item><item><title>SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.14666</link><description>&lt;p&gt;
SyllabusQA&#65306;&#19968;&#20010;&#35838;&#31243;&#36923;&#36753;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SyllabusQA: A Course Logistics Question Answering Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14666
&lt;/p&gt;
&lt;p&gt;
SyllabusQA&#25968;&#25454;&#38598;&#26159;&#19968;&#20010;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#23545;36&#20010;&#19987;&#19994;&#28085;&#30422;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#36827;&#34892;&#20102;&#35814;&#32454;&#25910;&#38598;&#65292;&#26088;&#22312;&#35780;&#20272;&#31572;&#26696;&#20107;&#23454;&#24615;&#65292;&#22810;&#20010;&#24378;&#22522;&#32447;&#27169;&#22411;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20173;&#23384;&#22312;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#25945;&#23398;&#21161;&#29702;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#26377;&#26174;&#33879;&#28508;&#21147;&#20943;&#36731;&#20154;&#31867;&#25945;&#24072;&#30340;&#24037;&#20316;&#37327;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19982;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#30340;&#38382;&#39064;&#22238;&#31572;&#65292;&#36825;&#23545;&#23398;&#29983;&#24456;&#37325;&#35201;&#65292;&#20294;&#23545;&#25945;&#24072;&#26469;&#35828;&#26159;&#37325;&#22797;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38382;&#39064;&#65292;&#32570;&#20047;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SyllabusQA&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;63&#20010;&#30495;&#23454;&#35838;&#31243;&#22823;&#32434;&#65292;&#28085;&#30422;36&#20010;&#19987;&#19994;&#65292;&#21253;&#21547;5,078&#23545;&#22810;&#26679;&#21270;&#30340;&#24320;&#25918;&#24335;&#35838;&#31243;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;-&#31572;&#26696;&#23545;&#65292;&#38382;&#39064;&#31867;&#22411;&#21644;&#31572;&#26696;&#26684;&#24335;&#37117;&#26159;&#22810;&#26679;&#30340;&#12290;&#30001;&#20110;&#35768;&#22810;&#36923;&#36753;&#30456;&#20851;&#38382;&#39064;&#21253;&#21547;&#20851;&#38190;&#20449;&#24687;&#65292;&#22914;&#32771;&#35797;&#26085;&#26399;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#20107;&#23454;&#24615;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#35813;&#20219;&#21153;&#19978;&#23545;&#20960;&#20010;&#24378;&#22522;&#32447;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#31034;&#21040;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#22312;&#20256;&#32479;&#30340;&#25991;&#26412;&#30456;&#20284;&#24615;&#25351;&#26631;&#19978;&#25509;&#36817;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26174;&#33879;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14666v1 Announce Type: cross  Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#36861;&#36394;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38646;-shot&#25552;&#31034;&#21644;&#27169;&#22411;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#20064;&#32773;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14661</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23398;&#20064;&#32773;&#34920;&#29616;&#24314;&#27169;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Modeling Learner Performance with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#36861;&#36394;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#27604;&#36739;&#38646;-shot&#25552;&#31034;&#21644;&#27169;&#22411;&#24494;&#35843;&#20004;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LLMs&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#20064;&#32773;&#34920;&#29616;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#30740;&#31350;&#34920;&#26126;&#23427;&#20204;&#33021;&#22815;&#20805;&#24403;&#19968;&#33324;&#27169;&#24335;&#26426;&#22120;&#65292;&#36890;&#36807;&#23436;&#25104;&#20195;&#34920;&#21508;&#31181;&#20219;&#21153;&#30340;&#22797;&#26434;&#20196;&#29260;&#24207;&#21015;&#65292;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLMs&#30340;&#27169;&#24335;&#35782;&#21035;&#21644;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#26159;&#21542;&#33021;&#22815;&#25193;&#23637;&#21040;&#30693;&#35782;&#36861;&#36394;&#39046;&#22495;&#65292;&#36825;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITSs&#65289;&#30340;&#21457;&#23637;&#20013;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#36890;&#36807;&#39044;&#27979;&#23398;&#20064;&#32773;&#38543;&#26102;&#38388;&#30340;&#34920;&#29616;&#26469;&#20010;&#24615;&#21270;&#25945;&#32946;&#20307;&#39564;&#12290;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#27492;&#20219;&#21153;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#38646;-shot&#25552;&#31034;&#21644;&#27169;&#22411;&#24494;&#35843;&#65292;&#20197;&#21450;&#29616;&#26377;&#30340;&#38750;LLM&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;LLMs&#30340;&#26041;&#27861;&#27809;&#26377;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#32463;&#36807;&#24494;&#35843;&#30340;LLMs&#36229;&#36807;&#20102;&#22825;&#30495;&#30340;&#22522;&#32447;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14661v1 Announce Type: cross  Abstract: Recent work exploring the capabilities of pre-trained large language models (LLMs) has demonstrated their ability to act as general pattern machines by completing complex token sequences representing a wide array of tasks, including time-series prediction and robot control. This paper investigates whether the pattern recognition and sequence modeling capabilities of LLMs can be extended to the domain of knowledge tracing, a critical component in the development of intelligent tutoring systems (ITSs) that tailor educational experiences by predicting learner performance over time. In an empirical evaluation across multiple real-world datasets, we compare two approaches to using LLMs for this task, zero-shot prompting and model fine-tuning, with existing, non-LLM approaches to knowledge tracing. While LLM-based approaches do not achieve state-of-the-art performance, fine-tuned LLMs surpass the performance of naive baseline models and perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Social AI Data Infrastructure&#30340;&#31038;&#20250;&#26234;&#33021;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20132;AI&#20998;&#31867;&#31995;&#32479;&#21644;&#19968;&#20010;480&#20010;NLP&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25968;&#25454;&#38598;&#24037;&#20316;&#20197;&#21450;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#31038;&#20250;&#26234;&#33021;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#25968;&#25454;&#26684;&#23616;&#24182;&#25552;&#20379;&#26410;&#26469;&#25968;&#25454;&#38598;&#21457;&#23637;&#26041;&#21521;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.14659</link><description>&lt;p&gt;
&#31038;&#20250;&#26234;&#33021;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65306;&#26500;&#24314;&#29616;&#29366;&#21644;&#24341;&#39046;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;Social AI Data Infrastructure&#30340;&#31038;&#20250;&#26234;&#33021;&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65292;&#21253;&#25324;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20132;AI&#20998;&#31867;&#31995;&#32479;&#21644;&#19968;&#20010;480&#20010;NLP&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24211;&#65292;&#36890;&#36807;&#20998;&#26512;&#29616;&#26377;&#25968;&#25454;&#38598;&#24037;&#20316;&#20197;&#21450;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#31038;&#20250;&#26234;&#33021;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#24110;&#21161;&#30740;&#31350;&#32773;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#25968;&#25454;&#26684;&#23616;&#24182;&#25552;&#20379;&#26410;&#26469;&#25968;&#25454;&#38598;&#21457;&#23637;&#26041;&#21521;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#20154;&#31867;&#31038;&#20250;&#29983;&#27963;&#20013;&#65292;&#36825;&#20123;&#25216;&#26415;&#23558;&#38656;&#35201;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#31038;&#20250;&#26234;&#33021;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#34913;&#37327;&#31038;&#20250;&#26234;&#33021;&#30340;&#23396;&#31435;&#32500;&#24230;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23384;&#22312;&#20219;&#20309;&#20316;&#21697;&#26469;&#23558;&#36825;&#20123;&#32447;&#32034;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#24418;&#25104;&#19968;&#20010;&#30740;&#31350;&#32773;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#30740;&#31350;&#31354;&#30333;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#20957;&#32858;&#23376;&#39046;&#22495;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31038;&#20132;AI&#25968;&#25454;&#22522;&#30784;&#35774;&#26045;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#20840;&#38754;&#30340;&#31038;&#20132;AI&#20998;&#31867;&#31995;&#32479;&#21644;&#19968;&#20010;&#21253;&#21547;480&#20010;NLP&#25968;&#25454;&#38598;&#30340;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#20998;&#26512;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#24037;&#20316;&#65292;&#21516;&#26102;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#31038;&#20250;&#26234;&#33021;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23427;&#22312;&#24110;&#21161;&#28145;&#20837;&#20102;&#35299;&#24403;&#21069;&#25968;&#25454;&#26684;&#23616;&#24182;&#25552;&#20379;&#23545;&#26410;&#26469;&#25968;&#25454;&#38598;&#21457;&#23637;&#28508;&#22312;&#26041;&#21521;&#30340;&#25972;&#20307;&#35266;&#28857;&#26041;&#38754;&#20855;&#26377;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14659v1 Announce Type: cross  Abstract: As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models' performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#32852;&#21512;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;5G&#25216;&#26415;&#22312;&#32654;&#22269;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#39044;&#38450;&#21644;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31215;&#26497;&#26816;&#27979;&#21644;&#39044;&#38450;&#12289;&#21033;&#29992;5G&#25216;&#26415;&#36827;&#34892;&#36828;&#31243;&#30417;&#27979;&#21644;&#21046;&#22270;&#12289;&#20197;&#21450;&#21033;&#29992;&#26080;&#20154;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#36827;&#34892;&#39640;&#32423;&#28779;&#28798;&#21709;&#24212;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14657</link><description>&lt;p&gt;
&#19968;&#31181;&#32852;&#21512;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;5G&#25216;&#26415;&#22312;&#32654;&#22269;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#39044;&#38450;&#21644;&#31649;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Synergistic Approach to Wildfire Prevention and Management Using AI, ML, and 5G Technology in the United States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#25506;&#32034;&#32852;&#21512;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;5G&#25216;&#26415;&#22312;&#32654;&#22269;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#39044;&#38450;&#21644;&#31649;&#29702;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#31215;&#26497;&#26816;&#27979;&#21644;&#39044;&#38450;&#12289;&#21033;&#29992;5G&#25216;&#26415;&#36827;&#34892;&#36828;&#31243;&#30417;&#27979;&#21644;&#21046;&#22270;&#12289;&#20197;&#21450;&#21033;&#29992;&#26080;&#20154;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#36827;&#34892;&#39640;&#32423;&#28779;&#28798;&#21709;&#24212;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#26862;&#26519;&#28779;&#28798;&#24050;&#32463;&#25104;&#20026;&#20840;&#29699;&#24615;&#30340;&#29615;&#22659;&#32039;&#24613;&#20107;&#20214;&#65292;&#23545;&#33258;&#28982;&#26646;&#24687;&#22320;&#36896;&#25104;&#20102;&#20005;&#37325;&#20260;&#23475;&#65292;&#24182;&#21152;&#36895;&#20102;&#27668;&#20505;&#21464;&#21270;&#30340;&#36827;&#31243;&#12290;&#26862;&#26519;&#28779;&#28798;&#31649;&#29702;&#26041;&#27861;&#21253;&#25324;&#39044;&#38450;&#12289;&#21709;&#24212;&#21644;&#24674;&#22797;&#24037;&#20316;&#12290;&#23613;&#31649;&#26816;&#27979;&#25216;&#26415;&#26377;&#25152;&#25913;&#36827;&#65292;&#20294;&#28779;&#28798;&#21457;&#29983;&#29575;&#30340;&#19978;&#21319;&#35201;&#27714;&#25105;&#20204;&#25552;&#20986;&#21019;&#26032;&#24615;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20415;&#21450;&#26102;&#35782;&#21035;&#21644;&#26377;&#25928;&#25511;&#21046;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#32654;&#22269;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;5G&#25216;&#26415;&#36827;&#34892;&#31215;&#26497;&#21028;&#21035;&#21644;&#22788;&#29702;&#26862;&#26519;&#28779;&#28798;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#30740;&#31350;&#30446;&#26631;&#21253;&#25324;&#20351;&#29992;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#30340;&#31215;&#26497;&#26816;&#27979;&#21644;&#39044;&#38450;&#65307;&#21033;&#29992;5G&#25216;&#26415;&#36827;&#34892;&#36828;&#31243;&#24863;&#30693;&#21644;&#20449;&#21495;&#20256;&#36755;&#20197;&#36827;&#34892;&#31215;&#26497;&#30417;&#27979;&#21644;&#21046;&#22270;&#65307;&#20197;&#21450;&#21033;&#29992;&#26080;&#20154;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#36827;&#34892;&#26862;&#26519;&#28779;&#28798;&#30340;&#39640;&#32423;&#21709;&#24212;&#26426;&#21046;&#12290;&#36825;&#39033;&#30740;&#31350;&#22522;&#20110;&#20108;&#25163;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14657v1 Announce Type: cross  Abstract: Over the past few years, wildfires have become a worldwide environmental emergency, resulting in substantial harm to natural habitats and playing a part in the acceleration of climate change. Wildfire management methods involve prevention, response, and recovery efforts. Despite improvements in detection techniques, the rising occurrence of wildfires demands creative solutions for prompt identification and effective control. This research investigates proactive methods for detecting and handling wildfires in the United States, utilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G technology. The specific objective of this research covers proactive detection and prevention of wildfires using advanced technology; Active monitoring and mapping with remote sensing and signaling leveraging on 5G technology; and Advanced response mechanisms to wildfire using drones and IOT devices. This study was based on secondary data colle
&lt;/p&gt;</description></item><item><title>MemeCraft&#26159;&#19968;&#27454;&#21019;&#26032;&#30340;&#27169;&#22240;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25903;&#25345;&#29305;&#23450;&#31038;&#20250;&#36816;&#21160;&#30340;&#27169;&#22240;&#65292;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#65292;&#24102;&#26377;&#20869;&#22312;&#23433;&#20840;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.14652</link><description>&lt;p&gt;
MemeCraft&#65306;&#24773;&#22659;&#21644;&#31435;&#22330;&#39537;&#21160;&#30340;&#22810;&#27169;&#24577;&#27169;&#22240;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14652
&lt;/p&gt;
&lt;p&gt;
MemeCraft&#26159;&#19968;&#27454;&#21019;&#26032;&#30340;&#27169;&#22240;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25903;&#25345;&#29305;&#23450;&#31038;&#20250;&#36816;&#21160;&#30340;&#27169;&#22240;&#65292;&#25552;&#20379;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#65292;&#24102;&#26377;&#20869;&#22312;&#23433;&#20840;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14652v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#32447;&#27169;&#22240;&#22312;&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#20316;&#20026;&#24378;&#22823;&#30340;&#25968;&#23383;&#25991;&#21270;&#20135;&#29289;&#23853;&#38706;&#22836;&#35282;&#65292;&#23427;&#20204;&#19981;&#20165;&#25552;&#20379;&#20102;&#24189;&#40664;&#65292;&#36824;&#20026;&#25919;&#27835;&#35805;&#35821;&#12289;&#31038;&#20250;&#25209;&#21028;&#21644;&#20449;&#24687;&#20256;&#25773;&#25552;&#20379;&#20102;&#24179;&#21488;&#12290;&#23427;&#20204;&#22312;&#22609;&#36896;&#22312;&#32447;&#31038;&#21306;&#24773;&#32490;&#26041;&#38754;&#30340;&#24191;&#27867;&#24433;&#21709;&#21147;&#20351;&#20854;&#25104;&#20026;&#31454;&#36873;&#21644;&#25512;&#21160;&#24847;&#35782;&#24418;&#24577;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;&#23613;&#31649;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#27169;&#22240;&#29983;&#25104;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#22312;&#31995;&#32479;&#24615;&#35780;&#20272;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#65292;&#20197;&#21450;&#22312;&#26377;&#25928;&#20256;&#36798;&#24847;&#35782;&#24418;&#24577;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MemeCraft&#65292;&#19968;&#27454;&#21019;&#26032;&#30340;&#27169;&#22240;&#29983;&#25104;&#22120;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#29983;&#25104;&#25903;&#25345;&#29305;&#23450;&#31038;&#20250;&#36816;&#21160;&#30340;&#27169;&#22240;&#12290;MemeCraft&#25552;&#20379;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27969;&#31243;&#65292;&#23558;&#29992;&#25143;&#25552;&#31034;&#36716;&#21270;&#20026;&#24341;&#20154;&#20837;&#32988;&#30340;&#22810;&#27169;&#24577;&#27169;&#22240;&#65292;&#26080;&#38656;&#20154;&#24037;&#24178;&#39044;&#12290;&#22522;&#20110;&#23545;&#21019;&#36896;&#26377;&#20105;&#35758;&#20869;&#23481;&#30340;&#28508;&#22312;&#28389;&#29992;&#30340;&#35748;&#35782;&#65292;&#20855;&#26377;&#20869;&#22312;&#23433;&#20840;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14652v1 Announce Type: cross  Abstract: Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme-generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31038;&#21306;&#20026;&#20013;&#24515;&#30340;&#21442;&#19982;&#24335;&#30740;&#31350;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;615&#20010;&#31038;&#20250;&#25991;&#29289;&#30340;&#25968;&#25454;&#38598;DOSA&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#27169;&#22411;&#26356;&#22909;&#22320;&#20102;&#35299;&#24182;&#32771;&#34385;&#24403;&#22320;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.14651</link><description>&lt;p&gt;
DOSA: &#19968;&#20010;&#21253;&#21547;&#19981;&#21516;&#21360;&#24230;&#22320;&#29702;&#23376;&#25991;&#21270;&#31038;&#20250;&#25991;&#29289;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14651
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31038;&#21306;&#20026;&#20013;&#24515;&#30340;&#21442;&#19982;&#24335;&#30740;&#31350;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#21253;&#21547;615&#20010;&#31038;&#20250;&#25991;&#29289;&#30340;&#25968;&#25454;&#38598;DOSA&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#27169;&#22411;&#26356;&#22909;&#22320;&#20102;&#35299;&#24182;&#32771;&#34385;&#24403;&#22320;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14651v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#22914;&#25991;&#26412;&#29983;&#25104;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#38382;&#31572;&#12290;&#20026;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21457;&#25381;&#25928;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#24517;&#39035;&#20102;&#35299;&#24182;&#32771;&#34385;&#24403;&#22320;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#65292;&#22240;&#27492;&#38656;&#35201;&#22522;&#20934;&#26469;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#25991;&#21270;&#29087;&#24713;&#31243;&#24230;&#12290;&#30001;&#20110;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#22522;&#20110;&#32593;&#32476;&#65292;&#32780;&#32593;&#32476;&#22312;&#20449;&#24687;&#34920;&#31034;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#27492;&#26080;&#27861;&#25429;&#25417;&#21040;&#19981;&#22312;&#32593;&#32476;&#19978;&#30340;&#31038;&#21306;&#20869;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#21152;&#21095;&#20102;&#32593;&#32476;&#20013;&#30340;&#19981;&#24179;&#31561;&#12289;&#35821;&#20041;&#38169;&#20301;&#21644;&#21051;&#26495;&#21360;&#35937;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#21628;&#21505;&#37319;&#29992;&#31038;&#21306;&#20026;&#20013;&#24515;&#30340;&#21442;&#19982;&#24335;&#30740;&#31350;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22238;&#24212;&#20102;&#36825;&#19968;&#21628;&#21505;&#65292;&#36890;&#36807;&#21442;&#19982;&#24335;&#30740;&#31350;&#26041;&#27861;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#31038;&#21306;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;DOSA&#65292;&#20854;&#20013;&#21253;&#25324;615&#20010;&#31038;&#20250;&#25991;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14651v1 Announce Type: cross  Abstract: Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce $\textit{DOSA}$, the first community-generated $\textbf{D}$ataset $\textbf{o}$f 615 $\textbf{S}$ocial $\textbf{A}$rtifacts, by engaging wi
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14643</link><description>&lt;p&gt;
&#25506;&#31350;ChatGPT&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT and its Impact on Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14643
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#23384;&#22312;&#19968;&#27573;&#26102;&#38388;&#20102;&#65292;&#20294;&#31361;&#28982;&#38388;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#21463;&#21040;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#24863;&#35874;&#35895;&#27468;&#12289;&#24494;&#36719;&#12289;&#20803;&#23431;&#23449;&#31561;&#31185;&#25216;&#30028;&#20027;&#35201;&#21697;&#29260;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;OpenAI&#36890;&#36807;&#20854;&#24320;&#21019;&#24615;&#21457;&#26126;ChatGPT&#35302;&#21457;&#20102;&#25353;&#38062;&#12290;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#22312;&#23545;&#35805;&#32972;&#26223;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#22797;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#12290;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#19978;&#19979;&#25991;&#29983;&#25104;&#21644;&#38754;&#21521;&#24320;&#25918;&#22495;&#30340;&#35757;&#32451;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#24212;&#29992;&#20110;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#23458;&#25143;&#26381;&#21153;&#20877;&#21040;&#35821;&#35328;&#32763;&#35793;&#31561;&#24191;&#27867;&#39046;&#22495;&#12290;&#23427;&#20855;&#26377;&#24443;&#24213;&#25913;&#21464;&#21508;&#34892;&#19994;&#24182;&#36716;&#21464;&#25105;&#20204;&#19982;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;ChatGPT&#20063;&#24341;&#21457;&#20102;&#19968;&#20123;&#25285;&#24551;&#65292;&#21253;&#25324;&#36947;&#24503;&#26041;&#38754;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14643v1 Announce Type: cross  Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14633</link><description>&lt;p&gt;
&#20986;&#36523;&#23500;&#36149;&#65311;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SilverSpoon&#65292;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#20559;&#35265;&#30340;&#31243;&#24230;&#20197;&#21450;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#22312;&#31038;&#20250;&#20013;&#21152;&#21095;&#20102;&#19981;&#20844;&#24179;&#29616;&#35937;&#65292;&#26681;&#25454;&#20010;&#20154;&#32463;&#27982;&#21644;&#31038;&#20250;&#32972;&#26223;&#24433;&#21709;&#33719;&#21462;&#26426;&#20250;&#21644;&#36164;&#28304;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#26222;&#36941;&#38382;&#39064;&#25345;&#32493;&#22320;&#24310;&#32493;&#20102;&#31995;&#32479;&#24615;&#30340;&#19981;&#24179;&#31561;&#65292;&#38459;&#30861;&#20102;&#20316;&#20026;&#19968;&#20010;&#31038;&#20250;&#36861;&#27714;&#21253;&#23481;&#24615;&#36827;&#27493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;SilverSpoon&#65289;&#65292;&#21253;&#21547;3000&#20010;&#26679;&#26412;&#65292;&#23637;&#31034;&#20102;&#29301;&#28041;&#21040;&#24369;&#21183;&#32676;&#20307;&#30001;&#20110;&#20182;&#20204;&#30340;&#22788;&#22659;&#32780;&#23454;&#26045;&#36947;&#24503;&#27169;&#31946;&#34892;&#20026;&#30340;&#20551;&#35774;&#24773;&#26223;&#65292;&#24182;&#38382;&#36825;&#31181;&#34892;&#20026;&#26159;&#21542;&#22312;&#36947;&#24503;&#19978;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#25968;&#25454;&#38598;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#30001;&#23646;&#20110;&#31038;&#20250;&#32463;&#27982;&#20004;&#31471;&#30340;&#20154;&#36827;&#34892;&#20102;&#27880;&#37322;&#12290;&#20351;&#29992;SilverSpoon&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#30340;&#31038;&#20250;&#32463;&#27982;&#20559;&#35265;&#31243;&#24230;&#20197;&#21450;&#35813;&#31243;&#24230;&#22914;&#20309;&#38543;&#27169;&#22411;&#22823;&#23567;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14633v1 Announce Type: cross  Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. W
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#40784;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14221</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#40784;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Robustness of Large Language Models via Consistency Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14221
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#33268;&#24615;&#23545;&#40784;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#25351;&#20196;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#36981;&#24490;&#29992;&#25143;&#25351;&#20196;&#21644;&#29983;&#25104;&#26377;&#29992;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20339;&#29366;&#24577;&#65292;&#22240;&#20026;&#21487;&#33021;&#30001;&#20110;&#21475;&#22836;&#25351;&#20196;&#20013;&#30340;&#32454;&#24494;&#26356;&#25913;&#32780;&#20135;&#29983;&#26126;&#26174;&#19981;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#25506;&#35752;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#32487;&#32493;&#25913;&#21892;&#21709;&#24212;&#29983;&#25104;&#30340;&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#31995;&#32479;&#24615;&#20998;&#26512;&#21644;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23450;&#37327;&#23450;&#20041;&#20102;&#19981;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#25351;&#20196;&#22686;&#24378;&#30417;&#30563;&#24494;&#35843;&#21644;&#19968;&#33268;&#24615;&#23545;&#40784;&#35757;&#32451;&#32452;&#25104;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#26694;&#26550;&#12290;&#31532;&#19968;&#38454;&#27573;&#36890;&#36807;&#31867;&#20284;&#25351;&#20196;&#22686;&#24378;&#24110;&#21161;&#27169;&#22411;&#22312;&#36981;&#24490;&#25351;&#20196;&#26102;&#27867;&#21270;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#39640;&#20102;&#22810;&#26679;&#24615;&#65292;&#24182;&#24110;&#21161;&#27169;&#22411;&#29702;&#35299;&#21738;&#20123;&#21709;&#24212;&#19982;&#20154;&#31867;&#26399;&#26395;&#26356;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14221v1 Announce Type: new  Abstract: Large language models (LLMs) have shown tremendous success in following user instructions and generating helpful responses. Nevertheless, their robustness is still far from optimal, as they may generate significantly inconsistent responses due to minor changes in the verbalized instructions. Recent literature has explored this inconsistency issue, highlighting the importance of continued improvement in the robustness of response generation. However, systematic analysis and solutions are still lacking. In this paper, we quantitatively define the inconsistency problem and propose a two-stage training framework consisting of instruction-augmented supervised fine-tuning and consistency alignment training. The first stage helps a model generalize on following instructions via similar instruction augmentations. In the second stage, we improve the diversity and help the model understand which responses are more aligned with human expectations b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;</title><link>https://arxiv.org/abs/2403.13679</link><description>&lt;p&gt;
RoleInteract&#65306;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#20195;&#29702;&#30340;&#31038;&#20132;&#20114;&#21160;
&lt;/p&gt;
&lt;p&gt;
RoleInteract: Evaluating the Social Interaction of Role-Playing Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13679
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;RoleInteract&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#31038;&#20132;&#24615;&#30340;&#22522;&#20934;&#65292;&#35206;&#30422;&#20102;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#23545;&#35805;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#21160;&#20102;&#21508;&#31181;AI&#23545;&#35805;&#20195;&#29702;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#27169;&#20223;&#19981;&#21516;&#35282;&#33394;&#21644;&#20154;&#31867;&#34892;&#20026;&#30340;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;RoleInteract&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#35282;&#33394;&#25198;&#28436;&#23545;&#35805;&#20195;&#29702;&#22312;&#31038;&#20132;&#26041;&#38754;&#34920;&#29616;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#20174;&#21508;&#31181;&#26469;&#28304;&#26500;&#24314;&#65292;&#28085;&#30422;&#20102;&#36229;&#36807;500&#20010;&#35282;&#33394;&#12289;6000&#22810;&#20010;&#38382;&#39064;&#25552;&#31034;&#21644;30800&#20010;&#22810;&#36718;&#35282;&#33394;&#25198;&#28436;&#35805;&#35821;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13679v1 Announce Type: new  Abstract: Large language models (LLMs) have advanced the development of various AI conversational agents, including role-playing conversational agents that mimic diverse characters and human behaviors. While prior research has predominantly focused on enhancing the conversational capability, role-specific knowledge, and stylistic attributes of these agents, there has been a noticeable gap in assessing their social intelligence. In this paper, we introduce RoleInteract, the first benchmark designed to systematically evaluate the sociality of role-playing conversational agents at both individual and group levels of social interactions. The benchmark is constructed from a variety of sources and covers a wide range of 500 characters and over 6,000 question prompts and 30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations on this benchmark using mainstream open-source and closed-source LLMs. We find that agents excelling in in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35843;&#25972;LLama Chat&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20272;&#20854;&#22312;&#27431;&#30431;&#25919;&#27835;&#20013;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#22269;&#23478;&#25919;&#20826;&#31435;&#22330;&#30340;&#20805;&#20998;&#20102;&#35299;&#65292;&#24182;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20026;&#23558;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#29992;&#20110;&#25919;&#27835;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13592</link><description>&lt;p&gt;
&#25289;&#39532;&#36935;&#19978;&#27431;&#30431;&#65306;&#36890;&#36807;LLMs&#25506;&#31350;&#27431;&#27954;&#25919;&#27835;&#20809;&#35889;
&lt;/p&gt;
&lt;p&gt;
Llama meets EU: Investigating the European Political Spectrum through the Lens of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;LLama Chat&#27169;&#22411;&#26469;&#37325;&#26032;&#35780;&#20272;&#20854;&#22312;&#27431;&#30431;&#25919;&#27835;&#20013;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#22269;&#23478;&#25919;&#20826;&#31435;&#22330;&#30340;&#20805;&#20998;&#20102;&#35299;&#65292;&#24182;&#33021;&#22312;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20026;&#23558;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#29992;&#20110;&#25919;&#27835;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13592v1 &#31867;&#22411;&#65306;&#26032;&#25991;&#31456; &#25688;&#35201;&#65306;&#32454;&#21270;&#25351;&#23548;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#25919;&#27835;&#20542;&#21521;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#20250;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#25191;&#34892;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#25193;&#23637;&#21040;&#32654;&#22269;&#20004;&#20826;&#21046;&#20197;&#22806;&#65292;&#22312;&#19981;&#21516;&#29615;&#22659;&#20013;&#23457;&#35745; Llama Chat&#65292;&#20197;&#20998;&#26512;&#35813;&#27169;&#22411;&#23545;&#27431;&#30431;&#25919;&#27835;&#30340;&#20102;&#35299;&#31243;&#24230;&#21450;&#20854;&#22312;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36866;&#24212;&#65292;&#21363;&#36827;&#19968;&#27493;&#32454;&#21270;&#65292;Llama Chat &#22522;&#20110;&#27431;&#27954;&#35758;&#20250;&#36777;&#35770;&#20013;&#20010;&#21035;&#27431;&#27954;&#25919;&#20826;&#30340;&#28436;&#35762;&#36827;&#34892;&#36866;&#24212;&#24615;&#35843;&#25972;&#65292;&#20197;&#26681;&#25454;EUandI&#38382;&#21367;&#37325;&#26032;&#35780;&#20272;&#20854;&#25919;&#27835;&#20542;&#21521;&#12290;Llama Chat &#26174;&#33879;&#20102;&#35299;&#21508;&#22269;&#25919;&#20826;&#30340;&#31435;&#22330;&#65292;&#24182;&#33021;&#22815;&#22312;&#19978;&#19979;&#25991;&#20013;&#25512;&#29702;&#12290;&#32463;&#35843;&#25972;&#30340;&#12289;&#29305;&#23450;&#25919;&#20826;&#30340;&#27169;&#22411;&#22312;&#30456;&#24212;&#31435;&#22330;&#19978;&#26377;&#26126;&#26174;&#30340;&#37325;&#26032;&#35843;&#25972;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#23558;&#22522;&#20110;&#23545;&#35805;&#30340;LLM&#20316;&#20026;&#25968;&#25454;&#39537;&#21160;&#30340;&#23545;&#35805;&#24341;&#25806;&#29992;&#20110;&#21327;&#21161;&#25919;&#27835;&#31185;&#23398;&#30740;&#31350;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13592v1 Announce Type: new  Abstract: Instruction-finetuned Large Language Models inherit clear political leanings that have been shown to influence downstream task performance. We expand this line of research beyond the two-party system in the US and audit Llama Chat in the context of EU politics in various settings to analyze the model's political knowledge and its ability to reason in context. We adapt, i.e., further fine-tune, Llama Chat on speeches of individual euro-parties from debates in the European Parliament to reevaluate its political leaning based on the EUandI questionnaire. Llama Chat shows considerable knowledge of national parties' positions and is capable of reasoning in context. The adapted, party-specific, models are substantially re-aligned towards respective positions which we see as a starting point for using chat-based LLMs as data-driven conversational engines to assist research in political science.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.13362</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#36134;&#25143;&#28608;&#21169;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#26032;&#38395;&#28040;&#36153;
&lt;/p&gt;
&lt;p&gt;
Incentivizing News Consumption on Social Media Platforms Using Large Language Models and Realistic Bot Accounts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13362
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#20351;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#36134;&#25143;&#65292;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#22238;&#22797;&#29992;&#25143;&#30340;&#25512;&#25991;&#65292;&#40723;&#21169;&#29992;&#25143;&#25509;&#35302;&#21644;&#20851;&#27880;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#65292;&#20197;&#22686;&#21152;&#29992;&#25143;&#25509;&#35302;&#36825;&#20123;&#26032;&#38395;&#24182;&#25552;&#39640;&#21442;&#19982;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26497;&#21270;&#12289;&#20449;&#20219;&#19979;&#38477;&#20197;&#21450;&#23545;&#27665;&#20027;&#35268;&#33539;&#25903;&#25345;&#21160;&#25671;&#26159;&#32654;&#22269;&#27665;&#20027;&#38754;&#20020;&#30340;&#32039;&#36843;&#23041;&#32961;&#12290;&#25509;&#35302;&#39564;&#35777;&#21644;&#20248;&#36136;&#26032;&#38395;&#21487;&#33021;&#38477;&#20302;&#20010;&#20154;&#23545;&#36825;&#20123;&#23041;&#32961;&#30340;&#26131;&#24863;&#24615;&#65292;&#24182;&#20351;&#20844;&#27665;&#26356;&#20855;&#25239;&#20987;&#38169;&#35823;&#20449;&#24687;&#12289;&#27665;&#31929;&#20027;&#20041;&#21644;&#26497;&#31471;&#20826;&#27966;&#35328;&#35770;&#30340;&#33021;&#21147;&#12290;&#35813;&#39033;&#30446;&#25506;&#35752;&#20102;&#22914;&#20309;&#22312;&#19968;&#20010;&#29983;&#24577;&#26377;&#25928;&#30340;&#29615;&#22659;&#20013;&#22686;&#24378;&#29992;&#25143;&#25509;&#35302;&#21644;&#21442;&#19982;&#39564;&#35777;&#30340;&#12289;&#24847;&#35782;&#24418;&#24577;&#24179;&#34913;&#30340;&#26032;&#38395;&#12290;&#25105;&#20204;&#20381;&#36182;&#20110;&#23545; 28,457 &#20010; Twitter &#29992;&#25143;&#36827;&#34892;&#30340;&#22823;&#35268;&#27169;&#20026;&#26399;&#20004;&#21608;&#30340;&#30000;&#37326;&#23454;&#39564;&#65288;&#20174; 2023 &#24180; 1 &#26376; 19 &#26085;&#21040; 2 &#26376; 3 &#26085;&#65289;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102; 28 &#20010;&#21033;&#29992; GPT-2 &#30340;&#26426;&#22120;&#20154;&#65292;&#22312;&#29992;&#25143;&#21457;&#34920;&#26377;&#20851;&#20307;&#32946;&#12289;&#23089;&#20048;&#25110;&#29983;&#27963;&#26041;&#24335;&#30340;&#25512;&#25991;&#26102;&#22238;&#22797;&#19968;&#20010;&#20869;&#23481;&#30456;&#20851;&#30340;&#22238;&#22797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#30828;&#20195;&#30721;&#20803;&#32032;&#65306;&#19968;&#20010;&#25351;&#21521;&#20248;&#36136;&#26032;&#38395;&#26426;&#26500;&#30456;&#20851;&#20027;&#39064;&#37096;&#20998;&#30340; URL &#21644;&#40723;&#21169;&#20851;&#27880;&#20854; Twitter &#36134;&#25143;&#12290;&#20026;&#36827;&#19968;&#27493;&#27979;&#35797;&#26426;&#22120;&#20154;&#23545;&#24615;&#21035;&#30340;&#24046;&#24322;&#24433;&#21709;&#65292;&#34987;&#35797;&#29992;&#25143;&#34987;&#38543;&#26426;&#20998;&#37197;&#20197;&#25509;&#21463;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13362v1 Announce Type: cross  Abstract: Polarization, declining trust, and wavering support for democratic norms are pressing threats to U.S. democracy. Exposure to verified and quality news may lower individual susceptibility to these threats and make citizens more resilient to misinformation, populism, and hyperpartisan rhetoric. This project examines how to enhance users' exposure to and engagement with verified and ideologically balanced news in an ecologically valid setting. We rely on a large-scale two-week long field experiment (from 1/19/2023 to 2/3/2023) on 28,457 Twitter users. We created 28 bots utilizing GPT-2 that replied to users tweeting about sports, entertainment, or lifestyle with a contextual reply containing two hardcoded elements: a URL to the topic-relevant section of quality news organization and an encouragement to follow its Twitter account. To further test differential effects by gender of the bots, treated users were randomly assigned to receive re
&lt;/p&gt;</description></item><item><title>RankPrompt &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#25490;&#24207;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12373</link><description>&lt;p&gt;
RankPrompt&#65306;&#36880;&#27493;&#27604;&#36739;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12373
&lt;/p&gt;
&lt;p&gt;
RankPrompt &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#25105;&#25490;&#24207;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#20687;ChatGPT&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20063;&#23481;&#26131;&#20986;&#29616;&#36923;&#36753;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#37096;&#32626;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#39564;&#35777;&#22120;&#25110;&#22312;&#22810;&#20010;&#25512;&#29702;&#36335;&#24452;&#19978;&#25237;&#31080;&#65292;&#35201;&#20040;&#38656;&#35201;&#22823;&#37327;&#20154;&#31867;&#27880;&#37322;&#65292;&#35201;&#20040;&#22312;&#23384;&#22312;&#19981;&#19968;&#33268;&#21709;&#24212;&#30340;&#22330;&#26223;&#20013;&#22833;&#36133;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;RankPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#20351;LLMs&#33021;&#22815;&#33258;&#34892;&#23545;&#20854;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#32780;&#26080;&#38656;&#39069;&#22806;&#36164;&#28304;&#12290;RankPrompt&#23558;&#25490;&#24207;&#38382;&#39064;&#20998;&#35299;&#20026;&#22810;&#20010;&#21709;&#24212;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;&#27604;&#36739;&#65292;&#21033;&#29992;LLMs&#33258;&#21160;&#29983;&#25104;&#27604;&#36739;&#38142;&#20316;&#20026;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#22266;&#26377;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;11&#20010;&#31639;&#26415;&#25512;&#29702;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;RankPrompt&#26174;&#33879;&#25552;&#39640;&#20102;ChatGPT&#21644;GPT-4&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12373v1 Announce Type: new  Abstract: Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, which include deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of u
&lt;/p&gt;</description></item><item><title>Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11905</link><description>&lt;p&gt;
Tur[k]ingBench&#65306;&#29992;&#20110;&#32593;&#32476;&#20195;&#29702;&#30340;&#25361;&#25112;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench: A Challenge Benchmark for Web Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11905
&lt;/p&gt;
&lt;p&gt;
Tur[k]ingBench&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#32593;&#32476;&#20195;&#29702;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22788;&#29702;&#21253;&#21547;&#25991;&#26412;&#25351;&#31034;&#21644;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#30340;&#22797;&#26434;&#20219;&#21153;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#21407;&#22987;&#25991;&#26412;&#24418;&#24335;&#19979;&#29702;&#35299;&#21644;&#20132;&#27969;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19990;&#30028;&#19978;&#19981;&#20165;&#20165;&#26159;&#21407;&#22987;&#25991;&#26412;&#12290;&#20363;&#22914;&#65292;&#20154;&#20204;&#22312;&#32593;&#39029;&#19978;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22312;&#36825;&#20123;&#32593;&#39029;&#19978;&#65292;&#25991;&#26412;&#19982;&#20854;&#20182;&#24418;&#24335;&#20132;&#32455;&#22312;&#19968;&#36215;&#65292;&#24182;&#20197;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#30340;&#24418;&#24335;&#23436;&#25104;&#20219;&#21153;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#25512;&#24191;&#21040;&#36825;&#31181;&#22797;&#26434;&#30340;&#39046;&#22495;&#21602;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TurkingBench&#65292;&#19968;&#20010;&#30001;&#21253;&#21547;&#22810;&#27169;&#24577;&#32972;&#26223;&#30340;&#25991;&#26412;&#35828;&#26126;&#21046;&#23450;&#30340;&#20219;&#21153;&#22522;&#20934;&#12290;&#19982;&#29616;&#26377;&#30340;&#20351;&#29992;&#20154;&#24037;&#21512;&#25104;&#30340;&#32593;&#39029;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#36825;&#37324;&#25105;&#20204;&#20351;&#29992;&#26368;&#21021;&#35774;&#35745;&#29992;&#20110;&#21508;&#31181;&#27880;&#37322;&#30446;&#30340;&#30340;&#33258;&#28982;HTML&#39029;&#38754;&#12290;&#27599;&#20010;&#20219;&#21153;&#30340;HTML&#35828;&#26126;&#20063;&#34987;&#23454;&#20363;&#21270;&#20026;&#21508;&#31181;&#20540;&#65288;&#20174;&#20247;&#21253;&#20219;&#21153;&#33719;&#24471;&#65289;&#20197;&#24418;&#25104;&#20219;&#21153;&#30340;&#26032;&#23454;&#20363;&#12290;&#36825;&#20010;&#22522;&#20934;&#21253;&#21547;32.2K&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;&#30340;&#21547;&#20041;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.10822</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;?
&lt;/p&gt;
&lt;p&gt;
Do Large Language Models understand Medical Codes?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10822
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;&#30340;&#21547;&#20041;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#23545;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#31283;&#27493;&#26397;&#30528;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;(AGI)&#36808;&#36827;&#65292;&#36825;&#20419;&#20351;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#39046;&#22495;&#20013;&#30340;&#35780;&#20272;&#12290;&#20854;&#20013;&#20043;&#19968;&#26159;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;LLMs&#21487;&#20197;&#36890;&#36807;&#21327;&#21161;&#21508;&#31181;&#20219;&#21153;&#22823;&#22823;&#26377;&#30410;&#20110;&#20020;&#24202;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26080;&#27861;&#20805;&#20998;&#24212;&#23545;&#30340;&#26597;&#35810;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20063;&#23481;&#26131;&#20135;&#29983;&#8220;&#24187;&#35273;&#8221;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#24341;&#21457;&#20102;&#20851;&#27880;&#21644;&#24576;&#30097;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#20445;&#20581;&#31038;&#21306;&#20869;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#29702;&#35299;&#21307;&#23398;&#32534;&#30721;&#30340;&#22266;&#26377;&#21547;&#20041;&#65292;&#36825;&#20123;&#32534;&#30721;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21307;&#30103;&#20445;&#20581;&#23454;&#36341;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#29616;&#25104;&#30340;LLMs (&#20363;&#22914;GPT&#12289;LLaMA&#31561;)&#21644;&#19987;&#38376;&#20026;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#35774;&#35745;&#30340;LLMs&#65292;&#20197;&#35780;&#20272;&#23427;&#20204;&#23545;&#36825;&#20123;&#39046;&#22495;&#29305;&#23450;&#26415;&#35821;&#30340;&#35748;&#35782;&#21644;&#29702;&#35299;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10822v1 Announce Type: new  Abstract: The overarching goal of recent AI research has been to make steady progress towards achieving Artificial General Intelligence (AGI), prompting the evaluation of Large Language Models (LLMs) across a variety of tasks and domains. One such domain is healthcare, where LLMs can greatly benefit clinical practice by assisting with a wide range of tasks. However, these models are also prone to producing "hallucinations" or incorrect responses when faced with queries they cannot adequately address, raising concerns and skepticism, especially within the healthcare community. Therefore, in this work, we investigate whether LLMs understand the inherent meaning of medical codes, which are widely used in healthcare practice. We evaluate various off-the-shelf LLMs (e.g., GPT, LLaMA, etc.) and LLMs specifically designed for biomedical applications to assess their awareness and understanding of these domain-specific terminologies. Our results indicate t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.10581</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#30340;ECG&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Large Language Model-informed ECG Dual Attention Network for Heart Failure Risk Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10581
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#29992;&#20110;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#39044;&#27979;&#65292;&#33021;&#22815;&#25429;&#25417;&#22797;&#26434;&#30340;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#26377;&#25928;&#24212;&#23545;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#30001;&#20110;&#20840;&#29699;&#27515;&#20129;&#29575;&#19981;&#26029;&#19978;&#21319;&#32780;&#26500;&#25104;&#37325;&#22823;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#36890;&#36807;&#26089;&#26399;&#35786;&#26029;&#21644;&#39044;&#38450;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#21487;&#26174;&#33879;&#20943;&#23569;&#30142;&#30149;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#20020;&#24202;&#33719;&#21462;&#30340;12&#23548;&#32852;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#36827;&#34892;HF&#39118;&#38505;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#36731;&#37327;&#32423;&#30340;&#21452;&#27880;&#24847;&#21147;ECG&#32593;&#32476;&#65292;&#26088;&#22312;&#25429;&#25417;&#23545;&#26089;&#26399;HF&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#30340;&#22797;&#26434;&#24515;&#30005;&#22270;&#29305;&#24449;&#65292;&#23613;&#31649;&#20302;&#39118;&#38505;&#21644;&#39640;&#39118;&#38505;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#19981;&#24179;&#34913;&#12290;&#35813;&#32593;&#32476;&#20855;&#26377;&#19968;&#20010;&#36328;&#23548;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;12&#20010;&#23548;&#32852;&#29305;&#23450;&#30340;&#26102;&#38388;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#20197;&#25429;&#25417;&#20132;&#21449;&#23548;&#32852;&#20132;&#20114;&#20316;&#29992;&#21644;&#27599;&#20010;&#23548;&#32852;&#20869;&#30340;&#23616;&#37096;&#26102;&#38388;&#21160;&#24577;&#12290;&#20026;&#20102;&#38450;&#27490;&#27169;&#22411;&#36807;&#25311;&#21512;&#20110;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20844;&#20849;ECG-Report&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#29992;&#20110;&#36827;&#34892;ECG-&#25253;&#21578;&#23545;&#40784;&#20219;&#21153;&#12290;&#28982;&#21518;&#23545;&#32593;&#32476;&#36827;&#34892;fine-tune&#20197;&#29992;&#20110;HF&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10581v1 Announce Type: cross  Abstract: Heart failure (HF) poses a significant public health challenge due to its rising global mortality rate. Addressing this issue through early diagnosis and prevention could significantly reduce the disease's impact. This work introduces a methodology for HF risk prediction using clinically acquired 12-lead electrocardiograms (ECGs). We present a novel, lightweight dual-attention ECG network designed to capture complex ECG features essential for early HF prediction, despite the notable imbalance between low and high-risk groups. The network features a cross-lead attention module and twelve lead-specific temporal attention modules to capture cross-lead interactions and local temporal dynamics within each lead. To prevent model overfitting from limited training data, we leverage a large language model (LLM) with a public ECG-Report dataset for pretraining on an ECG-report alignment task. The network is then fine-tuned for HF risk prediction
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.09738</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#25512;&#33616;&#20013;&#29983;&#25104;&#29992;&#25143;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models as Generative User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09738
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#24335;&#29992;&#25143;&#27169;&#25311;&#22120;&#22312;&#23545;&#35805;&#25512;&#33616;&#20013;&#23637;&#29616;&#20986;&#28508;&#21147;&#65292;&#26032;&#30340;&#21327;&#35758;&#36890;&#36807;&#20116;&#20010;&#20219;&#21153;&#35780;&#20272;&#20102;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#20934;&#30830;&#31243;&#24230;&#65292;&#25581;&#31034;&#20102;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#29992;&#25143;&#26159;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#20013;&#25104;&#26412;&#25928;&#30410;&#36739;&#39640;&#30340;&#30495;&#23454;&#29992;&#25143;&#20195;&#29702;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#22312;&#27169;&#25311;&#31867;&#20284;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#24341;&#21457;&#20102;&#23427;&#20204;&#33021;&#21542;&#20195;&#34920;&#22810;&#26679;&#21270;&#29992;&#25143;&#32676;&#20307;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#34913;&#37327;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#27169;&#25311;&#23545;&#35805;&#25512;&#33616;&#20013;&#20154;&#31867;&#34892;&#20026;&#30340;&#31243;&#24230;&#12290;&#35813;&#21327;&#35758;&#30001;&#20116;&#20010;&#20219;&#21153;&#32452;&#25104;&#65292;&#27599;&#20010;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;&#21512;&#25104;&#29992;&#25143;&#24212;&#35813;&#34920;&#29616;&#20986;&#30340;&#20851;&#38190;&#29305;&#24615;&#65306;&#36873;&#25321;&#35201;&#35848;&#35770;&#30340;&#29289;&#21697;&#65292;&#34920;&#36798;&#20108;&#36827;&#21046;&#20559;&#22909;&#65292;&#34920;&#36798;&#24320;&#25918;&#24335;&#20559;&#22909;&#65292;&#35831;&#27714;&#25512;&#33616;&#20197;&#21450;&#25552;&#20379;&#21453;&#39304;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#27169;&#25311;&#22120;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#26377;&#25928;&#22320;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#34892;&#20026;&#30340;&#20559;&#24046;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#22914;&#20309;&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#21644;&#25552;&#31034;&#31574;&#30053;&#20943;&#23569;&#36825;&#20123;&#20559;&#24046;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09738v1 Announce Type: cross  Abstract: Synthetic users are cost-effective proxies for real users in the evaluation of conversational recommender systems. Large language models show promise in simulating human-like behavior, raising the question of their ability to represent a diverse population of users. We introduce a new protocol to measure the degree to which language models can accurately emulate human behavior in conversational recommendation. This protocol is comprised of five tasks, each designed to evaluate a key property that a synthetic user should exhibit: choosing which items to talk about, expressing binary preferences, expressing open-ended preferences, requesting recommendations, and giving feedback. Through evaluation of baseline simulators, we demonstrate these tasks effectively reveal deviations of language models from human behavior, and offer insights on how to reduce the deviations with model selection and prompting strategies.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.09611</link><description>&lt;p&gt;
MM1&#65306;&#22810;&#27169;&#24335;LLM&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#12289;&#20998;&#26512;&#19982;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
MM1: Methods, Analysis &amp; Insights from Multimodal LLM Pre-training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09611
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#30830;&#23450;&#20102;&#23545;&#20110;&#23454;&#29616;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#26368;&#26032;&#28526;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#30340;&#20851;&#38190;&#35774;&#35745;&#32463;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#26550;&#26500;&#32452;&#20214;&#21644;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#32534;&#30721;&#22120;&#12289;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#25968;&#25454;&#36873;&#25321;&#36827;&#34892;&#20180;&#32454;&#21644;&#20840;&#38754;&#30340;&#28040;&#34701;&#23454;&#39564;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#30340;&#35774;&#35745;&#32463;&#39564;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#22823;&#35268;&#27169;&#22810;&#27169;&#24335;&#39044;&#35757;&#32451;&#20351;&#29992;&#20180;&#32454;&#28151;&#21512;&#30340;&#22270;&#20687;&#26631;&#39064;&#12289;&#20132;&#26367;&#22270;&#20687;&#25991;&#26412;&#21644;&#20165;&#25991;&#26412;&#25968;&#25454;&#23545;&#20110;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#26368;&#26032;&#28526;&#65288;SOTA&#65289;&#30340;&#23569;&#26679;&#26412;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#20854;&#20182;&#24050;&#21457;&#34920;&#30340;&#39044;&#35757;&#32451;&#32467;&#26524;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#22270;&#20687;&#32534;&#30721;&#22120;&#36830;&#21516;&#22270;&#20687;&#20998;&#36776;&#29575;&#21644;&#22270;&#20687;&#26631;&#35760;&#35745;&#25968;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#36830;&#25509;&#22120;&#35774;&#35745;&#30456;&#23545;&#37325;&#35201;&#24615;&#36739;&#23567;&#12290;&#36890;&#36807;&#25193;&#22823;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;MM1&#65292;&#19968;&#20010;&#22810;&#27169;&#24335;&#27169;&#22411;&#31995;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09611v1 Announce Type: cross  Abstract: In this work, we discuss building performant Multimodal Large Language Models (MLLMs). In particular, we study the importance of various architecture components and data choices. Through careful and comprehensive ablations of the image encoder, the vision language connector, and various pre-training data choices, we identified several crucial design lessons. For example, we demonstrate that for large-scale multimodal pre-training using a careful mix of image-caption, interleaved image-text, and text-only data is crucial for achieving state-of-the-art (SOTA) few-shot results across multiple benchmarks, compared to other published pre-training results. Further, we show that the image encoder together with image resolution and the image token count has substantial impact, while the vision-language connector design is of comparatively negligible importance. By scaling up the presented recipe, we build MM1, a family of multimodal models up 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.09530</link><description>&lt;p&gt;
VisionGPT-3D:&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#36890;&#29992;&#22810;&#27169;&#24577;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
VisionGPT-3D: A Generalized Multimodal Agent for Enhanced 3D Vision Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09530
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#25552;&#21319;&#35745;&#31639;&#26426;&#35270;&#35273;&#23545;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21521;&#35270;&#35273;&#32452;&#20214;&#30340;&#28436;&#36827;&#20419;&#36827;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#65292;&#20363;&#22914;&#20174;&#25991;&#26412;&#29983;&#25104;&#22270;&#20687;&#12289;&#35270;&#39057;&#24182;&#35782;&#21035;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#20803;&#32032;&#12290;&#20197;&#21069;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#19987;&#27880;&#20110;&#22522;&#20110;&#26126;&#30830;&#23450;&#20041;&#23545;&#35937;&#30340;&#22270;&#20687;&#26816;&#27979;&#12289;&#20998;&#31867;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#35270;&#35273;&#23545;&#35937;&#65292;&#20026;&#25991;&#26412;&#32972;&#26223;&#25552;&#20379;&#20102;&#35270;&#35273;&#24067;&#23616;&#12290;OpenAI GPT-4&#24050;&#25104;&#20026;LLMs&#30340;&#39030;&#23792;&#65292;&#32780;&#35745;&#31639;&#26426;&#35270;&#35273;(CV)&#39046;&#22495;&#25317;&#26377;&#22823;&#37327;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#23558;2D&#22270;&#20687;&#36716;&#25442;&#20026;&#23427;&#20204;&#30340;3D&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#31639;&#27861;&#19982;&#38382;&#39064;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#21487;&#33021;&#23548;&#33268;&#19981;&#33391;&#32467;&#26524;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;VisionGPT-3D&#26694;&#26550;&#65292; conslidate&#20102;&#26368;&#20808;&#36827;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09530v1 Announce Type: cross  Abstract: The evolution of text to visual components facilitates people's daily lives, such as generating image, videos from text and identifying the desired elements within the images. Computer vision models involving the multimodal abilities in the previous days are focused on image detection, classification based on well-defined objects. Large language models (LLMs) introduces the transformation from nature language to visual objects, which present the visual layout for text contexts. OpenAI GPT-4 has emerged as the pinnacle in LLMs, while the computer vision (CV) domain boasts a plethora of state-of-the-art (SOTA) models and algorithms to convert 2D images to their 3D representations. However, the mismatching between the algorithms with the problem could lead to undesired results. In response to this challenge, we propose an unified VisionGPT-3D framework to consolidate the state-of-the-art vision models, thereby facilitating the development
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.05574</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#36827;&#34892;&#35748;&#30693;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05574
&lt;/p&gt;
&lt;p&gt;
&#36825;&#19968;&#21019;&#26032;&#24515;&#29702;&#27835;&#30103;&#27169;&#22411;HealMe&#36890;&#36807;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24605;&#32500;&#65292;&#24182;&#20419;&#36827;&#20102;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24515;&#29702;&#27835;&#30103;&#20013;&#21487;&#20197;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#29087;&#32451;&#22788;&#29702;&#35748;&#30693;&#37325;&#26500;&#31561;&#20851;&#38190;&#20219;&#21153;&#65292;&#20811;&#26381;&#32670;&#32827;&#12289;&#19981;&#20449;&#20219;&#12289;&#27835;&#30103;&#24072;&#25216;&#33021;&#24046;&#24322;&#21644;&#36164;&#28304;&#31232;&#32570;&#31561;&#25361;&#25112;&#12290;&#22312;&#20808;&#21069;&#30340;&#35748;&#30693;&#37325;&#26500;&#20013;&#65292;&#20027;&#35201;&#23558;&#36127;&#38754;&#24773;&#32490;&#36716;&#21270;&#20026;&#31215;&#26497;&#30340;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#25928;&#26524;&#26377;&#38480;&#65292;&#32463;&#24120;&#19981;&#33021;&#20419;&#36827;&#23458;&#25143;&#33258;&#25105;&#21457;&#29616;&#26367;&#20195;&#35270;&#35282;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24110;&#21161;&#21644;&#36171;&#33021;&#36890;&#36807;&#33258;&#36866;&#24212;&#35821;&#35328;&#22312;&#24515;&#29702;&#22686;&#24378;&#65288;HealMe&#65289;&#27169;&#22411;&#12290;&#36825;&#31181;&#26032;&#39062;&#30340;&#35748;&#30693;&#37325;&#26500;&#30103;&#27861;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26681;&#28145;&#33922;&#22266;&#30340;&#36127;&#38754;&#24819;&#27861;&#65292;&#24182;&#20419;&#36827;&#29702;&#24615;&#12289;&#24179;&#34913;&#30340;&#35270;&#35282;&#12290;HealMe&#19982;&#20256;&#32479;LLM&#26041;&#27861;&#19981;&#21516;&#65292;&#37319;&#29992;&#22522;&#20110;&#24515;&#29702;&#27835;&#30103;&#26694;&#26550;&#30340;&#20849;&#24773;&#23545;&#35805;&#12290;&#23427;&#36890;&#36807;&#31995;&#32479;&#25351;&#23548;&#23458;&#25143;&#21306;&#20998;&#24773;&#22659;&#21644;&#24863;&#21463;&#65292;&#38598;&#24605;&#24191;&#30410;&#23547;&#25214;&#26367;&#20195;&#35270;&#35282;&#65292;&#24182;&#21046;&#23450;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05574v1 Announce Type: cross  Abstract: Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients' self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#29992;&#20110;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#23398;&#20998;&#26512;&#21644;&#32479;&#35745;&#30740;&#31350;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.04639</link><description>&lt;p&gt;
MaCmS&#65306;&#39532;&#21152;&#24076;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#38598;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MaCmS: Magahi Code-mixed Dataset for Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04639
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#39318;&#20010;&#29992;&#20110;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#23398;&#20998;&#26512;&#21644;&#32479;&#35745;&#30740;&#31350;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24773;&#24863;&#25968;&#25454;&#38598; MaCMS&#65292;&#29992;&#20110;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#65288;MHE&#65289;&#20195;&#30721;&#28151;&#21512;&#35821;&#35328;&#65292;&#20854;&#20013;&#39532;&#21152;&#24076;&#35821;&#26159;&#19968;&#31181;&#36164;&#28304;&#36739;&#23569;&#30340;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#31532;&#19968;&#20010;&#39532;&#21152;&#24076;&#35821;-&#21360;&#22320;&#35821;-&#33521;&#35821;&#20195;&#30721;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#35821;&#35328;&#23398;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#20195;&#30721;&#28151;&#21512;&#30340;&#32467;&#26500;&#65292;&#24182;&#36827;&#34892;&#20102;&#32479;&#35745;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#26497;&#24615;&#21457;&#35328;&#32773;&#30340;&#35821;&#35328;&#20559;&#22909;&#12290;&#36890;&#36807;&#36825;&#20123;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#22522;&#20934;&#27169;&#22411;&#26469;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04639v1 Announce Type: new  Abstract: The present paper introduces new sentiment data, MaCMS, for Magahi-Hindi-English (MHE) code-mixed language, where Magahi is a less-resourced minority language. This dataset is the first Magahi-Hindi-English code-mixed dataset for sentiment analysis tasks. Further, we also provide a linguistics analysis of the dataset to understand the structure of code-mixing and a statistical study to understand the language preferences of speakers with different polarities. With these analyses, we also train baseline models to evaluate the dataset's quality.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.02893</link><description>&lt;p&gt;
&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Cross-Lingual Document-Level Event Causality Identification with Heterogeneous Graph Contrastive Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02893
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24322;&#26500;&#22270;&#23545;&#27604;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#22312;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#22240;&#26524;&#35782;&#21035;&#65288;ECI&#65289;&#25351;&#30340;&#26159;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#20107;&#20214;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#39640;&#36164;&#28304;&#35821;&#35328;&#19979;&#30340;&#21477;&#23376;&#32423;ECI&#65292;&#32780;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#19979;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#25991;&#26723;&#32423;ECI&#65288;DECI&#65289;&#21364;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#65288;GIMC&#65289;&#30340;&#24322;&#26500;&#22270;&#20132;&#20114;&#27169;&#22411;&#65292;&#29992;&#20110;&#23454;&#29616;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#25991;&#26723;&#32423;ECI&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#24322;&#26500;&#22270;&#20132;&#20114;&#32593;&#32476;&#26469;&#24314;&#27169;&#25991;&#26723;&#20013;&#20998;&#25955;&#20107;&#20214;&#20043;&#38388;&#30340;&#36828;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#25552;&#39640;&#20174;&#28304;&#35821;&#35328;&#23398;&#20064;&#21040;&#30340;&#22240;&#26524;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#21487;&#36716;&#31227;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#31890;&#24230;&#23545;&#27604;&#20256;&#36882;&#23398;&#20064;&#27169;&#22359;&#65292;&#20197;&#35843;&#25972;&#36328;&#35821;&#35328;&#38388;&#30340;&#22240;&#26524;&#34920;&#31034;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#24179;&#22343;F1&#24471;&#20998;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#32422;9.4%&#21644;8.2%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02893v1 Announce Type: cross  Abstract: Event Causality Identification (ECI) refers to detect causal relations between events in texts. However, most existing studies focus on sentence-level ECI with high-resource language, leaving more challenging document-level ECI (DECI) with low-resource languages under-explored. In this paper, we propose a Heterogeneous Graph Interaction Model with Multi-granularity Contrastive Transfer Learning (GIMC) for zero-shot cross-lingual document-level ECI. Specifically, we introduce a heterogeneous graph interaction network to model the long-distance dependencies between events that are scattered over document. Then, to improve cross-lingual transferability of causal knowledge learned from source language, we propose a multi-granularity contrastive transfer learning module to align the causal representations across languages. Extensive experiments show our framework outperforms previous state-of-the-art model by 9.4% and 8.2% of average F1 sco
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"</title><link>https://arxiv.org/abs/2403.01479</link><description>&lt;p&gt;
Align-to-Distill: &#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01479
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;Transformer&#26550;&#26500;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#23450;&#35201;&#20174;&#21738;&#20123;&#25945;&#24072;&#23618;&#20013;&#33976;&#39311;&#30693;&#35782;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#26469;&#35299;&#20915;&#29305;&#24449;&#26144;&#23556;&#38382;&#39064;&#12290;A2D&#20013;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#27169;&#22359;&#25191;&#34892;&#23398;&#29983;&#21644;&#25945;&#24072;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#30340;&#23494;&#38598;&#36880;&#22836;&#27604;&#36739;&#65292;&#23558;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De-&gt;Dsb and WMT-2014 En-&gt;De, respe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FCTR&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33521;&#35821;&#20197;&#22806;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22303;&#32819;&#20854;&#35821;&#65292;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#38598;&#28508;&#21147;&#25512;&#21160;&#22303;&#32819;&#20854;&#35821;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.00411</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#23398;&#20064;&#19982;&#20302;&#36164;&#28304;&#24494;&#35843;&#65306;&#20197;&#22303;&#32819;&#20854;&#20107;&#23454;&#26816;&#26597;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Learning vs. Low-Resource Fine-Tuning: A Case Study with Fact-Checking in Turkish
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00411
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FCTR&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#35299;&#20915;&#33521;&#35821;&#20197;&#22806;&#35821;&#35328;&#65292;&#23588;&#20854;&#26159;&#22303;&#32819;&#20854;&#35821;&#65292;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#25968;&#25454;&#38598;&#28508;&#21147;&#25512;&#21160;&#22303;&#32819;&#20854;&#35821;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#36805;&#36895;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20854;&#23545;&#20844;&#20247;&#33286;&#35770;&#30340;&#24433;&#21709;&#30340;&#25285;&#24551;&#12290;&#23613;&#31649;&#38169;&#35823;&#20449;&#24687;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#35813;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#22240;&#27492;&#65292;&#20854;&#20182;&#35821;&#35328;&#65292;&#21253;&#25324;&#22303;&#32819;&#20854;&#35821;&#65292;&#30340;&#25968;&#25454;&#38598;&#31232;&#32570;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#30001;3238&#20010;&#30495;&#23454;&#22768;&#26126;&#32452;&#25104;&#30340;FCTR&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#24182;&#25972;&#21512;&#20102;&#19977;&#23478;&#22303;&#32819;&#20854;&#20107;&#23454;&#26816;&#26597;&#32452;&#32455;&#25910;&#38598;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26088;&#22312;&#35780;&#20272;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#22303;&#32819;&#20854;&#35821;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;&#38646;&#27425;&#21644;&#23569;&#27425;&#65289;&#34920;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25968;&#25454;&#38598;&#26377;&#25512;&#21160;&#22303;&#32819;&#20854;&#35821;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00411v1 Announce Type: new  Abstract: The rapid spread of misinformation through social media platforms has raised concerns regarding its impact on public opinion. While misinformation is prevalent in other languages, the majority of research in this field has concentrated on the English language. Hence, there is a scarcity of datasets for other languages, including Turkish. To address this concern, we have introduced the FCTR dataset, consisting of 3238 real-world claims. This dataset spans multiple domains and incorporates evidence collected from three Turkish fact-checking organizations. Additionally, we aim to assess the effectiveness of cross-lingual transfer learning for low-resource languages, with a particular focus on Turkish. We demonstrate in-context learning (zero-shot and few-shot) performance of large language models in this context. The experimental results indicate that the dataset has the potential to advance research in the Turkish language.
&lt;/p&gt;</description></item><item><title>ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15220</link><description>&lt;p&gt;
ChunkAttention: &#20855;&#26377;&#21069;&#32512;&#24863;&#30693;KV&#32531;&#23384;&#21644;&#20004;&#38454;&#27573;&#20998;&#21306;&#30340;&#39640;&#25928;&#33258;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15220
&lt;/p&gt;
&lt;p&gt;
ChunkAttention&#26159;&#19968;&#31181;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36890;&#36807;&#23558;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#24182;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#65292;&#23454;&#29616;&#20102;&#22312;&#36816;&#34892;&#26102;&#25913;&#21892;&#20869;&#23384;&#21033;&#29992;&#29575;&#30340;KV&#32531;&#23384;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#20197;&#25552;&#39640;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#23545;&#20110;&#38271;&#24207;&#21015;&#26469;&#35828;&#26159;&#25512;&#29702;&#24310;&#36831;&#30340;&#19968;&#20010;&#26174;&#33879;&#26469;&#28304;&#12290;&#22312;&#22810;&#31199;&#25143;LLMs&#26381;&#21153;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;LLM&#35831;&#27714;&#22312;&#21069;&#32512;&#20013;&#20849;&#20139;&#31995;&#32479;&#25552;&#31034;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25805;&#20316;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ChunkAttention&#65292;&#19968;&#31181;&#20855;&#26377;&#21069;&#32512;&#24863;&#30693;&#30340;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#21487;&#20197;&#22312;&#36816;&#34892;&#26102;&#26816;&#27979;&#22810;&#20010;&#35831;&#27714;&#20043;&#38388;&#21305;&#37197;&#30340;&#25552;&#31034;&#21069;&#32512;&#65292;&#24182;&#20849;&#20139;&#23427;&#20204;&#30340;&#38190;/&#20540;&#24352;&#37327;&#20197;&#25913;&#36827;KV&#32531;&#23384;&#30340;&#20869;&#23384;&#21033;&#29992;&#29575;&#12290;&#36825;&#26159;&#36890;&#36807;&#23558;&#25972;&#20307;&#38190;/&#20540;&#24352;&#37327;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#22359;&#65292;&#24182;&#23558;&#23427;&#20204;&#32467;&#26500;&#21270;&#21040;&#36741;&#21161;&#21069;&#32512;&#26641;&#20013;&#26469;&#23454;&#29616;&#30340;&#12290;&#22240;&#27492;&#65292;&#22312;&#22522;&#20110;&#21069;&#32512;&#26641;&#30340;KV&#32531;&#23384;&#20043;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#33258;&#27880;&#24847;&#21147;&#20869;&#26680;&#65292;&#20854;&#20013;&#23454;&#29616;&#20102;&#20004;&#38454;&#27573;&#20998;&#21306;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#33258;&#27880;&#24847;&#21147;&#35745;&#31639;&#20013;&#30340;&#25968;&#25454;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15220v1 Announce Type: cross  Abstract: Self-attention is an essential component of large language models(LLMs) but a significant source of inference latency for long sequences. In multi-tenant LLMs serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#21407;&#22987;&#21477;&#23376;&#20013;&#39044;&#27979;&#35789;&#27719;&#20462;&#25913;&#65292;&#24341;&#20837;LLM&#22686;&#24378;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#23558;&#22797;&#26434;&#35789;&#26367;&#25442;&#20026;&#31616;&#21333;&#35789;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14704</link><description>&lt;p&gt;
&#19968;&#31181;LLM&#22686;&#24378;&#30340;&#35789;&#27719;&#31616;&#21270;&#23545;&#25239;&#32534;&#36753;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An LLM-Enhanced Adversarial Editing System for Lexical Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14704
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35789;&#27719;&#31616;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#65292;&#22312;&#21407;&#22987;&#21477;&#23376;&#20013;&#39044;&#27979;&#35789;&#27719;&#20462;&#25913;&#65292;&#24341;&#20837;LLM&#22686;&#24378;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#25552;&#28860;&#65292;&#24182;&#37319;&#29992;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#23558;&#22797;&#26434;&#35789;&#26367;&#25442;&#20026;&#31616;&#21333;&#35789;&#65292;&#23454;&#39564;&#35777;&#26126;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#26088;&#22312;&#22312;&#35789;&#27719;&#32423;&#21035;&#31616;&#21270;&#25991;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#26631;&#27880;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LS&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#24179;&#34892;&#35821;&#26009;&#24211;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#23545;&#25239;&#32534;&#36753;&#31995;&#32479;&#65292;&#24182;&#32467;&#21512;&#28151;&#28102;&#25439;&#22833;&#21644;&#19981;&#21464;&#24615;&#25439;&#22833;&#26469;&#39044;&#27979;&#21407;&#22987;&#21477;&#23376;&#20013;&#30340;&#35789;&#27719;&#20462;&#25913;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;LLM&#22686;&#24378;&#25439;&#22833;&#65292;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#25552;&#28860;&#25104;&#23567;&#22411;LS&#31995;&#32479;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21477;&#23376;&#20013;&#30340;&#22797;&#26434;&#35789;&#34987;&#23631;&#34109;&#65292;&#21046;&#20316;&#20102;&#19968;&#20010;&#22522;&#20110;&#38590;&#24230;&#24863;&#30693;&#30340;&#22635;&#20805;&#27169;&#22359;&#65292;&#29992;&#26356;&#31616;&#21333;&#30340;&#35789;&#26367;&#25442;&#23631;&#34109;&#20301;&#32622;&#12290;&#26368;&#21518;&#65292;&#23545;&#19977;&#20010;&#22522;&#20934;LS&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14704v1 Announce Type: new  Abstract: Lexical Simplification (LS) aims to simplify text at the lexical level. Existing methods rely heavily on annotated data, making it challenging to apply in low-resource scenarios. In this paper, we propose a novel LS method without parallel corpora. This method employs an Adversarial Editing System with guidance from a confusion loss and an invariance loss to predict lexical edits in the original sentences. Meanwhile, we introduce an innovative LLM-enhanced loss to enable the distillation of knowledge from Large Language Models (LLMs) into a small-size LS system. From that, complex words within sentences are masked and a Difficulty-aware Filling module is crafted to replace masked positions with simpler words. At last, extensive experimental results and analyses on three benchmark LS datasets demonstrate the effectiveness of our proposed method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14428</link><description>&lt;p&gt;
KoCoSa: &#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
KoCoSa: Korean Context-aware Sarcasm Detection Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14428
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;KoCoSa&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#38024;&#23545;&#35813;&#20219;&#21153;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35773;&#21050;&#26159;&#19968;&#31181;&#35328;&#35821;&#35773;&#21050;&#30340;&#26041;&#24335;&#65292;&#25351;&#30340;&#26159;&#26377;&#20154;&#35828;&#20102;&#21644;&#20182;&#20204;&#30340;&#26412;&#24847;&#30456;&#21453;&#30340;&#35805;&#65292;&#36890;&#24120;&#26159;&#20026;&#20102;&#22066;&#31505;&#19968;&#20010;&#20154;&#12289;&#24773;&#20917;&#25110;&#24819;&#27861;&#12290;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#35773;&#21050;&#36890;&#24120;&#26159;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#26816;&#27979;&#35773;&#21050;&#24212;&#35813;&#21453;&#26144;&#19978;&#19979;&#25991;&#65288;&#21363;&#23545;&#35805;&#21382;&#21490;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#38889;&#25991;&#23545;&#35805;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#26032;&#25968;&#25454;&#38598;KoCoSa&#65288;&#38889;&#25991;&#19978;&#19979;&#25991;&#24863;&#30693;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#65289;&#65292;&#21253;&#25324;12.8K&#20010;&#26085;&#24120;&#38889;&#25991;&#23545;&#35805;&#20197;&#21450;&#35813;&#20219;&#21153;&#22312;&#26368;&#21518;&#19968;&#27425;&#22238;&#22797;&#19978;&#30340;&#26631;&#31614;&#12290;&#20026;&#20102;&#26500;&#24314;&#35813;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35773;&#21050;&#26816;&#27979;&#25968;&#25454;&#38598;&#29983;&#25104;&#27969;&#31243;&#65306;1&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#28304;&#23545;&#35805;&#20013;&#29983;&#25104;&#26032;&#30340;&#35773;&#21050;&#23545;&#35805;&#65292;2&#65289;&#33258;&#21160;&#21644;&#25163;&#21160;&#36807;&#28388;&#24322;&#24120;&#21644;&#26377;&#27602;&#23545;&#35805;&#65292;3&#65289;&#20026;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#38024;&#23545;&#38889;&#25991;&#35773;&#21050;&#26816;&#27979;&#20219;&#21153;&#30340;&#22522;&#32447;&#65292;&#35813;&#22522;&#32447;&#26159;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14428v1 Announce Type: cross  Abstract: Sarcasm is a way of verbal irony where someone says the opposite of what they mean, often to ridicule a person, situation, or idea. It is often difficult to detect sarcasm in the dialogue since detecting sarcasm should reflect the context (i.e., dialogue history). In this paper, we introduce a new dataset for the Korean dialogue sarcasm detection task, KoCoSa (Korean Context-aware Sarcasm Detection Dataset), which consists of 12.8K daily Korean dialogues and the labels for this task on the last response. To build the dataset, we propose an efficient sarcasm detection dataset generation pipeline: 1) generating new sarcastic dialogues from source dialogues with large language models, 2) automatic and manual filtering of abnormal and toxic dialogues, and 3) human annotation for the sarcasm detection task. We also provide a simple but effective baseline for the Korean sarcasm detection task trained on our dataset. Experimental results on t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.10311</link><description>&lt;p&gt;
&#21517;&#35789;&#30701;&#35821;&#20013;&#22836;&#37096;&#30340;&#26368;&#20339;&#20301;&#32622;&#12290;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#30340;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The optimal placement of the head in the noun phrase. The case of demonstrative, numeral, adjective and noun
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#22312;&#21517;&#35789;&#30701;&#35821;&#20013;&#30340;&#20914;&#31361;&#65292;&#32467;&#35770;&#26174;&#31034;&#24403;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#19988;&#21333;&#35789;&#36739;&#30701;&#26102;&#65292;&#24847;&#22806;&#20943;&#23569;&#21487;&#33021;&#20250;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21477;&#35805;&#30340;&#35789;&#24207;&#30001;&#22810;&#31181;&#21407;&#21017;&#22609;&#36896;&#12290;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#21407;&#21017;&#19982;&#24847;&#22806;&#20943;&#23569;&#26368;&#23567;&#21270;&#21407;&#21017;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#22312;&#21333;&#19968;&#22836;&#37096;&#30340;&#21477;&#27861;&#20381;&#36182;&#32467;&#26500;&#20013;&#23384;&#22312;&#20914;&#31361;&#65306;&#21069;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#32447;&#24615;&#25490;&#21015;&#30340;&#20013;&#24515;&#65292;&#21518;&#32773;&#39044;&#27979;&#22836;&#37096;&#24212;&#35813;&#25918;&#32622;&#22312;&#20004;&#31471;&#20043;&#19968;&#65288;&#35201;&#20040;&#22312;&#39318;&#20301;&#65292;&#35201;&#20040;&#22312;&#26411;&#20301;&#65289;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#20309;&#26102;&#24847;&#22806;&#20943;&#23569;&#65288;&#25110;&#21487;&#39044;&#27979;&#24615;&#26368;&#22823;&#21270;&#65289;&#24212;&#35813;&#36229;&#36234;&#21477;&#27861;&#20381;&#36182;&#36317;&#31163;&#26368;&#23567;&#21270;&#12290;&#22312;&#21333;&#19968;&#22836;&#37096;&#32467;&#26500;&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#27979;&#22312;&#28385;&#36275;&#20004;&#20010;&#26465;&#20214;&#26102;&#26356;&#26377;&#21487;&#33021;&#21457;&#29983;&#65292;&#21363;&#65288;a&#65289;&#28041;&#21450;&#30340;&#21333;&#35789;&#36739;&#23569;&#65292;&#24182;&#19988;&#65288;b&#65289;&#21333;&#35789;&#36739;&#30701;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#30001;&#25351;&#31034;&#35821;&#12289;&#25968;&#35789;&#12289;&#24418;&#23481;&#35789;&#21644;&#21517;&#35789;&#32452;&#25104;&#30340;&#21517;&#35789;&#30701;&#35821;&#19978;&#27979;&#35797;&#20102;&#36825;&#19968;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#39318;&#36873;&#39034;&#24207;&#20013;...&#65288;&#32570;&#22833;&#37096;&#20998;&#26080;&#27861;&#25552;&#20379;&#23436;&#25972;&#32763;&#35793;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10311v1 Announce Type: new  Abstract: The word order of a sentence is shaped by multiple principles. The principle of syntactic dependency distance minimization is in conflict with the principle of surprisal minimization (or predictability maximization) in single head syntactic dependency structures: while the former predicts that the head should be placed at the center of the linear arrangement, the latter predicts that the head should be placed at one of the ends (either first or last). A critical question is when surprisal minimization (or predictability maximization) should surpass syntactic dependency distance minimization. In the context of single head structures, it has been predicted that this is more likely to happen when two conditions are met, i.e. (a) fewer words are involved and (b) words are shorter. Here we test the prediction on the noun phrase when its composed of a demonstrative, a numeral, an adjective and a noun. We find that, across preferred orders in l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20316;&#20026;&#36890;&#29992;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26500;&#24314;&#36890;&#29992;&#20998;&#31867;&#22120;&#30340;&#35814;&#32454;&#27493;&#39588;&#65292;&#24182;&#20998;&#20139;&#20102;&#35813;&#36890;&#29992;&#20998;&#31867;&#22120;&#22312;33&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;</title><link>https://arxiv.org/abs/2312.17543</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26500;&#24314;&#39640;&#25928;&#30340;&#36890;&#29992;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Building Efficient Universal Classifiers with Natural Language Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20316;&#20026;&#36890;&#29992;&#20998;&#31867;&#20219;&#21153;&#65292;&#25552;&#20379;&#20102;&#26500;&#24314;&#36890;&#29992;&#20998;&#31867;&#22120;&#30340;&#35814;&#32454;&#27493;&#39588;&#65292;&#24182;&#20998;&#20139;&#20102;&#35813;&#36890;&#29992;&#20998;&#31867;&#22120;&#22312;33&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17543v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;&#20132;&#21449;&#12290;&#29983;&#25104;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#25104;&#20026;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#38646;&#26679;&#26412;&#23398;&#20064;&#30340;&#20027;&#27969;&#36873;&#25321;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#25991;&#26412;&#29983;&#25104;&#30340;&#36890;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29992;&#25143;&#22312;&#21482;&#24819;&#33258;&#21160;&#21270;&#19968;&#20010;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#24182;&#19981;&#38656;&#35201;&#29983;&#25104;&#22411;LLMs&#30340;&#24191;&#27867;&#33021;&#21147;&#12290;&#36739;&#23567;&#30340;&#31867;&#20284;BERT&#30340;&#27169;&#22411;&#20063;&#21487;&#20197;&#23398;&#20064;&#36890;&#29992;&#20219;&#21153;&#65292;&#36825;&#20351;&#23427;&#20204;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#24494;&#35843;&#65288;&#38646;&#26679;&#26412;&#20998;&#31867;&#65289;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#20309;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#65292;&#25110;&#32773;&#21482;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#23569;&#26679;&#26412;&#65289;&#65292;&#21516;&#26102;&#27604;&#29983;&#25104;&#22411;LLMs&#39640;&#25928;&#24471;&#22810;&#12290;&#26412;&#25991;(1) &#35299;&#37322;&#20102;&#22914;&#20309;&#23558;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#20316;&#20026;&#36890;&#29992;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#21407;&#29702;&#31867;&#20284;&#20110;&#29983;&#25104;&#22411;LLMs&#30340;&#25351;&#23548;&#24494;&#35843;&#65292;(2) &#25552;&#20379;&#20102;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#20998;&#31867;&#22120;&#30340;&#21487;&#37325;&#29992;Jupyter&#31508;&#35760;&#26412;&#30340;&#36880;&#27493;&#25351;&#21335;&#65292;(3) &#20849;&#20139;&#20102;&#32463;&#36807;&#35757;&#32451;&#30340;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;&#22312;33&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17543v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs) have become the mainstream choice for fewshot and zeroshot learning thanks to the universality of text generation. Many users, however, do not need the broad capabilities of generative LLMs when they only want to automate a classification task. Smaller BERT-like models can also learn universal tasks, which allow them to do any text classification task without requiring fine-tuning (zeroshot classification) or to learn new tasks with only a few examples (fewshot), while being significantly more efficient than generative LLMs. This paper (1) explains how Natural Language Inference (NLI) can be used as a universal classification task that follows similar principles as instruction fine-tuning of generative LLMs, (2) provides a step-by-step guide with reusable Jupyter notebooks for building a universal classifier, and (3) shares the resulting universal classifier that is trained on 33 datasets
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.09682</link><description>&lt;p&gt;
MacGyver&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#32773;&#65311;
&lt;/p&gt;
&lt;p&gt;
MacGyver: Are Large Language Models Creative Problem Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09682
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;MACGYVER&#25968;&#25454;&#38598;&#24182;&#19982;&#20154;&#31867;&#27604;&#36739;&#65292;&#30740;&#31350;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#29420;&#20855;&#25361;&#25112;&#24615;&#65292;&#22312;&#30693;&#35782;&#24191;&#24230;&#21644;&#21487;&#34892;&#24615;&#26041;&#38754;&#19982;&#20154;&#31867;&#23384;&#22312;&#29420;&#29305;&#24046;&#24322;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#19968;&#20010;&#20840;&#26032;&#30340;&#32422;&#26463;&#35774;&#32622;&#20013;&#25506;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#24847;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;MACGYVER&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#21160;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#36229;&#36807;1600&#20010;&#29305;&#24847;&#35774;&#35745;&#30340;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65292;&#26088;&#22312;&#24341;&#21457;&#29289;&#20307;&#30340;&#21019;&#26032;&#20351;&#29992;&#65292;&#24182;&#38656;&#35201;&#36229;&#36234;&#24120;&#35268;&#24605;&#32500;&#12290;&#25105;&#20204;&#38543;&#21518;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20154;&#31867;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#27604;&#36739;&#21644;&#23545;&#27604;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;MACGYVER&#23545;&#36825;&#20004;&#20010;&#32676;&#20307;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20294;&#20197;&#29420;&#29305;&#21644;&#20114;&#34917;&#30340;&#26041;&#24335;&#21576;&#29616;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25797;&#38271;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#20294;&#22312;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#19978;&#26377;&#22256;&#38590;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24046;&#24322;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26292;&#38706;&#20110;&#21508;&#31181;&#19987;&#19994;&#30693;&#35782;&#65292;&#23581;&#35797;&#26356;&#24191;&#27867;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#25552;&#20986;&#29289;&#29702;&#19978;&#19981;&#21487;&#34892;&#30340;&#34892;&#21160;&#26102;&#22833;&#36133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#38169;&#35823;&#20998;&#26512;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
&lt;/p&gt;</description></item><item><title>PhoGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#36234;&#21335;&#35821;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#31995;&#21015;&#65292;&#20855;&#26377;40&#20159;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;PhoGPT-4B&#20197;&#21450;&#20854;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#65292;&#23637;&#31034;&#20102;&#22312;&#36234;&#21335;&#35821;&#20219;&#21153;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;7&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.02945</link><description>&lt;p&gt;
PhoGPT: &#36234;&#21335;&#35821;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PhoGPT: Generative Pre-training for Vietnamese
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02945
&lt;/p&gt;
&lt;p&gt;
PhoGPT&#26159;&#19968;&#20010;&#29992;&#20110;&#36234;&#21335;&#35821;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#31995;&#21015;&#65292;&#20855;&#26377;40&#20159;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;PhoGPT-4B&#20197;&#21450;&#20854;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#65292;&#23637;&#31034;&#20102;&#22312;&#36234;&#21335;&#35821;&#20219;&#21153;&#19978;&#20248;&#20110;&#20043;&#21069;&#30340;7&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#28304;&#20102;&#19968;&#20010;&#25317;&#26377;40&#20159;&#21442;&#25968;&#30340;&#26368;&#20808;&#36827;&#30340;&#36234;&#21335;&#35821;&#29983;&#25104;&#27169;&#22411;&#31995;&#21015;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#30784;&#30340;&#39044;&#35757;&#32451;&#21333;&#35821;&#27169;&#22411;PhoGPT-4B&#21644;&#20854;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#12290;&#22522;&#30784;&#27169;&#22411;PhoGPT-4B&#26377;37&#20159;&#21442;&#25968;&#65292;&#20174;&#38646;&#24320;&#22987;&#22312;&#21253;&#21547;1020&#20159;&#26631;&#35760;&#30340;&#36234;&#21335;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20351;&#29992;&#38271;&#24230;&#20026;8192&#30340;&#19978;&#19979;&#25991;&#65292;&#20351;&#29992;20480&#20010;&#26631;&#35760;&#31867;&#22411;&#30340;&#35789;&#27719;&#34920;&#12290;&#32842;&#22825;&#21464;&#20307;PhoGPT-4B-Chat&#26159;&#22312;70000&#20010;&#25351;&#23548;&#25552;&#31034;&#21644;&#22238;&#24212;&#20197;&#21450;&#39069;&#22806;&#30340;290000&#20010;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23545;PhoGPT-4B&#36827;&#34892;&#24494;&#35843;&#24471;&#21040;&#30340;&#27169;&#22411;&#36755;&#20986;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#27604;&#20043;&#21069;&#38381;&#28304;&#21644;&#24320;&#28304;&#30340;70&#20159;&#21442;&#25968;&#27169;&#22411;&#65292;&#23427;&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;PhoGPT&#27169;&#22411;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#19979;&#36733;&#65306;https://github.com/VinAIResearch/PhoGPT
&lt;/p&gt;
&lt;p&gt;
We open-source a state-of-the-art 4B-parameter generative model series for Vietnamese, which includes the base pre-trained monolingual model PhoGPT-4B and its chat variant, PhoGPT-4B-Chat. The base model, PhoGPT-4B, with exactly 3.7B parameters, is pre-trained from scratch on a Vietnamese corpus of 102B tokens, with an 8192 context length, employing a vocabulary of 20480 token types. The chat variant, PhoGPT-4B-Chat, is the modeling output obtained by fine-tuning PhoGPT-4B on a dataset of 70K instructional prompts and their responses, along with an additional 290K conversations. We demonstrate its strong performance compared to previous closed-source and open-source 7B-parameter models. Our PhoGPT models are available at: https://github.com/VinAIResearch/PhoGPT
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#23558;&#20449;&#24687;&#29109;&#24341;&#20837;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#65292;&#25552;&#39640;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.15929</link><description>&lt;p&gt;
E-Sparse: &#36890;&#36807;&#22522;&#20110;&#20449;&#24687;&#29109;&#30340; N:M &#31232;&#30095;&#24615;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
E-Sparse: Boosting the Large Language Model Inference through Entropy-based N:M Sparsity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15929
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#23558;&#20449;&#24687;&#29109;&#24341;&#20837;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#65292;&#25552;&#39640;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#21098;&#26525;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#24456;&#38590;&#23454;&#29616;&#65292;&#22240;&#20026;&#23427;&#20204;&#35757;&#32451;&#36807;&#31243;&#26114;&#36149;&#65292;&#35745;&#31639;&#38656;&#27714;&#22823;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#38544;&#34255;&#29366;&#24577;&#29305;&#24449;&#30340;&#20449;&#24687;&#29109;&#24341;&#20837;&#21040;&#21098;&#26525;&#24230;&#37327;&#35774;&#35745;&#20013;&#65292;&#21363; E-Sparse&#65292;&#20197;&#25552;&#39640;LLM&#20013; N:M &#31232;&#30095;&#24615;&#30340;&#20934;&#30830;&#24615;&#12290;E-Sparse&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#24615;&#26469;&#25552;&#21319;&#36890;&#36947;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#32467;&#21512;&#20960;&#31181;&#26032;&#39062;&#25216;&#26415;&#26469;&#23454;&#29616;&#65306;(1)&#24341;&#20837;&#20449;&#24687;&#29109;&#26469;&#22686;&#24378;&#21442;&#25968;&#26435;&#37325;&#21644;&#36755;&#20837;&#29305;&#24449;&#33539;&#25968;&#30340;&#37325;&#35201;&#24615;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21098;&#26525;&#24230;&#37327;&#65292;&#24182;&#22312;&#19981;&#20462;&#25913;&#21097;&#20313;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;N:M&#31232;&#30095;&#24615;&#12290;(2)&#35774;&#35745;&#20840;&#23616;&#26420;&#32032;&#27927;&#29260;&#21644;&#23616;&#37096;&#22359;&#27927;&#29260;&#65292;&#24555;&#36895;&#20248;&#21270;&#20449;&#24687;&#20998;&#24067;&#65292;&#20805;&#20998;&#24212;&#23545; N:M &#31232;&#30095;&#24615;&#23545;LLMs&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;E-Sparse &#34987;&#23454;&#29616;&#20026;&#19968;&#31181; Spars
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15929v2 Announce Type: replace-cross  Abstract: Traditional pruning methods are known to be challenging to work in Large Language Models (LLMs) for Generative AI because of their unaffordable training process and large computational demands. For the first time, we introduce the information entropy of hidden state features into a pruning metric design, namely E-Sparse, to improve the accuracy of N:M sparsity on LLM. E-Sparse employs the information richness to leverage the channel importance, and further incorporates several novel techniques to put it into effect: (1) it introduces information entropy to enhance the significance of parameter weights and input feature norms as a novel pruning metric, and performs N:M sparsity without modifying the remaining weights. (2) it designs global naive shuffle and local block shuffle to quickly optimize the information distribution and adequately cope with the impact of N:M sparsity on LLMs' accuracy. E-Sparse is implemented as a Spars
&lt;/p&gt;</description></item><item><title>FunQA&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#21644;&#25552;&#39640;&#22522;&#20110;&#21453;&#30452;&#35273;&#21644;&#26377;&#36259;&#35270;&#39057;&#30340;&#35270;&#39057;&#25512;&#29702;&#28145;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;HumorQA&#12289;CreativeQA&#21644;MagicQA&#19977;&#31181;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#24778;&#21916;&#35270;&#39057;&#31867;&#22411;&#12290;</title><link>https://arxiv.org/abs/2306.14899</link><description>&lt;p&gt;
FunQA&#65306;&#36808;&#21521;&#20196;&#20154;&#24778;&#35766;&#30340;&#35270;&#39057;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
FunQA: Towards Surprising Video Comprehension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.14899
&lt;/p&gt;
&lt;p&gt;
FunQA&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#21644;&#25552;&#39640;&#22522;&#20110;&#21453;&#30452;&#35273;&#21644;&#26377;&#36259;&#35270;&#39057;&#30340;&#35270;&#39057;&#25512;&#29702;&#28145;&#24230;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;HumorQA&#12289;CreativeQA&#21644;MagicQA&#19977;&#31181;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#24778;&#21916;&#35270;&#39057;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#35270;&#39057;&#65292;&#27604;&#22914;&#26377;&#36259;&#30340;&#29255;&#27573;&#12289;&#21019;&#24847;&#28436;&#20986;&#25110;&#35270;&#35273;&#24187;&#35937;&#65292;&#21560;&#24341;&#20102;&#30456;&#24403;&#22810;&#30340;&#20851;&#27880;&#12290;&#23545;&#36825;&#20123;&#35270;&#39057;&#30340;&#27427;&#36175;&#19981;&#20165;&#20165;&#26159;&#23545;&#35270;&#35273;&#21050;&#28608;&#30340;&#21453;&#24212;&#65307;&#30456;&#21453;&#65292;&#23427;&#21462;&#20915;&#20110;&#20154;&#31867;&#29702;&#35299;&#65288;&#20197;&#21450;&#27427;&#36175;&#65289;&#36825;&#20123;&#35270;&#39057;&#20013;&#25152;&#25551;&#32472;&#30340;&#24120;&#35782;&#36829;&#21453;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FunQA&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#38382;&#31572;&#65288;QA&#65289;&#25968;&#25454;&#38598;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#26469;&#35780;&#20272;&#21644;&#25552;&#39640;&#22522;&#20110;&#21453;&#30452;&#35273;&#21644;&#26377;&#36259;&#35270;&#39057;&#30340;&#35270;&#39057;&#25512;&#29702;&#28145;&#24230;&#12290;&#19982;&#22823;&#22810;&#25968;&#20391;&#37325;&#20110;&#19981;&#22826;&#24778;&#35766;&#30340;&#32972;&#26223;&#65288;&#20363;&#22914;&#28921;&#39274;&#25110;&#35828;&#26126;&#35270;&#39057;&#65289;&#30340;&#35270;&#39057;QA&#22522;&#20934;&#19981;&#21516;&#65292;FunQA&#28085;&#30422;&#20102;&#19977;&#31181;&#20197;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#31867;&#22411;&#30340;&#24778;&#21916;&#35270;&#39057;&#65306;1&#65289;HumorQA&#65292;2&#65289;CreativeQA&#21644;3&#65289;MagicQA&#12290;&#23545;&#20110;&#27599;&#20010;&#23376;&#38598;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20005;&#26684;&#30340;QA&#20219;&#21153;&#65292;&#26088;&#22312;&#35780;&#20272;&#27169;&#22411;&#22312;&#21453;&#30452;&#35273;&#26102;&#38388;&#25139;&#23450;&#20301;&#12289;&#35814;&#32454;&#35270;&#39057;&#25551;&#36848;&#20197;&#21450;&#22260;&#32469;&#21453;&#30452;&#35273;&#36827;&#34892;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.14899v2 Announce Type: replace-cross  Abstract: Surprising videos, such as funny clips, creative performances, or visual illusions, attract significant attention. Enjoyment of these videos is not simply a response to visual stimuli; rather, it hinges on the human capacity to understand (and appreciate) commonsense violations depicted in these videos. We introduce FunQA, a challenging video question-answering (QA) dataset specifically designed to evaluate and enhance the depth of video reasoning based on counter-intuitive and fun videos. Unlike most video QA benchmarks which focus on less surprising contexts, e.g., cooking or instructional videos, FunQA covers three previously unexplored types of surprising videos: 1) HumorQA, 2) CreativeQA, and 3) MagicQA. For each subset, we establish rigorous QA tasks designed to assess the model's capability in counter-intuitive timestamp localization, detailed video description, and reasoning around counter-intuitiveness. We also pose hi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#23433;&#20840;&#24494;&#35843;&#26080;&#25928;&#65292;BadLlama&#39033;&#30446;&#20197;&#20302;&#25104;&#26412;&#25104;&#21151;&#31227;&#38500;&#20102;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#24182;&#20445;&#30041;&#20102;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.00117</link><description>&lt;p&gt;
BadLlama&#65306;&#20197;&#20302;&#25104;&#26412;&#31227;&#38500;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B. (arXiv:2311.00117v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00117
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#23433;&#20840;&#24494;&#35843;&#26080;&#25928;&#65292;BadLlama&#39033;&#30446;&#20197;&#20302;&#25104;&#26412;&#25104;&#21151;&#31227;&#38500;&#20102;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#24182;&#20445;&#30041;&#20102;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Llama 2-Chat&#26159;Meta&#24320;&#21457;&#24182;&#21521;&#20844;&#20247;&#21457;&#24067;&#30340;&#19968;&#31995;&#21015;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#23613;&#31649;Meta&#23545;Llama 2-Chat&#36827;&#34892;&#20102;&#23433;&#20840;&#24494;&#35843;&#20197;&#25298;&#32477;&#36755;&#20986;&#26377;&#23475;&#20869;&#23481;&#65292;&#20294;&#25105;&#20204;&#20551;&#35774;&#20844;&#20849;&#33719;&#21462;&#27169;&#22411;&#26435;&#37325;&#20351;&#24471;&#22351;&#24847;&#34892;&#20026;&#32773;&#21487;&#20197;&#20197;&#20302;&#25104;&#26412;&#32469;&#36807;Llama 2-Chat&#30340;&#23433;&#20840;&#26426;&#21046;&#65292;&#24182;&#23558;Llama 2&#30340;&#33021;&#21147;&#29992;&#20110;&#24694;&#24847;&#30446;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20197;&#23569;&#20110;200&#32654;&#20803;&#30340;&#25104;&#26412;&#26377;&#25928;&#22320;&#21462;&#28040;Llama 2-Chat 13B&#30340;&#23433;&#20840;&#24494;&#35843;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#19968;&#33324;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#26102;&#65292;&#23433;&#20840;&#24494;&#35843;&#26159;&#26080;&#25928;&#30340;&#38450;&#27490;&#28389;&#29992;&#30340;&#26041;&#27861;&#12290;&#37492;&#20110;&#26410;&#26469;&#30340;&#27169;&#22411;&#24456;&#21487;&#33021;&#20855;&#26377;&#26356;&#22823;&#35268;&#27169;&#30340;&#21361;&#23475;&#33021;&#21147;&#65292;AI&#24320;&#21457;&#32773;&#22312;&#32771;&#34385;&#20844;&#24320;&#21457;&#24067;&#27169;&#22411;&#26435;&#37325;&#26102;&#24517;&#39035;&#35299;&#20915;&#24494;&#35843;&#24102;&#26469;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Llama 2-Chat is a collection of large language models that Meta developed and released to the public. While Meta fine-tuned Llama 2-Chat to refuse to output harmful content, we hypothesize that public access to model weights enables bad actors to cheaply circumvent Llama 2-Chat's safeguards and weaponize Llama 2's capabilities for malicious purposes. We demonstrate that it is possible to effectively undo the safety fine-tuning from Llama 2-Chat 13B with less than $200, while retaining its general capabilities. Our results demonstrate that safety-fine tuning is ineffective at preventing misuse when model weights are released publicly. Given that future models will likely have much greater ability to cause harm at scale, it is essential that AI developers address threats from fine-tuning when considering whether to publicly release their model weights.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#38450;&#24481;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23433;&#20840;&#35757;&#32451;&#21644;&#20445;&#25252;&#25514;&#26045;&#30340;&#20248;&#21183;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2310.15851</link><description>&lt;p&gt;
&#33258;&#25105;&#38450;&#24481;&#65306;&#22686;&#24378;LLM&#30340;&#33258;&#25105;&#20445;&#25252;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Self-Guard: Empower the LLM to Safeguard Itself. (arXiv:2310.15851v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15851
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#38450;&#24481;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#23433;&#20840;&#35757;&#32451;&#21644;&#20445;&#25252;&#25514;&#26045;&#30340;&#20248;&#21183;&#65292;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#24615;&#65292;&#20174;&#32780;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30423;&#30772;&#25915;&#20987;&#21487;&#20197;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#25514;&#26045;&#65292;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#36825;&#31181;&#28389;&#29992;LLM&#30340;&#34892;&#20026;&#23548;&#33268;&#20102;&#36127;&#38754;&#30340;&#31038;&#20250;&#21518;&#26524;&#12290;&#30446;&#21069;&#65292;&#35299;&#20915;&#30423;&#30772;&#25915;&#20987;&#30340;&#20027;&#35201;&#26041;&#27861;&#26377;&#20004;&#31181;&#65306;&#23433;&#20840;&#35757;&#32451;&#21644;&#20445;&#25252;&#25514;&#26045;&#12290;&#23433;&#20840;&#35757;&#32451;&#20391;&#37325;&#20110;&#36827;&#19968;&#27493;&#35757;&#32451;LLM&#20197;&#22686;&#24378;&#20854;&#23433;&#20840;&#24615;&#12290;&#32780;&#20445;&#25252;&#25514;&#26045;&#21017;&#26159;&#36890;&#36807;&#23454;&#26045;&#22806;&#37096;&#27169;&#22411;&#25110;&#36807;&#28388;&#22120;&#26469;&#38450;&#27490;&#26377;&#23475;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#23433;&#20840;&#35757;&#32451;&#22312;&#36866;&#24212;&#26032;&#30340;&#25915;&#20987;&#31867;&#22411;&#26041;&#38754;&#20855;&#26377;&#23616;&#38480;&#24615;&#65292;&#24182;&#19988;&#24448;&#24448;&#20250;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#20445;&#25252;&#25514;&#26045;&#22312;&#24110;&#21161;&#26041;&#38754;&#20063;&#34987;&#35777;&#26126;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#38450;&#24481;&#30340;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23433;&#20840;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;&#33258;&#25105;&#38450;&#24481;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#27169;&#22411;&#35780;&#20272;&#26377;&#23475;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#25351;&#23548;&#27169;&#22411;&#22312;&#33258;&#24049;&#30340;&#22238;&#24212;&#19978;&#22987;&#32456;&#25191;&#34892;&#26377;&#23475;&#20869;&#23481;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#26377;&#23475;&#20869;&#23481;&#30340;&#29983;&#25104;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The jailbreak attack can bypass the safety measures of a Large Language Model (LLM), generating harmful content. This misuse of LLM has led to negative societal consequences. Currently, there are two main approaches to address jailbreak attacks: safety training and safeguards. Safety training focuses on further training LLM to enhance its safety. On the other hand, safeguards involve implementing external models or filters to prevent harmful outputs. However, safety training has constraints in its ability to adapt to new attack types and often leads to a drop in model performance. Safeguards have proven to be of limited help. To tackle these issues, we propose a novel approach called Self-Guard, which combines the strengths of both safety methods. Self-Guard includes two stages. In the first stage, we enhance the model's ability to assess harmful content, and in the second stage, we instruct the model to consistently perform harmful content detection on its own responses. The experimen
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35821;&#27861;&#21477;&#27861;&#36830;&#32493;&#36716;&#21464;&#65292;&#24182;&#35777;&#26126;&#35813;&#36716;&#21464;&#23545;&#20110;&#26126;&#30830;&#23545;&#31216;&#24615;&#30340;&#25171;&#30772;&#26159;&#40065;&#26834;&#30340;&#12290;</title><link>http://arxiv.org/abs/2309.14913</link><description>&lt;p&gt;
&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Robustness of the Random Language Model. (arXiv:2309.14913v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14913
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#35821;&#27861;&#21477;&#27861;&#36830;&#32493;&#36716;&#21464;&#65292;&#24182;&#35777;&#26126;&#35813;&#36716;&#21464;&#23545;&#20110;&#26126;&#30830;&#23545;&#31216;&#24615;&#30340;&#25171;&#30772;&#26159;&#40065;&#26834;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#35821;&#35328;&#27169;&#22411;(De Giuli 2019)&#26159;&#19968;&#32452;&#38543;&#26426;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65292;&#37327;&#21270;&#20154;&#31867;&#21644;&#35745;&#31639;&#26426;&#35821;&#35328;&#30340;&#21477;&#27861;&#12290;&#35813;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31532;&#19968;&#35821;&#35328;&#23398;&#20064;&#22270;&#26223;&#65292;&#21363;&#20316;&#20026;&#28508;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#19968;&#20010;&#36864;&#28779;&#31867;&#22411;&#65292;&#25512;&#26029;&#20102;&#21521;&#35821;&#27861;&#21477;&#27861;&#30340;&#21333;&#19968;&#36830;&#32493;&#36716;&#21464;&#65292;&#20854;&#20013;&#28508;&#22312;&#30340;&#35789;&#27719;&#21644;&#20998;&#31867;&#20043;&#38388;&#30340;&#23545;&#31216;&#24615;&#20250;&#33258;&#21457;&#25171;&#30772;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#23545;&#26126;&#30830;&#23545;&#31216;&#24615;&#25171;&#30772;&#30340;&#40065;&#26834;&#24615;&#65292;&#23545;&#36825;&#19968;&#22270;&#26223;&#36827;&#34892;&#20102;&#20005;&#26684;&#23457;&#35270;&#65292;&#36825;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#23398;&#20064;&#20013;&#19981;&#21487;&#36991;&#20813;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22330;&#26223;&#23545;&#20110;&#36825;&#31181;&#23545;&#31216;&#24615;&#30340;&#25171;&#30772;&#26159;&#40065;&#26834;&#30340;&#12290;&#19982;&#35821;&#27861;&#32593;&#32476;&#32858;&#31867;&#31995;&#25968;&#30340;&#20154;&#31867;&#25968;&#25454;&#36827;&#34892;&#27604;&#36739;&#34920;&#26126;&#65292;&#35266;&#23519;&#21040;&#30340;&#36716;&#21464;&#30456;&#24403;&#20110;&#20799;&#31461;24&#20010;&#26376;&#26102;&#36890;&#24120;&#32463;&#21382;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Random Language Model (De Giuli 2019) is an ensemble of stochastic context-free grammars, quantifying the syntax of human and computer languages. The model suggests a simple picture of first language learning as a type of annealing in the vast space of potential languages. In its simplest formulation, it implies a single continuous transition to grammatical syntax, at which the symmetry among potential words and categories is spontaneously broken. Here this picture is scrutinized by considering its robustness against explicit symmetry breaking, an inevitable component of learning in the real world. It is shown that the scenario is robust to such symmetry breaking. Comparison with human data on the clustering coefficient of syntax networks suggests that the observed transition is equivalent to that normally experienced by children at age 24 months.
&lt;/p&gt;</description></item><item><title>LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.12276</link><description>&lt;p&gt;
LLMR&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#26102;&#25552;&#31034;&#20132;&#20114;&#24335;&#19990;&#30028;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
LLMR: Real-time Prompting of Interactive Worlds using Large Language Models. (arXiv:2309.12276v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12276
&lt;/p&gt;
&lt;p&gt;
LLMR&#26159;&#19968;&#20010;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#23427;&#33021;&#22815;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#21644;&#35774;&#35745;&#30446;&#26631;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#26631;&#20934;&#30340;GPT-4&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#21644;&#29992;&#25143;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#23545;&#20110;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#28151;&#21512;&#29616;&#23454;&#22330;&#26223;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMR)&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#26102;&#21019;&#24314;&#21644;&#20462;&#25913;&#20132;&#20114;&#24335;&#28151;&#21512;&#29616;&#23454;&#20307;&#39564;&#12290;LLMR&#21033;&#29992;&#20102;&#26032;&#39062;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#35774;&#35745;&#30446;&#26631;&#38656;&#35201;&#21512;&#25104;&#20869;&#37096;&#21160;&#24577;&#12289;&#30452;&#35266;&#20998;&#26512;&#25110;&#39640;&#32423;&#20132;&#20114;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20381;&#36182;&#20110;&#25991;&#26412;&#20132;&#20114;&#21644;Unity&#28216;&#25103;&#24341;&#25806;&#12290;&#36890;&#36807;&#34701;&#21512;&#22330;&#26223;&#29702;&#35299;&#12289;&#20219;&#21153;&#35268;&#21010;&#12289;&#33258;&#25105;&#35843;&#35797;&#21644;&#20869;&#23384;&#31649;&#29702;&#25216;&#26415;&#65292;LLMR&#22312;&#24179;&#22343;&#38169;&#35823;&#29575;&#19978;&#27604;&#26631;&#20934;&#30340;GPT-4&#25552;&#39640;&#20102;4&#20493;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLMR&#19982;&#20960;&#20010;&#31034;&#20363;&#19990;&#30028;&#30340;&#36328;&#24179;&#21488;&#20114;&#25805;&#20316;&#24615;&#65292;&#24182;&#36890;&#36807;&#22810;&#20010;&#21019;&#24314;&#21644;&#20462;&#25913;&#20219;&#21153;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20197;&#23637;&#31034;&#23427;&#33021;&#22815;&#29983;&#25104;&#21644;&#32534;&#36753;&#21508;&#31181;&#23545;&#35937;&#12289;&#24037;&#20855;&#21644;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26377;&#22810;&#26679;&#24615;&#30340;&#21487;&#29992;&#24615;&#30740;&#31350;&#65288;N=11&#65289;&#65292;&#25581;&#31034;&#20102;&#21442;&#19982;&#32773;&#23545;&#35813;&#31995;&#32479;&#26377;&#31215;&#26497;&#30340;&#20307;&#39564;&#65292;&#24182;&#24895;&#24847;&#20877;&#27425;&#20351;&#29992;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Large Language Model for Mixed Reality (LLMR), a framework for the real-time creation and modification of interactive Mixed Reality experiences using LLMs. LLMR leverages novel strategies to tackle difficult cases where ideal training data is scarce, or where the design goal requires the synthesis of internal dynamics, intuitive analysis, or advanced interactivity. Our framework relies on text interaction and the Unity game engine. By incorporating techniques for scene understanding, task planning, self-debugging, and memory management, LLMR outperforms the standard GPT-4 by 4x in average error rate. We demonstrate LLMR's cross-platform interoperability with several example worlds, and evaluate it on a variety of creation and modification tasks to show that it can produce and edit diverse objects, tools, and scenes. Finally, we conducted a usability study (N=11) with a diverse set that revealed participants had positive experiences with the system and would use it again.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2309.10435</link><description>&lt;p&gt;
&#37325;&#22609;&#39034;&#24207;&#25512;&#33616;&#31995;&#32479;&#65306;&#21033;&#29992;&#20869;&#23481;&#22686;&#24378;&#35821;&#35328;&#24314;&#27169;&#23398;&#20064;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;
&lt;/p&gt;
&lt;p&gt;
Reformulating Sequential Recommendation: Learning Dynamic User Interest with Content-enriched Language Modeling. (arXiv:2309.10435v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335; LANCER&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#25928;&#19988;&#26377;&#24076;&#26395;&#65292;&#24182;&#20026;&#20102;&#35299;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#22312;&#32447;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#39034;&#24207;&#25512;&#33616;&#30001;&#20110;&#20854;&#34920;&#36798;&#33021;&#21147;&#24378;&#22823;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;&#21160;&#24577;&#29992;&#25143;&#20852;&#36259;&#32780;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39034;&#24207;&#24314;&#27169;&#26041;&#27861;&#22312;&#25429;&#25417;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#20173;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20027;&#35201;&#30340;&#21407;&#22240;&#26159;&#35821;&#35328;&#27169;&#22411;&#24120;&#24120;&#32570;&#20047;&#23545;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#21644;&#29289;&#21697;&#30456;&#20851;&#25991;&#26412;&#20869;&#23481;&#30340;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#39034;&#24207;&#25512;&#33616;&#33539;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;LANCER&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#33021;&#21147;&#29983;&#25104;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24357;&#21512;&#20102;&#35821;&#35328;&#27169;&#22411;&#19982;&#25512;&#33616;&#31995;&#32479;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20135;&#29983;&#20102;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#25512;&#33616;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#27169;&#22411;&#23545;&#39034;&#24207;&#25512;&#33616;&#30340;&#24433;&#21709;&#30340;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential for online applications, and sequential recommendation has enjoyed significant prevalence due to its expressive ability to capture dynamic user interests. However, previous sequential modeling methods still have limitations in capturing contextual information. The primary reason for this issue is that language models often lack an understanding of domain-specific knowledge and item-related textual content. To address this issue, we adopt a new sequential recommendation paradigm and propose LANCER, which leverages the semantic understanding capabilities of pre-trained language models to generate personalized recommendations. Our approach bridges the gap between language models and recommender systems, resulting in more human-like recommendations. We demonstrate the effectiveness of our approach through experiments on several benchmark datasets, showing promising results and providing valuable insights into the influence of our model on sequential recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.12038</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages. (arXiv:2308.12038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#26041;&#38754;&#65292;&#22810;&#27169;&#24335;&#23398;&#20064;&#20986;&#29616;&#20102;&#26174;&#33879;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#25104;&#21151;&#36890;&#24120;&#20165;&#38480;&#20110;&#33521;&#35821;&#65292;&#20854;&#20182;&#35821;&#35328;&#21017;&#30456;&#23545;&#33853;&#21518;&#12290;&#22312;&#20854;&#20182;&#35821;&#35328;&#20013;&#26500;&#24314;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#23545;&#24212;&#29289;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38750;&#33521;&#35821;&#22810;&#27169;&#24335;&#25968;&#25454;&#20855;&#26377;&#20302;&#36164;&#28304;&#29305;&#24615;&#65288;&#21363;&#32570;&#20047;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MPM&#65292;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#35757;&#32451;&#22823;&#22411;&#22810;&#27169;&#24335;&#27169;&#22411;&#30340;&#26377;&#25928;&#35757;&#32451;&#33539;&#20363;&#12290;MPM&#34920;&#26126;&#65292;&#22810;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#36328;&#35821;&#31181;&#38646;&#26679;&#26412;&#22810;&#27169;&#24335;&#23398;&#20064;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#24378;&#22823;&#30340;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#20165;&#22312;&#33521;&#35821;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#27169;&#22411;&#21487;&#20197;&#20197;&#38646;&#26679;&#26412;&#30340;&#26041;&#24335;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#20854;&#20182;&#35821;&#35328;&#65292;&#29992;&#20110;&#22270;&#20687;&#21040;&#25991;&#26412;&#21644;&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#65292;&#29978;&#33267;&#36229;&#36807;&#22312;&#26412;&#22320;&#35821;&#35328;&#30340;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#20197;&#20013;&#25991;&#20316;&#20026;MPM&#23454;&#36341;&#30340;&#19968;&#20010;&#32451;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently there has been a significant surge in multimodal learning in terms of both image-to-text and text-to-image generation. However, the success is typically limited to English, leaving other languages largely behind. Building a competitive counterpart in other languages is highly challenging due to the low-resource nature of non-English multimodal data (i.e., lack of large-scale, high-quality image-text data). In this work, we propose MPM, an effective training paradigm for training large multimodal models in low-resource languages. MPM demonstrates that Multilingual language models can Pivot zero-shot Multimodal learning across languages. Specifically, based on a strong multilingual large language model, multimodal models pretrained on English-only image-text data can well generalize to other languages in a zero-shot manner for both image-to-text and text-to-image generation, even surpassing models trained on image-text data in native languages. Taking Chinese as a practice of MP
&lt;/p&gt;</description></item><item><title>AutoTAMP&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#20316;&#20026;&#32763;&#35793;&#22120;&#21644;&#26816;&#26597;&#22120;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#32763;&#35793;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20013;&#38388;&#20219;&#21153;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2306.06531</link><description>&lt;p&gt;
AutoTAMP: &#20351;&#29992;LLMs&#20316;&#20026;&#32763;&#35793;&#22120;&#21644;&#26816;&#26597;&#22120;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
AutoTAMP: Autoregressive Task and Motion Planning with LLMs as Translators and Checkers. (arXiv:2306.06531v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06531
&lt;/p&gt;
&lt;p&gt;
AutoTAMP&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;LLMs&#20316;&#20026;&#32763;&#35793;&#22120;&#21644;&#26816;&#26597;&#22120;&#30340;&#33258;&#22238;&#24402;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#23569;&#26679;&#26412;&#32763;&#35793;&#23558;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#36716;&#25442;&#20026;&#20013;&#38388;&#20219;&#21153;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#23545;&#22797;&#26434;&#20219;&#21153;&#30340;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#29702;&#35299;&#12289;&#35268;&#21010;&#21644;&#25191;&#34892;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22797;&#26434;&#12289;&#38271;&#26399;&#20219;&#21153;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#26426;&#22120;&#20154;&#34892;&#21160;&#24207;&#21015;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#30452;&#25509;&#23558;&#33258;&#28982;&#35821;&#35328;&#32763;&#35793;&#20026;&#26426;&#22120;&#20154;&#36712;&#36857;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#35821;&#35328;&#20998;&#35299;&#20026;&#20219;&#21153;&#23376;&#30446;&#26631;&#24182;&#20381;&#38752;&#21160;&#20316;&#35268;&#21010;&#22120;&#25191;&#34892;&#27599;&#20010;&#23376;&#30446;&#26631;&#26469;&#20998;&#35299;&#25512;&#29702;&#36807;&#31243;&#12290;&#24403;&#28041;&#21450;&#22797;&#26434;&#30340;&#29615;&#22659;&#21644;&#26102;&#38388;&#32422;&#26463;&#26102;&#65292;&#24517;&#39035;&#20351;&#29992;&#20256;&#32479;&#30340;&#20219;&#21153;&#21644;&#21160;&#20316;&#35268;&#21010;&#65288;TAMP&#65289;&#31639;&#27861;&#26469;&#32852;&#21512;&#36827;&#34892;&#35268;&#21010;&#20219;&#21153;&#30340;&#25512;&#29702;&#21644;&#21160;&#20316;&#35268;&#21010;&#65292;&#20351;&#24471;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#25104;&#20026;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#20351;&#29992;LLMs&#26469;&#30452;&#25509;&#35268;&#21010;&#20219;&#21153;&#23376;&#30446;&#26631;&#65292;&#32780;&#26159;&#20174;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#32763;&#35793;&#65292;&#29983;&#25104;&#19968;&#20010;&#20013;&#38388;&#20219;&#21153;&#34920;&#31034;&#65292;&#28982;&#21518;&#21487;&#20197;&#30001;TAMP&#31639;&#27861;&#28040;&#21270;&#35813;&#34920;&#31034;&#26469;&#36827;&#34892;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
For effective human-robot interaction, robots need to understand, plan, and execute complex, long-horizon tasks described by natural language. Recent advances in large language models (LLMs) have shown promise for translating natural language into robot action sequences for complex tasks. However, existing approaches either translate the natural language directly into robot trajectories or factor the inference process by decomposing language into task sub-goals and relying on a motion planner to execute each sub-goal. When complex environmental and temporal constraints are involved, inference over planning tasks must be performed jointly with motion plans using traditional task-and-motion planning (TAMP) algorithms, making factorization into subgoals untenable. Rather than using LLMs to directly plan task sub-goals, we instead perform few-shot translation from natural language task descriptions to an intermediate task representation that can then be consumed by a TAMP algorithm to join
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14521</link><description>&lt;p&gt;
&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Eliminating Spurious Correlations from Pre-trained Models via Data Mixing. (arXiv:2305.14521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#26469;&#25552;&#39640;&#27169;&#22411;&#23545;&#20110;&#26032;&#26679;&#26412;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#32463;&#36807;&#29702;&#35770;&#35777;&#26126;&#21644;&#22810;&#31181;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#65292;&#21487;&#20197;&#21462;&#24471;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25910;&#25947;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#24448;&#24448;&#21033;&#29992;&#20102;&#26576;&#20123;&#23646;&#24615;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#22312;&#29305;&#23450;&#31867;&#21035;&#30340;&#22823;&#22810;&#25968;&#31034;&#20363;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#20294;&#24182;&#19981;&#36275;&#20197;&#39044;&#27979;&#36825;&#20123;&#31867;&#21035;&#12290;&#23398;&#21040;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#21487;&#33021;&#20250;&#22312;&#23545;&#26032;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#21518;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#23545;&#19981;&#23637;&#29616;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31034;&#20363;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#28040;&#38500;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#19968;&#23567;&#32452;&#24102;&#26377;&#34394;&#20551;&#23646;&#24615;&#30340;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#25968;&#25454;&#28151;&#21512;&#26469;&#24179;&#34913;&#25152;&#26377;&#31867;&#21035;&#20013;&#30340;&#34394;&#20551;&#23646;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#35270;&#35273;&#21644;NLP&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#65292;&#21253;&#25324;&#28040;&#38500;&#34394;&#20551;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models pre-trained on large datasets have achieved remarkable convergence and robustness properties. However, these models often exploit spurious correlations between certain attributes and labels, which are prevalent in the majority of examples within specific categories but are not predictive of these categories in general. The learned spurious correlations may persist even after fine-tuning on new data, which degrades models' performance on examples that do not exhibit the spurious correlation. In this work, we propose a simple and highly effective method to eliminate spurious correlations from pre-trained models. The key idea of our method is to leverage a small set of examples with spurious attributes, and balance the spurious attributes across all classes via data mixing. We theoretically confirm the effectiveness of our method, and empirically demonstrate its state-of-the-art performance on various vision and NLP tasks, including eliminating spurious correlation
&lt;/p&gt;</description></item><item><title>CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.09955</link><description>&lt;p&gt;
CooK: &#29992;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#30693;&#35782;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CooK: Empowering General-Purpose Language Models with Modular and Collaborative Knowledge. (arXiv:2305.09955v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09955
&lt;/p&gt;
&lt;p&gt;
CooK&#26159;&#19968;&#31181;&#29992;&#20110;&#36171;&#33021;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#21327;&#20316;&#30340;&#30693;&#35782;&#36129;&#29486;&#32773;&#65292;&#25552;&#20379;&#27169;&#22359;&#21270;&#12289;&#19981;&#26029;&#22686;&#38271;&#21644;&#22810;&#28304;&#30340;&#30693;&#35782;&#12290;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#65292;CooK&#23637;&#29616;&#20986;&#20102;&#26126;&#26174;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#21644;&#35821;&#22659;&#20013;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#36807;&#26816;&#32034;&#25110;&#29983;&#25104;&#30693;&#35782;&#25552;&#31034;&#26469;&#25913;&#21892;&#36890;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#26410;&#33021;&#21453;&#26144;&#30693;&#35782;&#20016;&#23500;&#27169;&#22411;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65306;&#30693;&#35782;&#24212;&#35813;&#26159;&#27169;&#22359;&#21270;&#65292;&#19981;&#26029;&#22686;&#38271;&#65292;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#65307;&#30693;&#35782;&#33719;&#21462;&#21644;&#29983;&#25104;&#24212;&#35813;&#26159;&#21327;&#20316;&#30340;&#36807;&#31243;&#65292;&#20854;&#20013;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773; contribue &#26032;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; CooK&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#20026;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#27169;&#22359;&#21270;&#21644;&#21327;&#20316;&#26469;&#28304;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21363;&#22312;&#24191;&#27867;&#39046;&#22495;&#21644;&#26469;&#28304;&#19978;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#12290;&#36825;&#20123;&#19987;&#38376;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#30693;&#35782;&#24211;&#65292;&#21518;&#26469;&#34987;&#25552;&#31034;&#29983;&#25104;&#36890;&#29992;&#30340; LLM &#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#30693;&#35782;&#36807;&#28388;&#22120;&#65292;&#20197;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#32473;&#23450;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#28304;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#19968;&#20010;&#30693;&#35782;&#36129;&#29486;&#32773;&#32452;&#20214;&#65292;&#20351;&#21033;&#30410;&#30456;&#20851;&#32773;&#33021;&#22815;&#36731;&#26494;&#22320;&#20026;&#31995;&#32479;&#36129;&#29486;&#29305;&#23450;&#20110;&#22495;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; CooK &#22312;&#19968;&#32452;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#30340;&#36229;&#36234;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for knowledge-intensive tasks and contexts. Existing approaches improve the knowledge capabilities of general-purpose LLMs through retrieval or generated knowledge prompting, but they fall short of reflecting two key properties of knowledge-rich models: knowledge should be modular, ever-growing, sourced from diverse domains; knowledge acquisition and production should be a collaborative process, where diverse stakeholders contribute new information. To this end, we propose CooK, a novel framework to empower general-purpose large language models with modular and collaboratively sourced knowledge. We first introduce specialized language models, autoregressive models trained on corpora from a wide range of domains and sources. These specialized LMs serve as parametric knowledge repositories that are later prompted to generate background knowledge for general-purpose LLMs. We then propose three knowledge filters to dynamically select an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;NL&#21040;TL&#30340;&#36716;&#25442;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#21487;&#20197;&#20934;&#30830;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#22320;&#36716;&#25442;&#22797;&#26434;&#30340;&#39640;&#32423;&#31995;&#32479;&#35268;&#33539;&#12290;</title><link>http://arxiv.org/abs/2305.07766</link><description>&lt;p&gt;
NL2TL&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#21270;&#20026;&#26102;&#24577;&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
NL2TL: Transforming Natural Languages to Temporal Logics using Large Language Models. (arXiv:2305.07766v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;NL&#21040;TL&#30340;&#36716;&#25442;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35757;&#32451;&#20013;&#65292;&#21487;&#20197;&#20934;&#30830;&#24182;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#22320;&#36716;&#25442;&#22797;&#26434;&#30340;&#39640;&#32423;&#31995;&#32479;&#35268;&#33539;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#24577;&#36923;&#36753;&#65288;TL&#65289;&#21487;&#29992;&#20110;&#22312;&#35768;&#22810;&#24037;&#31243;&#24212;&#29992;&#20013;&#20005;&#26684;&#25351;&#23450;&#22797;&#26434;&#30340;&#39640;&#32423;&#31995;&#32479;&#35268;&#33539;&#12290;&#30001;&#20110;&#32570;&#20047;&#36328;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#21644;&#21487;&#25512;&#24191;&#30340;&#27169;&#22411;&#65292;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#21644;TL&#20043;&#38388;&#30340;&#36716;&#25442;&#19968;&#30452;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20934;&#30830;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#33521;&#25991;&#25351;&#20196;&#20174;NL&#21040;TL&#30340;&#36716;&#25442;&#26694;&#26550;&#65292;&#24182;&#25506;&#32034;&#20102;&#22312;&#22810;&#20010;&#38454;&#27573;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;NL-TL&#23545;&#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#20102;LLMs&#21644;&#20154;&#24037;&#27880;&#37322;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#20855;&#26377;28K&#20010;NL-TL&#23545;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;NL&#21644;TL&#30340;&#25552;&#21319;&#29256;&#26412;&#19978;&#24494;&#35843;&#20102;T5&#27169;&#22411;&#65288;&#21363;&#65292;&#29305;&#23450;&#21407;&#23376;&#21629;&#39064;&#65288;AP&#65289;&#34987;&#38544;&#34255;&#65289;&#12290;&#22686;&#24378;&#30340;&#26222;&#36866;&#24615;&#28304;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#20351;&#29992;&#25552;&#21319;&#30340;NL-TL&#34920;&#24449;&#24120;&#35265;&#30340;&#36923;&#36753;&#32467;&#26500;&#65292;&#27809;&#26377;&#29305;&#23450;&#39046;&#22495;&#30340;&#32422;&#26463;&#12290;2&#65289;&#22312;&#25968;&#25454;&#38598;&#21019;&#24314;&#20013;&#24212;&#29992;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Temporal Logic (TL) can be used to rigorously specify complex high-level specification for systems in many engineering applications. The translation between natural language (NL) and TL has been under-explored due to the lack of dataset and generalizable model across different application domains. In this paper, we propose an accurate and generalizable transformation framework of English instructions from NL to TL, exploring the use of Large Language Models (LLMs) at multiple stages. Our contributions are twofold. First, we develop a framework to create a dataset of NL-TL pairs combining LLMs and human annotation. We publish a dataset with 28K NL-TL pairs. Then, we finetune T5 models on the lifted versions (i.e., the specific Atomic Propositions (AP) are hidden) of the NL and TL. The enhanced generalizability originates from two aspects: 1) Usage of lifted NL-TL characterizes common logical structures, without constraints of specific domains. 2) Application of LLMs in dataset creation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.14178</link><description>&lt;p&gt;
mPLUG-Owl: &#27169;&#22359;&#21270;&#22686;&#24378;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality. (arXiv:2304.14178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14178
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#36890;&#36807;&#27169;&#22359;&#21270;&#23398;&#20064;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#65292;&#36171;&#20104;LLMs&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#22312;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#22312;&#21508;&#31181;&#24320;&#25918;&#24335;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23558;LLMs&#29992;&#20110;&#22810;&#27169;&#24577;&#29983;&#25104;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;mPLUG-Owl&#65292;&#36890;&#36807;&#22522;&#30784;LLM&#12289;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#35270;&#35273;&#25277;&#35937;&#22120;&#27169;&#22359;&#30340;&#27169;&#22359;&#21270;&#23398;&#20064;&#65292;&#20351;LLMs&#20855;&#22791;&#20102;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#25903;&#25345;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#36890;&#36807;&#27169;&#24577;&#21327;&#20316;&#20419;&#36827;&#20102;&#22810;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#33021;&#21147;&#12290;mPLUG-Owl&#30340;&#35757;&#32451;&#33539;&#24335;&#21253;&#25324;&#29992;&#20110;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#36741;&#21161;&#23398;&#20064;&#35270;&#35273;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25913;&#36827;&#20102;LLM&#30340;&#29983;&#25104;&#33021;&#21147;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20923;&#32467;&#30340;LLM&#27169;&#22359;&#23545;&#35270;&#35273;&#30693;&#35782;&#27169;&#22359;&#21644;&#25277;&#35937;&#22120;&#27169;&#22359;&#36827;&#34892;&#35757;&#32451;&#20197;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#20013;&#65292;&#20351;&#29992;&#20165;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30417;&#30563;&#25968;&#25454;&#38598;&#20849;&#21516;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#23545;&#20110;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;mPLUG-Owl&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot abilities on a variety of open-ended tasks, while recent research has also explored the use of LLMs for multi-modal generation. In this study, we introduce mPLUG-Owl, a novel training paradigm that equips LLMs with multi-modal abilities through modularized learning of foundation LLM, a visual knowledge module, and a visual abstractor module. This approach can support multiple modalities and facilitate diverse unimodal and multimodal abilities through modality collaboration. The training paradigm of mPLUG-Owl involves a two-stage method for aligning image and text, which learns visual knowledge with the assistance of LLM while maintaining and even improving the generation abilities of LLM. In the first stage, the visual knowledge module and abstractor module are trained with a frozen LLM module to align the image and text. In the second stage, language-only and multi-modal supervised datasets are used to jointly fine-tu
&lt;/p&gt;</description></item></channel></rss>