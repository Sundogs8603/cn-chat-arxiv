<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>Covidia&#26159;&#19968;&#31181;COVID-19&#36328;&#23398;&#31185;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#35770;&#25991;&#20998;&#31867;&#21644;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#21644;&#25972;&#21512;&#65292;&#24357;&#21512;&#20102;&#19981;&#21516;&#39046;&#22495;&#23545;COVID-19&#30693;&#35782;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2304.07242</link><description>&lt;p&gt;
Covidia: COVID-19 &#36328;&#23398;&#31185;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Covidia: COVID-19 Interdisciplinary Academic Knowledge Graph. (arXiv:2304.07242v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07242
&lt;/p&gt;
&lt;p&gt;
Covidia&#26159;&#19968;&#31181;COVID-19&#36328;&#23398;&#31185;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#35770;&#25991;&#20998;&#31867;&#21644;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#21644;&#25972;&#21512;&#65292;&#24357;&#21512;&#20102;&#19981;&#21516;&#39046;&#22495;&#23545;COVID-19&#30693;&#35782;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#28608;&#21457;&#20102;&#19981;&#21516;&#39046;&#22495;&#24191;&#27867;&#30340;&#30740;&#31350;&#24037;&#20316;&#12290;&#29616;&#26377;&#30340;COVID-19&#25991;&#29486;&#21644;&#30693;&#35782;&#24179;&#21488;&#21482;&#20851;&#27880;&#29983;&#29289;&#23398;&#21644;&#21307;&#23398;&#39046;&#22495;&#30340;&#35770;&#25991;&#25910;&#38598;&#65292;&#24573;&#30053;&#20102;&#36328;&#23398;&#31185;&#30340;&#21162;&#21147;&#65292;&#36825;&#22952;&#30861;&#20102;&#39046;&#22495;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#21644;&#30740;&#31350;&#21512;&#20316;&#20197;&#35299;&#20915;&#38382;&#39064;&#12290;&#30740;&#31350;&#36328;&#23398;&#31185;&#30740;&#31350;&#38656;&#35201;&#26377;&#25928;&#30340;&#35770;&#25991;&#20998;&#31867;&#21644;&#39640;&#25928;&#30340;&#36328;&#39046;&#22495;&#30693;&#35782;&#25552;&#21462;&#21644;&#25972;&#21512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;Covidia&#65292;COVID-19&#36328;&#23398;&#31185;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#24357;&#21512;&#19981;&#21516;&#39046;&#22495;&#23545;COVID-19&#30693;&#35782;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#35774;&#35745;&#26694;&#26550;&#36827;&#34892;&#23398;&#31185;&#20998;&#31867;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#26415;&#30693;&#35782;&#22270;&#35889;&#26041;&#26696;&#65292;&#29992;&#20110;&#20132;&#21449;&#39046;&#22495;&#30340;&#23454;&#20307;&#25552;&#21462;&#12289;&#20851;&#31995;&#20998;&#31867;&#21644;&#26412;&#20307;&#35770;&#31649;&#29702;&#12290;&#22522;&#20110;Covidia&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#21457;&#29616;COVID-19&#30740;&#31350;&#30340;&#30693;&#35782;&#21457;&#29616;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The pandemic of COVID-19 has inspired extensive works across different research fields. Existing literature and knowledge platforms on COVID-19 only focus on collecting papers on biology and medicine, neglecting the interdisciplinary efforts, which hurdles knowledge sharing and research collaborations between fields to address the problem. Studying interdisciplinary researches requires effective paper category classification and efficient cross-domain knowledge extraction and integration. In this work, we propose Covidia, COVID-19 interdisciplinary academic knowledge graph to bridge the gap between knowledge of COVID-19 on different domains. We design frameworks based on contrastive learning for disciplinary classification, and propose a new academic knowledge graph scheme for entity extraction, relation classification and ontology management in accordance with interdisciplinary researches. Based on Covidia, we also establish knowledge discovery benchmarks for finding COVID-19 research
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#23558;&#20998;&#26512;&#21644;&#25968;&#20540;&#25512;&#23548;&#32467;&#21512;&#65292;&#22312;&#22522;&#20110;&#24191;&#20041; Potts &#27169;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#23545;&#32463;&#36807;&#25913;&#36827;&#36866;&#24212;&#36825;&#31181;&#27169;&#22411;&#30340;self-attention&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#32463;&#36807;&#20462;&#25913;&#30340;self-attention&#26426;&#21046;&#21487;&#20197;&#22312;&#26497;&#38480;&#37319;&#26679;&#19979;&#20934;&#30830;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#36825;&#20010;&#8220;&#20998;&#35299;&#8221;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07235</link><description>&lt;p&gt;
&#21033;&#29992;&#20998;&#35299;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21333;&#23618;Transformer&#23545;&#24191;&#20041;Potts&#27169;&#22411;&#36827;&#34892;&#26368;&#20248;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal inference of a generalised Potts model by single-layer transformers with factored attention. (arXiv:2304.07235v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07235
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#20998;&#26512;&#21644;&#25968;&#20540;&#25512;&#23548;&#32467;&#21512;&#65292;&#22312;&#22522;&#20110;&#24191;&#20041; Potts &#27169;&#22411;&#30340;&#25968;&#25454;&#19978;&#65292;&#23545;&#32463;&#36807;&#25913;&#36827;&#36866;&#24212;&#36825;&#31181;&#27169;&#22411;&#30340;self-attention&#26426;&#21046;&#36827;&#34892;&#35757;&#32451;&#65292;&#21457;&#29616;&#32463;&#36807;&#20462;&#25913;&#30340;self-attention&#26426;&#21046;&#21487;&#20197;&#22312;&#26497;&#38480;&#37319;&#26679;&#19979;&#20934;&#30830;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#36825;&#20010;&#8220;&#20998;&#35299;&#8221;&#27880;&#24847;&#21147;&#26426;&#21046;&#36890;&#36807;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30456;&#20851;&#23646;&#24615;&#65292;&#21487;&#20197;&#25552;&#39640;Transformer&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer &#26159;&#19968;&#31181;&#38761;&#21629;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#34507;&#30333;&#36136;&#31185;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#23454;&#36341;&#19978;&#30340;&#25104;&#21151;&#12290;&#23427;&#20204;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#26159;&#19968;&#20010;&#21483;&#20570;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26426;&#21046;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#21477;&#23376;&#20013;&#32570;&#22833;&#30340;&#35789;&#12290;&#23613;&#31649;Transformer&#22312;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#23454;&#36341;&#19978;&#30340;&#25104;&#21151;&#65292;&#20294;&#26159;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#31350;&#31455;&#20174;&#25968;&#25454;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#20197;&#21450;&#23427;&#26159;&#24590;&#20040;&#20570;&#21040;&#30340;&#36824;&#19981;&#26159;&#24456;&#28165;&#26970;&#12290;&#26412;&#25991;&#38024;&#23545;&#20174;&#20855;&#26377;&#30456;&#20114;&#20316;&#29992;&#30340;&#20301;&#32622;&#21644; Potts &#39068;&#33394;&#20013;&#25552;&#21462;&#30340;&#25968;&#25454;&#22312;&#35757;&#32451;&#30340;Transformer&#19978;&#32473;&#20986;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#21644;&#25968;&#20540;&#21051;&#30011;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#34429;&#28982;&#19968;&#33324;&#30340;transformer&#38656;&#35201;&#22810;&#23618;&#23398;&#20064;&#25165;&#33021;&#20934;&#30830;&#23398;&#20064;&#36825;&#20010;&#20998;&#24067;&#65292;&#20294;&#26159;&#32463;&#36807;&#23567;&#25913;&#36827;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#26080;&#38480;&#37319;&#26679;&#30340;&#26497;&#38480;&#19979;&#21487;&#20197;&#23436;&#32654;&#22320;&#23398;&#20064;Potts&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35745;&#31639;&#20102;&#36825;&#20010;&#20462;&#25913;&#21518;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#25152;&#35859;&#8220;&#20998;&#35299;&#8221;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#25968;&#20540;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#35299;&#37322;Transformer&#30340;&#20869;&#22312;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#25552;&#39640;&#20854;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers are the type of neural networks that has revolutionised natural language processing and protein science. Their key building block is a mechanism called self-attention which is trained to predict missing words in sentences. Despite the practical success of transformers in applications it remains unclear what self-attention learns from data, and how. Here, we give a precise analytical and numerical characterisation of transformers trained on data drawn from a generalised Potts model with interactions between sites and Potts colours. While an off-the-shelf transformer requires several layers to learn this distribution, we show analytically that a single layer of self-attention with a small modification can learn the Potts model exactly in the limit of infinite sampling. We show that this modified self-attention, that we call ``factored'', has the same functional form as the conditional probability of a Potts spin given the other spins, compute its generalisation error using t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;BPM&#30740;&#31350;&#24102;&#26469;&#35832;&#22810;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.07183</link><description>&lt;p&gt;
Just Tell Me: &#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Just Tell Me: Prompt Engineering in Business Process Management. (arXiv:2304.07183v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07183
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#25552;&#31034;&#24037;&#31243;&#26041;&#27861;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#30340;&#38382;&#39064;&#65292;&#24182;&#20026;BPM&#30740;&#31350;&#24102;&#26469;&#35832;&#22810;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3&#21644;&#20854;&#20182;&#20960;&#20010;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#26426;&#22120;&#32763;&#35793;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#26368;&#36817;&#65292;&#23427;&#20204;&#20063;&#22312;&#19994;&#21153;&#27969;&#31243;&#31649;&#29702;&#65288;BPM&#65289;&#39046;&#22495;&#25104;&#21151;&#24212;&#29992;&#65292;&#20363;&#22914;&#29992;&#20110;&#39044;&#27979;&#36807;&#31243;&#30417;&#25511;&#21644;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#23545;&#25152;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#21512;&#36866;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#25552;&#31034;&#24037;&#31243;&#65292;&#23427;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#35748;&#35782;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#35748;&#20026;&#25552;&#31034;&#24037;&#31243;&#21487;&#20197;&#24110;&#21161;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24341;&#20837;BPM&#30740;&#31350;&#12290;&#26412;&#31687;&#25991;&#31456;&#21033;&#29992;&#36825;&#19968;&#35266;&#28857;&#65292;&#36890;&#36807;&#30830;&#23450;&#30456;&#20851;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#65292;&#20026;BPM&#30740;&#31350;&#30340;&#25552;&#31034;&#24037;&#31243;&#20351;&#29992;&#21046;&#23450;&#30740;&#31350;&#35758;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3 and several other language models (LMs) can effectively address various natural language processing (NLP) tasks, including machine translation and text summarization. Recently, they have also been successfully employed in the business process management (BPM) domain, e.g., for predictive process monitoring and process extraction from text. This, however, typically requires fine-tuning the employed LM, which, among others, necessitates large amounts of suitable training data. A possible solution to this problem is the use of prompt engineering, which leverages pre-trained LMs without fine-tuning them. Recognizing this, we argue that prompt engineering can help bring the capabilities of LMs to BPM research. We use this position paper to develop a research agenda for the use of prompt engineering for BPM research by identifying the associated potentials and challenges.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SemEval 2023&#20219;&#21153;9&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25512;&#25991;&#20146;&#23494;&#24230;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#21644;&#20266;&#26631;&#35760;&#31034;&#20363;&#25193;&#23637;&#35757;&#32451;&#38598;&#65292;&#20197;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#38598;&#21512;&#65292;&#26368;&#32456;&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;</title><link>http://arxiv.org/abs/2304.07130</link><description>&lt;p&gt;
SemEval 2023&#20219;&#21153;9&#20013;&#38024;&#23545;&#22810;&#35821;&#35328;&#25512;&#25991;&#20146;&#23494;&#24230;&#20998;&#26512;&#30340;&#31616;&#27905;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OPI at SemEval 2023 Task 9: A Simple But Effective Approach to Multilingual Tweet Intimacy Analysis. (arXiv:2304.07130v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07130
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;SemEval 2023&#20219;&#21153;9&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#35821;&#35328;&#25512;&#25991;&#20146;&#23494;&#24230;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#21644;&#20266;&#26631;&#35760;&#31034;&#20363;&#25193;&#23637;&#35757;&#32451;&#38598;&#65292;&#20197;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#38598;&#21512;&#65292;&#26368;&#32456;&#22312;&#21313;&#31181;&#35821;&#35328;&#20013;&#25490;&#21517;&#21069;&#20116;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval 2023&#22810;&#35821;&#35328;&#25512;&#25991;&#20146;&#23494;&#24230;&#20998;&#26512;&#20219;&#21153;&#30340;&#25552;&#20132;&#12290;&#20219;&#21153;&#26088;&#22312;&#35780;&#20272;10&#31181;&#35821;&#35328;&#20013;Twitter&#24086;&#23376;&#30340;&#20146;&#23494;&#31243;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20960;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36827;&#34892;&#39046;&#22495;&#20869;&#39044;&#35757;&#32451;&#65292;&#21019;&#24314;&#36866;&#24212;Twitter&#25968;&#25454;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#26631;&#35760;&#30340;&#31034;&#20363;&#25193;&#23637;&#35757;&#32451;&#38598;&#65292;&#35757;&#32451;&#22238;&#24402;&#27169;&#22411;&#38598;&#21512;&#12290;&#25193;&#23637;&#30340;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#26368;&#32456;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;10&#31181;&#35821;&#35328;&#23376;&#20219;&#21153;&#20013;&#25490;&#21517;&#21069;&#20116;&#65292;&#24182;&#33719;&#24471;&#25152;&#26377;&#35821;&#35328;&#20013;&#26368;&#39640;&#30340;&#24179;&#22343;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our submission to the SemEval 2023 multilingual tweet intimacy analysis shared task. The goal of the task was to assess the level of intimacy of Twitter posts in ten languages. The proposed approach consists of several steps. First, we perform in-domain pre-training to create a language model adapted to Twitter data. In the next step, we train an ensemble of regression models to expand the training set with pseudo-labeled examples. The extended dataset is used to train the final solution. Our method was ranked first in five out of ten language subtasks, obtaining the highest average score across all languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#29992;&#20110;SemEval 2023&#35270;&#35273;&#35789;&#20041;&#28040;&#23696;&#20849;&#20139;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#12289;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19977;&#65292;&#22312;&#27874;&#26031;&#35821;&#36319;&#36394;&#20013;&#33719;&#32988;&#12290;</title><link>http://arxiv.org/abs/2304.07127</link><description>&lt;p&gt;
SemEval 2023&#20219;&#21153;1&#20013;&#30340;OPI&#65306;&#22522;&#20110;&#22270;&#20687;-&#25991;&#26412;&#23884;&#20837;&#21644;&#22810;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#35270;&#35273;&#35789;&#20041;&#28040;&#23696;
&lt;/p&gt;
&lt;p&gt;
OPI at SemEval 2023 Task 1: Image-Text Embeddings and Multimodal Information Retrieval for Visual Word Sense Disambiguation. (arXiv:2304.07127v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#29992;&#20110;SemEval 2023&#35270;&#35273;&#35789;&#20041;&#28040;&#23696;&#20849;&#20139;&#20219;&#21153;&#30340;&#31995;&#32479;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#12289;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19977;&#65292;&#22312;&#27874;&#26031;&#35821;&#36319;&#36394;&#20013;&#33719;&#32988;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35789;&#20041;&#28040;&#23696;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#26368;&#31526;&#21512;&#25552;&#20379;&#30340;&#35789;&#20041;&#25551;&#36848;&#30340;&#22270;&#20687;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#32467;&#21512;&#35821;&#35328;&#21644;&#22270;&#20687;&#29702;&#35299;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;SemEval 2023&#35270;&#35273;&#35789;&#20041;&#28040;&#23696;&#20849;&#20139;&#20219;&#21153;&#30340;&#25552;&#20132;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#38598;&#25104;&#20102;&#22810;&#27169;&#24577;&#23884;&#20837;&#12289;&#23398;&#20064;&#25490;&#24207;&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22522;&#20110;CLIP&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#20854;&#32467;&#26524;&#20351;&#29992;&#20174;&#32500;&#22522;&#30334;&#31185;&#21644;&#35789;&#27719;&#25968;&#25454;&#24211;&#26816;&#32034;&#21040;&#30340;&#38468;&#21152;&#20449;&#24687;&#36827;&#34892;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#22810;&#35821;&#35328;&#20219;&#21153;&#20013;&#25490;&#21517;&#31532;&#19977;&#65292;&#22312;&#27874;&#26031;&#35821;&#36319;&#36394;&#20013;&#33719;&#32988;&#65292;&#26159;&#19977;&#20010;&#35821;&#35328;&#23376;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of visual word sense disambiguation is to find the image that best matches the provided description of the word's meaning. It is a challenging problem, requiring approaches that combine language and image understanding. In this paper, we present our submission to SemEval 2023 visual word sense disambiguation shared task. The proposed system integrates multimodal embeddings, learning to rank methods, and knowledge-based approaches. We build a classifier based on the CLIP model, whose results are enriched with additional information retrieved from Wikipedia and lexical databases. Our solution was ranked third in the multilingual task and won in the Persian track, one of the three language subtasks.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CONVSR&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#23545;&#35805;&#24615;&#36136;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#35299;&#20915;&#23545;&#35805;&#24335;&#38382;&#31572;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.07125</link><description>&lt;p&gt;
&#35753;&#38382;&#39064;&#26356;&#20855;&#23545;&#35805;&#24615;&#65306;&#20351;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#35299;&#20915;&#38382;&#31572;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Keeping the Questions Conversational: Using Structured Representations to Resolve Dependency in Conversational Question Answering. (arXiv:2304.07125v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07125
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CONVSR&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#23545;&#35805;&#24615;&#36136;&#30340;&#21516;&#26102;&#65292;&#20351;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#35299;&#20915;&#23545;&#35805;&#24335;&#38382;&#31572;&#20013;&#30340;&#20381;&#36182;&#20851;&#31995;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25317;&#26377;&#19968;&#20010;&#33021;&#22815;&#21442;&#19982;&#23545;&#35805;&#24335;&#38382;&#31572;&#65288;ConvQA&#65289;&#30340;&#26234;&#33021;&#23545;&#35805;&#20195;&#29702;&#29616;&#22312;&#24050;&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#31185;&#24187;&#30005;&#24433;&#65292;&#24182;&#19988;&#24050;&#32463;&#25104;&#20026;&#29616;&#23454;&#12290;&#36825;&#20123;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#27491;&#30830;&#35299;&#37322;&#20316;&#20026;&#32473;&#23450;&#38382;&#39064;&#32972;&#26223;&#30340;&#39034;&#24207;&#36716;&#25442;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39034;&#24207;&#30340;&#38382;&#39064;&#26377;&#26102;&#20250;&#34987;&#30041;&#19979;&#38544;&#21547;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#35299;&#20915;&#19968;&#20123;&#33258;&#28982;&#35821;&#35328;&#29616;&#35937;&#65292;&#20363;&#22914;&#25351;&#20195;&#21644;&#30465;&#30053;&#12290;&#23545;&#38382;&#39064;&#37325;&#20889;&#30340;&#20219;&#21153;&#26377;&#28508;&#21147;&#36890;&#36807;&#23558;&#23427;&#20204;&#36716;&#25442;&#25104;&#26126;&#30830;&#24847;&#22270;&#30340;&#38382;&#39064;&#26469;&#35299;&#20915;&#35299;&#20915;&#19978;&#19979;&#25991;&#36716;&#25442;&#20043;&#38388;&#20381;&#36182;&#20851;&#31995;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#37325;&#20889;&#38544;&#24335;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#23384;&#22312;&#19968;&#20123;&#28508;&#22312;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#20887;&#38271;&#30340;&#38382;&#39064;&#24182;&#36890;&#36807;&#29983;&#25104;&#33258;&#21253;&#21547;&#38382;&#39064;&#30340;&#26041;&#24335;&#20351;&#23545;&#35805;&#20013;&#30340;&#24773;&#22659;&#21066;&#24369;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;CONVSR&#65288;&#20351;&#29992;&#32467;&#26500;&#21270;&#34920;&#31034;&#30340;CONVQA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Having an intelligent dialogue agent that can engage in conversational question answering (ConvQA) is now no longer limited to Sci-Fi movies only and has, in fact, turned into a reality. These intelligent agents are required to understand and correctly interpret the sequential turns provided as the context of the given question. However, these sequential questions are sometimes left implicit and thus require the resolution of some natural language phenomena such as anaphora and ellipsis. The task of question rewriting has the potential to address the challenges of resolving dependencies amongst the contextual turns by transforming them into intent-explicit questions. Nonetheless, the solution of rewriting the implicit questions comes with some potential challenges such as resulting in verbose questions and taking conversational aspect out of the scenario by generating self-contained questions. In this paper, we propose a novel framework, CONVSR (CONVQA using Structured Representations)
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;HLTPR@RWTH&#22242;&#38431;&#22312;DSTC9&#21644;DSTC10&#20013;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#25152;&#20570;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#65292;&#22312;DSTC10&#20013;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22242;&#38431;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.07101</link><description>&lt;p&gt;
HLTPR@RWTH&#22312;DSTC9&#21644;DSTC10&#20013;&#30340;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Task-oriented Document-Grounded Dialog Systems by HLTPR@RWTH for DSTC9 and DSTC10. (arXiv:2304.07101v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;HLTPR@RWTH&#22242;&#38431;&#22312;DSTC9&#21644;DSTC10&#20013;&#20026;&#20219;&#21153;&#23548;&#21521;&#22411;&#25991;&#26723;&#23545;&#35805;&#31995;&#32479;&#25152;&#20570;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#65292;&#22312;DSTC10&#20013;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#24182;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20197;&#21450;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#22242;&#38431;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#25105;&#20204;&#22312;&#31532;9&#21644;&#31532;10&#27425;Dialog System Technology Challenges&#65288;DSTC9&#21644;DSTC10&#65289;&#20013;&#20026;&#23545;&#35805;&#22522;&#20110;&#25991;&#26723;&#30340;&#20219;&#21153;&#20316;&#20986;&#30340;&#36129;&#29486;&#12290;&#22312;&#20004;&#27425;&#36845;&#20195;&#20013;&#65292;&#20219;&#21153;&#30001;&#19977;&#20010;&#23376;&#20219;&#21153;&#32452;&#25104;&#65306;&#39318;&#20808;&#26816;&#27979;&#24403;&#21069;&#22238;&#21512;&#26159;&#21542;&#38656;&#35201;&#30693;&#35782;&#65292;&#20854;&#27425;&#36873;&#25321;&#30456;&#20851;&#30340;&#30693;&#35782;&#25991;&#26723;&#65292;&#31532;&#19977;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#25991;&#26723;&#30340;&#22238;&#31572;&#12290;&#23545;&#20110;DSTC9&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#20351;&#36873;&#25321;&#20219;&#21153;&#26356;&#26377;&#25928;&#29575;&#12290;&#20854;&#20013;&#26368;&#22909;&#30340;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#36873;&#25321;&#65292;&#23454;&#38469;&#19978;&#27604;&#21407;&#22987;&#22522;&#32447;&#25913;&#36827;&#20102;&#32467;&#26524;&#65292;&#24182;&#25552;&#39640;&#20102;24&#20493;&#36895;&#24230;&#12290;&#22312;DSTC10&#36845;&#20195;&#20013;&#65292;&#25361;&#25112;&#26159;&#35201;&#20351;&#32463;&#36807;&#20070;&#38754;&#23545;&#35805;&#35757;&#32451;&#30340;&#31995;&#32479;&#33021;&#22815;&#22312;&#22024;&#26434;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#20013;&#34920;&#29616;&#33391;&#22909;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20197;&#21450;&#36866;&#24212;&#29983;&#25104;&#22238;&#31572;&#30340;&#39118;&#26684;&#65292;&#20351;&#20854;&#19982;&#21069;&#26399;&#23545;&#35805;&#30456;&#21305;&#37197;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22024;&#26434;&#30340;&#36890;&#36947;&#27169;&#22411;&#26469;&#30452;&#25509;&#24314;&#27169;&#35821;&#38899;&#35782;&#21035;&#38169;&#35823;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25152;&#26377;&#23376;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper summarizes our contributions to the document-grounded dialog tasks at the 9th and 10th Dialog System Technology Challenges (DSTC9 and DSTC10). In both iterations the task consists of three subtasks: first detect whether the current turn is knowledge seeking, second select a relevant knowledge document, and third generate a response grounded on the selected document. For DSTC9 we proposed different approaches to make the selection task more efficient. The best method, Hierarchical Selection, actually improves the results compared to the original baseline and gives a speedup of 24x. In the DSTC10 iteration of the task, the challenge was to adapt systems trained on written dialogs to perform well on noisy automatic speech recognition transcripts. Therefore, we proposed data augmentation techniques to increase the robustness of the models as well as methods to adapt the style of generated responses to fit well into the proceeding dialog. Additionally, we proposed a noisy channel
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;SEA&#65292;&#23427;&#21253;&#25324;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;EA&#27169;&#22411;&#24182;&#33021;&#22815;&#20351;&#29992;&#25143;&#36731;&#26494;&#24314;&#31435;&#12289;&#35780;&#20272;&#33258;&#24049;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;GNN&#30340;EA&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.07065</link><description>&lt;p&gt;
SEA: &#19968;&#20010;&#21487;&#25193;&#23637;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
SEA: A Scalable Entity Alignment System. (arXiv:2304.07065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07065
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;SEA&#65292;&#23427;&#21253;&#25324;&#20102;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;EA&#27169;&#22411;&#24182;&#33021;&#22815;&#20351;&#29992;&#25143;&#36731;&#26494;&#24314;&#31435;&#12289;&#35780;&#20272;&#33258;&#24049;&#30340;&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;GNN&#30340;EA&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26088;&#22312;&#22312;&#19981;&#21516;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#30456;&#24212;&#30340;&#23454;&#20307;&#12290;&#29616;&#26377;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#32534;&#30721;&#23454;&#20307;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#26159;&#22312;&#20840;&#25209;&#37327;&#27169;&#24335;&#19979;&#35757;&#32451;&#27169;&#22411;&#21644;&#35780;&#20272;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#23454;&#20307;&#23545;&#40784;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26080;&#27861;&#25193;&#23637;&#12290;&#20026;&#20102;&#22686;&#24378;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#23454;&#20307;&#23545;&#40784;&#31995;&#32479;SEA&#12290;&#23427;&#33021;&#22815;(i)&#35757;&#32451;&#22823;&#35268;&#27169;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#23454;&#20307;&#23545;&#40784;&#65292;(ii)&#21152;&#36895;&#24402;&#19968;&#21270;&#21644;&#35780;&#20272;&#36807;&#31243;&#65292;(iii)&#20026;&#29992;&#25143;&#25552;&#20379;&#28165;&#26224;&#30340;&#32467;&#26524;&#20197;&#20272;&#35745;&#19981;&#21516;&#30340;&#27169;&#22411;&#21644;&#21442;&#25968;&#35774;&#32622;&#12290;SEA&#21482;&#38656;&#35201;&#19968;&#20010;&#22270;&#24418;&#21345;&#23601;&#21487;&#20197;&#36816;&#34892;&#12290;&#27492;&#22806;&#65292;SEA&#21253;&#25324;&#20845;&#20010;&#26368;&#20808;&#36827;&#30340;&#23454;&#20307;&#23545;&#40784;&#27169;&#22411;&#65292;&#24182;&#20026;&#29992;&#25143;&#25552;&#20379;&#24555;&#36895;&#24314;&#31435;&#21644;&#35780;&#20272;&#33258;&#24049;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;SEA&#20801;&#35768;&#29992;&#25143;&#22312;&#19981;&#28041;&#21450;&#22797;&#26434;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#23454;&#20307;&#23545;&#40784;&#65292;&#22914;&#36127;&#25277;&#26679;&#21644;GPU&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment (EA) aims to find equivalent entities in different knowledge graphs (KGs). State-of-the-art EA approaches generally use Graph Neural Networks (GNNs) to encode entities. However, most of them train the models and evaluate the results in a fullbatch fashion, which prohibits EA from being scalable on largescale datasets. To enhance the usability of GNN-based EA models in real-world applications, we present SEA, a scalable entity alignment system that enables to (i) train large-scale GNNs for EA, (ii) speed up the normalization and the evaluation process, and (iii) report clear results for users to estimate different models and parameter settings. SEA can be run on a computer with merely one graphic card. Moreover, SEA encompasses six state-of-the-art EA models and provides access for users to quickly establish and evaluate their own models. Thus, SEA allows users to perform EA without being involved in tedious implementations, such as negative sampling and GPU-accelerated
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#30456;&#20381;&#24863;&#30693;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#26631;&#31614;&#20998;&#31867;&#35270;&#20026;&#30452;&#25509;&#38598;&#21512;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#26500;&#24314;&#20851;&#32852;&#30697;&#38453;&#24182;&#32467;&#21512;GCN&#23398;&#20064;&#26631;&#31614;&#20449;&#24687;&#65292;&#21516;&#26102;&#21033;&#29992;&#21477;&#23376;&#20449;&#24687;&#21644;&#26631;&#31614;&#20449;&#24687;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.07022</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#31614;&#30456;&#20381;&#24863;&#30693;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Label Dependencies-aware Set Prediction Networks for Multi-label Text Classification. (arXiv:2304.07022v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#31614;&#30456;&#20381;&#24863;&#30693;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#29992;&#20110;&#35299;&#20915;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#22810;&#26631;&#31614;&#20998;&#31867;&#35270;&#20026;&#30452;&#25509;&#38598;&#21512;&#39044;&#27979;&#38382;&#39064;&#65292;&#36890;&#36807;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#26500;&#24314;&#20851;&#32852;&#30697;&#38453;&#24182;&#32467;&#21512;GCN&#23398;&#20064;&#26631;&#31614;&#20449;&#24687;&#65292;&#21516;&#26102;&#21033;&#29992;&#21477;&#23376;&#20449;&#24687;&#21644;&#26631;&#31614;&#20449;&#24687;&#65292;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#20174;&#21477;&#23376;&#20013;&#25552;&#21462;&#25152;&#26377;&#30456;&#20851;&#26631;&#31614;&#65292;&#21487;&#35270;&#20026;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#38598;&#20013;&#30340;&#26631;&#31614;&#26159;&#26080;&#24207;&#30340;&#12290;&#25105;&#20204;&#24314;&#35758;&#23558;&#20854;&#35270;&#20026;&#30452;&#25509;&#38598;&#21512;&#39044;&#27979;&#38382;&#39064;&#65292;&#32780;&#19981;&#38656;&#35201;&#32771;&#34385;&#26631;&#31614;&#30340;&#39034;&#24207;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#24314;&#27169;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#36890;&#36807;&#26631;&#31614;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#26500;&#24314;&#20851;&#32852;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;GCN&#26469;&#23398;&#20064;&#26631;&#31614;&#20449;&#24687;&#12290;&#22522;&#20110;&#25152;&#23398;&#30340;&#26631;&#31614;&#20449;&#24687;&#65292;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#21477;&#23376;&#20449;&#24687;&#21644;&#26631;&#31614;&#20449;&#24687;&#36827;&#34892;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#36824;&#23545;&#38598;&#21512;&#39044;&#27979;&#32593;&#32476;&#30340;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#26045;&#21152;&#24191;&#20041;&#24052;&#27663;&#36317;&#31163;&#65292;&#20197;&#25552;&#39640;&#20854;&#21484;&#22238;&#29575;&#12290;&#22312;&#22235;&#20010;&#22810;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#20854;&#24615;&#33021;&#22823;&#22823;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-label text classification aims to extract all the related labels from a sentence, which can be viewed as a sequence generation problem. However, the labels in training dataset are unordered. We propose to treat it as a direct set prediction problem and don't need to consider the order of labels. Besides, in order to model the correlation between labels, the adjacency matrix is constructed through the statistical relations between labels and GCN is employed to learn the label information. Based on the learned label information, the set prediction networks can both utilize the sentence information and label information for multi-label text classification simultaneously. Furthermore, the Bhattacharyya distance is imposed on the output probability distributions of the set prediction networks to increase the recall ability. Experimental results on four multi-label datasets show the effectiveness of the proposed method and it outperforms previous method a substantial margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#28216;&#25103;&#36827;&#34892;&#23454;&#36341;&#24615;&#35821;&#35328;&#20351;&#29992;&#30340;&#27979;&#35797;&#65292;&#20197;&#35780;&#20215;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#8220;&#20154;&#24037;&#35821;&#35328;&#29702;&#35299;&#8221;&#33021;&#21147;&#65292;&#20174;&#32780;&#34917;&#20805;&#24418;&#24335;&#21270;&#35821;&#35328;&#29702;&#35299;&#27979;&#35797;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.07007</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#28216;&#25103;&#30340;&#35821;&#35328;&#29702;&#35299;&#22522;&#20934;&#27979;&#37327;&#65306;&#21160;&#26426;&#12289;&#20998;&#31867;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Dialogue Games for Benchmarking Language Understanding: Motivation, Taxonomy, Strategy. (arXiv:2304.07007v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#36890;&#36807;&#26500;&#24314;&#19981;&#21516;&#31867;&#22411;&#30340;&#23545;&#35805;&#28216;&#25103;&#36827;&#34892;&#23454;&#36341;&#24615;&#35821;&#35328;&#20351;&#29992;&#30340;&#27979;&#35797;&#65292;&#20197;&#35780;&#20215;&#35745;&#31639;&#26426;&#31243;&#24207;&#30340;&#8220;&#20154;&#24037;&#35821;&#35328;&#29702;&#35299;&#8221;&#33021;&#21147;&#65292;&#20174;&#32780;&#34917;&#20805;&#24418;&#24335;&#21270;&#35821;&#35328;&#29702;&#35299;&#27979;&#35797;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#27979;&#37327;&#8220;&#29702;&#35299;&#35821;&#35328;&#30340;&#33021;&#21147;&#8221;&#65311;&#22914;&#26524;&#26159;&#34913;&#37327;&#19968;&#20010;&#20154;&#30340;&#33021;&#21147;&#65292;&#36825;&#20960;&#20046;&#20174;&#26410;&#20197;&#19981;&#21512;&#26684;&#30340;&#26041;&#24335;&#25552;&#20986;&#65306;&#26080;&#35770;&#24212;&#29992;&#20309;&#31181;&#27491;&#24335;&#27979;&#35797;&#65292;&#37117;&#26159;&#22312;&#20154;&#20204;&#26085;&#24120;&#31038;&#20132;&#23454;&#36341;&#30340;&#35821;&#35328;&#20351;&#29992;&#32972;&#26223;&#19979;&#36827;&#34892;&#30340;&#65292;&#25152;&#27979;&#37327;&#30340;&#26159;&#19968;&#31181;&#19987;&#19994;&#30340;&#35821;&#35328;&#29702;&#35299;&#65288;&#20363;&#22914;&#31532;&#20108;&#35821;&#35328;&#25110;&#20070;&#38754;&#25216;&#26415;&#35821;&#35328;&#65289;&#12290;&#35745;&#31639;&#26426;&#31243;&#24207;&#27809;&#26377;&#36825;&#26679;&#30340;&#32972;&#26223;&#12290;&#36825;&#23545;&#20110;&#27491;&#24335;&#30340;&#35821;&#35328;&#29702;&#35299;&#27979;&#35797;&#30340;&#36866;&#29992;&#24615;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;&#25105;&#35748;&#20026;&#36825;&#20123;&#27979;&#35797;&#38656;&#35201;&#34917;&#20805;&#23884;&#20837;&#22312;&#23454;&#36341;&#20013;&#30340;&#35821;&#35328;&#20351;&#29992;&#27979;&#35797;&#65292;&#20197;&#24471;&#20986;&#26356;&#20840;&#38754;&#30340;&#8220;&#20154;&#24037;&#35821;&#35328;&#29702;&#35299;&#8221;&#35780;&#20272;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#36827;&#34892;&#36825;&#26679;&#30340;&#27979;&#35797;&#65292;&#25105;&#25552;&#35758;&#20351;&#29992;&#8220;&#23545;&#35805;&#28216;&#25103;&#8221;&#65292;&#26500;&#24314;&#25552;&#20379;&#24773;&#22659;&#23884;&#20837;&#30340;&#27963;&#21160;&#12290;&#25105;&#25551;&#36848;&#20102;&#19968;&#31181;&#19982;&#28508;&#22312;&#33021;&#21147;&#27169;&#22411;&#30456;&#20851;&#32852;&#30340;&#23545;&#35805;&#28216;&#25103;&#31867;&#22411;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does one measure "ability to understand language"? If it is a person's ability that is being measured, this is a question that almost never poses itself in an unqualified manner: Whatever formal test is applied, it takes place on the background of the person's language use in daily social practice, and what is measured is a specialised variety of language understanding (e.g., of a second language; or of written, technical language). Computer programs do not have this background. What does that mean for the applicability of formal tests of language understanding? I argue that such tests need to be complemented with tests of language use embedded in a practice, to arrive at a more comprehensive evaluation of "artificial language understanding". To do such tests systematically, I propose to use "Dialogue Games" -- constructed activities that provide a situational embedding for language use. I describe a taxonomy of Dialogue Game types, linked to a model of underlying capabilites that 
&lt;/p&gt;</description></item><item><title>SimpLex&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31616;&#21270;&#33521;&#25991;&#21477;&#23376;&#30340;&#26032;&#22411;&#31616;&#21270;&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#35789;&#23884;&#20837;&#25110;&#21477;&#23376;&#36716;&#25442;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#21477;&#23376;&#24182;&#38598;&#25104;&#21040;&#26131;&#29992;&#30340;&#36719;&#20214;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#20013;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;SARI&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#21017;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.07002</link><description>&lt;p&gt;
SimpLex&#65306;&#19968;&#31181;&#35789;&#27719;&#25991;&#26412;&#31616;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SimpLex: a lexical text simplification architecture. (arXiv:2304.07002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07002
&lt;/p&gt;
&lt;p&gt;
SimpLex&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31616;&#21270;&#33521;&#25991;&#21477;&#23376;&#30340;&#26032;&#22411;&#31616;&#21270;&#26550;&#26500;&#65292;&#23427;&#20351;&#29992;&#35789;&#23884;&#20837;&#25110;&#21477;&#23376;&#36716;&#25442;&#22120;&#26469;&#29983;&#25104;&#31616;&#21270;&#21477;&#23376;&#24182;&#38598;&#25104;&#21040;&#26131;&#29992;&#30340;&#36719;&#20214;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#20013;&#21457;&#29616;&#65292;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;SARI&#24471;&#20998;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#32780;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#21017;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26159;&#23558;&#32473;&#23450;&#21477;&#23376;&#25110;&#25991;&#26412;&#29983;&#25104;&#26131;&#20110;&#29702;&#35299;&#30340;&#21477;&#23376;&#30340;&#36807;&#31243;&#12290;&#31616;&#21270;&#30340;&#30446;&#30340;&#26159;&#22312;&#19981;&#25439;&#22833;&#21547;&#20041;&#25110;&#32454;&#24494;&#24046;&#21035;&#30340;&#24773;&#20917;&#19979;&#20943;&#23569;&#32473;&#23450;&#25991;&#26412;&#25110;&#21477;&#23376;&#30340;&#35789;&#27719;&#21644;&#35821;&#27861;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SimpLex&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#31616;&#21270;&#33521;&#25991;&#21477;&#23376;&#30340;&#26032;&#22411;&#31616;&#21270;&#26550;&#26500;&#12290;&#20026;&#20102;&#29983;&#25104;&#31616;&#21270;&#21477;&#23376;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20351;&#29992;&#35789;&#23884;&#20837;&#65288;&#21363;Word2Vec&#65289;&#21644;&#22256;&#24785;&#24230;&#25110;&#21477;&#23376;&#36716;&#25442;&#22120;&#65288;&#21363;BERT&#12289;RoBERTa&#21644;GPT2&#65289;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#20043;&#19968;&#12290;&#35813;&#35299;&#20915;&#26041;&#26696;&#38598;&#25104;&#21040;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#12289;&#26131;&#20110;&#20351;&#29992;&#30340;&#36719;&#20214;&#20013;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#25351;&#26631;&#65288;&#21363;SARI&#21644;&#22256;&#24785;&#24230;&#38477;&#20302;&#65289;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#12290;&#20174;&#23454;&#39564;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;SARI&#24471;&#20998;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20174;&#22256;&#24785;&#24230;&#26041;&#38754;&#26469;&#30475;&#65292;&#22522;&#20110;&#35789;&#23884;&#20837;&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text simplification (TS) is the process of generating easy-to-understand sentences from a given sentence or piece of text. The aim of TS is to reduce both the lexical (which refers to vocabulary complexity and meaning) and syntactic (which refers to the sentence structure) complexity of a given text or sentence without the loss of meaning or nuance. In this paper, we present \textsc{SimpLex}, a novel simplification architecture for generating simplified English sentences. To generate a simplified sentence, the proposed architecture uses either word embeddings (i.e., Word2Vec) and perplexity, or sentence transformers (i.e., BERT, RoBERTa, and GPT2) and cosine similarity. The solution is incorporated into a user-friendly and simple-to-use software. We evaluate our system using two metrics, i.e., SARI, and Perplexity Decrease. Experimentally, we observe that the transformer models outperform the other models in terms of the SARI score. However, in terms of Perplexity, the Word-Embeddings-
&lt;/p&gt;</description></item><item><title>HuaTuo &#26159;&#19968;&#31181;&#34701;&#21512;&#20013;&#21307;&#30693;&#35782;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#30340; LLaMA &#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#22238;&#31572;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06975</link><description>&lt;p&gt;
HuaTuo: &#34701;&#21512;&#20013;&#21307;&#30693;&#35782;&#20248;&#21270; LLaMA &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge. (arXiv:2304.06975v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06975
&lt;/p&gt;
&lt;p&gt;
HuaTuo &#26159;&#19968;&#31181;&#34701;&#21512;&#20013;&#21307;&#30693;&#35782;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#30340; LLaMA &#27169;&#22411;&#65292;&#21487;&#20197;&#26356;&#21487;&#38752;&#22320;&#22238;&#31572;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20363;&#22914; LLaMA &#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38656;&#35201;&#21307;&#23398;&#19987;&#19994;&#30693;&#35782;&#26469;&#22238;&#31572;&#38382;&#39064;&#65292;LLMs &#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20173;&#19981;&#29702;&#24819;&#12290;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HuaTuo&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110; LLaMA &#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#29983;&#25104;&#30340; QA&#65288;&#38382;&#39064;-&#22238;&#31572;&#65289;&#23454;&#20363;&#36827;&#34892;&#20102;&#30417;&#30563;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HuaTuo &#29983;&#25104;&#30340;&#21709;&#24212;&#20855;&#26377;&#26356;&#21487;&#38752;&#30340;&#21307;&#23398;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340; HuaTuo &#27169;&#22411;&#21487;&#22312; https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as the LLaMA model, have demonstrated their effectiveness in various general-domain natural language processing (NLP) tasks. Nevertheless, LLMs have not yet performed optimally in biomedical domain tasks due to the need for medical expertise in the responses. In response to this challenge, we propose HuaTuo, a LLaMA-based model that has been supervised-fine-tuned with generated QA (Question-Answer) instances. The experimental results demonstrate that HuaTuo generates responses that possess more reliable medical knowledge. Our proposed HuaTuo model is accessible at https://github.com/SCIR-HI/Huatuo-Llama-Med-Chinese.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;&#31574;&#30053;&#23545;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#27599;&#31181;&#31574;&#30053;&#20542;&#21521;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#20294;&#32852;&#21512;&#25928;&#26524;&#20026;&#36127;&#12290;</title><link>http://arxiv.org/abs/2304.06962</link><description>&lt;p&gt;
&#29992;&#20110;&#38646;&#26679;&#26412;&#24120;&#35782;&#25512;&#29702;&#30340;&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering and Calibration for Zero-Shot Commonsense Reasoning. (arXiv:2304.06962v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24182;&#35780;&#20272;&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;&#31574;&#30053;&#23545;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20116;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#27599;&#31181;&#31574;&#30053;&#20542;&#21521;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#20294;&#32852;&#21512;&#25928;&#26524;&#20026;&#36127;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#21644;&#26657;&#20934;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22810;&#39033;&#36873;&#25321;&#24120;&#35782;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20174;&#23454;&#38469;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#30740;&#31350;&#24182;&#35780;&#20272;&#20102;&#36825;&#20123;&#31574;&#30053;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#27599;&#31181;&#31574;&#30053;&#37117;&#20542;&#21521;&#20110;&#26576;&#20123;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#32852;&#21512;&#25928;&#26524;&#22823;&#22810;&#20026;&#36127;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt engineering and calibration make large language models excel at reasoning tasks, including multiple choice commonsense reasoning. From a practical perspective, we investigate and evaluate these strategies on smaller language models. Through experiments on five commonsense reasoning benchmarks, we find that each strategy favors certain models, but their joint effects are mostly negative.
&lt;/p&gt;</description></item><item><title>Multimodal C4&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#20197;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#26367;&#24418;&#24335;&#23384;&#22312;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20351;&#29992;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#23558;&#22270;&#20687;&#25918;&#21040;&#38271;&#25991;&#26412;&#27573;&#33853;&#20013;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21644;&#22797;&#26434;&#30456;&#20851;&#24230;&#25552;&#31034;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2304.06939</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;C4&#65306;&#19968;&#31181;&#21253;&#21547;&#22823;&#37327;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#24320;&#25918;&#24335;&#25968;&#25454;&#24211;
&lt;/p&gt;
&lt;p&gt;
Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved With Text. (arXiv:2304.06939v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06939
&lt;/p&gt;
&lt;p&gt;
Multimodal C4&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#12289;&#20197;&#22270;&#20687;&#19982;&#25991;&#26412;&#20132;&#26367;&#24418;&#24335;&#23384;&#22312;&#30340;&#25968;&#25454;&#24211;&#65292;&#20854;&#20351;&#29992;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#23558;&#22270;&#20687;&#25918;&#21040;&#38271;&#25991;&#26412;&#27573;&#33853;&#20013;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#21644;&#22797;&#26434;&#30456;&#20851;&#24230;&#25552;&#31034;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#38656;&#35201;&#25903;&#25345;&#20219;&#24847;&#20132;&#26367;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#24207;&#21015;&#20316;&#20026;&#36755;&#20837;, &#36825;&#31181;&#26684;&#24335;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20132;&#26367;&#29420;&#31435;&#30417;&#30563;&#30340;(&#22270;&#20687;,&#25991;&#26412;)&#31034;&#20363;&#26469;&#36827;&#34892;&#20302;&#27425;&#23398;&#20064;,&#32780;&#19988;&#21487;&#20197;&#24212;&#23545;&#26356;&#22797;&#26434;&#30340;&#25552;&#31034;, &#28041;&#21450;&#22270;&#20687;&#38388;&#20114;&#21160;,&#20363;&#22914;&#8220;&#22270;&#20687;A&#21644;&#22270;&#20687;B&#26377;&#20160;&#20040;&#20849;&#21516;&#20043;&#22788;?&#8221;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#31867;&#20284;&#20110;&#20132;&#26367;&#22270;&#20687;+&#25991;&#26412;&#30340;web&#35821;&#26009;&#24211;&#12290;&#20294;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#31181;&#24418;&#24335;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#36824;&#27809;&#26377;&#20844;&#24320;&#25552;&#20379;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;Multimodal C4 (mmc4)&#65292;&#36825;&#26159;&#19968;&#20010;&#21152;&#24378;&#29256;&#30340;c4&#25991;&#26412;&#24211;&#65292;&#20854;&#20013;&#25554;&#20837;&#20102;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#32447;&#24615;&#20998;&#37197;&#31639;&#27861;&#65292;&#20351;&#29992;CLIP&#29305;&#24449;&#23558;&#22270;&#20687;&#25918;&#21040;&#26356;&#38271;&#30340;&#25991;&#26412;&#20307;&#20013;&#65292;&#27492;&#36807;&#31243;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;mmc4&#28085;&#30422;&#20102;&#35832;&#22914;&#28921;&#39274;&#65292;&#26053;&#28216;&#65292;&#25216;&#26415;&#31561;&#26085;&#24120;&#20027;&#39064;&#12290;&#23545;&#38543;&#26426;&#26679;&#26412;&#30340;&#25163;&#21160;&#26816;&#26597;&#34920;&#26126;&#65292;&#32477;&#22823;&#22810;&#25968;(90%)&#30340;&#22270;&#20687;&#19982;&#20027;&#39064;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input. This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., "What do image A and image B have in common?" To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text. To date, however, large-scale data of this form have not been publicly available.  We release Multimodal C4 (mmc4), an augmentation of the popular text-only c4 corpus with images interleaved. We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives. mmc4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (90%) of images are topically relevant, and tha
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;&#65288;HCAM&#65289;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#20849;&#21516;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#20449;&#24687;&#20197;&#20849;&#21516;&#27880;&#24847;&#26041;&#24335;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24773;&#24863;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.06910</link><description>&lt;p&gt;
HCAM--&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#30340;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HCAM -- Hierarchical Cross Attention Model for Multi-modal Emotion Recognition. (arXiv:2304.06910v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06910
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;&#65288;HCAM&#65289;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;&#20351;&#29992;&#36882;&#24402;&#21644;&#20849;&#21516;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#65292;&#23558;&#36825;&#20004;&#31181;&#27169;&#24577;&#20449;&#24687;&#20197;&#20849;&#21516;&#27880;&#24847;&#26041;&#24335;&#32467;&#21512;&#65292;&#21462;&#24471;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#24773;&#24863;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#30001;&#20110;&#24773;&#24863;&#34920;&#36798;&#30340;&#22810;&#27169;&#24577;&#24615;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#36882;&#24402;&#21644;&#20849;&#21516;&#27880;&#24847;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20998;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22411;&#65288;HCAM&#65289;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#12290;&#27169;&#22411;&#30340;&#36755;&#20837;&#21253;&#25324;&#20004;&#31181;&#27169;&#24577;&#65292;&#21363;&#36890;&#36807;&#21487;&#23398;&#20064;wav2vec&#26041;&#27861;&#22788;&#29702;&#30340;&#38899;&#39057;&#25968;&#25454;&#21644;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#26469;&#33258;&#21464;&#21387;&#22120;&#65288;BERT&#65289;&#27169;&#22411;&#34920;&#31034;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#38899;&#39057;&#21644;&#25991;&#26412;&#34920;&#31034;&#20351;&#29992;&#19968;&#32452;&#21452;&#21521;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#23618;&#36827;&#34892;&#22788;&#29702;&#65292;&#20351;&#29992;&#33258;&#27880;&#24847;&#23558;&#32473;&#23450;&#23545;&#35805;&#20013;&#30340;&#27599;&#20010;&#35805;&#35821;&#36716;&#25442;&#20026;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#25972;&#21512;&#19978;&#19979;&#25991;&#30693;&#35782;&#21644;&#20004;&#31181;&#27169;&#24577;&#30340;&#20449;&#24687;&#65292;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#23618;&#23558;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#32452;&#21512;&#65292;&#35797;&#22270;&#34913;&#37327;&#19982;&#24773;&#24863;&#35782;&#21035;&#20219;&#21153;&#30456;&#20851;&#30340;&#35805;&#35821;&#32423;&#23884;&#20837;&#12290;&#22312;CMU-MOSI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;HCAM&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#24773;&#24863;&#35782;&#21035;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion recognition in conversations is challenging due to the multi-modal nature of the emotion expression. We propose a hierarchical cross-attention model (HCAM) approach to multi-modal emotion recognition using a combination of recurrent and co-attention neural network models. The input to the model consists of two modalities, i) audio data, processed through a learnable wav2vec approach and, ii) text data represented using a bidirectional encoder representations from transformers (BERT) model. The audio and text representations are processed using a set of bi-directional recurrent neural network layers with self-attention that converts each utterance in a given conversation to a fixed dimensional embedding. In order to incorporate contextual knowledge and the information across the two modalities, the audio and text embeddings are combined using a co-attention layer that attempts to weigh the utterance level embeddings relevant to the task of emotion recognition. The neural network
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.06875</link><description>&lt;p&gt;
&#19981;&#38656;&#37325;&#26032;&#25628;&#32034;&#30340;&#30740;&#31350;&#65306;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#21487;&#31934;&#30830;&#39044;&#27979;&#36328;&#23610;&#24230;&#30340;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales. (arXiv:2304.06875v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;muP&#65292;&#21487;&#20197;&#25552;&#39640;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#30340;&#25311;&#21512;&#31934;&#24230;&#65292;&#20943;&#23569;&#23545;&#22823;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#25628;&#32034;&#65292;&#20174;&#32780;&#23454;&#29616;&#22312;&#22823;&#35268;&#27169;&#27169;&#22411;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#22823;&#65292;&#39564;&#35777;&#30740;&#31350;&#24819;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#26114;&#36149;&#65292;&#22240;&#20026;&#23567;&#27169;&#22411;&#30340;&#32467;&#35770;&#19981;&#33021;&#31616;&#21333;&#22320;&#36716;&#31227;&#21040;&#22823;&#27169;&#22411;&#12290;&#35299;&#20915;&#26041;&#26696;&#26159;&#24314;&#31435;&#19968;&#20010;&#36890;&#29992;&#31995;&#32479;&#65292;&#20165;&#22522;&#20110;&#23567;&#27169;&#22411;&#30340;&#32467;&#26524;&#21644;&#36229;&#21442;&#25968;&#30452;&#25509;&#39044;&#27979;&#22823;&#27169;&#22411;&#30340;&#19968;&#20123;&#25351;&#26631;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#32553;&#25918;&#24459;&#30340;&#26041;&#27861;&#38656;&#35201;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#19978;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#26377;&#38480;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#26368;&#22823;&#26356;&#26032;&#21442;&#25968;&#21270;&#65288;muP&#65289;&#20351;&#24471;&#21487;&#20197;&#22312;&#38752;&#36817;&#24120;&#35265;&#25439;&#22833;&#27969;&#22495;&#30340;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#25311;&#21512;&#36229;&#21442;&#25968;&#30340;&#32553;&#25918;&#24459;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#25628;&#32034;&#12290;&#22240;&#27492;&#65292;&#19981;&#21516;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#22823;&#23610;&#24230;&#19978;&#36827;&#34892;&#25439;&#22833;&#39044;&#27979;&#65292;&#22312;&#35757;&#32451;&#24320;&#22987;&#20043;&#21069;&#23601;&#21487;&#20197;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#20316;&#20026;&#21487;&#38752;&#30340;&#23398;&#26415;&#30740;&#31350;&#30340;&#31532;&#19968;&#27493;&#65292;&#36866;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#19981;&#38656;&#22823;&#37327;&#30340;&#35745;&#31639;&#12290;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
As language models scale up, it becomes increasingly expensive to verify research ideas because conclusions on small models do not trivially transfer to large ones. A possible solution is to establish a generic system that directly predicts some metrics for large models solely based on the results and hyperparameters from small models. Existing methods based on scaling laws require hyperparameter search on the largest models, which is impractical with limited resources. We address this issue by presenting our discoveries indicating that Maximal Update parametrization (muP) enables accurate fitting of scaling laws for hyperparameters close to common loss basins, without any search. Thus, different models can be directly compared on large scales with loss prediction even before the training starts. We propose a new paradigm as a first step towards reliable academic research for any model scale without heavy computation. Code will be publicly available shortly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20004;&#20010;&#20559;&#35265;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38543;&#30528;&#25216;&#26415;&#36827;&#27493;&#65292;&#26368;&#26032;&#30340;&#12289;&#26356;&#24555;&#12289;&#26356;&#36731;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#36127;&#36131;&#20219;&#22320;&#38477;&#20302;&#20102;&#19982;&#26087;&#27169;&#22411;&#30456;&#27604;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2304.06861</link><description>&lt;p&gt;
&#23545;&#26368;&#26032;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Social Biases in Recent Large Pre-Trained Models. (arXiv:2304.06861v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#36817;&#21457;&#24067;&#30340;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20004;&#20010;&#20559;&#35265;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#25506;&#35752;&#20102;&#26159;&#21542;&#38543;&#30528;&#25216;&#26415;&#36827;&#27493;&#65292;&#26368;&#26032;&#30340;&#12289;&#26356;&#24555;&#12289;&#26356;&#36731;&#30340;&#27169;&#22411;&#22312;&#24320;&#21457;&#26102;&#36127;&#36131;&#20219;&#22320;&#38477;&#20302;&#20102;&#19982;&#26087;&#27169;&#22411;&#30456;&#27604;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24191;&#27867;&#24212;&#29992;&#22312;&#31038;&#21306;&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#26469;&#33258;&#20110;&#20114;&#32852;&#32593;&#31561;&#24320;&#25918;&#26469;&#28304;&#30340;&#26410;&#23457;&#26680;&#25110;&#26410;&#31579;&#36873;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30001;&#20110;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#22312;&#22312;&#32447;&#24179;&#21488;&#19978;&#30475;&#21040;&#30340;&#20559;&#35265;&#21453;&#26144;&#20102;&#31038;&#20250;&#19978;&#30340;&#20559;&#35265;&#65292;&#24182;&#34987;&#36825;&#20123;&#27169;&#22411;&#25152;&#25429;&#25417;&#21644;&#23398;&#20064;&#12290;&#36825;&#20123;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23427;&#20204;&#20869;&#22312;&#30340;&#20559;&#35265;&#23545;&#20110;&#23450;&#21521;&#30340;&#31038;&#20250;&#32676;&#20307;&#26159;&#26377;&#23475;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#24067;&#21518;&#30340;&#20559;&#35265;&#32553;&#20943;&#36235;&#21183;&#12290;&#36873;&#25321;&#20102;&#19977;&#20010;&#26368;&#26032;&#27169;&#22411;(ELECTRA&#12289;DeBERTa&#21644;DistilBERT)&#65292;&#24182;&#23545;&#20004;&#20010;&#20559;&#35265;&#22522;&#20934;&#65288;StereoSet&#21644;CrowS-Pairs&#65289;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#20204;&#20351;&#29992;&#30456;&#20851;&#24230;&#37327;&#26631;&#20934;&#19982;BERT&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25506;&#32034;&#26159;&#21542;&#38543;&#30528;&#25216;&#26415;&#30340;&#36827;&#27493;&#21644;&#26032;&#30340;&#12289;&#26356;&#24555;&#12289;&#26356;&#36731;&#30340;&#27169;&#22411;&#21457;&#24067;&#65292;&#23427;&#20204;&#26159;&#21542;&#36127;&#36131;&#20219;&#22320;&#21457;&#23637;&#65292;&#20351;&#20854;&#20869;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#19982;&#26087;&#27169;&#22411;&#30456;&#27604;&#26377;&#25152;&#38477;&#20302;&#65311;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models are widely used in the community. These models are usually trained on unmoderated and unfiltered data from open sources like the Internet. Due to this, biases that we see in platforms online which are a reflection of those in society are in turn captured and learned by these models. These models are deployed in applications that affect millions of people and their inherent biases are harmful to the targeted social groups. In this work, we study the general trend in bias reduction as newer pre-trained models are released. Three recent models ( ELECTRA, DeBERTa, and DistilBERT) are chosen and evaluated against two bias benchmarks, StereoSet and CrowS-Pairs. They are compared to the baseline of BERT using the associated metrics. We explore whether as advancements are made and newer, faster, lighter models are released: are they being developed responsibly such that their inherent social biases have been reduced compared to their older counterparts? The re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06858</link><description>&lt;p&gt;
Vax-Culture: &#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#30123;&#33495;&#35752;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#30123;&#33495;&#29369;&#35947;&#32487;&#32493;&#26159;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#35813;&#29369;&#35947;&#30772;&#22351;&#20102;&#30123;&#33495;&#36816;&#21160;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#30830;&#23450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21453;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#38271;&#26159;&#35813;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#25512;&#29305;&#20316;&#20026;&#35823;&#23548;&#20869;&#23481;&#30340;&#26469;&#28304;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#19982;&#30123;&#33495;&#26377;&#20851;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#20511;&#21161;&#19987;&#19994;&#27807;&#36890;&#21644;&#26032;&#38395;&#32972;&#26223;&#30340;&#27880;&#37322;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#26368;&#32456;&#24076;&#26395;&#36825;&#21487;&#20197;&#24102;&#26469;&#26377;&#25928;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#25509;&#35302;&#37027;&#20123;&#25345;&#21453;&#30123;&#33495;&#20449;&#20208;&#32773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
&lt;/p&gt;</description></item><item><title>SemEval-2023&#20030;&#21150;&#20102;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#25361;&#25112;&#36187;&#65288;AfriSenti-SemEval&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25361;&#25112;&#36187;&#21253;&#25324;&#21333;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#21442;&#19982;&#12290;</title><link>http://arxiv.org/abs/2304.06845</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;12: &#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65288;AfriSenti-SemEval&#65289;
&lt;/p&gt;
&lt;p&gt;
SemEval-2023 Task 12: Sentiment Analysis for African Languages (AfriSenti-SemEval). (arXiv:2304.06845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06845
&lt;/p&gt;
&lt;p&gt;
SemEval-2023&#20030;&#21150;&#20102;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#25361;&#25112;&#36187;&#65288;AfriSenti-SemEval&#65289;&#65292;&#26088;&#22312;&#25552;&#20379;&#38750;&#27954;&#35821;&#35328;&#30340;&#24773;&#24863;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#35813;&#25361;&#25112;&#36187;&#21253;&#25324;&#21333;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#38646;&#26679;&#26412;&#20998;&#31867;&#19977;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#21560;&#24341;&#20102;&#20247;&#22810;&#30740;&#31350;&#32773;&#30340;&#21442;&#19982;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#38750;&#27954;&#35821;&#26448;&#26009;&#30340;SemEval&#25361;&#25112;&#36187;&#8212;&#8212;&#38750;&#27954;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65288;AfriSenti-SemEval&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;14&#31181;&#38750;&#27954;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#38463;&#25289;&#20271;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#20234;&#21338;&#35821;&#12289;&#21346;&#26106;&#36798;&#35821;&#12289;&#25705;&#27931;&#21733;&#38463;&#25289;&#20271;&#35821;&#12289;&#33707;&#26705;&#27604;&#20811;&#33889;&#33796;&#29273;&#35821;&#12289;&#23612;&#26085;&#21033;&#20122;&#30382;&#38054;&#35821;&#12289;&#22885;&#32599;&#33707;&#35821;&#12289;&#26031;&#29926;&#24076;&#37324;&#35821;&#12289;&#25552;&#26684;&#37324;&#23612;&#20122;&#35821;&#12289;&#29305;&#23041;&#35821;&#12289;&#20811;&#26862;&#35821;&#21644;&#32422;&#40065;&#24052;&#35821;&#65289;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19977;&#20010;&#23376;&#20219;&#21153;&#65306;&#65288;1&#65289;&#21333;&#35821;&#20998;&#31867;&#65292;&#20849;&#25910;&#21040;44&#20010;&#25552;&#20132;&#32467;&#26524;&#65307;&#65288;2&#65289;&#22810;&#35821;&#35328;&#20998;&#31867;&#65292;&#20849;&#25910;&#21040;32&#20010;&#25552;&#20132;&#32467;&#26524;&#65307;&#65288;3&#65289;&#38646;&#26679;&#26412;&#20998;&#31867;&#65292;&#20849;&#25910;&#21040;34&#20010;&#25552;&#20132;&#32467;&#26524;&#12290;&#20854;&#20013;&#65292;NLNDE&#22242;&#38431;&#22312;&#20219;&#21153;A&#21644;B&#20013;&#20998;&#21035;&#33719;&#24471;&#20102;71.31&#21644;75.06&#21152;&#26435;F1&#20998;&#25968;&#30340;&#26368;&#20339;&#31995;&#32479;&#12290;UCAS-IIE-NLP&#22312;&#20219;&#21153;C&#19978;&#24179;&#22343;&#33719;&#24471;&#20102;58.15&#21152;&#26435;F1&#20998;&#25968;&#30340;&#26368;&#20339;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the first Africentric SemEval Shared task, Sentiment Analysis for African Languages (AfriSenti-SemEval) - the dataset is available at https://github.com/afrisenti-semeval/afrisent-semeval-2023. AfriSenti-SemEval is a sentiment classification challenge in 14 African languages - Amharic, Algerian Arabic, Hausa, Igbo, Kinyarwanda, Moroccan Arabic, Mozambican Portuguese, Nigerian Pidgin, Oromo, Swahili, Tigrinya, Twi, Xitsonga, and Yor\`ub\'a (Muhammad et al., 2023), using a 3-class labeled data: positive, negative, and neutral. We present three subtasks: (1) Task A: monolingual classification, which received 44 submissions; (2) Task B: multilingual classification, which received 32 submissions; and (3) Task C: zero-shot classification, which received 34 submissions. The best system for tasks A and B was achieved by NLNDE team with 71.31 and 75.06 weighted F1, respectively. UCAS-IIE-NLP achieved the best system on average for task C with 58.15 weighted F1. We describe the variou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;AI&#20013;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;FMs&#22312;&#22320;&#29702;&#23376;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#20294;&#22312;&#21457;&#23637;&#20013;&#20063;&#38754;&#20020;&#30528;&#32570;&#23569;&#25968;&#25454;&#38598;&#21644;&#38656;&#35201;&#19987;&#19994;&#25216;&#26415;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.06798</link><description>&lt;p&gt;
&#35770;&#22522;&#30784;&#27169;&#22411;&#22312;&#22320;&#29702;&#31354;&#38388;AI&#20013;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence. (arXiv:2304.06798v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22320;&#29702;&#31354;&#38388;AI&#20013;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#65292;&#27979;&#35797;&#20102;&#22810;&#31181;FMs&#22312;&#22320;&#29702;&#23376;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#23450;&#21046;&#27169;&#22411;&#65292;&#20294;&#22312;&#21457;&#23637;&#20013;&#20063;&#38754;&#20020;&#30528;&#32570;&#23569;&#25968;&#25454;&#38598;&#21644;&#38656;&#35201;&#19987;&#19994;&#25216;&#26415;&#24494;&#35843;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#26159;&#25351;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#20197;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#23569;&#26679;&#26412;&#29978;&#33267;&#38646;&#26679;&#26412;&#23398;&#20064;&#36866;&#29992;&#20110;&#24191;&#27867;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22823;&#33719;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#23578;&#26410;&#35265;&#21040;&#20026;&#22320;&#29702;&#31354;&#38388;&#20154;&#24037;&#26234;&#33021;&#65288;GeoAI&#65289;&#24320;&#21457;&#22522;&#30784;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;&#26412;&#25991;&#25506;&#35752;&#24320;&#21457;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#20197;&#24212;&#23545;GeoAI&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#22810;&#20010;&#22320;&#29702;&#31354;&#38388;&#23376;&#22495;&#20013;&#36827;&#34892;&#19971;&#39033;&#20219;&#21153;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#22320;&#29702;&#35821;&#20041;&#12289;&#20581;&#24247;&#22320;&#29702;&#23398;&#12289;&#22478;&#24066;&#22320;&#29702;&#23398;&#21644;&#36965;&#24863;&#31561;&#65292;&#30740;&#31350;&#20102;&#29616;&#26377;&#35768;&#22810;FMs&#30340;&#28508;&#21147;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20165;&#28041;&#21450;&#25991;&#26412;&#27169;&#24577;&#30340;&#19968;&#20123;&#22320;&#29702;&#31354;&#38388;&#20219;&#21153;&#65288;&#20363;&#22914;&#22320;&#21517;&#35782;&#21035;&#12289;&#20301;&#32622;&#25551;&#36848;&#35782;&#21035;&#20197;&#21450;&#32654;&#22269;&#24030;&#32423;/&#21439;&#32423;&#30196;&#21574;&#30151;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65289;&#20013;&#65292;&#36825;&#20123;&#20219;&#21153;&#26080;&#20851;&#30340;LLM&#20063;&#21487;&#20197;&#32988;&#20219;&#20219;&#21153;&#29305;&#23450;&#30340;&#23436;&#20840;&#23450;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20026;&#22320;&#29702;&#31354;&#38388;AI&#24320;&#21457;FMs&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#32570;&#20047;&#22823;&#35268;&#27169;&#22320;&#29702;&#31354;&#38388;&#25968;&#25454;&#38598;&#21644;&#38656;&#35201;&#19987;&#38376;&#30340;&#22320;&#29702;&#31354;&#38388;&#24494;&#35843;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;FMs&#30340;&#28508;&#22312;&#30740;&#31350;&#26041;&#21521;&#21644;&#24212;&#29992;&#65292;&#20197;&#24800;&#21450;&#22320;&#29702;&#31354;&#38388;AI&#31038;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained models, also known as foundation models (FMs), are trained in a task-agnostic manner on large-scale data and can be adapted to a wide range of downstream tasks by fine-tuning, few-shot, or even zero-shot learning. Despite their successes in language and vision tasks, we have yet seen an attempt to develop foundation models for geospatial artificial intelligence (GeoAI). In this work, we explore the promises and challenges of developing multimodal foundation models for GeoAI. We first investigate the potential of many existing FMs by testing their performances on seven tasks across multiple geospatial subdomains including Geospatial Semantics, Health Geography, Urban Geography, and Remote Sensing. Our results indicate that on several geospatial tasks that only involve text modality such as toponym recognition, location description recognition, and US state-level/county-level dementia time series forecasting, these task-agnostic LLMs can outperform task-specific fully-s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.06795</link><description>&lt;p&gt;
Token-and-Duration Transducer&#26550;&#26500;&#65306;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#19982;&#26102;&#38271;&#30340;&#39640;&#25928;&#24207;&#21015;&#36716;&#23548;
&lt;/p&gt;
&lt;p&gt;
Efficient Sequence Transduction by Jointly Predicting Tokens and Durations. (arXiv:2304.06795v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#24207;&#21015;&#36716;&#23548;&#26550;&#26500;TDT&#65292;&#23427;&#21487;&#20197;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#20174;&#32780;&#23454;&#29616;&#27604;&#20256;&#32479;Transducers&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#30340;&#26032;&#22411;Token-and-Duration Transducer(TDT)&#26550;&#26500;&#12290;TDT&#36890;&#36807;&#32852;&#21512;&#39044;&#27979;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#65292;&#21363;&#21457;&#23556;&#30340;&#26631;&#35760;&#35206;&#30422;&#30340;&#36755;&#20837;&#24103;&#30340;&#25968;&#37327;&#65292;&#26469;&#25193;&#23637;&#20256;&#32479;&#30340;RNN-Transducer&#26550;&#26500;&#12290;&#23427;&#20351;&#29992;&#20855;&#26377;&#20004;&#20010;&#29420;&#31435;&#26631;&#20934;&#21270;&#36755;&#20986;&#30340;&#32852;&#21512;&#32593;&#32476;&#26469;&#29983;&#25104;&#26631;&#35760;&#21644;&#25345;&#32493;&#26102;&#38388;&#30340;&#20998;&#24067;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;TDT&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#39044;&#27979;&#30340;&#25345;&#32493;&#26102;&#38388;&#36755;&#20986;&#36339;&#36807;&#36755;&#20837;&#24103;&#65292;&#20351;&#20854;&#27604;&#36880;&#24103;&#22788;&#29702;&#32534;&#30721;&#22120;&#36755;&#20986;&#30340;&#20256;&#32479;Transducers&#26174;&#30528;&#26356;&#24555;&#12290;&#22312;&#19981;&#21516;&#30340;&#24207;&#21015;&#36716;&#23548;&#20219;&#21153;&#19978;&#65292;TDT&#27169;&#22411;&#22343;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#26174;&#30528;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;&#35821;&#38899;&#35782;&#21035;&#30340;TDT&#27169;&#22411;&#27604;RNN-Transducers&#33719;&#24471;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#25512;&#29702;&#36895;&#24230;&#39640;&#36798;2.82&#20493;&#12290;&#35821;&#38899;&#32763;&#35793;&#30340;TDT&#27169;&#22411;&#19982;MUST-C&#27979;&#35797;&#30456;&#27604;&#25552;&#39640;&#20102;1&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than RNN-Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared wi
&lt;/p&gt;</description></item><item><title>RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06767</link><description>&lt;p&gt;
RAFT: &#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#29992;&#20110;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment. (arXiv:2304.06767v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06767
&lt;/p&gt;
&lt;p&gt;
RAFT&#26694;&#26550;&#24341;&#20837;&#20102;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#40784;&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#24102;&#26469;&#30340;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#22411;&#22522;&#30784;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#24191;&#27867;&#30340;&#26080;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#24102;&#26469;&#30340;&#38544;&#24335;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#23376;&#20248;&#26679;&#26412;&#12289;&#25197;&#26354;&#30340;&#32467;&#26524;&#21644;&#19981;&#20844;&#24179;&#65292;&#21487;&#33021;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#30340;&#20262;&#29702;&#21644;&#20559;&#22909;&#23545;&#40784;&#26159;&#30830;&#20445;&#23427;&#20204;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#36127;&#36131;&#20219;&#21644;&#26377;&#25928;&#30340;&#37096;&#32626;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#37319;&#29992;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288; RLHF&#65289;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312; RL &#31639;&#27861;&#30340;&#25351;&#23548;&#19979;&#65292;&#29992;&#20154;&#31867;&#21453;&#39304;&#25351;&#23548;&#30340;&#22870;&#21169;&#27169;&#22411;&#23545;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292; RL &#31639;&#27861;&#30340;&#20302;&#25928;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#24120;&#24120;&#20250;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25104;&#21151;&#23545;&#40784;&#20135;&#29983;&#37325;&#22823;&#38556;&#30861;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#21644;&#31616;&#21270;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22870;&#21169;&#25490;&#21517;&#24494;&#35843;&#65288; RAFT &#65289;&#65292;&#26088;&#22312;&#23545;&#40784;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative foundation models are susceptible to implicit biases that can arise from extensive unsupervised training data. Such biases can produce suboptimal samples, skewed outcomes, and unfairness, with potentially significant repercussions. Consequently, aligning these models with human ethics and preferences is an essential step toward ensuring their responsible and effective deployment in real-world applications. Prior research has primarily employed Reinforcement Learning from Human Feedback (RLHF) as a means of addressing this problem, wherein generative models are fine-tuned using RL algorithms guided by a human-feedback-informed reward model. However, the inefficiencies and instabilities associated with RL algorithms frequently present substantial obstacles to the successful alignment of generative models, necessitating the development of a more robust and streamlined approach. To this end, we introduce a new framework, Reward rAnked FineTuning (RAFT), designed to align generat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.06671</link><description>&lt;p&gt;
&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;&#21644;&#36845;&#20195;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation. (arXiv:2304.06671v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24067;&#23616;&#24341;&#23548;&#19979;&#22270;&#20687;&#29983;&#25104;&#30340;&#35786;&#26029;&#22522;&#20934;LayoutBench&#65292;&#23545;&#25968;&#37327;&#12289;&#20301;&#32622;&#12289;&#22823;&#23567;&#21644;&#24418;&#29366;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#22312;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#21487;&#33021;&#19981;&#20855;&#26377;&#33391;&#22909;&#30340;&#25512;&#24191;&#24615;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#26174;&#29616;&#20986;&#22312;OOD&#24067;&#23616;&#26041;&#38754;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#25511;&#21046;&#26159;&#21487;&#25511;&#22270;&#20687;&#29983;&#25104;&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;&#22312;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20855;&#26377;&#31867;&#20284;&#31354;&#38388;&#37197;&#32622;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#25968;&#25454;&#38598;&#19978;&#26377;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#20219;&#24847;&#19981;&#30830;&#23450;&#30340;&#24067;&#23616;&#30340;&#31163;&#32447;&#20998;&#24067;&#26679;&#26412;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#34920;&#29616;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutBench&#65292;&#36825;&#26159;&#19968;&#31181;&#23545;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#36827;&#34892;&#35786;&#26029;&#30340;&#22522;&#20934;&#65292;&#23427;&#26816;&#26597;&#20102;&#22235;&#31181;&#31354;&#38388;&#25511;&#21046;&#25216;&#33021;&#65306;&#25968;&#37327;&#65292;&#20301;&#32622;&#65292;&#22823;&#23567;&#21644;&#24418;&#29366;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#26368;&#36817;&#20195;&#34920;&#24615;&#30340;&#24067;&#23616;&#24341;&#23548;&#19979;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#35266;&#23519;&#21040;&#33391;&#22909;&#30340;ID&#24067;&#23616;&#25511;&#21046;&#21487;&#33021;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#20219;&#24847;&#24067;&#23616;&#30340;&#37326;&#22806;&#29615;&#22659;&#65288;&#20363;&#22914;&#65292;&#36793;&#30028;&#19978;&#30340;&#23545;&#35937;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;IterInpaint&#65292;&#23427;&#36890;&#36807;&#20462;&#22797;&#36880;&#27493;&#29983;&#25104;&#21069;&#26223;&#21644;&#32972;&#26223;&#21306;&#22495;&#65292;&#23637;&#31034;&#20986;&#22312;LayoutBench&#30340;OOD&#24067;&#23616;&#19978;&#26356;&#24378;&#30340;&#36890;&#29992;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#34920;&#26126;IterInpaint&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#22810;&#26679;&#21644;&#35270;&#35273;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#22270;&#20687;&#21644;&#21487;&#25511;&#30340;&#31354;&#38388;&#24067;&#23616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial control is a core capability in controllable image generation. Advancements in layout-guided image generation have shown promising results on in-distribution (ID) datasets with similar spatial configurations. However, it is unclear how these models perform when facing out-of-distribution (OOD) samples with arbitrary, unseen layouts. In this paper, we propose LayoutBench, a diagnostic benchmark for layout-guided image generation that examines four categories of spatial control skills: number, position, size, and shape. We benchmark two recent representative layout-guided image generation methods and observe that the good ID layout control may not generalize well to arbitrary layouts in the wild (e.g., objects at the boundary). Next, we propose IterInpaint, a new baseline that generates foreground and background regions in a step-by-step manner via inpainting, demonstrating stronger generalizability than existing models on OOD layouts in LayoutBench. We perform quantitative and q
&lt;/p&gt;</description></item><item><title>G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.06653</link><description>&lt;p&gt;
G2T: &#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
G2T: A simple but versatile framework for topic modeling based on pretrained language model and community detection. (arXiv:2304.06653v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06653
&lt;/p&gt;
&lt;p&gt;
G2T&#26159;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#31038;&#21306;&#26816;&#27979;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#33258;&#21160;&#35780;&#20272;&#34920;&#26126;&#65292;G2T&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#22343;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#32858;&#31867;&#30340;&#20027;&#39064;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36866;&#24403;&#30340;&#35789;&#35821;&#31579;&#36873;&#26041;&#27861;&#32858;&#31867;&#39640;&#36136;&#37327;&#30340;&#21477;&#23376;&#23884;&#20837;&#65292;&#29983;&#25104;&#27604;&#29983;&#25104;&#24335;&#27010;&#29575;&#20027;&#39064;&#27169;&#22411;&#26356;&#22909;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#36873;&#25321;&#21512;&#36866;&#21442;&#25968;&#30340;&#22256;&#38590;&#20197;&#21450;&#19981;&#23436;&#25972;&#30340;&#27169;&#22411;&#24573;&#30053;&#21333;&#35789;&#19982;&#20027;&#39064;&#21450;&#20027;&#39064;&#19982;&#25991;&#26412;&#20043;&#38388;&#30340;&#23450;&#37327;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#27905;&#20294;&#26377;&#25928;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#22270;&#20027;&#39064;&#65288;G2T&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been reported that clustering-based topic models, which cluster high-quality sentence embeddings with an appropriate word selection method, can generate better topics than generative probabilistic topic models. However, these approaches suffer from the inability to select appropriate parameters and incomplete models that overlook the quantitative relation between words with topics and topics with text. To solve these issues, we propose graph to topic (G2T), a simple but effective framework for topic modelling. The framework is composed of four modules. First, document representation is acquired using pretrained language models. Second, a semantic graph is constructed according to the similarity between document representations. Third, communities in document semantic graphs are identified, and the relationship between topics and documents is quantified accordingly. Fourth, the word--topic distribution is computed based on a variant of TFIDF. Automatic evaluation suggests that G2
&lt;/p&gt;</description></item><item><title>&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#65292;&#20294;&#20854;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06623</link><description>&lt;p&gt;
&#25506;&#32034;&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#29616;&#29366;
&lt;/p&gt;
&lt;p&gt;
Exploring the State of the Art in Legal QA Systems. (arXiv:2304.06623v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06623
&lt;/p&gt;
&lt;p&gt;
&#27861;&#24459;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#30340;&#30740;&#31350;&#38754;&#20020;&#30528;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#31561;&#25361;&#25112;&#65292;&#20294;&#20854;&#22312;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#31561;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#31572;&#19982;&#27861;&#24459;&#39046;&#22495;&#30456;&#20851;&#30340;&#38382;&#39064;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22797;&#26434;&#30340;&#27861;&#24459;&#25991;&#26723;&#31995;&#32479;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#20026;&#27861;&#24459;&#38382;&#39064;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#36890;&#24120;&#38656;&#35201;&#30456;&#20851;&#39046;&#22495;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#20351;&#24471;&#21363;&#20351;&#23545;&#20110;&#20154;&#31867;&#19987;&#23478;&#26469;&#35828;&#65292;&#36825;&#39033;&#20219;&#21153;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#26088;&#22312;&#29983;&#25104;&#23545;&#20197;&#20154;&#31867;&#35821;&#35328;&#25552;&#20986;&#30340;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#23427;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26469;&#29702;&#35299;&#38382;&#39064;&#24182;&#25628;&#32034;&#20449;&#24687;&#20197;&#25214;&#21040;&#30456;&#20851;&#31572;&#26696;&#12290;QA&#20855;&#26377;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#65292;&#21253;&#25324;&#23458;&#25143;&#26381;&#21153;&#12289;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#36328;&#35821;&#35328;&#20132;&#27969;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#35832;&#22914;&#25913;&#36827;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#22788;&#29702;&#22797;&#26434;&#21644;&#27169;&#31946;&#38382;&#39064;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query typically necessitates specialized knowledge in the relevant domain, which makes this task all the more challenging, even for human experts. QA (Question answering systems) are designed to generate answers to questions asked in human languages. They use natural language processing to understand questions and search through information to find relevant answers. QA has various practical applications, including customer service, education, research, and cross-lingual communication. However, they face challenges such as improving natural language understanding and handling complex and ambiguous questions. Answering questions related to the legal domain is a complex task, primarily due to the intricate nature and diverse range of legal document systems. Providing an accurate answer to a legal query
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2304.06447</link><description>&lt;p&gt;
PDF-VQA: &#19968;&#20010;&#26032;&#30340;&#29992;&#20110;PDF&#25991;&#20214;&#30495;&#23454;&#19990;&#30028;VQA&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PDF-VQA: A New Dataset for Real-World VQA on PDF Documents. (arXiv:2304.06447v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06447
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25991;&#26723;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20197;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#20316;&#20026;&#30740;&#31350;&#23545;&#35937;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#19982;&#22788;&#29702;&#25991;&#26723;&#20803;&#32032;&#12289;&#32467;&#26500;&#21644;&#20869;&#23481;&#31561;&#26041;&#38754;&#65292;&#20026;&#35299;&#20915;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26723;&#30340;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#30740;&#31350;&#25991;&#26723;&#22270;&#20687;&#30340;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#25991;&#26723;&#30340;VQA&#25968;&#25454;&#38598;PDF-VQA&#65292;&#20174;&#25991;&#26723;&#20803;&#32032;&#35782;&#21035;&#12289;&#25991;&#26723;&#24067;&#23616;&#32467;&#26500;&#29702;&#35299;&#20197;&#21450;&#19978;&#19979;&#25991;&#29702;&#35299;&#21644;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#31561;&#21508;&#20010;&#26041;&#38754;&#20840;&#38754;&#25506;&#35752;&#25991;&#26723;&#29702;&#35299;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;PDF-VQA&#25968;&#25454;&#38598;&#23558;&#25991;&#26723;&#29702;&#35299;&#30340;&#35268;&#27169;&#20174;&#21333;&#20010;&#25991;&#26723;&#39029;&#38754;&#25193;&#23637;&#21040;&#35810;&#38382;&#22810;&#20010;&#39029;&#38754;&#30340;&#23436;&#25972;&#25991;&#26723;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;VQA&#27169;&#22411;&#65292;&#26126;&#30830;&#22320;&#38598;&#25104;&#20102;&#19981;&#21516;&#25991;&#26723;&#20803;&#32032;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#23618;&#27425;&#32467;&#26500;&#20851;&#31995;&#65292;&#20197;&#25552;&#39640;&#25991;&#26723;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#12290;&#35813;&#24615;&#33021;&#19982;&#22810;&#20010;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#36739;&#65292;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#38382;&#39064;&#31867;&#22411;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document-based Visual Question Answering examines the document understanding of document images in conditions of natural language questions. We proposed a new document-based VQA dataset, PDF-VQA, to comprehensively examine the document understanding from various aspects, including document element recognition, document layout structural understanding as well as contextual understanding and key information extraction. Our PDF-VQA dataset extends the current scale of document understanding that limits on the single document page to the new scale that asks questions over the full document of multiple pages. We also propose a new graph-based VQA model that explicitly integrates the spatial and hierarchically structural relationships between different document elements to boost the document structural understanding. The performances are compared with several baselines over different question types and tasks\footnote{The full dataset will be released after paper acceptance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;I3D&#35270;&#39057;&#29305;&#24449;&#22521;&#35757;Transformer&#27169;&#22411;&#65292;&#25454;&#27492;&#23545;How2Sign&#25968;&#25454;&#38598;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20844;&#20849;&#20195;&#30721;&#21644;&#39318;&#20010;&#24320;&#28304;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.06371</link><description>&lt;p&gt;
&#20174;&#25351;&#23548;&#35270;&#39057;&#20013;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Sign Language Translation from Instructional Videos. (arXiv:2304.06371v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;I3D&#35270;&#39057;&#29305;&#24449;&#22521;&#35757;Transformer&#27169;&#22411;&#65292;&#25454;&#27492;&#23545;How2Sign&#25968;&#25454;&#38598;&#36827;&#34892;&#25163;&#35821;&#32763;&#35793;&#12290;&#20316;&#32773;&#25552;&#20379;&#20102;&#20844;&#20849;&#20195;&#30721;&#21644;&#39318;&#20010;&#24320;&#28304;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#25163;&#35821;&#32763;&#35793;&#65288;SLT&#65289;&#21040;&#21475;&#35821;&#35821;&#35328;&#30340;&#36827;&#23637;&#22823;&#37096;&#20998;&#37117;&#26159;&#22522;&#20110;&#35268;&#27169;&#26377;&#38480;&#12289;&#39046;&#22495;&#21463;&#38480;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#25552;&#20379;&#39318;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;"How2Sign"&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#25512;&#21160;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#25105;&#20204;&#20351;&#29992;I3D&#35270;&#39057;&#29305;&#24449;&#23545;Transformer&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#38477;&#20302;&#30340;BLEU&#20998;&#25968;&#20316;&#20026;&#39564;&#35777;&#30340;&#21442;&#32771;&#25351;&#26631;&#20195;&#26367;&#24191;&#27867;&#20351;&#29992;&#30340;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;8.03&#30340;BLEU&#20998;&#25968;&#65292;&#24182;&#21457;&#24067;&#20102;&#39318;&#20010;&#24320;&#28304;&#23454;&#29616;&#65292;&#20197;&#25512;&#21160;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advances in automatic sign language translation (SLT) to spoken languages have been mostly benchmarked with datasets of limited size and restricted domains. Our work advances the state of the art by providing the first baseline results on How2Sign, a large and broad dataset.  We train a Transformer over I3D video features, using the reduced BLEU as a reference metric for validation, instead of the widely used BLEU score. We report a result of 8.03 on the BLEU score, and publish the first open-source implementation of its kind to promote further advances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.04933</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#36741;&#23548;&#21592;&#22312;&#25968;&#23398;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#25903;&#25345;&#20102;&#20302;&#25104;&#32489;&#23398;&#29983;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Tutor Better Supported Lower Performers in a Math Task. (arXiv:2304.04933v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21487;&#29992;&#20110;&#25552;&#20379;&#33258;&#36866;&#24212;&#30340;&#25945;&#32946;&#25903;&#25345;&#65292;&#23588;&#20854;&#23545;&#20110;&#26368;&#21021;&#25104;&#32489;&#36739;&#20302;&#30340;&#23398;&#29983;&#20855;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36164;&#28304;&#38480;&#21046;&#20351;&#24471;&#20026;&#25152;&#26377;&#23398;&#29983;&#25552;&#20379;&#20010;&#24615;&#21270;&#25945;&#23398;&#21464;&#24471;&#22256;&#38590;&#12290;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#25104;&#20026;&#20943;&#23569;&#21457;&#23637;&#25104;&#26412;&#12289;&#25552;&#39640;&#26234;&#33021;&#36741;&#23548;&#36719;&#20214;&#25928;&#26524;&#30340;&#20851;&#38190;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#23398;&#29983;&#25552;&#20379;&#27491;&#30830;&#30340;&#25903;&#25345;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22914;&#20309;&#22312;&#21465;&#36848;&#25925;&#20107;&#32447;&#36719;&#20214;&#20013;&#20026;&#23398;&#20064;&#8220;&#23481;&#31215;&#8221;&#27010;&#24565;&#30340;&#23398;&#29983;&#25552;&#20379;&#33258;&#36866;&#24212;&#25945;&#32946;&#25903;&#25345;&#12290;&#36890;&#36807;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#65292;&#25105;&#20204;&#20063;&#25552;&#21462;&#20102;&#26377;&#20851;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#27934;&#35265;&#65292;&#35777;&#26126;&#20102;&#25152;&#24471;&#25919;&#31574;&#22312;&#19981;&#21516;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#20855;&#26377;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#36825;&#20004;&#39033;&#30740;&#31350;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#25925;&#20107;&#31995;&#32479;&#23545;&#26368;&#21021;&#30340;&#39044;&#27979;&#20998;&#25968;&#26368;&#20302;&#30340;&#23398;&#29983;&#26377;&#26368;&#22823;&#30340;&#30410;&#22788;&#65292;&#36825;&#34920;&#26126;&#20102;AI&#36866;&#24212;&#24182;&#20026;&#20302;&#25104;&#32489;&#23398;&#29983;&#25552;&#20379;&#25903;&#25345;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Resource limitations make it hard to provide all students with one of the most effective educational interventions: personalized instruction. Reinforcement learning could be a key tool to reduce the development cost and improve the effectiveness of, intelligent tutoring software that aims to provide the right support, at the right time, to a student. Here we illustrate that deep reinforcement learning can be used to provide adaptive pedagogical support to students learning about the concept of volume in a narrative storyline software. Using explainable artificial intelligence tools, we also extracted interpretable insights about the pedagogical policy learned, and we demonstrate that the resulting policy had similar performance in a different student population. Most importantly, in both studies the reinforcement-learning narrative system had the largest benefit for those students with the lowest initial pretest scores, suggesting the opportunity for AI to adapt and provide support for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30528;&#37325;&#35752;&#35770;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2304.02138</link><description>&lt;p&gt;
&#22303;&#26408;&#40550;&#40521;&#20256;&#22855;(GPT)&#65306;&#21033;&#29992;&#21450;&#26102;&#24037;&#31243;&#20811;&#26381;GPT&#24187;&#35273;&#20197;&#22312;&#23721;&#22303;&#24037;&#31243;&#20013;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Geotechnical Parrot Tales (GPT): Overcoming GPT hallucinations with prompt engineering for geotechnical applications. (arXiv:2304.02138v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30528;&#37325;&#35752;&#35770;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26222;&#21450;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#21487;&#33021;&#20250;&#24443;&#24213;&#25913;&#21464;&#21253;&#25324;&#23721;&#22303;&#24037;&#31243;&#22312;&#20869;&#30340;&#21508;&#20010;&#34892;&#19994;&#12290; &#20294;&#26159;&#65292;GPT&#27169;&#22411;&#26377;&#26102;&#20250;&#29983;&#25104;&#21548;&#36215;&#26469;&#24456;&#26377;&#36947;&#29702;&#20294;&#38169;&#35823;&#30340;&#36755;&#20986;&#65292;&#23548;&#33268;&#24187;&#35273;&#20135;&#29983;&#12290; &#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#32531;&#35299;&#36825;&#20123;&#39118;&#38505;&#21644;&#21033;&#29992;GPT&#22312;&#23721;&#22303;&#24037;&#31243;&#24212;&#29992;&#20013;&#30340;&#20840;&#37096;&#28508;&#21147;&#26041;&#38754;&#65292;&#21450;&#26102;&#24037;&#31243;&#30340;&#37325;&#35201;&#24615;&#12290; &#25105;&#20204;&#25506;&#35752;&#20102;&#19982;LLM&#30456;&#20851;&#30340;&#25361;&#25112;&#21644;&#38519;&#38449;&#65292;&#24182;&#24378;&#35843;&#20102;&#19978;&#19979;&#25991;&#22312;&#30830;&#20445;&#20934;&#30830;&#21644;&#26377;&#20215;&#20540;&#30340;&#21709;&#24212;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290; &#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#29305;&#23450;&#20110;&#19978;&#19979;&#25991;&#30340;&#25628;&#32034;&#24341;&#25806;&#30340;&#24320;&#21457;&#20197;&#21450;LLM&#25104;&#20026;&#22797;&#26434;&#20219;&#21153;&#65288;&#20363;&#22914;&#25968;&#25454;&#20998;&#26512;&#21644;&#35774;&#35745;&#65289;&#30340;&#33258;&#28982;&#30028;&#38754;&#30340;&#28508;&#21147;&#12290; &#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#33258;&#28982;&#35821;&#35328;&#25509;&#21475;&#65292;&#29992;&#20110;&#22788;&#29702;&#22797;&#26434;&#30340;&#23721;&#22303;&#24037;&#31243;&#20219;&#21153;&#21644;&#25968;&#25454;&#20998;&#26512;&#12290; &#36890;&#36807;&#23558;GPT&#38598;&#25104;&#21040;&#23721;&#22303;&#24037;&#31243;&#24037;&#20316;&#27969;&#20013;&#65292;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#31616;&#21270;&#20182;&#20204;&#30340;&#24037;&#20316;&#24182;&#21457;&#23637;&#21487;&#25345;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of large language models (LLMs), such as OpenAI's ChatGPT, could revolutionized various industries, including geotechnical engineering. However, GPT models can sometimes generate plausible-sounding but false outputs, leading to hallucinations. In this article, we discuss the importance of prompt engineering in mitigating these risks and harnessing the full potential of GPT for geotechnical applications. We explore the challenges and pitfalls associated with LLMs and highlight the role of context in ensuring accurate and valuable responses. Furthermore, we examine the development of context-specific search engines and the potential of LLMs to become a natural interface for complex tasks, such as data analysis and design. We also develop a unified interface using natural language to handle complex geotechnical engineering tasks and data analysis. By integrating GPT into geotechnical engineering workflows, professionals can streamline their work and develop sustain
&lt;/p&gt;</description></item><item><title>WebQAmGaze&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;332&#20301;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#33021;&#22815;&#21453;&#26144;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2303.17876</link><description>&lt;p&gt;
WebQAmGaze: &#19968;&#20221;&#22810;&#35821;&#35328;Webcam&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WebQAmGaze: A Multilingual Webcam Eye-Tracking-While-Reading Dataset. (arXiv:2303.17876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17876
&lt;/p&gt;
&lt;p&gt;
WebQAmGaze&#26159;&#19968;&#20010;&#22810;&#35821;&#35328;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;332&#20301;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#65292;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#33021;&#22815;&#21453;&#26144;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21019;&#24314;&#20102;WebQAmGaze&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35821;&#31181;&#20302;&#25104;&#26412;&#30340;&#38405;&#35835;&#26102;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25903;&#25345;&#20844;&#24179;&#36879;&#26126;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;WebQAmGaze&#21253;&#25324;&#20102;&#26469;&#33258;332&#20301;&#21442;&#19982;&#32773;&#38405;&#35835;&#33521;&#35821;&#12289;&#35199;&#29677;&#29273;&#35821;&#21644;&#24503;&#35821;&#25991;&#26412;&#26102;&#30340;&#32593;&#32476;&#25668;&#20687;&#22836;&#30524;&#21160;&#25968;&#25454;&#12290;&#27599;&#20010;&#21442;&#19982;&#32773;&#37117;&#20250;&#23436;&#25104;&#20004;&#20010;&#38405;&#35835;&#20219;&#21153;&#65292;&#21253;&#25324;&#20116;&#31687;&#25991;&#31456;&#30340;&#27491;&#24120;&#38405;&#35835;&#21644;&#20449;&#24687;&#23547;&#25214;&#20219;&#21153;&#12290;&#32463;&#36807;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#30456;&#20851;&#27573;&#33853;&#30340;&#27880;&#35270;&#20284;&#20046;&#24847;&#21619;&#30528;&#22238;&#31572;&#29702;&#35299;&#38382;&#39064;&#30340;&#27491;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#19982;&#39640;&#36136;&#37327;&#30340;&#30524;&#21160;&#36861;&#36394;&#25968;&#25454;&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;Webcam-ET&#33719;&#24471;&#30340;&#29305;&#24449;&#19982;&#21830;&#19994;ET&#35774;&#22791;&#30340;&#29305;&#24449;&#20043;&#38388;&#23384;&#22312;&#20013;&#31561;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20221;&#25968;&#25454;&#21487;&#20197;&#25512;&#21160;&#22522;&#20110;&#32593;&#32476;&#25668;&#20687;&#22836;&#30340;&#38405;&#35835;&#30740;&#31350;&#24182;&#24320;&#36767;&#26356;&#20415;&#23452;&#12289;&#26356;&#26131;&#33719;&#24471;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#24335;&#12290;WebQAmGaze&#23545;&#20110;&#20102;&#35299;&#38382;&#39064;&#22238;&#31572;&#30340;&#35748;&#30693;&#36807;&#31243;&#20197;&#21450;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#20844;&#24179;&#36879;&#26126;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We create WebQAmGaze, a multilingual low-cost eye-tracking-while-reading dataset, designed to support the development of fair and transparent NLP models. WebQAmGaze includes webcam eye-tracking data from 332 participants naturally reading English, Spanish, and German texts. Each participant performs two reading tasks composed of five texts, a normal reading and an information-seeking task. After preprocessing the data, we find that fixations on relevant spans seem to indicate correctness when answering the comprehension questions. Additionally, we perform a comparative analysis of the data collected to high-quality eye-tracking data. The results show a moderate correlation between the features obtained with the webcam-ET compared to those of a commercial ET device. We believe this data can advance webcam-based reading studies and open a way to cheaper and more accessible data collection. WebQAmGaze is useful to learn about the cognitive processes behind question answering (QA) and to a
&lt;/p&gt;</description></item><item><title>&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;</title><link>http://arxiv.org/abs/2303.10475</link><description>&lt;p&gt;
&#20165;&#20165;&#25552;&#31034;&#36275;&#22815;&#20102;&#21527;&#65311;&#19981;&#26159;&#30340;&#12290;&#25351;&#23548;&#23398;&#20064;&#30340;&#20840;&#38754;&#21644;&#26356;&#24191;&#38420;&#35270;&#35282;&#65288;arXiv&#65306;2303.10475v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Is Prompt All You Need? No. A Comprehensive and Broader View of Instruction Learning. (arXiv:2303.10475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10475
&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#65292;&#20294;&#36825;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#35821;&#20041;&#21487;&#20197;&#36890;&#36807;&#19968;&#32452;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#25110;&#19968;&#26465;&#25991;&#26412;&#25351;&#20196;&#26469;&#34920;&#36798;&#12290;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#22823;&#35268;&#27169;&#30340;&#20219;&#21153;&#29305;&#23450;&#31034;&#20363;&#30340;&#21487;&#29992;&#24615;&#12290;&#36825;&#24341;&#36215;&#20102;&#20004;&#20010;&#38382;&#39064;&#65306;&#39318;&#20808;&#65292;&#25910;&#38598;&#20219;&#21153;&#29305;&#23450;&#26631;&#35760;&#31034;&#20363;&#19981;&#36866;&#29992;&#20110;&#20219;&#21153;&#21487;&#33021;&#36807;&#20110;&#22797;&#26434;&#25110;&#25104;&#26412;&#36807;&#39640;&#20197;&#36827;&#34892;&#27880;&#37322;&#30340;&#22330;&#26223;&#65292;&#25110;&#32773;&#31995;&#32479;&#38656;&#35201;&#31435;&#21363;&#22788;&#29702;&#26032;&#20219;&#21153;&#12290;&#20854;&#27425;&#65292;&#36825;&#19981;&#26159;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#22240;&#20026;&#26368;&#32456;&#29992;&#25143;&#21487;&#33021;&#26356;&#24895;&#24847;&#22312;&#20351;&#29992;&#31995;&#32479;&#20043;&#21069;&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#32780;&#19981;&#26159;&#19968;&#32452;&#31034;&#20363;&#12290;&#22240;&#27492;&#65292;&#31038;&#21306;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26032;&#30340;&#30417;&#30563;&#23547;&#27714;&#33539;&#24335;--&#20174;&#20219;&#21153;&#25351;&#20196;&#23398;&#20064;--&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#31038;&#21306;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#20123;&#20849;&#21516;&#30340;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#26088;&#22312;&#24635;&#32467;&#25351;&#23548;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Task semantics can be expressed by a set of input-to-output examples or a piece of textual instruction. Conventional machine learning approaches for natural language processing (NLP) mainly rely on the availability of large-scale sets of task-specific examples. Two issues arise: first, collecting task-specific labeled examples does not apply to scenarios where tasks may be too complicated or costly to annotate, or the system is required to handle a new task immediately; second, this is not user-friendly since end-users are probably more willing to provide task description rather than a set of examples before using the system. Therefore, the community is paying increasing interest in a new supervision-seeking paradigm for NLP: learning from task instructions. Despite its impressive progress, there are some common issues that the community struggles with. This survey paper tries to summarize the current research on instruction learning, particularly, by answering the following questions:
&lt;/p&gt;</description></item><item><title>CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.09752</link><description>&lt;p&gt;
CoLT5: &#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;&#24555;&#36895;&#38271;&#36317;&#31163;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CoLT5: Faster Long-Range Transformers with Conditional Computation. (arXiv:2303.09752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09752
&lt;/p&gt;
&lt;p&gt;
CoLT5&#26159;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#35745;&#31639;&#30340;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20248;&#20808;&#22788;&#29702;&#37325;&#35201;&#26631;&#35760;&#26469;&#21152;&#36895;&#38271;&#36317;&#31163;&#36755;&#20837;&#30340;&#22788;&#29702;&#12290;CoLT5&#22312;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#26368;&#22909;&#65292;&#24182;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#38271;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#38656;&#35201;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#20294;&#20351;&#29992;Transformer&#22788;&#29702;&#38271;&#25991;&#26723;&#24456;&#26114;&#36149;&#8212;&#8212;&#36825;&#19981;&#20165;&#26159;&#22240;&#20026;&#20108;&#27425;&#27880;&#24847;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#23545;&#27599;&#20010;&#26631;&#35760;&#24212;&#29992;&#21069;&#39304;&#21644;&#25237;&#24433;&#23618;&#12290;&#28982;&#32780;&#65292;&#19981;&#26159;&#25152;&#26377;&#26631;&#35760;&#37117;&#21516;&#26679;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36739;&#38271;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;CoLT5&#65292;&#19968;&#31181;&#38271;&#36755;&#20837;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26465;&#20214;&#35745;&#31639;&#26469;&#21033;&#29992;&#27492;&#30452;&#35273;&#65292;&#22312;&#21069;&#39304;&#21644;&#27880;&#24847;&#23618;&#20013;&#20026;&#37325;&#35201;&#26631;&#35760;&#25552;&#20379;&#26356;&#22810;&#36164;&#28304;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoLT5&#27604;LongT5&#34920;&#29616;&#26356;&#24378;&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#36895;&#24230;&#26356;&#24555;&#65292;&#22312;&#38271;&#36755;&#20837;SCROLLS&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;SOTA&#12290;&#27492;&#22806;&#65292;CoLT5&#33021;&#22815;&#26377;&#25928;&#19988;&#21487;&#25511;&#22320;&#21033;&#29992;&#26497;&#38271;&#30340;&#36755;&#20837;&#65292;&#23637;&#31034;&#20102;&#39640;&#36798;64k&#36755;&#20837;&#38271;&#24230;&#30340;&#24378;&#22823;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing tasks benefit from long inputs, but processing long documents with Transformers is expensive -- not only due to quadratic attention complexity but also from applying feedforward and projection layers to every token. However, not all tokens are equally important, especially for longer documents. We propose CoLT5, a long-input Transformer model that builds on this intuition by employing conditional computation, devoting more resources to important tokens in both feedforward and attention layers. We show that CoLT5 achieves stronger performance than LongT5 with much faster training and inference, achieving SOTA on the long-input SCROLLS benchmark. Moreover, CoLT5 can effectively and tractably make use of extremely long inputs, showing strong gains up to 64k input length.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.13942</link><description>&lt;p&gt;
Inseq&#65306;&#19968;&#20010;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Inseq: An Interpretability Toolkit for Sequence Generation Models. (arXiv:2302.13942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Inseq&#65292;&#36825;&#26159;&#19968;&#20010;Python&#24037;&#20855;&#21253;&#65292;&#26088;&#22312;&#25512;&#24191;&#21487;&#35299;&#37322;&#24615;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;&#23427;&#20026;&#24120;&#35265;&#30340;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#25552;&#20379;&#20102;&#25552;&#21462;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#30340;&#30452;&#35266;&#20248;&#21270;&#26041;&#27861;&#12290;&#20316;&#32773;&#36824;&#22312;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#21644;GPT-2&#20013;&#23637;&#31034;&#20102;Inseq&#30340;&#28508;&#21147;&#65292;&#35777;&#26126;&#20854;&#26377;&#21161;&#20110;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#36807;&#21435;&#30340;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27969;&#34892;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#32780;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#32570;&#20047;&#19987;&#38376;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Inseq&#65292;&#19968;&#20010;Python&#24211;&#65292;&#29992;&#20110;&#20351;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#26222;&#21450;&#21270;&#12290;Inseq&#33021;&#22815;&#30452;&#35266;&#19988;&#20248;&#21270;&#22320;&#25552;&#21462;&#27969;&#34892;&#30340;&#20165;&#35299;&#30721;&#22120;&#21644;&#32534;&#30721;&#22120;&#35299;&#30721;&#22120;Transformers&#26550;&#26500;&#30340;&#27169;&#22411;&#20869;&#37096;&#20449;&#24687;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20351;&#29992;&#23427;&#26469;&#31361;&#20986;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#24182;&#22312;GPT-2&#20013;&#23450;&#20301;&#20107;&#23454;&#30693;&#35782;&#12290;&#30001;&#20110;&#20854;&#25903;&#25345;&#23545;&#27604;&#29305;&#24449;&#24402;&#22240;&#31561;&#21069;&#27839;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#25509;&#21475;&#65292;&#22240;&#27492;Inseq&#21487;&#20197;&#25512;&#21160;&#21487;&#35299;&#37322;&#24615;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#26410;&#26469;&#21457;&#23637;&#65292;&#38598;&#20013;&#20248;&#33391;&#23454;&#36341;&#65292;&#24182;&#23454;&#29616;&#20844;&#27491;&#21644;&#21487;&#37325;&#22797;&#30340;&#27169;&#22411;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work in natural language processing interpretability focused mainly on popular classification tasks while largely overlooking generation settings, partly due to a lack of dedicated tools. In this work, we introduce Inseq, a Python library to democratize access to interpretability analyses of sequence generation models. Inseq enables intuitive and optimized extraction of models' internal information and feature importance scores for popular decoder-only and encoder-decoder Transformers architectures. We showcase its potential by adopting it to highlight gender biases in machine translation models and locate factual knowledge inside GPT-2. Thanks to its extensible interface supporting cutting-edge techniques such as contrastive feature attribution, Inseq can drive future advances in explainable natural language generation, centralizing good practices and enabling fair and reproducible model evaluations.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38463;&#27931;&#26222;&#22827;&#27861;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;10,368&#21517;&#23398;&#29983;&#30340;29,349&#20010;&#38382;&#39064;&#21644;&#35299;&#37322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07738</link><description>&lt;p&gt;
Alloprof&#65306;&#19968;&#20010;&#26032;&#30340;&#27861;&#35821;&#38382;&#31572;&#25945;&#32946;&#25968;&#25454;&#38598;&#21450;&#20854;&#22312;&#20449;&#24687;&#26816;&#32034;&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Alloprof: a new French question-answer education dataset and its use in an information retrieval case study. (arXiv:2302.07738v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07738
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#38463;&#27931;&#26222;&#22827;&#27861;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#25910;&#38598;&#20102;&#26469;&#33258;10,368&#21517;&#23398;&#29983;&#30340;29,349&#20010;&#38382;&#39064;&#21644;&#35299;&#37322;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25945;&#24072;&#21644;&#23398;&#29983;&#36234;&#26469;&#36234;&#20381;&#36182;&#22312;&#32447;&#23398;&#20064;&#36164;&#28304;&#26469;&#34917;&#20805;&#23398;&#26657;&#25552;&#20379;&#30340;&#36164;&#28304;&#12290;&#21487;&#29992;&#36164;&#28304;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#30340;&#22686;&#21152;&#23545;&#23398;&#29983;&#26469;&#35828;&#26159;&#19968;&#20214;&#22909;&#20107;&#65292;&#20294;&#21069;&#25552;&#26159;&#20182;&#20204;&#33021;&#22815;&#25214;&#21040;&#31572;&#26696;&#12290;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#24050;&#21463;&#30410;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#20197;&#35757;&#32451;&#21644;&#35780;&#20272;&#20854;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#36825;&#20123;&#25968;&#25454;&#38598;&#37117;&#26159;&#33521;&#25991;&#25991;&#26412;&#65292;&#30001;&#25104;&#24180;&#20154;&#32534;&#20889;&#21644;&#38405;&#35835;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#20849;&#27861;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#20174;&#24635;&#37096;&#20301;&#20110;&#39745;&#21271;&#20811;&#30340;&#23567;&#23398;&#21644;&#20013;&#23398;&#24110;&#21161;&#32593;&#31449;Alloprof&#25910;&#38598;&#65292;&#21253;&#21547;29,349&#20010;&#38382;&#39064;&#21450;&#20854;&#35299;&#37322;&#65292;&#28085;&#30422;&#21508;&#31181;&#23398;&#31185;&#30340;10,368&#21517;&#23398;&#29983;&#65292;&#36229;&#36807;&#19968;&#21322;&#30340;&#35299;&#37322;&#21253;&#21547;&#38142;&#25509;&#21040;&#20854;&#20182;&#38382;&#39064;&#25110;&#32593;&#31449;&#19978;&#30340;2,596&#20010;&#21442;&#32771;&#39029;&#38754;&#20043;&#19968;&#12290;&#25105;&#20204;&#36824;&#21521;&#24744;&#23637;&#31034;&#20102;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#20351;&#29992;&#26412;&#25968;&#25454;&#38598;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Teachers and students are increasingly relying on online learning resources to supplement the ones provided in school. This increase in the breadth and depth of available resources is a great thing for students, but only provided they are able to find answers to their queries. Question-answering and information retrieval systems have benefited from public datasets to train and evaluate their algorithms, but most of these datasets have been in English text written by and for adults. We introduce a new public French question-answering dataset collected from Alloprof, a Quebec-based primary and high-school help website, containing 29 349 questions and their explanations in a variety of school subjects from 10 368 students, with more than half of the explanations containing links to other questions or some of the 2 596 reference pages on the website. We also present a case study of this dataset in an information retrieval task. This dataset was collected on the Alloprof public forum, with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;&#25552;&#21462;&#30456;&#26426;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#29305;&#24449;&#25104;&#21151;&#23454;&#29616;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#30340;"&#38646;&#26679;&#26412;"&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2301.04647</link><description>&lt;p&gt;
EXIF&#20316;&#20026;&#19968;&#31181;&#35821;&#35328;&#65306;&#23398;&#20064;&#22270;&#20687;&#19982;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
EXIF as Language: Learning Cross-Modal Associations Between Images and Camera Metadata. (arXiv:2301.04647v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.04647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23398;&#20064;&#22270;&#20687;&#21644;&#30456;&#26426;&#20803;&#25968;&#25454;&#20043;&#38388;&#30340;&#20132;&#21449;&#27169;&#24577;&#20851;&#32852;&#25552;&#21462;&#30456;&#26426;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#29305;&#24449;&#25104;&#21151;&#23454;&#29616;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#30340;"&#38646;&#26679;&#26412;"&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#35270;&#35273;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#21462;&#19982;&#25152;&#35760;&#24405;&#30340;&#29031;&#29255;&#30456;&#20851;&#30340;&#30456;&#26426;&#20449;&#24687;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#22359;&#21644;&#33258;&#21160;&#25554;&#20837;&#21040;&#22270;&#20687;&#25991;&#20214;&#20013;&#30340;EXIF&#20803;&#25968;&#25454;&#20043;&#38388;&#35757;&#32451;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#23558;&#20803;&#25968;&#25454;&#36716;&#25442;&#20026;&#25991;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;transformer&#36827;&#34892;&#22788;&#29702;&#26469;&#34920;&#31034;&#27492;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23398;&#20064;&#30340;&#29305;&#24449;&#22312;&#19979;&#28216;&#22270;&#20687;&#21462;&#35777;&#21644;&#26657;&#20934;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#33258;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#29305;&#24449;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#36890;&#36807;&#23545;&#22270;&#20687;&#20869;&#25152;&#26377;&#22359;&#30340;&#35270;&#35273;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#26469;&#23454;&#29616;"&#38646;&#26679;&#26412;"&#30340;&#25340;&#25509;&#22270;&#20687;&#21306;&#22495;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
We learn a visual representation that captures information about the camera that recorded a given photo. To do this, we train a multimodal embedding between image patches and the EXIF metadata that cameras automatically insert into image files. Our model represents this metadata by simply converting it to text and then processing it with a transformer. The features that we learn significantly outperform other self-supervised and supervised features on downstream image forensics and calibration tasks. In particular, we successfully localize spliced image regions "zero shot" by clustering the visual embeddings for all of the patches within an image.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#26368;&#23567;&#21270;&#24517;&#39035;&#30001;&#20154;&#24037;&#35780;&#20998;&#32773;&#36827;&#34892;&#35780;&#20998;&#30340;&#25991;&#31456;&#25968;&#37327;&#65292;&#21516;&#26102;&#25552;&#20379;&#35757;&#32451;&#29616;&#20195;&#33258;&#21160;&#21270;&#35770;&#25991;&#25171;&#20998;&#31995;&#32479;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.00628</link><description>&lt;p&gt;
&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#33258;&#21160;&#35780;&#20998;&#30340;&#20316;&#25991;
&lt;/p&gt;
&lt;p&gt;
Using Active Learning Methods to Strategically Select Essays for Automated Scoring. (arXiv:2301.00628v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#26368;&#23567;&#21270;&#24517;&#39035;&#30001;&#20154;&#24037;&#35780;&#20998;&#32773;&#36827;&#34892;&#35780;&#20998;&#30340;&#25991;&#31456;&#25968;&#37327;&#65292;&#21516;&#26102;&#25552;&#20379;&#35757;&#32451;&#29616;&#20195;&#33258;&#21160;&#21270;&#35770;&#25991;&#25171;&#20998;&#31995;&#32479;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#35780;&#20998;&#30340;&#30740;&#31350;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20316;&#20026;&#19968;&#31181;&#35780;&#20272;&#23398;&#29983;&#22823;&#35268;&#27169;&#20070;&#38754;&#20316;&#31572;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#38543;&#30528;&#23398;&#29983;&#36716;&#21521;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#65292;&#38656;&#35201;&#35780;&#20272;&#22823;&#37327;&#20070;&#38754;&#20316;&#31572;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25551;&#36848;&#21644;&#35780;&#20272;&#19977;&#31181;&#21487;&#20197;&#29992;&#20110;&#26368;&#23567;&#21270;&#24517;&#39035;&#30001;&#20154;&#24037;&#35780;&#20998;&#32773;&#36827;&#34892;&#35780;&#20998;&#30340;&#20316;&#25991;&#25968;&#37327;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#20173;&#28982;&#25552;&#20379;&#35757;&#32451;&#29616;&#20195;&#33258;&#21160;&#21270;&#35770;&#25991;&#25171;&#20998;&#31995;&#32479;&#25152;&#38656;&#30340;&#25968;&#25454;&#12290;&#36825;&#19977;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26159;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#12289;&#22522;&#20110;&#25299;&#25169;&#12289;&#28151;&#21512;&#26041;&#27861;&#12290;&#36825;&#19977;&#31181;&#26041;&#27861;&#34987;&#29992;&#20110;&#36873;&#25321;&#20316;&#20026;&#33258;&#21160;&#21270;&#23398;&#29983;&#35780;&#20272;&#22870;&#31454;&#36187;&#30340;&#19968;&#37096;&#20998;&#30340;&#25991;&#31456;&#65292;&#20043;&#21518;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#27861;&#20174;&#21464;&#25442;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#35780;&#20998;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on automated essay scoring has become increasing important because it serves as a method for evaluating students' written-responses at scale. Scalable methods for scoring written responses are needed as students migrate to online learning environments resulting in the need to evaluate large numbers of written-response assessments. The purpose of this study is to describe and evaluate three active learning methods than can be used to minimize the number of essays that must be scored by human raters while still providing the data needed to train a modern automated essay scoring system. The three active learning methods are the uncertainty-based, the topological-based, and the hybrid method. These three methods were used to select essays included as part of the Automated Student Assessment Prize competition that were then classified using a scoring model that was training with the bidirectional encoder representations from transformer language model. All three active learning met
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;SDOH&#20449;&#24687;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;EHR&#20013;&#30340;&#20020;&#24202;&#21465;&#36848;&#65292;&#25429;&#33719;&#35814;&#32454;&#30340;SDOH&#20449;&#24687;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#34920;&#31034;&#19982;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#32452;&#21512;&#20197;&#33719;&#24471;&#20449;&#24687;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2212.07538</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22686;&#24378;&#30005;&#23376;&#30149;&#21382;&#20013;&#30340;&#32467;&#26500;&#21270;&#31038;&#20250;&#21355;&#29983;&#22240;&#32032;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Leveraging Natural Language Processing to Augment Structured Social Determinants of Health Data in the Electronic Health Record. (arXiv:2212.07538v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07538
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;SDOH&#20449;&#24687;&#25552;&#21462;&#22120;&#65292;&#36890;&#36807;&#23558;&#20854;&#24212;&#29992;&#20110;EHR&#20013;&#30340;&#20020;&#24202;&#21465;&#36848;&#65292;&#25429;&#33719;&#35814;&#32454;&#30340;SDOH&#20449;&#24687;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#34920;&#31034;&#19982;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#34892;&#32452;&#21512;&#20197;&#33719;&#24471;&#20449;&#24687;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#31038;&#20250;&#21355;&#29983;&#22240;&#32032;&#65288;SDOH&#65289;&#24433;&#21709;&#20581;&#24247;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;&#32467;&#26500;&#21270;&#25968;&#25454;&#21644;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#35760;&#24405;&#22312;&#30005;&#23376;&#30149;&#21382;&#65288;EHR&#65289;&#20013;&#36827;&#34892;&#35760;&#24405;&#12290;&#28982;&#32780;&#65292;&#20020;&#24202;&#35760;&#24405;&#36890;&#24120;&#21253;&#21547;&#26356;&#20840;&#38754;&#30340;SDOH&#20449;&#24687;&#65292;&#35814;&#32454;&#35828;&#26126;&#26041;&#38754;&#65292;&#20363;&#22914;&#29366;&#24577;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#26102;&#38388;&#24615;&#12290;&#26412;&#25991;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;i&#65289;&#24320;&#21457;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20449;&#24687;&#25552;&#21462;&#27169;&#22411;&#65292;&#20197;&#25429;&#33719;&#35814;&#32454;&#30340;SDOH&#20449;&#24687;&#65307;ii&#65289;&#36890;&#36807;&#23558;SDOH&#25277;&#21462;&#22120;&#24212;&#29992;&#20110;&#20020;&#24202;&#21465;&#36848;&#24182;&#23558;&#25552;&#21462;&#30340;&#34920;&#31034;&#19982;&#29616;&#26377;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#32452;&#21512;&#26469;&#35780;&#20272;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Social determinants of health (SDOH) impact health outcomes and are documented in the electronic health record (EHR) through structured data and unstructured clinical notes. However, clinical notes often contain more comprehensive SDOH information, detailing aspects such as status, severity, and temporality. This work has two primary objectives: i) develop a natural language processing (NLP) information extraction model to capture detailed SDOH information and ii) evaluate the information gain achieved by applying the SDOH extractor to clinical narratives and combining the extracted representations with existing structured data.  Materials and Methods: We developed a novel SDOH extractor using a deep learning entity and relation extraction architecture to characterize SDOH across various dimensions. In an EHR case study, we applied the SDOH extractor to a large clinical data set with 225,089 patients and 430,406 notes with social history sections and compared the extracted S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#20351;&#29992;AUC&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#36164;&#28304;NER&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2212.04800</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#30340;AUC&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUC Maximization for Low-Resource Named Entity Recognition. (arXiv:2212.04800v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20302;&#36164;&#28304;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20013;&#20351;&#29992;AUC&#26368;&#22823;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#36164;&#28304;NER&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#39046;&#22495;&#30340;&#24037;&#20316;&#20351;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25110;&#26465;&#20214;&#38543;&#26426;&#22330;&#65288;CRF&#65289;&#20316;&#20026;&#20248;&#21270;NER&#27169;&#22411;&#30340;&#30446;&#26631;/&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#20256;&#32479;&#30340;NER&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#22312;&#25968;&#25454;&#20998;&#24067;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#36275;&#22815;&#30340;&#27880;&#37322;&#35757;&#32451;&#26679;&#20363;&#26102;&#21487;&#20197;&#20135;&#29983;&#36275;&#22815;&#30340;&#24615;&#33021;&#12290;&#20294;&#30001;&#20110;NER&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#19981;&#24179;&#34913;&#30340;&#26631;&#35760;&#38382;&#39064;&#65292;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#36825;&#20123;&#26631;&#20934;&#30446;&#26631;&#20989;&#25968;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#22522;&#20110;&#26368;&#22823;&#21270;ROC&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#26469;&#20248;&#21270;NER&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#32467;&#21512;&#20004;&#20010;&#26368;&#22823;&#21270;AUC&#20998;&#25968;&#30340;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#65292;&#22312;&#20302;&#36164;&#28304;NER&#35774;&#32622;&#19979;&#23454;&#29616;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#39640;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current work in named entity recognition (NER) uses either cross entropy (CE) or conditional random fields (CRF) as the objective/loss functions to optimize the underlying NER model. Both of these traditional objective functions for the NER problem generally produce adequate performance when the data distribution is balanced and there are sufficient annotated training examples. But since NER is inherently an imbalanced tagging problem, the model performance under the low-resource settings could suffer using these standard objective functions. Based on recent advances in area under the ROC curve (AUC) maximization, we propose to optimize the NER model by maximizing the AUC score. We give evidence that by simply combining two binary-classifiers that maximize the AUC score, significant performance improvement over traditional loss functions is achieved under low-resource NER settings. We also conduct extensive experiments to demonstrate the advantages of our method under the low-resource 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#32463;&#27982;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;&#65292;&#21363;&#23558;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#20018;&#32852;&#20197;&#26500;&#24314;&#26032;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#32487;&#32493;&#35757;&#32451;&#33021;&#22815;&#25913;&#36827;Transformer&#21644;Conformer&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38271;&#36798;0.9 WER&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.15398</link><description>&lt;p&gt;
&#26368;&#23567;&#21270;&#22686;&#24191;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#65306;&#35821;&#38899;&#35782;&#21035;&#19982;&#32763;&#35793;&#20013;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Make More of Your Data: Minimal Effort Data Augmentation for Automatic Speech Recognition and Translation. (arXiv:2210.15398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31616;&#21333;&#12289;&#32463;&#27982;&#30340;&#25968;&#25454;&#25193;&#20805;&#26041;&#27861;&#65292;&#21363;&#23558;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#20018;&#32852;&#20197;&#26500;&#24314;&#26032;&#30340;&#35757;&#32451;&#23454;&#20363;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#32487;&#32493;&#35757;&#32451;&#33021;&#22815;&#25913;&#36827;Transformer&#21644;Conformer&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38271;&#36798;0.9 WER&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24191;&#26159;&#19968;&#31181;&#26681;&#25454;&#24050;&#26377;&#25968;&#25454;&#29983;&#25104;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#23558;&#21407;&#22987;&#25968;&#25454;&#26679;&#26412;&#20018;&#32852;&#20197;&#26500;&#24314;&#26032;&#30340;&#35757;&#32451;&#23454;&#20363;&#30340;&#31616;&#21333;&#19988;&#32463;&#27982;&#30340;&#26041;&#27861;&#12290;&#32487;&#32493;&#20351;&#29992;&#36825;&#26679;&#30340;&#22686;&#24191;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#33021;&#22815;&#25913;&#36827;&#21407;&#22987;&#25968;&#25454;&#20248;&#21270;&#30340;Transformer&#21644;Conformer&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;LibriSpeech-960h&#27979;&#35797;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65288;test-clean&#21644;test-other&#30340;WER&#20998;&#21035;&#20026;2.83&#21644;6.87&#65289;&#65292;&#36825;&#20123;&#25913;&#36827;&#20063;&#22312;&#19982;&#27973;&#23618;&#34701;&#21512;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#20013;&#24471;&#20197;&#20307;&#29616;&#65288;WER&#20026;2.55&#21644;6.27&#65289;&#12290;&#25105;&#20204;&#30340;&#32487;&#32493;&#35757;&#32451;&#26041;&#27861;&#36824;&#22312;CoVoST-2&#30340;&#22235;&#31181;&#38750;&#33521;&#35821;&#35821;&#35328;&#30340;ASR&#37096;&#20998;&#20013;&#23454;&#29616;&#20102;&#38271;&#36798;0.9 WER&#30340;&#25913;&#36827;&#65292;&#24182;&#19988;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#25910;&#30410;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#39640;&#24230;&#30456;&#20851;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#20018;&#32852;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#35828;&#35805;&#20154;&#20449;&#24687;&#21363;&#21487;&#23454;&#29616;&#20854;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#21487;&#36866;&#29992;&#20110;&#32763;&#35793;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a technique to generate new training data based on existing data. We evaluate the simple and cost-effective method of concatenating the original data examples to build new training instances. Continued training with such augmented data is able to improve off-the-shelf Transformer and Conformer models that were optimized on the original data only. We demonstrate considerable improvements on the LibriSpeech-960h test sets (WER 2.83 and 6.87 for test-clean and test-other), which carry over to models combined with shallow fusion (WER 2.55 and 6.27). Our method of continued training also leads to improvements of up to 0.9 WER on the ASR part of CoVoST-2 for four non English languages, and we observe that the gains are highly dependent on the size of the original training data. We compare different concatenation strategies and found that our method does not need speaker information to achieve its improvements. Finally, we demonstrate on two datasets that our methods also
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#65292;&#24182;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#65292;&#36824;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10260</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#65292;&#24182;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#65292;&#36824;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20256;&#32479;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#23884;&#22871;&#22330;&#26223;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23884;&#22871;&#23454;&#20307;&#35782;&#21035;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#34987;&#36716;&#31227;&#24212;&#29992;&#20110;&#24212;&#23545;&#23454;&#20307;&#23884;&#22871;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#38382;&#39064;&#22312;&#20110;&#38656;&#35201;&#25163;&#21160;&#21019;&#24314;&#26597;&#35810;&#21521;&#37327;&#65292;&#26080;&#27861;&#36866;&#24212;&#19978;&#19979;&#25991;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#35758;&#22120;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#12290;&#28982;&#21518;&#65292;&#22238;&#24402;&#22120;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#27492;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition is a traditional task in natural language processing. In particular, nested entity recognition receives extensive attention for the widespread existence of the nesting scenario. The latest research migrates the well-established paradigm of set prediction in object detection to cope with entity nesting. However, the manual creation of query vectors, which fail to adapt to the rich semantic information in the context, limits these approaches. An end-to-end entity detection approach with proposer and regressor is presented in this paper to tackle the issues. First, the proposer utilizes the feature pyramid network to generate high-quality entity proposals. Then, the regressor refines the proposals for generating the final prediction. The model adopts encoder-only architecture and thus obtains the advantages of the richness of query semantics, high precision of entity localization, and easiness of model training. Moreover, we introduce the novel spatially modulated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2210.08471</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#22686;&#24378;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#25552;&#39640;&#35821;&#20041;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Improving Semantic Matching through Dependency-Enhanced Pre-trained Model with Adaptive Fusion. (arXiv:2210.08471v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20381;&#36182;&#22686;&#24378;&#30340;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#23427;&#23558;&#20381;&#36182;&#20449;&#24687;&#19982;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#33258;&#36866;&#24212;&#34701;&#21512;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#22914;BERT&#65292;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#21516;&#26102;&#65292;&#20381;&#36182;&#24615;&#20808;&#39564;&#30693;&#35782;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#20063;&#26174;&#31034;&#20986;&#26222;&#36941;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#20381;&#36182;&#24615;&#20808;&#39564;&#32467;&#26500;&#26377;&#25928;&#22320;&#38598;&#25104;&#21040;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#65292;&#20197;&#26356;&#22909;&#22320;&#27169;&#25311;&#22797;&#26434;&#30340;&#35821;&#20041;&#21305;&#37197;&#20851;&#31995;&#65292;&#20173;&#26410;&#30830;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DAFA&#30340;&#20381;&#36182;&#22686;&#24378;&#33258;&#36866;&#24212;&#34701;&#21512;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#36825;&#23558;&#20381;&#36182;&#32467;&#26500;&#26126;&#30830;&#22320;&#24341;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#21040;&#35821;&#20041;&#20449;&#24687;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;DAFA&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#25935;&#24863;&#33539;&#24335;&#26469;&#26500;&#24314;&#19968;&#20010;&#20381;&#36182;&#30697;&#38453;&#65292;&#20197;&#26657;&#20934;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#23427;&#37319;&#29992;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26469;&#38598;&#25104;&#33719;&#21462;&#30340;&#20381;&#36182;&#20449;&#24687;&#21644;&#21407;&#22987;&#35821;&#20041;&#20449;&#21495;&#12290;&#27492;&#22806;&#65292;DAFA&#37325;&#26500;&#20102;&#27880;&#24847;&#21147;&#35745;&#31639;&#27969;&#31243;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained models like BERT have achieved great progress on Semantic Sentence Matching. Meanwhile, dependency prior knowledge has also shown general benefits in multiple NLP tasks. However, how to efficiently integrate dependency prior structure into pre-trained models to better model complex semantic matching relations is still unsettled. In this paper, we propose the \textbf{D}ependency-Enhanced \textbf{A}daptive \textbf{F}usion \textbf{A}ttention (\textbf{DAFA}), which explicitly introduces dependency structure into pre-trained models and adaptively fuses it with semantic information. Specifically, \textbf{\emph{(i)}} DAFA first proposes a structure-sensitive paradigm to construct a dependency matrix for calibrating attention weights. It adopts an adaptive fusion module to integrate the obtained dependency information and the original semantic signals. Moreover, DAFA reconstructs the attention calculation flow and provides better interpretability. By applying it o
&lt;/p&gt;</description></item><item><title>StyLEx&#26159;&#19968;&#31181;&#21487;&#20197;&#20174;&#20154;&#24037;&#27880;&#37322;&#30340;&#35821;&#35328;&#39118;&#26684;&#29305;&#24449;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#25552;&#20379;&#21477;&#23376;&#32423;&#39118;&#26684;&#39044;&#27979;&#20197;&#21450;&#20154;&#31867;-like&#30340;&#39118;&#26684;&#35789;&#27719;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.07469</link><description>&lt;p&gt;
StyLEx&#65306;&#20351;&#29992;&#20154;&#31867;&#35789;&#27719;&#27880;&#37322;&#35299;&#37322;&#35821;&#35328;&#39118;&#26684;
&lt;/p&gt;
&lt;p&gt;
StyLEx: Explaining Style Using Human Lexical Annotations. (arXiv:2210.07469v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07469
&lt;/p&gt;
&lt;p&gt;
StyLEx&#26159;&#19968;&#31181;&#21487;&#20197;&#20174;&#20154;&#24037;&#27880;&#37322;&#30340;&#35821;&#35328;&#39118;&#26684;&#29305;&#24449;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#25552;&#20379;&#21477;&#23376;&#32423;&#39118;&#26684;&#39044;&#27979;&#20197;&#21450;&#20154;&#31867;-like&#30340;&#39118;&#26684;&#35789;&#27719;&#35299;&#37322;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#39118;&#26684;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#23398;&#20064;&#21040;&#34394;&#20551;&#30340;&#29305;&#23450;&#39046;&#22495;&#35789;&#27719;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StyLEx&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#20174;&#20154;&#24037;&#27880;&#37322;&#30340;&#35821;&#35328;&#39118;&#26684;&#29305;&#24449;&#35299;&#37322;&#20013;&#23398;&#20064;&#65292;&#24182;&#32852;&#21512;&#23398;&#20064;&#25191;&#34892;&#20219;&#21153;&#24182;&#39044;&#27979;&#36825;&#20123;&#29305;&#24449;&#20316;&#20026;&#27169;&#22411;&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;StyLEx&#21487;&#20197;&#22312;&#19981;&#29306;&#29298;&#20219;&#21153;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#39118;&#26684;&#35789;&#27719;&#35299;&#37322;&#30340;&#21477;&#23376;&#32423;&#39118;&#26684;&#39044;&#27979;&#65292;&#21253;&#25324;&#22495;&#20869;&#21644;&#22495;&#22806;&#25968;&#25454;&#38598;&#12290;&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#26174;&#33879;&#24615;&#22270;&#24418;&#21487;&#35270;&#21270;&#26041;&#27861;&#30456;&#27604;&#65292;StyLEx&#30340;&#35299;&#37322;&#22312;&#35299;&#37322;&#25351;&#26631;&#65288;&#20805;&#20998;&#24615;&#65292;&#21487;&#20449;&#24230;&#65289;&#21644;&#20154;&#31867;&#35780;&#27880;&#35780;&#20272;&#26102;&#22343;&#26377;&#26174;&#30528;&#25552;&#21319;&#65292;&#24182;&#19988;&#26356;&#23481;&#26131;&#34987;&#20154;&#31867;&#35780;&#21028;&#32773;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models have achieved impressive results on various style classification tasks, but they often learn spurious domain-specific words to make predictions (Hayati et al., 2021). While human explanation highlights stylistic tokens as important features for this task, we observe that model explanations often do not align with them. To tackle this issue, we introduce StyLEx, a model that learns from human-annotated explanations of stylistic features and jointly learns to perform the task and predict these features as model explanations. Our experiments show that StyLEx can provide human-like stylistic lexical explanations without sacrificing the performance of sentence-level style prediction on both in-domain and out-of-domain datasets. Explanations from StyLEx show significant improvements in explanation metrics (sufficiency, plausibility) and when evaluated with human annotations. They are also more understandable by human judges compared to the widely-used salien
&lt;/p&gt;</description></item><item><title>DABERT &#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#22686;&#24378;&#20102; BERT &#22312;&#25429;&#25417;&#21477;&#23376;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.03454</link><description>&lt;p&gt;
DABERT&#65306;&#21452;&#37325;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;BERT&#35821;&#20041;&#21305;&#37197;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DABERT: Dual Attention Enhanced BERT for Semantic Matching. (arXiv:2210.03454v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03454
&lt;/p&gt;
&lt;p&gt;
DABERT &#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#22686;&#24378;&#20102; BERT &#22312;&#25429;&#25417;&#21477;&#23376;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#22312;&#35821;&#20041;&#21477;&#23376;&#21305;&#37197;&#26041;&#38754;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#22312;&#25429;&#25417;&#24494;&#23567;&#24046;&#24322;&#30340;&#33021;&#21147;&#19978;&#23384;&#22312;&#19981;&#36275;&#12290;&#22914;&#21152;&#20837;&#12289;&#21024;&#38500;&#25110;&#20462;&#25913;&#21477;&#23376;&#20013;&#30340;&#19968;&#20010;&#21333;&#35789;&#31561;&#22122;&#22768;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#20986;&#38169;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#27880;&#24847;&#21147;&#22686;&#24378;&#30340;BERT&#27169;&#22411;&#65288;DABERT&#65289;&#65292;&#20197;&#22686;&#24378;BERT&#22312;&#25429;&#25417;&#21477;&#23376;&#23545;&#20043;&#38388;&#32454;&#24494;&#24046;&#24322;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;DABERT&#30001;&#65288;1&#65289;&#21452;&#37325;&#27880;&#24847;&#21147;&#27169;&#22359;&#21644;&#65288;2&#65289;&#33258;&#36866;&#24212;&#34701;&#21512;&#27169;&#22359;&#26500;&#25104;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#30340;&#35821;&#20041;&#21305;&#37197;&#21644;&#40065;&#26834;&#24615;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;DABERT&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based pre-trained language models such as BERT have achieved remarkable results in Semantic Sentence Matching. However, existing models still suffer from insufficient ability to capture subtle differences. Minor noise like word addition, deletion, and modification of sentences may cause flipped predictions. To alleviate this problem, we propose a novel Dual Attention Enhanced BERT (DABERT) to enhance the ability of BERT to capture fine-grained differences in sentence pairs. DABERT comprises (1) Dual Attention module, which measures soft word matches by introducing a new dual channel alignment mechanism to model affinity and difference attention. (2) Adaptive Fusion module, this module uses attention to learn the aggregation of difference and affinity features, and generates a vector describing the matching details of sentence pairs. We conduct extensive experiments on well-studied semantic matching and robustness test datasets, and the experimental results show the effectiv
&lt;/p&gt;</description></item><item><title>UniCausal&#26159;&#19968;&#20010;&#36328;&#19977;&#20010;&#20219;&#21153;&#30340;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#32479;&#19968;&#22522;&#20934;&#65292;&#25972;&#21512;&#20102;&#20845;&#20010;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#30340;&#27880;&#37322;&#12290;UniCausal&#21487;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#21644;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2208.09163</link><description>&lt;p&gt;
UniCausal&#65306;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#30340;&#32479;&#19968;&#22522;&#20934;&#19982;&#20179;&#24211;
&lt;/p&gt;
&lt;p&gt;
UniCausal: Unified Benchmark and Repository for Causal Text Mining. (arXiv:2208.09163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.09163
&lt;/p&gt;
&lt;p&gt;
UniCausal&#26159;&#19968;&#20010;&#36328;&#19977;&#20010;&#20219;&#21153;&#30340;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#32479;&#19968;&#22522;&#20934;&#65292;&#25972;&#21512;&#20102;&#20845;&#20010;&#39640;&#36136;&#37327;&#30340;&#35821;&#26009;&#24211;&#30340;&#27880;&#37322;&#12290;UniCausal&#21487;&#29992;&#20110;&#35780;&#20272;&#29616;&#26377;&#27169;&#22411;&#33021;&#21147;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#26032;&#30340;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#26041;&#27861;&#21644;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#25968;&#25454;&#38598;&#22312;&#30446;&#26631;&#12289;&#25968;&#25454;&#35206;&#30422;&#21644;&#27880;&#37322;&#26041;&#26696;&#31561;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#12290;&#36825;&#20123;&#19981;&#19968;&#33268;&#30340;&#21162;&#21147;&#22952;&#30861;&#20102;&#24314;&#27169;&#33021;&#21147;&#21644;&#27169;&#22411;&#24615;&#33021;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#25968;&#25454;&#38598;&#21253;&#25324;&#22240;&#26524;&#20851;&#31995;&#36328;&#24230;&#27880;&#37322;&#65292;&#36825;&#26159;&#36827;&#34892;&#31471;&#21040;&#31471;&#22240;&#26524;&#20851;&#31995;&#25552;&#21462;&#25152;&#24517;&#38656;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;UniCausal&#65292;&#36825;&#26159;&#19968;&#20010;&#36328;&#19977;&#20010;&#20219;&#21153;&#30340;&#32479;&#19968;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#22522;&#20934;&#65306;&#65288;I&#65289;&#22240;&#26524;&#24207;&#21015;&#20998;&#31867;&#65292;&#65288;II&#65289;&#22240;&#26524;&#36328;&#24230;&#26816;&#27979;&#21644;&#65288;III&#65289;&#22240;&#26524;&#23545;&#20998;&#31867;&#12290;&#25105;&#20204;&#25972;&#21512;&#21644;&#23545;&#40784;&#20102;&#20845;&#20010;&#39640;&#36136;&#37327;&#30340;&#65292;&#20027;&#35201;&#26159;&#20154;&#24037;&#27880;&#37322;&#30340;&#35821;&#26009;&#24211;&#30340;&#27880;&#37322;&#65292;&#20998;&#21035;&#20026;&#27599;&#20010;&#20219;&#21153;&#25552;&#20379;&#20102;&#24635;&#25968;&#20026;58,720&#12289;12,144&#21644;69,165&#20010;&#31034;&#20363;&#12290;&#30001;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#23450;&#20041;&#21487;&#33021;&#26159;&#20027;&#35266;&#30340;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#20801;&#35768;&#30740;&#31350;&#20154;&#21592;&#22312;&#26576;&#20123;&#25110;&#25152;&#26377;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#19978;&#24037;&#20316;&#12290;&#20026;&#20102;&#21019;&#24314;&#19968;&#20010;&#21021;&#22987;&#22522;&#20934;&#65292;&#25105;&#20204;&#23545;BERT&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20998;&#21035;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#23454;&#29616;&#20102;70.10&#65285;&#30340;&#20108;&#36827;&#21046;F1&#12289;52.42&#65285;&#30340;&#23439;F1&#21644;67.18&#65285;&#30340;&#23439;F1&#12290;UniCausal&#21487;&#20197;&#20316;&#20026;&#35780;&#20272;&#29616;&#26377;&#27169;&#22411;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#22240;&#26524;&#20851;&#31995;&#25991;&#26412;&#25366;&#25496;&#30340;&#26032;&#26041;&#27861;&#21644;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causal text mining across three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, 
&lt;/p&gt;</description></item></channel></rss>