<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31471;&#21040;&#31471;&#25277;&#21462;&#26041;&#27861;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#32452;&#21512;&#33647;&#29289;&#27835;&#30103;&#20013;&#20851;&#38190;&#30340;$n$&#20803;&#20851;&#31995;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;CombDrugExt&#27979;&#35797;&#38598;&#20013;&#20026;&#27491;&#65288;&#25110;&#26377;&#25928;&#65289;&#32452;&#21512;&#23454;&#29616;&#20102;66.7\%&#30340;F1&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2303.16886</link><description>&lt;p&gt;
&#32452;&#21512;&#33647;&#29289;&#27835;&#30103;&#30340;&#31471;&#21040;&#31471;$n$&#20803;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
End-to-End $n$-ary Relation Extraction for Combination Drug Therapies. (arXiv:2303.16886v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16886
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31471;&#21040;&#31471;&#25277;&#21462;&#26041;&#27861;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#32452;&#21512;&#33647;&#29289;&#27835;&#30103;&#20013;&#20851;&#38190;&#30340;$n$&#20803;&#20851;&#31995;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#37319;&#29992;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#26041;&#27861;&#22312;CombDrugExt&#27979;&#35797;&#38598;&#20013;&#20026;&#27491;&#65288;&#25110;&#26377;&#25928;&#65289;&#32452;&#21512;&#23454;&#29616;&#20102;66.7\%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33647;&#29289;&#30103;&#27861;&#26159;&#28041;&#21450;&#20004;&#31181;&#25110;&#22810;&#31181;&#33647;&#29289;&#30340;&#27835;&#30103;&#26041;&#26696;&#65292;&#36890;&#24120;&#29992;&#20110;&#30284;&#30151;&#12289;&#33406;&#28363;&#30149;&#12289;&#30111;&#30142;&#25110;&#32467;&#26680;&#30149;&#24739;&#32773;&#12290;&#30446;&#21069;&#65292;PubMed&#19978;&#26377;&#36229;&#36807;35&#19975;&#31687;&#25991;&#31456;&#20351;&#29992;&#20102;&#8220;&#32452;&#21512;&#33647;&#29289;&#27835;&#30103;&#8221;MeSH(&#21270;&#23398;&#29289;&#36136;&#20027;&#39064;&#35789;&#34920;)&#22836;&#34900;&#65292;&#27599;&#24180;&#33267;&#23569;&#21457;&#34920;1&#19975;&#31687;&#25991;&#31456;&#65292;&#25345;&#32493;&#26102;&#38388;&#24050;&#36798;&#20004;&#20010;&#21313;&#24180;&#12290;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20986;&#32452;&#21512;&#30103;&#27861;&#22266;&#26377;&#22320;&#26500;&#25104;&#20102;&#19968;&#20010;$n$&#20803;&#20851;&#31995;&#25277;&#21462;&#38382;&#39064;&#12290;&#19981;&#21516;&#20110;&#26222;&#36890;&#30340;$n$&#20803;&#35774;&#32622;&#65292;&#20854;&#20013;$n$&#26159;&#22266;&#23450;&#30340;&#65288;&#20363;&#22914;&#33647;&#29289;-&#22522;&#22240;-&#31361;&#21464;&#20851;&#31995;&#20013;&#30340;$n=3$&#65289;&#65292;&#25552;&#21462;&#32452;&#21512;&#30103;&#27861;&#26159;&#19968;&#20010;&#29305;&#27530;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;$n \geq2$&#26159;&#21160;&#24577;&#30340;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#27599;&#20010;&#23454;&#20363;&#12290;&#26368;&#36817;&#65292;Tiktinsky&#31561;&#20154;&#65288;NAACL 2022&#65289;&#25512;&#20986;&#20102;&#19968;&#20010;&#39318;&#20010;&#25968;&#25454;&#38598;CombDrugExt&#65292;&#29992;&#20110;&#20174;&#25991;&#29486;&#20013;&#25552;&#21462;&#27492;&#31867;&#30103;&#27861;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#31471;&#21040;&#31471;&#25277;&#21462;&#26041;&#27861;&#65292;&#22312;CombDrugExt&#27979;&#35797;&#38598;&#20013;&#20026;&#27491;&#65288;&#25110;&#26377;&#25928;&#65289;&#32452;&#21512;&#23454;&#29616;&#20102;66.7\%&#30340;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Combination drug therapies are treatment regimens that involve two or more drugs, administered more commonly for patients with cancer, HIV, malaria, or tuberculosis. Currently there are over 350K articles in PubMed that use the "combination drug therapy" MeSH heading with at least 10K articles published per year over the past two decades. Extracting combination therapies from scientific literature inherently constitutes an $n$-ary relation extraction problem. Unlike in the general $n$-ary setting where $n$ is fixed (e.g., drug-gene-mutation relations where $n=3$), extracting combination therapies is a special setting where $n \geq 2$ is dynamic, depending on each instance. Recently, Tiktinsky et al. (NAACL 2022) introduced a first of its kind dataset, CombDrugExt, for extracting such therapies from literature. Here, we use a sequence-to-sequence style end-to-end extraction method to achieve an F1-Score of $66.7\%$ on the CombDrugExt test set for positive (or effective) combinations. Th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#65292;&#24179;&#34913;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#25104;&#26412;&#12289;&#26631;&#27880;&#21592;&#36127;&#25285;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#22810;&#20010;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;DidYouMean&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.16857</link><description>&lt;p&gt;
&#8220;&#20320;&#25351;&#30340;&#26159;...&#65311;&#8221;&#65306;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#32622;&#20449;&#24230;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Did You Mean...? Confidence-based Trade-offs in Semantic Parsing. (arXiv:2303.16857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16857
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#36890;&#36807;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#65292;&#24179;&#34913;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#25104;&#26412;&#12289;&#26631;&#27880;&#21592;&#36127;&#25285;&#12289;&#20934;&#30830;&#24615;&#12289;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#22810;&#20010;&#26435;&#34913;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#20197;&#26356;&#22909;&#22320;&#24179;&#34913;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;DidYouMean&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#19968;&#20010;&#26657;&#20934;&#22909;&#30340;&#27169;&#22411;&#26469;&#24179;&#34913;&#20219;&#21153;&#23548;&#21521;&#35299;&#26512;&#20013;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#22312;&#19968;&#20010;&#27169;&#25311;&#30340;&#26631;&#27880;&#21592;&#20132;&#20114;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26657;&#20934;&#30340;&#32622;&#20449;&#20998;&#25968;&#22914;&#20309;&#24179;&#34913;&#25104;&#26412;&#21644;&#26631;&#27880;&#21592;&#36127;&#25285;&#65292;&#29992;&#36739;&#23569;&#30340;&#20132;&#20114;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32622;&#20449;&#24230;&#20998;&#25968;&#22914;&#20309;&#24110;&#21161;&#20248;&#21270;&#21487;&#29992;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#32622;&#20449;&#24230;&#38408;&#20540;&#30340;&#35299;&#26512;&#38169;&#35823;&#25968;&#37327;&#22823;&#24133;&#20943;&#23569;&#30340;&#31995;&#32479;DidYouMean&#65292;&#28982;&#32780;&#36825;&#20063;&#29306;&#29298;&#20102;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We illustrate how a calibrated model can help balance common trade-offs in task-oriented parsing. In a simulated annotator-in-the-loop experiment, we show that well-calibrated confidence scores allow us to balance cost with annotator load, improving accuracy with a small number of interactions. We then examine how confidence scores can help optimize the trade-off between usability and safety. We show that confidence-based thresholding can substantially reduce the number of incorrect low-confidence programs executed; however, this comes at a cost to usability. We propose the DidYouMean system which better balances usability and safety.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#65292;&#21363;&#8220;&#20808;&#35299;&#37322;&#20877;&#27880;&#37322;&#8221;&#65292;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#20026;&#26356;&#22909;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;&#65292;&#39318;&#20808;&#20026;&#27599;&#20010;&#28436;&#31034;&#23454;&#20363;&#21019;&#24314;&#25552;&#31034;&#65292;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#25552;&#31034;&#25552;&#31034;LLM&#25552;&#20379;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.16854</link><description>&lt;p&gt;
AnnoLLM&#65306;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;
&lt;/p&gt;
&lt;p&gt;
AnnoLLM: Making Large Language Models to Be Better Crowdsourced Annotators. (arXiv:2303.16854v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#27861;&#65292;&#21363;&#8220;&#20808;&#35299;&#37322;&#20877;&#27880;&#37322;&#8221;&#65292;&#20197;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25104;&#20026;&#26356;&#22909;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;&#65292;&#39318;&#20808;&#20026;&#27599;&#20010;&#28436;&#31034;&#23454;&#20363;&#21019;&#24314;&#25552;&#31034;&#65292;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#25552;&#31034;&#25552;&#31034;LLM&#25552;&#20379;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20381;&#36182;&#20110;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#27880;&#37322;&#21487;&#33021;&#26159;&#19968;&#20010;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#24403;&#20219;&#21153;&#28041;&#21450;&#22823;&#37327;&#25968;&#25454;&#25110;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#26102;&#12290;&#26368;&#36817;&#65292;GPT-3.5&#31995;&#21015;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#26412;&#25991;&#39318;&#20808;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#65292;&#21487;&#20197;&#36890;&#36807;&#20026;&#23427;&#20204;&#25552;&#20379;&#20805;&#20998;&#30340;&#25351;&#23548;&#21644;&#28436;&#31034;&#31034;&#20363;&#26469;&#20316;&#20026;&#20248;&#31168;&#30340;&#20247;&#21253;&#26631;&#27880;&#22120;&#12290;&#20026;&#20102;&#20351;LLMs&#25104;&#20026;&#26356;&#22909;&#30340;&#26631;&#27880;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#26041;&#27861;&#65292;&#8220;&#20808;&#35299;&#37322;&#20877;&#27880;&#37322;&#8221;&#12290;&#26356;&#30830;&#20999;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#27599;&#20010;&#28436;&#31034;&#31034;&#20363;&#21019;&#24314;&#25552;&#31034;&#65292;&#38543;&#21518;&#21033;&#29992;&#36825;&#20123;&#25552;&#31034;&#25552;&#31034;LLM&#25552;&#20379;&#20851;&#20110;&#20026;&#20160;&#20040;&#23545;&#20110;&#29305;&#23450;&#31034;&#20363;&#36873;&#25321;&#20102;&#29305;&#23450;&#30340;&#22522;&#30784;&#30495;&#30456;&#22238;&#31572;/&#26631;&#31614;&#30340;&#35299;&#37322;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;few-shot&#24605;&#32500;&#38142;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing (NLP) tasks rely on labeled data to train machine learning models to achieve high performance. However, data annotation can be a time-consuming and expensive process, especially when the task involves a large amount of data or requires specialized domains. Recently, GPT-3.5 series models have demonstrated remarkable few-shot and zero-shot ability across various NLP tasks. In this paper, we first claim that large language models (LLMs), such as GPT-3.5, can serve as an excellent crowdsourced annotator by providing them with sufficient guidance and demonstrated examples. To make LLMs to be better annotators, we propose a two-step approach, 'explain-then-annotate'. To be more precise, we begin by creating prompts for every demonstrated example, which we subsequently utilize to prompt a LLM to provide an explanation for why the specific ground truth answer/label was chosen for that particular example. Following this, we construct the few-shot chain-of-thoug
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16839</link><description>&lt;p&gt;
MaMMUT: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#32852;&#21512;&#23398;&#20064;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#20174;&#32534;&#30721;-&#35299;&#30721;&#36716;&#21521;&#20165;&#35299;&#30721;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26222;&#36941;&#35748;&#20026;&#65292;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#29983;&#25104;&#20219;&#21153;&#21644;&#23545;&#27604;&#20219;&#21153;&#65292;&#24448;&#24448;&#20114;&#30456;&#20914;&#31361;&#65292;&#38590;&#20197;&#22312;&#19968;&#20010;&#26550;&#26500;&#20013;&#23481;&#32435;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#20165;&#35299;&#30721;&#27169;&#22411;&#65292;&#36825;&#22312;&#32852;&#21512;&#23398;&#20064;&#36825;&#20123;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;MaMMUT&#23454;&#29616;&#30340;&#12290;&#23427;&#30001;&#21333;&#19968;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25991;&#26412;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#35299;&#30721;&#22120;&#19978;&#30340;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#19981;&#21516;&#30446;&#26631;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#26159;&#31616;&#21333;&#30340;&#65292;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#27169;&#22411;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26550;&#26500;&#20351;&#24471;&#23545;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#26816;&#27979;&#30340;&#31616;&#21333;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#36825;&#19968;&#20219;&#21153;&#31867;&#21035;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#12290;&#23454;&#39564;&#27979;&#35797;&#20102;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#20854;&#26410;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.16835</link><description>&lt;p&gt;
&#38646;&#26679;&#26412; Entrailment &#29992;&#20110; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Entailment of Leaderboards for Empirical AI Research. (arXiv:2303.16835v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16835
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#36825;&#19968;&#20219;&#21153;&#31867;&#21035;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#12290;&#23454;&#39564;&#27979;&#35797;&#20102;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#22312;&#20854;&#26410;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#26412;&#25991;&#21019;&#24314;&#20102;&#19968;&#20010;&#38646;&#26679;&#26412;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#19968;&#20010;&#29305;&#23450;&#30340;&#25991;&#26412;&#34164;&#21547;&#65288;RTE&#65289;&#20219;&#21153;&#31867;&#21035;&#20013;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#23398;&#20064;&#29616;&#35937;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#21363;&#33258;&#21160;&#25366;&#25496; Empirical AI Research &#39046;&#22495;&#25490;&#34892;&#27036;&#12290;&#35813;&#39046;&#22495;&#30340;&#25490;&#34892;&#27036;&#25552;&#21462;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#65292;&#22312;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#25253;&#21578;&#20102;&#39640;&#20110;90%&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#26680;&#24515;&#30340;&#30740;&#31350;&#38382;&#39064;&#20173;&#26410;&#34987;&#26816;&#39564;&#65306;&#36825;&#20123;&#27169;&#22411;&#30495;&#30340;&#23398;&#20064;&#20102; entailment &#21527;&#65311;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#30340;&#23454;&#39564;&#20013;&#65292;&#27979;&#35797;&#20102;&#20004;&#20010;&#20808;&#21069;&#25253;&#21578;&#30340;&#26368;&#26032;&#25216;&#26415;&#27169;&#22411;&#65292;&#22312;&#20854;&#35757;&#32451;&#36807;&#31243;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#25490;&#34892;&#27036;&#26631;&#31614;&#19978;&#65292;&#27979;&#35797;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#25110; entailment &#33021;&#21147;&#12290;&#25105;&#20204;&#20551;&#35774;&#65292;&#22914;&#26524;&#27169;&#22411;&#23398;&#20064;&#20102; entailment&#65292;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#20063;&#21487;&#33021;&#26159;&#20013;&#31561;&#30340;&#65292;&#25110;&#32773;&#20855;&#20307;&#26469;&#35828;&#65292;&#22909;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;&#26412;&#25991;&#36890;&#36807;&#36828;&#31243;&#26631;&#27880;&#21019;&#24314;&#20102;&#38646;&#26679;&#26412;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a large-scale empirical investigation of the zero-shot learning phenomena in a specific recognizing textual entailment (RTE) task category, i.e. the automated mining of leaderboards for Empirical AI Research. The prior reported state-of-the-art models for leaderboards extraction formulated as an RTE task, in a non-zero-shot setting, are promising with above 90% reported performances. However, a central research question remains unexamined: did the models actually learn entailment? Thus, for the experiments in this paper, two prior reported state-of-the-art models are tested out-of-the-box for their ability to generalize or their capacity for entailment, given leaderboard labels that were unseen during training. We hypothesize that if the models learned entailment, their zero-shot performances can be expected to be moderately high as well--perhaps, concretely, better than chance. As a result of this work, a zero-shot labeled dataset is created via distant labeling formulating
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#23186;&#20307;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;&#36866;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#12289;&#30005;&#35270;&#24191;&#25773;&#25110;&#24191;&#25773;&#33410;&#30446;&#20869;&#23481;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.16779</link><description>&lt;p&gt;
&#22522;&#20110;&#23186;&#20307;&#20559;&#22909;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;
&lt;/p&gt;
&lt;p&gt;
Language Models Trained on Media Diets Can Predict Public Opinion. (arXiv:2303.16779v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16779
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#23186;&#20307;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;&#36866;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#12289;&#30005;&#35270;&#24191;&#25773;&#25110;&#24191;&#25773;&#33410;&#30446;&#20869;&#23481;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;&#24182;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#20247;&#33286;&#35770;&#21453;&#26144;&#21644;&#22609;&#36896;&#31038;&#20250;&#34892;&#20026;&#65292;&#20294;&#20256;&#32479;&#30340;&#22522;&#20110;&#35843;&#26597;&#30340;&#24037;&#20855;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25506;&#32034;&#23186;&#20307;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;&#36866;&#29992;&#20110;&#22312;&#32447;&#26032;&#38395;&#12289;&#30005;&#35270;&#24191;&#25773;&#25110;&#24191;&#25773;&#33410;&#30446;&#20869;&#23481;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#27169;&#25311;&#24050;&#28040;&#36153;&#19968;&#32452;&#23186;&#20307;&#30340;&#20122;&#32676;&#20307;&#30340;&#24847;&#35265;&#12290;&#20026;&#20102;&#39564;&#35777;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#20351;&#29992;&#32654;&#22269;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#20013;&#38024;&#23545;COVID-19&#21644;&#28040;&#36153;&#32773;&#20449;&#24515;&#30340;&#35266;&#28857;&#34920;&#36798;&#20316;&#20026;&#22522;&#26412;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#65288;1&#65289;&#21487;&#20197;&#39044;&#27979;&#35843;&#26597;&#21709;&#24212;&#20998;&#24067;&#20013;&#30340;&#20154;&#31867;&#21028;&#26029;&#65292;&#24182;&#19988;&#23545;&#25514;&#36766;&#21644;&#23186;&#20307;&#26333;&#20809;&#28192;&#36947;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#65288;2&#65289;&#22312;&#24314;&#27169;&#26356;&#23494;&#20999;&#20851;&#27880;&#23186;&#20307;&#30340;&#20154;&#26041;&#38754;&#26356;&#20934;&#30830;&#65292;&#65288;3&#65289;&#31526;&#21512;&#20851;&#20110;&#21738;&#20123;&#31867;&#22411;&#30340;&#35266;&#28857;&#21463;&#23186;&#20307;&#28040;&#36153;&#24433;&#21709;&#30340;&#25991;&#29486;&#12290;&#25506;&#27979;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26032;&#26041;&#27861;&#26469;&#30740;&#31350;&#23186;&#20307;&#25928;&#24212;&#65292;&#20855;&#26377;&#22312;&#34917;&#20805;&#35843;&#26597;&#21644;&#39044;&#27979;&#20844;&#20247;&#33286;&#35770;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#20063;&#28041;&#21450;&#35299;&#20915;&#19982;&#23186;&#20307;&#20559;&#35265;&#26377;&#20851;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Public opinion reflects and shapes societal behavior, but the traditional survey-based tools to measure it are limited. We introduce a novel approach to probe media diet models -- language models adapted to online news, TV broadcast, or radio show content -- that can emulate the opinions of subpopulations that have consumed a set of media. To validate this method, we use as ground truth the opinions expressed in U.S. nationally representative surveys on COVID-19 and consumer confidence. Our studies indicate that this approach is (1) predictive of human judgements found in survey response distributions and robust to phrasing and channels of media exposure, (2) more accurate at modeling people who follow media more closely, and (3) aligned with literature on which types of opinions are affected by media consumption. Probing language models provides a powerful new method for investigating media effects, has practical applications in supplementing polls and forecasting public opinion, and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;30&#19975;&#20221;&#39135;&#35889;&#25353;&#29031;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20998;&#31867;&#21040;9&#20010;&#31867;&#21035;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#23545;&#20854;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.16778</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#20004; &#30334;&#19975;&#20221;&#26631;&#35760;&#32654;&#39135;&#39135;&#35889;&#25968;&#25454;&#38598; - 3A2M
&lt;/p&gt;
&lt;p&gt;
Assorted, Archetypal and Annotated Two Million (3A2M) Cooking Recipes Dataset based on Active Learning. (arXiv:2303.16778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23558;30&#19975;&#20221;&#39135;&#35889;&#25353;&#29031;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#36827;&#34892;&#20998;&#31867;&#21040;9&#20010;&#31867;&#21035;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#28151;&#21512;&#26041;&#27861;&#23545;&#20854;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28921;&#39274;&#39135;&#35889;&#21487;&#20197;&#20132;&#25442;&#28921;&#39274;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#39135;&#21697;&#30340;&#21046;&#20316;&#35828;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#35813;&#39046;&#22495;&#20869;&#30001;&#20110;&#32570;&#20047;&#36275;&#22815;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23558;&#22312;&#32447;&#25214;&#21040;&#30340;&#21407;&#22987;&#39135;&#35889;&#20998;&#31867;&#21040;&#21512;&#36866;&#30340;&#39135;&#21697;&#31867;&#22411;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#23558;&#39135;&#35889;&#20998;&#31867;&#21487;&#33021;&#26159;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#22522;&#20110;&#19968;&#20010;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#21576;&#29616;&#20102;&#19968;&#20010;&#20004;&#30334;&#19975;&#20221;&#28921;&#39274;&#39135;&#35889;&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#23558;&#20854;&#26631;&#35760;&#22312;&#21508;&#33258;&#30340;&#31867;&#21035;&#20013;&#12290;&#20026;&#20102;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#20174;RecipeNLG&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#39135;&#35889;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#21487;&#20449;&#24230;&#24471;&#20998;&#39640;&#20110;86.667&#65285;&#30340;&#20154;&#31867;&#19987;&#23478;&#25353;&#29031;&#20854;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#23558;30&#19975;&#20221;&#39135;&#35889;&#20998;&#31867;&#21040;&#20061;&#20010;&#31867;&#21035;&#20043;&#19968;&#65306;&#28888;&#28953;&#12289;&#39278;&#26009;&#12289;&#33636;&#33756;&#12289;&#34092;&#33756;&#12289;&#24555;&#39184;&#12289;&#40614;&#29255;&#12289;&#39184;&#28857;&#12289;&#37197;&#33756;&#21644;&#34701;&#21512;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;Query-by-Committee&#21644;Human&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#21097;&#20313;&#30340;1900K&#20221;&#39135;&#35889;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooking recipes allow individuals to exchange culinary ideas and provide food preparation instructions. Due to a lack of adequate labeled data, categorizing raw recipes found online to the appropriate food genres is a challenging task in this domain. Utilizing the knowledge of domain experts to categorize recipes could be a solution. In this study, we present a novel dataset of two million culinary recipes labeled in respective categories leveraging the knowledge of food experts and an active learning technique. To construct the dataset, we collect the recipes from the RecipeNLG dataset. Then, we employ three human experts whose trustworthiness score is higher than 86.667% to categorize 300K recipe by their Named Entity Recognition (NER) and assign it to one of the nine categories: bakery, drinks, non-veg, vegetables, fast food, cereals, meals, sides and fusion. Finally, we categorize the remaining 1900K recipes using Active Learning method with a blend of Query-by-Committee and Human 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#35821;&#35328;&#26816;&#27979;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#19968;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#26159;&#20302;&#36136;&#37327;&#26631;&#31614;&#21644;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2303.16777</link><description>&lt;p&gt;
&#19981;&#20919;&#38745;&#65292;&#19981;&#20919;&#38745;&#65292;&#20063;&#19981;&#38215;&#23450;&#65306;&#20351;&#29992;&#24773;&#24863;&#35821;&#35328;&#26816;&#27979;COVID-19&#30340;&#38169;&#35823;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Not cool, calm or collected: Using emotional language to detect COVID-19 misinformation. (arXiv:2303.16777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#24773;&#24863;&#35821;&#35328;&#26816;&#27979;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#19968;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#20294;&#35813;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#26159;&#20302;&#36136;&#37327;&#26631;&#31614;&#21644;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22914;Twitter&#19978;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#23545;&#26377;&#25928;&#30340;&#30123;&#24773;&#31649;&#29702;&#26500;&#25104;&#23041;&#32961;&#12290;&#20808;&#21069;&#22312;&#25512;&#29305;&#19978;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#30340;&#24037;&#20316;&#21542;&#35748;&#20102;&#25512;&#29305;&#19978;&#26222;&#36941;&#23384;&#22312;&#30340;&#35832;&#22914;&#24102;&#30005;&#24773;&#24863;&#30340;&#35821;&#20041;&#29305;&#24449;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#25512;&#29305;&#24773;&#24863;&#32534;&#30721;&#22120;&#21644;COVID-19&#38169;&#35823;&#20449;&#24687;&#32534;&#30721;&#22120;&#26469;&#39044;&#27979;&#25512;&#25991;&#26159;&#21542;&#21253;&#21547;COVID-19&#38169;&#35823;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#24773;&#24863;&#32534;&#30721;&#22120;&#22312;&#19968;&#32452;&#26032;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#25105;&#20204;&#30340;COVID-19&#38169;&#35823;&#20449;&#24687;&#32534;&#30721;&#22120;&#22312;COVID-HeRA&#25968;&#25454;&#38598;&#30340;&#23376;&#38598;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#24773;&#24863;&#21644;&#38169;&#35823;&#20449;&#24687;&#32534;&#30721;&#22120;&#30340;&#32452;&#21512;&#27604;&#21333;&#29420;&#30340;&#38169;&#35823;&#20449;&#24687;&#20998;&#31867;&#22120;&#20135;&#29983;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#20998;&#26512;&#65292;&#24378;&#35843;&#20102;&#20302;&#36136;&#37327;&#26631;&#31614;&#21644;&#19981;&#21305;&#37197;&#30340;&#26631;&#31614;&#20998;&#24067;&#26159;&#25105;&#20204;&#30740;&#31350;&#30340;&#20027;&#35201;&#38480;&#21046;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
COVID-19 misinformation on social media platforms such as twitter is a threat to effective pandemic management. Prior works on tweet COVID-19 misinformation negates the role of semantic features common to twitter such as charged emotions. Thus, we present a novel COVID-19 misinformation model, which uses both a tweet emotion encoder and COVID-19 misinformation encoder to predict whether a tweet contains COVID-19 misinformation. Our emotion encoder was fine-tuned on a novel annotated dataset and our COVID-19 misinformation encoder was fine-tuned on a subset of the COVID-HeRA dataset. Experimental results show superior results using the combination of emotion and misinformation encoders as opposed to a misinformation classifier alone. Furthermore, extensive result analysis was conducted, highlighting low quality labels and mismatched label distributions as key limitations to our study.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#24102;&#26377;&#34892;&#21160;&#39033;&#30340;&#20250;&#35758;&#20869;&#23481;&#65292;&#24182;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25163;&#21160;&#34892;&#21160;&#39033;&#27880;&#37322;&#30340;&#20013;&#25991;&#20250;&#35758;&#35821;&#26009;&#24211;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16763</link><description>&lt;p&gt;
&#24102;&#27491;&#21017;&#21270;&#19978;&#19979;&#25991;&#24314;&#27169;&#30340;&#20250;&#35758;&#34892;&#21160;&#39033;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Meeting Action Item Detection with Regularized Context Modeling. (arXiv:2303.16763v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#20002;&#22833;&#25216;&#26415;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#24102;&#26377;&#34892;&#21160;&#39033;&#30340;&#20250;&#35758;&#20869;&#23481;&#65292;&#24182;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25163;&#21160;&#34892;&#21160;&#39033;&#27880;&#37322;&#30340;&#20013;&#25991;&#20250;&#35758;&#35821;&#26009;&#24211;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#36739;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35758;&#23545;&#20110;&#21327;&#20316;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20250;&#35758;&#20869;&#23481;&#20013;&#30340;&#34892;&#21160;&#39033;&#23545;&#20110;&#31649;&#29702;&#20250;&#21518;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#36825;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#24635;&#32467;&#12290;&#34892;&#21160;&#39033;&#26816;&#27979;&#20219;&#21153;&#26088;&#22312;&#33258;&#21160;&#26816;&#27979;&#19982;&#34892;&#21160;&#39033;&#30456;&#20851;&#30340;&#20250;&#35758;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#25163;&#21160;&#27880;&#37322;&#34892;&#21160;&#39033;&#26816;&#27979;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#24456;&#23569;&#19988;&#35268;&#27169;&#36739;&#23567;&#12290;&#25105;&#20204;&#26500;&#24314;&#24182;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#24102;&#26377;&#25163;&#21160;&#34892;&#21160;&#39033;&#27880;&#37322;&#30340;&#20013;&#25991;&#20250;&#35758;&#35821;&#26009;&#24211;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#20002;&#22833;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#19978;&#19979;&#25991;&#65292;&#24182;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#34892;&#21160;&#39033;&#26816;&#27979;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#26469;&#21033;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#20013;&#25991;&#20250;&#35758;&#35821;&#26009;&#24211;&#21644;&#33521;&#35821;AMI&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meetings are increasingly important for collaborations. Action items in meeting transcripts are crucial for managing post-meeting to-do tasks, which usually are summarized laboriously. The Action Item Detection task aims to automatically detect meeting content associated with action items. However, datasets manually annotated with action item detection labels are scarce and in small scale. We construct and release the first Chinese meeting corpus with manual action item annotations. In addition, we propose a Context-Drop approach to utilize both local and global contexts by contrastive learning, and achieve better accuracy and robustness for action item detection. We also propose a Lightweight Model Ensemble method to exploit different pre-trained models. Experimental results on our Chinese meeting corpus and the English AMI corpus demonstrate the effectiveness of the proposed approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#39640;&#25928;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16760</link><description>&lt;p&gt;
&#20351;&#29992;&#34433;&#32676;&#20248;&#21270;&#30340;&#26032;&#22411;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger
&lt;/p&gt;
&lt;p&gt;
ACO-tagger: A Novel Method for Part-of-Speech Tagging using Ant Colony Optimization. (arXiv:2303.16760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34433;&#32676;&#20248;&#21270;&#30340;&#39640;&#25928;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32676;&#26234;&#33021;&#31639;&#27861;&#22240;&#20854;&#35299;&#20915;&#22797;&#26434;&#21644;&#38750;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31639;&#27861;&#21463;&#33258;&#28982;&#29983;&#29289;&#30340;&#38598;&#20307;&#34892;&#20026;&#21551;&#21457;&#65292;&#27169;&#25311;&#36825;&#31181;&#34892;&#20026;&#20197;&#24320;&#21457;&#29992;&#20110;&#35745;&#31639;&#20219;&#21153;&#30340;&#26234;&#33021; agent&#12290;&#20854;&#20013;&#19968;&#31181;&#31639;&#27861;&#26159;&#21463;&#21040;&#34434;&#34433;&#35269;&#39135;&#34892;&#20026;&#21450;&#20854;&#20449;&#24687;&#32032;&#37322;&#25918;&#26426;&#21046;&#21551;&#21457;&#30340;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#25955;&#21644;&#32452;&#21512;&#24615;&#30340;&#22256;&#38590;&#38382;&#39064;&#12290;&#35789;&#24615;&#26631;&#27880;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#30784;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#21477;&#23376;&#20013;&#30340;&#27599;&#20010;&#21333;&#35789;&#20998;&#37197;&#19968;&#20010;&#35789;&#24615;&#35282;&#33394;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ACO&#30340;&#39640;&#24615;&#33021;&#35789;&#24615;&#26631;&#27880;&#26041;&#27861;ACO-tagger&#12290;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#39640;&#36798;96.867&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24555;&#36895;&#39640;&#25928;&#65292;&#26159;&#23454;&#38469;&#24212;&#29992;&#30340;&#21487;&#34892;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Swarm Intelligence algorithms have gained significant attention in recent years as a means of solving complex and non-deterministic problems. These algorithms are inspired by the collective behavior of natural creatures, and they simulate this behavior to develop intelligent agents for computational tasks. One such algorithm is Ant Colony Optimization (ACO), which is inspired by the foraging behavior of ants and their pheromone laying mechanism. ACO is used for solving difficult problems that are discrete and combinatorial in nature. Part-of-Speech (POS) tagging is a fundamental task in natural language processing that aims to assign a part-of-speech role to each word in a sentence. In this research paper, proposed a high-performance POS-tagging method based on ACO called ACO-tagger. This method achieved a high accuracy rate of 96.867%, outperforming several state-of-the-art methods. The proposed method is fast and efficient, making it a viable option for practical applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#23545; COVID-19 &#30123;&#24773;&#20013;&#30340;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#25512;&#25991;&#65292;&#21457;&#29616;&#20844;&#20247;&#20154;&#29289;&#30340;&#20449;&#24687;&#23545;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.16759</link><description>&lt;p&gt;
&#25506;&#31350;&#21517;&#20154;&#23545;&#20844;&#20247;&#24577;&#24230;&#24433;&#21709;&#30340;&#30740;&#31350;&#65306;&#22522;&#20110;&#31038;&#20132;&#23186;&#20307;&#24773;&#24863;&#20998;&#26512;&#30340; COVID-19 &#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploring celebrity influence on public attitude towards the COVID-19 pandemic: social media shared sentiment analysis. (arXiv:2303.16759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#20849;&#20139;&#30340;&#20449;&#24687;&#23545; COVID-19 &#30123;&#24773;&#20013;&#30340;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25910;&#38598;&#21644;&#20998;&#26512;&#25512;&#25991;&#65292;&#21457;&#29616;&#20844;&#20247;&#20154;&#29289;&#30340;&#20449;&#24687;&#23545;&#20844;&#20247;&#24773;&#24863;&#21644;&#22823;&#20247;&#24847;&#35265;&#20855;&#26377;&#26174;&#33879;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19 &#30123;&#24773;&#20026;&#20581;&#24247;&#27807;&#36890;&#24102;&#26469;&#20102;&#26032;&#26426;&#36935;&#65292;&#22686;&#21152;&#20102;&#20844;&#20247;&#20351;&#29992;&#22312;&#32447;&#28192;&#36947;&#33719;&#21462;&#19982;&#20581;&#24247;&#30456;&#20851;&#24773;&#32490;&#30340;&#26426;&#20250;&#12290;&#20154;&#20204;&#24050;&#32463;&#36716;&#21521;&#31038;&#20132;&#23186;&#20307;&#32593;&#32476;&#20998;&#20139;&#19982; COVID-19 &#30123;&#24773;&#24433;&#21709;&#30456;&#20851;&#30340;&#24773;&#24863;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20844;&#20247;&#20154;&#29289;&#65288;&#21363;&#36816;&#21160;&#21592;&#12289;&#25919;&#27835;&#23478;&#12289;&#26032;&#38395;&#24037;&#20316;&#32773;&#65289;&#20849;&#20139;&#30340;&#31038;&#20132;&#20449;&#24687;&#22312;&#20915;&#23450;&#25972;&#20307;&#20844;&#20849;&#35805;&#35821;&#26041;&#21521;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174; 2020 &#24180; 1 &#26376; 1 &#26085;&#21040; 2022 &#24180; 3 &#26376; 1 &#26085;&#25910;&#38598;&#20102;&#32422; 1300 &#19975;&#26465;&#25512;&#29305;&#12290;&#20351;&#29992;&#19968;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340; DistilRoBERTa &#27169;&#22411;&#35745;&#31639;&#20102;&#27599;&#26465;&#25512;&#25991;&#30340;&#24773;&#32490;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#27604;&#36739;&#19982;&#20844;&#20247;&#20154;&#29289;&#25552;&#21450;&#21516;&#26102;&#20986;&#29616;&#30340; COVID-19 &#30123;&#33495;&#30456;&#20851;&#25512;&#29305;&#21457;&#24067;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312; COVID-19 &#30123;&#24773;&#30340;&#21069;&#20004;&#24180;&#37324;&#65292;&#19982;&#20844;&#20247;&#20154;&#29289;&#20849;&#20139;&#30340;&#20449;&#24687;&#21516;&#26102;&#20986;&#29616;&#30340;&#24773;&#24863;&#20869;&#23481;&#20855;&#26377;&#19968;&#33268;&#30340;&#27169;&#24335;&#65292;&#24433;&#21709;&#20102;&#20844;&#20247;&#33286;&#35770;&#21644;&#22823;&#20247;&#12290;
&lt;/p&gt;
&lt;p&gt;
The COVID-19 pandemic has introduced new opportunities for health communication, including an increase in the public use of online outlets for health-related emotions. People have turned to social media networks to share sentiments related to the impacts of the COVID-19 pandemic. In this paper we examine the role of social messaging shared by Persons in the Public Eye (i.e. athletes, politicians, news personnel) in determining overall public discourse direction. We harvested approximately 13 million tweets ranging from 1 January 2020 to 1 March 2022. The sentiment was calculated for each tweet using a fine-tuned DistilRoBERTa model, which was used to compare COVID-19 vaccine-related Twitter posts (tweets) that co-occurred with mentions of People in the Public Eye. Our findings suggest the presence of consistent patterns of emotional content co-occurring with messaging shared by Persons in the Public Eye for the first two years of the COVID-19 pandemic influenced public opinion and larg
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#32034;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#65292;&#20197;&#36866;&#29992;&#20110;DRG&#20998;&#32452;&#65292;&#35299;&#20915;&#20102;&#28431;&#35786;&#38382;&#39064;&#23548;&#33268;&#21307;&#30103;&#35760;&#24405;&#19981;&#23436;&#25972;&#12289;&#24433;&#21709;DRG&#25307;&#29983;&#27491;&#30830;&#29575;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16757</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22914;&#20309;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#27979;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#20197;&#36866;&#29992;&#20110;DRG
&lt;/p&gt;
&lt;p&gt;
How can Deep Learning Retrieve the Write-Missing Additional Diagnosis from Chinese Electronic Medical Record For DRG. (arXiv:2303.16757v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#32034;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#65292;&#20197;&#36866;&#29992;&#20110;DRG&#20998;&#32452;&#65292;&#35299;&#20915;&#20102;&#28431;&#35786;&#38382;&#39064;&#23548;&#33268;&#21307;&#30103;&#35760;&#24405;&#19981;&#23436;&#25972;&#12289;&#24433;&#21709;DRG&#25307;&#29983;&#27491;&#30830;&#29575;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28431;&#35786;&#30340;&#26816;&#27979;&#30340;&#30446;&#30340;&#26159;&#25214;&#21040;&#24050;&#32463;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#28165;&#26224;&#35786;&#26029;&#20294;&#34987;&#28431;&#25481;&#30340;&#30142;&#30149;&#12290;&#19981;&#21516;&#20110;&#28431;&#35786;&#30340;&#23450;&#20041;&#65292;&#28431;&#35786;&#22312;&#21307;&#30103;&#35760;&#24405;&#20013;&#26126;&#26174;&#34920;&#29616;&#65292;&#26080;&#39035;&#36827;&#19968;&#27493;&#25512;&#29702;&#12290;&#28431;&#35786;&#38382;&#39064;&#24456;&#24120;&#35265;&#65292;&#36890;&#24120;&#26159;&#30001;&#20110;&#21307;&#29983;&#30095;&#24573;&#36896;&#25104;&#30340;&#12290;&#28431;&#35786;&#20250;&#23548;&#33268;&#21307;&#30103;&#35760;&#24405;&#30340;&#19981;&#23436;&#25972;&#24615;&#12290;&#22312;DRG&#30340;&#20998;&#32452;&#19979;&#65292;&#28431;&#35786;&#23558;&#38169;&#36807;&#37325;&#35201;&#30340;&#39069;&#22806;&#35786;&#26029;&#65288;CC&#65292;MCC&#65289;&#65292;&#20174;&#32780;&#24433;&#21709;DRG&#25307;&#29983;&#30340;&#27491;&#30830;&#29575;&#12290;&#22312;&#22269;&#23478;&#26222;&#36941;&#24320;&#22987;&#37319;&#29992;DRG&#25307;&#29983;&#21644;&#25903;&#20184;&#30340;&#24773;&#20917;&#19979;&#65292;&#28431;&#35786;&#38382;&#39064;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#24403;&#21069;&#30340;&#22522;&#20110;&#25163;&#21160;&#26041;&#27861;&#30001;&#20110;&#20840;&#38754;&#21307;&#30103;&#35760;&#24405;&#30340;&#22797;&#26434;&#20869;&#23481;&#32780;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#25454;&#25105;&#25152;&#30693;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20174;&#20013;&#22269;&#30005;&#23376;&#30149;&#21382;&#20013;&#26816;&#32034;&#28431;&#35786;&#30340;&#39069;&#22806;&#35786;&#26029;&#65292;&#20197;&#36866;&#29992;&#20110;DRG&#20998;&#32452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of write-missing diagnosis detection is to find diseases that have been clearly diagnosed from medical records but are missed in the discharge diagnosis. Unlike the definition of missed diagnosis, the write-missing diagnosis is clearly manifested in the medical record without further reasoning. The write-missing diagnosis is a common problem, often caused by physician negligence. The write-missing diagnosis will result in an incomplete diagnosis of medical records. While under DRG grouping, the write-missing diagnoses will miss important additional diagnoses (CC, MCC), thus affecting the correct rate of DRG enrollment.  Under the circumstance that countries generally start to adopt DRG enrollment and payment, the problem of write-missing diagnosis is a common and serious problem. The current manual-based method is expensive due to the complex content of the full medical record. We think this problem is suitable to be solved as natural language processing. But to the best of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16756</link><description>&lt;p&gt;
LLM&#29992;&#20110;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;: &#38754;&#21521;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
LLM for Patient-Trial Matching: Privacy-Aware Data Augmentation Towards Better Performance and Generalizability. (arXiv:2303.16756v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24739;&#32773;&#19982;&#36866;&#21512;&#30340;&#20020;&#24202;&#35797;&#39564;&#36827;&#34892;&#21305;&#37197;&#26159;&#25512;&#36827;&#21307;&#23398;&#30740;&#31350;&#21644;&#25552;&#20379;&#26368;&#20339;&#25252;&#29702;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#38754;&#20020;&#25968;&#25454;&#26631;&#20934;&#21270;&#12289;&#20262;&#29702;&#32771;&#34385;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#19982;&#20020;&#24202;&#35797;&#39564;&#26631;&#20934;&#20043;&#38388;&#20114;&#25805;&#20316;&#24615;&#32570;&#20047;&#31561;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#26469;&#25913;&#21892;EHRs&#21644;&#20020;&#24202;&#35797;&#39564;&#25551;&#36848;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;LLM&#30340;&#24739;&#32773;-&#35797;&#39564;&#21305;&#37197;&#65288;LLM-PTM&#65289;&#30340;&#38544;&#31169;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#24179;&#34913;&#20102;LLMs&#30340;&#22909;&#22788;&#65292;&#21516;&#26102;&#30830;&#20445;&#25935;&#24863;&#24739;&#32773;&#25968;&#25454;&#30340;&#23433;&#20840;&#21644;&#20445;&#23494;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;LLM-PTM&#26041;&#27861;&#65292;&#24615;&#33021;&#24179;&#22343;&#25552;&#39640;&#20102;7.32&#65285;&#65292;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#39640;&#20102;12.12&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
The process of matching patients with suitable clinical trials is essential for advancing medical research and providing optimal care. However, current approaches face challenges such as data standardization, ethical considerations, and a lack of interoperability between Electronic Health Records (EHRs) and clinical trial criteria. In this paper, we explore the potential of large language models (LLMs) to address these challenges by leveraging their advanced natural language generation capabilities to improve compatibility between EHRs and clinical trial descriptions. We propose an innovative privacy-aware data augmentation approach for LLM-based patient-trial matching (LLM-PTM), which balances the benefits of LLMs while ensuring the security and confidentiality of sensitive patient data. Our experiments demonstrate a 7.32% average improvement in performance using the proposed LLM-PTM method, and the generalizability to new data is improved by 12.12%. Additionally, we present case stud
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;</title><link>http://arxiv.org/abs/2303.16755</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#35268;&#27169;&#21270;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Language Models with Language Feedback at Scale. (arXiv:2303.16755v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#36890;&#36807;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#20197;&#29983;&#25104;&#26356;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#32463;&#24120;&#29983;&#25104;&#19981;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#30340;&#36755;&#20986;&#65292;&#20363;&#22914;&#26377;&#23475;&#30340;&#25991;&#26412;&#25110;&#20107;&#23454;&#19981;&#27491;&#30830;&#30340;&#25688;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#23398;&#20064;&#19968;&#31181;&#31616;&#21333;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#65288;&#21363;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#20043;&#38388;&#30340;&#27604;&#36739;&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#27604;&#36739;&#21453;&#39304;&#21482;&#33021;&#20256;&#36798;&#26377;&#38480;&#30340;&#20851;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65288;ILF&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#21453;&#39304;&#12290;ILF&#30001;&#19977;&#20010;&#36845;&#20195;&#27493;&#39588;&#32452;&#25104;&#65306;&#31532;&#19968;&#27493;&#65292;&#26681;&#25454;&#36755;&#20837;&#65292;&#21021;&#22987;LM&#36755;&#20986;&#21644;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35843;&#33410;&#20197;&#29983;&#25104;&#25913;&#36827;&#12290;&#31532;&#20108;&#27493;&#65292;&#36873;&#25321;&#26368;&#22810;&#21453;&#39304;&#30340;&#25913;&#36827;&#12290;&#31532;&#19977;&#27493;&#65292;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#26368;&#22823;&#21270;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#30340;&#25913;&#36827;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;ILF&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#31867;&#20284;&#20110;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;ILF&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models often generate outputs that are not in line with human preferences, such as harmful text or factually incorrect summaries. Recent work approaches the above issues by learning from a simple form of human feedback: comparisons between pairs of model-generated outputs. However, comparison feedback only conveys limited information about human preferences. In this paper, we introduce Imitation learning from Language Feedback (ILF), a new approach that utilizes more informative language feedback. ILF consists of three steps that are applied iteratively: first, conditioning the language model on the input, an initial LM output, and feedback to generate refinements. Second, selecting the refinement incorporating the most feedback. Third, finetuning the language model to maximize the likelihood of the chosen refinement given the input. We show theoretically that ILF can be viewed as Bayesian Inference, similar to Reinforcement Learning from human feedback. We evaluate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;MPO&#20998;&#35299;&#20849;&#20139;&#20013;&#22830;&#24352;&#37327;&#24182;&#20445;&#25345;&#23618;&#29305;&#23450;&#30340;&#36741;&#21161;&#24352;&#37327;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16753</link><description>&lt;p&gt;
&#20351;&#29992;&#21442;&#25968;&#26377;&#25928;&#30340;&#32467;&#26500;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#33267;&#26356;&#28145;&#30340;&#28145;&#24230;
&lt;/p&gt;
&lt;p&gt;
Scaling Pre-trained Language Models to Deeper via Parameter-efficient Architecture. (arXiv:2303.16753v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;MPO&#20998;&#35299;&#20849;&#20139;&#20013;&#22830;&#24352;&#37327;&#24182;&#20445;&#25345;&#23618;&#29305;&#23450;&#30340;&#36741;&#21161;&#24352;&#37327;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24230;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#30697;&#38453;&#20056;&#31215;&#31639;&#23376;&#65288;MPO&#65289;&#30340;&#26356;&#20855;&#33021;&#21147;&#30340;&#21442;&#25968;&#20849;&#20139;&#26550;&#26500;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#27169;&#22411;&#28145;&#24230;&#12290;&#36890;&#36807;MPO&#20998;&#35299;&#65292;&#25105;&#20204;&#30340;&#26550;&#26500;&#36328;&#25152;&#26377;&#23618;&#20849;&#20139;&#20013;&#22830;&#24352;&#37327;&#20197;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#65292;&#24182;&#20445;&#25345;&#23618;&#29305;&#23450;&#30340;&#36741;&#21161;&#24352;&#37327;&#20197;&#22686;&#24378;&#36866;&#24212;&#24615;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a highly parameter-efficient approach to scaling pre-trained language models (PLMs) to a deeper model depth. Unlike prior work that shares all parameters or uses extra blocks, we design a more capable parameter-sharing architecture based on matrix product operator (MPO). MPO decomposition can reorganize and factorize the information of a parameter matrix into two parts: the major part that contains the major information (central tensor) and the supplementary part that only has a small proportion of parameters (auxiliary tensors). Based on such a decomposition, our architecture shares the central tensor across all layers for reducing the model size and meanwhile keeps layer-specific auxiliary tensors (also using adapters) for enhancing the adaptation flexibility. To improve the model training, we further propose a stable initialization algorithm tailored for the MPO-based architecture. Extensive experiments have demonstrated the effectiveness of our proposed mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2303.16751</link><description>&lt;p&gt;
&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#31995;&#32479;&#65306;&#20174;&#31163;&#23130;&#26696;&#20214;&#20013;&#25552;&#21462;&#20107;&#20214;&#20197;&#26816;&#27979;&#35009;&#21028;&#20013;&#30340;&#20105;&#35758;
&lt;/p&gt;
&lt;p&gt;
Judicial Intelligent Assistant System: Extracting Events from Divorce Cases to Detect Disputes for the Judge. (arXiv:2303.16751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27665;&#20107;&#26696;&#20214;&#30340;&#27491;&#24335;&#31243;&#24207;&#20013;&#65292;&#30001;&#19981;&#21516;&#24403;&#20107;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#36164;&#26009;&#25551;&#36848;&#20102;&#26696;&#20214;&#30340;&#21457;&#23637;&#36807;&#31243;&#12290;&#20174;&#36825;&#20123;&#25991;&#26412;&#26448;&#26009;&#20013;&#25552;&#21462;&#26696;&#20214;&#30340;&#20851;&#38190;&#20449;&#24687;&#24182;&#28548;&#28165;&#30456;&#20851;&#24403;&#20107;&#20154;&#30340;&#20105;&#35758;&#28966;&#28857;&#26159;&#19968;&#39033;&#22256;&#38590;&#32780;&#24517;&#35201;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20004;&#36718;&#26631;&#27880;&#20107;&#20214;&#25552;&#21462;&#25216;&#26415;&#30340;&#31163;&#23130;&#26696;&#20214;&#20105;&#35758;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#25353;&#29031;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#21496;&#27861;&#26234;&#33021;&#21161;&#25163;&#65288;JIA&#65289;&#31995;&#32479;&#65292;&#20197;&#33258;&#21160;&#20174;&#31163;&#23130;&#26696;&#20214;&#26448;&#26009;&#20013;&#25552;&#21462;&#37325;&#28857;&#20107;&#20214;&#65292;&#36890;&#36807;&#35782;&#21035;&#20854;&#20013;&#30340;&#20849;&#25351;&#26469;&#23545;&#20107;&#20214;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#26816;&#27979;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
In formal procedure of civil cases, the textual materials provided by different parties describe the development process of the cases. It is a difficult but necessary task to extract the key information for the cases from these textual materials and to clarify the dispute focus of related parties. Currently, officers read the materials manually and use methods, such as keyword searching and regular matching, to get the target information. These approaches are time-consuming and heavily depending on prior knowledge and carefulness of the officers. To assist the officers to enhance working efficiency and accuracy, we propose an approach to detect disputes from divorce cases based on a two-round-labeling event extracting technique in this paper. We implement the Judicial Intelligent Assistant (JIA) system according to the proposed approach to 1) automatically extract focus events from divorce case materials, 2) align events by identifying co-reference among them, and 3) detect conflicts a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16749</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Improving Code Generation by Training with Natural Language Feedback. (arXiv:2303.16749v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16749
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;ILF&#65292;&#36890;&#36807;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#26469;&#26174;&#33879;&#25552;&#39640;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20063;&#21487;&#20197;&#33719;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#20808;&#35757;&#32451;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#30340;&#28508;&#21147;&#26159;&#26368;&#36817;&#30340;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Language Feedback&#65288;ILF&#65289;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#12290;ILF&#22312;&#35757;&#32451;&#26399;&#38388;&#20165;&#38656;&#35201;&#23569;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#21453;&#39304;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#26102;&#19981;&#38656;&#35201;&#30456;&#21516;&#30340;&#21453;&#39304;&#65292;&#22240;&#27492;&#20351;&#29992;&#36215;&#26469;&#26082;&#26041;&#20415;&#21448;&#39640;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;ILF&#21487;&#20197;&#34987;&#35270;&#20026;&#26368;&#23567;&#21270;&#19982;&#22522;&#20934;&#20998;&#24067;&#30340;KL&#25955;&#24230;&#30340;&#19968;&#31181;&#24418;&#24335;&#65292;&#24182;&#22312;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#12290;&#25105;&#20204;&#20351;&#29992;ILF&#22312;Mostly Basic Python Problems(MBPP)&#22522;&#20934;&#27979;&#35797;&#19978;&#23558;Codegen-Mono 6.1B&#27169;&#22411;&#30340;pass @ 1&#35206;&#30422;&#29575;&#30456;&#23545;&#25552;&#39640;&#20102;38%&#65288;&#32477;&#23545;&#25552;&#39640;&#20102;10%&#65289;&#65292;&#32988;&#36807;&#20102;&#22312;MBPP&#19978;&#24494;&#35843;&#21644;&#22312;&#20154;&#31867;&#20462;&#22797;&#30340;&#31243;&#24207;&#19978;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#21453;&#39304;&#65292;&#20174;&#20154;&#31867;&#32534;&#20889;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#20013;&#36827;&#34892;&#23398;&#20064;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential for pre-trained large language models (LLMs) to use natural language feedback at inference time has been an exciting recent development. We build upon this observation by formalizing an algorithm for learning from natural language feedback at training time instead, which we call Imitation learning from Language Feedback (ILF). ILF requires only a small amount of human-written feedback during training and does not require the same feedback at test time, making it both user-friendly and sample-efficient. We further show that ILF can be seen as a form of minimizing the KL divergence to the ground truth distribution and demonstrate a proof-of-concept on a neural program synthesis task. We use ILF to improve a Codegen-Mono 6.1B model's pass@1 rate by 38% relative (and 10% absolute) on the Mostly Basic Python Problems (MBPP) benchmark, outperforming both fine-tuning on MBPP and fine-tuning on repaired programs written by humans. Overall, our results suggest that learning from h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#35780;&#20272;&#30340;&#20851;&#38190;&#26415;&#35821;&#21644;&#21306;&#21035;&#65292;&#24182;&#38416;&#36848;&#20102;INLG&#35780;&#20272;&#26368;&#20339;&#35770;&#25991;&#22870;&#35774;&#31435;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.16742</link><description>&lt;p&gt;
&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#31995;&#32479;&#65306;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Evaluating NLG systems: A brief introduction. (arXiv:2303.16742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20013;&#35780;&#20272;&#30340;&#20851;&#38190;&#26415;&#35821;&#21644;&#21306;&#21035;&#65292;&#24182;&#38416;&#36848;&#20102;INLG&#35780;&#20272;&#26368;&#20339;&#35770;&#25991;&#22870;&#35774;&#31435;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#24180;&#65292;&#22269;&#38469;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#20250;&#35758;&#65288;INLG&#65289;&#23558;&#35780;&#36873;&#35780;&#20272;&#26368;&#20339;&#35770;&#25991;&#22870;&#39033;&#12290;&#35813;&#22870;&#39033;&#30340;&#30446;&#30340;&#26159;&#20026;&#20102;&#28608;&#21169;NLG&#30740;&#31350;&#32773;&#20204;&#26356;&#21152;&#20851;&#27880;&#22914;&#20309;&#35780;&#20272;&#20182;&#20204;&#31995;&#32479;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#30701;&#30340;NLG&#35780;&#20272;&#23548;&#35770;&#65292;&#35299;&#37322;&#20102;&#20851;&#38190;&#26415;&#35821;&#21644;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This year the International Conference on Natural Language Generation (INLG) will feature an award for the paper with the best evaluation. The purpose of this award is to provide an incentive for NLG researchers to pay more attention to the way they assess the output of their systems. This essay provides a short introduction to evaluation in NLG, explaining key terms and distinctions.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27010;&#36848;&#20102;&#31185;&#25216;&#20889;&#20316;&#20013;&#30340;&#25991;&#26412;&#20462;&#35746;&#65292;&#21253;&#25324;&#31185;&#23398;&#20889;&#20316;&#30340;&#29305;&#28857;&#12289;&#24120;&#35265;&#26684;&#24335;&#21644;&#32422;&#23450;&#65292;&#20197;&#21450;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20462;&#35746;&#20889;&#20316;&#36741;&#21161;&#24037;&#20855;&#65292;&#28982;&#32780;&#36825;&#20123;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20855;&#21487;&#35775;&#38382;&#24615;&#12289;&#23545;&#19978;&#19979;&#25991;&#30340;&#26377;&#38480;&#32771;&#34385;&#21644;&#23545;&#35805;&#20449;&#24687;&#30340;&#21547;&#20041;&#19981;&#28165;&#31561;&#12290;</title><link>http://arxiv.org/abs/2303.16726</link><description>&lt;p&gt;
&#31185;&#25216;&#20889;&#20316;&#36741;&#21161;&#20013;&#30340;&#25991;&#26412;&#20462;&#35746;&#65306;&#27010;&#36848;
&lt;/p&gt;
&lt;p&gt;
Text revision in Scientific Writing Assistance: An Overview. (arXiv:2303.16726v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31185;&#25216;&#20889;&#20316;&#20013;&#30340;&#25991;&#26412;&#20462;&#35746;&#65292;&#21253;&#25324;&#31185;&#23398;&#20889;&#20316;&#30340;&#29305;&#28857;&#12289;&#24120;&#35265;&#26684;&#24335;&#21644;&#32422;&#23450;&#65292;&#20197;&#21450;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#20462;&#35746;&#20889;&#20316;&#36741;&#21161;&#24037;&#20855;&#65292;&#28982;&#32780;&#36825;&#20123;&#24037;&#20855;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#24037;&#20855;&#21487;&#35775;&#38382;&#24615;&#12289;&#23545;&#19978;&#19979;&#25991;&#30340;&#26377;&#38480;&#32771;&#34385;&#21644;&#23545;&#35805;&#20449;&#24687;&#30340;&#21547;&#20041;&#19981;&#28165;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#35770;&#25991;&#25776;&#20889;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#26159;&#19968;&#31181;&#39640;&#24230;&#35268;&#33539;&#21270;&#30340;&#25991;&#20307;&#12290;&#33391;&#22909;&#30340;&#20889;&#20316;&#25216;&#24039;&#23545;&#20110;&#24688;&#24403;&#22320;&#20256;&#36798;&#30740;&#31350;&#24037;&#20316;&#30340;&#24605;&#24819;&#21644;&#32467;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#31185;&#23398;&#25991;&#31456;&#30446;&#21069;&#26159;&#29992;&#33521;&#35821;&#20889;&#30340;&#65292;&#22240;&#27492;&#38750;&#33521;&#35821;&#27597;&#35821;&#30340;&#20154;&#38754;&#20020;&#30340;&#35821;&#35328;&#38382;&#39064;&#26356;&#21152;&#22256;&#38590;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#31185;&#23398;&#39046;&#22495;&#20889;&#20316;&#36741;&#21161;&#20013;&#25991;&#26412;&#20462;&#35746;&#30340;&#27010;&#36848;&#12290;&#25105;&#20204;&#23558;&#23457;&#26597;&#31185;&#23398;&#20889;&#20316;&#30340;&#29305;&#28857;&#65292;&#21253;&#25324;&#22312;&#30740;&#31350;&#25991;&#31456;&#20013;&#24120;&#29992;&#30340;&#26684;&#24335;&#21644;&#32422;&#23450;&#12290;&#27492;&#22806;&#65292;&#36825;&#31687;&#27010;&#36848;&#23558;&#25506;&#35752;&#25991;&#26412;&#20462;&#35746;&#30340;&#21508;&#31181;&#31867;&#22411;&#30340;&#20889;&#20316;&#36741;&#21161;&#24037;&#20855;&#12290;&#23613;&#31649;&#36825;&#20123;&#24037;&#20855;&#32972;&#21518;&#30340;&#25216;&#26415;&#32463;&#36807;&#20102;&#22810;&#24180;&#30340;&#21457;&#23637;&#65292;&#20174;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#21040;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#30340;&#26041;&#27861;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#65288;&#24037;&#20855;&#21487;&#35775;&#38382;&#24615;&#65292;&#23545;&#19978;&#19979;&#25991;&#30340;&#26377;&#38480;&#32771;&#34385;&#65292;&#23545;&#35805;&#20449;&#24687;&#30340;&#21547;&#20041;&#19981;&#28165;&#31561;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Writing a scientific article is a challenging task as it is a highly codified genre. Good writing skills are essential to properly convey ideas and results of research work. Since the majority of scientific articles are currently written in English, this exercise is all the more difficult for non-native English speakers as they additionally have to face language issues. This article aims to provide an overview of text revision in writing assistance in the scientific domain.  We will examine the specificities of scientific writing, including the format and conventions commonly used in research articles.  Additionally, this overview will explore the various types of writing assistance tools available for text revision. Despite the evolution of the technology behind these tools through the years, from rule-based approaches to deep neural-based ones, challenges still exist (tools' accessibility, limited consideration of the context, inexplicit use of discursive information, etc.)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#29305;&#23450;&#20449;&#24687;&#20256;&#25773;&#21518;&#22312;&#32447;&#35752;&#35770;&#21464;&#21270;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#29615;&#22659;&#26032;&#38395;&#21457;&#24067;&#21644;&#27668;&#20505;&#21464;&#21270;&#25512;&#25991;&#26469;&#25581;&#31034;&#20102;&#25112;&#30053;&#20256;&#25773;&#30340;&#21709;&#24212;&#21576;&#37325;&#23614;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2303.16694</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25991;&#26412;&#23884;&#20837;&#34913;&#37327;&#25112;&#30053;&#20256;&#25773;&#30340;&#31038;&#20132;&#23186;&#20307;&#22238;&#21709;
&lt;/p&gt;
&lt;p&gt;
Using Semantic Similarity and Text Embedding to Measure the Social Media Echo of Strategic Communications. (arXiv:2303.16694v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#26469;&#34913;&#37327;&#29305;&#23450;&#20449;&#24687;&#20256;&#25773;&#21518;&#22312;&#32447;&#35752;&#35770;&#21464;&#21270;&#30340;&#26032;&#25216;&#26415;&#65292;&#24182;&#20351;&#29992;&#29615;&#22659;&#26032;&#38395;&#21457;&#24067;&#21644;&#27668;&#20505;&#21464;&#21270;&#25512;&#25991;&#26469;&#25581;&#31034;&#20102;&#25112;&#30053;&#20256;&#25773;&#30340;&#21709;&#24212;&#21576;&#37325;&#23614;&#20998;&#24067;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#35752;&#35770;&#28041;&#21450;&#21508;&#31181;&#35805;&#39064;&#65292;&#35768;&#22810;&#21442;&#19982;&#32773;&#36890;&#36807;&#31934;&#24515;&#21046;&#20316;&#30340;&#20449;&#24687;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#27963;&#21160;&#24433;&#21709;&#22312;&#32447;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#22312;&#32447;&#23186;&#20307;&#20869;&#23481;&#30340;&#35268;&#27169;&#21644;&#22810;&#26679;&#24615;&#20351;&#24471;&#35780;&#20272;&#29305;&#23450;&#20449;&#24687;&#30340;&#24433;&#21709;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#20041;&#30456;&#20284;&#24615;&#23450;&#37327;&#34913;&#37327;&#29305;&#23450;&#20449;&#24687;&#21457;&#24067;&#21518;&#35752;&#35770;&#21464;&#21270;&#30340;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#20351;&#29992;&#29615;&#22659;&#32452;&#32455;&#30340;&#26032;&#38395;&#21457;&#24067;&#21644;&#27668;&#20505;&#21464;&#21270;&#36777;&#35770;&#20013;&#30340;&#25512;&#25991;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#25581;&#31034;&#20102;&#25112;&#30053;&#20256;&#25773;&#30340;&#22312;&#32447;&#35752;&#35770;&#21709;&#24212;&#21576;&#37325;&#23614;&#20998;&#24067;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online discourse covers a wide range of topics and many actors tailor their content to impact online discussions through carefully crafted messages and targeted campaigns. Yet the scale and diversity of online media content make it difficult to evaluate the impact of a particular message. In this paper, we present a new technique that leverages semantic similarity to quantify the change in the discussion after a particular message has been published. We use a set of press releases from environmental organisations and tweets from the climate change debate to show that our novel approach reveals a heavy-tailed distribution of response in online discourse to strategic communications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21360;&#24230;&#35821;&#35328;&#20316;&#20026;&#28304;&#21644;&#30446;&#26631;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#35821;&#35328;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#36827;&#34892;&#25688;&#35201;&#30340;&#34920;&#29616;&#65292;&#24182;&#25253;&#21578;&#20102;ROUGE&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2303.16657</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#35821;&#35328;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#24635;&#32467;&#21360;&#24230;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
Summarizing Indian Languages using Multilingual Transformers based Models. (arXiv:2303.16657v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21360;&#24230;&#35821;&#35328;&#20316;&#20026;&#28304;&#21644;&#30446;&#26631;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#22810;&#35821;&#35328;&#21464;&#24418;&#37329;&#21018;&#27169;&#22411;&#36827;&#34892;&#25688;&#35201;&#30340;&#34920;&#29616;&#65292;&#24182;&#25253;&#21578;&#20102;ROUGE&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31867;&#20284;mBART&#12289;mT5&#12289;IndicBART&#31561;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#20302;&#36164;&#28304;&#21360;&#24230;&#35821;&#35328;&#30340;&#24635;&#32467;&#27491;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20294;&#26159;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25968;&#37327;&#20173;&#28982;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;( HakunaMatata&#22242;&#38431;)&#30740;&#31350;&#20102;&#36825;&#20123;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21360;&#24230;&#35821;&#35328;&#20316;&#20026;&#28304;&#21644;&#30446;&#26631;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25688;&#35201;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;IndicBART&#21644;mT5&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#25253;&#21578;ROUGE-1&#12289;ROUGE-2&#12289;ROUGE-3&#21644;ROUGE-4&#20998;&#25968;&#20316;&#20026;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of multilingual models like mBART, mT5, IndicBART etc., summarization in low resource Indian languages is getting a lot of attention now a days. But still the number of datasets is low in number. In this work, we (Team HakunaMatata) study how these multilingual models perform on the datasets which have Indian languages as source and target text while performing summarization. We experimented with IndicBART and mT5 models to perform the experiments and report the ROUGE-1, ROUGE-2, ROUGE-3 and ROUGE-4 scores as a performance metric.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16634</link><description>&lt;p&gt;
GPTEval&#65306;&#20351;&#29992;GPT-4&#21644;&#26356;&#22909;&#30340;&#20154;&#31867;&#23545;&#40784;&#26469;&#35780;&#20272;NLG
&lt;/p&gt;
&lt;p&gt;
GPTEval: NLG Evaluation using GPT-4 with Better Human Alignment. (arXiv:2303.16634v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GPTEval&#65292;&#19968;&#20010;&#21033;&#29992;&#38142;&#24335;&#24605;&#32771;&#21644;&#24418;&#24335;&#22635;&#20805;&#35780;&#20215;NLG&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#25991;&#26412;&#25688;&#35201;&#20219;&#21153;&#20013;&#65292;GPTEval&#32467;&#21512;GPT-4&#21462;&#24471;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#31995;&#32479;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#24456;&#38590;&#36827;&#34892;&#33258;&#21160;&#27979;&#37327;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#21442;&#32771;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;BLEU&#21644;ROUGE&#24050;&#34987;&#35777;&#26126;&#22312;&#38656;&#35201;&#21019;&#36896;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#20219;&#21153;&#20013;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#30456;&#23545;&#36739;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24314;&#35758;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#26080;&#21442;&#32771;&#30340;NLG&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#27169;&#22411;&#36866;&#29992;&#20110;&#32570;&#20047;&#20154;&#31867;&#21442;&#32771;&#30340;&#26032;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#35780;&#20272;&#22120;&#20173;&#28982;&#27604;&#20013;&#31561;&#35268;&#27169;&#30340;&#31070;&#32463;&#35780;&#20272;&#22120;&#30340;&#20154;&#31867;&#23545;&#24212;&#24230;&#20302;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GPTEval&#65292;&#19968;&#20010;&#20351;&#29992;&#38142;&#24335;&#24605;&#32771;&#65288;CoT&#65289;&#21644;&#24418;&#24335;&#22635;&#20805;&#33539;&#24335;&#26469;&#35780;&#20272;NLG&#36755;&#20986;&#36136;&#37327;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#38024;&#23545;&#20004;&#20010;&#29983;&#25104;&#20219;&#21153;&#65292;&#25991;&#26412;&#25688;&#35201;&#21644;&#23545;&#35805;&#29983;&#25104;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;GPTEval&#32467;&#21512;GPT-4&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#22312;&#25688;&#35201;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;0.514&#30340;&#26031;&#30382;&#23572;&#26364;&#30456;&#20851;&#31995;&#25968;&#65292;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present GPTEval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that GPTEval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperform
&lt;/p&gt;</description></item><item><title>AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16621</link><description>&lt;p&gt;
AraSpot&#65306;&#38463;&#25289;&#20271;&#35821;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
AraSpot: Arabic Spoken Command Spotting. (arXiv:2303.16621v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16621
&lt;/p&gt;
&lt;p&gt;
AraSpot&#26159;&#19968;&#27454;&#20351;&#29992;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#30340;&#35757;&#32451;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#20197;99.59%&#30340;&#20934;&#30830;&#29575;&#36229;&#20986;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21475;&#35821;&#20851;&#38190;&#35782;&#21035;&#65288;KWS&#65289;&#26159;&#25351;&#22312;&#38899;&#39057;&#27969;&#20013;&#35782;&#21035;&#20851;&#38190;&#35789;&#65292;&#24191;&#27867;&#29992;&#20110;&#26234;&#33021;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#20197;&#21551;&#21160;&#35821;&#38899;&#21161;&#25163;&#21644;&#36827;&#34892;&#20813;&#25552;&#20219;&#21153;&#12290;&#35813;&#20219;&#21153;&#38754;&#20020;&#30528;&#39640;&#31934;&#24230;&#21644;&#22312;&#20302;&#21151;&#29575;&#21644;&#21487;&#33021;&#30340;&#26377;&#38480;&#35745;&#31639;&#33021;&#21147;&#35774;&#22791;&#19978;&#20445;&#25345;&#31995;&#32479;&#36816;&#34892;&#25928;&#29575;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#19981;&#21516;&#30340;&#22312;&#32447;&#25968;&#25454;&#22686;&#24378;&#21644;&#24341;&#20837;ConformerGRU&#27169;&#22411;&#26550;&#26500;&#30340;AraSpot&#65292;&#29992;&#20110;&#35757;&#32451;40&#20010;&#38463;&#25289;&#20271;&#35821;&#20851;&#38190;&#35789;&#30340;&#35782;&#21035;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#36827;&#34892;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;AraSpot&#20197;SOTA 99.59&#65285;&#36229;&#36807;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spoken keyword spotting (KWS) is the task of identifying a keyword in an audio stream and is widely used in smart devices at the edge in order to activate voice assistants and perform hands-free tasks. The task is daunting as there is a need, on the one hand, to achieve high accuracy while at the same time ensuring that such systems continue to run efficiently on low power and possibly limited computational capabilities devices. This work presents AraSpot for Arabic keyword spotting trained on 40 Arabic keywords, using different online data augmentation, and introducing ConformerGRU model architecture. Finally, we further improve the performance of the model by training a text-to-speech model for synthetic data generation. AraSpot achieved a State-of-the-Art SOTA 99.59% result outperforming previous approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2303.16618</link><description>&lt;p&gt;
&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#20016;&#23500;&#30340;&#20803;&#25968;&#25454;&#27880;&#37322;&#30340;&#20449;&#24687;&#36827;&#34892;&#23631;&#24149;&#35282;&#33394;&#30340;&#20010;&#24615;&#21270;&#35821;&#35328;&#24314;&#27169;&#65292;&#27979;&#35797;&#34920;&#26126;&#36825;&#26679;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26500;&#24314;&#65292;&#21363;&#20351;&#23545;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35828;&#23478;&#20063;&#21487;&#20197;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#20026;&#23545;&#35805;&#25935;&#24863;&#65292;&#33021;&#26356;&#22909;&#22320;&#25429;&#25417;&#29305;&#23450;&#29305;&#24449;&#30340;&#20154;&#21592;&#21644;/&#25110;&#29305;&#23450;&#29615;&#22659;&#20013;&#30340;&#35828;&#35805;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#20016;&#23500;&#30340;&#35282;&#33394;&#27880;&#37322;&#38590;&#20197;&#24471;&#21040;&#21644;&#25104;&#21151;&#21033;&#29992;&#12290;&#22312;&#27492;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#24182;&#25551;&#36848;&#20102;&#19968;&#32452;&#26032;&#39062;&#30340;&#25163;&#21160;&#27880;&#37322;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27969;&#34892;&#30340; Cornell &#30005;&#24433;&#23545;&#35805;&#35821;&#26009;&#24211;&#30340; 863 &#21517;&#28436;&#35762;&#32773;&#65292;&#21253;&#25324;&#29305;&#24449;&#24341;&#29992;&#21644;&#35282;&#33394;&#25551;&#36848;&#65292;&#20197;&#21450;&#36229;&#36807; 95&#65285; &#30340;&#29305;&#33394;&#30005;&#24433;&#30340;&#19968;&#32452;&#33258;&#21160;&#25552;&#21462;&#30340;&#20845;&#20010;&#20803;&#25968;&#25454;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;&#27492;&#31867;&#27880;&#37322;&#26469;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#23558;&#22256;&#24785;&#20943;&#23569;&#39640;&#36798; 8.5&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29978;&#33267;&#21487;&#20197;&#24212;&#29992;&#20110;&#38646;&#26679;&#26412;&#30340;&#28436;&#35762;&#32773;&#65292;&#21363;&#23545;&#20110;&#27809;&#26377;&#20808;&#21069;&#22521;&#35757;&#25968;&#25454;&#30340;&#28436;&#35762;&#32773;&#65292;&#20381;&#36182;&#20110;&#35282;&#33394;&#30340;&#20154;&#21475;&#29305;&#24449;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#25910;&#38598;&#27492;&#31867;&#20803;&#25968;&#25454;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#25105;&#20204;&#36824;&#36129;&#29486;&#20102;&#19968;&#39033;&#25104;&#26412;&#25928;&#30410;&#20998;&#26512;&#65292;&#20197;&#31361;&#20986;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
Personalisation of language models for dialogue sensitises them to better capture the speaking patterns of people of specific characteristics, and/or in specific environments. However, rich character annotations are difficult to come by and to successfully leverage. In this work, we release and describe a novel set of manual annotations for 863 speakers from the popular Cornell Movie Dialog Corpus, including features like characteristic quotes and character descriptions, and a set of six automatically extracted metadata for over 95% of the featured films. We perform extensive experiments on two corpora and show that such annotations can be effectively used to personalise language models, reducing perplexity by up to 8.5%. Our method can be applied even zero-shot for speakers for whom no prior training data is available, by relying on combinations of characters' demographic characteristics. Since collecting such metadata is costly, we also contribute a cost-benefit analysis to highlight
&lt;/p&gt;</description></item><item><title>LMExplainer&#26159;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27169;&#22359;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;&#20851;&#38190;&#20915;&#31574;&#20449;&#21495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2303.16537</link><description>&lt;p&gt;
LMExplainer&#65306;&#19968;&#31181;&#21152;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#33021;&#21147;&#30340;&#30693;&#35782;&#25552;&#21319;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
LMExplainer: a Knowledge-Enhanced Explainer for Language Models. (arXiv:2303.16537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16537
&lt;/p&gt;
&lt;p&gt;
LMExplainer&#26159;&#19968;&#31181;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#27169;&#22359;&#65292;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;&#20851;&#38190;&#20915;&#31574;&#20449;&#21495;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24040;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-4&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#21487;&#20197;&#22788;&#29702;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#23618;&#38750;&#32447;&#24615;&#27169;&#22411;&#32467;&#26500;&#21644;&#25968;&#30334;&#19975;&#20010;&#21442;&#25968;&#65292;&#24456;&#38590;&#35299;&#37322;&#20854;&#32467;&#26524;&#12290;&#23545;&#20110;&#29992;&#25143;&#32780;&#35328;&#65292;&#20102;&#35299;&#27169;&#22411;&#30340;&#24037;&#20316;&#26041;&#24335;&#32570;&#20047;&#29702;&#35299;&#65292;&#21487;&#33021;&#20351;&#27169;&#22411;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#19981;&#21487;&#38752;&#24615;&#21644;&#21361;&#38505;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#24037;&#20316;&#21033;&#29992;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#25552;&#20379;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35299;&#37322;&#26080;&#27861;&#25903;&#25345;&#19981;&#26029;&#22686;&#38271;&#30340;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#26080;&#27861;&#25512;&#29702;&#20854;&#20915;&#31574;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LMExplainer&#65292;&#19968;&#31181;&#20026;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#35299;&#37322;&#30340;&#30693;&#35782;&#22686;&#24378;&#27169;&#22359;&#12290;&#25105;&#20204;&#20351;&#29992;&#30693;&#35782;&#22270;&#21644;&#22270;&#27880;&#24847;&#21147;&#31070;&#32463;&#32593;&#32476;&#26469;&#25552;&#21462;LM&#30340;&#20851;&#38190;&#20915;&#31574;&#20449;&#21495;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25506;&#35752;&#35299;&#37322;&#33021;&#21542;&#20063;&#24110;&#21161;&#20154;&#24037;&#26234;&#33021;&#26356;&#22909;&#22320;&#29702;&#35299;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) such as GPT-4 are very powerful and can process different kinds of natural language processing (NLP) tasks. However, it can be difficult to interpret the results due to the multi-layer nonlinear model structure and millions of parameters. Lack of understanding of how the model works can make the model unreliable and dangerous for everyday users in real-world scenarios. Most recent works exploit the weights of attention to provide explanations for model predictions. However, pure attention-based explanation is unable to support the growing complexity of the models, and cannot reason about their decision-making processes. Thus, we propose LMExplainer, a knowledge-enhanced interpretation module for language models that can provide human-understandable explanations. We use a knowledge graph (KG) and a graph attention neural network to extract the key decision signals of the LM. We further explore whether interpretation can also help AI understand the task better
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#23433;&#20840;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#26631;&#20934;&#27861;&#35268;&#31561;&#26041;&#38754;&#30340;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;&#25299;&#23637;&#20154;&#20204;&#23545;&#35813;&#25216;&#26415;&#30340;&#29702;&#35299;&#21644;&#23547;&#25214;&#26032;&#30340;&#29992;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.16528</link><description>&lt;p&gt;
&#24314;&#31435;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Building a Knowledge Graph of Distributed Ledger Technologies. (arXiv:2303.16528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;&#23433;&#20840;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#26631;&#20934;&#27861;&#35268;&#31561;&#26041;&#38754;&#30340;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#26377;&#21161;&#20110;&#25299;&#23637;&#20154;&#20204;&#23545;&#35813;&#25216;&#26415;&#30340;&#29702;&#35299;&#21644;&#23547;&#25214;&#26032;&#30340;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#21644;&#25104;&#21151;&#25512;&#24191;&#65292;&#23588;&#20854;&#26159;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#20154;&#20204;&#23545;&#35813;&#25216;&#26415;&#21450;&#20854;&#33021;&#21147;&#30340;&#21508;&#31181;&#35823;&#35299;&#65292;&#22240;&#20026;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#34987;&#35270;&#20316;&#21516;&#20041;&#35789;&#65292;&#20854;&#20182;&#29992;&#36884;&#20063;&#34987;&#32463;&#24120;&#24573;&#35270;&#12290;&#22240;&#27492;&#65292;&#23545;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#35748;&#35782;&#21644;&#24212;&#29992;&#34987;&#38480;&#21046;&#20110;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#39046;&#22495;&#12290;&#29616;&#26377;&#30340;&#35789;&#27719;&#34920;&#21644;&#26412;&#20307;&#35770;&#24448;&#24448;&#21482;&#20851;&#27880;&#25216;&#26415;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#26377;&#26102;&#29978;&#33267;&#21482;&#20851;&#27880;&#21333;&#20010;&#20135;&#21697;&#65292;&#36825;&#21487;&#33021;&#20250;&#24573;&#30053;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#21450;&#20854;&#28508;&#22312;&#30340;&#29992;&#36884;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20998;&#31867;&#36134;&#25216;&#26415;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#26412;&#20307;&#35770;&#65292;&#21253;&#25324;&#23433;&#20840;&#32771;&#34385;&#12289;&#24212;&#29992;&#39046;&#22495;&#12289;&#30456;&#20851;&#26631;&#20934;&#21644;&#27861;&#35268;&#31561;&#65292;&#20197;&#25913;&#21892;&#23545;&#35813;&#25216;&#26415;&#30340;&#25972;&#20307;&#29702;&#35299;&#65292;&#24182;&#26377;&#21161;&#20110;&#21457;&#29616;&#21306;&#22359;&#38142;&#21644;&#21152;&#23494;&#36135;&#24065;&#20197;&#22806;&#30340;&#26032;&#29992;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed ledger systems have become more prominent and successful in recent years, with a focus on blockchains and cryptocurrency. This has led to various misunderstandings about both the technology itself and its capabilities, as in many cases blockchain and cryptocurrency is used synonymously and other applications are often overlooked. Therefore, as a whole, the view of distributed ledger technology beyond blockchains and cryptocurrencies is very limited. Existing vocabularies and ontologies often focus on single aspects of the technology, or in some cases even just on one product. This potentially leads to other types of distributed ledgers and their possible use cases being neglected. In this paper, we present a knowledge graph and an ontology for distributed ledger technologies, which includes security considerations to model aspects such as threats and vulnerabilities, application domains, as well as relevant standards and regulations. Such a knowledge graph improves the over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21542;&#23450;&#21644;&#35282;&#33394;&#21453;&#36716;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36807;&#21435;&#30340;&#32467;&#35770;&#21487;&#33021;&#34987;&#23567;&#22411;&#27979;&#35797;&#38598;&#35823;&#23548;&#12290;&#21516;&#26102;&#65292;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16445</link><description>&lt;p&gt;
&#26356;&#22823;&#30340;&#25506;&#38024;&#35762;&#36848;&#19981;&#21516;&#30340;&#25925;&#20107;: &#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#24515;&#29702;&#35821;&#35328;&#23398;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Larger Probes Tell a Different Story: Extending Psycholinguistic Datasets Via In-Context Learning. (arXiv:2303.16445v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16445
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25193;&#23637;&#21542;&#23450;&#21644;&#35282;&#33394;&#21453;&#36716;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#36807;&#21435;&#30340;&#32467;&#35770;&#21487;&#33021;&#34987;&#23567;&#22411;&#27979;&#35797;&#38598;&#35823;&#23548;&#12290;&#21516;&#26102;&#65292;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#25506;&#27979;&#36890;&#24120;&#29992;&#26469;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#30340;&#29305;&#23450;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#25506;&#27979;&#22522;&#20934;&#23567;&#19988;&#32570;&#20047;&#32479;&#35745;&#21151;&#25928;&#26102;&#65292;&#36825;&#31867;&#30740;&#31350;&#30340;&#32467;&#35770;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21463;&#24515;&#29702;&#35821;&#35328;&#23398;&#30740;&#31350;&#21551;&#21457;&#30340;&#21542;&#23450;&#65288;NEG-1500-SIMP&#65289;&#21644;&#35282;&#33394;&#21453;&#36716;&#65288;ROLE-1500&#65289;&#30340;&#26032;&#30340;&#12289;&#26356;&#22823;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;GPT3&#23558;&#29616;&#26377;&#30340;NEG-136&#21644;ROLE-88&#22522;&#20934;&#36827;&#34892;&#20102;&#22823;&#24133;&#25193;&#23637;&#65292;&#23558;&#23427;&#20204;&#30340;&#35268;&#27169;&#20174;18&#21644;44&#20010;&#21477;&#23545;&#20998;&#21035;&#22686;&#21152;&#21040;&#20102;750&#20010;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#21478;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#27169;&#26495;&#30340;&#29983;&#25104;&#21019;&#24314;&#30340;&#25193;&#23637;&#21542;&#23450;&#25968;&#25454;&#38598;(NEG-1500-SIMP-TEMP)&#65292;&#23427;&#30001;770&#20010;&#21477;&#23545;&#32452;&#25104;&#12290;&#25105;&#20204;&#22312;&#25193;&#23637;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;22&#20010;&#27169;&#22411;&#65292;&#21457;&#29616;&#27169;&#22411;&#24615;&#33021;&#19982;&#21407;&#22987;&#36739;&#23567;&#22522;&#20934;&#30456;&#27604;&#19979;&#38477;&#20102;20-57%&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;BERT&#21644;ALBERT&#31561;&#27169;&#22411;&#20855;&#26377;&#36739;&#39640;&#30340;&#21542;&#23450;&#25935;&#24863;&#24615;&#65292;&#36825;&#34920;&#26126;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#21487;&#33021;&#30001;&#20110;&#36739;&#23567;&#30340;&#27979;&#35797;&#38598;&#32780;&#23384;&#22312;&#35823;&#24046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;GPT3&#29983;&#25104;&#20102;&#25152;&#26377;&#30340;&#23454;&#20363;&#65292;&#20294;&#21477;&#23376;&#30340;&#35821;&#27861;&#36136;&#37327;&#21463;&#21040;&#19968;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model probing is often used to test specific capabilities of these models. However, conclusions from such studies may be limited when the probing benchmarks are small and lack statistical power. In this work, we introduce new, larger datasets for negation (NEG-1500-SIMP) and role reversal (ROLE-1500) inspired by psycholinguistic studies. We dramatically extend existing NEG-136 and ROLE-88 benchmarks using GPT3, increasing their size from 18 and 44 sentence pairs to 750 each. We also create another version of extended negation dataset (NEG-1500-SIMP-TEMP), created using template-based generation. It consists of 770 sentence pairs. We evaluate 22 models on the extended datasets, seeing model performance dip 20-57% compared to the original smaller benchmarks. We observe high levels of negation sensitivity in models like BERT and ALBERT demonstrating that previous findings might have been skewed due to smaller test sets. Finally, we observe that while GPT3 has generated all the ex
&lt;/p&gt;</description></item><item><title>TaskMatrix.AI&#26159;&#19968;&#20010;&#20219;&#21153;&#23436;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#65292;&#25552;&#39640;&#23436;&#25104;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#20840;&#38754;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16434</link><description>&lt;p&gt;
TaskMatrix.AI&#65306;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#20197;&#23436;&#25104;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs. (arXiv:2303.16434v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16434
&lt;/p&gt;
&lt;p&gt;
TaskMatrix.AI&#26159;&#19968;&#20010;&#20219;&#21153;&#23436;&#25104;&#31995;&#32479;&#65292;&#21487;&#20197;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#65292;&#25552;&#39640;&#23436;&#25104;&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#20840;&#38754;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#23436;&#25104;&#31995;&#32479;TaskMatrix.AI&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23558;&#22522;&#30784;&#27169;&#22411;&#19982;&#25968;&#30334;&#19975;&#20010;API&#36830;&#25509;&#20197;&#23436;&#25104;&#19981;&#21516;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;TaskMatrix.AI&#20351;&#29992;API&#25968;&#25454;&#38598;&#26469;&#23558;&#22522;&#30784;&#27169;&#22411;&#30340;&#36755;&#20837;&#19982;&#21487;&#29992;&#30340;API&#36827;&#34892;&#21305;&#37197;&#65292;&#20174;&#32780;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#24320;&#25918;&#22495;&#21644;&#39046;&#22495;&#29305;&#23450;&#20219;&#21153;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;TaskMatrix.AI&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has made incredible progress recently. On the one hand, advanced foundation models like ChatGPT can offer powerful conversation, in-context learning and code generation abilities on a broad range of open-domain tasks. They can also generate high-level solution outlines for domain-specific tasks based on the common sense knowledge they have acquired. However, they still face difficulties with some specialized tasks because they lack enough domain-specific data during pre-training or they often have errors in their neural network computations on those tasks that need accurate executions. On the other hand, there are also many existing models and systems (symbolic-based or neural-based) that can do some domain-specific tasks very well. However, due to the different implementation or working mechanisms, they are not easily accessible or compatible with foundation models. Therefore, there is a clear and pressing need for a mechanism that can leverage foundation 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;LLM&#65292;&#33021;&#22815;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#31867;&#22411;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2303.16421</link><description>&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;&#38382;&#39064;&#27714;&#35299;&#22120;&#65306;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24120;&#35782;&#38382;&#39064;&#30340;&#35843;&#26597;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a Knowledgeable but Inexperienced Solver: An Investigation of Commonsense Problem in Large Language Models. (arXiv:2303.16421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16421
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#20010;&#30693;&#35782;&#28170;&#21338;&#20294;&#32463;&#39564;&#19981;&#36275;&#30340;LLM&#65292;&#33021;&#22815;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65292;&#20294;&#22312;&#26576;&#20123;&#31867;&#22411;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;GPT-4&#65292;&#22312;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35760;&#24518;&#12289;&#34920;&#36798;&#21644;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#19968;&#30452;&#26159;LLMs&#30340;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#30340;&#30171;&#28857;&#12290;&#30446;&#21069;&#20173;&#19981;&#28165;&#26970;&#20197;&#19979;&#20960;&#28857;&#65306;&#65288;1&#65289;GPT&#33021;&#21542;&#26377;&#25928;&#22238;&#31572;&#24120;&#35782;&#38382;&#39064;&#65311;&#65288;2&#65289;GPT&#23545;&#24120;&#35782;&#30693;&#35782;&#26159;&#21542;&#31934;&#36890;&#65311;&#65288;3&#65289;GPT&#26159;&#21542;&#20102;&#35299;&#29992;&#20110;&#22238;&#31572;&#29305;&#23450;&#38382;&#39064;&#30340;&#24213;&#23618;&#24120;&#35782;&#30693;&#35782;&#65311;&#65288;4&#65289;GPT&#33021;&#21542;&#26377;&#25928;&#21033;&#29992;&#24120;&#35782;&#22238;&#31572;&#38382;&#39064;&#65311;&#20026;&#20102;&#35780;&#20272;&#20197;&#19978;&#24120;&#35782;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#35780;&#20272;ChatGPT&#30340;&#24120;&#35782;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;(1) GPT&#22312;&#24120;&#35782;&#20219;&#21153;&#20013;&#33021;&#22815;&#33719;&#24471;&#33391;&#22909;&#30340;&#38382;&#31572;&#20934;&#30830;&#24615;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#26576;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#12290;(2) ChatGPT&#20855;&#26377;&#23398;&#35782;&#28170;&#21338;&#65292;&#21487;&#20197;&#20351;&#29992;&#30693;&#35782;&#25552;&#31034;&#20934;&#30830;&#22320;&#20135;&#29983;&#22823;&#37096;&#20998;&#24120;&#35782;&#30693;&#35782;&#12290;(3)&#23613;&#31649;&#20855;&#26377;&#30693;&#35782;&#65292;ChatGPT&#26159;&#19968;&#20010;&#32570;&#20047;&#32463;&#39564;&#30340;&#24120;&#35782;&#38382;&#39064;&#27714;&#35299;&#22120;&#65292;&#26080;&#27861;&#26377;&#25928;&#22320;&#21033;&#29992;&#24120;&#35782;&#30693;&#35782;&#22238;&#31572;&#26576;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT and GPT-4 have made significant progress in NLP. However, their ability to memorize, represent, and leverage commonsense knowledge has been a well-known pain point for LLMs. It remains unclear that: (1) Can GPTs effectively answer commonsense questions? (2) Are GPTs knowledgeable in commonsense? (3) Are GPTs aware of the underlying commonsense knowledge for answering a specific question? (4) Can GPTs effectively leverage commonsense for answering questions? To evaluate the above commonsense problems, we conduct a series of experiments to evaluate ChatGPT's commonsense abilities, and the experimental results show that: (1) GPTs can achieve good QA accuracy in commonsense tasks, while they still struggle with certain types of knowledge. (2) ChatGPT is knowledgeable, and can accurately generate most of the commonsense knowledge using knowledge prompts. (3) Despite its knowledge, ChatGPT is an inexperienced commonsense problem solver, which cann
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT &#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616; ChatGPT &#22312;&#26494;&#24347;&#21305;&#37197; F1 &#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110; GPT-3&#12290;&#34429;&#28982;&#20854;&#24615;&#33021;&#20173;&#20302;&#20110; BioClinicalBERT &#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102; ChatGPT &#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202; NER &#20219;&#21153;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16416</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Clinical Entity Recognition using ChatGPT. (arXiv:2303.16416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992; ChatGPT &#36827;&#34892;&#38646;&#26679;&#26412;&#20020;&#24202;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#24182;&#21457;&#29616; ChatGPT &#22312;&#26494;&#24347;&#21305;&#37197; F1 &#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110; GPT-3&#12290;&#34429;&#28982;&#20854;&#24615;&#33021;&#20173;&#20302;&#20110; BioClinicalBERT &#27169;&#22411;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102; ChatGPT &#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#26377;&#24456;&#22823;&#30340;&#20020;&#24202; NER &#20219;&#21153;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;OpenAI&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#22312;2010&#24180;i2b2&#25361;&#25112;&#20013;&#25351;&#23450;&#30340;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#36827;&#34892;&#20102;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;GPT-3&#22312;&#31867;&#20284;&#30340;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#21450;&#20351;&#29992;MTSamples&#30340;&#19968;&#32452;&#21512;&#25104;&#30340;&#20020;&#24202;&#31508;&#35760;&#23545;BioClinicalBERT&#27169;&#22411;&#36827;&#34892;&#20248;&#21270;&#24494;&#35843;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#31934;&#30830;&#21305;&#37197;&#21644;&#26494;&#24347;&#21305;&#37197;&#30340;F1&#20998;&#21035;&#20026;0.418&#65288;vs.0.250&#65289;&#21644;0.620&#65288;vs.0.480&#65289;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;GPT-3&#30340;&#34920;&#29616;&#36739;&#24046;&#12290;&#21478;&#22806;&#65292;&#25552;&#31034;&#31574;&#30053;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;ChatGPT&#30340;&#24615;&#33021;&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#19979;&#26494;&#24347;&#21305;&#37197;&#30340;F1&#20998;&#21035;&#20026;0.628&#21644;0.541&#12290;&#34429;&#28982;ChatGPT&#30340;&#24615;&#33021;&#20173;&#20302;&#20110;&#21463;&#30417;&#30563;&#30340;BioClinicalBERT&#27169;&#22411;&#65288;&#21363;&#26494;&#24347;&#21305;&#37197;F1&#20998;&#25968;&#20998;&#21035;&#20026;0.628&#21644;0.870&#65289;&#65292;&#20294;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;ChatGPT&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#20020;&#24202;NER&#20219;&#21153;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we investigated the potential of ChatGPT, a large language model developed by OpenAI, for the clinical named entity recognition task defined in the 2010 i2b2 challenge, in a zero-shot setting with two different prompt strategies. We compared its performance with GPT-3 in a similar zero-shot setting, as well as a fine-tuned BioClinicalBERT model using a set of synthetic clinical notes from MTSamples. Our findings revealed that ChatGPT outperformed GPT-3 in the zero-shot setting, with F1 scores of 0.418 (vs.0.250) and 0.620 (vs. 0.480) for exact- and relaxed-matching, respectively. Moreover, prompts affected ChatGPT's performance greatly, with relaxed-matching F1 scores of 0.628 vs.0.541 for two different prompt strategies. Although ChatGPT's performance was still lower than that of the supervised BioClinicalBERT model (i.e., relaxed-matching F1 scores of 0.628 vs. 0.870), our study demonstrates the great potential of ChatGPT for clinical NER tasks in a zero-shot setting, 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HiREST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23558;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#36880;&#27493;&#24635;&#32467;&#20174;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25512;&#24191;&#65292;&#20351;&#24471;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#19979;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;</title><link>http://arxiv.org/abs/2303.16406</link><description>&lt;p&gt;
&#23618;&#27425;&#21270;&#35270;&#39057;&#30636;&#38388;&#26816;&#32034;&#21644;&#20998;&#27493;&#23383;&#24149;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Video-Moment Retrieval and Step-Captioning. (arXiv:2303.16406v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16406
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;HiREST&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#23558;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#36880;&#27493;&#24635;&#32467;&#20174;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25512;&#24191;&#65292;&#20351;&#24471;&#22312;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#19979;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20154;&#20204;&#22312;&#23547;&#25214;&#22823;&#22411;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#30340;&#20449;&#24687;&#26041;&#38754;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#29420;&#31435;&#30740;&#31350;&#20102;&#30456;&#20851;&#20219;&#21153;&#65292;&#22914;&#22522;&#20110;&#25991;&#26412;&#30340;&#35270;&#39057;&#26816;&#32034;&#12289;&#30636;&#38388;&#26816;&#32034;&#12289;&#35270;&#39057;&#25688;&#35201;&#21644;&#35270;&#39057;&#23383;&#24149;&#65292;&#27809;&#26377;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35774;&#32622;&#21487;&#20197;&#20849;&#21516;&#25628;&#32034;&#35270;&#39057;&#35821;&#26009;&#24211;&#65292;&#24182;&#29983;&#25104;&#25688;&#35201;&#12290;&#36825;&#26679;&#30340;&#31471;&#21040;&#31471;&#35774;&#32622;&#23558;&#20801;&#35768;&#35768;&#22810;&#26377;&#36259;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20363;&#22914;&#22522;&#20110;&#25991;&#26412;&#30340;&#25628;&#32034;&#65292;&#20174;&#35270;&#39057;&#35821;&#26009;&#24211;&#20013;&#25214;&#21040;&#30456;&#20851;&#30340;&#35270;&#39057;&#65292;&#25552;&#21462;&#26368;&#30456;&#20851;&#30340;&#30636;&#38388;&#65292;&#24182;&#23558;&#30636;&#38388;&#20998;&#25104;&#37325;&#35201;&#30340;&#27493;&#39588;&#65292;&#24182;&#21152;&#19978;&#23383;&#24149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiREST(Hierarchical REtrieval and STep-captioning)&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#25945;&#23398;&#35270;&#39057;&#35821;&#26009;&#24211;&#30340;&#20998;&#23618;&#20449;&#24687;&#26816;&#32034;&#21644;&#35270;&#35273;/&#25991;&#26412;&#20998;&#38454;&#27573;&#24635;&#32467;&#12290;HiREST&#30001;&#26469;&#33258;&#25945;&#23398;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;3.4K&#20010;&#25991;&#26412;-&#35270;&#39057;&#23545;&#32452;&#25104;&#65292;&#20854;&#20013;1.1K&#20010;&#35270;&#39057;&#20855;&#26377;&#19982;&#25991;&#26412;&#26597;&#35810;&#30456;&#20851;&#30340;&#30636;&#38388;&#36328;&#24230;&#27880;&#37322;&#21644;&#32454;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is growing interest in searching for information from large video corpora. Prior works have studied relevant tasks, such as text-based video retrieval, moment retrieval, video summarization, and video captioning in isolation, without an end-to-end setup that can jointly search from video corpora and generate summaries. Such an end-to-end setup would allow for many interesting applications, e.g., a text-based search that finds a relevant video from a video corpus, extracts the most relevant moment from that video, and segments the moment into important steps with captions. To address this, we present the HiREST (HIerarchical REtrieval and STep-captioning) dataset and propose a new benchmark that covers hierarchical information retrieval and visual/textual stepwise summarization from an instructional video corpus. HiREST consists of 3.4K text-video pairs from an instructional video dataset, where 1.1K videos have annotations of moment spans relevant to text query and breakdown of e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#25776;&#20889;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;2.5k&#20010;&#26679;&#26412;&#19978;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;99.2%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16352</link><description>&lt;p&gt;
&#29992;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#20197;&#36229;&#36807;99%&#30340;&#20934;&#30830;&#24230;&#21306;&#20998;ChatGPT&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#30340;&#20316;&#32773;&#36523;&#20221;
&lt;/p&gt;
&lt;p&gt;
ChatGPT or academic scientist? Distinguishing authorship with over 99% accuracy using off-the-shelf machine learning tools. (arXiv:2303.16352v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16352
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#29992;&#20110;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#25776;&#20889;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;2.5k&#20010;&#26679;&#26412;&#19978;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;99.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20351;&#24191;&#22823;&#32676;&#20247;&#33021;&#22815;&#35775;&#38382;AI&#29983;&#25104;&#30340;&#25991;&#31456;&#65292;&#22312;&#30701;&#30701;&#20960;&#20010;&#26376;&#20869;&#65292;&#36825;&#20010;&#20135;&#21697;&#39072;&#35206;&#20102;&#30693;&#35782;&#32463;&#27982;&#65292;&#24341;&#21457;&#20102;&#20154;&#20204;&#24037;&#20316;&#12289;&#23398;&#20064;&#21644;&#20889;&#20316;&#26041;&#24335;&#30340;&#25991;&#21270;&#21464;&#38761;&#12290;&#29616;&#22312;&#21306;&#20998;&#20154;&#31867;&#20889;&#20316;&#21644;AI&#20889;&#20316;&#30340;&#38656;&#27714;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#21644;&#32039;&#36843;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#31561;&#25945;&#32946;&#21644;&#23398;&#26415;&#20889;&#20316;&#31561;&#39046;&#22495;&#65292;AI&#22312;&#20043;&#21069;&#19981;&#26366;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#25110;&#32773;&#36129;&#29486;&#32773;&#12290;&#38024;&#23545;&#36825;&#19968;&#38656;&#27714;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21306;&#20998;ChatGPT&#29983;&#25104;&#25991;&#26412;&#21644;&#65288;&#20154;&#31867;&#65289;&#23398;&#26415;&#31185;&#23398;&#23478;&#25776;&#20889;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#20381;&#38752;&#26222;&#21450;&#19988;&#26131;&#20110;&#33719;&#21462;&#30340;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#37325;&#28857;&#22312;&#20110;&#65292;&#23398;&#26415;&#31185;&#23398;&#23478;&#36825;&#19968;&#29305;&#23450;&#20154;&#32676;&#30340;&#20889;&#20316;&#19982;ChatGPT&#26377;&#20309;&#19981;&#21516;&#65292;&#36825;&#31181;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#23548;&#33268;&#25105;&#20204;&#21457;&#29616;&#20102;&#29992;&#20110;&#21306;&#20998;&#65288;&#36825;&#20123;&#65289;&#20154;&#31867;&#21644;AI&#30340;&#26032;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#31185;&#23398;&#23478;&#32463;&#24120;&#20889;&#38271;&#27573;&#33853;&#65292;&#21916;&#27426;&#20351;&#29992;&#27169;&#31946;&#35821;&#35328;&#65292;&#32463;&#24120;&#20351;&#29992;but&#12289;however&#21644;although&#31561;&#35789;&#27719;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#27979;&#35797;&#20102;&#36825;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#20102;2.5k&#20010;&#26679;&#26412;&#65292;&#25104;&#21151;&#23454;&#29616;&#20102;99.2%&#30340;&#20934;&#30830;&#29575;&#65292;&#21306;&#20998;ChatGPT&#21644;&#23398;&#26415;&#31185;&#23398;&#23478;&#30340;&#20889;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has enabled access to AI-generated writing for the masses, and within just a few months, this product has disrupted the knowledge economy, initiating a culture shift in the way people work, learn, and write. The need to discriminate human writing from AI is now both critical and urgent, particularly in domains like higher education and academic writing, where AI had not been a significant threat or contributor to authorship. Addressing this need, we developed a method for discriminating text generated by ChatGPT from (human) academic scientists, relying on prevalent and accessible supervised classification methods. We focused on how a particular group of humans, academic scientists, write differently than ChatGPT, and this targeted approach led to the discovery of new features for discriminating (these) humans from AI; as examples, scientists write long paragraphs and have a penchant for equivocal language, frequently using words like but, however, and although. With a set of 2
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.16342</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#25351;&#23548;&#30340;&#19977;&#27169;&#24577;&#19968;&#33268;&#24615;&#36827;&#34892;&#38899;&#39057;-&#35270;&#39057;&#28304;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Language-Guided Audio-Visual Source Separation via Trimodal Consistency. (arXiv:2303.16342v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16342
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#26469;&#36827;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#38899;&#39057;&#30340;&#19968;&#33268;&#24615;&#23545;&#40784;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#22312;&#35270;&#39057;&#20013;&#23398;&#20064;&#25191;&#34892;&#38899;&#39057;&#28304;&#20998;&#31163;&#65292;&#20165;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#38899;&#39057;&#23545;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#39033;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#23398;&#20064;&#23558;&#21457;&#22768;&#29289;&#20307;&#30340;&#35821;&#35328;&#25551;&#36848;&#19982;&#20854;&#35270;&#35273;&#29305;&#24449;&#21644;&#30456;&#24212;&#30340;&#38899;&#39057;&#27874;&#24418;&#32452;&#20214;&#32852;&#31995;&#36215;&#26469;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#35775;&#38382;&#27880;&#37322;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#25552;&#20379;&#20266;&#30446;&#26631;&#30417;&#30563;&#65292;&#24182;&#20419;&#36827;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#27169;&#24577;&#20043;&#38388;&#26356;&#24378;&#30340;&#23545;&#40784;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#32473;&#23450;&#25991;&#26412;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#36755;&#20837;&#25110;&#20165;&#32473;&#23450;&#25991;&#26412;&#21644;&#38899;&#39057;&#36755;&#20837;&#26102;&#20998;&#31163;&#22768;&#38899;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#38899;&#39057;-&#35270;&#39057;&#20998;&#31163;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;MUSIC&#12289;SOLOS&#21644;AudioSet&#65289;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#33258;&#30417;&#30563;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#30340;&#27169;&#22411;&#32988;&#36807;&#20102;&#29616;&#26377;&#24378;&#30417;&#30563;&#26041;&#27861;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a self-supervised approach for learning to perform audio source separation in videos based on natural language queries, using only unlabeled video and audio pairs as training data. A key challenge in this task is learning to associate the linguistic description of a sound-emitting object to its visual features and the corresponding components of the audio waveform, all without access to annotations during training. To overcome this challenge, we adapt off-the-shelf vision-language foundation models to provide pseudo-target supervision via two novel loss functions and encourage a stronger alignment between the audio, visual and natural language modalities. During inference, our approach can separate sounds given text, video and audio input, or given text and audio input alone. We demonstrate the effectiveness of our self-supervised approach on three audio-visual separation datasets, including MUSIC, SOLOS and AudioSet, where we outperform state-of-the-art strongly supervised 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2303.16281</link><description>&lt;p&gt;
&#22823;&#35937;&#30340;&#36879;&#35270;&#38236;&#65306;&#35843;&#26597;&#35895;&#27468;&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#30340;&#35821;&#35328;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Perspectival Mirror of the Elephant: Investigating Language Bias on Google, ChatGPT, Wikipedia, and YouTube. (arXiv:2303.16281v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16281
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22312;Google&#12289;ChatGPT&#12289;&#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#19978;&#65292;&#25628;&#32034;&#32467;&#26524;&#21463;&#38480;&#20110;&#35821;&#35328;&#65292;&#21453;&#26144;&#20102;&#19982;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#65292;&#32570;&#20047;&#36328;&#25991;&#21270;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#35895;&#27468;&#25628;&#32034;&#8220;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#20449;&#24687;&#65292;&#20197;&#20415;&#20320;&#21487;&#20197;&#24418;&#25104;&#33258;&#24049;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#8221;&#30340;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#35895;&#27468;&#21450;&#20854;&#26368;&#31361;&#20986;&#30340;&#25628;&#32034;&#32467;&#26524; - &#32500;&#22522;&#30334;&#31185;&#21644;YouTube&#65292;&#20165;&#21453;&#26144;&#19982;&#8220;&#20315;&#25945;&#8221;&#12289;&#8220;&#33258;&#30001;&#20027;&#20041;&#8221;&#12289;&#8220;&#27542;&#27665;&#21270;&#8221;&#12289;&#8220;&#20234;&#26391;&#8221;&#21644;&#8220;&#32654;&#22269;&#8221;&#31561;&#22797;&#26434;&#20027;&#39064;&#30456;&#20851;&#30340;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#12290;&#31616;&#21333;&#22320;&#35828;&#65292;&#22312;&#19981;&#21516;&#35821;&#35328;&#30340;&#30456;&#21516;&#25628;&#32034;&#20013;&#65292;&#23427;&#20204;&#20197;&#19981;&#21516;&#31243;&#24230;&#21576;&#29616;&#19981;&#21516;&#30340;&#20449;&#24687;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#35821;&#35328;&#20559;&#35265;&#8221;&#65289;&#65292;&#32780;&#19981;&#26159;&#21576;&#29616;&#22797;&#26434;&#20027;&#39064;&#30340;&#20840;&#29699;&#22270;&#29255;&#12290;&#25105;&#20204;&#30340;&#22312;&#32447;&#25628;&#32034;&#20351;&#25105;&#20204;&#25104;&#20026;&#35866;&#35821;&#20013;&#30340;&#30450;&#20154;&#65292;&#20165;&#35302;&#25720;&#23567;&#35937;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#19981;&#30693;&#36947;&#20854;&#20182;&#25991;&#21270;&#30340;&#35270;&#35282;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#29992;&#20110;&#25628;&#32034;&#30340;&#35821;&#35328;&#26368;&#32456;&#25104;&#20026;&#20419;&#36827;&#26412;&#26063;&#20013;&#24515;&#20027;&#20041;&#35266;&#28857;&#30340;&#25991;&#21270;&#36807;&#28388;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#26681;&#25454;&#33258;&#24049;&#30340;&#25991;&#21270;&#35780;&#20272;&#20854;&#20182;&#20154;&#25110;&#24605;&#24819;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;ChatGPT&#20013;&#28145;&#28145;&#23884;&#20837;&#20102;&#35821;&#35328;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to Google Search's mission of delivering information from "many angles so you can form your own understanding of the world," we find that Google and its most prominent returned results -- Wikipedia and YouTube, simply reflect the narrow set of cultural stereotypes tied to the search language for complex topics like "Buddhism," "Liberalism," "colonization," "Iran" and "America." Simply stated, they present, to varying degrees, distinct information across the same search in different languages (we call it 'language bias'). Instead of presenting a global picture of a complex topic, our online searches turn us into the proverbial blind person touching a small portion of an elephant, ignorant of the existence of other cultural perspectives. The language we use to search ends up as a cultural filter to promote ethnocentric views, where a person evaluates other people or ideas based on their own culture. We also find that language bias is deeply embedded in ChatGPT. As it is primaril
&lt;/p&gt;</description></item><item><title>&#24403;&#21069;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#24182;&#19981;&#20248;&#31168;&#12290;&#20316;&#32773;&#35748;&#20026;&#65292;&#36825;&#31181;&#29366;&#20917;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36807;&#20110;&#19987;&#27880;&#20110;&#35821;&#35328;&#30340;&#20449;&#24687;&#20869;&#23481;&#32780;&#24573;&#30053;&#20102;&#20854;&#31038;&#20250;&#22240;&#32032;&#12290;&#20182;&#20204;&#25552;&#20986;&#23558;&#31038;&#20250;&#22240;&#32032;&#34701;&#20837;&#21040;&#20889;&#20316;&#21161;&#25163;&#30340;&#24314;&#35774;&#20013;&#65292;&#20197;&#26500;&#24314;&#26234;&#33021;&#12289;&#26377;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#20889;&#20316;&#21161;&#25163;&#65292;&#20174;&#32780;&#25552;&#39640;&#29992;&#25143;&#30340;&#20351;&#29992;&#20307;&#39564;&#21644;&#25509;&#21463;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.16275</link><description>&lt;p&gt;
&#20889;&#20316;&#21161;&#25163;&#24212;&#35813;&#27169;&#25311;&#35821;&#35328;&#30340;&#31038;&#20250;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Writing Assistants Should Model Social Factors of Language. (arXiv:2303.16275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16275
&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#30340;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#23427;&#20204;&#30340;&#34920;&#29616;&#24182;&#19981;&#20248;&#31168;&#12290;&#20316;&#32773;&#35748;&#20026;&#65292;&#36825;&#31181;&#29366;&#20917;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#36807;&#20110;&#19987;&#27880;&#20110;&#35821;&#35328;&#30340;&#20449;&#24687;&#20869;&#23481;&#32780;&#24573;&#30053;&#20102;&#20854;&#31038;&#20250;&#22240;&#32032;&#12290;&#20182;&#20204;&#25552;&#20986;&#23558;&#31038;&#20250;&#22240;&#32032;&#34701;&#20837;&#21040;&#20889;&#20316;&#21161;&#25163;&#30340;&#24314;&#35774;&#20013;&#65292;&#20197;&#26500;&#24314;&#26234;&#33021;&#12289;&#26377;&#25928;&#21644;&#20010;&#24615;&#21270;&#30340;&#20889;&#20316;&#21161;&#25163;&#65292;&#20174;&#32780;&#25552;&#39640;&#29992;&#25143;&#30340;&#20351;&#29992;&#20307;&#39564;&#21644;&#25509;&#21463;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#36825;&#31687;&#25991;&#31456;&#65292;&#26234;&#33021;&#20889;&#20316;&#21161;&#25163;&#22312;&#24403;&#20170;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#26356;&#21463;&#27426;&#36814;&#65292;&#20294;&#20182;&#20204;&#30340;&#36827;&#19968;&#27493;&#26222;&#21450;&#21463;&#21040;&#20122;&#20248;&#21270;&#34920;&#29616;&#30340;&#38480;&#21046;&#12290;&#20316;&#32773;&#20204;&#35748;&#20026;&#36896;&#25104;&#36825;&#31181;&#29366;&#24577;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#21482;&#20851;&#27880;&#35821;&#35328;&#30340;&#20449;&#24687;&#20869;&#23481;&#32780;&#24573;&#30053;&#20102;&#20854;&#31038;&#20250;&#22240;&#32032;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#31038;&#20250;&#22240;&#32032;&#22312;&#20889;&#20316;&#21161;&#25163;&#20013;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#23558;&#20854;&#32435;&#20837;&#26356;&#26234;&#33021;&#12289;&#26356;&#26377;&#25928;&#21644;&#30495;&#27491;&#20010;&#24615;&#21270;&#30340;&#20889;&#20316;&#21161;&#25163;&#30340;&#24314;&#35774;&#20013;&#65292;&#20174;&#32780;&#20016;&#23500;&#29992;&#25143;&#20307;&#39564;&#24182;&#20419;&#36827;&#29992;&#25143;&#37319;&#32435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent writing assistants powered by large language models (LLMs) are more popular today than ever before, but their further widespread adoption is precluded by sub-optimal performance. In this position paper, we argue that a major reason for this sub-optimal performance and adoption is a singular focus on the information content of language while ignoring its social aspects. We analyze the different dimensions of these social factors in the context of writing assistants and propose their incorporation into building smarter, more effective, and truly personalized writing assistants that would enrich the user experience and contribute to increased user adoption.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#25163;&#20889;&#25991;&#23383;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#30772;&#35793;&#21382;&#21490;&#26410;&#24320;&#21457;&#35821;&#35328;&#21644;&#23383;&#27597;&#30340;&#35789;&#20856;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#35835;&#21462;&#25163;&#20889;&#32034;&#24341;&#21345;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24341;&#29992;&#38142;&#25509;&#21040;&#21487;&#25628;&#32034;&#30340;&#35789;&#20856;&#26465;&#30446;&#21015;&#34920;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#25163;&#20889;&#25991;&#26412;&#24182;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16256</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#21450;&#23383;&#27597;&#35789;&#20856;&#30340;&#21487;&#25193;&#23637;&#25163;&#20889;&#25991;&#23383;&#35782;&#21035;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Scalable handwritten text recognition system for lexicographic sources of under-resourced languages and alphabets. (arXiv:2303.16256v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#25163;&#20889;&#25991;&#23383;&#35782;&#21035;&#31995;&#32479;&#65292;&#21487;&#29992;&#20110;&#30772;&#35793;&#21382;&#21490;&#26410;&#24320;&#21457;&#35821;&#35328;&#21644;&#23383;&#27597;&#30340;&#35789;&#20856;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20197;&#35835;&#21462;&#25163;&#20889;&#32034;&#24341;&#21345;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24341;&#29992;&#38142;&#25509;&#21040;&#21487;&#25628;&#32034;&#30340;&#35789;&#20856;&#26465;&#30446;&#21015;&#34920;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#35782;&#21035;&#25163;&#20889;&#25991;&#26412;&#24182;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#30772;&#35793;&#21382;&#21490;&#35789;&#20856;&#22823;&#37327;&#25163;&#20889;&#32034;&#24341;&#21345;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#24037;&#20316;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35835;&#21462;&#36825;&#20123;&#32034;&#24341;&#21345;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24341;&#25991;&#38142;&#25509;&#21040;&#21487;&#25628;&#32034;&#30340;&#35789;&#20856;&#26465;&#30446;&#21015;&#34920;&#65292;&#29992;&#20110;&#19968;&#20010;&#21517;&#20026;&#8220;17&#19990;&#32426;&#21644;18&#19990;&#32426;&#27874;&#20848;&#35821;&#35789;&#20856;&#8221;&#30340;&#22823;&#22411;&#21382;&#21490;&#35789;&#20856;&#65292;&#20854;&#21253;&#25324;280&#19975;&#20010;&#32034;&#24341;&#21345;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;&#37327;&#36523;&#23450;&#21046;&#30340;&#25163;&#20889;&#25991;&#26412;&#35782;&#21035;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#65288;1&#65289;&#20248;&#21270;&#30340;&#26816;&#27979;&#27169;&#22411;&#65307;&#65288;2&#65289;&#29992;&#20110;&#35299;&#23494;&#25163;&#20889;&#20869;&#23481;&#30340;&#35782;&#21035;&#27169;&#22411;&#65292;&#35774;&#35745;&#20026;&#19968;&#20010;&#31354;&#38388;&#21464;&#25442;&#32593;&#32476;(STN)&#65292;&#21518;&#36319;&#19968;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RCNN&#65289;&#21644;&#19968;&#20010;&#32852;&#32467;&#26102;&#24207;&#20998;&#31867;&#23618;(CTC)&#65292;&#20351;&#29992;&#19968;&#20010;&#30001;50&#19975;&#20010;&#19981;&#21516;&#38271;&#24230;&#30340;&#27874;&#20848;&#35821;&#21333;&#35789;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65307; (3) &#20351;&#29992;&#21463;&#38480;&#21046;&#30340;Word Beam Search(WBC)&#36827;&#34892;&#21518;&#22788;&#29702;:&#39044;&#27979;&#34987;&#19982;&#20107;&#20808;&#24050;&#30693;&#30340;&#35789;&#20856;&#26465;&#30446;&#21015;&#34920;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21333;&#35789;&#32423;&#21035;&#30340;&#20934;&#30830;&#24230;&#19978;&#36798;&#21040;&#20102;0.881&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper discusses an approach to decipher large collections of handwritten index cards of historical dictionaries. Our study provides a working solution that reads the cards, and links their lemmas to a searchable list of dictionary entries, for a large historical dictionary entitled the Dictionary of the 17thand 18th-century Polish, which comprizes 2.8 million index cards. We apply a tailored handwritten text recognition (HTR) solution that involves (1) an optimized detection model; (2) a recognition model to decipher the handwritten content, designed as a spatial transformer network (STN) followed by convolutional neural network (RCNN) with a connectionist temporal classification layer (CTC), trained using a synthetic set of 500,000 generated Polish words of different length; (3) a post-processing step using constrained Word Beam Search (WBC): the predictions were matched against a list of dictionary entries known in advance. Our model achieved the accuracy of 0.881 on the word l
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#26550;&#26500;&#21644;&#23545;&#35805;&#21382;&#21490;&#25688;&#35201;&#23454;&#29616;&#27867;&#21270;&#30340;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#22312;&#26631;&#20934;&#39046;&#22495;&#36716;&#31227;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16252</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#25688;&#35201;&#21644;&#39046;&#22495;&#26550;&#26500;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Generalizable End-to-End Task-Oriented Dialog System using Context Summarization and Domain Schema. (arXiv:2303.16252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#39046;&#22495;&#26550;&#26500;&#21644;&#23545;&#35805;&#21382;&#21490;&#25688;&#35201;&#23454;&#29616;&#27867;&#21270;&#30340;&#38646;&#26679;&#26412;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#22312;&#26631;&#20934;&#39046;&#22495;&#36716;&#31227;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#36890;&#36807;&#20419;&#36827;&#30452;&#35266;&#21644;&#34920;&#36798;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#23436;&#25104;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290; &#22312;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#31995;&#32479;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#22312;&#30417;&#30563;&#35774;&#32622;&#20013;&#24494;&#35843;&#39044;&#20808;&#35757;&#32451;&#30340;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#12290; &#36825;&#38656;&#35201;&#20026;&#27599;&#20010;&#26032;&#39046;&#22495;&#25110;&#20219;&#21153;&#25552;&#20379;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#26159;&#26497;&#20854;&#36153;&#21147;&#21644;&#26114;&#36149;&#30340;&#65292;&#20174;&#32780;&#20351;&#20854;&#25104;&#20026;&#25193;&#23637;&#31995;&#32479;&#21040;&#21508;&#31181;&#39046;&#22495;&#30340;&#29942;&#39048;&#12290; &#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38646;&#26679;&#26412;&#27867;&#21270;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;ZS-ToD&#65292;&#23427;&#21033;&#29992;&#39046;&#22495;&#26550;&#26500;&#65292;&#20801;&#35768;&#23545;&#26410;&#30693;&#39046;&#22495;&#36827;&#34892;&#24378;&#22823;&#30340;&#27867;&#21270;&#65292;&#24182;&#21033;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;&#26377;&#25928;&#25688;&#35201;&#12290; &#25105;&#20204;&#20351;&#29992;GPT-2&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#27493;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#20013;&#31532;&#19968;&#27493;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#23545;&#35805;&#25968;&#25454;&#30340;&#19968;&#33324;&#32467;&#26500;&#65292;&#31532;&#20108;&#27493;&#36890;&#36807;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#32452;&#21512;&#26469;&#20248;&#21270;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290; &#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#39046;&#22495;&#36716;&#31227;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented dialog systems empower users to accomplish their goals by facilitating intuitive and expressive natural language interactions. State-of-the-art approaches in task-oriented dialog systems formulate the problem as a conditional sequence generation task and fine-tune pre-trained causal language models in the supervised setting. This requires labeled training data for each new domain or task, and acquiring such data is prohibitively laborious and expensive, thus making it a bottleneck for scaling systems to a wide range of domains. To overcome this challenge, we introduce a novel Zero-Shot generalizable end-to-end Task-oriented Dialog system, ZS-ToD, that leverages domain schemas to allow for robust generalization to unseen domains and exploits effective summarization of the dialog history. We employ GPT-2 as a backbone model and introduce a two-step training process where the goal of the first step is to learn the general structure of the dialog data and the second step opti
&lt;/p&gt;</description></item><item><title>&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16166</link><description>&lt;p&gt;
&#27809;&#26377;&#27491;&#30830;&#24615;&#30340;&#21487;&#37325;&#22797;&#24615;&#24182;&#19981;&#37325;&#35201;&#65306;&#22312;NLP&#39046;&#22495;&#20013;&#27979;&#35797;&#20195;&#30721;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reproducibility is Nothing without Correctness: The Importance of Testing Code in NLP. (arXiv:2303.16166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16166
&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19981;&#33021;&#20165;&#20973;&#24863;&#30693;&#36136;&#37327;&#20551;&#23450;&#20195;&#30721;&#27491;&#30830;&#24615;&#65292;&#24212;&#35813;&#25512;&#21160;&#37319;&#29992;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#20197;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#27491;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20854;&#22312;&#30740;&#31350;&#23454;&#39564;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#20195;&#30721;&#27491;&#30830;&#24615;&#24448;&#24448;&#20165;&#22522;&#20110;&#32467;&#26524;&#30340;&#24863;&#30693;&#36136;&#37327;&#32780;&#34987;&#20551;&#23450;&#12290;&#36825;&#24102;&#26469;&#20102;&#38169;&#35823;&#32467;&#26524;&#21644;&#28508;&#22312;&#35823;&#23548;&#24615;&#21457;&#29616;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#20851;&#27880;&#32467;&#26524;&#37325;&#29616;&#24212;&#35813;&#19982;&#24378;&#35843;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#26469;&#25903;&#25345;&#25105;&#20204;&#21521;NLP&#31038;&#21306;&#21457;&#20986;&#30340;&#21495;&#21484;&#65292;&#22312;&#36825;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20986;&#24182;&#32416;&#27491;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#26368;&#20808;&#36827;Conformer&#26550;&#26500;&#30340;&#24320;&#28304;&#23454;&#29616;&#20013;&#30340;&#19977;&#20010;Bug&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#35821;&#35328;&#29615;&#22659;&#19979;&#36827;&#34892;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#21644;&#32763;&#35793;&#30340;&#27604;&#36739;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Bug&#30340;&#23384;&#22312;&#24182;&#19981;&#20250;&#22952;&#30861;&#33719;&#24471;&#33391;&#22909;&#30340;&#21644;&#21487;&#37325;&#22797;&#30340;&#32467;&#26524;&#65292;&#21453;&#32780;&#21487;&#33021;&#23548;&#33268;&#19981;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#33021;&#25552;&#20379;&#38169;&#35823;&#30340;&#25351;&#23548;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#39033;&#30740;&#31350;&#21628;&#21505;&#37319;&#29992;&#26088;&#22312;&#20419;&#36827;NLP&#30740;&#31350;&#20013;&#27491;&#30830;&#24615;&#30340;&#32534;&#30721;&#26368;&#20339;&#23454;&#36341;&#65292;&#24182;&#25552;&#39640;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite its pivotal role in research experiments, code correctness is often presumed only on the basis of the perceived quality of the results. This comes with the risk of erroneous outcomes and potentially misleading findings. To address this issue, we posit that the current focus on result reproducibility should go hand in hand with the emphasis on coding best practices. We bolster our call to the NLP community by presenting a case study, in which we identify (and correct) three bugs in widely used open-source implementations of the state-of-the-art Conformer architecture. Through comparative experiments on automatic speech recognition and translation in various language settings, we demonstrate that the existence of bugs does not prevent the achievement of good and reproducible results and can lead to incorrect conclusions that potentially misguide future research. In response to this, this study is a call to action toward the adoption of coding best practices aimed at fostering cor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22768;&#23398;&#21644;&#35270;&#35273;&#20449;&#24687;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#19982;&#21475;&#35821;&#25991;&#26412;&#20018;&#32852;&#65292;&#20174;&#32780;&#23558;&#38750;&#35821;&#35328;&#20449;&#24687;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#38750;&#35821;&#35328;&#32447;&#32034;&#19982;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#34892;&#20026;&#29702;&#35299;&#20219;&#21153;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.15430</link><description>&lt;p&gt;
TextMI:&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#21270;&#20026;&#25991;&#26412;&#24418;&#24335;&#65292;&#20351;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#38750;&#35821;&#35328;&#32447;&#32034;
&lt;/p&gt;
&lt;p&gt;
TextMI: Textualize Multimodal Information for Integrating Non-verbal Cues in Pre-trained Language Models. (arXiv:2303.15430v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22768;&#23398;&#21644;&#35270;&#35273;&#20449;&#24687;&#36716;&#21270;&#20026;&#25991;&#26412;&#25551;&#36848;&#24182;&#19982;&#21475;&#35821;&#25991;&#26412;&#20018;&#32852;&#65292;&#20174;&#32780;&#23558;&#38750;&#35821;&#35328;&#20449;&#24687;&#32435;&#20837;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#38750;&#35821;&#35328;&#32447;&#32034;&#19982;&#35821;&#35328;&#26080;&#32541;&#38598;&#25104;&#65292;&#23545;&#20110;&#22810;&#27169;&#24577;&#34892;&#20026;&#29702;&#35299;&#20219;&#21153;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#21516;&#19968;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#20110;&#22810;&#27169;&#24335;&#34892;&#20026;&#29702;&#35299;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#39057;&#24773;&#24863;/&#24189;&#40664;&#26816;&#27979;&#65289;&#65292;&#38500;&#38750;&#21487;&#20197;&#23558;&#38750;&#35821;&#35328;&#29305;&#24449;&#65288;&#20363;&#22914;&#65292;&#22768;&#23398;&#21644;&#35270;&#35273;&#65289;&#19982;&#35821;&#35328;&#38598;&#25104;&#12290;&#32852;&#21512;&#24314;&#27169;&#22810;&#20010;&#27169;&#24577;&#26174;&#30528;&#22686;&#21152;&#20102;&#27169;&#22411;&#22797;&#26434;&#24615;&#65292;&#24182;&#20351;&#35757;&#32451;&#36807;&#31243;&#21464;&#24471;&#38656;&#25968;&#25454;&#37327;&#22823;&#12290;&#34429;&#28982;&#36890;&#36807;&#32593;&#32476;&#21487;&#20197;&#33719;&#24471;&#22823;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;&#20294;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#22810;&#27169;&#24577;&#34892;&#20026;&#35270;&#39057;&#25968;&#25454;&#38598;&#26497;&#20854;&#26114;&#36149;&#65292;&#26080;&#35770;&#26159;&#26102;&#38388;&#19978;&#36824;&#26159;&#37329;&#38065;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#20197;&#25991;&#26412;&#24418;&#24335;&#21576;&#29616;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#25104;&#21151;&#22320;&#23558;&#38750;&#35821;&#35328;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22768;&#38899;&#21644;&#35270;&#35273;&#20449;&#24687;&#36716;&#21270;&#20026;&#23545;&#24212;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#23558;&#20854;&#19982;&#21475;&#35821;&#25991;&#26412;&#20018;&#32852;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#22686;&#24378;&#30340;&#36755;&#20837;&#39304;&#36865;&#32473;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26174;&#31034;&#20854;&#24615;&#33021;&#19982;&#30452;&#25509;&#20351;&#29992;&#38750;&#35821;&#35328;&#29305;&#24449;&#30340;&#30456;&#21516;&#27169;&#22411;&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TextMI&#19981;&#20165;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#25910;&#38598;&#21644;&#24314;&#27169;&#36807;&#31243;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#38750;&#35821;&#35328;&#32447;&#32034;&#19982;&#35821;&#35328;&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#36825;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models have recently achieved ground-breaking performance in a wide variety of language understanding tasks. However, the same model can not be applied to multimodal behavior understanding tasks (e.g., video sentiment/humor detection) unless non-verbal features (e.g., acoustic and visual) can be integrated with language. Jointly modeling multiple modalities significantly increases the model complexity, and makes the training process data-hungry. While an enormous amount of text data is available via the web, collecting large-scale multimodal behavioral video datasets is extremely expensive, both in terms of time and money. In this paper, we investigate whether large language models alone can successfully incorporate non-verbal information when they are presented in textual form. We present a way to convert the acoustic and visual information into corresponding textual descriptions and concatenate them with the spoken text. We feed this augmented input to a pr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.13351</link><description>&lt;p&gt;
DBLP-QuAD&#65306;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph. (arXiv:2303.13351v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;DBLP&#23398;&#26415;&#30693;&#35782;&#22270;&#19978;&#21019;&#24314;&#20102;&#19968;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;DBLP&#26159;&#19968;&#20010;&#22312;&#32447;&#35745;&#31639;&#26426;&#31185;&#23398;&#20027;&#35201;&#20986;&#29256;&#29289;&#30340;&#21442;&#32771;&#25991;&#29486;&#20449;&#24687;&#32034;&#24341;&#65292;&#32034;&#24341;&#20102;&#36229;&#36807;440&#19975;&#31687;&#35770;&#25991;&#65292;&#30001;220&#19975;&#22810;&#20301;&#20316;&#32773;&#21457;&#34920;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;10000&#20010;&#38382;&#39064;-&#31572;&#26696;&#23545;&#20197;&#21450;&#30456;&#24212;&#30340;SPARQL&#26597;&#35810;&#65292;&#21487;&#20197;&#22312;DBLP KG&#19978;&#25191;&#34892;&#20197;&#33719;&#24471;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;DBLP-QuAD&#26159;&#26368;&#22823;&#30340;&#23398;&#26415;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we create a question answering dataset over the DBLP scholarly knowledge graph (KG). DBLP is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding SPARQL queries which can be executed over the DBLP KG to fetch the correct answer. DBLP-QuAD is the largest scholarly question answering dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#36798;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22312;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#19988;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;</title><link>http://arxiv.org/abs/2303.09859</link><description>&lt;p&gt;
&#32463;&#36807;&#35757;&#32451;&#30340;1&#20159;&#21333;&#35789;&#20173;&#28982;&#20445;&#25345;&#29366;&#24577;&#65306;BERT&#32467;&#21512;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
Trained on 100 million words and still in shape: BERT meets British National Corpus. (arXiv:2303.09859v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#36798;&#21040;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#22312;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#19988;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20013;&#65292;&#20182;&#20204;&#35777;&#26126;&#20102;&#36825;&#26679;&#30340;&#35821;&#26009;&#24211;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#29616;&#20195;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#30340;&#35821;&#26009;&#24211;&#35268;&#27169;&#36234;&#26469;&#36234;&#22823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32553;&#23567;&#35757;&#32451;&#35268;&#27169;&#21040;&#19968;&#20010;&#35268;&#27169;&#36866;&#20013;&#12289;&#20195;&#34920;&#24615;&#22909;&#12289;&#24179;&#34913;&#24615;&#22909;&#19988;&#20844;&#24320;&#21487;&#29992;&#30340;&#33521;&#25991;&#25991;&#26412;&#28304;-&#33521;&#22269;&#22269;&#23478;&#35821;&#26009;&#24211;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36825;&#20010;&#31934;&#24515;&#31574;&#21010;&#30340;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;&#21487;&#20197;&#36798;&#21040;&#27604;&#21407;&#22987;BERT&#27169;&#22411;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#31867;&#22411;&#30340;&#35821;&#26009;&#24211;&#20855;&#26377;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;&#22522;&#20934;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#31181;&#28508;&#21147;&#65292;&#25105;&#20204;&#20197;&#20844;&#24179;&#12289;&#21487;&#37325;&#22797;&#21644;&#25968;&#25454;&#26377;&#25928;&#30340;&#27604;&#36739;&#30740;&#31350;&#20026;&#29305;&#33394;&#65292;&#22312;&#20854;&#20013;&#35780;&#20272;&#20102;&#20960;&#20010;&#35757;&#32451;&#30446;&#26631;&#21644;&#27169;&#22411;&#26550;&#26500;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#24335;&#22797;&#21046;&#20102;&#20808;&#21069;&#30340;&#32463;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;LM&#20307;&#31995;&#32467;&#26500;&#31216;&#20026;LTG-BERT&#12290;
&lt;/p&gt;
&lt;p&gt;
While modern masked language models (LMs) are trained on ever larger corpora, we here explore the effects of down-scaling training to a modestly-sized but representative, well-balanced, and publicly available English text source -the British National Corpus. We show that pre-training on this carefully curated corpus can reach better performance than the original BERT model. We argue that this type of corpora has great potential as a language modeling benchmark. To showcase this potential, we present fair, reproducible and data-efficient comparative studies of LMs, in which we evaluate several training objectives and model architectures and replicate previous empirical results in a systematic way. We propose an optimized LM architecture called LTG-BERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2303.09038</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#21644;Prompt Learning&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65306;&#32467;&#26524;&#12289;&#38480;&#21046;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Translating Radiology Reports into Plain Language using ChatGPT and GPT-4 with Prompt Learning: Promising Results, Limitations, and Potential. (arXiv:2303.09038v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#25506;&#35752;&#21033;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;&#29575;&#21644;&#20449;&#24687;&#38169;&#35823;&#29575;&#22343;&#36739;&#20302;&#65292;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#22823;&#37117;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#20854;&#31867;&#20284;&#20154;&#31867;&#34920;&#36798;&#21644;&#25512;&#29702;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#30340;&#21487;&#34892;&#24615;&#65292;&#20197;&#20415;&#24739;&#32773;&#21644;&#21307;&#30103;&#26381;&#21153;&#25552;&#20379;&#32773;&#24471;&#21040;&#26356;&#22909;&#30340;&#21307;&#30103;&#25945;&#32946;&#12290;&#30740;&#31350;&#37319;&#38598;&#20102;62&#20221;&#20302;&#21058;&#37327;&#33016;&#37096;CT&#32954;&#30284;&#31579;&#26597;&#25195;&#25551;&#21644;76&#20221;&#33041;MRI&#36716;&#31227;&#24615;&#31579;&#26597;&#25195;&#25551;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#26681;&#25454;&#25918;&#23556;&#31185;&#21307;&#24072;&#30340;&#35780;&#20215;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#23558;&#25918;&#23556;&#23398;&#25253;&#21578;&#32763;&#35793;&#25104;&#36890;&#20439;&#26131;&#25026;&#30340;&#35821;&#35328;&#65292;&#24179;&#22343;&#24471;&#20998;&#20026;5&#20998;&#21046;&#30340;4.1&#20998;&#65292;&#20449;&#24687;&#32570;&#22833;0.07&#22788;&#65292;&#20449;&#24687;&#38169;&#35823;0.11&#22788;&#12290;&#23601;ChatGPT&#25552;&#20379;&#30340;&#24314;&#35758;&#32780;&#35328;&#65292;&#23427;&#20204;&#26159;&#19968;&#33324;&#24615;&#30340;&#30456;&#20851;&#24314;&#35758;&#65292;&#20363;&#22914;&#20445;&#25345;&#19982;&#21307;&#29983;&#30340;&#38543;&#35775;&#21644;&#23494;&#20999;&#30417;&#27979;&#20219;&#20309;&#30151;&#29366;&#65292;&#23545;&#20110;&#20849;138&#20010;&#30149;&#20363;&#20013;&#30340;&#32422;37&#65285;&#65292;ChatGPT&#25552;&#20379;&#20102;&#19982;&#25918;&#23556;&#23398;&#25253;&#21578;&#26377;&#20851;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The large language model called ChatGPT has drawn extensively attention because of its human-like expression and reasoning abilities. In this study, we investigate the feasibility of using ChatGPT in experiments on using ChatGPT to translate radiology reports into plain language for patients and healthcare providers so that they are educated for improved healthcare. Radiology reports from 62 low-dose chest CT lung cancer screening scans and 76 brain MRI metastases screening scans were collected in the first half of February for this study. According to the evaluation by radiologists, ChatGPT can successfully translate radiology reports into plain language with an average score of 4.1 in the five-point system with 0.07 places of information missing and 0.11 places of misinformation. In terms of the suggestions provided by ChatGPT, they are general relevant such as keeping following-up with doctors and closely monitoring any symptoms, and for about 37% of 138 cases in total ChatGPT offer
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.12095</link><description>&lt;p&gt;
&#35770;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65306;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. (arXiv:2302.12095v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#19978;&#26377;&#19968;&#33268;&#30340;&#20248;&#21183;&#65292;&#20294;&#32477;&#23545;&#34920;&#29616;&#20173;&#26377;&#25552;&#39640;&#31354;&#38388;&#65292;&#40065;&#26834;&#24615;&#20173;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;OpenAI&#26368;&#36817;&#21457;&#24067;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#26381;&#21153;&#65292;&#24182;&#22312;&#36807;&#21435;&#20960;&#20010;&#26376;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#24050;&#23545;ChatGPT&#30340;&#21508;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20294;&#20854;&#40065;&#26834;&#24615;&#65292;&#21363;&#23545;&#20110;&#26410;&#39044;&#26399;&#36755;&#20837;&#30340;&#34920;&#29616;&#65292;&#20173;&#19981;&#28165;&#26970;&#12290;&#40065;&#26834;&#24615;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#20013;&#29305;&#21035;&#21463;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#23545;&#25239;&#24615;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#35282;&#24230;&#23545;ChatGPT&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#20102;&#24443;&#24213;&#35780;&#20272;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;AdvGLUE&#21644;ANLI&#22522;&#20934;&#26469;&#35780;&#20272;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#37319;&#29992;Flipkart&#35780;&#35770;&#21644;DDXPlus&#21307;&#23398;&#35786;&#26029;&#25968;&#25454;&#38598;&#36827;&#34892;OOD&#35780;&#20272;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#22522;&#20934;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;ChatGPT&#22312;&#22823;&#22810;&#25968;&#23545;&#25239;&#24615;&#21644;OOD&#20998;&#31867;&#21644;&#32763;&#35793;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#20248;&#21183;&#12290;&#20294;&#26159;&#65292;&#32477;&#23545;&#30340;&#34920;&#29616;&#36828;&#38750;&#23436;&#32654;&#65292;&#36825;&#34920;&#26126;&#23545;&#25239;&#24615;&#21644;OOD&#40065;&#26834;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is a recent chatbot service released by OpenAI and is receiving increasing attention over the past few months. While evaluations of various aspects of ChatGPT have been done, its robustness, i.e., the performance to unexpected inputs, is still unclear to the public. Robustness is of particular concern in responsible AI, especially for safety-critical applications. In this paper, we conduct a thorough evaluation of the robustness of ChatGPT from the adversarial and out-of-distribution (OOD) perspective. To do so, we employ the AdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart review and DDXPlus medical diagnosis datasets for OOD evaluation. We select several popular foundation models as baselines. Results show that ChatGPT shows consistent advantages on most adversarial and OOD classification and translation tasks. However, the absolute performance is far from perfection, which suggests that adversarial and OOD robustness remains a significant threat 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2212.10537</link><description>&lt;p&gt;
CLIP&#26159;&#21542;&#25414;&#32465;&#27010;&#24565;&#65311;&#25506;&#32034;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#30340;&#32452;&#21512;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does CLIP Bind Concepts? Probing Compositionality in Large Image Models. (arXiv:2212.10537v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;CLIP&#30340;&#32452;&#21512;&#24615;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#23427;&#20204;&#25805;&#20316;&#30340;&#27010;&#24565;&#30340;&#32452;&#25104;&#24615;&#34920;&#31034;&#65292;&#22914;&#36890;&#36807;&#23545;&#8220;&#32418;&#33394;&#31435;&#26041;&#20307;&#8221;&#36827;&#34892;&#25512;&#29702;&#20197;&#27491;&#30830;&#35782;&#21035;&#8220;&#32418;&#33394;&#8221;&#21644;&#8220;&#31435;&#26041;&#20307;&#8221;&#36825;&#20123;&#25104;&#20998;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#19968;&#20010;&#22823;&#22411;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;CLIP&#65289;&#32534;&#30721;&#32452;&#21512;&#27010;&#24565;&#30340;&#33021;&#21147;&#20197;&#21450;&#20197;&#32467;&#26500;&#25935;&#24863;&#30340;&#26041;&#24335;&#25414;&#32465;&#21464;&#37327;&#30340;&#33021;&#21147;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#31435;&#26041;&#20307;&#22312;&#29699;&#20307;&#21518;&#38754;&#8221;&#21644;&#8220;&#29699;&#20307;&#22312;&#31435;&#26041;&#20307;&#21518;&#38754;&#8221;&#65289;&#12290;&#20026;&#20102;&#26816;&#26597;CLIP&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#35768;&#22810;&#26469;&#33258;&#32452;&#21512;&#20998;&#24067;&#35821;&#20041;&#27169;&#22411;&#65288;CDSMs&#65289;&#30340;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#35797;&#22270;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#23454;&#29616;&#20256;&#32479;&#32452;&#21512;&#35821;&#35328;&#32467;&#26500;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#25105;&#20204;&#21457;&#29616;CLIP&#33021;&#22815;&#22312;&#21333;&#19968;&#23545;&#35937;&#30340;&#24773;&#20917;&#19979;&#32452;&#21512;&#27010;&#24565;&#65292;&#20294;&#22312;&#38656;&#35201;&#27010;&#24565;&#25414;&#32465;&#30340;&#24773;&#20917;&#19979;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20984;&#26174;&#20102;&#35780;&#20272;&#22823;&#22411;&#27169;&#22411;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20986;&#20102;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale neural network models combining text and images have made incredible progress in recent years. However, it remains an open question to what extent such models encode compositional representations of the concepts over which they operate, such as correctly identifying ''red cube'' by reasoning over the constituents ''red'' and ''cube''. In this work, we focus on the ability of a large pretrained vision and language model (CLIP) to encode compositional concepts and to bind variables in a structure-sensitive way (e.g., differentiating ''cube behind sphere'' from ''sphere behind cube''). In order to inspect the performance of CLIP, we compare several architectures from research on compositional distributional semantics models (CDSMs), a line of research that attempts to implement traditional compositional linguistic structures within embedding spaces. We find that CLIP can compose concepts in a single-object setting, but in situations where concept binding is needed, performance
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.09561</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#30340;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are reasoners with Self-Verification. (arXiv:2212.09561v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#65292;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#26469;&#26500;&#24314;&#26032;&#26679;&#26412;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#21407;&#22987;&#26465;&#20214;&#65292;&#20197;&#25552;&#39640;&#25512;&#29702;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#26102;&#65292;&#23427;&#38750;&#24120;&#25935;&#24863;&#20110;&#20010;&#21035;&#38169;&#35823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24517;&#39035;&#35757;&#32451;&#39564;&#35777;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#33258;&#25105;&#39564;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;CoT&#30340;&#32467;&#35770;&#20316;&#20026;&#26465;&#20214;&#26469;&#26500;&#24314;&#19968;&#20010;&#26032;&#26679;&#26412;&#65292;&#24182;&#35201;&#27714;LLM&#37325;&#26032;&#39044;&#27979;&#34987;&#25513;&#30422;&#30340;&#21407;&#22987;&#26465;&#20214;&#12290;&#25105;&#20204;&#22522;&#20110;&#20934;&#30830;&#24615;&#35745;&#31639;&#21487;&#35299;&#37322;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20351;&#29992;&#23569;&#37327;&#26679;&#26412;&#23398;&#20064;&#26102;&#25552;&#39640;&#22810;&#20010;&#31639;&#26415;&#21644;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#24050;&#32463;&#35777;&#26126;LLM&#21487;&#20197;&#23545;&#20854;&#33258;&#24049;&#30340;&#32467;&#35770;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#33258;&#25105;&#39564;&#35777;&#24182;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#25512;&#29702;&#24615;&#33021;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24110;&#21161;&#22810;&#31181;&#24102;&#26377;&#33258;&#25105;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
When a large language model (LLM) performs complex reasoning by chain of thought (CoT), it can be highly sensitive to individual mistakes. We have had to train verifiers to address this issue. As we all know, after human inferring a conclusion, they often check it by re-verifying it, which can avoid some mistakes. We propose a new method called self-verification that uses the conclusion of the CoT as a condition to build a new sample and asks the LLM to re-predict the original conditions which be masked. We calculate an explainable verification score based on the accuracy. This method can improve the accuracy of multiple arithmetics and logical reasoning datasets when using few-shot learning. we have demonstrated that LLMs can conduct explainable self-verification of their own conclusions and achieve competitive reasoning performance. Extensive experimentals have demonstrated that our method can help multiple large language models with self-verification can avoid interference from inco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24494;&#35843;&#12290;&#22312;&#24494;&#35843;&#26399;&#38388;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#36825;&#20123;&#21608;&#22260;&#29255;&#27573;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#20943;&#23567;&#36816;&#34892;&#24320;&#38144;&#65292;&#24182;&#22312;S&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.08542</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#24494;&#35843;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Context-aware Fine-tuning of Self-supervised Speech Models. (arXiv:2212.08542v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24494;&#35843;&#12290;&#22312;&#24494;&#35843;&#26399;&#38388;&#20351;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21487;&#20197;&#20351;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#36825;&#20123;&#21608;&#22260;&#29255;&#27573;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#20943;&#23567;&#36816;&#34892;&#24320;&#38144;&#65292;&#24182;&#22312;S&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;transformer&#22312;&#21508;&#31181;&#35821;&#38899;&#20219;&#21153;&#26041;&#38754;&#37117;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#30001;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#30456;&#23545;&#36739;&#30701;&#30340;&#29255;&#27573;&#65288;&#20363;&#22914;&#35821;&#38899;&#65289;&#32423;&#21035;&#19978;&#25805;&#20316;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#24494;&#35843;&#26399;&#38388;&#20351;&#29992;&#19978;&#19979;&#25991;&#65288;&#21363;&#21608;&#22260;&#29255;&#27573;&#65289;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#24863;&#30693;&#24494;&#35843;&#12290;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#21518;&#19968;&#23618;&#19978;&#38468;&#21152;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#27169;&#22359;&#65292;&#23558;&#25972;&#20010;&#29255;&#27573;&#32534;&#30721;&#20026;&#19968;&#20010;&#19978;&#19979;&#25991;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#23558;&#20854;&#29992;&#20316;&#26368;&#32456;&#39044;&#27979;&#30340;&#21478;&#19968;&#20010;&#29305;&#24449;&#12290;&#22312;&#24494;&#35843;&#38454;&#27573;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36741;&#21161;&#25439;&#22833;&#65292;&#40723;&#21169;&#35813;&#19978;&#19979;&#25991;&#23884;&#20837;&#21521;&#37327;&#19982;&#21608;&#22260;&#29255;&#27573;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#30456;&#20284;&#12290;&#36825;&#20801;&#35768;&#27169;&#22411;&#22312;&#25512;&#29702;&#26102;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#36825;&#20123;&#21608;&#22260;&#29255;&#27573;&#65292;&#19982;&#26631;&#20934;&#24494;&#35843;&#27169;&#22411;&#30456;&#27604;&#65292;&#20165;&#38656;&#35201;&#26497;&#23567;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#20351;&#29992;S&#25968;&#25454;&#38598;&#23436;&#25104;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pre-trained transformers have improved the state of the art on a variety of speech tasks. Due to the quadratic time and space complexity of self-attention, they usually operate at the level of relatively short (e.g., utterance) segments. In this paper, we study the use of context, i.e., surrounding segments, during fine-tuning and propose a new approach called context-aware fine-tuning. We attach a context module on top of the last layer of a pre-trained model to encode the whole segment into a context embedding vector which is then used as an additional feature for the final prediction. During the fine-tuning stage, we introduce an auxiliary loss that encourages this context embedding vector to be similar to context vectors of surrounding segments. This allows the model to make predictions without access to these surrounding segments at inference time and requires only a tiny overhead compared to standard fine-tuned models. We evaluate the proposed approach using the S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#29983;&#25104;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;VL-NLE&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2212.04231</link><description>&lt;p&gt;
&#21033;&#29992;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#25552;&#21319;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Power of Multi-Task Pretraining for Ground-Truth Level Natural Language Explanations. (arXiv:2212.04231v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#29983;&#25104;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;VL-NLE&#20219;&#21153;&#30340;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#65292;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#33021;&#22815;&#25552;&#20379;&#23545;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#36807;&#31243;&#30340;&#30452;&#35266;&#29702;&#35299;&#12290;&#24403;&#21069;&#30340;VL-NLE&#27169;&#22411;&#22312;&#20219;&#21153;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#21487;&#20449;&#24230;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#31995;&#21015;&#38382;&#39064;&#65306;&#19968;&#20123;&#27169;&#22411;&#35774;&#35745;&#19978;&#23384;&#22312;&#32570;&#38519;&#65292;&#35299;&#37322;&#29983;&#25104;&#27169;&#22359;&#19982;&#20219;&#21153;&#31572;&#26696;&#39044;&#27979;&#27169;&#22359;&#38598;&#25104;&#19981;&#33391;&#65307;&#22312;&#35757;&#32451;&#39592;&#24178;&#27169;&#22411;&#26102;&#65292;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#36739;&#23567;&#30340;&#24773;&#20917;&#65307;&#27169;&#22411;&#37319;&#29992;&#20020;&#26102;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#24615;&#33021;&#31561;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#29983;&#25104;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#26469;&#24212;&#23545;VL-NLE&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#20248;&#20110;&#26368;&#36817;&#30340;&#27169;&#22411;&#65292;&#22312;&#19977;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#65292;&#20154;&#31867;&#27880;&#37322;&#21592;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#25105;&#20204;&#29983;&#25104;&#30340;&#35299;&#37322;&#32780;&#38750;&#30495;&#23454;&#35299;&#37322;&#12290;&#36825;&#26159;VL-NLE&#30740;&#31350;&#20013;&#30340;&#19968;&#39033;&#26032;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language explanations promise to offer intuitively understandable explanations of a neural network's decision process in complex vision-language tasks, as pursued in recent VL-NLE models. While current models offer impressive performance on task accuracy and explanation plausibility, they suffer from a range of issues: Some models feature a modular design where the explanation generation module is poorly integrated with a separate module for task-answer prediction, employ backbone models trained on limited sets of tasks, or incorporate ad hoc solutions to increase performance on single datasets. We propose to evade these limitations by applying recent advances in large-scale multi-task pretraining of generative Transformer models to the problem of VL-NLE tasks. Our approach outperforms recent models by a large margin, with human annotators preferring the generated explanations over the ground truth in two out of three evaluated datasets. As a novel challenge in VL-NLE research,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2212.04089</link><description>&lt;p&gt;
&#20351;&#29992;&#20219;&#21153;&#31639;&#26415;&#32534;&#36753;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Editing Models with Task Arithmetic. (arXiv:2212.04089v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20219;&#21153;&#21521;&#37327;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26032;&#33539;&#24335;&#65292;&#20219;&#21153;&#21521;&#37327;&#21487;&#36890;&#36807;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#24615;&#33021;&#19988;&#23545;&#25511;&#21046;&#20219;&#21153;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25913;&#21464;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#26041;&#24335;&#65288;&#27604;&#22914;&#25552;&#39640;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#25110;&#20943;&#36731;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#20559;&#24046;&#65289;&#26159;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26102;&#24120;&#35265;&#30340;&#20570;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22260;&#32469;&#8220;&#20219;&#21153;&#21521;&#37327;&#8221;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#34892;&#20026;&#30340;&#26032;&#33539;&#24335;&#12290;&#20219;&#21153;&#21521;&#37327;&#25351;&#23450;&#20102;&#19968;&#20010;&#26041;&#21521;&#65292;&#21363;&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#31354;&#38388;&#20013;&#30340;&#26041;&#21521;&#65292;&#27839;&#30528;&#35813;&#26041;&#21521;&#31227;&#21160;&#21487;&#20197;&#25552;&#39640;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36890;&#36807;&#20174;&#32463;&#36807;&#24494;&#35843;&#20219;&#21153;&#21518;&#30340;&#30456;&#21516;&#27169;&#22411;&#30340;&#26435;&#37325;&#20013;&#20943;&#21435;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26435;&#37325;&#26469;&#26500;&#24314;&#20219;&#21153;&#21521;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#20219;&#21153;&#21521;&#37327;&#21487;&#20197;&#36890;&#36807;&#21542;&#23450;&#21644;&#21152;&#27861;&#31561;&#31639;&#26415;&#25805;&#20316;&#36827;&#34892;&#20462;&#25913;&#21644;&#32452;&#21512;&#65292;&#20174;&#32780;&#24341;&#23548;&#29983;&#25104;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#21542;&#23450;&#20219;&#21153;&#21521;&#37327;&#20250;&#38477;&#20302;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;&#23545;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#24433;&#21709;&#19981;&#22823;&#12290;&#27492;&#22806;&#65292;&#23558;&#20219;&#21153;&#21521;&#37327;&#30456;&#21152;&#21487;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#21644;&#25511;&#21046;&#20219;&#21153;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Changing how pre-trained models behave -- e.g., improving their performance on a downstream task or mitigating biases learned during pre-training -- is a common practice when developing machine learning systems. In this work, we propose a new paradigm for steering the behavior of neural networks, centered around \textit{task vectors}. A task vector specifies a direction in the weight space of a pre-trained model, such that movement in that direction improves performance on the task. We build task vectors by subtracting the weights of a pre-trained model from the weights of the same model after fine-tuning on a task. We show that these task vectors can be modified and combined together through arithmetic operations such as negation and addition, and the behavior of the resulting model is steered accordingly. Negating a task vector decreases performance on the target task, with little change in model behavior on control tasks. Moreover, adding task vectors together can improve performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#35875;&#35328;&#65292;&#24182;&#21487;&#36866;&#24212;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.01117</link><description>&lt;p&gt;
&#22522;&#20110;Prompt&#23398;&#20064;&#19982;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#35875;&#35328;&#65292;&#24182;&#21487;&#36866;&#24212;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#65292;&#35875;&#35328;&#38543;&#30528;&#20107;&#20214;&#30340;&#21457;&#29983;&#32780;&#20256;&#25773;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#36164;&#28304;&#65292;&#24456;&#38590;&#26816;&#27979;&#20986;&#20351;&#29992;&#23569;&#25968;&#35821;&#35328;&#30340;&#35875;&#35328;&#12290;&#32780;&#19988;&#65292;&#26152;&#22825;&#27809;&#26377;&#28041;&#21450;&#21040;&#30340;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#21152;&#21095;&#20102;&#25968;&#25454;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#25110;&#29992;&#19981;&#21516;&#35821;&#35328;&#23637;&#29616;&#30340;&#35875;&#35328;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#22810;&#26679;&#30340;&#20256;&#25773;&#32447;&#31243;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;Prompt&#32534;&#30721;&#26426;&#21046;&#65292;&#23398;&#20064;&#20102;&#26080;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#20419;&#36827;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#19979;&#30340;&#35875;&#35328;&#25968;&#25454;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20256;&#25773;&#32447;&#31243;&#20013;&#24314;&#27169;&#39046;&#22495;&#19981;&#21464;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20197;&#25972;&#21512;&#26377;&#24433;&#21709;&#21147;&#30340;&#31038;&#21306;&#21453;&#24212;&#30340;&#32467;&#26500;&#20301;&#32622;&#34920;&#31034;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#26032;&#30340;&#34394;&#25311;&#21709;&#24212;&#26426;&#21046;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of rumors along with breaking events seriously hinders the truth in the era of social media. Previous studies reveal that due to the lack of annotated resources, rumors presented in minority languages are hard to be detected. Furthermore, the unforeseen breaking events not involved in yesterday's news exacerbate the scarcity of data resources. In this work, we propose a novel zero-shot framework based on prompt learning to detect rumors falling in different domains or presented in different languages. More specifically, we firstly represent rumor circulated on social media as diverse propagation threads, then design a hierarchical prompt encoding mechanism to learn language-agnostic contextual representations for both prompts and rumor data. To further enhance domain adaptation, we model the domain-invariant structural features from the propagation threads, to incorporate structural position representations of influential community response. In addition, a new virtual respon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#20316;&#32773;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;</title><link>http://arxiv.org/abs/2211.07443</link><description>&lt;p&gt;
&#26657;&#20934;&#35299;&#37322;&#65306;&#35821;&#20041;&#35299;&#26512;&#20013;&#30340;&#32622;&#20449;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Calibrated Interpretation: Confidence Estimation in Semantic Parsing. (arXiv:2211.07443v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07443
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#24120;&#35265;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#20316;&#32773;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#36234;&#26469;&#36234;&#34987;&#29992;&#26469;&#23558;&#35821;&#35328;&#32763;&#35793;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#21363;&#25191;&#34892;&#35821;&#20041;&#35299;&#26512;&#12290;&#35821;&#20041;&#35299;&#26512;&#26088;&#22312;&#25191;&#34892;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#21160;&#20316;&#65292;&#22240;&#27492;&#24320;&#21457;&#23433;&#20840;&#31995;&#32479;&#26159;&#26377;&#24517;&#35201;&#30340;&#65292;&#32780;&#27979;&#37327;&#26657;&#20934;&#21017;&#26159;&#23433;&#20840;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#65292;&#22240;&#27492;&#23588;&#20854;&#37325;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#29983;&#25104;&#27169;&#22411;&#22312;&#22235;&#20010;&#27969;&#34892;&#30340;&#35821;&#20041;&#35299;&#26512;&#25968;&#25454;&#38598;&#19978;&#30340;&#26657;&#20934;&#24615;&#65292;&#21457;&#29616;&#20854;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#20043;&#38388;&#21464;&#21270;&#24040;&#22823;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#19982;&#26657;&#20934;&#35823;&#24046;&#30456;&#20851;&#30340;&#22240;&#32032;&#65292;&#24182;&#21457;&#24067;&#20102;&#20004;&#20010;&#35299;&#26512;&#25968;&#25454;&#38598;&#30340;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#25361;&#25112;&#25286;&#20998;&#12290;&#20026;&#20102;&#26041;&#20415;&#23558;&#26657;&#20934;&#32435;&#20837;&#35821;&#20041;&#35299;&#26512;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#26657;&#20934;&#24230;&#37327;&#30340;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sequence generation models are increasingly being used to translate language into executable programs, i.e. to perform executable semantic parsing. The fact that semantic parsing aims to execute actions in the real world motivates developing safe systems, which in turn makes measuring calibration -- a central component to safety -- particularly important. We investigate the calibration of common generation models across four popular semantic parsing datasets, finding that it varies across models and datasets. We then analyze factors associated with calibration error and release new confidence-based challenge splits of two parsing datasets. To facilitate the inclusion of calibration in semantic parsing evaluations, we release a library for computing calibration metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;</title><link>http://arxiv.org/abs/2210.17406</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;
&lt;/p&gt;
&lt;p&gt;
Emergent Linguistic Structures in Neural Networks are Fragile. (arXiv:2210.17406v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.17406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#35821;&#27861;&#30340;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#36890;&#36807;&#22810;&#39033;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20013; emergent &#35821;&#35328;&#32467;&#26500;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#24378;&#21170;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#24182;&#19981;&#33021;&#34913;&#37327;&#27169;&#22411;&#22312;&#20195;&#34920;&#22797;&#26434;&#35821;&#35328;&#32467;&#26500;&#26041;&#38754;&#30340;&#36136;&#37327;&#12290;&#26412;&#25991;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#20195;&#34920;&#35821;&#27861;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#35821;&#35328;&#34920;&#36848;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#20581;&#24615;&#30340;&#26694;&#26550;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20123;&#31283;&#20581;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24230;&#37327;&#26041;&#24335;&#65292;&#36825;&#20123;&#24230;&#37327;&#26041;&#24335;&#21033;&#29992;&#26368;&#36817;&#22312;&#36890;&#36807;&#25506;&#27979;&#20219;&#21153;&#20174;LLM&#20013;&#25552;&#21462;&#35821;&#35328;&#32467;&#26500;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#21363;&#29992;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#20449;&#24687;&#30340;&#31616;&#21333;&#20219;&#21153;&#65292;&#22914;&#35821;&#27861;&#37325;&#26500;&#21644;&#26681;&#35782;&#21035;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#22235;&#31181;LLM&#22312;&#20845;&#20010;&#19981;&#21516;&#30340;&#35821;&#26009;&#24211;&#19978;&#23545;&#35821;&#27861;&#20445;&#25345;&#25200;&#21160;&#30340;&#24615;&#33021;&#21644;&#31283;&#20581;&#24615;&#26469;&#30740;&#31350;&#25152;&#25552;&#20986;&#30340;&#31283;&#20581;&#24230;&#37327;&#26041;&#24335;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been reported to have strong performance on natural language processing tasks. However, performance metrics such as accuracy do not measure the quality of the model in terms of its ability to robustly represent complex linguistic structure. In this paper, focusing on the ability of language models to represent syntax, we propose a framework to assess the consistency and robustness of linguistic representations. To this end, we introduce measures of robustness of neural network models that leverage recent advances in extracting linguistic constructs from LLMs via probing tasks, i.e., simple tasks used to extract meaningful information about a single facet of a language model, such as syntax reconstruction and root identification. Empirically, we study the performance of four LLMs across six different corpora on the proposed robustness measures by analysing their performance and robustness with respect to syntax-preserving perturbations. We provide evide
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.14868</link><description>&lt;p&gt;
&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-lingual Evaluation of Code Generation Models. (arXiv:2210.14868v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#31181;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#37327;&#25552;&#31034;&#33021;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#25317;&#26377;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65306;MBXP&#21644;Multilingual HumanEval&#65292;&#20197;&#21450;MathQA-X&#12290;&#36825;&#20123;&#25968;&#25454;&#38598;&#28085;&#30422;&#20102;10&#31181;&#20197;&#19978;&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#36716;&#25442;&#26694;&#26550;&#23558;&#21407;&#22987;Python&#25968;&#25454;&#38598;&#20013;&#30340;&#25552;&#31034;&#21644;&#27979;&#35797;&#29992;&#20363;&#36716;&#35793;&#25104;&#30446;&#26631;&#35821;&#35328;&#20013;&#30340;&#30456;&#24212;&#25968;&#25454;&#12290;&#21033;&#29992;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#33021;&#22815;&#20197;&#22810;&#35821;&#35328;&#26041;&#24335;&#35780;&#20272;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#39046;&#22495;&#35821;&#35328;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#21333;&#35821;&#35328;&#27169;&#22411;&#19978;&#30340;&#20248;&#21183;&#12289;&#23569;&#37327;&#25552;&#31034;&#25945;&#25480;&#27169;&#22411;&#26032;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450;&#22312;&#21333;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#38646;-shot&#32763;&#35793;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#24341;&#23548;&#65292;&#20197;&#33719;&#21462;&#22810;&#31181;&#35821;&#35328;&#30340;&#21512;&#25104;&#35268;&#33539;&#35299;&#65292;&#36825;&#20123;&#35299;&#21487;&#29992;&#20110;&#20854;&#20182;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#35780;&#20272;&#65292;&#22914;&#20195;&#30721;&#25554;&#20837;&#12289;&#40065;&#26834;&#24615;&#25110;&#25688;&#35201;&#20219;&#21153;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present new benchmarks on evaluation code generation models: MBXP and Multilingual HumanEval, and MathQA-X. These datasets cover over 10 programming languages and are generated using a scalable conversion framework that transpiles prompts and test cases from the original Python datasets into the corresponding data in the target language. Using these benchmarks, we are able to assess the performance of code generation models in a multi-lingual fashion, and discovered generalization ability of language models on out-of-domain languages, advantages of multi-lingual models over mono-lingual, the ability of few-shot prompting to teach the model new languages, and zero-shot translation abilities even on mono-lingual settings. Furthermore, we use our code generation model to perform large-scale bootstrapping to obtain synthetic canonical solutions in several languages, which can be used for other code-related evaluations such as code insertion, robustness, or summarization tasks. Overall, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#25628;&#32034;&#20195;&#29702;&#21644;&#28151;&#21512;&#29615;&#22659;&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#30340;&#22522;&#20110;&#26415;&#35821;&#30340;&#26816;&#32034;&#21644;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#28151;&#21512;&#26816;&#32034;&#29615;&#22659;&#36824;&#21487;&#20197;&#23558;&#22522;&#32447;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2209.15469</link><description>&lt;p&gt;
&#24102;&#26377;&#25628;&#32034;&#20195;&#29702;&#21644;&#28151;&#21512;&#29615;&#22659;&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Retrieval with Search Agents and Hybrid Environments. (arXiv:2209.15469v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#24102;&#26377;&#25628;&#32034;&#20195;&#29702;&#21644;&#28151;&#21512;&#29615;&#22659;&#30340;&#38646;&#26679;&#26412;&#26816;&#32034;&#65292;&#23454;&#39564;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#20256;&#32479;&#30340;&#22522;&#20110;&#26415;&#35821;&#30340;&#26816;&#32034;&#21644;&#31070;&#32463;&#26816;&#32034;&#22120;&#65292;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#28151;&#21512;&#26816;&#32034;&#29615;&#22659;&#36824;&#21487;&#20197;&#23558;&#22522;&#32447;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25628;&#32034;&#26159;&#24314;&#31435;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#20195;&#29702;&#21487;&#20197;&#23398;&#20064;&#20351;&#29992;&#25628;&#32034;&#26694;&#26469;&#33258;&#20027;&#22320;&#26597;&#25214;&#20449;&#24687;&#12290;&#30446;&#21069;&#24050;&#32463;&#34920;&#26126;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#31526;&#21495;&#26597;&#35810;&#37325;&#26500;&#31574;&#30053;&#65292;&#32467;&#21512;&#20256;&#32479;&#30340;&#22522;&#20110;&#26415;&#35821;&#30340;&#26816;&#32034;&#65292;&#20294;&#26080;&#27861;&#36229;&#36234;&#31070;&#32463;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#23558;&#20043;&#21069;&#30340;&#23398;&#20064;&#25628;&#32034;&#35774;&#32622;&#25193;&#23637;&#21040;&#28151;&#21512;&#29615;&#22659;&#20013;&#65292;&#35813;&#29615;&#22659;&#22312;&#36890;&#36807;&#21452;&#32534;&#30721;&#22120;&#30340;&#31532;&#19968;&#36941;&#26816;&#32034;&#27493;&#39588;&#21518;&#25509;&#21463;&#31163;&#25955;&#30340;&#26597;&#35810;&#32454;&#21270;&#25805;&#20316;&#12290;&#22312;BEIR&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#35757;&#32451;&#30340;&#25628;&#32034;&#20195;&#29702;&#20248;&#20110;&#22522;&#20110;&#32452;&#21512;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#22120;&#21644;&#20132;&#21449;&#32534;&#30721;&#22120;&#37325;&#26032;&#25490;&#21517;&#30340;&#24213;&#23618;&#25628;&#32034;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#28151;&#21512;&#26816;&#32034;&#29615;&#22659;&#65288;HRE&#65289;&#21487;&#20197;&#23558;&#22522;&#32447;&#24615;&#33021;&#25552;&#39640;&#20960;&#20010;nDCG&#28857;&#12290;&#22522;&#20110;HRE&#30340;&#25628;&#32034;&#20195;&#29702;&#65288;HARE&#65289;&#19982;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#39046;&#22495;&#20869;&#35780;&#20272;&#20013;&#37117;&#24179;&#34913;&#65292;&#24182;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#34892;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning to search is the task of building artificial agents that learn to autonomously use a search box to find information. So far, it has been shown that current language models can learn symbolic query reformulation policies, in combination with traditional term-based retrieval, but fall short of outperforming neural retrievers. We extend the previous learning to search setup to a hybrid environment, which accepts discrete query refinement operations, after a first-pass retrieval step via a dual encoder. Experiments on the BEIR task show that search agents, trained via behavioral cloning, outperform the underlying search system based on a combined dual encoder retriever and cross encoder reranker. Furthermore, we find that simple heuristic Hybrid Retrieval Environments (HRE) can improve baseline performance by several nDCG points. The search agent based on HRE (HARE) matches state-of-the-art performance, balanced in both zero-shot and in-domain evaluations, via interpretable action
&lt;/p&gt;</description></item><item><title>SmallCap&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#26816;&#32034;&#21040;&#30456;&#20851;&#23383;&#24149;&#20026;&#26465;&#20214;&#29983;&#25104;&#25991;&#23383;&#25551;&#36848;&#12290;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#21363;&#21487;&#36328;&#39046;&#22495;&#36801;&#31227;&#65292;&#36824;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#21644;&#32593;&#32476;&#25968;&#25454;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.15323</link><description>&lt;p&gt;
SmallCap&#65306;&#20197;&#26816;&#32034;&#22686;&#24378;&#20026;&#26465;&#20214;&#30340;&#36731;&#37327;&#32423;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation. (arXiv:2209.15323v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15323
&lt;/p&gt;
&lt;p&gt;
SmallCap&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#26816;&#32034;&#21040;&#30456;&#20851;&#23383;&#24149;&#20026;&#26465;&#20214;&#29983;&#25104;&#25991;&#23383;&#25551;&#36848;&#12290;&#27169;&#22411;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#21363;&#21487;&#36328;&#39046;&#22495;&#36801;&#31227;&#65292;&#36824;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#21644;&#32593;&#32476;&#25968;&#25454;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#25216;&#26415;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25193;&#23637;&#25968;&#25454;&#21644;&#27169;&#22411;&#35268;&#27169;&#65292;&#22823;&#22823;&#22686;&#21152;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#25104;&#26412;&#12290;&#20026;&#20102;&#36991;&#20813;&#22823;&#27169;&#22411;&#30340;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SmallCap&#27169;&#22411;&#65292;&#36890;&#36807;&#20174;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#26816;&#32034;&#21040;&#30340;&#30456;&#20851;&#23383;&#24149;&#20026;&#26465;&#20214;&#29983;&#25104;&#25991;&#23383;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;&#36731;&#37327;&#19988;&#35757;&#32451;&#36895;&#24230;&#24555;&#65292;&#20165;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#30340;CLIP&#32534;&#30721;&#22120;&#21644;GPT-2&#35299;&#30721;&#22120;&#20043;&#38388;&#21152;&#20837;&#26032;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#36827;&#34892;&#23398;&#20064;&#12290;SmallCap&#27169;&#22411;&#26080;&#38656;&#39069;&#22806;&#24494;&#35843;&#21363;&#21487;&#23436;&#25104;&#36328;&#39046;&#22495;&#36801;&#31227;&#65292;&#36824;&#33021;&#22815;&#36731;&#26494;&#21033;&#29992;&#25968;&#25454;&#23384;&#20648;&#24211;&#20013;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20165;&#20351;&#29992;COCO&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;SmallCap&#27169;&#22411;&#22312;&#27492;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#20165;&#36890;&#36807;&#20174;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#20013;&#26816;&#32034;&#23601;&#21487;&#20197;&#36827;&#34892;&#36328;&#22495;&#36801;&#31227;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#20154;&#24037;&#26631;&#27880;&#21644;&#32593;&#32476;&#25968;&#25454;&#65292;SmallCap&#27169;&#22411;&#30340;&#24615;&#33021;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#21644;&#34920;&#24449;&#25289;&#19969;&#32654;&#27954;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#28304;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#20811;&#19992;&#20122;&#35821;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#26696;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20197;&#19981;&#21516;&#29978;&#33267;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#23384;&#22312;&#20559;&#35265;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;</title><link>http://arxiv.org/abs/2207.06591</link><description>&lt;p&gt;
&#22312;&#25289;&#19969;&#32654;&#27954;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#34920;&#29616;&#20986;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#29305;&#24449;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A methodology to characterize bias and harmful stereotypes in natural language processing in Latin America. (arXiv:2207.06591v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.06591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#26816;&#27979;&#21644;&#34920;&#24449;&#25289;&#19969;&#32654;&#27954;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#28304;&#30340;&#20998;&#26512;&#65292;&#24182;&#22312;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#20811;&#19992;&#20122;&#35821;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#26696;&#20363;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20197;&#19981;&#21516;&#29978;&#33267;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#23384;&#22312;&#20559;&#35265;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20915;&#31574;&#31995;&#32479;&#65292;&#23588;&#20854;&#26159;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#31995;&#32479;&#22312;&#25105;&#20204;&#30340;&#29983;&#27963;&#20013;&#26080;&#22788;&#19981;&#22312;&#12290;&#23427;&#20204;&#19981;&#20165;&#26159;&#25105;&#20204;&#27599;&#22825;&#20351;&#29992;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24341;&#25806;&#30340;&#32972;&#21518;&#65292;&#36824;&#26377;&#26356;&#20026;&#20851;&#38190;&#30340;&#20316;&#29992;&#65306;&#31579;&#36873;&#24037;&#20316;&#20505;&#36873;&#20154;&#65292;&#30830;&#23450;&#29359;&#32618;&#23244;&#30097;&#20154;&#65292;&#35786;&#26029;&#33258;&#38381;&#30151;&#31561;&#12290;&#36825;&#20123;&#33258;&#21160;&#21270;&#31995;&#32479;&#20250;&#20986;&#29616;&#38169;&#35823;&#65292;&#36825;&#20123;&#38169;&#35823;&#21487;&#33021;&#22312;&#24456;&#22810;&#26041;&#38754;&#37117;&#26159;&#26377;&#23475;&#30340;&#65292;&#26080;&#35770;&#26159;&#22240;&#20026;&#21518;&#26524;&#30340;&#20005;&#37325;&#24615;&#65288;&#20363;&#22914;&#20581;&#24247;&#38382;&#39064;&#65289;&#36824;&#26159;&#22240;&#20026;&#25152;&#28041;&#21450;&#30340;&#20154;&#25968;&#20043;&#22810;&#12290;&#24403;&#30001;&#33258;&#21160;&#21270;&#31995;&#32479;&#36896;&#25104;&#30340;&#38169;&#35823;&#23545;&#26576;&#20010;&#32676;&#20307;&#30340;&#24433;&#21709;&#36229;&#36807;&#20854;&#20182;&#32676;&#20307;&#26102;&#65292;&#25105;&#20204;&#31216;&#27492;&#31995;&#32479;&#23384;&#22312;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#25216;&#26415;&#26159;&#22522;&#20110;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20174;&#22823;&#37327;&#25991;&#26412;&#20013;&#33719;&#24471;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35789;&#23884;&#20837;&#12290;&#30001;&#20110;&#23427;&#20204;&#26159;&#36890;&#36807;&#24212;&#29992;&#23376;&#31526;&#21495;&#26426;&#22120;&#23398;&#20064;&#21019;&#24314;&#30340;&#65292;&#20027;&#35201;&#26159;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#25152;&#20197;&#23427;&#20204;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#19988;&#26080;&#27861;&#36890;&#36807;&#30452;&#25509;&#26816;&#26597;&#26469;&#35299;&#37322;&#65292;&#22240;&#27492;&#24456;&#38590;&#29702;&#35299;&#36825;&#20123;&#31995;&#32479;&#20309;&#26102;&#23384;&#22312;&#20559;&#35265;&#25110;&#20309;&#26102;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#26816;&#27979;&#21644;&#34920;&#24449;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20013;&#30340;&#20559;&#35265;&#21644;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#65292;&#24212;&#29992;&#20110;&#25289;&#19969;&#32654;&#27954;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#20998;&#26512;&#26469;&#33258;&#19981;&#21516;&#22320;&#21306;&#30340;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#20998;&#26512;&#29992;&#20110;&#35757;&#32451;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#25968;&#25454;&#28304;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38024;&#23545;&#35199;&#29677;&#29273;&#35821;&#12289;&#33889;&#33796;&#29273;&#35821;&#21644;&#20811;&#19992;&#20122;&#35821;&#30340;&#26377;&#23475;&#21051;&#26495;&#21360;&#35937;&#26816;&#27979;&#26696;&#20363;&#30740;&#31350;&#20013;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#20197;&#19981;&#21516;&#29978;&#33267;&#24847;&#24819;&#19981;&#21040;&#30340;&#26041;&#24335;&#23384;&#22312;&#20559;&#35265;&#21644;&#25918;&#22823;&#26377;&#23475;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20984;&#26174;&#20102;&#27979;&#35797;&#21644;&#25913;&#21892;&#27492;&#31867;&#31995;&#32479;&#36136;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated decision-making systems, especially those based on natural language processing, are pervasive in our lives. They are not only behind the internet search engines we use daily, but also take more critical roles: selecting candidates for a job, determining suspects of a crime, diagnosing autism and more. Such automated systems make errors, which may be harmful in many ways, be it because of the severity of the consequences (as in health issues) or because of the sheer number of people they affect. When errors made by an automated system affect a population more than others, we call the system \textit{biased}.  Most modern natural language technologies are based on artifacts obtained from enormous volumes of text using machine learning, namely language models and word embeddings. Since they are created by applying subsymbolic machine learning, mostly artificial neural networks, they are opaque and practically uninterpretable by direct inspection, thus making it very difficult to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ousiometrics&#21644;Telegnomics&#65292;&#21457;&#29616;&#21333;&#35789;&#20256;&#36798;&#30340;&#22522;&#26412;&#21547;&#20041;&#26368;&#22909;&#29992;&#25351;&#21335;&#38024;&#33324;&#30340;&#24378;&#24230;-&#21361;&#38505;&#65288;PD&#65289;&#26694;&#26550;&#26469;&#25551;&#36848;&#65292;&#32780;&#33258;&#28982;&#35821;&#35328;&#23545;&#23433;&#20840;&#20302;&#21361;&#38505;&#30340;&#35789;&#27719;&#26377;&#31995;&#32479;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2110.06847</link><description>&lt;p&gt;
Ousiometrics&#21644;Telegnomics&#65306;&#22522;&#20110;&#24378;&#24230;-&#24369;&#24230;&#21644;&#21361;&#38505;-&#23433;&#20840;&#20004;&#20010;&#32500;&#24230;&#30340;&#24847;&#20041;&#26412;&#36136;&#26694;&#26550;&#65292;&#20855;&#26377;&#23433;&#20840;&#20559;&#35265;&#30340;&#22810;&#26679;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ousiometrics and Telegnomics: The essence of meaning conforms to a two-dimensional powerful-weak and dangerous-safe framework with diverse corpora presenting a safety bias. (arXiv:2110.06847v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.06847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ousiometrics&#21644;Telegnomics&#65292;&#21457;&#29616;&#21333;&#35789;&#20256;&#36798;&#30340;&#22522;&#26412;&#21547;&#20041;&#26368;&#22909;&#29992;&#25351;&#21335;&#38024;&#33324;&#30340;&#24378;&#24230;-&#21361;&#38505;&#65288;PD&#65289;&#26694;&#26550;&#26469;&#25551;&#36848;&#65292;&#32780;&#33258;&#28982;&#35821;&#35328;&#23545;&#23433;&#20840;&#20302;&#21361;&#38505;&#30340;&#35789;&#27719;&#26377;&#31995;&#32479;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23450;&#20041;&#8220;ousiometrics&#8221;&#20026;&#22312;&#20219;&#20309;&#26377;&#24847;&#20041;&#20449;&#21495;&#20256;&#36882;&#30340;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;&#22522;&#26412;&#24847;&#20041;&#30340;&#23398;&#31185;&#65292;&#8220;telegnomics&#8221;&#20026;&#36828;&#31243;&#24863;&#30693;&#30693;&#35782;&#30340;&#30740;&#31350;&#12290;&#36890;&#36807;&#20013;&#26399;&#20986;&#29616;&#30340;&#24037;&#20316;&#65292;&#22522;&#26412;&#24847;&#20041;&#24050;&#34987;&#26222;&#36941;&#25509;&#21463;&#20026;&#30001;&#35780;&#20272;&#65288;evaluation&#65289;&#12289;&#25928;&#33021;&#65288;potency&#65289;&#21644;&#28608;&#27963;&#65288;activation&#65289;&#19977;&#20010;&#27491;&#20132;&#32500;&#24230;&#24456;&#22909;&#22320;&#25429;&#25417;&#12290;&#36890;&#36807;&#37325;&#26032;&#26816;&#26597;&#33521;&#35821;&#35821;&#35328;&#30340;&#31867;&#22411;&#21644;&#26631;&#35760;&#65292;&#20197;&#21450;&#36890;&#36807;&#20351;&#29992;&#33258;&#21160;&#27880;&#37322;&#30340;&#30452;&#26041;&#22270;&#8212;&#8212;&#8220;ousiograms&#8221;&#65292;&#25105;&#20204;&#21457;&#29616;&#65306;1. &#29992;&#21333;&#35789;&#20256;&#36798;&#30340;&#22522;&#26412;&#21547;&#20041;&#26368;&#22909;&#29992;&#25351;&#21335;&#38024;&#33324;&#30340;&#24378;&#24230;-&#21361;&#38505;&#65288;PD&#65289;&#26694;&#26550;&#26469;&#25551;&#36848;&#12290;2. &#23545;&#22823;&#35268;&#27169;&#33521;&#35821;&#35821;&#35328;&#35821;&#26009;&#24211;&#65288;&#25991;&#23398;&#12289;&#26032;&#38395;&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;&#33073;&#21475;&#31168;&#21644;&#31038;&#20132;&#23186;&#20307;&#65289;&#30340;&#20998;&#25955;&#38598;&#21512;&#36827;&#34892;&#20998;&#26512;&#26174;&#31034;&#65292;&#33258;&#28982;&#35821;&#35328;&#23545;&#23433;&#20840;&#12289;&#20302;&#21361;&#38505;&#30340;&#35789;&#27719;&#23384;&#22312;&#31995;&#32479;&#20559;&#35265;&#65292;&#36825;&#26159;&#23545;Pollyanna&#21407;&#21017;&#30340;&#20070;&#38754;&#34920;&#36798;&#31215;&#26497;&#20559;&#24046;&#30340;&#37325;&#26032;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We define `ousiometrics' to be the study of essential meaning in whatever context that meaningful signals are communicated, and `telegnomics' as the study of remotely sensed knowledge. From work emerging through the middle of the 20th century, the essence of meaning has become generally accepted as being well captured by the three orthogonal dimensions of evaluation, potency, and activation (EPA). By re-examining first types and then tokens for the English language, and through the use of automatically annotated histograms -`ousiograms' -- we find here that: 1. The essence of meaning conveyed by words is instead best described by a compass-like power-danger (PD) framework, and 2. Analysis of a disparate collection of large-scale English language corpora -literature, news, Wikipedia, talk radio, and social media -- shows that natural language exhibits a systematic bias toward safe, low danger words -- a reinterpretation of the Pollyanna principle's positivity bias for written expres
&lt;/p&gt;</description></item></channel></rss>