<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05658</link><description>&lt;p&gt;
TidyBot: &#24212;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#26426;&#22120;&#20154;&#29289;&#29702;&#36741;&#21161;
&lt;/p&gt;
&lt;p&gt;
TidyBot: Personalized Robot Assistance with Large Language Models. (arXiv:2305.05658v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#65292;&#26426;&#22120;&#20154;&#21487;&#20197;&#23398;&#20064;&#29992;&#25143;&#30340;&#20559;&#22909;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26377;&#25928;&#20010;&#24615;&#21270;&#22320;&#25552;&#20379;&#29289;&#29702;&#36741;&#21161;&#65292;&#23427;&#24517;&#39035;&#23398;&#20064;&#29992;&#25143;&#30340;&#20010;&#20154;&#21916;&#22909;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#26410;&#26469;&#30340;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#26426;&#22120;&#20154;&#36827;&#34892;&#23478;&#24237;&#28165;&#25195;&#30340;&#20010;&#24615;&#21270;&#38382;&#39064;&#65292;&#36825;&#20123;&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#25441;&#36215;&#29289;&#21697;&#24182;&#23558;&#20854;&#25918;&#22238;&#21407;&#22788;&#26469;&#25972;&#29702;&#25151;&#38388;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#30830;&#23450;&#27599;&#20010;&#29289;&#21697;&#30340;&#27491;&#30830;&#20301;&#32622;&#65292;&#22240;&#20026;&#20154;&#20204;&#30340;&#21916;&#22909;&#21487;&#20197;&#22240;&#20010;&#20154;&#21697;&#21619;&#25110;&#25991;&#21270;&#32972;&#26223;&#32780;&#22823;&#19981;&#30456;&#21516;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#25277;&#23625;&#37324;&#65292;&#32780;&#21478;&#19968;&#20010;&#20154;&#21487;&#33021;&#21916;&#27426;&#25226;&#34924;&#34923;&#25918;&#22312;&#26550;&#23376;&#19978;&#12290;&#25105;&#20204;&#26088;&#22312;&#24314;&#31435;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#36890;&#36807;&#19982;&#29305;&#23450;&#20154;&#30340;&#20808;&#21069;&#20132;&#20114;&#23398;&#20064;&#36825;&#26679;&#30340;&#21916;&#22909;&#65292;&#32780;&#21482;&#38656;&#35201;&#20960;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#21487;&#20197;&#23558;&#22522;&#20110;&#35821;&#35328;&#30340;&#35268;&#21010;&#21644;&#24863;&#30693;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#23569;&#26679;&#26412;&#25688;&#35201;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25512;&#26029;&#20986;&#24191;&#27867;&#36866;&#29992;&#20110;&#26410;&#26469;&#20132;&#20114;&#30340;&#29992;&#25143;&#20559;&#22909;&#12290;&#36825;&#31181;&#26041;&#27861;&#23454;&#29616;&#20102;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#21462;&#24471;&#20102;91.2%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a robot to personalize physical assistance effectively, it must learn user preferences that can be generally reapplied to future scenarios. In this work, we investigate personalization of household cleanup with robots that can tidy up rooms by picking up objects and putting them away. A key challenge is determining the proper place to put each object, as people's preferences can vary greatly depending on personal taste or cultural background. For instance, one person may prefer storing shirts in the drawer, while another may prefer them on the shelf. We aim to build systems that can learn such preferences from just a handful of examples via prior interactions with a particular person. We show that robots can combine language-based planning and perception with the few-shot summarization capabilities of large language models (LLMs) to infer generalized user preferences that are broadly applicable to future interactions. This approach enables fast adaptation and achieves 91.2% accurac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Instruction Tuning (FedIT)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#23545;LLMs&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05644</link><description>&lt;p&gt;
&#20026;&#24314;&#31435;&#32852;&#37030; GPT &#20570;&#20986;&#21162;&#21147;&#65306;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Towards Building the Federated GPT: Federated Instruction Tuning. (arXiv:2305.05644v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Federated Instruction Tuning (FedIT)&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#27169;&#22411;&#23545;LLMs&#36827;&#34892;&#25351;&#20196;&#35843;&#25972;&#65292;&#20197;&#35299;&#20915;&#33719;&#21462;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#20174;&#32780;&#25552;&#39640;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#8220;&#25351;&#20196;&#35843;&#25972;&#8221;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#26032;&#20219;&#21153;&#27010;&#25324;&#33021;&#21147;&#65292;&#20294;&#35757;&#32451;&#38454;&#27573;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#22810;&#26679;&#21644;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65288;&#22914;ChatGPT&#21644;GPT-4&#65289;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;&#29305;&#21035;&#26159;&#20154;&#24037;&#25776;&#20889;&#30340;&#25968;&#25454;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#26174;&#33879;&#30340;&#25104;&#26412;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#19982;&#38544;&#31169;&#26377;&#20851;&#30340;&#38382;&#39064;&#21487;&#33021;&#20250;&#36827;&#19968;&#27493;&#38480;&#21046;&#23545;&#36825;&#20123;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#20351;&#24471;&#33719;&#21462;&#25968;&#25454;&#30340;&#36807;&#31243;&#21464;&#24471;&#22797;&#26434;&#32780;&#24494;&#22937;&#12290;&#22240;&#27492;&#65292;&#36825;&#38480;&#21046;&#20102;&#35843;&#25972;&#27169;&#22411;&#30340;&#36890;&#29992;&#24615;&#24182;&#21487;&#33021;&#38480;&#21046;&#20854;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#30340;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32852;&#37030;&#25351;&#20196;&#35843;&#25972;&#65288;FedIT&#65289;&#65292;&#23427;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;LLMs&#25351;&#20196;&#35843;&#25972;&#30340;&#23398;&#20064;&#26694;&#26550;&#12290;&#36825;&#26159;FL&#22312;LLMs&#25351;&#20196;&#35843;&#25972;&#20013;&#30340;&#39318;&#27425;&#25506;&#32034;&#12290;&#36825;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
While ``instruction-tuned" generative large language models (LLMs) have demonstrated an impressive ability to generalize to new tasks, the training phases heavily rely on large amounts of diverse and high-quality instruction data (such as ChatGPT and GPT-4). Unfortunately, acquiring high-quality data, especially when it comes to human-written data, can pose significant challenges both in terms of cost and accessibility. Moreover, concerns related to privacy can further limit access to such data, making the process of obtaining it a complex and nuanced undertaking. Consequently, this hinders the generality of the tuned models and may restrict their effectiveness in certain contexts. To tackle this issue, our study introduces a new approach called Federated Instruction Tuning (FedIT), which leverages federated learning (FL) as the learning framework for the instruction tuning of LLMs. This marks the first exploration of FL-based instruction tuning for LLMs. This is especially important s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#31890;&#24230;&#26356;&#32454;&#30340;&#26631;&#31614;&#26041;&#26696;&#19978;&#34920;&#29616;&#26356;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#33258;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26368;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.05627</link><description>&lt;p&gt;
&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#22312;&#27861;&#24459;&#21644;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
An Exploration of Encoder-Decoder Approaches to Multi-Label Classification for Legal and Biomedical Text. (arXiv:2305.05627v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#31890;&#24230;&#26356;&#32454;&#30340;&#26631;&#31614;&#26041;&#26696;&#19978;&#34920;&#29616;&#26356;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#33258;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#30340;&#26631;&#20934;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#20165;&#20855;&#26377;&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#22312;&#20854;&#20182;&#20998;&#31867;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#26356;&#26377;&#25928;&#12290;&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#22235;&#31181;&#22810;&#26631;&#31614;&#20998;&#31867;&#26041;&#27861;&#65292;&#20854;&#20013;&#20004;&#31181;&#22522;&#20110;&#20165;&#20855;&#26377;&#32534;&#30721;&#22120;&#65292;&#20004;&#31181;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20854;&#20013;&#20004;&#20010;&#26159;&#27861;&#24459;&#39046;&#22495;&#30340;&#65292;&#20004;&#20010;&#26159;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#65292;&#27599;&#20010;&#25968;&#25454;&#38598;&#37117;&#26377;&#20004;&#20010;&#26631;&#31614;&#31890;&#24230;&#32423;&#21035;&#65292;&#24182;&#22987;&#32456;&#20174;&#21516;&#19968;&#39044;&#35757;&#32451;&#27169;&#22411;T5&#20986;&#21457;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26041;&#27861;&#20248;&#20110;&#20165;&#20855;&#26377;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#22312;&#26356;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#21644;&#26631;&#31614;&#31890;&#24230;&#26356;&#32454;&#30340;&#26631;&#31614;&#26041;&#26696;&#19978;&#20248;&#21183;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#29305;&#21035;&#26159;&#22312;&#38750;&#33258;&#22238;&#24402;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#65292;&#25972;&#20307;&#19978;&#34920;&#29616;&#26368;&#20339;&#65292;&#22240;&#27492;&#25105;&#20204;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#31181;&#26041;&#27861;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard methods for multi-label text classification largely rely on encoder-only pre-trained language models, whereas encoder-decoder models have proven more effective in other classification tasks. In this study, we compare four methods for multi-label classification, two based on an encoder only, and two based on an encoder-decoder. We carry out experiments on four datasets -two in the legal domain and two in the biomedical domain, each with two levels of label granularity- and always depart from the same pre-trained model, T5. Our results show that encoder-decoder methods outperform encoder-only methods, with a growing advantage on more complex datasets and labeling schemes of finer granularity. Using encoder-decoder models in a non-autoregressive fashion, in particular, yields the best performance overall, so we further study this approach through ablations to better understand its strengths.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;GPT4&#21644;GPT3.5&#22312;&#22797;&#26434;&#20020;&#24202;&#30149;&#20363;&#35786;&#26029;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#21327;&#21161;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20316;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#23398;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.05609</link><description>&lt;p&gt;
ChatGPT&#26696;&#20363;&#35760;&#24405;&#65306;&#35821;&#35328;&#27169;&#22411;&#19982;&#22797;&#26434;&#20020;&#24202;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Case Records of ChatGPT: Language Models and Complex Clinical Questions. (arXiv:2305.05609v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05609
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;GPT4&#21644;GPT3.5&#22312;&#22797;&#26434;&#20020;&#24202;&#30149;&#20363;&#35786;&#26029;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#34920;&#26126;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#22312;&#21327;&#21161;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20316;&#29992;&#65292;&#23588;&#20854;&#26159;&#22312;&#21307;&#23398;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#21253;&#25324;&#22312;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#26041;&#38754;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#35768;&#21487;&#32771;&#35797;&#20013;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35299;&#20915;&#22797;&#26434;&#12289;&#24320;&#25918;&#24615;&#30340;&#30149;&#20363;&#30340;&#33021;&#21147;&#65288;&#21487;&#33021;&#20195;&#34920;&#20020;&#24202;&#23454;&#36341;&#65289;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#26041;&#27861;&#65306;&#26412;&#30740;&#31350;&#20351;&#29992;&#40635;&#30465;&#24635;&#21307;&#38498;&#30340;&#26696;&#20363;&#35760;&#24405;&#65292;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;GPT4&#21644;GPT3.5&#22312;&#35786;&#26029;&#22797;&#26434;&#20020;&#24202;&#30149;&#20363;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#20849;&#35782;&#21035;&#20102;50&#20010;&#38656;&#35201;&#35786;&#26029;&#21644;&#35786;&#26029;&#27979;&#35797;&#30340;&#30149;&#20363;&#65292;&#21457;&#34920;&#20110;2022&#24180;1&#26376;1&#26085;&#33267;2022&#24180;4&#26376;16&#26085;&#12290;&#23545;&#20110;&#27599;&#20010;&#30149;&#20363;&#65292;&#27169;&#22411;&#20250;&#25910;&#21040;&#19968;&#20010;&#25552;&#31034;&#65292;&#35831;&#27714;&#21069;&#19977;&#20010;&#20855;&#20307;&#30340;&#35786;&#26029;&#21644;&#30456;&#20851;&#30340;&#35786;&#26029;&#27979;&#35797;&#65292;&#28982;&#21518;&#26159;&#26696;&#20363;&#25991;&#26412;&#12289;&#23454;&#39564;&#23460;&#21644;&#22270;&#20363;&#12290;&#23558;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#26368;&#32456;&#20020;&#24202;&#35786;&#26029;&#36827;&#34892;&#27604;&#36739;&#65292;&#26816;&#26597;&#27169;&#22411;&#39044;&#27979;&#30340;&#27979;&#35797;&#26159;&#21542;&#20250;&#23548;&#33268;&#27491;&#30830;&#30340;&#35786;&#26029;&#12290;&#32467;&#26524;&#65306;GPT3.5&#21644;GPT4&#37117;&#34920;&#29616;&#20986;&#22312;&#29983;&#25104;&#27491;&#30830;&#30340;&#35786;&#26029;&#21644;&#25512;&#33616;&#27979;&#35797;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#24615;&#65292;&#20854;&#20013;GPT4&#30340;&#34920;&#29616;&#20248;&#20110;GPT3.5&#12290;&#32467;&#35770;&#65306;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#21327;&#21161;&#22788;&#29702;&#22797;&#26434;&#20020;&#24202;&#20915;&#31574;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#23398;&#19987;&#23478;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Artificial intelligence language models have shown promise in various applications, including assisting with clinical decision-making as demonstrated by strong performance of large language models on medical licensure exams. However, their ability to solve complex, open-ended cases, which may be representative of clinical practice, remains unexplored. Methods: In this study, the accuracy of large language AI models GPT4 and GPT3.5 in diagnosing complex clinical cases was investigated using published Case Records of the Massachusetts General Hospital. A total of 50 cases requiring a diagnosis and diagnostic test published from January 1, 2022 to April 16, 2022 were identified. For each case, models were given a prompt requesting the top three specific diagnoses and associated diagnostic tests, followed by case text, labs, and figure legends. Model outputs were assessed in comparison to the final clinical diagnosis and whether the model-predicted test would result in a correc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;QA&#39046;&#22495;&#36866;&#24212;&#30340;&#39046;&#22495;&#19981;&#21464;&#24494;&#35843;&#21644;&#23545;&#25239;&#24615;&#26631;&#31614;&#26657;&#27491;&#30340;&#26041;&#27861;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#30446;&#26631;&#22495;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#28304;&#22495;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#28304;&#22495;&#30340;&#30417;&#30563;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05589</link><description>&lt;p&gt;
DomainInv: &#38754;&#21521;QA&#39046;&#22495;&#36866;&#24212;&#30340;&#39046;&#22495;&#19981;&#21464;&#36136;&#37327;&#35843;&#25972;&#19982;&#23545;&#25239;&#24615;&#26631;&#31614;&#30699;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
DomainInv: Domain Invariant Fine Tuning and Adversarial Label Correction For QA Domain Adaptation. (arXiv:2305.05589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#38754;&#21521;QA&#39046;&#22495;&#36866;&#24212;&#30340;&#39046;&#22495;&#19981;&#21464;&#24494;&#35843;&#21644;&#23545;&#25239;&#24615;&#26631;&#31614;&#26657;&#27491;&#30340;&#26041;&#27861;&#12290;&#26356;&#31934;&#30830;&#22320;&#35828;&#65292;&#26412;&#25991;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;&#30446;&#26631;&#22495;&#30340;&#34920;&#31034;&#36716;&#25442;&#20026;&#28304;&#22495;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#28304;&#22495;&#30340;&#30417;&#30563;&#26469;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#26080;&#26631;&#31614;&#30446;&#26631;&#39046;&#22495;&#30340;&#39046;&#22495;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#38382;&#31572;&#65288;QA&#65289;&#31995;&#32479;&#21463;&#38480;&#20110;&#22238;&#31572;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#25110;&#20219;&#20309;&#39046;&#22495;&#20043;&#22806;&#30340;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#37096;&#32626;&#21040;&#30495;&#23454;&#22330;&#26223;&#20013;&#26102;&#19981;&#22826;&#21487;&#38752;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;QA&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#35201;&#20040;&#22522;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#35201;&#20040;&#26159;&#20266;&#26631;&#35760;&#30446;&#26631;&#39046;&#22495;&#25968;&#25454;&#12290;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#20266;&#26631;&#35760;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#35201;&#20040;&#38656;&#35201;&#35745;&#31639;&#36164;&#28304;&#65292;&#35201;&#20040;&#38656;&#35201;&#39069;&#22806;&#30340;&#31934;&#24515;&#36873;&#25321;&#32622;&#20449;&#38408;&#20540;&#26469;&#23558;&#38750;&#24178;&#20928;&#30340;&#26679;&#26412;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20998;&#31163;&#20986;&#26469;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#30446;&#26631;&#34920;&#31034;&#36716;&#31227;&#33267;&#25509;&#36817;&#28304;&#39046;&#22495;&#65292;&#21516;&#26102;&#20173;&#20351;&#29992;&#28304;&#39046;&#22495;&#30340;&#30417;&#30563;&#36827;&#34892;&#22521;&#35757;&#65292;&#20174;&#32780;&#36866;&#24212;&#26631;&#35760;&#30340;&#30446;&#26631;&#39046;&#22495;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39046;&#22495;&#19981;&#21464;&#30340;&#35843;&#20248;&#19982;&#23545;&#25239;&#24615;&#26631;&#31614;&#31998;&#27491;&#30340;&#24819;&#27861;&#65292;&#20197;&#35782;&#21035;&#19982;&#28304;&#22495;&#30456;&#36317;&#36739;&#36828;&#30340;&#30446;&#26631;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing Question Answering (QA) systems limited by the capability of answering questions from unseen domain or any out-of-domain distributions making them less reliable for deployment to real scenarios. Most importantly all the existing QA domain adaptation methods are either based on generating synthetic data or pseudo labeling the target domain data. The domain adaptation methods based on synthetic data and pseudo labeling suffers either from the requirement of computational resources or an extra overhead of carefully selecting the confidence threshold to separate the noisy examples from being in the training dataset. In this paper, we propose the unsupervised domain adaptation for unlabeled target domain by transferring the target representation near to source domain while still using the supervision from source domain. Towards that we proposed the idea of domain invariant fine tuning along with adversarial label correction to identify the target instances which lie far apart from 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.05588</link><description>&lt;p&gt;
StrAE&#65306;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#30340;&#33258;&#32534;&#30721;&#39044;&#35757;&#32451;&#23884;&#20837;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StrAE: Autoencoding for Pre-Trained Embeddings using Explicit Structure. (arXiv:2305.05588v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;StrAE&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#34920;&#29616;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24320;&#21457;StrAE&#36825;&#19968;&#33258;&#32534;&#30721;&#26694;&#26550;&#65292;&#25506;&#31350;&#20102;&#22312;NLP&#20013;&#20351;&#29992;&#26174;&#24335;&#32467;&#26500;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23454;&#29992;&#24615;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#21477;&#23376;&#32467;&#26500;&#26080;&#30417;&#30563;&#22320;&#23398;&#20064;&#22810;&#32423;&#33410;&#28857;&#23884;&#20837;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#23545;&#27604;&#25439;&#22833;&#22312;&#20869;&#30340;&#19981;&#21516;&#31867;&#22411;&#21477;&#23376;&#32467;&#26500;&#21644;&#30446;&#26631;&#19979;&#20351;&#29992;StrAE&#36827;&#34892;&#27169;&#22411;&#35757;&#32451;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#20869;&#22312;&#21644;&#22806;&#22312;&#20219;&#21153;&#19978;&#35780;&#20272;&#25152;&#23398;&#23884;&#20837;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;StrAE&#21033;&#29992;&#26174;&#24335;&#32467;&#26500;&#21487;&#20197;&#25552;&#39640;&#23884;&#20837;&#65292;&#26032;&#30340;&#23545;&#27604;&#30446;&#26631;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#19982;&#20197;&#24448;&#30340;&#20570;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;&#23436;&#20840;&#24544;&#23454;&#20110;&#32467;&#26500;&#30830;&#23454;&#33021;&#22815;&#26681;&#25454;&#30456;&#24212;&#27169;&#22411;&#30340;&#24615;&#33021;&#28040;&#38500;&#32467;&#26500;&#31867;&#22411;&#20043;&#38388;&#30340;&#27495;&#20041;&#12290;&#20316;&#20026;StrAE&#23454;&#29992;&#24615;&#30340;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;p
&lt;/p&gt;
&lt;p&gt;
This work explores the utility of explicit structure for representation learning in NLP by developing StrAE -- an autoencoding framework that faithfully leverages sentence structure to learn multi-level node embeddings in an unsupervised fashion. We use StrAE to train models across different types of sentential structure and objectives, including a novel contrastive loss over structure, and evaluate the learnt embeddings on a series of both intrinsic and extrinsic tasks. Our experiments indicate that leveraging explicit structure through StrAE leads to improved embeddings over prior work, and that our novel contrastive objective over structure outperforms the standard cross-entropy objective. Moreover, in contrast to findings from prior work that weakly leverages structure, we find that being completely faithful to structure does enable disambiguation between types of structure based on the corresponding model's performance. As further evidence of StrAE's utility, we develop a simple p
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20154;&#24615;&#21270;&#25216;&#26415;&#30340;&#26032;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#36234;&#35821;&#35328;&#12289;&#32844;&#19994;&#21644;&#21487;&#35775;&#38382;&#24615;&#20998;&#27495;&#30340;&#20154;&#20204;&#20013;&#12290;&#23427;&#20204;&#36890;&#36807;&#35299;&#20915;&#19977;&#20010;&#26426;&#26800;&#21270;&#29942;&#39048;&#65306;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#21487;&#35775;&#38382;&#30340;&#20869;&#23481;&#65292;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#23383;&#24037;&#20855;&#65292;&#20197;&#21450;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05576</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20154;&#24615;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Humanize Technology. (arXiv:2305.05576v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05576
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#31034;&#20102;&#20154;&#24615;&#21270;&#25216;&#26415;&#30340;&#26032;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#36328;&#36234;&#35821;&#35328;&#12289;&#32844;&#19994;&#21644;&#21487;&#35775;&#38382;&#24615;&#20998;&#27495;&#30340;&#20154;&#20204;&#20013;&#12290;&#23427;&#20204;&#36890;&#36807;&#35299;&#20915;&#19977;&#20010;&#26426;&#26800;&#21270;&#29942;&#39048;&#65306;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#21487;&#35775;&#38382;&#30340;&#20869;&#23481;&#65292;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#23383;&#24037;&#20855;&#65292;&#20197;&#21450;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26368;&#36817;&#20960;&#20010;&#26376;&#21644;&#20960;&#21608;&#20869;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#24341;&#21457;&#20102;&#26377;&#20851;&#23558;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12289;&#23427;&#20204;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#24433;&#21709;&#20197;&#21450;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#28508;&#22312;&#38656;&#35201;&#36827;&#34892;&#30417;&#31649;&#30340;&#25285;&#24551;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35752;&#35770;&#36890;&#24120;&#32570;&#20047;&#20851;&#27880;&#36843;&#20999;&#24615;&#65292;&#21363;&#24191;&#27867;&#20256;&#25773;LLM&#30340;&#31038;&#20250;&#25928;&#30410;&#12290;&#20026;&#20102;&#30830;&#23450;&#36825;&#31181;&#31038;&#20250;&#25928;&#30410;&#65292;&#25105;&#20204;&#26029;&#35328;LLM&#34920;&#29616;&#20986;&#27604;&#20197;&#21069;&#30340;&#25216;&#26415;&#26356;&#26377;&#25928;&#22320;&#20351;&#25216;&#26415;&#20154;&#24615;&#21270;&#30340;&#26032;&#33021;&#21147;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36328;&#36234;&#35821;&#35328;&#12289;&#32844;&#19994;&#21644;&#21487;&#35775;&#38382;&#24615;&#20998;&#27495;&#30340;&#20154;&#20204;&#12290;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#36825;&#26679;&#20570;&#26159;&#36890;&#36807;&#35299;&#20915;&#24403;&#20170;&#35745;&#31639;&#25216;&#26415;&#20013;&#30340;&#19977;&#20010;&#26426;&#26800;&#21270;&#29942;&#39048;&#23454;&#29616;&#30340;&#65306;&#21019;&#24314;&#22810;&#26679;&#21270;&#21644;&#21487;&#35775;&#38382;&#30340;&#20869;&#23481;&#12289;&#23398;&#20064;&#22797;&#26434;&#30340;&#25968;&#23383;&#24037;&#20855;&#20197;&#21450;&#20010;&#24615;&#21270;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#26696;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20004;&#20010;&#31034;&#20363;&#35828;&#26126;&#27599;&#20010;&#29942;&#39048;&#65292;&#20854;&#20013;&#24403;&#21069;&#25216;&#26415;&#26045;&#21152;&#29942;&#39048;&#65292;&#32780;LLM&#35777;&#26126;&#20102;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have made rapid progress in recent months and weeks, garnering significant public attention. This has sparked concerns about aligning these models with human values, their impact on labor markets, and the potential need for regulation in further research and development. However, the discourse often lacks a focus on the imperative to widely diffuse the societal benefits of LLMs. To qualify this societal benefit, we assert that LLMs exhibit emergent abilities to humanize technology more effectively than previous technologies, and for people across language, occupation, and accessibility divides. We argue that they do so by addressing three mechanizing bottlenecks in today's computing technologies: creating diverse and accessible content, learning complex digital tools, and personalizing machine learning algorithms. We adopt a case-based approach and illustrate each bottleneck with two examples where current technology imposes bottlenecks that LLMs demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26356;&#19968;&#33324;&#19979;&#30028;&#24418;&#24335;&#30340;&#25351;&#23548;&#19979;&#35843;&#33410;&#36328;&#27169;&#24577;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#23450;&#28857;&#21644;&#20854;&#36127;&#38754;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.05496</link><description>&lt;p&gt;
&#21033;&#29992;&#20266;&#22270;&#20687;&#35828;&#26126;&#36827;&#34892;&#22810;&#27169;&#24577;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Exploiting Pseudo Image Captions for Multimodal Summarization. (arXiv:2305.05496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#20013;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26356;&#19968;&#33324;&#19979;&#30028;&#24418;&#24335;&#30340;&#25351;&#23548;&#19979;&#35843;&#33410;&#36328;&#27169;&#24577;&#30456;&#20284;&#24230;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#23450;&#28857;&#21644;&#20854;&#36127;&#38754;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#38754;&#20020;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#24403;&#23384;&#22312;&#22122;&#22768;&#26102;&#65292;&#21253;&#25324;&#36127;&#26679;&#26412;&#30340;MI&#20063;&#24456;&#37325;&#35201;&#12290;&#22312;&#26356;&#19968;&#33324;&#30340;&#20248;&#21270;&#19979;&#30028;&#24418;&#24335;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30001;&#36880;&#27493;&#32454;&#21270;&#30340;&#36328;&#27169;&#24577;&#30456;&#20284;&#24230;&#35843;&#33410;&#30340;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#26356;&#31934;&#30830;&#22320;&#20248;&#21270;&#22270;&#20687;/&#25991;&#26412;&#38170;&#23450;&#28857;&#21644;&#20854;&#36127;&#38754;&#25991;&#26412;/&#22270;&#20687;&#20043;&#38388;&#30340;MI&#65292;&#32780;&#19981;&#26159;&#38169;&#35823;&#22320;&#23558;&#20854;&#26368;&#23567;&#21270;&#12290;&#22312;&#22235;&#20010;&#19979;&#28216;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#31454;&#20105;&#21147;&#65292;&#24182;&#22312;&#29702;&#35770;&#25351;&#23548;&#19979;&#31995;&#32479;&#22320;&#24179;&#34913;&#20102;&#65288;&#37096;&#20998;&#65289;&#35823;&#36127;&#26679;&#26412;&#30340;&#26377;&#21033;&#21644;&#26377;&#23475;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal contrastive learning in vision language pretraining (VLP) faces the challenge of (partial) false negatives. In this paper, we study this problem from the perspective of Mutual Information (MI) optimization. It is common sense that InfoNCE loss used in contrastive learning will maximize the lower bound of MI between anchors and their positives, while we theoretically prove that MI involving negatives also matters when noises commonly exist. Guided by a more general lower bound form for optimization, we propose a contrastive learning strategy regulated by progressively refined cross-modal similarity, to more accurately optimize MI between an image/text anchor and its negative texts/images instead of improperly minimizing it. Our method performs competitively on four downstream cross-modal tasks and systematically balances the beneficial and harmful effects of (partial) false negative samples under theoretical guidance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#33258;&#21160;&#25910;&#38598;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#20197;&#24110;&#21161;&#35757;&#32451;&#31070;&#32463; passage &#26816;&#32034;&#22120;&#65292;&#24182;&#36890;&#36807;&#21457;&#24067; MAUPQA &#25968;&#25454;&#38598;&#21644; HerBERT-QA &#31070;&#32463; retriever&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#32570;&#20047;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05486</link><description>&lt;p&gt;
MAUPQA&#65306;&#33258;&#21160;&#21019;&#24314;&#30340;&#28023;&#37327;&#27874;&#20848;&#38382;&#31572;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
MAUPQA: Massive Automatically-created Polish Question Answering Dataset. (arXiv:2305.05486v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#33258;&#21160;&#25910;&#38598;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#20197;&#24110;&#21161;&#35757;&#32451;&#31070;&#32463; passage &#26816;&#32034;&#22120;&#65292;&#24182;&#36890;&#36807;&#21457;&#24067; MAUPQA &#25968;&#25454;&#38598;&#21644; HerBERT-QA &#31070;&#32463; retriever&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#22256;&#38590;&#65292;&#32570;&#20047;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#31995;&#32479;&#24320;&#22987; heavily &#20381;&#36182;&#26631;&#27880;&#25968;&#25454;&#38598;&#20197;&#35757;&#32451;&#31070;&#32463; passage &#26816;&#32034;&#22120;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#26631;&#27880;&#36825;&#20123;&#25968;&#25454;&#38598;&#26082;&#22256;&#38590;&#21448;&#32791;&#26102;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23545;&#19981;&#22826;&#27969;&#34892;&#30340;&#35821;&#35328;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#33258;&#21160;&#25910;&#38598;&#24369;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#31070;&#32463; passage &#26816;&#32034;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;&#25105;&#20204;&#24037;&#20316;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102; MAUPQA &#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#23558;&#36817; 400,000 &#20010;&#27874;&#20848;&#38382;&#31572;&#23545;&#65292;&#20197;&#21450; HerBERT-QA &#31070;&#32463;&#26816;&#32034;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, open-domain question answering systems have begun to rely heavily on annotated datasets to train neural passage retrievers. However, manually annotating such datasets is both difficult and time-consuming, which limits their availability for less popular languages. In this work, we experiment with several methods for automatically collecting weakly labeled datasets and show how they affect the performance of the neural passage retrieval models. As a result of our work, we publish the MAUPQA dataset, consisting of nearly 400,000 question-passage pairs for Polish, as well as the HerBERT-QA neural retriever.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.05480</link><description>&lt;p&gt;
&#25506;&#31350;&#23376;&#35789;&#20998;&#21106;&#23545;transformer&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Investigating the effect of sub-word segmentation on the performance of transformer language models. (arXiv:2305.05480v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#26356;&#39640;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24819;&#30740;&#31350;&#35789;&#27573;&#22914;&#20309;&#24433;&#21709;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#21333;&#35821;&#35789;&#27573;&#31639;&#27861;StateMorph&#65292;&#22312;&#33452;&#20848;&#35821;&#21644;&#20420;&#35821;&#20013;&#35757;&#32451;&#20102;GPT-2&#21644;BERT&#27169;&#22411;&#12290;&#20316;&#20026;&#27604;&#36739;&#65292;&#25105;&#20204;&#36824;&#35757;&#32451;&#20102;&#19968;&#20010;&#20351;&#29992;BPE&#21644;Morfessor&#20998;&#21106;&#31639;&#27861;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;StateMorph&#21487;&#20197;&#24110;&#21161;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#25910;&#25947;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#39564;&#35777;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We would like to explore how morphemes can affect the performance of a language model. We trained GPT-2 and Bert model with StateMorph for both Finnish and Russian, which is a morpheme segmenting algorithm. As a comparison, we also trained a model with BPE and Morfessor. Our preliminary result shows that StateMorph can help the model to converge more efficiently and achieve a better validation score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#23454;&#26102;&#24847;&#22270;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#32454;&#21270;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;33pp&#12290;</title><link>http://arxiv.org/abs/2305.05474</link><description>&lt;p&gt;
&#36229;&#36234;&#30740;&#31350;&#25968;&#25454;&#38598;&#65306;&#24037;&#19994;&#22330;&#26223;&#20013;&#30340;&#26032;&#22411;&#24847;&#22270;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Going beyond research datasets: Novel intent discovery in the industry setting. (arXiv:2305.05474v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05474
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#23454;&#26102;&#24847;&#22270;&#21457;&#29616;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21033;&#29992;&#23545;&#35805;&#32467;&#26500;&#32454;&#21270;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;33pp&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#24847;&#22270;&#21457;&#29616;&#33258;&#21160;&#21270;&#20102;&#23558;&#30456;&#20284;&#30340;&#20449;&#24687;&#65288;&#38382;&#39064;&#65289;&#20998;&#32452;&#20197;&#35782;&#21035;&#20197;&#21069;&#26410;&#30693;&#30340;&#24847;&#22270;&#30340;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20165;&#20855;&#26377;&#38382;&#39064;&#23383;&#27573;&#24182;&#19988;&#19982;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#38598;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#20844;&#20849;&#21487;&#29992;&#25968;&#25454;&#38598;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25913;&#36827;&#22312;&#22823;&#22411;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#20013;&#37096;&#32626;&#30340;&#24847;&#22270;&#21457;&#29616;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39046;&#22495;&#20869;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25910;&#30410;&#65306;&#26082;&#26377;&#33258;&#30417;&#30563;&#30340;&#26041;&#24335;&#65292;&#20063;&#26377;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26368;&#20339;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31934;&#35843;&#32858;&#31867;&#20219;&#21153;&#26399;&#38388;&#21033;&#29992;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#38598;&#30340;&#23545;&#35805;&#32467;&#26500;&#65288;&#21363;&#38382;&#39064;&#21644;&#31572;&#26696;&#65289;&#65292;&#25105;&#20204;&#31216;&#20854;&#20026;Conv&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25152;&#26377;&#26041;&#27861;&#32508;&#21512;&#21033;&#29992;&#20102;&#29616;&#23454;&#29983;&#27963;&#25968;&#25454;&#38598;&#65292;&#20026;&#21482;&#38024;&#23545;&#38382;&#39064;&#30340;Constrained Deep Adaptive Clustering (CDAC)&#27169;&#22411;&#25552;&#20379;&#20102;&#39640;&#36798;33pp&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20165;&#38024;&#23545;&#38382;&#39064;&#25968;&#25454;&#30340;CDAC&#27169;&#22411;&#21482;&#27604;&#22522;&#20934;&#32447;&#39640;&#36798;13pp&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel intent discovery automates the process of grouping similar messages (questions) to identify previously unknown intents. However, current research focuses on publicly available datasets which have only the question field and significantly differ from real-life datasets. This paper proposes methods to improve the intent discovery pipeline deployed in a large e-commerce platform. We show the benefit of pre-training language models on in-domain data: both self-supervised and with weak supervision. We also devise the best method to utilize the conversational structure (i.e., question and answer) of real-life datasets during fine-tuning for clustering tasks, which we call Conv. All our methods combined to fully utilize real-life datasets give up to 33pp performance boost over state-of-the-art Constrained Deep Adaptive Clustering (CDAC) model for question only. By comparison CDAC model for the question data only gives only up to 13pp performance boost over the naive baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NLP4SGPAPERS&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26144;&#23556;&#12289;&#20219;&#21153;&#21450;&#26041;&#27861;&#30340;&#30830;&#23450;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#25972;&#20010;ACL&#25991;&#38598;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#23637;&#31034;&#20102;NLP4SG&#39046;&#22495;&#30340;&#20840;&#35980;&#12290;</title><link>http://arxiv.org/abs/2305.05471</link><description>&lt;p&gt;
&#36229;&#36234;&#21892;&#24847;&#65306;NLP&#29992;&#20110;&#31038;&#20250;&#20844;&#30410;&#30340;&#30740;&#31350;&#29616;&#29366;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Beyond Good Intentions: Reporting the Research Landscape of NLP for Social Good. (arXiv:2305.05471v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NLP4SGPAPERS&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#23545;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#36827;&#34892;&#20998;&#31867;&#12289;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;&#26144;&#23556;&#12289;&#20219;&#21153;&#21450;&#26041;&#27861;&#30340;&#30830;&#23450;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#22312;&#25972;&#20010;ACL&#25991;&#38598;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#23637;&#31034;&#20102;NLP4SG&#39046;&#22495;&#30340;&#20840;&#35980;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24212;&#29992;&#31243;&#24207;&#20986;&#29616;&#22312;&#21508;&#31181;&#29992;&#20363;&#20013;&#12290;&#22312;&#20247;&#22810;&#30340;NLP&#24212;&#29992;&#20013;&#65292;&#35768;&#22810;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#21463;&#21040;&#28608;&#21169;&#65292;&#24076;&#26395;&#36890;&#36807;&#24037;&#20316;&#20855;&#26377;&#31215;&#26497;&#30340;&#31038;&#20250;&#24433;&#21709;&#65292;&#31526;&#21512;NLP for Social Good (NLP4SG)&#30340;&#26368;&#26032;&#20513;&#35758;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#24182;&#19981;&#24635;&#26159;&#28165;&#26970;&#22320;&#20102;&#35299;&#33258;&#24049;&#30340;&#30740;&#31350;&#24037;&#20316;&#22914;&#20309;&#35299;&#20915;&#24403;&#20170;&#30340;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;NLP4SGPAPERS&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#19977;&#20010;&#30456;&#20851;&#20219;&#21153;&#30340;&#31185;&#23398;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;NLP4SG&#35770;&#25991;&#65292;&#24182;&#36890;&#36807;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#23545;NLP4SG&#36827;&#34892;&#25551;&#36848;: (1)&#30830;&#23450;&#35299;&#20915;&#31038;&#20250;&#38382;&#39064;&#30340;&#35770;&#25991;&#65292;(2)&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#30456;&#24212;&#30340;&#32852;&#21512;&#22269;&#21487;&#25345;&#32493;&#21457;&#23637;&#30446;&#26631;(SDGs)&#65292;&#20197;&#21450;(3)&#35782;&#21035;&#23427;&#20204;&#27491;&#22312;&#35299;&#20915;&#30340;&#20219;&#21153;&#21644;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#23558;&#23427;&#20204;&#29992;&#20110;&#25972;&#20010;ACL&#25991;&#38598;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#21487;&#35270;&#21270;&#24037;&#20316;&#21306;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#23545;NLP4SG&#39046;&#22495;&#30340;&#40479;&#30640;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in natural language processing (NLP), a vast number of applications have emerged across various use cases. Among the plethora of NLP applications, many academic researchers are motivated to do work that has a positive social impact, in line with the recent initiatives of NLP for Social Good (NLP4SG). However, it is not always obvious to researchers how their research efforts are tackling today's big social problems. Thus, in this paper, we introduce NLP4SGPAPERS, a scientific dataset with three associated tasks that can help identify NLP4SG papers and characterize the NLP4SG landscape by: (1) identifying the papers that address a social problem, (2) mapping them to the corresponding UN Sustainable Development Goals (SDGs), and (3) identifying the task they are solving and the methods they are using. Using state-of-the-art NLP models, we address each of these tasks and use them on the entire ACL Anthology, resulting in a visualization workspace that gives resear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#23383;&#31526;&#32423;&#21035;BERT&#31867;&#22411;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26500;&#24314;&#21644;&#35757;&#32451;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#22909;&#30340;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#22312;&#30456;&#21516;&#25968;&#25454;&#19978;&#20351;&#29992;&#30456;&#21516;&#35774;&#32622;&#35757;&#32451;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#24050;&#20934;&#22791;&#22909;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#35757;&#32451;&#23383;&#31526;&#32423;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23376;&#35789;&#32423;&#21035;&#30340;&#26631;&#35760;&#12290;</title><link>http://arxiv.org/abs/2305.05461</link><description>&lt;p&gt;
&#23383;&#31526;&#32423;&#21035;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26368;&#20339;&#37197;&#26041;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is the best recipe for character-level encoder-only modelling?. (arXiv:2305.05461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#23383;&#31526;&#32423;&#21035;BERT&#31867;&#22411;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#21644;&#21508;&#31181;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#27604;&#36739;&#65292;&#25214;&#21040;&#20102;&#26500;&#24314;&#21644;&#35757;&#32451;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#26368;&#22909;&#30340;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#22312;&#30456;&#21516;&#25968;&#25454;&#19978;&#20351;&#29992;&#30456;&#21516;&#35774;&#32622;&#35757;&#32451;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#24050;&#20934;&#22791;&#22909;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#35757;&#32451;&#23383;&#31526;&#32423;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23376;&#35789;&#32423;&#21035;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23545;&#20197;&#23383;&#31526;&#32423;&#21035;&#36755;&#20986;&#19978;&#19979;&#25991;&#34920;&#31034;&#30340;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#20013;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36825;&#26679;&#30340;&#24314;&#27169;&#32467;&#26500;&#21644;&#35757;&#32451;&#36825;&#20123;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#20294;&#24403;&#21069;&#20173;&#19981;&#28165;&#26970;&#26550;&#26500;&#19982;&#39044;&#35757;&#32451;&#30446;&#26631;&#23545;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#30340;&#30456;&#23545;&#36129;&#29486;&#12290;&#20316;&#32773;&#25506;&#32034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#27604;&#36739;&#20102;&#20307;&#31995;&#32467;&#26500;&#21019;&#26032;&#21644;&#21508;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#22312;&#19968;&#20010;&#22266;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#19968;&#22871;&#35780;&#20272;&#20219;&#21153;&#26469;&#23547;&#25214;&#30446;&#21069;&#26500;&#24314;&#21644;&#35757;&#32451;&#23383;&#31526;&#32423;&#21035;BERT&#31867;&#22411;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#20316;&#32773;&#21457;&#29616;&#65292;&#26368;&#22909;&#30340;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#22312;&#30456;&#21516;&#25968;&#25454;&#19978;&#20351;&#29992;&#30456;&#21516;&#35774;&#32622;&#35757;&#32451;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#27169;&#22411;&#65292;&#36825;&#34920;&#26126;&#23383;&#31526;&#32423;&#21035;&#27169;&#22411;&#24050;&#20934;&#22791;&#22909;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35757;&#32451;&#23383;&#31526;&#32423;&#27169;&#22411;&#30340;&#26368;&#20339;&#26041;&#27861;&#20173;&#28982;&#20381;&#36182;&#20110;&#23376;&#35789;&#32423;&#21035;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to benchmark recent progress in language understanding models that output contextualised representations at the character level. Many such modelling architectures and methods to train those architectures have been proposed, but it is currently unclear what the relative contributions of the architecture vs. the pretraining objective are to final model performance. We explore the design space of such models, comparing architectural innovations and a variety of different pretraining objectives on a suite of evaluation tasks with a fixed training procedure in order to find the currently optimal way to build and train character-level BERT-like models. We find that our best performing character-level model exceeds the performance of a token-based model trained with the same settings on the same data, suggesting that character-level models are ready for more widespread adoption. Unfortunately, the best method to train character-level models still relies on a subword-level toke
&lt;/p&gt;</description></item><item><title>WikiWeb2M&#26159;&#19968;&#20010;&#20445;&#30041;&#23436;&#25972;&#32593;&#39029;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32467;&#26500;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#32593;&#39029;&#25551;&#36848;&#29983;&#25104;&#12289;&#31456;&#33410;&#25688;&#35201;&#21644;&#19978;&#19979;&#25991;&#22270;&#20687;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05432</link><description>&lt;p&gt;
WikiWeb2M: &#19968;&#20010;&#22522;&#20110;&#39029;&#38754;&#30340;&#22810;&#27169;&#24577;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WikiWeb2M: A Page-Level Multimodal Wikipedia Dataset. (arXiv:2305.05432v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05432
&lt;/p&gt;
&lt;p&gt;
WikiWeb2M&#26159;&#19968;&#20010;&#20445;&#30041;&#23436;&#25972;&#32593;&#39029;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32467;&#26500;&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#32593;&#39029;&#25551;&#36848;&#29983;&#25104;&#12289;&#31456;&#33410;&#25688;&#35201;&#21644;&#19978;&#19979;&#25991;&#22270;&#20687;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#19968;&#30452;&#26159;&#35821;&#35328;&#21644;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#30340;&#20016;&#23500;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#32593;&#39029;&#30340;&#26576;&#20123;&#37096;&#20998;&#34987;&#20445;&#30041;&#19979;&#26469;&#65306;&#22270;&#20687; - &#26631;&#39064;&#23545;&#12289;&#38271;&#25991;&#26412;&#25991;&#31456;&#25110;&#21407;&#22987; HTML&#65292;&#20174;&#26410;&#38598;&#25104;&#21040;&#19968;&#20010;&#22320;&#26041;&#12290;&#22240;&#27492;&#65292;&#32593;&#39029;&#20219;&#21153;&#21463;&#21040;&#30340;&#20851;&#27880;&#24456;&#23569;&#65292;&#32467;&#26500;&#21270;&#30340;&#22270;&#20687; - &#25991;&#26412;&#25968;&#25454;&#34987;&#20302;&#20272;&#20102;&#12290;&#20026;&#20102;&#30740;&#31350;&#22810;&#27169;&#24577;&#32593;&#39029;&#29702;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32500;&#22522;&#30334;&#31185;&#32593;&#39029; 2M&#65288;WikiWeb2M&#65289;&#22871;&#20214;&#65307;&#23427;&#26159;&#31532;&#19968;&#20010;&#20445;&#30041;&#32593;&#39029;&#20013;&#20840;&#37096;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32467;&#26500;&#25968;&#25454;&#30340;&#25968;&#25454;&#38598;&#12290;WikiWeb2M&#21487;&#20197;&#29992;&#20110;&#35832;&#22914;&#39029;&#38754;&#25551;&#36848;&#29983;&#25104;&#12289;&#31456;&#33410;&#25688;&#35201;&#21644;&#19978;&#19979;&#25991;&#22270;&#20687;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Webpages have been a rich resource for language and vision-language tasks. Yet only pieces of webpages are kept: image-caption pairs, long text articles, or raw HTML, never all in one place. Webpage tasks have resultingly received little attention and structured image-text data underused. To study multimodal webpage understanding, we introduce the Wikipedia Webpage 2M (WikiWeb2M) suite; the first to retain the full set of images, text, and structure data available in a page. WikiWeb2M can be used for tasks like page description generation, section summarization, and contextual image captioning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#39532;&#21704;&#24052;&#25289;&#22612;&#20013;&#30340;&#30456;&#20851;&#35789;&#35821;&#12290;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#24182;&#26500;&#24314;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#30456;&#20851;&#24615;&#20998;&#25968;&#21644;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#65292;&#20174;&#32780;&#20174;&#39532;&#21704;&#24052;&#25289;&#22612;&#20013;&#25552;&#21462;&#20986;&#30456;&#20851;&#35789;&#27719;&#12290;</title><link>http://arxiv.org/abs/2305.05420</link><description>&lt;p&gt;
&#21033;&#29992;&#39532;&#21704;&#24052;&#25289;&#22612;&#21465;&#20107;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#35745;&#31639;&#30456;&#20851;&#35789;&#27719;
&lt;/p&gt;
&lt;p&gt;
Estimating related words computationally using language model from the Mahabharata - an Indian epic. (arXiv:2305.05420v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#39532;&#21704;&#24052;&#25289;&#22612;&#20013;&#30340;&#30456;&#20851;&#35789;&#35821;&#12290;&#36890;&#36807;&#23545;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#24182;&#26500;&#24314;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#30456;&#20851;&#24615;&#20998;&#25968;&#21644;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#65292;&#20174;&#32780;&#20174;&#39532;&#21704;&#24052;&#25289;&#22612;&#20013;&#25552;&#21462;&#20986;&#30456;&#20851;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#21704;&#24052;&#25289;&#22612;&#21465;&#20107;&#25991;&#26412;&#26159;&#21360;&#24230;&#26368;&#27969;&#34892;&#30340;&#25991;&#23398;&#20316;&#21697;&#20043;&#19968;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#30446;&#30340;&#34987;&#24341;&#29992;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#26426;&#20132;&#20114;&#30340;&#26102;&#20195;&#65292;&#36825;&#20010;&#25991;&#26412;&#21487;&#20197;&#26681;&#25454;&#39046;&#22495;&#38656;&#27714;&#36827;&#34892;&#22788;&#29702;&#12290;&#22312;&#20998;&#26512;&#39532;&#21704;&#24052;&#25289;&#22612;&#26102;&#65292;&#20154;&#31867;&#20998;&#26512;&#32773;&#20250;&#26377;&#24773;&#24863;&#22240;&#32032;&#65292;&#32780;&#19988;&#20063;&#26080;&#27861;&#35760;&#24518;&#21477;&#23376;&#20013;&#24120;&#35265;&#30340;&#21333;&#35789;&#21644;&#21477;&#23376;&#30340;&#24179;&#22343;&#38271;&#24230;&#31561;&#35745;&#31639;&#32454;&#33410;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#26041;&#27861;&#26469;&#20272;&#35745;&#39532;&#21704;&#24052;&#25289;&#22612;&#20013;&#30340;&#30456;&#20851;&#35789;&#35821;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#23545;&#25991;&#26412;&#36827;&#34892;&#39044;&#22788;&#29702;&#21644;&#26500;&#24314;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#27169;&#22411;&#20272;&#35745;&#21333;&#35789;&#30340;&#20849;&#29616;&#27010;&#29575;&#21644;&#30456;&#20851;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#20272;&#35745;&#20174;&#39532;&#21704;&#24052;&#25289;&#22612;&#20013;&#25552;&#21462;&#20986;&#30340;&#30456;&#20851;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
'Mahabharata' is the most popular among many Indian pieces of literature referred to in many domains for completely different purposes. This text itself is having various dimension and aspects which is useful for the human being in their personal life and professional life. This Indian Epic is originally written in the Sanskrit Language. Now in the era of Natural Language Processing, Artificial Intelligence, Machine Learning, and Human-Computer interaction this text can be processed according to the domain requirement. It is interesting to process this text and get useful insights from Mahabharata. The limitation of the humans while analyzing Mahabharata is that they always have a sentiment aspect towards the story narrated by the author. Apart from that, the human cannot memorize statistical or computational details, like which two words are frequently coming in one sentence? What is the average length of the sentences across the whole literature? Which word is the most popular word a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Holistically Thought&#65288;HoT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32508;&#21512;&#24615;&#24605;&#32771;&#65292;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.05410</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20013;&#38656;&#35201;&#36827;&#34892;&#25972;&#20307;&#24615;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Need Holistically Thought in Medical Conversational QA. (arXiv:2305.05410v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Holistically Thought&#65288;HoT&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32508;&#21512;&#24615;&#24605;&#32771;&#65292;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#31995;&#32479;&#26088;&#22312;&#25552;&#20379;&#19968;&#31995;&#21015;&#19987;&#19994;&#30340;&#21307;&#30103;&#26381;&#21153;&#65292;&#20197;&#25552;&#39640;&#21307;&#30103;&#25252;&#29702;&#25928;&#29575;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#12289;&#36923;&#36753;&#21644;&#24120;&#35782;&#38382;&#31572;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#38543;&#30528;&#21307;&#23398;&#39046;&#22495;&#30340;&#26085;&#30410;&#22797;&#26434;&#21644;&#19987;&#19994;&#21270;&#65292;&#23427;&#20204;&#20173;&#38656;&#35201;&#25552;&#39640;&#12290;&#36825;&#26159;&#22240;&#20026;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#19981;&#20165;&#38656;&#35201;&#24378;&#22823;&#30340;&#21307;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#36824;&#38656;&#35201;&#24191;&#27867;&#28145;&#20837;&#30340;&#24605;&#32500;&#33021;&#21147;&#12290;&#26412;&#25991;&#38024;&#23545;&#36825;&#20123;&#38656;&#35201;&#20174;&#35768;&#22810;&#26041;&#38754;&#32771;&#34385;&#21644;&#29702;&#35299;&#30340;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#20219;&#21153;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20840;&#38754;&#24605;&#32771;&#65288;HoT&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25193;&#25955;&#21644;&#32858;&#28966;&#24605;&#32771;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21307;&#23398;&#21709;&#24212;&#12290;&#25152;&#25552;&#20986;&#30340;HoT&#26041;&#27861;&#24050;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#25163;&#21160;&#35780;&#20272;&#65292;&#22312;&#21253;&#21547;&#33521;&#25991;&#21644;&#20013;&#25991;&#30340;&#19977;&#20010;&#19981;&#21516;&#30340;&#21307;&#30103;&#23545;&#35805;&#38382;&#31572;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The medical conversational question answering (CQA) system aims at providing a series of professional medical services to improve the efficiency of medical care. Despite the success of large language models (LLMs) in complex reasoning tasks in various fields, such as mathematics, logic, and commonsense QA, they still need to improve with the increased complexity and specialization of the medical field. This is because medical CQA tasks require not only strong medical reasoning, but also the ability to think broadly and deeply. In this paper, to address these challenges in medical CQA tasks that need to be considered and understood in many aspects, we propose the Holistically Thought (HoT) method, which is designed to guide the LLMs to perform the diffused and focused thinking for generating high-quality medical responses. The proposed HoT method has been evaluated through automated and manual assessments in three different medical CQA datasets containing the English and Chinese languag
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38754;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#26102;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2305.05403</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#30693;&#35782;&#24211;&#20013;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#38754;&#23545;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30340;&#30693;&#35782;&#26102;&#65292;&#20174;&#19994;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#24212;&#35813;&#22914;&#20309;&#22788;&#29702;&#36825;&#20123;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#30693;&#35782;&#24211;&#26159;&#30693;&#35782;&#20013;&#24515;&#30340;AI&#30340;&#22522;&#30707;&#12290;&#35768;&#22810;&#30693;&#35782;&#24211;&#26159;&#20174;Web&#26469;&#28304;&#23454;&#29992;&#20027;&#20041;&#26500;&#24314;&#30340;&#65292;&#22240;&#27492;&#36828;&#38750;&#23436;&#25972;&#12290;&#36825;&#32473;&#20869;&#23481;&#30340;&#28040;&#36153;&#21644;&#31649;&#29702;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#35843;&#26597;&#35752;&#35770;&#20102;&#22914;&#20309;&#34920;&#36798;&#12289;&#25552;&#21462;&#21644;&#25512;&#26029;&#30693;&#35782;&#24211;&#20013;&#30340;&#23436;&#25972;&#24615;&#12289;&#21484;&#22238;&#29575;&#21644;&#21542;&#23450;&#24615;&#20449;&#24687;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#65288;i&#65289;&#37096;&#20998;&#23553;&#38381;&#19990;&#30028;&#35821;&#20041;&#19979;&#30340;&#30693;&#35782;&#34920;&#31034;&#21644;&#26597;&#35810;&#30340;&#36923;&#36753;&#22522;&#30784;&#65307;&#65288;ii&#65289;&#36890;&#36807;&#32479;&#35745;&#27169;&#24335;&#20272;&#35745;&#27492;&#20449;&#24687;&#65307;&#65288;iii&#65289;&#20174;&#30693;&#35782;&#24211;&#21644;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#20110;&#21484;&#22238;&#29575;&#30340;&#20449;&#24687;&#65307;&#65288;iv&#65289;&#36776;&#21035;&#26377;&#36259;&#30340;&#21542;&#23450;&#35821;&#21477;&#65307;&#20197;&#21450;&#65288;v&#65289;&#30456;&#23545;&#21484;&#22238;&#29575;&#30340;&#23485;&#26494;&#27010;&#24565;&#12290;&#26412;&#35843;&#26597;&#38024;&#23545;&#20004;&#31867;&#21463;&#20247;&#65306;&#65288;1&#65289;&#23547;&#27714;&#22788;&#29702;&#19981;&#23436;&#25972;&#21644;&#19981;&#30830;&#23450;&#30693;&#35782;&#25351;&#21335;&#30340;&#20174;&#19994;&#32773;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26088;&#22312;&#25512;&#36827;&#30693;&#35782;&#24211;&#31649;&#29702;&#12289;&#36136;&#37327;&#35780;&#20272;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#30340;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric AI. Many of them are constructed pragmatically from Web sources, and are thus far from complete. This poses challenges for the consumption as well as the curation of their content. While several surveys target the problem of completing incomplete KBs, the first problem is arguably to know whether and where the KB is incomplete in the first place, and to which degree.  In this survey we discuss how knowledge about completeness, recall, and negation in KBs can be expressed, extracted, and inferred. We cover (i) the logical foundations of knowledge representation and querying under partial closed-world semantics; (ii) the estimation of this information via statistical patterns; (iii) the extraction of information about recall from KBs and text; (iv) the identification of interesting negative statements; and (v) relaxed notions of relative recall.  This survey is targeted at two types of audiences: (1) practitione
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05402</link><description>&lt;p&gt;
&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Consistent Text Categorization using Data Augmentation in e-Commerce. (arXiv:2305.05402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#20351;&#29992;&#25968;&#25454;&#22686;&#24378;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26088;&#22312;&#25913;&#36827;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#20998;&#31867;&#26159;&#19968;&#39033;&#20851;&#38190;&#30340;&#12289;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#26088;&#22312;&#25913;&#36827;&#19968;&#23478;&#20027;&#35201;&#32593;&#32476;&#20844;&#21496;&#24050;&#32463;&#22312;&#20351;&#29992;&#30340;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#22810;&#31181;&#24212;&#29992;&#12290;&#22312;&#35813;&#27169;&#22411;&#26680;&#24515;&#20013;&#65292;&#20135;&#21697;&#20998;&#31867;&#27169;&#22411;&#26159;&#19968;&#20010;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#65292;&#25509;&#21463;&#20135;&#21697;&#26631;&#39064;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#20174;&#25968;&#21315;&#20010;&#21487;&#29992;&#20505;&#36873;&#39033;&#20013;&#36755;&#20986;&#26368;&#21512;&#36866;&#30340;&#31867;&#21035;&#12290;&#32463;&#36807;&#36827;&#19968;&#27493;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31867;&#20284;&#29289;&#21697;&#26631;&#31614;&#19978;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65292;&#26631;&#39064;&#20013;&#20851;&#20110;&#39068;&#33394;&#25110;&#23610;&#23544;&#30340;&#23567;&#21464;&#21270;&#65292;&#20250;&#23545;&#27169;&#22411;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;&#36825;&#31181;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#19979;&#28216;&#30340;&#25512;&#33616;&#25110;&#25628;&#32034;&#24212;&#29992;&#36896;&#25104;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#29992;&#25143;&#20307;&#39564;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#19968;&#33268;&#30340;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20445;&#25345;&#20854;&#29983;&#20135;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The categorization of massive e-Commerce data is a crucial, well-studied task, which is prevalent in industrial settings. In this work, we aim to improve an existing product categorization model that is already in use by a major web company, serving multiple applications. At its core, the product categorization model is a text classification model that takes a product title as an input and outputs the most suitable category out of thousands of available candidates. Upon a closer inspection, we found inconsistencies in the labeling of similar items. For example, minor modifications of the product title pertaining to colors or measurements majorly impacted the model's output. This phenomenon can negatively affect downstream recommendation or search applications, leading to a sub-optimal user experience.  To address this issue, we propose a new framework for consistent text categorization. Our goal is to improve the model's consistency while maintaining its production-level performance. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27861;&#24459;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;CaseEncoder&#65292;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#30340;&#29305;&#27530;&#39046;&#22495;&#38656;&#27714;&#65292;CaseEncoder&#22312;&#25968;&#25454;&#37319;&#26679;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#37117;&#20351;&#29992;&#20102;&#27861;&#24459;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#27861;&#24459;&#26465;&#27454;&#20449;&#24687;&#24341;&#23548;&#27491;&#36127;&#26679;&#26412;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#35774;&#35745;&#20102;&#19982;&#30456;&#20851;&#27861;&#24459;&#26696;&#20363;&#30340;&#35780;&#21028;&#26631;&#20934;&#30456;&#19968;&#33268;&#30340;&#27861;&#24459;&#29305;&#23450;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#21644;&#27861;&#24459;&#38382;&#31572;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PLMs&#12290;</title><link>http://arxiv.org/abs/2305.05393</link><description>&lt;p&gt;
CaseEncoder&#65306;&#19968;&#31181;&#34701;&#21512;&#27861;&#24459;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
CaseEncoder: A Knowledge-enhanced Pre-trained Model for Legal Case Encoding. (arXiv:2305.05393v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27861;&#24459;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;CaseEncoder&#65292;&#38024;&#23545;&#27861;&#24459;&#26696;&#20363;&#30340;&#29305;&#27530;&#39046;&#22495;&#38656;&#27714;&#65292;CaseEncoder&#22312;&#25968;&#25454;&#37319;&#26679;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#20013;&#37117;&#20351;&#29992;&#20102;&#27861;&#24459;&#30693;&#35782;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#27861;&#24459;&#26465;&#27454;&#20449;&#24687;&#24341;&#23548;&#27491;&#36127;&#26679;&#26412;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#35774;&#35745;&#20102;&#19982;&#30456;&#20851;&#27861;&#24459;&#26696;&#20363;&#30340;&#35780;&#21028;&#26631;&#20934;&#30456;&#19968;&#33268;&#30340;&#27861;&#24459;&#29305;&#23450;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#21644;&#27861;&#24459;&#38382;&#31572;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PLMs&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#27861;&#24459;&#20449;&#24687;&#31995;&#32479;&#20013;&#65292;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#26159;&#20851;&#38190;&#30340;&#27969;&#31243;&#12290;&#23613;&#31649;&#36817;&#26399;&#30340;&#30740;&#31350;&#21033;&#29992;&#20102;&#22522;&#20110;&#36890;&#29992;&#39046;&#22495;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#33539;&#24335;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#26500;&#24314;&#20102;&#29992;&#20110;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#20351;&#29992;&#36890;&#29992;&#39046;&#22495;&#30340;PLMs&#20316;&#20026;&#39592;&#24178;&#27169;&#22411;&#26377;&#20854;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#27861;&#24459;&#26696;&#20363;&#25991;&#26723;&#20013;&#30340;&#28508;&#22312;&#27861;&#24459;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CaseEncoder&#65292;&#19968;&#31181;&#27861;&#24459;&#25991;&#26723;&#32534;&#30721;&#22120;&#65292;&#23427;&#22312;&#25968;&#25454;&#37319;&#26679;&#21644;&#39044;&#35757;&#32451;&#38454;&#27573;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#27861;&#24459;&#30693;&#35782;&#12290;&#22312;&#25968;&#25454;&#37319;&#26679;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#32454;&#31890;&#24230;&#30340;&#27861;&#24459;&#26465;&#27454;&#20449;&#24687;&#24341;&#23548;&#27491;&#36127;&#26679;&#26412;&#30340;&#36873;&#25321;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#36136;&#37327;&#12290;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19982;&#30456;&#20851;&#27861;&#24459;&#26696;&#20363;&#30340;&#35780;&#21028;&#26631;&#20934;&#30456;&#19968;&#33268;&#30340;&#27861;&#24459;&#29305;&#23450;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#26681;&#25454;&#36825;&#20123;&#20219;&#21153;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#8212;&#8212;&#20559;&#32622;&#22278;&#24418;&#25439;&#22833;(Biased Circle Loss)&#26469;&#22788;&#29702;&#27861;&#24459;&#26696;&#20363;&#25968;&#25454;&#38598;&#20013;&#27491;&#36127;&#26679;&#26412;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;CaseEncoder&#22312;&#27861;&#24459;&#26696;&#20363;&#26816;&#32034;&#21644;&#27861;&#24459;&#38382;&#31572;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;PLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Legal case retrieval is a critical process for modern legal information systems. While recent studies have utilized pre-trained language models (PLMs) based on the general domain self-supervised pre-training paradigm to build models for legal case retrieval, there are limitations in using general domain PLMs as backbones. Specifically, these models may not fully capture the underlying legal features in legal case documents. To address this issue, we propose CaseEncoder, a legal document encoder that leverages fine-grained legal knowledge in both the data sampling and pre-training phases. In the data sampling phase, we enhance the quality of the training data by utilizing fine-grained law article information to guide the selection of positive and negative examples. In the pre-training phase, we design legal-specific pre-training tasks that align with the judging criteria of relevant legal cases. Based on these tasks, we introduce an innovative loss function called Biased Circle Loss to 
&lt;/p&gt;</description></item><item><title>COKE&#26159;&#19968;&#20010;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;&#65292;&#23558;ToM&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#32463;&#25163;&#21160;&#39564;&#35777;&#30340;&#35748;&#30693;&#38142;&#65292;&#21487;&#20197;&#24110;&#21161;AI&#31995;&#32479;&#22312;&#31038;&#20132;&#26234;&#33021;&#31561;&#20219;&#21153;&#19978;&#20855;&#22791;&#25512;&#29702;&#20154;&#31867;&#24515;&#26234;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05390</link><description>&lt;p&gt;
COKE&#65306;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
COKE: A Cognitive Knowledge Graph for Machine Theory of Mind. (arXiv:2305.05390v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05390
&lt;/p&gt;
&lt;p&gt;
COKE&#26159;&#19968;&#20010;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;&#65292;&#23558;ToM&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#32463;&#25163;&#21160;&#39564;&#35777;&#30340;&#35748;&#30693;&#38142;&#65292;&#21487;&#20197;&#24110;&#21161;AI&#31995;&#32479;&#22312;&#31038;&#20132;&#26234;&#33021;&#31561;&#20219;&#21153;&#19978;&#20855;&#22791;&#25512;&#29702;&#20154;&#31867;&#24515;&#26234;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#26234;&#29702;&#35770;&#65288;ToM&#65289;&#26159;&#25351;&#20154;&#31867;&#29702;&#35299;&#21644;&#25512;&#26029;&#20182;&#20154;&#27442;&#26395;&#12289;&#20449;&#24565;&#21644;&#24847;&#22270;&#30340;&#33021;&#21147;&#12290;&#33719;&#21462;ToM&#23545;&#20154;&#31867;&#30340;&#31038;&#20250;&#35748;&#30693;&#21644;&#20154;&#38469;&#20851;&#31995;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23613;&#31649;ToM&#23545;&#20110;&#31038;&#20132;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#29616;&#20195;AI&#21644;NLP&#31995;&#32479;&#20173;&#28982;&#32570;&#20047;&#35813;&#33021;&#21147;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#35775;&#38382;&#35757;&#32451;&#35821;&#26009;&#24211;&#20043;&#19979;&#30340;&#20154;&#31867;&#24515;&#26234;&#29366;&#24577;&#21644;&#35748;&#30693;&#36807;&#31243;&#12290;&#20026;&#20102;&#36171;&#20104;AI&#31995;&#32479;ToM&#33021;&#21147;&#65292;&#32553;&#23567;&#23427;&#20204;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COKE&#65306;&#31532;&#19968;&#20010;&#26426;&#22120;&#24515;&#26234;&#29702;&#35770;&#30340;&#35748;&#30693;&#30693;&#35782;&#22270;&#35889;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;COKE&#23558;ToM&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;45k+&#32463;&#25163;&#21160;&#39564;&#35777;&#30340;&#35748;&#30693;&#38142;&#65292;&#36825;&#20123;&#38142;&#25551;&#32472;&#20102;&#20154;&#31867;&#22312;&#29305;&#23450;&#31038;&#20132;&#29615;&#22659;&#19979;&#30340;&#24515;&#29702;&#27963;&#21160;&#21644;&#38543;&#21518;&#30340;&#34892;&#20026;/&#24773;&#24863;&#21453;&#24212;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#19968;&#27493;&#25512;&#24191;&#20102;COKE&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#35748;&#30693;&#29983;&#25104;&#27169;&#22411;COKE+&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#26041;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COKE&#23545;&#20110;&#21508;&#31181;&#19982;ToM&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#31038;&#20132;&#24120;&#35782;&#25512;&#29702;&#12289;&#24773;&#24863;&#35782;&#21035;&#21644;&#21487;&#35299;&#37322;&#23545;&#35805;&#65292;&#20855;&#26377;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Theory of mind (ToM) refers to humans' ability to understand and infer the desires, beliefs, and intentions of others. The acquisition of ToM plays a key role in humans' social cognition and interpersonal relations. Though indispensable for social intelligence, ToM is still lacking for modern AI and NLP systems since they cannot access the human mental state and cognitive process beneath the training corpus. To empower AI systems with the ToM ability and narrow the gap between them and humans, in this paper, we propose COKE: the first cognitive knowledge graph for machine theory of mind. Specifically, COKE formalizes ToM as a collection of 45k+ manually verified cognitive chains that characterize human mental activities and subsequent behavioral/affective responses when facing specific social circumstances. Beyond that, we further generalize COKE using pre-trained language models and build a powerful cognitive generation model COKE+. Experimental results in both automatic and human eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20195;&#30721;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21464;&#24322;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;Python&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;CodeExecutor&#27169;&#22411;&#20197;&#22686;&#24378;&#35821;&#20041;&#29702;&#35299;&#65292;&#35813;&#27169;&#22411;&#22312;&#20195;&#30721;&#25191;&#34892;&#12289;&#38646;-shot&#20195;&#30721;&#21040;&#20195;&#30721;&#25628;&#32034;&#21644;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#31561;&#26041;&#38754;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2305.05383</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#30721;&#25191;&#34892;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Code Execution with Pre-trained Language Models. (arXiv:2305.05383v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23545;&#20195;&#30721;&#25191;&#34892;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#21464;&#24322;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;Python&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;CodeExecutor&#27169;&#22411;&#20197;&#22686;&#24378;&#35821;&#20041;&#29702;&#35299;&#65292;&#35813;&#27169;&#22411;&#22312;&#20195;&#30721;&#25191;&#34892;&#12289;&#38646;-shot&#20195;&#30721;&#21040;&#20195;&#30721;&#25628;&#32034;&#21644;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#31561;&#26041;&#38754;&#26377;&#28508;&#22312;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#25191;&#34892;&#26159;&#32534;&#31243;&#35821;&#35328;&#35821;&#20041;&#23398;&#20013;&#30340;&#22522;&#26412;&#26041;&#38754;&#65292;&#23427;&#21453;&#26144;&#20102;&#20195;&#30721;&#30340;&#30830;&#20999;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#38754;&#21521;&#20195;&#30721;&#26234;&#33021;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#24573;&#30053;&#20102;&#25191;&#34892;&#36712;&#36857;&#65292;&#21482;&#20381;&#38752;&#28304;&#20195;&#30721;&#21644;&#21477;&#27861;&#32467;&#26500;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#33021;&#21542;&#29702;&#35299;&#21644;&#25191;&#34892;&#20195;&#30721;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#24322;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#21644;&#36924;&#30495;&#30340;Python&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#65292;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#22914;Codex&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CodeExecutor&#65292;&#19968;&#20010;&#21033;&#29992;&#20195;&#30721;&#25191;&#34892;&#39044;&#35757;&#32451;&#21644;&#35838;&#31243;&#23398;&#20064;&#26469;&#22686;&#24378;&#20854;&#35821;&#20041;&#29702;&#35299;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;&#20195;&#30721;&#25191;&#34892;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#26395;&#30340;&#34920;&#29616;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#23427;&#22312;&#20195;&#30721;&#26234;&#33021;&#20219;&#21153;&#20013;&#30340;&#28508;&#22312;&#22909;&#22788;&#65292;&#22914;&#38646;-shot&#20195;&#30721;&#21040;&#20195;&#30721;&#25628;&#32034;&#21644;&#25991;&#26412;&#21040;&#20195;&#30721;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25552;&#20379;&#20102;&#26377;&#20851;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#21644;&#27867;&#21270;&#30340;&#27934;&#35265;&#65292;&#24182;&#20026;&#20195;&#30721;&#26234;&#33021;&#30340;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#32534;&#30721;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;HTML DOM&#26641;&#30340;&#32593;&#39029;&#20998;&#31867;&#26041;&#27861;PLM-GNN&#65292;&#34920;&#29616;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.05378</link><description>&lt;p&gt;
&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32593;&#39029;&#20998;&#31867;&#26041;&#27861;PLM-GNN
&lt;/p&gt;
&lt;p&gt;
PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network. (arXiv:2305.05378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05378
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#32534;&#30721;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#21644;HTML DOM&#26641;&#30340;&#32593;&#39029;&#20998;&#31867;&#26041;&#27861;PLM-GNN&#65292;&#34920;&#29616;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#39029;&#25968;&#37327;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#31215;&#32047;&#20102;&#22823;&#37327;&#30340;&#32593;&#32476;&#25968;&#25454;&#12290;&#22312;&#32593;&#32476;&#20449;&#24687;&#25366;&#25496;&#20013;&#65292;&#23545;&#32593;&#39029;&#36827;&#34892;&#20998;&#31867;&#26159;&#20854;&#20013;&#30340;&#20851;&#38190;&#36807;&#31243;&#12290;&#19968;&#20123;&#32463;&#20856;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#25163;&#21160;&#26500;&#24314;&#32593;&#39029;&#29305;&#24449;&#65292;&#24182;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#26500;&#24314;&#29305;&#24449;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#30693;&#35782;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#39564;&#35777;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#12290;&#32771;&#34385;&#21040;&#32593;&#39029;&#26159;&#30001;&#25991;&#26412;&#21644;HTML&#25991;&#26723;&#30446;&#26631;&#27169;&#22411;(DOM)&#26641;&#32467;&#21512;&#29983;&#25104;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#21644;&#20998;&#31867;&#26041;&#27861;&#65292;&#31216;&#20026;PLM-GNN&#12290;&#23427;&#26159;&#22522;&#20110;&#23545;&#32593;&#39029;&#20013;&#25991;&#26412;&#21644;HTML DOM&#26641;&#30340;&#32852;&#21512;&#32534;&#30721;&#23454;&#29616;&#30340;&#12290;&#23427;&#22312;KI-04&#21644;SWDE&#25968;&#25454;&#38598;&#20197;&#21450;&#23398;&#32773;&#20027;&#39029;&#29228;&#21462;&#39033;&#30446;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;AHS&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The number of web pages is growing at an exponential rate, accumulating massive amounts of data on the web. It is one of the key processes to classify webpages in web information mining. Some classical methods are based on manually building features of web pages and training classifiers based on machine learning or deep learning. However, building features manually requires specific domain knowledge and usually takes a long time to validate the validity of features. Considering webpages generated by the combination of text and HTML Document Object Model(DOM) trees, we propose a representation and classification method based on a pre-trained language model and graph neural network, named PLM-GNN. It is based on the joint encoding of text and HTML DOM trees in the web pages. It performs well on the KI-04 and SWDE datasets and on practical dataset AHS for the project of scholar's homepage crawling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#32463;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#33719;&#24471;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.05364</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Programs. (arXiv:2305.05364v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#25193;&#23637;&#20854;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#26410;&#32463;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#33719;&#24471;&#19981;&#38169;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#31034;&#20363;&#26469;&#25191;&#34892;&#25351;&#20196;&#24182;&#25191;&#34892;&#26032;&#30340;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#22312;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#21442;&#25968;&#21270;LLMs&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#20197;&#27604;&#24494;&#35843;&#20302;&#24471;&#22810;&#30340;&#25104;&#26412;&#25299;&#23637;&#23427;&#20204;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#19968;&#25512;&#29702;&#32447;&#36335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;LLM&#23884;&#20837;&#31639;&#27861;&#25110;&#31243;&#24207;&#20013;&#65292;&#36827;&#19968;&#27493;&#25193;&#23637;LLM&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35777;&#25454;&#25903;&#25345;&#30340;&#38382;&#31572;&#30340;&#35828;&#26126;&#24615;&#20363;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#26356;&#20855;&#31639;&#27861;&#24615;&#30340;&#26041;&#27861;&#32780;&#27809;&#26377;&#20219;&#20309;&#24494;&#35843;&#65292;&#22312;&#36890;&#36807;&#19968;&#31995;&#21015;&#24605;&#36335;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#33719;&#24471;&#20102;6.4%&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#36825;&#20010;&#35282;&#24230;&#31361;&#20986;&#20102;&#26368;&#36817;&#30340;&#24037;&#20316;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#26631;&#20934;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#31181;&#27169;&#22411;&#20174;&#35770;&#25454;&#25991;&#26412;&#20013;&#26816;&#27979;&#20154;&#31867;&#20215;&#20540;&#65292;&#26368;&#20339;&#32452;&#21512;&#22312;&#20027;&#35201;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24635;&#20307;F1&#20998;&#25968;&#20026;0.48&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05335</link><description>&lt;p&gt;
Rudolf Christoph Eucken&#22312;SemEval-2023&#20219;&#21153;4&#20013;&#65306;&#22522;&#20110;&#38598;&#21512;&#26041;&#27861;&#35782;&#21035;&#35770;&#25454;&#20013;&#20154;&#31867;&#20215;&#20540;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Rudolf Christoph Eucken at SemEval-2023 Task 4: An Ensemble Approach for Identifying Human Values from Arguments. (arXiv:2305.05335v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#19977;&#31181;&#27169;&#22411;&#20174;&#35770;&#25454;&#25991;&#26412;&#20013;&#26816;&#27979;&#20154;&#31867;&#20215;&#20540;&#65292;&#26368;&#20339;&#32452;&#21512;&#22312;&#20027;&#35201;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24635;&#20307;F1&#20998;&#25968;&#20026;0.48&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#32463;&#39564;&#33719;&#24471;&#24494;&#22937;&#30340;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#36825;&#20123;&#20215;&#20540;&#35266;&#25903;&#37197;&#30528;&#25105;&#20204;&#30340;&#24605;&#32500;&#65292;&#24182;&#20307;&#29616;&#22312;&#25105;&#20204;&#30340;&#35328;&#36766;&#20013;&#12290;&#22312;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#20013;&#35782;&#21035;&#36825;&#20123;&#20215;&#20540;&#35266;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#35745;&#31639;&#35770;&#35777;&#26159;&#22788;&#29702;&#20154;&#31867;&#35770;&#35777;&#33021;&#21147;&#30340;&#39046;&#22495;&#65292;&#21487;&#20197;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#20215;&#20540;&#35266;&#21463;&#30410;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#21512;&#26041;&#27861;&#26469;&#20174;&#35770;&#25454;&#25991;&#26412;&#20013;&#26816;&#27979;&#20154;&#31867;&#20215;&#20540;&#12290;&#25105;&#20204;&#30340;&#38598;&#21512;&#21253;&#25324;&#19977;&#20010;&#27169;&#22411;&#65306;&#65288;i&#65289;&#19968;&#31181;&#22522;&#20110;&#34164;&#21547;&#20851;&#31995;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#30830;&#23450;&#22522;&#20110;&#20854;&#25551;&#36848;&#30340;&#20154;&#31867;&#20215;&#20540;&#65292;&#65288;ii&#65289;&#22522;&#20110;Roberta&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#39044;&#27979;&#35770;&#28857;&#20013;&#20154;&#31867;&#20215;&#20540;&#30340;&#38598;&#21512;&#65292;&#65288;iii&#65289;&#22522;&#20110;Roberta&#30340;&#20998;&#31867;&#22120;&#65292;&#21487;&#20197;&#20174;&#35770;&#25454;&#39044;&#27979;&#32553;&#23567;&#30340;&#20154;&#31867;&#20215;&#20540;&#38598;&#21512;&#12290;&#25105;&#20204;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#32452;&#21512;&#26041;&#24335;&#24182;&#25253;&#21578;&#20102;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26368;&#20339;&#30340;&#32452;&#21512;&#22312;&#20027;&#35201;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#24635;&#20307;F1&#20998;&#25968;&#20026;0.48&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The subtle human values we acquire through life experiences govern our thoughts and gets reflected in our speech. It plays an integral part in capturing the essence of our individuality and making it imperative to identify such values in computational systems that mimic human actions. Computational argumentation is a field that deals with the argumentation capabilities of humans and can benefit from identifying such values. Motivated by that, we present an ensemble approach for detecting human values from argument text. Our ensemble comprises three models: (i) An entailment-based model for determining the human values based on their descriptions, (ii) A Roberta-based classifier that predicts the set of human values from an argument. (iii) A Roberta-based classifier to predict a reduced set of human values from an argument. We experiment with different ways of combining the models and report our results. Furthermore, our best combination achieves an overall F1 score of 0.48 on the main 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#20174;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#20013;&#23398;&#24471;&#30340;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#19982;&#21464;&#20998;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#26082;&#20855;&#22791;&#25512;&#33616;&#24615;&#33021;&#21448;&#20855;&#26377;&#35299;&#37322;&#24615;&#33021;&#30340;&#35299;&#37322;&#25512;&#33616;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05331</link><description>&lt;p&gt;
&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#29942;&#39048;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Explainable Recommender with Geometric Information Bottleneck. (arXiv:2305.05331v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05331
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#27169;&#22411;&#65292;&#23558;&#20174;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#20013;&#23398;&#24471;&#30340;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#19982;&#21464;&#20998;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#26082;&#20855;&#22791;&#25512;&#33616;&#24615;&#33021;&#21448;&#20855;&#26377;&#35299;&#37322;&#24615;&#33021;&#30340;&#35299;&#37322;&#25512;&#33616;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#33021;&#22815;&#35299;&#37322;&#20854;&#25512;&#33616;&#20915;&#31574;&#65292;&#22686;&#24378;&#29992;&#25143;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#22823;&#22810;&#25968;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#31995;&#32479;&#35201;&#20040;&#20381;&#36182;&#20110;&#20154;&#24037;&#26631;&#27880;&#30340;&#21407;&#29702;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#29983;&#25104;&#35299;&#37322;&#65292;&#35201;&#20040;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#20174;&#35780;&#35770;&#20013;&#25552;&#21462;&#37325;&#35201;&#30340;&#25991;&#26412;&#27573;&#33853;&#20316;&#20026;&#35299;&#37322;&#12290;&#25552;&#21462;&#30340;&#21407;&#29702;&#24448;&#24448;&#23616;&#38480;&#20110;&#21333;&#20010;&#35780;&#35770;&#65292;&#21487;&#33021;&#26080;&#27861;&#35782;&#21035;&#35780;&#35770;&#25991;&#26412;&#20043;&#22806;&#30340;&#38544;&#21547;&#29305;&#24449;&#12290;&#20026;&#20102;&#36991;&#20813;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#36807;&#31243;&#24182;&#29983;&#25104;&#36229;&#20986;&#21333;&#20010;&#35780;&#35770;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#20174;&#29992;&#25143;-&#21830;&#21697;&#20132;&#20114;&#20013;&#23398;&#24471;&#30340;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#19982;&#21464;&#20998;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#35813;&#32593;&#32476;&#20174;&#29992;&#25143;-&#21830;&#21697;&#35780;&#35770;&#20013;&#25512;&#26029;&#28508;&#22312;&#22240;&#23376;&#12290;&#21333;&#20010;&#29992;&#25143;-&#21830;&#21697;&#23545;&#30340;&#28508;&#22312;&#22240;&#23376;&#21487;&#29992;&#20110;&#25512;&#33616;&#21644;&#35299;&#37322;&#29983;&#25104;&#65292;&#33258;&#28982;&#22320;&#32487;&#25215;&#20102;&#32534;&#30721;&#22312;&#20808;&#39564;&#30693;&#35782;&#20013;&#30340;&#20840;&#23616;&#29305;&#24449;&#12290;&#19977;&#20010;&#30005;&#23376;&#21830;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25512;&#33616;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#37117;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable recommender systems can explain their recommendation decisions, enhancing user trust in the systems. Most explainable recommender systems either rely on human-annotated rationales to train models for explanation generation or leverage the attention mechanism to extract important text spans from reviews as explanations. The extracted rationales are often confined to an individual review and may fail to identify the implicit features beyond the review text. To avoid the expensive human annotation process and to generate explanations beyond individual reviews, we propose to incorporate a geometric prior learnt from user-item interactions into a variational network which infers latent factors from user-item reviews. The latent factors from an individual user-item pair can be used for both recommendation and explanation generation, which naturally inherit the global characteristics encoded in the prior knowledge. Experimental results on three e-commerce datasets show that our mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;transformer&#21644;&#38598;&#25104;&#25216;&#26415;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#65292;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05325</link><description>&lt;p&gt;
&#21033;&#29992;transformer&#21644;&#38598;&#25104;&#25216;&#26415;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;
&lt;/p&gt;
&lt;p&gt;
Detection of depression on social networks using transformers and ensembles. (arXiv:2305.05325v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;transformer&#21644;&#38598;&#25104;&#25216;&#26415;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#19978;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#65292;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#21644;&#20004;&#31181;&#31867;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31185;&#25216;&#22312;&#25105;&#20204;&#29983;&#27963;&#20013;&#30340;&#24433;&#21709;&#19981;&#26029;&#22686;&#24378;&#65292;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#20063;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#23427;&#19981;&#20165;&#26159;&#19968;&#31181;&#27807;&#36890;&#24037;&#20855;&#65292;&#36824;&#21487;&#20197;&#29992;&#26469;&#21521;&#31038;&#21306;&#20998;&#20139;&#25105;&#20204;&#30340;&#35266;&#28857;&#21644;&#24863;&#21463;&#12290;&#23545;&#20110;&#25233;&#37057;&#30151;&#31561;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#65292;&#20154;&#20204;&#20063;&#20250;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#26469;&#34920;&#36798;&#33258;&#24049;&#30340;&#24819;&#27861;&#23547;&#27714;&#24110;&#21161;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#22788;&#29702;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#24182;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#36857;&#35937;&#26469;&#25552;&#20379;&#24110;&#21161;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#22823;&#37327;&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;BERT&#12289;RoBERTA&#12289;BERTweet&#21644;mentalBERT&#65292;&#24182;&#26500;&#24314;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#38598;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;Reddit&#21644;Twitter&#20004;&#20010;&#31038;&#20132;&#24179;&#21488;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#36328;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;transformer&#38598;&#25104;&#27169;&#22411;&#27604;&#21333;&#19968;&#30340;transformer&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the impact of technology on our lives is increasing, we witness increased use of social media that became an essential tool not only for communication but also for sharing information with community about our thoughts and feelings. This can be observed also for people with mental health disorders such as depression where they use social media for expressing their thoughts and asking for help. This opens a possibility to automatically process social media posts and detect signs of depression. We build several large pre-trained language model based classifiers for depression detection from social media posts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also construct two types of ensembles. We analyze the performance of our models on two data sets of posts from social platforms Reddit and Twitter, and investigate also the performance of transfer learning across the two data sets. The results show that transformer ensembles improve over the single transformer-based
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22788;&#29702;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;Pointer Network&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20197;&#21069;&#25552;&#20986;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20026;&#20934;&#30830;&#30340;SSA&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05311</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#22522;&#20110;&#36716;&#31227;&#30340;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Structured Sentiment Analysis as Transition-based Dependency Parsing. (arXiv:2305.05311v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#23558;&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#20316;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22788;&#29702;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#20854;&#22522;&#20110;Pointer Network&#20307;&#31995;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#20197;&#21069;&#25552;&#20986;&#30340;&#22270;&#24418;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#20026;&#20934;&#30830;&#30340;SSA&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#24773;&#24863;&#20998;&#26512;&#65288;SSA&#65289;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#33258;&#21160;&#25552;&#21462;&#20154;&#20204;&#30340;&#35266;&#28857;&#65292;&#24182;&#20197;&#22270;&#24418;&#32467;&#26500;&#20805;&#20998;&#34920;&#31034;&#35813;&#20449;&#24687;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20934;&#30830;&#30340;&#25191;&#34892;SSA&#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#35270;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#20219;&#21153;&#12290;&#23613;&#31649;&#25105;&#20204;&#21487;&#20197;&#22312;&#25991;&#29486;&#20013;&#21457;&#29616;&#22522;&#20110;&#36716;&#31227;&#30340;&#31639;&#27861;&#22312;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20294;&#25152;&#26377;&#23581;&#35797;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;SSA&#30340;&#26041;&#27861;&#37117;&#22522;&#20110;&#22522;&#20110;&#22270;&#24418;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#23558;SSA&#20316;&#20026;&#20381;&#23384;&#21477;&#27861;&#20998;&#26512;&#22788;&#29702;&#30340;&#22522;&#20110;&#36716;&#31227;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36716;&#31227;&#31995;&#32479;&#65292;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#22788;&#29702;&#36755;&#20837;&#25991;&#26412;&#65292;&#36880;&#27493;&#29983;&#25104;&#21253;&#21547;&#25152;&#26377;&#35782;&#21035;&#20986;&#30340;&#35266;&#28857;&#30340;&#22270;&#24418;&#32467;&#26500;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23454;&#29616;&#25105;&#20204;&#30340;&#26368;&#32456;&#22522;&#20110;&#36716;&#31227;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20511;&#21161;&#20102;Pointer Network&#20307;&#31995;&#32467;&#26500;&#20316;&#20026;&#25903;&#25745;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#36229;&#36234;&#20102;SSA&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#22312;&#21253;&#25324;SemEval 2014 Task 4&#22312;&#20869;&#30340;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36804;&#20170;&#20026;&#27490;&#25253;&#21578;&#30340;&#26368;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a dependency parsing task. Although we can find in the literature how transition-based algorithms excel in dependency parsing in terms of accuracy and efficiency, all proposed attempts to tackle SSA following that approach were based on graph-based models. In this article, we present the first transition-based method to address SSA as dependency parsing. Specifically, we design a transition system that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions. To effectively implement our final transition-based model, we resort to a Pointer Network architecture as a backbone. From an extensive evaluation, we demonstrate that our mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#27169;&#22411;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20197;&#33394;&#21015;&#27861;&#38498;&#31995;&#32479;&#23545;&#24615;&#26292;&#21147;&#21463;&#23475;&#32773;&#30340;&#21496;&#27861;&#24577;&#24230;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#8220;&#24378;&#22904;&#31070;&#35805;&#8221;&#22312;&#23545;&#21463;&#23475;&#32773;&#21487;&#20449;&#24230;&#30340;&#21496;&#27861;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.05302</link><description>&lt;p&gt;
&#23436;&#32654;&#30340;&#21463;&#23475;&#32773;&#65306;&#23545;&#24615;&#26292;&#21147;&#21463;&#23475;&#32773;&#21496;&#27861;&#24577;&#24230;&#30340;&#35745;&#31639;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Perfect Victim: Computational Analysis of Judicial Attitudes towards Victims of Sexual Violence. (arXiv:2305.05302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05302
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20351;&#29992;&#35745;&#31639;&#27169;&#22411;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#35780;&#20272;&#20197;&#33394;&#21015;&#27861;&#38498;&#31995;&#32479;&#23545;&#24615;&#26292;&#21147;&#21463;&#23475;&#32773;&#30340;&#21496;&#27861;&#24577;&#24230;&#12290;&#30740;&#31350;&#37325;&#28857;&#20851;&#27880;&#20102;&#8220;&#24378;&#22904;&#31070;&#35805;&#8221;&#22312;&#23545;&#21463;&#23475;&#32773;&#21487;&#20449;&#24230;&#30340;&#21496;&#27861;&#35780;&#20272;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#35745;&#31639;&#27169;&#22411;&#65292;&#20998;&#26512;&#27861;&#24237;&#38472;&#36848;&#20197;&#35780;&#20272;&#20197;&#33394;&#21015;&#27861;&#38498;&#31995;&#32479;&#23545;&#24615;&#26292;&#21147;&#21463;&#23475;&#32773;&#30340;&#21496;&#27861;&#24577;&#24230;&#12290;&#35813;&#30740;&#31350;&#32771;&#23519;&#20102;&#8220;&#24378;&#22904;&#31070;&#35805;&#8221;&#22312;&#21009;&#20107;&#21496;&#27861;&#31995;&#32479;&#23545;&#24615;&#29359;&#32618;&#30340;&#21453;&#24212;&#20013;&#30340;&#20849;&#40483;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#21463;&#23475;&#32773;&#21487;&#20449;&#24230;&#30340;&#21496;&#27861;&#35780;&#20272;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#26412;&#20307;&#35770;&#26469;&#35780;&#20272;&#27861;&#23448;&#23545;&#21463;&#23475;&#32773;&#21487;&#20449;&#24230;&#30340;&#24577;&#24230;&#65292;&#20855;&#26377;&#20843;&#20010;&#24207;&#25968;&#26631;&#31614;&#21644;&#20108;&#20803;&#20998;&#31867;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#24076;&#20271;&#26469;&#35821;&#20013;&#27861;&#23448;&#23545;&#21463;&#23475;&#32773;&#21487;&#20449;&#24230;&#30340;&#35780;&#20272;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27861;&#38498;&#21028;&#20915;&#26696;&#20214;&#20013;&#25552;&#21462;&#21487;&#20449;&#24230;&#26631;&#31614;&#30340;&#27169;&#22411;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;1990&#24180;&#33267;2021&#24180;&#30340;855&#20010;&#24615;&#20405;&#29359;&#26696;&#20214;&#30340;&#21028;&#20915;&#25991;&#20070;&#65292;&#30001;&#27861;&#24459;&#19987;&#23478;&#21644;&#21463;&#36807;&#35757;&#32451;&#30340;&#27861;&#23398;&#29983;&#24110;&#21161;&#27880;&#37322;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#21477;&#27861;&#21644;&#28508;&#22312;&#32467;&#26500;&#30340;&#32452;&#21512;&#26041;&#27861;&#65292;&#25214;&#21040;&#20256;&#36798;&#27861;&#23448;&#23545;&#21463;&#23475;&#32773;&#24577;&#24230;&#30340;&#21477;&#23376;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop computational models to analyze court statements in order to assess judicial attitudes toward victims of sexual violence in the Israeli court system. The study examines the resonance of "rape myths" in the criminal justice system's response to sex crimes, in particular in judicial assessment of victim's credibility. We begin by formulating an ontology for evaluating judicial attitudes toward victim's credibility, with eight ordinal labels and binary categorizations. Second, we curate a manually annotated dataset for judicial assessments of victim's credibility in the Hebrew language, as well as a model that can extract credibility labels from court cases. The dataset consists of 855 verdict decision documents in sexual assault cases from 1990-2021, annotated with the help of legal experts and trained law students. The model uses a combined approach of syntactic and latent structures to find sentences that convey the judge's attitude towards the victim and classify them accor
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#25552;&#20986;&#35757;&#32451;&#25490;&#21517;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#25928;&#29575;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#23454;&#39564;&#34920;&#26126;&#22312;&#36328;&#35821;&#35328;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#19981;&#24433;&#21709;&#21333;&#35821;&#26816;&#32034;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36828;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2305.05295</link><description>&lt;p&gt;
&#36890;&#36807;&#35757;&#32451;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#25968;&#25454;&#26469;&#25552;&#21319;&#38646;&#26679;&#26412;&#36328;&#35821;&#35328;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Boosting Zero-shot Cross-lingual Retrieval by Training on Artificially Code-Switched Data. (arXiv:2305.05295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05295
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#25552;&#20986;&#35757;&#32451;&#25490;&#21517;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#36328;&#35821;&#35328;&#26816;&#32034;&#30340;&#25928;&#29575;&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#20102;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#30340;&#25968;&#25454;&#65292;&#24182;&#19988;&#23454;&#39564;&#34920;&#26126;&#22312;&#36328;&#35821;&#35328;&#26816;&#32034;&#21644;&#22810;&#35821;&#35328;&#26816;&#32034;&#20013;&#20250;&#24102;&#26469;&#26174;&#33879;&#25913;&#36827;&#65292;&#22312;&#19981;&#24433;&#21709;&#21333;&#35821;&#26816;&#32034;&#30340;&#22522;&#30784;&#19978;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#36828;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20197;&#33521;&#35821;&#20026;&#20195;&#34920;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#27169;&#22411;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#36801;&#31227;&#21040;&#20854;&#20182;&#35821;&#35328;&#24050;&#25104;&#20026;&#34987;&#24191;&#27867;&#37319;&#29992;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#24403;&#26597;&#35810;&#21644;&#25991;&#26723;&#20197;&#19981;&#21516;&#35821;&#35328;&#23384;&#22312;&#26102;&#65292;&#38646;&#26679;&#26412;&#25490;&#21517;&#22120;&#30340;&#26377;&#25928;&#24615;&#20250;&#38477;&#20302;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20154;&#24037;&#20195;&#30721;&#20999;&#25442;&#25968;&#25454;&#26469;&#35757;&#32451;&#25490;&#21517;&#27169;&#22411;&#65292;&#32780;&#25105;&#20204;&#29983;&#25104;&#36825;&#20123;&#25968;&#25454;&#26159;&#36890;&#36807;&#21033;&#29992;&#21452;&#35821;&#35789;&#34920;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20174;&#65288;1&#65289;&#36328;&#35821;&#35328;&#35789;&#23884;&#20837;&#21644;&#65288;2&#65289;&#24179;&#34892;&#32500;&#22522;&#30334;&#31185;&#39029;&#38754;&#26631;&#39064;&#24471;&#20986;&#30340;&#35789;&#34920;&#12290;&#25105;&#20204;&#20351;&#29992;mMARCO&#25968;&#25454;&#38598;&#23545;&#28085;&#30422;&#21333;&#35821;IR&#65288;MoIR&#65289;&#12289;&#36328;&#35821;&#35328;IR&#65288;CLIR&#65289;&#21644;&#22810;&#35821;&#35328;IR&#65288;MLIR&#65289;&#30340;36&#31181;&#35821;&#35328;&#23545;&#30340;&#37325;&#25490;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20195;&#30721;&#20999;&#25442;&#21487;&#20197;&#22312;&#20445;&#25345;MoIR&#24615;&#33021;&#31283;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;CLIR&#20013;&#20135;&#29983;5.1 MRR@10&#30340;&#19968;&#33268;&#21644;&#26174;&#33879;&#22686;&#30410;&#65292;&#20197;&#21450;&#22312;MLIR&#20013;&#20135;&#29983;3.9 MRR@10&#30340;&#22686;&#30410;&#12290;&#20196;&#20154;&#40723;&#33310;&#30340;&#26159;&#65292;&#36828;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#22686;&#30410;&#29305;&#21035;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transferring information retrieval (IR) models from a high-resource language (typically English) to other languages in a zero-shot fashion has become a widely adopted approach. In this work, we show that the effectiveness of zero-shot rankers diminishes when queries and documents are present in different languages. Motivated by this, we propose to train ranking models on artificially code-switched data instead, which we generate by utilizing bilingual lexicons. To this end, we experiment with lexicons induced from (1) cross-lingual word embeddings and (2) parallel Wikipedia page titles. We use the mMARCO dataset to extensively evaluate reranking models on 36 language pairs spanning Monolingual IR (MoIR), Cross-lingual IR (CLIR), and Multilingual IR (MLIR). Our results show that code-switching can yield consistent and substantial gains of 5.1 MRR@10 in CLIR and 3.9 MRR@10 in MLIR, while maintaining stable performance in MoIR. Encouragingly, the gains are especially pronounced for distan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#23545;&#35805;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24067;&#26391;&#26725;&#36807;&#31243;&#24314;&#27169;&#23545;&#35805;&#36335;&#24452;&#30340;&#26102;&#38388;&#21160;&#24577;&#20197;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05290</link><description>&lt;p&gt;
&#36890;&#36807;&#24067;&#26391;&#26725;&#38543;&#26426;&#36807;&#31243;&#36827;&#34892;&#30446;&#26631;&#23548;&#21521;&#20027;&#21160;&#23545;&#35805;&#30340;&#23545;&#35805;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Dialogue Planning via Brownian Bridge Stochastic Process for Goal-directed Proactive Dialogue. (arXiv:2305.05290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05290
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#36807;&#31243;&#23545;&#35805;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#24067;&#26391;&#26725;&#36807;&#31243;&#24314;&#27169;&#23545;&#35805;&#36335;&#24452;&#30340;&#26102;&#38388;&#21160;&#24577;&#20197;&#23454;&#29616;&#30446;&#26631;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#22411;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#22810;&#36718;&#23545;&#35805;&#20027;&#21160;&#22320;&#36798;&#21040;&#39044;&#20808;&#30830;&#23450;&#30340;&#30446;&#26631;&#12290;&#23454;&#29616;&#27492;&#20219;&#21153;&#30340;&#20851;&#38190;&#22312;&#20110;&#35268;&#21010;&#23545;&#35805;&#36335;&#24452;&#65292;&#20351;&#20854;&#24179;&#31283;&#24182;&#36830;&#36143;&#22320;&#25351;&#21521;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#36825;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#24182;&#26410;&#34987;&#28145;&#20837;&#25506;&#31350;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#36143;&#23545;&#35805;&#35268;&#21010;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#26469;&#24314;&#27169;&#23545;&#35805;&#36335;&#24452;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#36890;&#36807;&#24067;&#26391;&#26725;&#36807;&#31243;&#25429;&#25417;&#20102;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#30340;&#36830;&#36143;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#25105;&#20204;&#28789;&#27963;&#22320;&#23558;&#29992;&#25143;&#21453;&#39304;&#32435;&#20837;&#23545;&#35805;&#35268;&#21010;&#12290;&#22522;&#20110;&#23548;&#20986;&#30340;&#28508;&#22312;&#36712;&#36857;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26174;&#24335;&#22320;&#29983;&#25104;&#23545;&#35805;&#36335;&#24452;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#36335;&#24452;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24341;&#23548;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#26356;&#36830;&#36143;&#30340;&#35805;&#35821;&#65292;&#24182;&#20197;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#23454;&#29616;&#20102;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Goal-directed dialogue systems aim to proactively reach a pre-determined target through multi-turn conversations. The key to achieving this task lies in planning dialogue paths that smoothly and coherently direct conversations towards the target. However, this is a challenging and under-explored task. In this work, we propose a coherent dialogue planning approach that uses a stochastic process to model the temporal dynamics of dialogue paths. We define a latent space that captures the coherence of goal-directed behavior using a Brownian bridge process, which allows us to incorporate user feedback flexibly in dialogue planning. Based on the derived latent trajectories, we generate dialogue paths explicitly using pre-trained language models. We finally employ these paths as natural language prompts to guide dialogue generation. Our experiments show that our approach generates more coherent utterances and achieves the goal with a higher success rate.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;</title><link>http://arxiv.org/abs/2305.05280</link><description>&lt;p&gt;
VCSUM&#65306;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VCSUM: A Versatile Chinese Meeting Summarization Dataset. (arXiv:2305.05280v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05280
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312;GitHub&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#26032;&#38395;&#21644;&#32842;&#22825;&#25688;&#35201;&#30456;&#27604;&#65292;&#30001;&#20110;&#25968;&#25454;&#21463;&#38480;&#65292;&#20250;&#35758;&#25688;&#35201;&#30340;&#21457;&#23637;&#21463;&#21040;&#26497;&#22823;&#30340;&#20943;&#36895;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#20013;&#25991;&#20250;&#35758;&#25688;&#35201;&#25968;&#25454;&#38598;VCSum&#65292;&#21253;&#25324;239&#20010;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#20250;&#35758;&#65292;&#24635;&#26102;&#38271;&#36229;&#36807;230&#23567;&#26102;&#12290;&#25105;&#20204;&#22768;&#31216;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26159;&#22810;&#21151;&#33021;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#20026;&#27599;&#20010;&#20250;&#35758;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#20027;&#39064;&#21010;&#20998;&#12289;&#22836;&#26465;&#12289;&#20998;&#27573;&#25688;&#35201;&#12289;&#25972;&#20010;&#20250;&#35758;&#25688;&#35201;&#21644;&#26174;&#35201;&#21477;&#23376;&#31561;&#27880;&#37322;&#12290;&#22240;&#27492;&#65292;&#35813;&#25968;&#25454;&#38598;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#25688;&#35201;&#20219;&#21153;&#25110;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#20110;&#20998;&#21106;&#30340;&#25688;&#35201;&#12289;&#22810;&#31890;&#24230;&#25688;&#35201;&#21644;&#26816;&#32034;-&#29983;&#25104;&#25688;&#35201;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#35777;&#23454;&#20102;VCSum&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20851;&#20110;&#19981;&#21516;&#19979;&#28216;&#25688;&#35201;&#20219;&#21153;&#30340;&#22522;&#20934;&#27169;&#22411;&#65292;&#20197;&#20415;&#36827;&#19968;&#27493;&#30740;&#31350;VCSum&#12290;&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#23558;&#22312; \url{https://github.com/hahahawu/VCSum} &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at \url{https://github.com/hahahawu/VCSum}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#23383;&#31526;&#34920;&#31034;&#32534;&#30721;&#32454;&#31890;&#24230;&#21457;&#38899;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22768;&#23398;&#20559;&#32622;&#65292;&#20197;&#25552;&#39640;&#38024;&#23545;&#22768;&#38899;&#30456;&#20284;&#24615;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#65292;&#22312;&#31070;&#32463;&#21464;&#25442;&#22120;&#31561;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24615;&#33021;&#20013;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.05271</link><description>&lt;p&gt;
&#31070;&#32463;&#21464;&#25442;&#22120;&#20013;&#30340;&#40065;&#26834;&#24615;&#35821;&#38899;&#21644;&#35821;&#20041;&#19978;&#19979;&#25991;&#20559;&#32622;
&lt;/p&gt;
&lt;p&gt;
Robust Acoustic and Semantic Contextual Biasing in Neural Transducers for Speech Recognition. (arXiv:2305.05271v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36731;&#37327;&#32423;&#23383;&#31526;&#34920;&#31034;&#32534;&#30721;&#32454;&#31890;&#24230;&#21457;&#38899;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22768;&#23398;&#20559;&#32622;&#65292;&#20197;&#25552;&#39640;&#38024;&#23545;&#22768;&#38899;&#30456;&#20284;&#24615;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#65292;&#22312;&#31070;&#32463;&#21464;&#25442;&#22120;&#31561;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#24615;&#33021;&#20013;&#37325;&#35201;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#26041;&#27861;&#24050;&#32463;&#22312;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;E2E ASR&#65289;&#31995;&#32479;&#20013;&#65292;&#22914;&#31070;&#32463;&#21464;&#25442;&#22120;&#20013;&#65292;&#23637;&#29616;&#20986;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22823;&#20247;&#30340;&#25110;&#20010;&#24615;&#21270;&#30340;&#32597;&#35265;&#35789;&#30340;&#35782;&#21035;&#26356;&#26159;&#22914;&#27492;&#12290;&#36825;&#20123;&#26041;&#27861;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#23558;&#27169;&#22411;&#20559;&#32622;&#20110;&#27880;&#20837;&#20026;&#20559;&#32622;&#30701;&#35821;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#23454;&#20307;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#23376;&#21333;&#35789;&#32534;&#30721;&#22120;&#26469;&#32534;&#30721;&#20559;&#32622;&#30701;&#35821;&#12290;&#28982;&#32780;&#65292;&#23376;&#21333;&#35789;&#26631;&#35760;&#31895;&#31961;&#65292;&#26080;&#27861;&#25429;&#25417;&#20851;&#38190;&#30340;&#21457;&#38899;&#20449;&#24687;&#65292;&#36825;&#23545;&#20110;&#22522;&#20110;&#22768;&#38899;&#30456;&#20284;&#24615;&#30340;&#20559;&#32622;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#36731;&#37327;&#32423;&#23383;&#31526;&#34920;&#31034;&#26469;&#32534;&#30721;&#32454;&#31890;&#24230;&#30340;&#21457;&#38899;&#29305;&#24449;&#65292;&#20197;&#25913;&#21892;&#21463;&#22768;&#38899;&#30456;&#20284;&#24615;&#24341;&#23548;&#30340;&#19978;&#19979;&#25991;&#20559;&#32622;&#65288;&#31216;&#20026;&#22768;&#23398;&#20559;&#32622;&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25972;&#21512;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;(NLM)&#30340;&#32534;&#30721;&#22120;&#65292;&#23558;&#35805;&#35821;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#19982;&#19978;&#19979;&#25991;&#23454;&#20307;&#19968;&#36215;&#32534;&#30721;&#20197;&#25552;&#39640;&#19979;&#25991;&#20559;&#32622;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based contextual biasing approaches have shown significant improvements in the recognition of generic and/or personal rare-words in End-to-End Automatic Speech Recognition (E2E ASR) systems like neural transducers. These approaches employ cross-attention to bias the model towards specific contextual entities injected as bias-phrases to the model. Prior approaches typically relied on subword encoders for encoding the bias phrases. However, subword tokenizations are coarse and fail to capture granular pronunciation information which is crucial for biasing based on acoustic similarity. In this work, we propose to use lightweight character representations to encode fine-grained pronunciation features to improve contextual biasing guided by acoustic similarity between the audio and the contextual entities (termed acoustic biasing). We further integrate pretrained neural language model (NLM) based encoders to encode the utterance's semantic context along with contextual entities to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#36793;&#30028;&#24178;&#25200;&#30340;&#26032;&#22411;&#21333;&#35789;&#20462;&#25913;NER&#25915;&#20987;&#8212;&#8212;&#34394;&#25311;&#36793;&#30028;&#25915;&#20987;(ViBA)&#65292;&#35813;&#25915;&#20987;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25915;&#20987;&#26368;&#20808;&#36827;&#30340;NER&#27169;&#22411;&#26102;&#26174;&#31034;&#20986;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05253</link><description>&lt;p&gt;
&#23454;&#20307;&#36793;&#30028;&#24178;&#25200;: &#19968;&#31181;&#25915;&#20987;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Attack Named Entity Recognition by Entity Boundary Interference. (arXiv:2305.05253v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05253
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23454;&#20307;&#36793;&#30028;&#24178;&#25200;&#30340;&#26032;&#22411;&#21333;&#35789;&#20462;&#25913;NER&#25915;&#20987;&#8212;&#8212;&#34394;&#25311;&#36793;&#30028;&#25915;&#20987;(ViBA)&#65292;&#35813;&#25915;&#20987;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25915;&#20987;&#26368;&#20808;&#36827;&#30340;NER&#27169;&#22411;&#26102;&#26174;&#31034;&#20986;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;(NER)&#26159;NLP&#20219;&#21153;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#31283;&#20581;&#24615;&#40092;&#21463;&#20851;&#27880;&#12290;&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#20174;&#21477;&#23376;&#20998;&#31867;&#27966;&#29983;&#30340;NER&#25915;&#20987;&#21407;&#21017;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36731;&#26131;&#36829;&#21453;&#21407;&#22987;&#21644;&#23545;&#25239;&#24615;NER&#31034;&#20363;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#36825;&#26159;&#30001;&#20110;NER&#30340;&#31934;&#32454;&#21270;&#24615;&#36136;&#65292;&#21363;&#21477;&#23376;&#20013;&#21363;&#20351;&#26159;&#24494;&#23567;&#30340;&#21333;&#35789;&#21464;&#21270;&#20063;&#20250;&#23548;&#33268;&#20219;&#20309;&#23454;&#20307;&#30340;&#20986;&#29616;&#25110;&#21464;&#24322;&#65292;&#20174;&#32780;&#23548;&#33268;&#26080;&#25928;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;NER&#23454;&#20307;&#36793;&#30028;&#20301;&#32622;&#30340;&#20851;&#38190;&#27934;&#23519;&#30340;&#26032;&#22411;&#21333;&#35789;&#20462;&#25913;NER&#25915;&#20987;&#12290;&#25105;&#20204;&#25925;&#24847;&#25554;&#20837;&#26032;&#30340;&#36793;&#30028;&#21040;&#21477;&#23376;&#20013;&#65292;&#35302;&#21457;&#23454;&#20307;&#36793;&#30028;&#24178;&#25200;&#65292;&#22312;&#36825;&#20010;&#36793;&#30028;&#35789;&#25110;&#21477;&#23376;&#20013;&#30340;&#20854;&#20182;&#21333;&#35789;&#19978;&#65292;&#20351;&#21463;&#23475;&#27169;&#22411;&#20570;&#20986;&#38169;&#35823;&#39044;&#27979;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#25915;&#20987;&#20026;&#34394;&#25311;&#36793;&#30028;&#25915;&#20987;(ViBA)&#65292;&#22312;&#22235;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#25915;&#20987;&#26368;&#20808;&#36827;&#30340;NER&#27169;&#22411;&#26102;&#26174;&#31034;&#20986;&#20102;&#38750;&#24120;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named Entity Recognition (NER) is a cornerstone NLP task while its robustness has been given little attention. This paper rethinks the principles of NER attacks derived from sentence classification, as they can easily violate the label consistency between the original and adversarial NER examples. This is due to the fine-grained nature of NER, as even minor word changes in the sentence can result in the emergence or mutation of any entities, resulting in invalid adversarial examples. To this end, we propose a novel one-word modification NER attack based on a key insight, NER models are always vulnerable to the boundary position of an entity to make their decision. We thus strategically insert a new boundary into the sentence and trigger the Entity Boundary Interference that the victim model makes the wrong prediction either on this boundary word or on other words in the sentence. We call this attack Virtual Boundary Attack (ViBA), which is shown to be remarkably effective when attackin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#30693;&#35782;&#26377;&#25928;&#22320;&#33976;&#39311;&#21040;&#31649;&#36947;&#27169;&#22411;&#20013;&#24182;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05226</link><description>&lt;p&gt;
&#25991;&#26412;&#22270;&#20687;&#26426;&#22120;&#32763;&#35793;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Multi-Teacher Knowledge Distillation For Text Image Machine Translation. (arXiv:2305.05226v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#30693;&#35782;&#26377;&#25928;&#22320;&#33976;&#39311;&#21040;&#31649;&#36947;&#27169;&#22411;&#20013;&#24182;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#20687;&#26426;&#22120;&#32763;&#35793;&#65288;TIMT&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#23427;&#23558;&#22270;&#20687;&#20013;&#30340;&#28304;&#35821;&#35328;&#25991;&#26412;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#21477;&#23376;&#12290;TIMT&#30340;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20004;&#31181;&#31867;&#21035;&#65306;&#35782;&#21035;-&#28982;&#21518;-&#32763;&#35793;&#27969;&#31243;&#27169;&#22411;&#21644;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20174;&#31649;&#36947;&#27169;&#22411;&#21521;&#31471;&#21040;&#31471;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#25945;&#24072;&#30693;&#35782;&#33976;&#39311;&#65288;MTKD&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#33976;&#39311;&#21040;&#31649;&#36947;&#27169;&#22411;&#20013;&#24182;&#20256;&#36882;&#32473;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#21033;&#29992;&#19977;&#20010;&#25945;&#24072;&#26469;&#25552;&#39640;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#20013;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#20351;&#29992;&#35782;&#21035;&#25945;&#24072;&#32534;&#30721;&#22120;&#30340;&#30693;&#35782;&#33976;&#39311;&#25351;&#23548;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#39034;&#24207;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#21017;&#36890;&#36807;&#20174;&#32763;&#35793;&#39034;&#24207;&#21644;&#35299;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#20256;&#36882;&#30693;&#35782;&#26469;&#25913;&#21892;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text image machine translation (TIMT) has been widely used in various real-world applications, which translates source language texts in images into another target language sentence. Existing methods on TIMT are mainly divided into two categories: the recognition-then-translation pipeline model and the end-to-end model. However, how to transfer knowledge from the pipeline model into the end-to-end model remains an unsolved problem. In this paper, we propose a novel Multi-Teacher Knowledge Distillation (MTKD) method to effectively distillate knowledge into the end-to-end TIMT model from the pipeline model. Specifically, three teachers are utilized to improve the performance of the end-to-end TIMT model. The image encoder in the end-to-end TIMT model is optimized with the knowledge distillation guidance from the recognition teacher encoder, while the sequential encoder and decoder are improved by transferring knowledge from the translation sequential and decoder teacher models. Furthermo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#21033;&#29992;&#23494;&#20999;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#65292;&#27880;&#20837;&#22122;&#22768;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#27169;&#22411;&#26356;&#33021;&#25269;&#24481;&#35789;&#27719;&#24046;&#24322;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.05214</link><description>&lt;p&gt;
&#21033;&#29992;&#35789;&#27719;&#30456;&#20284;&#24615;&#23454;&#29616;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#38646;&#26679;&#26412;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Utilizing Lexical Similarity to Enable Zero-Shot Machine Translation for Extremely Low-resource Languages. (arXiv:2305.05214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#21033;&#29992;&#23494;&#20999;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#65292;&#27880;&#20837;&#22122;&#22768;&#20316;&#20026;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#27169;&#22411;&#26356;&#33021;&#25269;&#24481;&#35789;&#27719;&#24046;&#24322;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20174;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;LRL&#65289;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#37319;&#29992;&#20174;&#23494;&#20999;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#65288;HRL&#65289;&#36827;&#34892;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#23545;&#20110;&#35768;&#22810;&#36825;&#20123;&#35821;&#35328;&#65292;&#27809;&#26377;&#24179;&#34892;&#35821;&#26009;&#24211;&#21487;&#29992;&#65292;&#21363;&#20351;&#26159;&#21333;&#35821;&#26009;&#24211;&#20063;&#24456;&#26377;&#38480;&#65292;&#24182;&#19988;&#22312;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#34920;&#31034;&#20063;&#32570;&#22833;&#12290;&#36825;&#20123;&#22240;&#32032;&#38480;&#21046;&#20102;&#20174;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#22909;&#22788;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#26497;&#20302;&#36164;&#28304;&#35821;&#35328;&#19982;&#30456;&#20851;&#30340;&#39640;&#36164;&#28304;&#35821;&#35328;&#20855;&#26377;&#24456;&#39640;&#30340;&#35789;&#27719;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20010;&#23646;&#24615;&#65292;&#23558;&#23383;&#31526;&#21644;&#23383;&#31526;&#36328;&#24230;&#30340;&#22122;&#22768;&#27880;&#20837;&#21040;HRL&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#28982;&#21518;&#20877;&#23398;&#20064;&#35789;&#27719;&#34920;&#12290;&#36825;&#20316;&#20026;&#19968;&#20010;&#27491;&#21017;&#21270;&#22120;&#65292;&#20351;&#27169;&#22411;&#26356;&#33021;&#25269;&#24481;HRL&#21644;LRL&#20043;&#38388;&#30340;&#35789;&#27719;&#24046;&#24322;&#65292;&#24182;&#26356;&#22909;&#22320;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#22312;&#26469;&#33258;&#22810;&#20010;&#35821;&#35328;&#23478;&#26063;&#30340;&#23494;&#20999;&#30456;&#20851;&#30340;HRL&#21644;LRL&#23545;&#19978;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#22522;&#32447;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address the task of machine translation from an extremely low-resource language (LRL) to English using cross-lingual transfer from a closely related high-resource language (HRL). For many of these languages, no parallel corpora are available, even monolingual corpora are limited and representations in pre-trained sequence-to-sequence models are absent. These factors limit the benefits of cross-lingual transfer from shared embedding spaces in multilingual models. However, many extremely LRLs have a high level of lexical similarity with related HRLs. We utilize this property by injecting character and character-span noise into the training data of the HRL prior to learning the vocabulary. This serves as a regularizer which makes the model more robust to lexical divergences between the HRL and LRL and better facilitates cross-lingual transfer. On closely related HRL and LRL pairs from multiple language families, we observe that our method significantly outperforms the baseline MT as we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#20102;&#26085;&#35821;&#20013;&#26410;&#26631;&#27880;&#25968;&#25454;&#30456;&#23545;&#20110;&#36328;&#35821;&#35328;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26085;&#35821;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05201</link><description>&lt;p&gt;
&#25506;&#31350;&#35821;&#35328;&#20381;&#36182;&#24615;&#22312;&#26085;&#35821;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploration of Language Dependency for Japanese Self-Supervised Speech Representation Models. (arXiv:2305.05201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#22312;&#26085;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#20102;&#26085;&#35821;&#20013;&#26410;&#26631;&#27880;&#25968;&#25454;&#30456;&#23545;&#20110;&#36328;&#35821;&#35328;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26085;&#35821;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#21333;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#29615;&#22659;&#20013;&#37117;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20004;&#31181;&#24773;&#20917;&#36890;&#24120;&#26159;&#20998;&#24320;&#30740;&#31350;&#30340;&#65292;&#23545;&#20110;&#36328;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#25928;&#26524;&#30340;&#30740;&#31350;&#21364;&#24456;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26085;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20219;&#21153;&#30340;&#23454;&#35777;&#30740;&#31350;&#26469;&#25506;&#35752;&#36825;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#22312;&#23613;&#21487;&#33021;&#20445;&#25345;&#22768;&#23398;&#22495;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#36739;&#36328;&#35821;&#35328;&#21644;&#21333;&#35821;&#35328;&#27169;&#22411;&#22312;&#20004;&#31181;&#19981;&#21516;&#35821;&#35328;&#20219;&#21153;&#30340;ASR&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#26085;&#35821;&#20013;&#25910;&#38598;&#21040;&#30340;&#22810;&#23569;&#26410;&#26631;&#27880;&#25968;&#25454;&#21487;&#20197;&#36798;&#21040;&#19982;&#39044;&#20808;&#36890;&#36807;&#25968;&#19975;&#23567;&#26102;&#30340;&#33521;&#35821;&#21644;/&#25110;&#22810;&#35821;&#35328;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36328;&#35821;&#35328;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24191;&#27867;&#30740;&#31350;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#26085;&#35821;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;ASR&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has been dramatically successful not only in monolingual but also in cross-lingual settings. However, since the two settings have been studied individually in general, there has been little research focusing on how effective a cross-lingual model is in comparison with a monolingual model. In this paper, we investigate this fundamental question empirically with Japanese automatic speech recognition (ASR) tasks. First, we begin by comparing the ASR performance of cross-lingual and monolingual models for two different language tasks while keeping the acoustic domain as identical as possible. Then, we examine how much unlabeled data collected in Japanese is needed to achieve performance comparable to a cross-lingual model pre-trained with tens of thousands of hours of English and/or multilingual data. Finally, we extensively investigate the effectiveness of SSL in Japanese and demonstrate state-of-the-art performance on multiple ASR tasks. Since there is no c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22240;&#26524;&#25512;&#26029;&#35282;&#24230;&#20986;&#21457;&#30340;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;-shot&#26694;&#26550;COLA&#26469;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.05191</link><description>&lt;p&gt;
COLA: &#22240;&#26524;&#25512;&#26029;&#35282;&#24230;&#19979;&#30340;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective. (arXiv:2305.05191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#22240;&#26524;&#25512;&#26029;&#35282;&#24230;&#20986;&#21457;&#30340;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;-shot&#26694;&#26550;COLA&#26469;&#35299;&#20915;&#27492;&#20219;&#21153;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#20107;&#20214;&#20043;&#38388;&#30340;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#65288;&#22240;&#26524;&#20851;&#31995;&#65289;&#38271;&#26399;&#20197;&#26469;&#26159;&#19968;&#39033;&#22522;&#26412;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#37492;&#20110;&#20107;&#20214;&#30340;&#22797;&#26434;&#24615;&#65292;&#19968;&#20010;&#20107;&#20214;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#33021;&#26377;&#19981;&#21516;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#22312;&#26816;&#27979;&#22240;&#26524;&#20851;&#31995;&#20013;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#20808;&#21069;&#20851;&#20110;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#30340;&#24037;&#20316;&#21482;&#32771;&#34385;&#20004;&#20010;&#20107;&#20214;&#24182;&#24573;&#30053;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#65292;&#31616;&#21270;&#20102;&#20219;&#21153;&#30340;&#21046;&#23450;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#26816;&#27979;&#20107;&#20214;&#24207;&#21015;&#65288;&#21363;&#19978;&#19979;&#25991;&#65289;&#20013;&#20004;&#20010;&#20107;&#20214;&#20043;&#38388;&#30340;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#65292;&#31216;&#20026;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#12290;&#25105;&#20204;&#36824;&#20174;&#22240;&#26524;&#25512;&#26029;&#30340;&#35282;&#24230;&#35774;&#35745;&#20102;&#19968;&#20010;&#38646;-shot&#26694;&#26550;&#65306;COLA&#65288;&#24773;&#22659;&#21270;&#24120;&#35782;&#22240;&#26524;&#25512;&#29702;&#22120;&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#12290;&#36825;&#20010;&#26694;&#26550;&#20174;&#26102;&#38388;&#24615;&#33719;&#24471;&#20102;&#20016;&#23500;&#30340;&#20598;&#21457;&#30417;&#30563;&#65292;&#24182;&#24179;&#34913;&#20102;&#26469;&#33258;&#22810;&#20010;&#26102;&#38388;&#25139;&#30340;&#21327;&#21464;&#37327;&#20197;&#28040;&#38500;&#28151;&#28102;&#25928;&#24212;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;COLA&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#26816;&#27979;&#24120;&#35782;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.05189</link><description>&lt;p&gt;
SUR-adapter&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#25991;&#26412;-&#22270;&#20687;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models. (arXiv:2305.05189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SUR-adapter&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#22686;&#24378;&#39044;&#20808;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#29983;&#25104;&#22270;&#29255;&#26102;&#20351;&#29992;&#31616;&#30701;&#30340;&#21465;&#36848;&#25552;&#31034;&#12290;&#20316;&#32773;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#20855;&#26377;&#39640;&#36136;&#37327;&#21644;&#20869;&#23481;&#20016;&#23500;&#24230;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#24403;&#36755;&#20837;&#30340;&#25552;&#31034;&#20026;&#31616;&#30701;&#30340;&#21465;&#36848;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#35821;&#20041;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#30340;&#36136;&#37327;&#36739;&#20302;&#12290;&#20026;&#20102;&#25552;&#39640;&#21465;&#36848;&#25552;&#31034;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;Semantic Understanding&#21644;Reasoning adapter&#65288;SUR-adapter&#65289;&#65292;&#29992;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#21644;&#27880;&#37322;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SURD&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;57,000&#20010;&#35821;&#20041;&#20462;&#27491;&#30340;&#22810;&#27169;&#24577;&#26679;&#26412;&#12290;&#27599;&#20010;&#26679;&#26412;&#37117;&#21253;&#21547;&#19968;&#20010;&#31616;&#21333;&#30340;&#21465;&#36848;&#25552;&#31034;&#65292;&#19968;&#20010;&#22797;&#26434;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#21644;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#21465;&#36848;&#25552;&#31034;&#30340;&#35821;&#20041;&#34920;&#31034;&#19982;&#22797;&#26434;&#25552;&#31034;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#23558;&#20854;&#36716;&#31227;&#33267;&#25105;&#20204;&#30340;SUR-adapter&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models, which have emerged to become popular text-to-image generation models, can produce high-quality and content-rich images guided by textual prompts. However, there are limitations to semantic understanding and commonsense reasoning in existing models when the input prompts are concise narrative, resulting in low-quality image generation. To improve the capacities for narrative prompts, we propose a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models. To reach this goal, we first collect and annotate a new dataset SURD which consists of more than 57,000 semantically corrected multi-modal samples. Each sample contains a simple narrative prompt, a complex keyword-based prompt, and a high-quality image. Then, we align the semantic representation of narrative prompts to the complex prompts and transfer knowledge of large language models (LLMs) to our SUR-adapter vi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#35821;&#26009;&#24211;CSED&#65292;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21477;&#27861;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;CSED-R&#21644;CSED-C&#20219;&#21153;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.05183</link><description>&lt;p&gt;
CSED: &#19968;&#20010;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
CSED: A Chinese Semantic Error Diagnosis Corpus. (arXiv:2305.05183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#35821;&#26009;&#24211;CSED&#65292;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#21477;&#27861;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;CSED-R&#21644;CSED-C&#20219;&#21153;&#30340;&#26368;&#20339;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20013;&#25991;&#25991;&#26412;&#38169;&#35823;&#32416;&#27491;&#30340;&#24037;&#20316;&#22823;&#22810;&#38598;&#20013;&#22312;&#20013;&#25991;&#25340;&#20889;&#26816;&#26597;&#65288;CSC&#65289;&#21644;&#20013;&#25991;&#35821;&#27861;&#38169;&#35823;&#35786;&#26029;&#65288;CGED&#65289;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23545;&#20110;&#32570;&#20047;&#30456;&#20851;&#25968;&#25454;&#38598;&#30340;&#20013;&#25991;&#35821;&#20041;&#38169;&#35823;&#35786;&#26029;&#65288;CSED&#65289;&#36825;&#19968;&#22797;&#26434;&#38382;&#39064;&#65292;&#21364;&#27809;&#26377;&#24471;&#21040;&#36275;&#22815;&#30340;&#20851;&#27880;&#12290;&#30740;&#31350;&#35821;&#20041;&#38169;&#35823;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#24120;&#35265;&#65292;&#24182;&#19988;&#21487;&#33021;&#23548;&#33268;&#21477;&#27861;&#19981;&#35268;&#21017;&#29978;&#33267;&#29702;&#35299;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;CSED&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#25968;&#25454;&#38598;&#12290;&#19968;&#20010;&#26159;&#29992;&#20110;CSED&#35782;&#21035;&#65288;CSED-R&#65289;&#20219;&#21153;&#65292;&#21478;&#19968;&#20010;&#26159;&#29992;&#20110;CSED&#26356;&#27491;&#65288;CSED-C&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27880;&#37322;&#36890;&#36807;&#36136;&#37327;&#20445;&#35777;&#26426;&#21046;&#20445;&#35777;&#20102;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36825;&#20010;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;CSED&#20219;&#21153;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#21363;&#20351;&#26159;&#20154;&#31867;&#24471;&#20998;&#20063;&#24456;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21477;&#27861;&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#38024;&#23545;CSED&#20219;&#21153;&#36827;&#34892;&#36866;&#24212;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;CSED-R&#21644;CSED-C&#20219;&#21153;&#19978;&#22343;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#34920;&#29616;&#65292;&#36229;&#36807;&#20102;&#20197;&#21069;&#30340;&#26368;&#20339;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, much Chinese text error correction work has focused on Chinese Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In contrast, little attention has been paid to the complicated problem of Chinese Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of semantic errors is important because they are very common and may lead to syntactic irregularities or even problems of comprehension. To investigate this, we build the CSED corpus, which includes two datasets. The one is for the CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C) task. Our annotation guarantees high-quality data through quality assurance mechanisms. Our experiments show that powerful pre-trained models perform poorly on this corpus. We also find that the CSED task is challenging, as evidenced by the fact that even humans receive a low score. This paper proposes syntax-aware models to specifically adapt to the CSED task. The experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05181</link><description>&lt;p&gt;
MoT&#65306;&#39044;&#24605;&#32771;&#21644;&#22238;&#24518;&#21151;&#33021;&#20351; ChatGPT &#22312;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#20013;&#33258;&#25105;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
MoT: Pre-thinking and Recalling Enable ChatGPT to Self-Improve with Memory-of-Thoughts. (arXiv:2305.05181v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; MoT &#30340;&#26694;&#26550;&#65292;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#8220;&#24605;&#24819;&#35760;&#24518;&#8221;&#33258;&#25105;&#36827;&#21270;&#65292;&#26080;&#38656;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#33021;&#21147;&#12290;&#20294;&#35201;&#23454;&#29616;&#23427;&#20204;&#30340;&#26681;&#26412;&#24615;&#25913;&#36827;&#65292;&#38656;&#35201;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#12290;&#30456;&#21453;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#24605;&#32771;&#21644;&#35760;&#24518;&#36731;&#26494;&#25552;&#39640;&#33258;&#25105;&#27700;&#24179;&#65292;&#32780;&#19981;&#38656;&#35201;&#22806;&#37096;&#36164;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550; MoT&#65292;&#22312;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#21644;&#21442;&#25968;&#26356;&#26032;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24605;&#24819;&#35760;&#24518;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#25105;&#36827;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;1. &#22312;&#27979;&#35797;&#38454;&#27573;&#20043;&#21069;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26410;&#21152;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#24605;&#32771;&#65292;&#24182;&#23558;&#39640;&#32622;&#20449;&#24230;&#30340;&#24819;&#27861;&#20445;&#23384;&#20026;&#22806;&#37096;&#35760;&#24518;&#12290;2. &#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#27979;&#35797;&#38382;&#39064;&#65292;&#25105;&#20204;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22238;&#24518;&#30456;&#20851;&#30340;&#35760;&#24518;&#65292;&#24110;&#21161;&#33258;&#24049;&#36827;&#34892;&#25512;&#29702;&#21644;&#22238;&#31572;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161; ChatGPT &#22312;&#25968;&#23398;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#20107;&#23454;&#25512;&#29702;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20854;&#33021;&#21147;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27599;&#20010;&#32452;&#20214;&#37117;&#21457;&#25381;&#20102;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have shown impressive abilities on various tasks. However, fundamentally improving them depends on high-quality datasets or computationally expensive fine-tuning. On the contrary, human can easily improve themselves by thinking and memory, without external resources. In this paper, we propose a framework, MoT, to let the LLM self-improve through Memory of Thoughts, without annotated datasets and parameter updates. Specifically, the framework is divided into two stages: 1. before the test stage, we let the LLM pre-think on the unlabeled dataset and save the high-confidence thoughts as external memory; 2. during inference, given a test question, we let the LLM recall relevant memory to help itself reason and answer it. Experimental results show that the proposed framework can help ChatGPT significantly improve its abilities in math reasoning, commonsense reasoning, factual reasoning and natural language inference. Further analyses show that each component contribute
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#25552;&#31034;&#36866;&#24212;&#65292;LLM&#36817;&#20284;&#21644;LLM&#32423;&#32852;&#65292;&#24182;&#19988;&#36890;&#36807; FrugalGPT &#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#25110;&#26159;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05176</link><description>&lt;p&gt;
FrugalGPT: &#22914;&#20309;&#22312;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance. (arXiv:2305.05176v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21516;&#26102;&#38477;&#20302;&#25104;&#26412;&#21644;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#21253;&#25324;&#25552;&#31034;&#36866;&#24212;&#65292;LLM&#36817;&#20284;&#21644;LLM&#32423;&#32852;&#65292;&#24182;&#19988;&#36890;&#36807; FrugalGPT &#36825;&#31181;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22823;&#22823;&#38477;&#20302;&#25104;&#26412;&#25110;&#26159;&#25552;&#39640;&#20934;&#30830;&#29575;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#36234;&#26469;&#36234;&#22810;&#30340;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#20184;&#36153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#26597;&#35810;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#26597;&#35810;&#27969;&#34892;&#30340;LLM API&#65288;&#20363;&#22914;GPT-4&#65292;ChatGPT&#65292;J1-Jumbo&#65289;&#28041;&#21450;&#30340;&#25104;&#26412;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24322;&#26500;&#30340;&#20215;&#26684;&#32467;&#26500;&#65292;&#36153;&#29992;&#21487;&#33021;&#30456;&#24046;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#29305;&#21035;&#26159;&#22312;&#22823;&#37327;&#26597;&#35810;&#21644;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;LLM&#21487;&#33021;&#20250;&#24456;&#26114;&#36149;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#19977;&#31181;&#31574;&#30053;&#65292;&#29992;&#25143;&#21487;&#20197;&#21033;&#29992;&#36825;&#20123;&#31574;&#30053;&#26469;&#20943;&#23569;&#20351;&#29992;LLM&#30340;&#27719;&#32534;&#25104;&#26412;&#65306;1&#65289;&#25552;&#31034;&#36866;&#24212;&#65292;2&#65289;LLM&#36817;&#20284;&#21644;3&#65289;LLM&#32423;&#32852;&#12290;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FrugalGPT&#65292;&#23427;&#26159;LLM&#32423;&#32852;&#30340;&#19968;&#20010;&#31616;&#21333;&#32780;&#28789;&#27963;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#23398;&#20064;&#20351;&#29992;&#21738;&#20123;LLM&#32452;&#21512;&#26469;&#22788;&#29702;&#19981;&#21516;&#26597;&#35810;&#65292;&#20197;&#38477;&#20302;&#25104;&#26412;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FrugalGPT&#21487;&#20197;&#22312;&#20165;&#20351;&#29992;&#36153;&#29992;&#30340;98&#65285;&#25110;&#19982;GPT-4&#30456;&#21516;&#30340;&#25104;&#26412;&#19979;&#65292;&#36798;&#21040;&#26368;&#20339;&#21333;&#20010;LLM&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;GPT-4&#65289;&#65292;&#25110;&#32773;&#20197;4&#65285;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;GPT-4&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#25511;&#21046;&#38271;&#24230;&#30340;&#25688;&#35201;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#38271;&#24230;&#24182;&#29983;&#25104;&#38271;&#24230;&#26368;&#20339;&#30340;&#25688;&#35201;&#65292;&#22312;&#20445;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05171</link><description>&lt;p&gt;
&#31934;&#30830;&#25511;&#21046;&#38271;&#24230;&#30340;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Summarization with Precise Length Control. (arXiv:2305.05171v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#30830;&#25511;&#21046;&#38271;&#24230;&#30340;&#25688;&#35201;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#39044;&#27979;&#38271;&#24230;&#24182;&#29983;&#25104;&#38271;&#24230;&#26368;&#20339;&#30340;&#25688;&#35201;&#65292;&#22312;&#20445;&#25345;&#25991;&#26412;&#36136;&#37327;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#26412;&#29983;&#25104;&#24212;&#29992;&#65292;&#22914;&#25688;&#35201;&#65292;&#38656;&#35201;&#31934;&#30830;&#25511;&#21046;&#25991;&#26412;&#38271;&#24230;&#12290;&#29616;&#26377;&#30340;&#25511;&#21046;&#38271;&#24230;&#30340;&#25688;&#35201;&#26041;&#27861;&#35201;&#20040;&#34920;&#29616;&#19979;&#38477;&#65292;&#35201;&#20040;&#21482;&#33021;&#36817;&#20284;&#25511;&#21046;&#38271;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#25351;&#23450;&#26631;&#35760;&#25110;&#21477;&#23376;&#25968;&#37327;&#30340;&#25688;&#35201;&#65292;&#21516;&#26102;&#20445;&#25345;&#25110;&#29978;&#33267;&#25552;&#39640;&#25991;&#26412;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32852;&#21512;&#35757;&#32451;&#27169;&#22411;&#26469;&#39044;&#27979;&#38271;&#24230;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#38271;&#24230;&#26368;&#20339;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#22312;CNNDM&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many applications of text generation such as summarization benefit from accurately controlling the text length. Existing approaches on length-controlled summarization either result in degraded performance or can only control the length approximately. In this work, we present a framework to generate summaries with precisely the specified number of tokens or sentences, while maintaining or even improving the text quality. In addition, we jointly train the models to predict the lengths, so our model can generate summaries with optimal length. We evaluate the proposed framework on the CNNDM dataset and show improved performance compared to existing methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#22270;&#26426;&#22120;&#32763;&#35793;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#21033;&#29992;&#29616;&#26377;OCR&#21644;MT&#25968;&#25454;&#24211;&#21644;&#26032;&#22411;&#27169;&#24577;&#36866;&#37197;&#22120;&#23558;OCR&#32534;&#30721;&#22120;&#21644;MT&#35299;&#30721;&#22120;&#36830;&#25509;&#65292;&#20351;&#24471;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#32423;&#32852;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#32423;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.05166</link><description>&lt;p&gt;
E2TIMT: &#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#22270;&#26426;&#22120;&#32763;&#35793;&#27169;&#24577;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
E2TIMT: Efficient and Effective Modal Adapter for Text Image Machine Translation. (arXiv:2305.05166v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#25928;&#30340;&#25991;&#22270;&#26426;&#22120;&#32763;&#35793;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#21033;&#29992;&#29616;&#26377;OCR&#21644;MT&#25968;&#25454;&#24211;&#21644;&#26032;&#22411;&#27169;&#24577;&#36866;&#37197;&#22120;&#23558;OCR&#32534;&#30721;&#22120;&#21644;MT&#35299;&#30721;&#22120;&#36830;&#25509;&#65292;&#20351;&#24471;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#32423;&#32852;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#32423;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#22270;&#20687;&#26426;&#22120;&#32763;&#35793;&#26088;&#22312;&#23558;&#23884;&#20837;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#20174;&#19968;&#31181;&#28304;&#35821;&#35328;&#32763;&#35793;&#25104;&#21478;&#19968;&#31181;&#30446;&#26631;&#35821;&#35328;&#12290;&#29616;&#26377;&#26041;&#27861;&#65292;&#26080;&#35770;&#26159;&#20004;&#38454;&#27573;&#32423;&#32852;&#36824;&#26159;&#19968;&#32423;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#37117;&#23384;&#22312;&#19981;&#21516;&#30340;&#38382;&#39064;&#12290;&#32423;&#32852;&#27169;&#22411;&#21487;&#20197;&#21463;&#30410;&#20110;&#22823;&#35268;&#27169;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#65288;OCR&#65289;&#21644; MT &#25968;&#25454;&#38598;&#65292;&#20294;&#20004;&#38454;&#27573;&#26550;&#26500;&#21017;&#26159;&#20887;&#20313;&#30340;&#12290;&#31471;&#21040;&#31471;&#27169;&#22411;&#26159;&#39640;&#25928;&#30340;&#65292;&#20294;&#36973;&#21463;&#35757;&#32451;&#25968;&#25454;&#19981;&#36275;&#30340;&#22256;&#25200;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;TIMT&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992;&#29616;&#26377;OCR&#21644;MT&#25968;&#25454;&#38598;&#20013;&#30340;&#30693;&#35782;&#65292;&#36861;&#27714;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#26694;&#26550;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#22411;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#26377;&#25928;&#22320;&#36830;&#25509;OCR&#32534;&#30721;&#22120;&#21644;MT&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#20351;&#29992;&#31471;&#21040;&#31471;TIMT&#25439;&#22833;&#21644;&#36328;&#27169;&#24577;&#23545;&#27604;&#25439;&#22833;&#26469;&#23545;&#40784;OCR&#21644;MT&#20219;&#21153;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#32763;&#35793;&#36136;&#37327;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20004;&#38454;&#27573;&#32423;&#32852;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#19968;&#32423;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text image machine translation (TIMT) aims to translate texts embedded in images from one source language to another target language. Existing methods, both two-stage cascade and one-stage end-to-end architectures, suffer from different issues. The cascade models can benefit from the large-scale optical character recognition (OCR) and MT datasets but the two-stage architecture is redundant. The end-to-end models are efficient but suffer from training data deficiency. To this end, in our paper, we propose an end-to-end TIMT model fully making use of the knowledge from existing OCR and MT datasets to pursue both an effective and efficient framework. More specifically, we build a novel modal adapter effectively bridging the OCR encoder and MT decoder. End-to-end TIMT loss and cross-modal contrastive loss are utilized jointly to align the feature distribution of the OCR and MT tasks. Extensive experiments show that the proposed method outperforms the existing two-stage cascade models and o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#21307;&#30103;&#20195;&#30721;&#65292;&#24182;&#22312;&#26631;&#31614;&#31354;&#38388;&#19982;&#20020;&#24202;&#25991;&#26412;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#25216;&#26415;&#27700;&#24179;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.05162</link><description>&lt;p&gt;
&#36890;&#36807;&#26631;&#31614;&#20869;&#37096;&#23545;&#40784;&#23454;&#29616;&#26377;&#25928;&#30340;&#21307;&#30103;&#20195;&#30721;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Medical Code Prediction via Label Internal Alignment. (arXiv:2305.05162v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#21307;&#30103;&#20195;&#30721;&#65292;&#24182;&#22312;&#26631;&#31614;&#31354;&#38388;&#19982;&#20020;&#24202;&#25991;&#26412;&#20043;&#38388;&#36827;&#34892;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#25216;&#26415;&#27700;&#24179;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35760;&#24405;&#36890;&#24120;&#26159;&#30001;&#21307;&#29983;&#36755;&#20837;&#31995;&#32479;&#30340;&#12290;&#36825;&#20123;&#35760;&#24405;&#38656;&#35201;&#26631;&#35760;&#26631;&#20934;&#30340;&#21307;&#30103;&#20195;&#30721;&#65292;&#32780;&#27599;&#20010;&#20195;&#30721;&#20195;&#34920;&#19968;&#31181;&#35786;&#26029;&#25110;&#21307;&#30103;&#27835;&#30103;&#31243;&#24207;&#12290;&#23545;&#36825;&#20123;&#35760;&#24405;&#36827;&#34892;&#27880;&#37322;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#35270;&#35282;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#20020;&#24202;&#25991;&#26412;&#20013;&#39044;&#27979;&#21307;&#30103;&#20195;&#30721;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#19977;&#20010;&#20449;&#24687;&#26041;&#38754;&#65306;&#20020;&#24202;&#25991;&#26412;&#30340;&#35821;&#20041;&#19978;&#19979;&#25991;&#65292;&#26631;&#31614;&#65288;&#21307;&#30103;&#20195;&#30721;&#65289;&#31354;&#38388;&#20043;&#38388;&#30340;&#20851;&#31995;&#20197;&#21450;&#27599;&#20010;&#20020;&#24202;&#25991;&#26412;&#21644;&#21307;&#30103;&#20195;&#30721;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#23545;&#20808;&#21069;&#30340;&#25216;&#26415;&#27700;&#24179;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical notes are usually typed into the system by physicians. They are typically required to be marked by standard medical codes, and each code represents a diagnosis or medical treatment procedure. Annotating these notes is time consuming and prone to error. In this paper, we proposed a multi-view attention based Neural network to predict medical codes from clinical texts. Our method incorporates three aspects of information, the semantic context of the clinical text, the relationship among the label (medical codes) space, and the alignment between each pair of a clinical text and medical code. Our method is verified to be effective on the open source dataset. The experimental result shows that our method achieves better performance against the prior state-of-art on multiple metrics.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#25552;&#20379;&#20102;&#35786;&#26029;&#35777;&#25454;&#21644;&#20010;&#24615;&#21270;&#24314;&#35758;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05138</link><description>&lt;p&gt;
&#35835;&#21462;&#12289;&#35786;&#26029;&#21644;&#32842;&#22825;&#65306;&#38754;&#21521;&#31038;&#20132;&#23186;&#20307;&#20013;&#21487;&#35299;&#37322;&#21644;&#20132;&#20114;&#30340;LLMs&#22686;&#24378;&#25233;&#37057;&#30151;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Read, Diagnose and Chat: Towards Explainable and Interactive LLMs-Augmented Depression Detection in Social Media. (arXiv:2305.05138v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#25552;&#20379;&#20102;&#35786;&#26029;&#35777;&#25454;&#21644;&#20010;&#24615;&#21270;&#24314;&#35758;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#26032;&#22411;&#25233;&#37057;&#30151;&#26816;&#27979;&#31995;&#32479;&#65292;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21448;&#20855;&#26377;&#20114;&#21160;&#24615;&#12290;&#23427;&#19981;&#20165;&#25552;&#20379;&#35786;&#26029;&#32467;&#26524;&#65292;&#36824;&#26681;&#25454;&#19982;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#65292;&#25552;&#20379;&#35786;&#26029;&#35777;&#25454;&#21644;&#20010;&#24615;&#21270;&#24314;&#35758;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#22823;&#37327;&#25991;&#26412;&#22788;&#29702;&#21644;&#38598;&#25104;&#19987;&#19994;&#35786;&#26029;&#26631;&#20934;&#31561;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new depression detection system based on LLMs that is both interpretable and interactive. It not only provides a diagnosis, but also diagnostic evidence and personalized recommendations based on natural language dialogue with the user. We address challenges such as the processing of large amounts of text and integrate professional diagnostic criteria. Our system outperforms traditional methods across various settings and is demonstrated through case studies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20351;&#29992;ChatGPT&#29983;&#25104;&#24694;&#24847;&#25552;&#31034;&#21487;&#20197;&#29983;&#25104;&#21151;&#33021;&#24615;&#30340;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#26469;&#27169;&#20223;&#27969;&#34892;&#21697;&#29260;&#24182;&#27169;&#25311;&#22810;&#31181;&#35268;&#36991;&#31574;&#30053;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#20351;&#29992;&#26222;&#36890;&#30340;ChatGPT&#29983;&#25104;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#20808;&#21069;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65288;&#36234;&#29425;&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.05133</link><description>&lt;p&gt;
&#21033;&#29992;ChatGPT&#29983;&#25104;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Generating Phishing Attacks using ChatGPT. (arXiv:2305.05133v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20351;&#29992;ChatGPT&#29983;&#25104;&#24694;&#24847;&#25552;&#31034;&#21487;&#20197;&#29983;&#25104;&#21151;&#33021;&#24615;&#30340;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#26469;&#27169;&#20223;&#27969;&#34892;&#21697;&#29260;&#24182;&#27169;&#25311;&#22810;&#31181;&#35268;&#36991;&#31574;&#30053;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#20351;&#29992;&#26222;&#36890;&#30340;ChatGPT&#29983;&#25104;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#20808;&#21069;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65288;&#36234;&#29425;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#20154;&#31867;&#22238;&#31572;&#29983;&#25104;&#21644;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#20102;&#23545;&#35805;&#20195;&#29702;&#12289;&#20869;&#23481;&#21019;&#20316;&#12289;&#25968;&#25454;&#20998;&#26512;&#20197;&#21450;&#30740;&#31350;&#21644;&#21019;&#26032;&#30340;&#28909;&#38376;&#24037;&#20855;&#12290;&#20294;&#20854;&#26377;&#25928;&#24615;&#21644;&#26131;&#29992;&#24615;&#20063;&#20351;&#20854;&#25104;&#20026;&#29983;&#25104;&#24694;&#24847;&#20869;&#23481;&#30340;&#20027;&#35201;&#24037;&#20855;&#65292;&#20363;&#22914;&#32593;&#32476;&#38035;&#40060;&#25915;&#20987;&#65292;&#36825;&#20250;&#20351;&#29992;&#25143;&#22788;&#20110;&#21361;&#38505;&#20043;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#21487;&#25552;&#20379;&#32473;ChatGPT&#30340;&#22810;&#31181;&#24694;&#24847;&#25552;&#31034;&#65292;&#20197;&#29983;&#25104;&#21151;&#33021;&#24615;&#30340;&#32593;&#32476;&#38035;&#40060;&#32593;&#31449;&#12290;&#36890;&#36807;&#36845;&#20195;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#38035;&#40060;&#32593;&#31449;&#21487;&#20197;&#27169;&#20223;&#27969;&#34892;&#21697;&#29260;&#24182;&#27169;&#25311;&#22810;&#31181;&#35268;&#36991;&#31574;&#30053;&#65292;&#36825;&#20123;&#31574;&#30053;&#24050;&#34987;&#30693;&#36947;&#21487;&#20197;&#36991;&#20813;&#21453;&#32593;&#32476;&#38035;&#40060;&#26426;&#26500;&#30340;&#26816;&#27979;&#12290;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#20351;&#29992;&#26222;&#36890;&#30340;ChatGPT&#29983;&#25104;&#65292;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#20808;&#21069;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65288;&#36234;&#29425;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of ChatGPT to generate human-like responses and understand context has made it a popular tool for conversational agents, content creation, data analysis, and research and innovation. However, its effectiveness and ease of accessibility makes it a prime target for generating malicious content, such as phishing attacks, that can put users at risk. In this work, we identify several malicious prompts that can be provided to ChatGPT to generate functional phishing websites. Through an iterative approach, we find that these phishing websites can be made to imitate popular brands and emulate several evasive tactics that have been known to avoid detection by anti-phishing entities. These attacks can be generated using vanilla ChatGPT without the need of any prior adversarial exploits (jailbreaking).
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#27169;&#22411;(NAP)&#65292;&#36890;&#36807;&#32534;&#30721;&#24207;&#21015;&#30452;&#25509;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23454;&#29616;&#35299;&#30721;&#27493;&#39588;&#30340;&#35268;&#36991;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20004;&#20010;&#22330;&#26223;&#19979;&#65292;NAP&#20998;&#21035;&#21487;&#20197;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#21644;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.05098</link><description>&lt;p&gt;
&#35841;&#38656;&#35201;&#35299;&#30721;&#22120;&#65311;&#39640;&#25928;&#39044;&#27979;&#24207;&#21015;&#32423;&#23646;&#24615;&#12290;&#65288;arXiv:2305.05098v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Who Needs Decoders? Efficient Estimation of Sequence-level Attributes. (arXiv:2305.05098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05098
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#27169;&#22411;(NAP)&#65292;&#36890;&#36807;&#32534;&#30721;&#24207;&#21015;&#30452;&#25509;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23454;&#29616;&#35299;&#30721;&#27493;&#39588;&#30340;&#35268;&#36991;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#38899;&#35782;&#21035;&#20004;&#20010;&#22330;&#26223;&#19979;&#65292;NAP&#20998;&#21035;&#21487;&#20197;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#21644;&#39640;&#31934;&#24230;&#22320;&#39044;&#27979;&#24615;&#33021;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#21270;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#33258;&#22238;&#24402;&#35299;&#30721;&#65292;&#36825;&#24448;&#24448;&#38750;&#24120;&#28040;&#32791;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#36234;&#30028;&#26816;&#27979;&#21644;&#36164;&#28304;&#20998;&#37197;&#65292;&#23454;&#38469;&#35299;&#30721;&#36755;&#20986;&#24182;&#19981;&#38656;&#35201;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#24207;&#21015;&#30340;&#26631;&#37327;&#23646;&#24615;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#19979;&#65292;&#30693;&#36947;&#31995;&#32479;&#36755;&#20986;&#36136;&#37327;&#20197;&#39044;&#27979;&#24615;&#33021;&#36739;&#24046;&#27604;&#30693;&#36947;&#36755;&#20986;&#26412;&#36523;&#26356;&#20026;&#37325;&#35201;&#65292;&#37027;&#20040;&#26159;&#21542;&#21487;&#20197;&#32469;&#36807;&#33258;&#22238;&#24402;&#35299;&#30721;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#38750;&#33258;&#22238;&#24402;&#20195;&#29702;&#65288;NAP&#65289;&#27169;&#22411;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#39044;&#27979;&#36890;&#29992;&#26631;&#37327;&#20540;&#24207;&#21015;&#32423;&#23646;&#24615;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;NAP&#30452;&#25509;&#20174;&#32534;&#30721;&#39044;&#27979;&#36825;&#20123;&#25351;&#26631;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#20010;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#65306;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#21644;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#12290;&#22312;MT&#30340;&#36234;&#30028;&#26816;&#27979;&#20013;&#65292;NAP&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#38598;&#25104;&#65292;&#21516;&#26102;&#36895;&#24230;&#26174;&#33879;&#26356;&#24555;&#12290;NAP&#20063;&#34987;&#35777;&#26126;&#33021;&#22815;&#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;ASR&#30340;&#24615;&#33021;&#25351;&#26631;&#65292;&#20363;&#22914;&#35789;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22312;&#23646;&#24615;&#21487;&#20197;&#20174;&#32534;&#30721;&#20013;&#30452;&#25509;&#39044;&#27979;&#30340;&#20219;&#21153;&#20013;&#65292;NAP&#20026;&#20256;&#32479;&#22522;&#20110;&#35299;&#30721;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#25991;&#26412;&#38598;&#21512;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#12289;&#34987;&#39046;&#22495;&#19987;&#23478;&#35270;&#20026;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#26082;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21448;&#20943;&#23569;&#20102;&#25163;&#21160;&#32534;&#30721;&#30340;&#24037;&#20316;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.05094</link><description>&lt;p&gt;
&#22823;&#22411;&#25991;&#26412;&#38598;&#21512;&#20013;&#30340;&#20132;&#20114;&#24335;&#27010;&#24565;&#23398;&#20064;&#29992;&#20110;&#25581;&#31034;&#28508;&#22312;&#20027;&#39064;
&lt;/p&gt;
&lt;p&gt;
Interactive Concept Learning for Uncovering Latent Themes in Large Text Collections. (arXiv:2305.05094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#25991;&#26412;&#38598;&#21512;&#20013;&#25581;&#31034;&#28508;&#22312;&#30340;&#12289;&#34987;&#39046;&#22495;&#19987;&#23478;&#35270;&#20026;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#26082;&#23454;&#29616;&#20102;&#33258;&#21160;&#21270;&#21448;&#20943;&#23569;&#20102;&#25163;&#21160;&#32534;&#30721;&#30340;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#36234;&#19981;&#21516;&#23398;&#31185;&#39046;&#22495;&#30340;&#19987;&#23478;&#20204;&#36890;&#24120;&#26377;&#20852;&#36259;&#29702;&#35299;&#22823;&#22411;&#25991;&#26412;&#38598;&#21512;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22024;&#26434;&#30340;&#26080;&#30417;&#30563;&#25216;&#26415;&#65288;&#22914;&#20027;&#39064;&#27169;&#22411;&#65289;&#25110;&#25163;&#21160;&#20027;&#39064;&#21457;&#29616;&#27969;&#31243;&#26469;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20027;&#39064;&#30340;&#23450;&#20041;&#65292;&#19981;&#20165;&#32771;&#34385;&#35789;&#20998;&#24067;&#65292;&#36824;&#21253;&#25324;&#34987;&#39046;&#22495;&#19987;&#23478;&#35270;&#20026;&#30456;&#20851;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#30340;&#25277;&#35937;&#32423;&#21035;&#19978;&#25509;&#25910;&#21644;&#32534;&#30721;&#19987;&#23478;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#33258;&#21160;&#21270;&#21644;&#25163;&#21160;&#32534;&#30721;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#20801;&#35768;&#19987;&#23478;&#25511;&#21046;&#20182;&#20204;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20943;&#23569;&#25152;&#38656;&#30340;&#25163;&#21160;&#24037;&#20316;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experts across diverse disciplines are often interested in making sense of large text collections. Traditionally, this challenge is approached either by noisy unsupervised techniques such as topic models, or by following a manual theme discovery process. In this paper, we expand the definition of a theme to account for more than just a word distribution, and include generalized concepts deemed relevant by domain experts. Then, we propose an interactive framework that receives and encodes expert feedback at different levels of abstraction. Our framework strikes a balance between automation and manual coding, allowing experts to maintain control of their study while reducing the manual effort required.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;</title><link>http://arxiv.org/abs/2305.05091</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#25991;&#26412;&#28216;&#25103;&#20013;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-enhanced Agents for Interactive Text Games. (arXiv:2305.05091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#21152;&#24378;&#30340;&#20195;&#29702;&#26694;&#26550;&#65292;&#29992;&#20110;&#21152;&#24378;&#20195;&#29702;&#22312;&#25991;&#26412;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#65292;&#20855;&#26377;&#35760;&#24518;&#20808;&#21069;&#25805;&#20316;&#21644;&#29615;&#22659;&#23545;&#35937;&#21487;&#34892;&#24615;&#20004;&#39033;&#39046;&#22495;&#30693;&#35782;&#65292;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#30340;&#20132;&#27969;&#26159;&#26234;&#33021;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#38656;&#35201;&#35745;&#31639;&#27169;&#22411;&#23398;&#20064;&#21644;&#25512;&#29702;&#26377;&#20851;&#19990;&#30028;&#27010;&#24565;&#30340;&#30693;&#35782;&#65292;&#20854;&#30417;&#30563;&#31243;&#24230;&#20063;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#21152;&#24378;&#20195;&#29702;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#28216;&#25103;&#20013;&#30340;&#21151;&#33021;&#22522;&#30784;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#20004;&#31181;&#39046;&#22495;&#30693;&#35782;&#27880;&#20837;&#22522;&#20110;&#23398;&#20064;&#30340;&#20195;&#29702;&#20013;&#65306;&#20808;&#21069;&#27491;&#30830;&#25805;&#20316;&#30340;&#35760;&#24518;&#21644;&#29615;&#22659;&#20013;&#30456;&#20851;&#23545;&#35937;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#19977;&#20010;&#20195;&#34920;&#24615;&#27169;&#22411;&#31867;&#65306;`&#32431;`&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#22522;&#20110;&#35760;&#24518;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015; (seq2seq) &#27169;&#22411;&#21644;&#22522;&#20110;&#27880;&#24847;&#26426;&#21046;&#30340; seq2seq&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication via natural language is a crucial aspect of intelligence, and it requires computational models to learn and reason about world concepts, with varying levels of supervision. While there has been significant progress made on fully-supervised non-interactive tasks, such as question-answering and procedural text understanding, much of the community has turned to various sequential interactive tasks, as in semi-Markov text-based games, which have revealed limitations of existing approaches in terms of coherence, contextual awareness, and their ability to learn effectively from the environment. In this paper, we propose a framework for enabling improved functional grounding of agents in text-based games. Specifically, we consider two forms of domain knowledge that we inject into learning-based agents: memory of previous correct actions and affordances of relevant objects in the environment. Our framework supports three representative model classes: `pure' reinforcement learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20316;&#32773;&#24402;&#23646;&#20219;&#21153;&#20026;&#20363;&#36827;&#34892;&#20102;&#23454;&#20363;&#21270;&#65292;&#24182;&#20351;&#29992;&#22810;&#38454;&#27573;&#20219;&#21153;&#30340;NoveltyTask&#35780;&#20272;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#39046;&#22495;&#30340;&#22788;&#29702;&#26032;&#39062;&#24615;&#23454;&#20363;&#30340;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05079</link><description>&lt;p&gt;
NLP&#20013;&#22788;&#29702;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20316;&#32773;&#24402;&#23646;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Unified Evaluation Framework for Novelty Detection and Accommodation in NLP with an Instantiation in Authorship Attribution. (arXiv:2305.05079v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#20316;&#32773;&#24402;&#23646;&#20219;&#21153;&#20026;&#20363;&#36827;&#34892;&#20102;&#23454;&#20363;&#21270;&#65292;&#24182;&#20351;&#29992;&#22810;&#38454;&#27573;&#20219;&#21153;&#30340;NoveltyTask&#35780;&#20272;&#20102;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#39046;&#22495;&#30340;&#22788;&#29702;&#26032;&#39062;&#24615;&#23454;&#20363;&#30340;&#38382;&#39064;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#24050;&#32463;&#22312;&#8220;&#23553;&#38381;&#30340;&#19990;&#30028;&#8221;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20854;&#20013;&#35757;&#32451;&#38598;&#20013;&#30340;&#25152;&#26377;&#26631;&#31614;&#22312;&#35780;&#20272;&#38598;&#21512;&#20013;&#37117;&#26159;&#24050;&#30693;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#36890;&#24120;&#20250;&#35266;&#23519;&#21040;&#19981;&#23646;&#20110;&#20219;&#20309;&#24050;&#30693;&#31867;&#21035;&#30340;&#8220;&#26032;&#39062;&#8221;&#23454;&#20363;&#65292;&#36825;&#20351;&#24471;&#22788;&#29702;&#26032;&#39062;&#24615;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#24341;&#21457;&#23545;&#8220;&#22788;&#29702;&#26032;&#39062;&#24615;&#8221;&#30340;&#36825;&#19968;&#37325;&#35201;&#39046;&#22495;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#8220;NoveltyTask&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#38454;&#27573;&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#31995;&#32479;&#22312;&#27969;&#27700;&#32447;&#26032;&#39062;&#24615;&#8220;&#26816;&#27979;&#8221;&#21644;&#8220;&#36866;&#24212;&#24615;&#8221;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NoveltyTask&#30340;&#25968;&#23398;&#20844;&#24335;&#65292;&#24182;&#20197;&#20316;&#32773;&#24402;&#23646;&#20219;&#21153;&#20026;&#20363;&#36827;&#34892;&#23454;&#20363;&#21270;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#35782;&#21035;&#32473;&#23450;&#25991;&#26412;&#30340;&#27491;&#30830;&#20316;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20122;&#39532;&#36874;&#35780;&#35770;&#35821;&#26009;&#24211;&#65292;&#24182;&#20026;NoveltyTask&#32534;&#21046;&#20102;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;&#26469;&#33258;200&#20010;&#20316;&#32773;/&#26631;&#31614;&#30340;250k&#20010;&#23454;&#20363;&#65289;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#25506;&#32034;&#20102;&#20219;&#21153;&#30340;&#20960;&#20010;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22788;&#29702;&#26032;&#39062;&#24615;&#23454;&#20363;&#30340;&#38382;&#39064;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#19987;&#38376;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#24615;&#26816;&#27979;&#21644;&#36866;&#24212;&#24615;&#30340;&#32479;&#19968;&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#38500;&#20316;&#32773;&#24402;&#23646;&#20197;&#22806;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art natural language processing models have been shown to achieve remarkable performance in 'closed-world' settings where all the labels in the evaluation set are known at training time. However, in real-world settings, 'novel' instances that do not belong to any known class are often observed. This renders the ability to deal with novelties crucial. To initiate a systematic research in this important area of 'dealing with novelties', we introduce 'NoveltyTask', a multi-stage task to evaluate a system's performance on pipelined novelty 'detection' and 'accommodation' tasks. We provide mathematical formulation of NoveltyTask and instantiate it with the authorship attribution task that pertains to identifying the correct author of a given text. We use Amazon reviews corpus and compile a large dataset (consisting of 250k instances across 200 authors/labels) for NoveltyTask. We conduct comprehensive experiments and explore several baseline methods for the task. Our results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23567;&#22411;GPT&#20013;&#30340;&#30456;&#24178;&#27874;&#21160;&#21644;&#35821;&#35328;&#29983;&#25104;&#26426;&#21046;&#65292;&#21457;&#29616;&#27874;&#21160;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#33268;&#21644;&#21487;&#37325;&#22797;&#30340;&#20869;&#22312;&#25391;&#33633;&#27169;&#24335;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21487;&#22609;&#24615;&#21644;&#34920;&#29616;&#21147;&#65292;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#65292;&#20026;&#29702;&#35299;&#21644;&#25511;&#21046;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#27169;&#24335;&#24418;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2305.05061</link><description>&lt;p&gt;
&#19968;&#31181;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#30340;&#30456;&#24178;&#27874;&#21160;&#21644;&#35821;&#35328;&#29983;&#25104;&#26426;&#21046;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Coherent Wave Dynamics and Language Generation of a Generative Pre-trained Transformer. (arXiv:2305.05061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#23567;&#22411;GPT&#20013;&#30340;&#30456;&#24178;&#27874;&#21160;&#21644;&#35821;&#35328;&#29983;&#25104;&#26426;&#21046;&#65292;&#21457;&#29616;&#27874;&#21160;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#33268;&#21644;&#21487;&#37325;&#22797;&#30340;&#20869;&#22312;&#25391;&#33633;&#27169;&#24335;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21487;&#22609;&#24615;&#21644;&#34920;&#29616;&#21147;&#65292;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#65292;&#20026;&#29702;&#35299;&#21644;&#25511;&#21046;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#27169;&#24335;&#24418;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65288;GPT&#65289;&#65292;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#26032;&#20852;&#30340;&#33021;&#21147;&#20063;&#24341;&#21457;&#20102;&#35768;&#22810;&#38656;&#35201;&#35299;&#20915;&#30340;&#38382;&#39064;&#12289;&#20851;&#27880;&#21644;&#25361;&#25112;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#23567;&#22411;GPT&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#21644;&#36890;&#36947;&#27874;&#21160;&#26426;&#21046;&#65292;&#37325;&#28857;&#20851;&#27880;&#20132;&#21449;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#20010;&#20307;&#33258;&#30456;&#20851;&#24615;&#26041;&#38754;&#30340;&#27874;&#21160;&#27169;&#24335;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27874;&#21160;&#21160;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#33268;&#21644;&#21487;&#37325;&#22797;&#30340;&#20869;&#22312;&#25391;&#33633;&#27169;&#24335;&#65292;&#20197;&#21450;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#21487;&#22609;&#24615;&#21644;&#34920;&#29616;&#21147;&#65292;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#12290;&#36890;&#36807;&#20998;&#26512;&#27874;&#21160;&#27169;&#24335;&#12289;&#30456;&#24178;&#24615;&#21644;&#32858;&#31867;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#21644;&#35299;&#37322;&#38544;&#34255;&#29366;&#24577;&#36890;&#36947;&#30340;&#21151;&#33021;&#65292;&#20026;&#29702;&#35299;&#21644;&#25511;&#21046;&#26356;&#39640;&#32423;&#21035;&#30340;&#35821;&#35328;&#27169;&#24335;&#24418;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25340;&#20889;&#38169;&#35823;&#30340;&#27850;&#26494;&#32479;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as the Generative Pretrained Transformer (GPT), have achieved tremendous success in various language tasks, but their emergent abilities have also raised many questions, concerns, and challenges that need to be addressed. To gain a better understanding of the models' inner mechanisms, we analyze the hidden state and channel wave dynamics in a small GPT, focusing on the coherence of wave patterns in terms of cross-channel correlation and individual auto-correlation. Our findings suggest that wave dynamics offer consistent and repeatable intrinsic oscillation modes, along with context-aware plasticity and expressiveness in language generation. By analyzing wave patterns, coherence, and clustering, we provide a systematic way to identify and interpret the functionality of the hidden state channels, paving the way to understand and control higher-level language pattern formation. In addition, we investigate the Poisson statistics of spelling errors in tex
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;&#26790;&#30340;&#25253;&#21578;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26412;&#23383;&#31526;&#20018;&#65292;&#21457;&#29616;&#26790;&#30340;&#25253;&#21578;&#25972;&#20307;&#19978;&#19981;&#20559;&#31163;&#32500;&#22522;&#30334;&#31185;&#65292;&#32780;&#21333;&#20010;&#26790;&#25253;&#21578;&#27604;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#26356;&#21487;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.05054</link><description>&lt;p&gt;
&#26790;&#27604;&#20320;&#24819;&#35937;&#30340;&#26356;&#8220;&#21487;&#39044;&#27979;&#8221;&#65288;arXiv&#65306;2305.05054v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Dreams Are More "Predictable'' Than You Think. (arXiv:2305.05054v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05054
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;&#26790;&#30340;&#25253;&#21578;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#25991;&#26412;&#23383;&#31526;&#20018;&#65292;&#21457;&#29616;&#26790;&#30340;&#25253;&#21578;&#25972;&#20307;&#19978;&#19981;&#20559;&#31163;&#32500;&#22522;&#30334;&#31185;&#65292;&#32780;&#21333;&#20010;&#26790;&#25253;&#21578;&#27604;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#26356;&#21487;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#26790;&#30340;&#25253;&#21578;&#22312;&#35821;&#20041;&#20869;&#23481;&#19978;&#19982;&#20854;&#20182;&#31867;&#22411;&#30340;&#25991;&#26412;&#35760;&#24405;&#26174;&#30528;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#26790;/&#30561;&#30496;&#30740;&#31350;&#30028;&#26222;&#36941;&#35748;&#20026;&#65292;&#26790;&#30340;&#25253;&#21578;&#26500;&#25104;&#20102;&#30456;&#24403;&#8220;&#29420;&#29305;&#8221;&#30340;&#25991;&#26412;&#23383;&#31526;&#20018;&#12290;&#36825;&#21487;&#33021;&#26159;&#36234;&#26469;&#36234;&#22810;&#30340;&#26041;&#27861;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24037;&#20855;&#26469;&#33258;&#21160;&#20998;&#26512;&#26790;&#30340;&#25253;&#21578;&#30340;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#22312;&#32593;&#32476;&#19978;&#29228;&#21462;&#30340;&#38750;&#26790;&#24819;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#30340;&#31070;&#32463;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20316;&#32773;&#23558;&#37319;&#29992;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#30740;&#31350;&#26790;&#24819;&#25253;&#21578;&#26159;&#21542;&#20559;&#31163;&#20854;&#20182;&#20154;&#29983;&#25104;&#30340;&#25991;&#26412;&#23383;&#31526;&#20018;&#65288;&#20363;&#22914;&#32500;&#22522;&#30334;&#31185;&#65289;&#65292;&#20197;&#21450;&#22914;&#20309;&#20559;&#31163;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;DreamBank&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#19981;&#20559;&#31163;&#32500;&#22522;&#30334;&#31185;&#12290;&#27492;&#22806;&#65292;&#23601;&#24179;&#22343;&#32780;&#35328;&#65292;&#21333;&#20010;&#26790;&#25253;&#21578;&#27604;&#32500;&#22522;&#30334;&#31185;&#25991;&#31456;&#26356;&#21487;&#39044;&#27979;&#12290;&#21021;&#27493;&#35777;&#25454;&#34920;&#26126;&#65292;&#23383;&#25968;&#12289;&#24615;&#21035;&#21644;&#35270;&#35273;&#38556;&#30861;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#26790;&#30340;&#21487;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A consistent body of evidence suggests that dream reports significantly vary from other types of textual transcripts with respect to semantic content. Furthermore, it appears to be a widespread belief in the dream/sleep research community that dream reports constitute rather ``unique'' strings of text. This might be a notable issue for the growing amount of approaches using natural language processing (NLP) tools to automatically analyse dream reports, as they largely rely on neural models trained on non-dream corpora scraped from the web. In this work, I will adopt state-of-the-art (SotA) large language models (LLMs), to study if and how dream reports deviate from other human-generated text strings, such as Wikipedia. Results show that, taken as a whole, DreamBank does not deviate from Wikipedia. Moreover, on average, single dream reports are significantly more predictable than Wikipedia articles. Preliminary evidence suggests that word count, gender, and visual impairment can signifi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.05050</link><description>&lt;p&gt;
ANALOGICAL- &#19968;&#31181;&#26032;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#25991;&#26412;&#31867;&#27604;&#35780;&#27979;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ANALOGICAL - A New Benchmark for Analogy of Long Text for Large Language Models. (arXiv:2305.05050v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#29992;&#20197;&#20869;&#22312;&#35780;&#20272;LLMs&#22312;&#38271;&#25991;&#26412;&#31867;&#27604;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#65292;&#24182;&#20351;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#26469;&#35780;&#20272;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#20197;&#35789;&#32423;&#21035;&#30340;&#31867;&#27604;&#20026;&#24418;&#24335;&#30340;&#31867;&#27604;&#22312;&#34913;&#37327;&#35832;&#22914;word2vec&#20043;&#31867;&#30340;&#35789;&#23884;&#20837;&#26041;&#27861;&#30340;&#36136;&#37327;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20027;&#35201;&#26681;&#25454;GLUE&#21644;SuperGLUE&#31561;&#22522;&#20934;&#30340;&#22806;&#22312;&#37327;&#24230;&#36827;&#34892;&#35780;&#20272;&#65292;&#32780;&#22312;LLMs&#26159;&#21542;&#33021;&#22815;&#22312;&#38271;&#25991;&#26412;&#20013;&#32472;&#21046;&#31867;&#27604;&#30340;&#26041;&#38754;&#65292;&#21482;&#26377;&#23569;&#25968;&#20960;&#39033;&#30740;&#31350;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;ANALOGICAL&#8221;&#30340;&#26032;&#22411;&#22522;&#20934;&#65292;&#20197;&#20845;&#20010;&#22797;&#26434;&#32423;&#21035;&#30340;&#38271;&#25991;&#26412;&#31867;&#27604;&#20998;&#31867;&#23545;LLMs&#36827;&#34892;&#20869;&#22312;&#35780;&#20272;&#65292;&#20998;&#21035;&#20026; (i)&#21333;&#35789;&#12289;(ii)&#21333;&#35789;vs&#21477;&#23376;&#12289;(iii)&#35821;&#27861;&#12289;(iv)&#21542;&#23450;&#12289;(v)&#34164;&#21547;&#21644;(vi)&#38544;&#21947;&#12290;&#21033;&#29992;13&#20010;&#25968;&#25454;&#38598;&#21644;&#19977;&#31181;&#19981;&#21516;&#30340;&#36317;&#31163;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;8&#20010;LLMs&#22312;&#35821;&#20041;&#21521;&#37327;&#31354;&#38388;&#20013;&#35782;&#21035;&#31867;&#27604;&#23545;&#30340;&#33021;&#21147;(&#20363;&#22914;&#65292;&#8220;&#25105;&#33021;&#35828;&#20004;&#31181;&#35821;&#35328;&#8221;&#24212;&#35813;&#26356;&#25509;&#36817;&#8220;&#25105;&#26159;&#21452;&#35821;&#30340;&#8221;&#65292;&#32780;&#8220;&#25105;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#21644;&#8220;&#25105;&#19981;&#21916;&#27426;&#24039;&#20811;&#21147;&#8221;&#24212;&#35813;&#26159;&#27491;&#20132;&#30340;)&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, analogies, in the form of word-level analogies, have played a significant role as an intrinsic measure of evaluating the quality of word embedding methods such as word2vec. Modern large language models (LLMs), however, are primarily evaluated on extrinsic measures based on benchmarks such as GLUE and SuperGLUE, and there are only a few investigations on whether LLMs can draw analogies between long texts. In this paper, we present ANALOGICAL, a new benchmark to intrinsically evaluate LLMs across a taxonomy of analogies of long text with six levels of complexity -- (i) word, (ii) word vs. sentence, (iii) syntactic, (iv) negation, (v) entailment, and (vi) metaphor. Using thirteen datasets and three different distance measures, we evaluate the abilities of eight LLMs in identifying analogical pairs in the semantic vector space (e.g., "I can speak two languages" should be closer to "I am bilingual" while "I like chocolate" and "I do not like chocolate" should be orthog
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05027</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Web Content Filtering through knowledge distillation of Large Language Models. (arXiv:2305.05027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#33976;&#39311;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#65292;&#20854;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569; 175 &#20493;&#30340;&#24773;&#20917;&#19979;&#65292;&#31934;&#24230;&#25552;&#21319;&#20102; 9%&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340; URL &#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#30340;&#20027;&#35201;&#30446;&#26631;&#65306;&#20445;&#38556;&#32452;&#32455;&#20813;&#21463;&#27861;&#24459;&#21644;&#20262;&#29702;&#39118;&#38505;&#65292;&#38480;&#21046;&#35775;&#38382;&#39640;&#39118;&#38505;&#25110;&#21487;&#30097;&#32593;&#31449;&#65292;&#20197;&#21450;&#20419;&#36827;&#23433;&#20840;&#30340;&#19987;&#19994;&#24037;&#20316;&#29615;&#22659;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20934;&#30830;&#30340;&#20998;&#31867;&#65292;&#24182;&#21033;&#29992;&#24050;&#26377;&#30340;&#30693;&#35782;&#33976;&#39311;&#25216;&#26415;&#21019;&#24314;&#26356;&#23567;&#12289;&#26356;&#19987;&#19994;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#20197;&#29992;&#20110;&#32593;&#32476;&#20869;&#23481;&#36807;&#28388;&#12290;&#22312;&#23558;&#36890;&#36807;&#22823;&#22411;&#23433;&#20840;&#20379;&#24212;&#21830;&#25910;&#38598;&#30340;&#23458;&#25143;&#36965;&#27979;&#25968;&#25454;&#30340; 30 &#20010;&#19981;&#21516;&#20869;&#23481;&#31867;&#21035;&#30340;&#32593;&#31449;&#36827;&#34892;&#20998;&#31867;&#30340;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;&#33976;&#39311;&#32467;&#26524;&#23454;&#29616;&#20102; 9% &#30340;&#20998;&#31867;&#31934;&#24230;&#25552;&#21319;&#65292;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#21442;&#25968;&#25968;&#37327;&#19978;&#19982;&#21407;&#22987;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#20943;&#23569;&#20102; 175 &#20493;&#65292;&#20174;&#32780;&#36798;&#21040;&#20102;&#19982;&#32769;&#24072;&#27169;&#22411;&#30456;&#21305;&#37197;&#30340;&#24615;&#33021;&#65292;&#21487;&#20197;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25195;&#25551;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a state-of-the-art approach for URL categorization that leverages the power of Large Language Models (LLMs) to address the primary objectives of web content filtering: safeguarding organizations from legal and ethical risks, limiting access to high-risk or suspicious websites, and fostering a secure and professional work environment. Our method utilizes LLMs to generate accurate classifications and then employs established knowledge distillation techniques to create smaller, more specialized student models tailored for web content filtering. Distillation results in a student model with a 9\% accuracy rate improvement in classifying websites, sourced from customer telemetry data collected by a large security vendor, into 30 distinct content categories based on their URLs, surpassing the current state-of-the-art approach. Our student model matches the performance of the teacher LLM with 175 times less parameters, allowing the model to be used for in-line scanning of large vo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#20989;&#25968;PTLoss&#65292;&#36890;&#36807;&#25200;&#21160;&#32769;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05010</link><description>&lt;p&gt;
&#19981;&#35201;&#30450;&#30446;&#27169;&#20223;&#32769;&#24072;&#65306;&#20351;&#29992;&#25200;&#21160;&#25439;&#22833;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Do Not Blindly Imitate the Teacher: Using Perturbed Loss for Knowledge Distillation. (arXiv:2305.05010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#20989;&#25968;PTLoss&#65292;&#36890;&#36807;&#25200;&#21160;&#32769;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#65292;&#20351;&#20854;&#26356;&#25509;&#36817;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#25945;&#24072;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#20013;&#12290;&#36890;&#24120;&#65292;&#23398;&#29983;&#36890;&#36807;&#26368;&#23567;&#21270;&#20854;&#36755;&#20986;&#20998;&#24067;&#21644;&#25945;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#27169;&#20223;&#25945;&#24072;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#23398;&#20064;&#30446;&#26631;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#25945;&#24072;&#30340;&#36755;&#20986;&#20998;&#24067;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#20998;&#24067;&#23384;&#22312;&#24046;&#24322;&#12290;&#22240;&#27492;&#65292;&#24378;&#21046;&#23398;&#29983;&#30450;&#30446;&#27169;&#20223;&#19981;&#21487;&#38752;&#30340;&#25945;&#24072;&#36755;&#20986;&#20998;&#24067;&#20250;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#30446;&#26631;&#20989;&#25968;PTLoss&#65292;&#39318;&#20808;&#36890;&#36807;Maclaurin&#32423;&#25968;&#34920;&#31034;&#39321;&#33609;KL&#33976;&#39311;&#25439;&#22833;&#20989;&#25968;&#65292;&#28982;&#21518;&#25200;&#21160;&#35813;&#32423;&#25968;&#20013;&#30340;&#20027;&#23548;&#39033;&#12290;&#36825;&#31181;&#25200;&#21160;&#25439;&#22833;&#38544;&#24335;&#22320;&#23558;&#21407;&#22987;&#32769;&#24072;&#36716;&#25442;&#20026;&#20855;&#26377;&#26356;&#25509;&#36817;&#22320;&#38754;&#30495;&#23454;&#20998;&#24067;&#30340;&#20195;&#29702;&#32769;&#24072;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#39321;&#33609;KL&#33976;&#39311;&#21644;&#25200;&#21160;KL&#33976;&#39311;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation is a popular technique to transfer knowledge from large teacher models to a small student model. Typically, the student learns to imitate the teacher by minimizing the KL divergence of its output distribution with the teacher's output distribution. In this work, we argue that such a learning objective is sub-optimal because there exists a discrepancy between the teacher's output distribution and the ground truth label distribution. Therefore, forcing the student to blindly imitate the unreliable teacher output distribution leads to inferior performance. To this end, we propose a novel knowledge distillation objective PTLoss by first representing the vanilla KL-based distillation loss function via a Maclaurin series and then perturbing the leading-order terms in this series. This perturbed loss implicitly transforms the original teacher into a proxy teacher with a distribution closer to the ground truth distribution. We establish the theoretical connection between
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22242;&#38431;&#21442;&#19982;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#21307;&#24739;&#20132;&#27969;&#20013;&#20020;&#24202;&#35760;&#24405;&#25688;&#35201;&#30340;&#33258;&#21160;&#21270;&#29983;&#25104;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#26412;&#26041;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26497;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05001</link><description>&lt;p&gt;
GersteinLab&#22312;MEDIQA-Chat 2023&#20013;&#30340;&#36129;&#29486;&#65306;&#36890;&#36807;&#31934;&#35843;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#25972;&#21512;&#21307;&#24739;&#20132;&#27969;&#20013;&#30340;&#20020;&#24202;&#35760;&#24405;&#25688;&#35201;(arXiv:2305.05001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning. (arXiv:2305.05001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22242;&#38431;&#21442;&#19982;MEDIQA-Chat 2023&#20849;&#20139;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#21307;&#24739;&#20132;&#27969;&#20013;&#20020;&#24202;&#35760;&#24405;&#25688;&#35201;&#30340;&#33258;&#21160;&#21270;&#29983;&#25104;&#12290;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24494;&#35843;&#21644;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#26412;&#26041;&#27861;&#22312;&#20851;&#38190;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26497;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;MEDIQA-2023 Dialogue2Note&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#36129;&#29486;&#65292;&#21253;&#25324;&#23376;&#20219;&#21153;A&#21644;&#23376;&#20219;&#21153;B&#12290;&#25105;&#20204;&#23558;&#20219;&#21153;&#35270;&#20026;&#19968;&#31181;&#23545;&#35805;&#25688;&#35201;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#27969;&#31243;&#65306;&#65288;a&#65289;&#23545;&#39044;&#35757;&#32451;&#30340;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#21644;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#65288;b&#65289;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#36827;&#34892;&#23569;&#37327;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#12290;&#20004;&#31181;&#26041;&#27861;&#22312;ROUGE-1 F1&#65292;BERTScore F1&#65288;deberta-xlarge-mnli&#65289;&#21644;BLEURT&#26041;&#38754;&#22343;&#21462;&#24471;&#20102;&#26497;&#22909;&#30340;&#32467;&#26524;&#65292;&#20998;&#21035;&#20026;0.4011&#12289;0.7058&#21644;0.5421&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;RoBERTa&#21644;SciBERT&#22522;&#20110;&#20998;&#31867;&#27169;&#22411;&#26469;&#39044;&#27979;&#30456;&#20851;&#30340;&#31456;&#33410;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#22312;&#25152;&#26377;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;&#22235;&#65292;&#27599;&#20010;&#22242;&#38431;&#20801;&#35768;&#25552;&#20132;&#19977;&#27425;&#36816;&#34892;&#20316;&#20026;&#20854;&#25552;&#20132;&#30340;&#19968;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#19987;&#23478;&#27880;&#37322;&#35777;&#26126;&#20102;&#36890;&#36807;ICL GPT-4&#29983;&#25104;&#30340;&#25688;&#35201;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents our contribution to the MEDIQA-2023 Dialogue2Note shared task, encompassing both subtask A and subtask B. We approach the task as a dialogue summarization problem and implement two distinct pipelines: (a) a fine-tuning of a pre-trained dialogue summarization model and GPT-3, and (b) few-shot in-context learning (ICL) using a large language model, GPT-4. Both methods achieve excellent results in terms of ROUGE-1 F1, BERTScore F1 (deberta-xlarge-mnli), and BLEURT, with scores of 0.4011, 0.7058, and 0.5421, respectively. Additionally, we predict the associated section headers using RoBERTa and SciBERT based classification models. Our team ranked fourth among all teams, while each team is allowed to submit three runs as part of their submission. We also utilize expert annotations to demonstrate that the notes generated through the ICL GPT-4 are better than all other baselines. The code for our submission is available.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04990</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#24494;&#35843;&#20351;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#24378;&#38887;
&lt;/p&gt;
&lt;p&gt;
Explanation-based Finetuning Makes Models More Robust to Spurious Cues. (arXiv:2305.04990v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#26041;&#27861;&#8212;&#8212;&#35299;&#37322;&#24615;&#24494;&#35843;&#65292;&#36890;&#36807;&#35753;&#27169;&#22411;&#22312;&#32473;&#20986;&#31572;&#26696;&#30340;&#21516;&#26102;&#29983;&#25104;&#25903;&#25345;&#35813;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#65292;&#26469;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#65292;&#20351;&#24471;&#27169;&#22411;&#23545;&#34394;&#20551;&#25552;&#31034;&#26356;&#21152;&#24378;&#38887;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#38750;&#24120;&#24378;&#22823;&#65292;&#26377;&#26102;&#20250;&#23398;&#20064;&#21040;&#26631;&#31614;&#21644;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#22312;&#20998;&#24067;&#22806;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#35299;&#37322;&#24615;&#24494;&#35843;&#20316;&#20026;&#20943;&#36731;LLMs&#20381;&#36182;&#34394;&#20551;&#20851;&#32852;&#30340;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#21482;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#31572;&#26696;&#19981;&#21516;&#65292;&#25105;&#20204;&#24494;&#35843;&#27169;&#22411;&#20197;&#29983;&#25104;&#25903;&#25345;&#20854;&#31572;&#26696;&#30340;&#33258;&#30001;&#25991;&#26412;&#35299;&#37322;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#20154;&#24037;&#26500;&#24314;&#30340;&#35757;&#32451;&#38598;&#19978;&#24494;&#35843;&#27169;&#22411;&#65292;&#35813;&#35757;&#32451;&#38598;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#30340;&#34394;&#20551;&#25552;&#31034;&#65292;&#24182;&#22312;&#27809;&#26377;&#36825;&#20123;&#25552;&#31034;&#30340;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#19982;&#26631;&#20934;&#24494;&#35843;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;&#20998;&#31867;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#19979;&#38477;&#26041;&#38754;&#20351;&#27169;&#22411;&#26497;&#20854;&#24378;&#38887;&#65306;ComVE&#65288;+1.2&#65289;&#65292;CREAK&#65288;+9.1&#65289;&#65292;e-SNLI&#65288;+15.4&#65289;&#21644;SBIC&#65288;+6.5&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#37322;&#21516;&#26679;&#26377;&#25928;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are so powerful that they sometimes learn correlations between labels and features that are irrelevant to the task, leading to poor generalization on out-of-distribution data. We propose explanation-based finetuning as a novel and general approach to mitigate LLMs' reliance on spurious correlations. Unlike standard finetuning where the model only predicts the answer given the input, we finetune the model to additionally generate a free-text explanation supporting its answer. To evaluate our method, we finetune the model on artificially constructed training sets containing different types of spurious cues, and test it on a test set without these cues. Compared to standard finetuning, our method makes models remarkably more robust against spurious cues in terms of accuracy drop across four classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC (+6.5). Moreover, our method works equally well with explanations generated by the model, implyin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#35780;&#20272;Self-Attention&#21464;&#21387;&#22120;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#20135;&#29983;&#30340;&#25511;&#21046;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#19981;&#23558;&#23545;&#35937;&#21644;&#27010;&#24565;&#32423;&#21035;&#30340;&#21547;&#20041;&#21644;&#35821;&#20041;&#36171;&#20104;&#25152;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#24335;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#20013;&#25152;&#25551;&#36848;&#30340;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.04989</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#25351;&#23548;&#19979;&#35821;&#35328;&#27169;&#22411;&#35821;&#20041;&#35780;&#20272;&#20197;&#25552;&#39640;&#29992;&#25143;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust. (arXiv:2305.04989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#35780;&#20272;Self-Attention&#21464;&#21387;&#22120;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#20135;&#29983;&#30340;&#25511;&#21046;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#19981;&#23558;&#23545;&#35937;&#21644;&#27010;&#24565;&#32423;&#21035;&#30340;&#21547;&#20041;&#21644;&#35821;&#20041;&#36171;&#20104;&#25152;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#24335;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#20013;&#25152;&#25551;&#36848;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#19968;&#20010;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#65306;&#35821;&#35328;&#27169;&#22411;&#25429;&#25417;&#21040;&#20102;&#20160;&#20040;&#26679;&#30340;&#35821;&#35328;&#32467;&#26500;&#21644;&#35821;&#20041;&#65311;&#20687;&#30693;&#35782;&#22270;&#35889;&#36825;&#26679;&#30340;&#22270;&#34920;&#36798;&#24418;&#24335;&#24456;&#23481;&#26131;&#36827;&#34892;&#35780;&#20272;&#65292;&#22240;&#20026;&#23427;&#20204;&#26126;&#30830;&#22320;&#34920;&#36798;&#20102;&#35821;&#35328;&#35821;&#20041;&#21644;&#32467;&#26500;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#30340;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#26469;&#35780;&#20272;Self-Attention&#21464;&#21387;&#22120;&#20013;&#32534;&#30721;&#30340;&#35821;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#20174;&#30693;&#35782;&#22270;&#35889;&#33719;&#21462;&#30340;&#22270;&#24418;&#36335;&#24452;&#24207;&#21015;&#24182;&#23581;&#35797;&#20174;Self-Attention&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#22797;&#21046;/&#37325;&#26500;&#21516;&#26679;&#36335;&#24452;&#26469;&#27979;&#37327;&#37325;&#26500;&#35823;&#24046;&#12290;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#36879;&#26126;&#24615;&#23545;&#20110;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#20915;&#31574;&#32467;&#26524;&#31561;&#31038;&#20250;&#38382;&#39064;&#26377;&#30528;&#24040;&#22823;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#35821;&#35328;&#27169;&#22411;&#26159;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#20135;&#29983;&#30340;&#25511;&#21046;&#36807;&#31243;&#30340;&#27169;&#22411;&#65292;&#20294;&#26159;&#23427;&#20204;&#19981;&#23558;&#23545;&#35937;&#21644;&#27010;&#24565;&#32423;&#21035;&#30340;&#21547;&#20041;&#21644;&#35821;&#20041;&#36171;&#20104;&#25152;&#23398;&#20064;&#30340;&#38543;&#26426;&#27169;&#24335;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#20013;&#25152;&#25551;&#36848;&#30340;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#31216;&#20026;NeuroComparatives&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#36827;&#34892;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#65292;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#30693;&#35782;&#36827;&#34892;&#20005;&#26684;&#36807;&#28388;&#12290;</title><link>http://arxiv.org/abs/2305.04978</link><description>&lt;p&gt;
NeuroComparatives&#65306;&#27604;&#36739;&#30693;&#35782;&#30340;&#31070;&#32463;&#31526;&#21495;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
NeuroComparatives: Neuro-Symbolic Distillation of Comparative Knowledge. (arXiv:2305.04978v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#21517;&#31216;&#20026;NeuroComparatives&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#36827;&#34892;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#65292;&#20197;&#21450;&#23545;&#29983;&#25104;&#30340;&#30693;&#35782;&#36827;&#34892;&#20005;&#26684;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#30693;&#35782;&#26159;&#25105;&#20204;&#19990;&#30028;&#30693;&#35782;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20294;&#22312;&#20197;&#21069;&#30340;&#25991;&#29486;&#20013;&#30740;&#31350;&#19981;&#36275;&#12290;&#26412;&#25991;&#30740;&#31350;&#27604;&#36739;&#30693;&#35782;&#33719;&#21462;&#20219;&#21153;&#65292;&#21463;&#21040;&#20687;GPT-3&#36825;&#26679;&#26497;&#31471;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#26174;&#30528;&#25552;&#39640;&#30340;&#25512;&#21160;&#65292;&#25512;&#21160;&#20102;&#23558;&#20182;&#20204;&#30340;&#30693;&#35782;&#25910;&#38598;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#21162;&#21147;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;API&#35775;&#38382;&#21463;&#21040;&#38480;&#21046;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#30693;&#35782;&#33719;&#21462;&#30340;&#33539;&#22260;&#21644;&#22810;&#26679;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30475;&#20284;&#19981;&#21487;&#34892;&#30340;&#38382;&#39064;&#65306;&#26356;&#26131;&#20110;&#35775;&#38382;&#12289;&#35268;&#27169;&#26356;&#23567;&#12289;&#24615;&#33021;&#26356;&#24369;&#30340;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33719;&#21462;&#27604;&#36739;&#30693;&#35782;&#65292;&#20174;&#32780;&#36798;&#21040;&#19982;&#22823;&#35268;&#27169;&#27169;&#22411;&#30456;&#24403;&#30340;&#36136;&#37327;&#65311;&#25105;&#20204;&#24341;&#20837;&#20102;NeuroComparatives&#65292;&#19968;&#31181;&#20351;&#29992;&#35789;&#27719;&#32422;&#26463;&#35299;&#30721;&#30340;&#27604;&#36739;&#30693;&#35782;&#25552;&#28860;&#26032;&#26694;&#26550;&#65292;&#20854;&#21518;&#32039;&#23494;&#36807;&#28388;&#29983;&#25104;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comparative knowledge (e.g., steel is stronger and heavier than styrofoam) is an essential component of our world knowledge, yet understudied in prior literature. In this paper, we study the task of comparative knowledge acquisition, motivated by the dramatic improvements in the capabilities of extreme-scale language models like GPT-3, which have fueled efforts towards harvesting their knowledge into knowledge bases. However, access to inference API for such models is limited, thereby restricting the scope and the diversity of the knowledge acquisition. We thus ask a seemingly implausible question: whether more accessible, yet considerably smaller and weaker models such as GPT-2, can be utilized to acquire comparative knowledge, such that the resulting quality is on par with their large-scale counterparts?  We introduce NeuroComparatives, a novel framework for comparative knowledge distillation using lexically-constrained decoding, followed by stringent filtering of generated knowledge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#65292;&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;LABO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#65292;&#24182;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#26631;&#31614;&#24179;&#28369;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.04971</link><description>&lt;p&gt;
LABO: &#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#26368;&#20339;&#26631;&#31614;&#27491;&#21017;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization. (arXiv:2305.04971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#65292;&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;LABO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#65292;&#24182;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#26631;&#31614;&#24179;&#28369;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#25216;&#26415;&#23545;&#20110;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20381;&#36182;&#20110;&#26435;&#37325;&#34928;&#20943;&#12289;&#20002;&#24323;&#12289;&#25209;/&#23618;&#24402;&#19968;&#21270;&#31561;&#25216;&#26415;&#26469;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#21478;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;LS&#20551;&#35774;&#27599;&#20010;&#38750;&#30446;&#26631;&#31867;&#21035;&#20986;&#29616;&#30340;&#27010;&#29575;&#30456;&#31561;&#65292;&#19981;&#33021;&#26681;&#25454;&#23454;&#20363;&#23545;&#26631;&#31614;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#21452;&#23618;&#20248;&#21270;&#65288;LABO&#65289;&#38382;&#39064;&#26469;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20869;&#29615;&#33410;&#30340;&#30830;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#35299;&#65292;&#32780;&#26080;&#38656;&#23384;&#20648;&#32463;&#36807;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#32852;&#21512;&#35270;&#39057;&#25688;&#35201;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#32447;&#32034;&#21305;&#37197;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#23454;&#29616;&#26816;&#32034;&#35270;&#39057;&#20013;&#26368;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#26102;&#21051;&#12290;</title><link>http://arxiv.org/abs/2305.04961</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#32852;&#21512;&#26102;&#21051;&#26816;&#32034;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Joint Moment Retrieval and Highlight Detection Via Natural Language Queries. (arXiv:2305.04961v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#30340;&#32852;&#21512;&#35270;&#39057;&#25688;&#35201;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#32447;&#32034;&#21305;&#37197;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#23454;&#29616;&#26816;&#32034;&#35270;&#39057;&#20013;&#26368;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#26102;&#21051;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20114;&#32852;&#32593;&#19978;&#35270;&#39057;&#20869;&#23481;&#30340;&#19981;&#26029;&#22686;&#22810;&#65292;&#35270;&#39057;&#25688;&#35201;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#32852;&#21512;&#35270;&#39057;&#25688;&#35201;&#21644;&#31934;&#21326;&#29255;&#27573;&#26816;&#27979;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20351;&#29992;&#35270;&#35273;&#21644;&#38899;&#39057;&#32447;&#32034;&#26469;&#21305;&#37197;&#29992;&#25143;&#30340;&#33258;&#28982;&#35821;&#35328;&#26597;&#35810;&#65292;&#20197;&#26816;&#32034;&#35270;&#39057;&#20013;&#26368;&#30456;&#20851;&#21644;&#26377;&#36259;&#30340;&#26102;&#21051;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#22810;&#20010;&#26368;&#36817;&#29992;&#20110;&#35270;&#35273;&#36716;&#25442;&#22120;(ViTs)&#30340;&#25216;&#26415;&#65292;&#21019;&#24314;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#30340;Transformer&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#20363;&#22914;YouTube Highlights&#21644;TVSum&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video summarization has become an increasingly important task in the field of computer vision due to the vast amount of video content available on the internet. In this project, we propose a new method for natural language query based joint video summarization and highlight detection using multi-modal transformers. This approach will use both visual and audio cues to match a user's natural language query to retrieve the most relevant and interesting moments from a video. Our approach employs multiple recent techniques used in Vision Transformers (ViTs) to create a transformer-like encoder-decoder model. We evaluated our approach on multiple datasets such as YouTube Highlights and TVSum to demonstrate the flexibility of our proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;</title><link>http://arxiv.org/abs/2305.04940</link><description>&lt;p&gt;
&#26089;&#36215;&#30340;&#40479;&#20799;&#25417;&#21040;&#34411;&#65306;&#21033;&#29992;&#32534;&#30721;&#22120;&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#36827;&#34892;&#26356;&#26377;&#25928;&#30340;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
The EarlyBIRD Catches the Bug: On Exploiting Early Layers of Encoder Models for More Efficient Code Classification. (arXiv:2305.04940v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26089;&#26399;&#23618;&#32452;&#21512;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#21033;&#29992;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#20195;&#30721;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#24179;&#22343;&#21487;&#25552;&#39640;2&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#22914;&#28431;&#27934;&#26816;&#27979;&#21644;&#31867;&#22411;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#28145;&#24230;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20123;&#25216;&#26415;&#65292;&#26088;&#22312;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#20013;&#36164;&#28304;&#21644;&#21487;&#29992;&#20449;&#24687;&#30340;&#26368;&#20339;&#21033;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;EarlyBIRD&#65292;&#20174;&#39044;&#35757;&#32451;&#30340;transformer&#27169;&#22411;&#30340;&#26089;&#26399;&#23618;&#26500;&#24314;&#20195;&#30721;&#30340;&#22797;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#27604;&#36739;12&#31181;&#21019;&#24314;&#22797;&#21512;&#34920;&#31034;&#30340;&#31574;&#30053;&#19982;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#20010;&#32534;&#30721;&#22120;&#23618;&#30340;&#26631;&#20934;&#23454;&#36341;&#65292;&#22312;CodeBERT&#27169;&#22411;&#19978;&#23454;&#35777;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#22312;4&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#20960;&#20010;&#26089;&#26399;&#23618;&#30340;&#32452;&#21512;&#22312;&#32570;&#38519;&#26816;&#27979;&#26041;&#38754;&#20135;&#29983;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19968;&#20123;&#32452;&#21512;&#21017;&#25913;&#36827;&#20102;&#22810;&#31867;&#20998;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#24179;&#22343;&#26816;&#27979;&#22686;&#24378;2&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of modern Natural Language Processing (NLP) techniques has shown to be beneficial for software engineering tasks, such as vulnerability detection and type inference. However, training deep NLP models requires significant computational resources. This paper explores techniques that aim at achieving the best usage of resources and available information in these models.  We propose a generic approach, EarlyBIRD, to build composite representations of code from the early layers of a pre-trained transformer model. We empirically investigate the viability of this approach on the CodeBERT model by comparing the performance of 12 strategies for creating composite representations with the standard practice of only using the last encoder layer.  Our evaluation on four datasets shows that several early layer combinations yield better performance on defect detection, and some combinations improve multi-class classification. More specifically, we obtain a +2 average improvement of detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.04928</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#29983;&#29289;&#21307;&#23398;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transformer-based method for zero and few-shot biomedical named entity recognition. (arXiv:2305.04928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#23398;&#20064;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#65292;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#26679;&#26412;&#24773;&#20917;&#19979;&#36798;&#21040;&#33391;&#22909;&#30340;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#20013;&#65292;&#26377;&#30417;&#30563;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#20381;&#36182;&#20110;&#20855;&#26377;&#32473;&#23450;&#21629;&#21517;&#23454;&#20307;&#30340;&#22823;&#37327;&#27880;&#37322;&#25991;&#26412;&#65292;&#20854;&#21019;&#24314;&#21487;&#33021;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#27492;&#22806;&#65292;&#25552;&#21462;&#26032;&#23454;&#20307;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#39069;&#22806;&#30340;&#27880;&#37322;&#20219;&#21153;&#21644;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;NER&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#23558;&#22810;&#31867;&#26631;&#35760;&#20998;&#31867;&#20219;&#21153;&#36716;&#25442;&#20026;&#20108;&#20803;&#26631;&#35760;&#20998;&#31867;&#65288;&#26631;&#35760;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#25110;&#19981;&#21253;&#21547;&#25628;&#32034;&#30340;&#23454;&#20307;&#65289;&#65292;&#24182;&#22312;&#26356;&#22810;&#30340;&#25968;&#25454;&#38598;&#21644;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#20174;&#32780;&#21487;&#23398;&#20064;&#21040;&#32473;&#23450;&#21644;&#28508;&#22312;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#22312;9&#31181;&#19981;&#21516;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#19978;&#65292;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;NER&#12289;&#19968;&#27425;&#26679;&#26412;NER&#12289;10&#27425;&#26679;&#26412;NER&#21644;100&#27425;&#26679;&#26412;NER&#19978;&#23454;&#29616;&#20102;&#24179;&#22343;F1&#24471;&#20998;&#20998;&#21035;&#20026;35.44&#65285;&#12289;50.10&#65285;&#12289;69.94&#65285;&#21644;79.51&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised named entity recognition (NER) in the biomedical domain is dependent on large sets of annotated texts with the given named entities, whose creation can be time-consuming and expensive. Furthermore, the extraction of new entities often requires conducting additional annotation tasks and retraining the model. To address these challenges, this paper proposes a transformer-based method for zero- and few-shot NER in the biomedical domain. The method is based on transforming the task of multi-class token classification into binary token classification (token contains the searched entity or does not contain the searched entity) and pre-training on a larger amount of datasets and biomedical entities, from where the method can learn semantic relations between the given and potential classes. We have achieved average F1 scores of 35.44% for zero-shot NER, 50.10% for one-shot NER, 69.94% for 10-shot NER, and 79.51% for 100-shot NER on 9 diverse evaluated biomedical entities with PubMed
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#25991;&#21457;&#24067;&#20043;&#21069;&#35782;&#21035;&#20986;&#21363;&#23558;&#34987;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#24182;&#25512;&#29702;&#20854;&#28508;&#22312;&#21361;&#23475;&#21644;&#36829;&#21453;&#24179;&#21488;&#25919;&#31574;&#30340;&#21407;&#22240;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#36827;&#26356;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.04927</link><description>&lt;p&gt;
&#25552;&#21069;&#26816;&#27979;&#21644;&#25512;&#29702;&#21024;&#38500;&#25512;&#25991;
&lt;/p&gt;
&lt;p&gt;
Detecting and Reasoning of Deleted Tweets before they are Posted. (arXiv:2305.04927v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#25512;&#25991;&#21457;&#24067;&#20043;&#21069;&#35782;&#21035;&#20986;&#21363;&#23558;&#34987;&#21024;&#38500;&#30340;&#20869;&#23481;&#65292;&#24182;&#25512;&#29702;&#20854;&#28508;&#22312;&#21361;&#23475;&#21644;&#36829;&#21453;&#24179;&#21488;&#25919;&#31574;&#30340;&#21407;&#22240;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#25512;&#36827;&#26356;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#22312;&#20449;&#24687;&#20256;&#25773;&#21644;&#28040;&#36153;&#31561;&#26041;&#38754;&#32473;&#25105;&#20204;&#24102;&#26469;&#20102;&#35768;&#22810;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24179;&#21488;&#20063;&#23384;&#22312;&#30528;&#34987;&#28389;&#29992;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#24694;&#24847;&#29992;&#25143;&#29992;&#23427;&#20204;&#26469;&#25955;&#25773;&#20167;&#24680;&#35328;&#35770;&#12289;&#25915;&#20987;&#24615;&#20869;&#23481;&#12289;&#35875;&#35328;&#31561;&#65292;&#20197;&#33719;&#24471;&#31038;&#20250;&#21644;&#25919;&#27835;&#35758;&#31243;&#65292;&#25110;&#32773;&#20260;&#23475;&#20010;&#20154;&#12289;&#23454;&#20307;&#21644;&#32452;&#32455;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19968;&#20123;&#29992;&#25143;&#22312;&#26410;&#32463;&#39564;&#35777;&#30340;&#24773;&#20917;&#19979;&#26080;&#24847;&#20013;&#20998;&#20139;&#20449;&#24687;&#65292;&#25110;&#32773;&#26080;&#24847;&#20013;&#21457;&#24067;&#26377;&#23475;&#20449;&#24687;&#12290;&#19968;&#20123;&#27492;&#31867;&#20869;&#23481;&#32463;&#24120;&#34987;&#24179;&#21488;&#21024;&#38500;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#36829;&#21453;&#20102;&#26465;&#27454;&#21644;&#25919;&#31574;&#65292;&#20063;&#21487;&#33021;&#26159;&#30001;&#20110;&#29992;&#25143;&#33258;&#24049;&#30340;&#19981;&#21516;&#21407;&#22240;&#65292;&#27604;&#22914;&#21518;&#24724;&#20102;&#12290;&#30446;&#21069;&#26377;&#35768;&#22810;&#30740;&#31350;&#23545;&#21024;&#38500;&#20869;&#23481;&#36827;&#34892;&#20102;&#34920;&#24449;&#12289;&#29702;&#35299;&#21644;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#26088;&#22312;&#35782;&#21035;&#21024;&#38500;&#30340;&#32454;&#33268;&#21407;&#22240;&#65288;&#20363;&#22914;&#65292;&#24086;&#23376;&#20196;&#20154;&#21453;&#24863;&#12289;&#20167;&#24680;&#35328;&#35770;&#25110;&#27809;&#26377;&#21487;&#35782;&#21035;&#30340;&#21407;&#22240;&#65289;&#30340;&#30740;&#31350;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21363;&#23558;&#21457;&#24067;&#30340;&#34987;&#21024;&#38500;&#30340;&#25512;&#25991;&#65292;&#24182;&#25512;&#29702;&#23427;&#20204;&#30340;&#28508;&#22312;&#21361;&#23475;&#21644;&#36829;&#21453;&#24179;&#21488;&#25919;&#31574;&#30340;&#21407;&#22240;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25512;&#25991;&#30340;&#21508;&#31181;&#29305;&#24449;&#26469;&#39044;&#27979;&#23427;&#20204;&#26159;&#21542;&#20250;&#34987;&#21024;&#38500;&#65292;&#22914;&#26524;&#20250;&#34987;&#21024;&#38500;&#65292;&#21017;&#26159;&#20026;&#20160;&#20040;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035;&#21024;&#38500;&#30340;&#25512;&#25991;&#21450;&#20854;&#21407;&#22240;&#26041;&#38754;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#25104;&#20026;&#20419;&#36827;&#26356;&#23433;&#20840;&#21644;&#36127;&#36131;&#20219;&#30340;&#31038;&#20132;&#23186;&#20307;&#20351;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms empower us in several ways, from information dissemination to consumption. While these platforms are useful in promoting citizen journalism, public awareness etc., they have misuse potentials. Malicious users use them to disseminate hate-speech, offensive content, rumor etc. to gain social and political agendas or to harm individuals, entities and organizations. Often times, general users unconsciously share information without verifying it, or unintentionally post harmful messages. Some of such content often get deleted either by the platform due to the violation of terms and policies, or users themselves for different reasons, e.g., regrets. There is a wide range of studies in characterizing, understanding and predicting deleted content. However, studies which aims to identify the fine-grained reasons (e.g., posts are offensive, hate speech or no identifiable reason) behind deleted content, are limited. In this study we address this gap, by identifying deleted 
&lt;/p&gt;</description></item><item><title>MultiModal-GPT&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#20154;&#31867;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#30340;&#21508;&#31181;&#25351;&#20196;&#65292;&#24182;&#19988;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.04790</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;-GPT: &#29992;&#20110;&#19982;&#20154;&#31867;&#23545;&#35805;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MultiModal-GPT: A Vision and Language Model for Dialogue with Humans. (arXiv:2305.04790v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04790
&lt;/p&gt;
&lt;p&gt;
MultiModal-GPT&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#20154;&#31867;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#30340;&#21508;&#31181;&#25351;&#20196;&#65292;&#24182;&#19988;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MultiModal-GPT&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#19982;&#20154;&#31867;&#36827;&#34892;&#22810;&#36718;&#23545;&#35805;&#12290; MultiModal-GPT&#21487;&#20197;&#36981;&#24490;&#20154;&#31867;&#30340;&#21508;&#31181;&#25351;&#20196;&#65292;&#20363;&#22914;&#29983;&#25104;&#35814;&#32454;&#30340;&#23383;&#24149;&#65292;&#35745;&#31639;&#24863;&#20852;&#36259;&#23545;&#35937;&#30340;&#25968;&#37327;&#20197;&#21450;&#22238;&#31572;&#29992;&#25143;&#30340;&#24120;&#35265;&#38382;&#39064;&#12290; &#25105;&#20204;&#36890;&#36807;OpenFlamingo&#36827;&#34892;&#21442;&#25968;&#26377;&#25928;&#22320;&#24494;&#35843;MultiModal-GPT&#65292;&#24182;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#21449;&#20851;&#27880;&#37096;&#20998;&#21644;&#33258;&#25105;&#20851;&#27880;&#37096;&#20998;&#20013;&#28155;&#21152;&#20102;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;LoRA&#65289;&#12290; &#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#26500;&#24314;&#25351;&#20196;&#27169;&#26495;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65292;&#35753;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290; &#25105;&#20204;&#21457;&#29616;&#23545;&#35805;&#34920;&#29616;&#30340;&#35757;&#32451;&#25968;&#25454;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#24456;&#23569;&#21253;&#21547;&#31616;&#30701;&#22238;&#31572;&#30340;&#25968;&#25454;&#20250;&#20351;&#27169;&#22411;&#23545;&#20219;&#20309;&#25351;&#20196;&#37117;&#20316;&#20986;&#31616;&#30701;&#22238;&#31572;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;MultiModal-GPT&#19982;&#20154;&#31867;&#32842;&#22825;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#21033;&#29992;&#20165;&#35821;&#35328;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#32852;&#21512;&#35757;&#32451;MultiModal-GPT&#12290;&#32852;&#21512;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;MultiModal-GPT&#22312;&#23545;&#35805;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans. MultiModal-GPT can follow various instructions from humans, such as generating a detailed caption, counting the number of interested objects, and answering general questions from users. MultiModal-GPT is parameter-efficiently fine-tuned from OpenFlamingo, with Low-rank Adapter (LoRA) added both in the cross-attention part and the self-attention part of the language model. We first construct instruction templates with vision and language data for multi-modality instruction tuning to make the model understand and follow human instructions. We find the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions. To further enhance the ability to chat with humans of the MultiModal-GPT, we utilize language-only instruction-following data to train the MultiModal-GPT jointly. The joint tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35760;&#24518;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#39044;&#35757;&#32451;&#35760;&#24518;&#30340;&#25351;&#26631;PreCog&#65292;&#24182;&#21457;&#29616;&#39640;&#24230;&#35760;&#24518;&#30340;&#20363;&#23376;&#20998;&#31867;&#25928;&#26524;&#26356;&#22909;&#65292;&#35828;&#26126;&#35760;&#24518;&#23545;BERT&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.04673</link><description>&lt;p&gt;
PreCog&#65306;&#25506;&#31350;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#35760;&#24518;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
PreCog: Exploring the Relation between Memorization and Performance in Pre-trained Language Models. (arXiv:2305.04673v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#35760;&#24518;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#39044;&#35757;&#32451;&#35760;&#24518;&#30340;&#25351;&#26631;PreCog&#65292;&#24182;&#21457;&#29616;&#39640;&#24230;&#35760;&#24518;&#30340;&#20363;&#23376;&#20998;&#31867;&#25928;&#26524;&#26356;&#22909;&#65292;&#35828;&#26126;&#35760;&#24518;&#23545;BERT&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BERT&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24778;&#20154;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#33021;&#22815;&#35760;&#20303;&#19968;&#20123;&#27867;&#21270;&#30340;&#23398;&#20064;&#20363;&#23376;&#12290;&#26412;&#25991;&#26088;&#22312;&#38024;&#23545;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#35760;&#24518;&#19982;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#36827;&#34892;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;PreCog&#8212;&#8212;&#19968;&#31181;&#35780;&#20272;&#39044;&#35757;&#32451;&#35760;&#24518;&#30340;&#25351;&#26631;&#65292;&#24182;&#20998;&#26512;&#20102;&#23427;&#19982;BERT&#24615;&#33021;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#39640;&#24230;&#35760;&#24518;&#30340;&#20363;&#23376;&#20998;&#31867;&#25928;&#26524;&#26356;&#22909;&#65292;&#35828;&#26126;&#35760;&#24518;&#23545;BERT&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models such as BERT are impressive machines with the ability to memorize, possibly generalized learning examples. We present here a small, focused contribution to the analysis of the interplay between memorization and performance of BERT in downstream tasks. We propose PreCog, a measure for evaluating memorization from pre-training, and we analyze its correlation with the BERT's performance. Our experiments show that highly memorized examples are better classified, suggesting memorization is an essential key to success for BERT.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.04476</link><description>&lt;p&gt;
AlignSTS&#65306;&#36890;&#36807;&#36328;&#27169;&#24577;&#23545;&#40784;&#23454;&#29616;&#35821;&#38899;&#36716;&#21809;
&lt;/p&gt;
&lt;p&gt;
AlignSTS: Speech-to-Singing Conversion via Cross-Modal Alignment. (arXiv:2305.04476v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#24182;&#20351;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#35813;&#27169;&#22411;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#38899;&#36716;&#21809; (STS) &#20219;&#21153;&#26088;&#22312;&#22312;&#38754;&#23545;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26102;&#65292;&#29983;&#25104;&#19982;&#35821;&#38899;&#24405;&#38899;&#30456;&#23545;&#24212;&#30340;&#21809;&#27468;&#26679;&#26412;&#65306;&#22312;&#27809;&#26377;&#25991;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#30446;&#26631;&#65288;&#21809;&#27468;&#65289;&#38899;&#39640;&#36718;&#24275;&#21644;&#28304;&#65288;&#35821;&#38899;&#65289;&#20869;&#23481;&#20043;&#38388;&#30340;&#23545;&#40784;&#38590;&#20197;&#23398;&#20064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#26174;&#24335;&#36328;&#27169;&#24577;&#23545;&#40784;&#30340;STS&#27169;&#22411;AlignSTS&#65292;&#23558;&#35821;&#38899;&#21464;&#21270;&#65288;&#22914;&#38899;&#39640;&#21644;&#20869;&#23481;&#65289;&#35270;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#21809;&#20986;&#26059;&#24459;&#30340;&#27468;&#35789;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;AlignSTS: 1&#65289;&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#33410;&#22863;&#36866;&#37197;&#22120;&#26469;&#39044;&#27979;&#30446;&#26631;&#33410;&#22863;&#34920;&#31034;&#65292;&#20197;&#24357;&#21512;&#20869;&#23481;&#21644;&#38899;&#39640;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#65292;&#20854;&#20013;&#33410;&#22863;&#34920;&#31034;&#20197;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#35745;&#31639;&#65292;&#24182;&#37327;&#21270;&#20026;&#31163;&#25955;&#31354;&#38388;&#65307;2&#65289;&#20351;&#29992;&#39044;&#27979;&#30340;&#33410;&#22863;&#34920;&#31034;&#22522;&#20110;&#20132;&#21449;&#27880;&#24847;&#21147;&#37325;&#26032;&#23545;&#40784;&#20869;&#23481;&#65292;&#24182;&#36827;&#34892;&#36328;&#27169;&#24577;&#34701;&#21512;&#37325;&#26032;&#21512;&#25104;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;AlignSTS&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The speech-to-singing (STS) voice conversion task aims to generate singing samples corresponding to speech recordings while facing a major challenge: the alignment between the target (singing) pitch contour and the source (speech) content is difficult to learn in a text-free situation. This paper proposes AlignSTS, an STS model based on explicit cross-modal alignment, which views speech variance such as pitch and content as different modalities. Inspired by the mechanism of how humans will sing the lyrics to the melody, AlignSTS: 1) adopts a novel rhythm adaptor to predict the target rhythm representation to bridge the modality gap between content and pitch, where the rhythm representation is computed in a simple yet effective way and is quantized into a discrete space; and 2) uses the predicted rhythm representation to re-align the content based on cross-attention and conducts a cross-modal fusion for re-synthesize. Extensive experiments show that AlignSTS achieves superior performanc
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;UIT-OpenViIC&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22312;&#36234;&#21335;&#20302;&#36164;&#28304;&#30740;&#31350;&#31038;&#21306;&#20013;&#23384;&#22312;&#30340;&#22256;&#22659;&#32780;&#24341;&#20837;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36234;&#21335;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#24182;&#20165;&#30001;&#36234;&#21335;&#20154;&#26681;&#25454;&#20005;&#26684;&#30340;&#35268;&#21017;&#21644;&#30417;&#30563;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04166</link><description>&lt;p&gt;
UIT-OpenViIC&#65306;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#30340;&#26032;&#22411;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
UIT-OpenViIC: A Novel Benchmark for Evaluating Image Captioning in Vietnamese. (arXiv:2305.04166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;UIT-OpenViIC&#65292;&#36825;&#26159;&#20026;&#20102;&#35299;&#20915;&#30446;&#21069;&#22312;&#36234;&#21335;&#20302;&#36164;&#28304;&#30740;&#31350;&#31038;&#21306;&#20013;&#23384;&#22312;&#30340;&#22256;&#22659;&#32780;&#24341;&#20837;&#30340;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#36234;&#21335;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#24182;&#20165;&#30001;&#36234;&#21335;&#20154;&#26681;&#25454;&#20005;&#26684;&#30340;&#35268;&#21017;&#21644;&#30417;&#30563;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#12290;&#25968;&#25454;&#38598;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#26159;&#19968;&#31181;&#20173;&#28982;&#21560;&#24341;&#20840;&#29699;&#30740;&#31350;&#31038;&#21306;&#20852;&#36259;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#12290;&#34429;&#28982;MS-COCO&#23383;&#24149;&#22522;&#20934;&#26159;&#22312;2015&#24180;&#21457;&#24067;&#30340;&#65292;&#20294;&#23427;&#20173;&#28982;&#34987;&#24191;&#27867;&#20351;&#29992;&#26469;&#35780;&#20272;&#39640;&#32423;&#23383;&#24149;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20165;&#22312;MS-COCO&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#26368;&#26032;&#23383;&#24149;&#27169;&#22411;&#20165;&#22312;&#33521;&#35821;&#35821;&#35328;&#27169;&#24335;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65307;&#23427;&#20204;&#22312;&#36234;&#21335;&#25429;&#25417;&#30340;&#19978;&#19979;&#25991;&#25110;&#20351;&#29992;&#36234;&#21335;&#35821;&#27969;&#30021;&#23383;&#24149;&#22270;&#20687;&#26041;&#38754;&#30340;&#34920;&#29616;&#24182;&#19981;&#22909;&#12290;&#20026;&#20102;&#36129;&#29486;&#20110;&#20687;&#36234;&#21335;&#36825;&#26679;&#30340;&#20302;&#36164;&#28304;&#30740;&#31350;&#31038;&#21306;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36234;&#21335;&#35821;&#22270;&#20687;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;UIT-OpenViIC&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#36234;&#21335;&#30340;&#22797;&#26434;&#22330;&#26223;&#65292;&#24182;&#30001;&#36234;&#21335;&#20154;&#26681;&#25454;&#20005;&#26684;&#30340;&#35268;&#21017;&#21644;&#30417;&#30563;&#36827;&#34892;&#25163;&#21160;&#27880;&#37322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#35814;&#32454;&#22320;&#20171;&#32461;&#20102;&#25968;&#25454;&#38598;&#30340;&#21019;&#24314;&#36807;&#31243;&#12290;&#20174;&#21021;&#27493;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#23545;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#32763;&#35793;&#27169;&#22411;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Captioning is one of the vision-language tasks that still interest the research community worldwide in the 2020s. MS-COCO Caption benchmark is commonly used to evaluate the performance of advanced captioning models, although it was published in 2015. Recent captioning models trained on the MS-COCO Caption dataset only have good performance in language patterns of English; they do not have such good performance in contexts captured in Vietnam or fluently caption images using Vietnamese. To contribute to the low-resources research community as in Vietnam, we introduce a novel image captioning dataset in Vietnamese, the Open-domain Vietnamese Image Captioning dataset (UIT-OpenViIC). The introduced dataset includes complex scenes captured in Vietnam and manually annotated by Vietnamese under strict rules and supervision. In this paper, we present in more detail the dataset creation process. From preliminary analysis, we show that our dataset is challenging to recent state-of-the-art 
&lt;/p&gt;</description></item><item><title>DAMO-NLP&#22242;&#38431;&#30340;U-RaNER&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#24102;&#26377;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#23384;&#22312;&#30340;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#21644;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03688</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;2&#20013;&#30340;DAMO-NLP: &#19968;&#31181;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32479;&#19968;&#26816;&#32034;&#22686;&#24378;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition. (arXiv:2305.03688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03688
&lt;/p&gt;
&lt;p&gt;
DAMO-NLP&#22242;&#38431;&#30340;U-RaNER&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#21152;&#20837;&#24102;&#26377;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#35299;&#20915;&#20102;&#20854;&#20182;&#31995;&#32479;&#23384;&#22312;&#30340;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#21644;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MultiCoNER 2&#20849;&#20139;&#20219;&#21153;&#26088;&#22312;&#35299;&#20915;&#22810;&#35821;&#35328;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32454;&#31890;&#24230;&#21644;&#22024;&#26434;&#24773;&#20917;&#65292;&#24182;&#32487;&#25215;&#20102;MultiCoNER 1&#20219;&#21153;&#30340;&#35821;&#20041;&#27495;&#20041;&#21644;&#20302;&#19978;&#19979;&#25991;&#29615;&#22659;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;MultiCoNER 1&#20013;&#30340;&#21069;&#20960;&#20010;&#39030;&#23574;&#31995;&#32479;&#35201;&#20040;&#32435;&#20837;&#30693;&#35782;&#24211;&#25110;&#19987;&#26377;&#21517;&#35789;&#34920;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#30693;&#35782;&#19981;&#36275;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#26377;&#38480;&#20197;&#21450;&#21333;&#19968;&#26816;&#32034;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30340;DAMO-NLP&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#35821;&#35328;&#30340;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#32479;&#19968;&#26816;&#32034;&#22686;&#24378;&#31995;&#32479;&#65288;U-RaNER&#65289;&#12290;&#25105;&#20204;&#23545;&#19978;&#36848;&#20960;&#20010;&#39030;&#23574;&#31995;&#32479;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#29942;&#39048;&#22312;&#20110;&#30693;&#35782;&#19981;&#36275;&#65292;&#32780;&#19988;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#20351;&#24471;&#26816;&#32034;&#30693;&#35782;&#23545;&#27169;&#22411;&#19981;&#21487;&#35265;&#12290;&#20026;&#20102;&#22686;&#24378;&#26816;&#32034;&#19978;&#19979;&#25991;&#65292;&#25105;&#20204;&#21152;&#20837;&#20102;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;Wikidata&#30693;&#35782;&#24211;&#65292;&#24182;&#37319;&#29992;infusion&#26041;&#27861;&#26469;&#25299;&#23485;&#19978;&#19979;&#25991;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The MultiCoNER \RNum{2} shared task aims to tackle multilingual named entity recognition (NER) in fine-grained and noisy scenarios, and it inherits the semantic ambiguity and low-context setting of the MultiCoNER \RNum{1} task. To cope with these problems, the previous top systems in the MultiCoNER \RNum{1} either incorporate the knowledge bases or gazetteers. However, they still suffer from insufficient knowledge, limited context length, single retrieval strategy. In this paper, our team \textbf{DAMO-NLP} proposes a unified retrieval-augmented system (U-RaNER) for fine-grained multilingual NER. We perform error analysis on the previous top systems and reveal that their performance bottleneck lies in insufficient knowledge. Also, we discover that the limited context length causes the retrieval knowledge to be invisible to the model. To enhance the retrieval context, we incorporate the entity-centric Wikidata knowledge base, while utilizing the infusion approach to broaden the contextua
&lt;/p&gt;</description></item><item><title>Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.03509</link><description>&lt;p&gt;
Diffusion Explainer&#65306;&#29992;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#31283;&#23450;&#25193;&#25955;&#30340;&#21487;&#35270;&#21270;&#35299;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion. (arXiv:2305.03509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03509
&lt;/p&gt;
&lt;p&gt;
Diffusion Explainer&#26159;&#31532;&#19968;&#20010;&#21487;&#20132;&#20114;&#30340;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#21019;&#36896;&#36924;&#30495;&#30340;&#22270;&#20687;&#32780;&#33719;&#24471;&#20102;&#20840;&#29699;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22797;&#26434;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#25805;&#20316;&#24448;&#24448;&#20351;&#24471;&#38750;&#19987;&#19994;&#20154;&#21592;&#38590;&#20197;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; Diffusion Explainer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20132;&#20114;&#24335;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#29992;&#20110;&#35299;&#37322;&#31283;&#23450;&#25193;&#25955;&#22914;&#20309;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#22270;&#20687;&#12290;Diffusion Explainer&#32039;&#23494;&#22320;&#23558;&#31283;&#23450;&#25193;&#25955;&#30340;&#22797;&#26434;&#32452;&#20214;&#30340;&#35270;&#35273;&#27010;&#36848;&#19982;&#20854;&#28508;&#22312;&#25805;&#20316;&#30340;&#35814;&#32454;&#35828;&#26126;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21160;&#30011;&#21644;&#20132;&#20114;&#20803;&#32032;&#20351;&#29992;&#25143;&#21487;&#20197;&#27969;&#30021;&#22320;&#22312;&#22810;&#20010;&#25277;&#35937;&#32423;&#21035;&#20043;&#38388;&#36807;&#28193;&#12290;&#36890;&#36807;&#27604;&#36739;&#30001;&#20004;&#20010;&#30456;&#20851;&#25991;&#26412;&#25552;&#31034;&#24341;&#23548;&#30340;&#22270;&#20687;&#34920;&#31034;&#30340;&#28436;&#21464;&#26469;&#25351;&#23548;&#31934;&#32454;&#26102;&#38388;&#27493;&#38271;&#65292;&#29992;&#25143;&#21487;&#20197;&#21457;&#29616;&#25552;&#31034;&#23545;&#22270;&#20687;&#29983;&#25104;&#30340;&#24433;&#21709;&#12290;Diffusion Explainer&#22312;&#29992;&#25143;&#30340;Web&#27983;&#35272;&#22120;&#20013;&#26412;&#22320;&#36816;&#34892;&#65292;&#26080;&#38656;&#23433;&#35013;&#25110;&#19987;&#38376;&#30340;&#30828;&#20214;&#65292;&#25193;&#22823;&#20102;&#20844;&#20247;&#23545;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#25945;&#32946;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.03453</link><description>&lt;p&gt;
T-SciQ: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#25480;&#22810;&#27169;&#24577;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#22312;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
T-SciQ: Teaching Multimodal Chain-of-Thought Reasoning via Large Language Model Signals for Science Question Answering. (arXiv:2305.03453v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20449;&#21495;&#25945;&#32946;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#36890;&#36807;&#29983;&#25104;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#20154;&#24037;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36817;&#26399;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#20182;&#20204;&#36824;&#23637;&#31034;&#20102;&#25191;&#34892;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22797;&#26434;&#22810;&#27169;&#24577;&#22330;&#26223;&#19979;&#30340;&#38142;&#24335;&#24605;&#32500;&#25512;&#29702;&#65292;&#20363;&#22914;&#36890;&#36807;&#29992;&#39640;&#36136;&#37327;&#20154;&#24037;&#27880;&#37322;&#30340;&#38142;&#24335;&#24605;&#36335;&#26469;&#35843;&#25972;&#22810;&#27169;&#22411;&#27169;&#22411;&#36827;&#34892;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#39640;&#36136;&#37327;COT&#21512;&#29702;&#21270;&#36890;&#24120;&#26159;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#28041;&#21450;&#20887;&#20313;&#20449;&#24687;&#25110;&#20002;&#22833;&#37325;&#35201;&#20449;&#24687;&#65292;&#27880;&#37322;&#21512;&#29702;&#21270;&#36890;&#24120;&#19981;&#22826;&#20934;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;T-SciQ&#65292;&#26088;&#22312;&#20351;&#29992;LLM&#20449;&#21495;&#25945;&#25480;&#31185;&#23398;&#38382;&#39064;&#22238;&#31572;&#12290;T-SciQ&#26041;&#27861;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;CoT&#21512;&#29702;&#21270;&#20449;&#21495;&#65292;&#24182;&#20808;&#36827;&#22320;&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#20197;&#22312;&#22797;&#26434;&#27169;&#24577;&#20013;&#25191;&#34892;CoT&#24605;&#32500;&#25512;&#29702;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26174;&#30528;&#25552;&#39640;&#27169;&#22411;&#27867;&#21270;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#26174;&#30528;&#20943;&#23569;&#20102;&#23545;&#20154;&#24037;&#27880;&#37322;&#21512;&#29702;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. They have also shown the ability to perform chain-of-thought (CoT) reasoning to solve complex problems. Recent studies have explored CoT reasoning in complex multimodal scenarios, such as the science question answering task, by fine-tuning multimodal models with high-quality human-annotated CoT rationales. However, collecting high-quality COT rationales is usually time-consuming and costly. Besides, the annotated rationales are hardly accurate due to the redundant information involved or the essential information missed. To address these issues, we propose a novel method termed \emph{T-SciQ} that aims at teaching science question answering with LLM signals. The T-SciQ approach generates high-quality CoT rationales as teaching signals and is advanced to train much smaller models to perform CoT reasoning in complex modalities. Additionally, we introduce a no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#65292;&#24037;&#19994;&#30028;&#30340;&#23384;&#22312;&#19982;&#24433;&#21709;&#21576;&#29616;&#24613;&#21095;&#22686;&#38271;&#65292;&#19968;&#20123;&#20844;&#21496;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#65292;&#24182;&#21521;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#36164;&#37329;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.02797</link><description>&lt;p&gt;
&#25151;&#38388;&#37324;&#30340;&#22823;&#35937;&#65306;&#20998;&#26512;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#23384;&#22312;
&lt;/p&gt;
&lt;p&gt;
The Elephant in the Room: Analyzing the Presence of Big Tech in Natural Language Processing Research. (arXiv:2305.02797v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24037;&#19994;&#30028;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#65292;&#24037;&#19994;&#30028;&#30340;&#23384;&#22312;&#19982;&#24433;&#21709;&#21576;&#29616;&#24613;&#21095;&#22686;&#38271;&#65292;&#19968;&#20123;&#20844;&#21496;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#65292;&#24182;&#21521;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#36164;&#37329;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#21830;&#19994;&#26426;&#20250;&#65292;&#24182;&#19988;&#20351;&#24471;NLP&#30740;&#31350;&#23545;&#20135;&#19994;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;NLP&#39046;&#22495;&#30340;&#22823;&#29609;&#23478;&#20043;&#19968;&#65292;&#36830;&#21516;&#25919;&#24220;&#21644;&#22823;&#23398;&#19968;&#36215;&#65292;&#36319;&#36394;&#20135;&#19994;&#23545;&#30740;&#31350;&#30340;&#24433;&#21709;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#33268;&#21147;&#20110;&#37327;&#21270;&#21644;&#34920;&#24449;&#24037;&#19994;&#30028;&#22312;NLP&#31038;&#21306;&#20013;&#30340;&#23384;&#22312;&#12290;&#20351;&#29992;&#20855;&#26377;78,187&#31687;NLP&#20986;&#29256;&#29289;&#21644;701&#20010;NLP&#20316;&#32773;&#31616;&#21382;&#30340;&#20840;&#38754;&#20803;&#25968;&#25454;&#35821;&#26009;&#24211;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#33258;&#19978;&#19990;&#32426;90&#24180;&#20195;&#20197;&#26469;&#35813;&#39046;&#22495;&#20013;&#30340;&#24037;&#19994;&#23384;&#22312;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;NLP&#20316;&#32773;&#20013;&#30340;&#24037;&#19994;&#23384;&#22312;&#22312;&#36807;&#21435;&#20116;&#24180;&#20013;&#24613;&#21095;&#22686;&#38271;&#65288;&#20174;2017&#24180;&#21040;2022&#24180;&#30340;&#22686;&#38271;&#29575;&#20026;180&#65285;&#65289;&#12290;&#19968;&#20123;&#20844;&#21496;&#21344;&#25454;&#20102;&#22823;&#37096;&#20998;&#20986;&#29256;&#29289;&#65292;&#24182;&#36890;&#36807;&#25320;&#27454;&#21644;&#23454;&#20064;&#20026;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#36164;&#37329;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24037;&#19994;&#30028;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#23384;&#22312;&#21644;&#24433;&#21709;&#26159;&#26174;&#33879;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in deep learning methods for natural language processing (NLP) have created new business opportunities and made NLP research critical for industry development. As one of the big players in the field of NLP, together with governments and universities, it is important to track the influence of industry on research. In this study, we seek to quantify and characterize industry presence in the NLP community over time. Using a corpus with comprehensive metadata of 78,187 NLP publications and 701 resumes of NLP publication authors, we explore the industry presence in the field since the early 90s. We find that industry presence among NLP authors has been steady before a steep increase over the past five years (180% growth from 2017 to 2022). A few companies account for most of the publications and provide funding to academic researchers through grants and internships. Our study shows that the presence and impact of the industry on natural language processing research are signi
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;</title><link>http://arxiv.org/abs/2305.02750</link><description>&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#32508;&#36848;&#65306;&#38382;&#39064;&#12289;&#26041;&#27861;&#21644;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects. (arXiv:2305.02750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#65292;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#19982;&#24191;&#27867;&#30340;&#29616;&#23454;&#19990;&#30028;&#23545;&#35805;&#24212;&#29992;&#30456;&#20851;&#65292;&#20351;&#23545;&#35805;&#20195;&#29702;&#33021;&#22815;&#24341;&#23548;&#23545;&#35805;&#26041;&#21521;&#65292;&#20197;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#25110;&#28385;&#36275;&#31995;&#32479;&#26041;&#38754;&#30340;&#29305;&#23450;&#30446;&#26631;&#12290;&#23427;&#36890;&#36807;&#20808;&#36827;&#25216;&#26415;&#36171;&#33021;&#20197;&#36827;&#23637;&#21040;&#38656;&#35201;&#25112;&#30053;&#24615;&#21644;&#28608;&#21169;&#24615;&#20132;&#20114;&#30340;&#26356;&#22797;&#26434;&#20219;&#21153;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#19981;&#21516;&#31867;&#22411;&#23545;&#35805;&#20013;&#23545;&#35805;&#20195;&#29702;&#20027;&#21160;&#24615;&#30340;&#31361;&#20986;&#38382;&#39064;&#21644;&#20808;&#36827;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#31526;&#21512;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#20294;&#38656;&#35201;&#26410;&#26469;&#26356;&#22823;&#30740;&#31350;&#37325;&#28857;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#20027;&#21160;&#23545;&#35805;&#31995;&#32479;&#30340;&#31532;&#19968;&#31687;&#32508;&#36848;&#21487;&#20197;&#20026;&#31038;&#21306;&#25552;&#20379;&#24555;&#36895;&#35775;&#38382;&#21644;&#25972;&#20307;&#22270;&#29255;&#65292;&#28608;&#21457;&#26356;&#22810;&#30340;&#20250;&#35805; AI &#36827;&#23637;&#21040;&#19979;&#19968;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.01528</link><description>&lt;p&gt;
FIREBALL&#65306;&#19968;&#20221;&#21253;&#21547;&#32467;&#26500;&#21270;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIREBALL: A Dataset of Dungeons and Dragons Actual-Play with Structured Game State Information. (arXiv:2305.01528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;Dungeons &amp; Dragons&#23454;&#38469;&#28216;&#25103;&#25968;&#25454;&#38598;FIREBALL&#65292;&#23427;&#21487;&#20197;&#25913;&#21892;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;LLMs&#21487;&#20197;&#20351;&#29992;FIREBALL&#20013;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26469;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#28216;&#25103;&#22238;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons&#65288;D&#65286;D&#65289;&#26159;&#19968;&#27454;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65292;&#20854;&#29609;&#23478;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#21644;&#38544;&#34255;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25317;&#26377;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#28216;&#25103;&#22238;&#21512;&#27604;&#20165;&#20351;&#29992;&#23545;&#35805;&#21382;&#21490;&#30340;LLMs&#26356;&#20855;&#39640;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20351;&#29992;&#30340;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#26159;&#21551;&#21457;&#24335;&#21019;&#24314;&#30340;&#65292;&#24182;&#19981;&#26159;&#30495;&#27491;&#30340;&#40644;&#37329;&#26631;&#20934;&#28216;&#25103;&#29366;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;FIREBALL&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#30495;&#23454;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;Discord&#30340;&#36817;25,000&#20010;&#30495;&#23454;D&#65286;D&#28216;&#25103;&#20250;&#35805;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#20351;&#29992;Avrae&#26426;&#22120;&#20154;&#30340;&#29609;&#23478;&#30340;&#28216;&#25103;&#20250;&#35805;&#65292;&#35813;&#26426;&#22120;&#20154;&#26159;&#20026;&#20102;&#24110;&#21161;&#20154;&#20204;&#22312;&#32447;&#29609;D&#65286;D&#32780;&#24320;&#21457;&#30340;&#65292;&#24182;&#25429;&#33719;&#20102;&#35821;&#35328;&#12289;&#28216;&#25103;&#21629;&#20196;&#21644;&#22522;&#30784;&#28216;&#25103;&#29366;&#24577;&#20449;&#24687;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;Avrae&#29366;&#24577;&#20449;&#24687;&#65292;FIREBALL&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#65292;&#20174;&#32780;&#25552;&#39640;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#30340;&#36136;&#37327;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#29983;&#25104;&#8230;
&lt;/p&gt;
&lt;p&gt;
Dungeons &amp; Dragons (D&amp;D) is a tabletop roleplaying game with complex natural language interactions between players and hidden state information. Recent work has shown that large language models (LLMs) that have access to state information can generate higher quality game turns than LLMs that use dialog history alone. However, previous work used game state information that was heuristically created and was not a true gold standard game state. We present FIREBALL, a large dataset containing nearly 25,000 unique sessions from real D\&amp;D gameplay on Discord with true game state info. We recorded game play sessions of players who used the Avrae bot, which was developed to aid people in playing D&amp;D online, capturing language, game commands and underlying game state information. We demonstrate that FIREBALL can improve natural language generation (NLG) by using Avrae state information, improving both automated metrics and human judgments of quality. Additionally, we show that LLMs can generate
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.01219</link><description>&lt;p&gt;
&#35302;&#21457;&#35789;&#20316;&#20026;&#21518;&#38376;&#25915;&#20987;&#30340;&#35302;&#21457;&#22120;&#65306;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;
&lt;/p&gt;
&lt;p&gt;
Prompt as Triggers for Backdoor Attack: Examining the Vulnerability in Language Models. (arXiv:2305.01219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#33539;&#20363;&#24357;&#21512;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#20960;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#24212;&#29992;&#24191;&#27867;&#65292;&#20294;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#12290;&#25991;&#26412;&#21518;&#38376;&#25915;&#20987;&#26088;&#22312;&#36890;&#36807;&#27880;&#20837;&#35302;&#21457;&#22120;&#24182;&#20462;&#25913;&#26631;&#31614;&#26469;&#22312;&#27169;&#22411;&#20013;&#24341;&#20837;&#26377;&#38024;&#23545;&#24615;&#30340;&#28431;&#27934;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35302;&#21457;&#22120;&#30340;&#23384;&#22312;&#21644;&#27602;&#30244;&#25968;&#25454;&#26631;&#27880;&#19981;&#27491;&#30830;&#31561;&#32570;&#38519;&#65292;&#36825;&#31181;&#25915;&#20987;&#23384;&#22312;&#24322;&#24120;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#8220;ProAttack&#8221;&#26041;&#27861;&#65292;&#22522;&#20110;&#25552;&#31034;&#26469;&#25191;&#34892;&#24178;&#20928;&#26631;&#31614;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20351;&#29992;&#30340;&#26159;&#25552;&#31034;&#26412;&#36523;&#20316;&#20026;&#35302;&#21457;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#22806;&#37096;&#35302;&#21457;&#22120;&#65292;&#24182;&#30830;&#20445;&#27602;&#30244;&#25968;&#25454;&#30340;&#26631;&#27880;&#27491;&#30830;&#65292;&#25552;&#39640;&#20102;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#12290;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#36164;&#28304;&#21644;&#23569;&#26679;&#26412;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;ProAttack&#26041;&#27861;&#22312;&#20445;&#25345;&#24178;&#20928;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prompt-based learning paradigm, which bridges the gap between pre-training and fine-tuning, achieves state-of-the-art performance on several NLP tasks, particularly in few-shot settings. Despite being widely applied, prompt-based learning is vulnerable to backdoor attacks. Textual backdoor attacks are designed to introduce targeted vulnerabilities into models by poisoning a subset of training samples through trigger injection and label modification. However, they suffer from flaws such as abnormal natural language expressions resulting from the trigger and incorrect labeling of poisoned samples. In this study, we propose {\bf ProAttack}, a novel and efficient method for performing clean-label backdoor attacks based on the prompt, which uses the prompt itself as a trigger. Our method does not require external triggers and ensures correct labeling of poisoned samples, improving the stealthy nature of the backdoor attack. With extensive experiments on rich-resource and few-shot text c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.00217</link><description>&lt;p&gt;
&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#24180;&#65289;&#30340;&#22238;&#24212;&#65306;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#35777;&#26126;&#38750;&#27597;&#35821;&#29992;&#25143;&#27604;&#20363;&#23545;&#35821;&#35328;&#22797;&#26434;&#24230;&#26377;&#24433;&#21709;&#65288;arXiv:2305.00217v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
Still no evidence for an effect of the proportion of non-native speakers on language complexity -- A response to Kauhanen, Einhaus &amp; Walkden (2023). (arXiv:2305.00217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20026;&#23545;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;2023&#65289;&#30340;&#22238;&#24212;&#65292;&#20173;&#28982;&#27809;&#26377;&#35777;&#25454;&#34920;&#26126;&#22823;&#37327;&#30340;L2&#29992;&#25143;&#24433;&#21709;&#35821;&#35328;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#12298;&#35821;&#35328;&#36827;&#21270;&#26434;&#24535;&#12299;&#21457;&#34920;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Kauhanen&#12289;Einhaus&#21644;Walkden&#65288;https://doi.org/10.1093/jole/lzad005&#65292;KEW&#65289;&#25361;&#25112;&#20102;&#25105;&#22312;&#19968;&#31687;&#35770;&#25991;&#20013;&#65288;Koplenig&#65292;Royal Society Open Science&#65292;6&#65292;181274&#65288;2019&#65289;&#65292;https://doi.org/10.1098/rsos.181274&#65289;&#25152;&#21576;&#29616;&#30340;&#32467;&#26524;&#12290;&#22312;&#35813;&#35770;&#25991;&#20013;&#65292;&#25105;&#35797;&#22270;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#32479;&#35745;&#20998;&#26512;&#26469;&#34920;&#26126;&#22823;&#37327;L2&#65288;&#31532;&#20108;&#35821;&#35328;&#65289;&#29992;&#25143;&#20284;&#20046;&#19981;&#20250;&#24433;&#21709;&#35821;&#35328;&#30340;&#65288;&#35821;&#27861;&#25110;&#32479;&#35745;&#65289;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#19987;&#27880;&#20110;Ethnologue&#35780;&#20272;&#35821;&#35328;&#22320;&#20301;&#30340;&#26041;&#24335;&#65306;&#22914;&#26524;&#19968;&#31181;&#35821;&#35328;&#38500;&#20102;&#34987;L1&#65288;&#31532;&#19968;&#35821;&#35328;&#65289;&#20351;&#29992;&#32773;&#20043;&#22806;&#65292;&#36824;&#24212;&#35813;&#26377;&#22823;&#37327;&#30340;L2&#20351;&#29992;&#32773;&#65292;&#37027;&#20040;&#35813;&#35821;&#35328;&#23601;&#34987;&#25551;&#36848;&#20026;&#20256;&#25773;&#24615;&#30340;&#12290;KEW&#25209;&#35780;&#20102;&#23558;&#20256;&#25773;&#24615;&#20316;&#20026;&#35821;&#35328;&#26159;&#21542;&#25317;&#26377;&#22823;&#37327;L2&#20351;&#29992;&#32773;&#65288;&#20108;&#20803;&#65289;&#25351;&#26631;&#30340;&#20351;&#29992;&#65292;&#20197;&#21450;&#22312;&#30452;&#25509;&#20272;&#35745;L2&#27604;&#20363;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;L2&#29992;&#25143;&#27604;&#20363;&#24402;&#20026;&#38750;&#20256;&#25773;&#24615;&#35821;&#35328;&#30340;&#24819;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper published in the Journal of Language Evolution, Kauhanen, Einhaus &amp; Walkden (https://doi.org/10.1093/jole/lzad005, KEW) challenge the results presented in one of my papers (Koplenig, Royal Society Open Science, 6, 181274 (2019), https://doi.org/10.1098/rsos.181274), in which I tried to show through a series of statistical analyses that large numbers of L2 (second language) speakers do not seem to affect the (grammatical or statistical) complexity of a language. To this end, I focus on the way in which the Ethnologue assesses language status: a language is characterised as vehicular if, in addition to being used by L1 (first language) speakers, it should also have a significant number of L2 users. KEW criticise both the use of vehicularity as a (binary) indicator of whether a language has a significant number of L2 users and the idea of imputing a zero proportion of L2 speakers to non-vehicular languages whenever a direct estimate of that proportion is unavailable. Whi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25104;&#32489;&#26174;&#31034;&#20986;&#23427;&#22312;&#21307;&#23398;&#32771;&#35797;&#19978;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#22806;&#65292;ChatGPT-4 &#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2304.11957</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT-4&#30340;ACR&#25918;&#23556;&#32959;&#30244;&#20869;&#31185;&#65288;TXIT&#65289;&#32771;&#35797;&#21644;Red Journal Gray Zone&#26696;&#20363;&#30340;&#22522;&#20934;&#27979;&#35797;&#65306;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#21644;&#25918;&#23556;&#32959;&#30244;&#27835;&#30103;&#20915;&#31574;&#30340;&#28508;&#21147;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology. (arXiv:2304.11957v2 [physics.med-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#25104;&#32489;&#26174;&#31034;&#20986;&#23427;&#22312;&#21307;&#23398;&#32771;&#35797;&#19978;&#26377;&#24456;&#22823;&#30340;&#20248;&#21183;&#65292;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#21478;&#22806;&#65292;ChatGPT-4 &#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#23398;&#19978;&#30340;&#25945;&#32946;&#21644;&#20915;&#31574;&#26041;&#38754;&#30340;&#28508;&#21147;&#24050;&#32463;&#24471;&#21040;&#35777;&#26126;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#32654;&#22269;&#21307;&#23398;&#35768;&#21487;&#32771;&#35797;&#65288;USMLE&#65289;&#21644;MedQA&#32771;&#35797;&#31561;&#21307;&#23398;&#32771;&#35797;&#20013;&#21462;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#32489;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#19987;&#19994;&#39046;&#22495;&#30340;&#34920;&#29616;&#65292;&#20351;&#29992;&#20102;&#31532;38&#23626;&#32654;&#22269;&#25918;&#23556;&#23398;&#38498;&#65288;ACR&#65289;&#25918;&#23556;&#32959;&#30244;&#20869;&#31185;&#65288;TXIT&#65289;&#32771;&#35797;&#21644;2022&#24180;&#30340;Red Journal Gray Zone&#26696;&#20363;&#12290;&#22522;&#20110;TXIT&#32771;&#35797;&#65292;ChatGPT-4&#22312;&#25918;&#23556;&#32959;&#30244;&#23398;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;ACR&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#39592;&#39612;&#21644;&#36719;&#32452;&#32455;&#20197;&#21450;&#22919;&#31185;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#22312;&#20020;&#24202;&#36335;&#24452;&#26041;&#38754;&#65292;ChatGPT-4&#22312;2022&#24180;&#30340;Red Journal Gray Zone&#26696;&#20363;&#20013;&#34920;&#29616;&#36739;&#22909;&#65292;&#20855;&#26377;70.65&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;ChatGPT-4&#65289;&#36827;&#34892;&#25918;&#23556;&#32959;&#30244;&#23398;AI&#36741;&#21161;&#21307;&#23398;&#25945;&#32946;&#21644;&#20915;&#31574;&#21046;&#23450;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models in medicine for education and decision making purposes has been demonstrated as they achieve decent scores on medical exams such as the United States Medical Licensing Exam (USMLE) and the MedQA exam. In this work, we evaluate the performance of ChatGPT-4 in the specialized field of radiation oncology using the 38th American College of Radiology (ACR) radiation oncology in-training (TXIT) exam and the 2022 red journal gray zone cases. For the TXIT exam, ChatGPT-3.5 and ChatGPT-4 have achieved the scores of 63.65% and 74.57%, respectively, highlighting the advantage of the latest ChatGPT-4 model. Based on the TXIT exam, ChatGPT-4's strong and weak areas in radiation oncology are identified to some extent. Specifically, ChatGPT-4 demonstrates good knowledge of statistics, CNS &amp; eye, pediatrics, biology, and physics but has limitations in bone &amp; soft tissue and gynecology, as per the ACR knowledge domain. Regarding clinical care paths, ChatGPT-4 perf
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32467;&#21512;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#38024;&#23545;&#25968;&#25454;&#32570;&#20047;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#22312;&#36890;&#29992;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.08891</link><description>&lt;p&gt;
&#20026;&#26426;&#22120;&#32763;&#35793;&#36136;&#37327;&#35780;&#20272;&#37327;&#36523;&#23450;&#21046;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tailoring Domain Adaptation for Machine Translation Quality Estimation. (arXiv:2304.08891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32467;&#21512;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#65292;&#38024;&#23545;&#25968;&#25454;&#32570;&#20047;&#21644;&#39046;&#22495;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#22312;&#36890;&#29992;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36136;&#37327;&#35780;&#20272;&#23545;&#32763;&#35793;&#27969;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#20854;&#26377;&#25928;&#24615;&#21462;&#20915;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#21644;&#36136;&#37327;&#12290;&#23545;&#20110;&#29305;&#23450;&#30340;&#36136;&#37327;&#35780;&#20272;&#32780;&#35328;&#65292;&#30001;&#20110;&#26631;&#35760;&#36825;&#26679;&#30340;&#25968;&#25454;&#30340;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#39640;&#26114;&#65292;&#22240;&#27492;&#39640;&#36136;&#37327;&#30340;&#26631;&#35760;&#25968;&#25454;&#32463;&#24120;&#32570;&#20047;&#12290;&#38500;&#20102;&#25968;&#25454;&#32570;&#20047;&#26041;&#38754;&#30340;&#25361;&#25112;&#22806;&#65292;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#36824;&#24212;&#20855;&#26377;&#27867;&#21270;&#24615;&#65292;&#21363;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#65292;&#21253;&#25324;&#36890;&#29992;&#39046;&#22495;&#21644;&#29305;&#23450;&#39046;&#22495;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#23558;&#39046;&#22495;&#33258;&#36866;&#24212;&#21644;&#25968;&#25454;&#22686;&#24378;&#36827;&#34892;&#20102;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#36136;&#37327;&#35780;&#20272;&#31995;&#32479;&#12290;&#26041;&#27861;&#26159;&#20808;&#35757;&#32451;&#19968;&#20010;&#36890;&#29992;&#36136;&#37327;&#35780;&#20272;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#20445;&#30041;&#36890;&#29992;&#30693;&#35782;&#30340;&#21516;&#26102;&#23545;&#29305;&#23450;&#39046;&#22495;&#36827;&#34892;&#24494;&#35843;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#35821;&#35328;&#23545;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#30340;&#36328;&#35821;&#35328;&#25512;&#26029;&#65292;&#30456;&#27604;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#22312;&#38646;-shot&#23398;&#20064;&#26041;&#26696;&#20013;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While quality estimation (QE) can play an important role in the translation process, its effectiveness relies on the availability and quality of training data. For QE in particular, high-quality labeled data is often lacking due to the high-cost and effort associated with labeling such data. Aside from the data scarcity challenge, QE models should also be generalizable, i.e., they should be able to handle data from different domains, both generic and specific. To alleviate these two main issues -- data scarcity and domain mismatch -- this paper combines domain adaptation and data augmentation within a robust QE system. Our method is to first train a generic QE model and then fine-tune it on a specific domain while retaining generic knowledge. Our results show a significant improvement for all the language pairs investigated, better cross-lingual inference, and a superior performance in zero-shot learning scenarios as compared to state-of-the-art baselines.
&lt;/p&gt;</description></item><item><title>ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.02426</link><description>&lt;p&gt;
ParroT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32842;&#22825;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ParroT: Translating During Chat Using Large Language Models. (arXiv:2304.02426v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02426
&lt;/p&gt;
&lt;p&gt;
ParroT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24320;&#28304;LLM&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#30340;&#32842;&#22825;&#32763;&#35793;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914; ChatGPT &#21644; GPT-4 &#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#32842;&#22825;&#36807;&#31243;&#20013;&#23436;&#25104;&#21508;&#31181;&#26426;&#22120;&#32763;&#35793;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21482;&#33021;&#36890;&#36807;&#21463;&#38480;&#30340;API&#35775;&#38382;&#65292;&#36825;&#20026;&#26032;&#30340;&#30740;&#31350;&#21644;&#39046;&#22495;&#36827;&#23637;&#24102;&#26469;&#20102;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ParroT &#26694;&#26550;&#65292;&#22522;&#20110;&#24320;&#28304;LLM&#65288;&#22914;LLaMA-7b&#65289;&#21644;&#20154;&#24037;&#32534;&#20889;&#30340;&#32763;&#35793;&#35780;&#20272;&#25968;&#25454;&#26469;&#22686;&#24378;&#21644;&#35268;&#33539;&#32842;&#22825;&#32763;&#35793;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ParroT&#23558;&#32763;&#35793;&#25968;&#25454;&#36716;&#21270;&#20026;&#25351;&#20196;&#25191;&#34892;&#30340;&#26679;&#24335;&#65292;&#24182;&#24341;&#20837; "Hint " &#23383;&#27573;&#20197;&#21152;&#20837;&#39069;&#22806;&#35201;&#27714;&#26469;&#35268;&#33539;&#32763;&#35793;&#36807;&#31243;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#25351;&#20196;&#31867;&#22411;&#26469;&#24494;&#35843; ParroT &#27169;&#22411;&#65292;&#21253;&#25324;&#32763;&#35793;&#25351;&#20196;&#12289;&#23545;&#27604;&#25351;&#20196;&#21644;&#35823;&#24046;&#24341;&#23548;&#25351;&#20196;&#12290;&#22312;&#20004;&#20010; Flores &#23376;&#38598;&#21644; WMT22 &#27979;&#35797;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992; ParroT &#21487;&#20197;&#22823;&#24133;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#19988;&#38656;&#35201;&#30456;&#23545;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) like ChatGPT and GPT-4 have exhibited remarkable abilities on a wide range of natural language processing (NLP) tasks, including various machine translation abilities accomplished during chat. However, these models are only accessible through restricted APIs, which creates barriers to new research and advancements in the field. Therefore, we propose the $\mathbf{ParroT}$ framework to enhance and regulate the translation abilities during chat based on open-sourced LLMs (i.e., LLaMA-7b) and human written translation and evaluation data. Specifically, ParroT reformulates translation data into the instruction-following style, and introduces a "Hint" field for incorporating extra requirements to regulate the translation process. Accordingly, we propose three instruction types for finetuning ParroT models, including translation instruction, contrastive instruction, and error-guided instruction. Experiments on two Flores subsets and WMT22 test sets suggest that tr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Team SheffieldVeraAI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26032;&#38395;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#20998;&#31867;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#12290;&#35813;&#22242;&#38431;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2303.09421</link><description>&lt;p&gt;
SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#65306;Team SheffieldVeraAI&#22312;&#26032;&#38395;&#31867;&#22411;&#12289;&#20027;&#39064;&#21644;&#35828;&#26381;&#25216;&#24039;&#20998;&#31867;&#26041;&#38754;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Team SheffieldVeraAI at SemEval-2023 Task 3: Mono and multilingual approaches for news genre, topic and persuasion technique classification. (arXiv:2303.09421v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Team SheffieldVeraAI&#22312;SemEval-2023&#20219;&#21153;3&#20013;&#30340;&#34920;&#29616;&#12290;&#20182;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#26032;&#38395;&#31867;&#22411;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#20998;&#31867;&#30340;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#12290;&#35813;&#22242;&#38431;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#21644;&#36866;&#37197;&#22120;&#65292;&#21462;&#24471;&#20102;&#22312;&#19981;&#21516;&#35821;&#35328;&#19979;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;&#29615;&#22659;&#19979;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;SemEval-2023&#20219;&#21153;3&#65306;&#22312;&#22312;&#32447;&#26032;&#38395;&#20013;&#26816;&#27979;&#31867;&#21035;&#12289;&#26694;&#26550;&#21644;&#35828;&#26381;&#25216;&#24039;&#12290; &#23545;&#20110;&#23376;&#20219;&#21153;1&#65288;&#26032;&#38395;&#31867;&#22411;&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23436;&#20840;&#35757;&#32451;&#21644;&#36866;&#37197;&#22120;mBERT&#27169;&#22411;&#30340;&#38598;&#25104;&#65292;&#20854;&#22312;&#24503;&#35821;&#20013;&#25490;&#21517;&#31532;&#19968;&#65292;&#24182;&#19988;&#20855;&#26377;&#22810;&#35821;&#35328;&#22242;&#38431;&#20013;&#26368;&#39640;&#30340;&#24179;&#22343;&#25490;&#21517;&#12290; &#23545;&#20110;&#23376;&#20219;&#21153;2&#65288;&#26694;&#26550;&#65289;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#30340;&#38598;&#25104;&#65306;&#19968;&#20010;&#21333;&#35821;RoBERTa-MUPPETLARGE&#21644;&#19968;&#20010;XLM-RoBERTaLARGE&#30340;&#38598;&#25104;&#65292;&#20998;&#21035;&#20351;&#29992;&#36866;&#37197;&#22120;&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65292;&#22312;3&#31181;&#35821;&#35328;&#20013;&#33719;&#24471;&#31532;&#19968;&#21517;&#65292;&#24182;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#33719;&#24471;&#26368;&#20339;&#24179;&#22343;&#25490;&#21517;&#12290; &#23545;&#20110;&#23376;&#20219;&#21153;3&#65288;&#35828;&#26381;&#25216;&#24039;&#65289;&#65292;&#25105;&#20204;&#20026;&#33521;&#35821;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#35821;&#35328;RoBERTa-Base&#27169;&#22411;&#21644;&#19968;&#20010;&#36866;&#29992;&#20110;&#21097;&#20313;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;mBERT&#27169;&#22411;&#65292;&#20854;&#22312;&#25152;&#26377;&#35821;&#35328;&#20013;&#22343;&#25490;&#21517;&#21069;10&#65292;&#20854;&#20013;&#33521;&#35821;&#25490;&#21517;&#31532;&#20108;&#12290; &#23545;&#20110;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21333;&#35821;&#21644;&#22810;&#35821;&#35328;&#26041;&#27861;&#65292;&#24182;&#32771;&#34385;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our approach for SemEval-2023 Task 3: Detecting the category, the framing, and the persuasion techniques in online news in a multi-lingual setup. For Subtask 1 (News Genre), we propose an ensemble of fully trained and adapter mBERT models which was ranked joint-first for German, and had the highest mean rank of multi-language teams. For Subtask 2 (Framing), we achieved first place in 3 languages, and the best average rank across all the languages, by using two separate ensembles: a monolingual RoBERTa-MUPPETLARGE and an ensemble of XLM-RoBERTaLARGE with adapters and task adaptive pretraining. For Subtask 3 (Persuasion Techniques), we train a monolingual RoBERTa-Base model for English and a multilingual mBERT model for the remaining languages, which achieved top 10 for all languages, including 2nd for English. For each subtask, we compare monolingual and multilingual approaches, and consider class imbalance techniques.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;BLOOM&#30340;0-shot&#24615;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#22312;&#20960;&#20010;shot&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#29616;&#24471;&#21040;&#26497;&#22823;&#30340;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#35821;&#35328;&#23545;&#20013;&#34920;&#29616;&#38750;&#24120;&#22909;&#12290;</title><link>http://arxiv.org/abs/2303.01911</link><description>&lt;p&gt;
&#23545;&#19968;&#31181;&#22823;&#22411;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;BLOOM&#65289;&#30340;&#32763;&#35793;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#65306;WMT&#12289;Flores-101&#21644;DiaBLa&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Investigating the Translation Performance of a Large Multilingual Language Model: the Case of BLOOM. (arXiv:2303.01911v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01911
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;BLOOM&#30340;0-shot&#24615;&#33021;&#23384;&#22312;&#38382;&#39064;&#65292;&#20294;&#22312;&#20960;&#20010;shot&#30340;&#24773;&#20917;&#19979;&#65292;&#34920;&#29616;&#24471;&#21040;&#26497;&#22823;&#30340;&#25913;&#21892;&#65292;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#35821;&#35328;&#23545;&#20013;&#34920;&#29616;&#38750;&#24120;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;NLP&#31038;&#21306;&#35265;&#35777;&#20102;&#26032;&#30340;&#22823;&#22411;&#24320;&#25918;&#24335;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;BLOOM&#30340;&#21457;&#24067;&#65292;&#35206;&#30422;&#20102;46&#31181;&#35821;&#35328;&#12290;&#26412;&#25991;&#36890;&#36807;&#35780;&#20272;BLOOM&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#65288;WMT&#12289;Flores-101&#21644;DiaBLa&#65289;&#21644;&#35821;&#35328;&#23545;&#65288;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#65289;&#30340;&#26426;&#22120;&#32763;&#35793;&#34920;&#29616;&#65292;&#37325;&#28857;&#20851;&#27880;BLOOM&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;0-shot&#24615;&#33021;&#23384;&#22312;&#36807;&#24230;&#29983;&#25104;&#21644;&#29983;&#25104;&#38169;&#35823;&#35821;&#35328;&#30340;&#38382;&#39064;&#65292;&#20294;&#22312;&#20960;&#20010;shot&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#24471;&#21040;&#20102;&#26497;&#22823;&#30340;&#25913;&#21892;&#65292;&#22312;&#26576;&#20123;&#35821;&#35328;&#23545;&#20013;&#33719;&#24471;&#20102;&#38750;&#24120;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#25552;&#31034;&#35774;&#35745;&#12289;&#27169;&#22411;&#22823;&#23567;&#12289;&#36328;&#35821;&#35328;&#36716;&#31227;&#21644;&#20351;&#29992;&#35805;&#35821;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP community recently saw the release of a new large open-access multilingual language model, BLOOM (BigScience et al., 2022) covering 46 languages. We focus on BLOOM's multilingual ability by evaluating its machine translation performance across several datasets (WMT, Flores-101 and DiaBLa) and language pairs (high- and low-resourced). Our results show that 0-shot performance suffers from overgeneration and generating in the wrong language, but this is greatly improved in the few-shot setting, with very good results for a number of language pairs. We study several aspects including prompt design, model sizes, cross-lingual transfer and the use of discursive context.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.06675</link><description>&lt;p&gt;
&#20248;&#21270;&#31639;&#27861;&#30340;&#31526;&#21495;&#24335;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Symbolic Discovery of Optimization Algorithms. (arXiv:2302.06675v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06675
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#21457;&#29616;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#20182;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;Lion&#65292;&#23427;&#27604;Adam&#26356;&#33410;&#30465;&#20869;&#23384;&#24182;&#19988;&#22312;ImageNet&#19978;&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;2&#65285;&#65292;&#24182;&#19988;&#39044;&#35757;&#32451;&#30340;&#35745;&#31639;&#26102;&#38388;&#20063;&#20943;&#23569;&#20102;&#22810;&#36798;5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#31639;&#27861;&#21457;&#29616;&#35270;&#20026;&#31243;&#24207;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#20110;&#21457;&#29616;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#25928;&#25628;&#32034;&#25216;&#26415;&#26469;&#25506;&#32034;&#26080;&#38480;&#21644;&#31232;&#30095;&#30340;&#31243;&#24207;&#31354;&#38388;&#12290;&#20026;&#20102;&#22635;&#34917;&#20195;&#29702;&#20219;&#21153;&#21644;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#24040;&#22823;&#30340;&#27867;&#21270;&#24046;&#36317;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#31243;&#24207;&#36873;&#25321;&#21644;&#31616;&#21270;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21457;&#29616;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;$ \textbf {Lion} $&#65288;$ \textit {Evo $\textbf {L} $ved S $ \textbf {i} $ gn M $ \textbf {o} $ me $ \textbf {n} $ tum} $&#65289;&#12290;&#23427;&#30340;&#35760;&#24518;&#25928;&#29575;&#27604;Adam&#26356;&#39640;&#65292;&#22240;&#20026;&#23427;&#21482;&#36319;&#36394;&#21160;&#37327;&#12290;&#19982;&#33258;&#36866;&#24212;&#20248;&#21270;&#22120;&#19981;&#21516;&#65292;&#36890;&#36807;&#31526;&#21495;&#36816;&#31639;&#35745;&#31639;&#30340;&#27599;&#20010;&#21442;&#25968;&#30340;&#26356;&#26032;&#20855;&#26377;&#30456;&#21516;&#30340;&#22823;&#23567;&#12290;&#25105;&#20204;&#23558;Lion&#19982;&#24191;&#27867;&#20351;&#29992;&#30340;&#20248;&#21270;&#22120;&#65288;&#20363;&#22914;Adam&#21644;Adafactor&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#20197;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#21508;&#31181;&#27169;&#22411;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#65292;Lion&#23558;&#22312;ImageNet&#19978;ViT&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#26368;&#22810;2&#65285;&#65292;&#24182;&#33410;&#30465;&#20102;&#22810;&#36798;5&#20493;&#30340;&#39044;&#35757;&#32451;&#35745;&#31639;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\textbf{Lion}$ ($\textit{Evo$\textbf{L}$ved S$\textbf{i}$gn M$\textbf{o}$me$\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#29305;&#24449;&#21464;&#25442;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06198</link><description>&lt;p&gt;
&#21306;&#20998;&#24230;&#26657;&#20934;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Distinguishability Calibration to In-Context Learning. (arXiv:2302.06198v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#29305;&#24449;&#21464;&#25442;&#26657;&#20934;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22312;&#36716;&#25442;&#22120;&#20013;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#27169;&#22411;&#33021;&#22815;&#22312;&#23569;&#37327;&#26631;&#27880;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#23427;&#20204;&#36866;&#29992;&#20110;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#26102;&#65292;&#30446;&#26631;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (PLM) &#26469;&#39044;&#27979;&#39044;&#23450;&#20041;&#27169;&#26495;&#20013;&#30340;&#32570;&#22833;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#26550;&#26500;&#26500;&#24314;&#30340; PLM &#20542;&#21521;&#20110;&#29983;&#25104;&#30456;&#20284;&#30340;&#36755;&#20986;&#23884;&#20837;&#65292;&#24456;&#38590;&#21306;&#20998;&#19981;&#21516;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#24403;&#22788;&#29702;&#28041;&#21450;&#35768;&#22810;&#32454;&#31890;&#24230;&#31867;&#21035;&#26631;&#31614;&#30340;&#20998;&#31867;&#20219;&#21153;&#26102;&#65292;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#21152;&#21095;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#29305;&#24449;&#26059;&#36716;&#21644;&#32553;&#25918;&#30340;&#26657;&#20934;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20010;&#20449;&#24687;&#25193;&#25955;&#38382;&#39064;&#65292;&#21363;&#24403;&#19981;&#21516;&#30340;&#20196;&#29260;&#32463;&#36807;&#36716;&#25442;&#22120;&#20013;&#22534;&#21472;&#30340;&#22810;&#20010;&#33258;&#27880;&#24847;&#23618;&#26102;&#65292;&#23427;&#20204;&#20849;&#20139;&#22823;&#37327;&#30456;&#20284;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed increasing interests in prompt-based learning in which models can be trained on only a few annotated instances, making them suitable in low-resource settings. When using prompt-based learning for text classification, the goal is to use a pre-trained language model (PLM) to predict a missing token in a pre-defined template given an input text, which can be mapped to a class label. However, PLMs built on the transformer architecture tend to generate similar output embeddings, making it difficult to discriminate between different class labels. The problem is further exacerbated when dealing with classification tasks involving many fine-grained class labels. In this work, we alleviate this information diffusion issue, i.e., different tokens share a large proportion of similar information after going through stacked multiple self-attention layers in a transformer, by proposing a calibration method built on feature transformations through rotation and scaling to m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#21644;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#30340;&#20316;&#21697;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#29983;&#25104;&#21746;&#23398;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25307;&#21215;&#22823;&#37327;&#21442;&#19982;&#32773;&#26469;&#21306;&#20998;&#30495;&#27491;&#30340;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#23383;&#12290;&#19987;&#23478;&#25104;&#21151;&#29575;&#36798;&#21040;51&#65285;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#39044;&#26399;&#30340;80&#65285;&#65292;&#35813;&#27169;&#22411;&#26377;&#21487;&#33021;&#36229;&#36234;&#20154;&#31867;&#30340;&#24605;&#32500;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.01339</link><description>&lt;p&gt;
&#21019;&#36896;&#19968;&#20010;&#21746;&#23398;&#23478;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Creating a Large Language Model of a Philosopher. (arXiv:2302.01339v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01339
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#21644;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#30340;&#20316;&#21697;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25506;&#32034;&#20102;&#29983;&#25104;&#21746;&#23398;&#25991;&#26412;&#30340;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25307;&#21215;&#22823;&#37327;&#21442;&#19982;&#32773;&#26469;&#21306;&#20998;&#30495;&#27491;&#30340;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#23383;&#12290;&#19987;&#23478;&#25104;&#21151;&#29575;&#36798;&#21040;51&#65285;&#65292;&#20294;&#27809;&#26377;&#36798;&#21040;&#39044;&#26399;&#30340;80&#65285;&#65292;&#35813;&#27169;&#22411;&#26377;&#21487;&#33021;&#36229;&#36234;&#20154;&#31867;&#30340;&#24605;&#32500;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#21542;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#38590;&#20197;&#19982;&#20154;&#31867;&#21746;&#23398;&#23478;&#30340;&#25991;&#26412;&#21306;&#20998;&#30340;&#21746;&#23398;&#25991;&#23383;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21746;&#23398;&#23478;&#20025;&#23612;&#29305;&#30340;&#20316;&#21697;&#20316;&#20026;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#24494;&#35843;OpenAI&#30340;GPT-3&#12290;&#20026;&#20102;&#25506;&#32034;&#20025;&#23612;&#29305;&#27169;&#22411;&#65292;&#25105;&#20204;&#21521;&#30495;&#27491;&#30340;&#20025;&#23612;&#29305;&#25552;&#20986;&#20102;&#21313;&#20010;&#21746;&#23398;&#38382;&#39064;&#65292;&#28982;&#21518;&#21521;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#65292;&#27599;&#20010;&#38382;&#39064;&#25910;&#38598;&#20102;&#22235;&#20010;&#22238;&#31572;&#65292;&#27809;&#26377;&#36827;&#34892;&#31579;&#36873;&#12290;&#25105;&#20204;&#25307;&#21215;&#20102;425&#21517;&#21442;&#19982;&#32773;&#26469;&#21306;&#20998;&#20025;&#23612;&#29305;&#30340;&#31572;&#26696;&#21644;&#22235;&#20010;&#26426;&#22120;&#29983;&#25104;&#30340;&#31572;&#26696;&#12290;&#29087;&#24713;&#20025;&#23612;&#29305;&#20316;&#21697;&#30340;&#19987;&#23478;&#65288;N = 25&#65289;&#30340;&#25104;&#21151;&#29575;&#20026;51&#65285;&#65292;&#39640;&#20110;20&#65285;&#30340;&#26426;&#20250;&#29575;&#65292;&#20294;&#19981;&#21450;&#25105;&#20204;&#39044;&#26399;&#30340;80&#65285;&#30340;&#27491;&#30830;&#29575;&#12290;&#23545;&#20110;&#20854;&#20013;&#30340;&#20004;&#20010;&#38382;&#39064;&#65292;&#35821;&#35328;&#27169;&#22411;&#33267;&#23569;&#29983;&#25104;&#20102;&#19968;&#20010;&#31572;&#26696;&#65292;&#19987;&#23478;&#20204;&#26356;&#39057;&#32321;&#22320;&#36873;&#25321;&#35813;&#31572;&#26696;&#32780;&#38750;&#20025;&#23612;&#29305;&#33258;&#24049;&#30340;&#31572;&#26696;&#12290;&#21746;&#23398;&#21338;&#23458;&#35835;&#32773;&#65288;N = 302&#65289;&#30340;&#34920;&#29616;&#19982;&#19987;&#23478;&#30456;&#20284;&#65292;&#32780;&#26222;&#36890;&#30740;&#31350;&#21442;&#19982;&#32773;&#65288;N = 98&#65289;&#21017;&#36817;&#20284;&#20110;&#38543;&#26426;&#29468;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25913;&#36827;&#23454;&#26102;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.13294</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Adaptive Machine Translation with Large Language Models. (arXiv:2301.13294v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25913;&#36827;&#23454;&#26102;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#26377;&#24076;&#26395;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates how to use in-context learning of large language models to improve real-time adaptive machine translation, and the experimental results show promising effects.
&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#26159;&#39640;&#36136;&#37327;&#32763;&#35793;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#39033;&#30446;&#20013;&#65292;&#36981;&#24490;&#39044;&#20808;&#25209;&#20934;&#30340;&#26415;&#35821;&#24182;&#36866;&#24212;&#26356;&#27491;&#30340;&#32763;&#35793;&#23588;&#20026;&#37325;&#35201;&#12290;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#39046;&#22495;&#36866;&#24212;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#36866;&#24212;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#36259;&#33021;&#21147;&#65292;&#23427;&#20204;&#23398;&#20064;&#22797;&#21046;&#26576;&#20123;&#36755;&#20837;-&#36755;&#20986;&#25991;&#26412;&#29983;&#25104;&#27169;&#24335;&#65292;&#32780;&#26080;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#25512;&#29702;&#26102;&#38388;&#23558;LLM&#25552;&#20379;&#32473;&#30001;&#32763;&#35793;&#23545;&#21015;&#34920;&#32452;&#25104;&#30340;&#25552;&#31034;&#65292;&#23427;&#21487;&#20197;&#27169;&#25311;&#39046;&#22495;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25913;&#36827;&#23454;&#26102;&#33258;&#36866;&#24212;MT&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#22312;&#32763;&#35793;&#26102;&#38388;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20363;&#22914;&#65292;GPT-3.5&#21487;&#20197;&#22312;&#32763;&#35793;&#26032;&#21477;&#23376;&#26102;&#36866;&#24212;&#19968;&#32452;&#39046;&#22495;&#20869;&#30340;&#21477;&#23376;&#23545;&#21644;/&#25110;&#26415;&#35821;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#26426;&#22120;&#32763;&#35793;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency is a key requirement of high-quality translation. It is especially important to adhere to pre-approved terminology and adapt to corrected translations in domain-specific projects. Machine translation (MT) has achieved significant progress in the area of domain adaptation. However, real-time adaptation remains challenging. Large-scale language models (LLMs) have recently shown interesting capabilities of in-context learning, where they learn to replicate certain input-output text generation patterns, without further fine-tuning. By feeding an LLM at inference time with a prompt that consists of a list of translation pairs, it can then simulate the domain and style characteristics. This work aims to investigate how we can utilize in-context learning to improve real-time adaptive MT. Our extensive experiments show promising results at translation time. For example, GPT-3.5 can adapt to a set of in-domain sentence pairs and/or terminology while translating a new sentence. We ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#36335;&#30001;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;BiGS&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;BERT&#39044;&#35757;&#32451;&#20934;&#30830;&#24230;&#30456;&#24403;&#30340;GLUE&#27979;&#35797;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2212.10544</link><description>&lt;p&gt;
&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Pretraining Without Attention. (arXiv:2212.10544v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10544
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#24207;&#21015;&#36335;&#30001;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;BiGS&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;BERT&#39044;&#35757;&#32451;&#20934;&#30830;&#24230;&#30456;&#24403;&#30340;GLUE&#27979;&#35797;&#32467;&#26524;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#65292;Transformer&#27169;&#22411;&#26159;&#39044;&#35757;&#32451;&#20013;&#21462;&#24471;&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;&#34429;&#28982;&#20063;&#26377;&#20854;&#20182;&#26550;&#26500;&#34987;&#29992;&#20110;&#39044;&#35757;&#32451;&#65292;&#20294;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#29575;&#35201;&#20040;&#26174;&#33879;&#19979;&#38477;&#65292;&#35201;&#20040;&#38656;&#35201;&#27880;&#24847;&#21147;&#26426;&#21046;&#25165;&#33021;&#36798;&#21040;&#26631;&#20934;&#27979;&#35797;&#30340;&#22522;&#20934;&#65288;&#22914;GLUE&#65289;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#19981;&#20381;&#36182;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#37319;&#29992;&#26368;&#36817;&#22312;&#22522;&#20110;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#30340;&#24207;&#21015;&#36335;&#30001;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;Bidirectional Gated SSM&#65288;BiGS&#65289;&#32467;&#21512;&#20102;SSM&#23618;&#21644;&#20056;&#24615;&#38376;&#25511;&#26550;&#26500;&#65292;&#36825;&#22312;&#31616;&#21270;&#24207;&#21015;&#24314;&#27169;&#26550;&#26500;&#20013;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#19981;&#32771;&#34385;&#25104;&#23545;&#20132;&#20114;&#30340;&#38745;&#24577;&#23618;&#12290;&#21363;&#20351;&#22914;&#27492;&#65292;BiGS&#33021;&#22815;&#36798;&#21040;&#19982;BERT&#39044;&#35757;&#32451;&#20934;&#30830;&#24230;&#30456;&#24403;&#30340;GLUE&#27979;&#35797;&#32467;&#26524;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#36817;&#20284;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;4096&#20010;&#26631;&#35760;&#30340;&#38271;&#24418;&#24335;&#39044;&#35757;&#32451;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#30340;&#24179;&#22343;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20294;&#19982;BERT&#30456;&#27604;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#20132;&#20114;&#21644;&#21477;&#27861;&#34920;&#31034;&#26041;&#38754;&#20855;&#26377;&#19981;&#21516;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#25152;&#26377;&#27169;&#22411;&#21487;&#22312; https://git &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://git
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10325</link><description>&lt;p&gt;
SeqDiffuSeq: &#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#65292;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#24615;&#36136;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#33258;&#28982;&#35821;&#35328;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#32780;&#19988;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#36739;&#23569;&#12290;&#24207;&#21015;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#36234;&#29983;&#25104;&#24615;&#33021;&#33021;&#21542;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;SeqDiffuSeq&#65292;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#12290;SeqDiffuSeq&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#26469;&#24314;&#27169;&#21435;&#22122;&#20989;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;SeqDiffuSeq&#32467;&#21512;&#20102;&#33258;&#25105;&#35843;&#33410;&#25216;&#26415;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#12290;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#20855;&#26377;&#22343;&#21248;&#21435;&#22122;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GanLM&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#24341;&#20837;&#20102;&#36741;&#21161;&#37492;&#21035;&#22120;&#26469;&#32479;&#19968;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65306;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#21644;&#26367;&#25442;&#20196;&#29260;&#21435;&#22122;&#12290;</title><link>http://arxiv.org/abs/2212.10218</link><description>&lt;p&gt;
GanLM: &#24102;&#36741;&#21161;&#37492;&#21035;&#22120;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
GanLM: Encoder-Decoder Pre-training with an Auxiliary Discriminator. (arXiv:2212.10218v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GanLM&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23427;&#24341;&#20837;&#20102;&#36741;&#21161;&#37492;&#21035;&#22120;&#26469;&#32479;&#19968;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65306;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#21644;&#26367;&#25442;&#20196;&#29260;&#21435;&#22122;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#35821;&#35328;&#29702;&#35299;&#23545;&#29983;&#25104;&#30340;&#22909;&#22788;&#12290;&#21463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#24605;&#24819;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GAN&#39118;&#26684;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#36741;&#21161;&#37492;&#21035;&#22120;&#65292;&#22312;&#21333;&#20010;&#27169;&#22411;&#20013;&#32479;&#19968;&#20102;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21517;&#20026;GanLM&#65292;&#20351;&#29992;&#20004;&#20010;&#39044;&#35757;&#32451;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65306;&#26367;&#25442;&#20196;&#29260;&#26816;&#27979;&#21644;&#26367;&#25442;&#20196;&#29260;&#21435;&#22122;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#32473;&#23450;&#25513;&#30721;&#28304;&#21477;&#23376;&#65292;&#29983;&#25104;&#22120;&#36755;&#20986;&#30446;&#26631;&#20998;&#24067;&#65292;&#37492;&#21035;&#22120;&#39044;&#27979;&#20174;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#30446;&#26631;&#20196;&#29260;&#26159;&#21542;&#19981;&#27491;&#30830;&#12290;&#23558;&#30446;&#26631;&#21477;&#23376;&#26367;&#25442;&#20026;&#38169;&#35823;&#20998;&#31867;&#30340;&#20196;&#29260;&#20197;&#26500;&#36896;&#24102;&#22122;&#22768;&#30340;&#20808;&#21069;&#19978;&#19979;&#25991;&#65292;&#29992;&#20110;&#29983;&#25104;&#40644;&#37329;&#21477;&#23376;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#36825;&#20004;&#20010;&#20219;&#21153;&#36890;&#36807;&#26377;&#36873;&#25321;&#24615;&#22320;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained models have achieved remarkable success in natural language processing (NLP). However, existing pre-training methods underutilize the benefits of language understanding for generation. Inspired by the idea of Generative Adversarial Networks (GANs), we propose a GAN-style model for encoder-decoder pre-training by introducing an auxiliary discriminator, unifying the ability of language understanding and generation in a single model. Our model, named as GanLM, is trained with two pre-training objectives: replaced token detection and replaced token denoising. Specifically, given masked source sentences, the generator outputs the target distribution and the discriminator predicts whether the target sampled tokens from distribution are incorrect. The target sentence is replaced with misclassified tokens to construct noisy previous context, which is used to generate the gold sentence. In general, both tasks improve the ability of language understanding and generation by selectivel
&lt;/p&gt;</description></item><item><title>ReLM&#26159;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#39564;&#35777;&#21644;&#26597;&#35810;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;LLM&#25968;&#25454;&#35760;&#24518;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#35821;&#35328;&#29702;&#35299;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#24191;&#27867;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.15458</link><description>&lt;p&gt;
&#20351;&#29992;ReLM&#39564;&#35777;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Validating Large Language Models with ReLM. (arXiv:2211.15458v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15458
&lt;/p&gt;
&lt;p&gt;
ReLM&#26159;&#19968;&#31181;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#39564;&#35777;&#21644;&#26597;&#35810;LLM&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;LLM&#25968;&#25454;&#35760;&#24518;&#12289;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#35821;&#35328;&#29702;&#35299;&#31561;&#38382;&#39064;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#21644;&#24191;&#27867;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21363;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22240;&#20026;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#30340;&#25991;&#26412;&#32780;&#22791;&#21463;&#25512;&#23815;&#65292;&#20294;&#26159;&#36234;&#26469;&#36234;&#22810;&#20154;&#20851;&#27880;LLM&#21487;&#33021;&#24102;&#26469;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#22914;&#25968;&#25454;&#35760;&#24518;&#12289;&#20559;&#35265;&#21644;&#19981;&#24688;&#24403;&#35821;&#35328;&#20351;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#30340;&#22797;&#26434;&#24615;&#21644;&#29983;&#25104;&#33021;&#21147;&#20351;&#24471;&#39564;&#35777;&#65288;&#21644;&#32416;&#27491;&#65289;&#36825;&#20123;&#38382;&#39064;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ReLM&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#26631;&#20934;&#27491;&#21017;&#34920;&#36798;&#24335;&#39564;&#35777;&#21644;&#26597;&#35810;LLM&#30340;&#31995;&#32479;&#12290;ReLM&#23558;&#24191;&#27867;&#30340;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#24418;&#24335;&#21270;&#24182;&#21551;&#29992;&#65292;&#23558;&#22797;&#26434;&#30340;&#35780;&#20272;&#35268;&#21017;&#31616;&#21270;&#20026;&#31616;&#21333;&#30340;&#27491;&#21017;&#34920;&#36798;&#24335;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25506;&#32034;&#20102;&#20851;&#20110;&#35760;&#24518;&#12289;&#24615;&#21035;&#20559;&#35265;&#12289;&#27602;&#24615;&#21644;&#35821;&#35328;&#29702;&#35299;&#30340;&#26597;&#35810;&#65292;&#26174;&#31034;ReLM&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#29305;&#23450;&#26597;&#35810;&#25216;&#26415;&#36798;&#21040;&#20102;&#39640;&#36798;15&#20493;&#30340;&#31995;&#32479;&#25928;&#29575;&#12289;2.5&#20493;&#30340;&#25968;&#25454;&#25928;&#29575;&#20197;&#21450;&#26356;&#24191;&#27867;&#30340;&#32479;&#35745;&#21644;&#25552;&#31034;&#35843;&#25972;&#35206;&#30422;&#33539;&#22260;&#12290;ReLM&#20026;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;LLM&#39564;&#35777;&#38382;&#39064;&#25552;&#20379;&#20102;&#31454;&#20105;&#24615;&#21644;&#36890;&#29992;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large language models (LLMs) have been touted for their ability to generate natural-sounding text, there are growing concerns around possible negative effects of LLMs such as data memorization, bias, and inappropriate language. Unfortunately, the complexity and generation capacities of LLMs make validating (and correcting) such concerns difficult. In this work, we introduce ReLM, a system for validating and querying LLMs using standard regular expressions. ReLM formalizes and enables a broad range of language model evaluations, reducing complex evaluation rules to simple regular expression queries. Our results exploring queries surrounding memorization, gender bias, toxicity, and language understanding show that ReLM achieves up to 15x higher system efficiency, 2.5x data efficiency, and increased statistical and prompt-tuning coverage compared to state-of-the-art ad-hoc queries. ReLM offers a competitive and general baseline for the increasingly important problem of LLM valida
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20840;&#23616;&#21644;&#26412;&#22320;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#26469;&#25552;&#21319;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.13873</link><description>&lt;p&gt;
&#20840;&#23616;&#21644;&#26412;&#22320;&#20998;&#23618;&#24863;&#30693;&#23545;&#27604;&#26694;&#26550;&#29992;&#20110;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Global and Local Hierarchy-aware Contrastive Framework for Implicit Discourse Relation Recognition. (arXiv:2211.13873v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GOLF&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20840;&#23616;&#21644;&#26412;&#22320;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#26469;&#25552;&#21319;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#26174;&#24335;&#30340;&#36830;&#25509;&#35789;&#65292;&#38544;&#21547;&#31687;&#31456;&#20851;&#31995;&#35782;&#21035;(IDRR)&#20173;&#28982;&#26159;&#31687;&#31456;&#20998;&#26512;&#20013;&#30340;&#38590;&#39064;&#12290;IDRR&#30340;&#20851;&#38190;&#27493;&#39588;&#26159;&#23398;&#20064;&#20004;&#20010;&#35770;&#28857;&#20043;&#38388;&#39640;&#36136;&#37327;&#30340;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36235;&#21521;&#20110;&#23558;&#25972;&#20010;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#20449;&#24687;&#25972;&#21512;&#21040;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#20013;&#36827;&#34892;&#22810;&#32423;&#21035;&#24863;&#30693;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#26410;&#33021;&#20805;&#20998;&#25972;&#21512;&#21253;&#21547;&#25152;&#26377;&#24863;&#30693;&#30340;&#38745;&#24577;&#20998;&#23618;&#32467;&#26500;&#65288;&#23450;&#20041;&#20026;&#20840;&#23616;&#20998;&#23618;&#32467;&#26500;&#65289;&#65292;&#24182;&#24573;&#30053;&#20102;&#19982;&#27599;&#20010;&#23454;&#20363;&#23545;&#24212;&#30340;&#23618;&#27425;&#24863;&#30693;&#26631;&#31614;&#24207;&#21015;&#65288;&#23450;&#20041;&#20026;&#26412;&#22320;&#20998;&#23618;&#32467;&#26500;&#65289;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#20840;&#23616;&#21644;&#26412;&#22320;&#24863;&#30693;&#23618;&#27425;&#32467;&#26500;&#26469;&#23398;&#20064;&#26356;&#22909;&#30340;&#31687;&#31456;&#20851;&#31995;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#21644;&#26412;&#22320;&#20998;&#23618;&#24863;&#30693;&#23545;&#27604;&#26694;&#26550;(GOLF)&#65292;&#20511;&#21161;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#23545;&#27604;&#23398;&#20064;&#26469;&#27169;&#25311;&#20004;&#31181;&#23618;&#27425;&#24863;&#30693;&#12290;&#22312;PDTB 2.0&#21644;PDTB-EDT&#35821;&#26009;&#24211;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;GOLF&#22312;IDRR&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the absence of explicit connectives, implicit discourse relation recognition (IDRR) remains a challenging task in discourse analysis. The critical step for IDRR is to learn high-quality discourse relation representations between two arguments. Recent methods tend to integrate the whole hierarchical information of senses into discourse relation representations for multi-level sense recognition. Nevertheless, they insufficiently incorporate the static hierarchical structure containing all senses (defined as global hierarchy), and ignore the hierarchical sense label sequence corresponding to each instance (defined as local hierarchy). For the purpose of sufficiently exploiting global and local hierarchies of senses to learn better discourse relation representations, we propose a novel GlObal and Local Hierarchy-aware Contrastive Framework (GOLF), to model two kinds of hierarchies with the aid of multi-task learning and contrastive learning. Experimental results on PDTB 2.0 and PDTB
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#36830;&#32493;&#30340;&#25805;&#20316;&#21387;&#32553;&#29575;&#33539;&#22260;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24179;&#28369;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2211.02332</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Once-for-All Sequence Compression for Self-Supervised Speech Models. (arXiv:2211.02332v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25903;&#25345;&#36830;&#32493;&#30340;&#25805;&#20316;&#21387;&#32553;&#29575;&#33539;&#22260;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#29616;&#20986;&#24179;&#28369;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#65292;&#26102;&#38388;&#36724;&#19978;&#30340;&#24207;&#21015;&#38271;&#24230;&#36890;&#24120;&#26159;&#35745;&#31639;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#20026;&#20102;&#38477;&#20302;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#20943;&#23569;&#24207;&#21015;&#38271;&#24230;&#12290;&#28982;&#32780;&#65292;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#23545;&#24207;&#21015;&#21387;&#32553;&#26377;&#19981;&#21516;&#30340;&#23481;&#24525;&#24230;&#65292;&#22240;&#27492;&#29983;&#20135;&#22266;&#23450;&#21387;&#32553;&#29575;&#30340;&#27169;&#22411;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#30340;&#19968;&#27425;&#24615;&#24207;&#21015;&#21387;&#32553;&#26694;&#26550;&#65292;&#25903;&#25345;&#36830;&#32493;&#30340;&#25805;&#20316;&#21387;&#32553;&#29575;&#33539;&#22260;&#12290;&#35813;&#26694;&#26550;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#19982;&#22266;&#23450;&#21387;&#32553;&#29575;&#21464;&#20307;&#30456;&#27604;&#65292;&#34920;&#29616;&#20986;&#24179;&#28369;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#33258;&#36866;&#24212;&#21387;&#32553;&#29575;&#23398;&#20064;&#65292;&#28436;&#31034;&#20102;&#36873;&#25321;&#20219;&#21153;&#29305;&#23450;&#30340;&#20248;&#20808;&#24103;&#26102;&#26399;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#36827;&#34892;&#32593;&#26684;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sequence length along the time axis is often the dominant factor of the computation in speech processing. Works have been proposed to reduce the sequence length for lowering the computational cost in self-supervised speech models. However, different downstream tasks have different tolerance of sequence compressing, so a model that produces a fixed compressing rate may not fit all tasks. In this work, we introduce a once-for-all (OFA) sequence compression framework for self-supervised speech models that supports a continuous range of operating compressing rates. The framework is evaluated on various tasks, showing marginal degradation compared to the fixed compressing rate variants with a smooth performance-efficiency trade-off. We further explore adaptive compressing rate learning, demonstrating the ability to select task-specific preferred frame periods without needing a grid search.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#26469;&#20102;&#35299;&#25919;&#27835;&#27963;&#21160;&#30340;&#29305;&#28857;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2210.10669</link><description>&lt;p&gt;
&#38024;&#23545;Facebook&#19978;&#30340;&#25919;&#27835;&#27963;&#21160;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Learning for Analyzing Political Campaigns on Facebook. (arXiv:2210.10669v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#20197;&#21450;&#20854;&#20351;&#29992;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#26469;&#20102;&#35299;&#25919;&#27835;&#27963;&#21160;&#30340;&#29305;&#28857;&#21644;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#30446;&#21069;&#26159;&#25919;&#27835;&#20449;&#24687;&#20256;&#25773;&#30340;&#20027;&#35201;&#28192;&#36947;&#65292;&#25919;&#27835;&#23478;&#20204;&#33021;&#22815;&#36890;&#36807;&#36825;&#20123;&#24179;&#21488;&#38024;&#23545;&#29305;&#23450;&#20154;&#32676;&#36827;&#34892;&#23459;&#20256;&#65292;&#24182;&#26681;&#25454;&#20182;&#20204;&#30340;&#21453;&#24212;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20351;&#36825;&#31181;&#20132;&#27969;&#36879;&#26126;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20449;&#24687;&#20256;&#25773;&#19982;&#30446;&#26631;&#21463;&#20247;&#32039;&#23494;&#30456;&#36830;&#65292;&#24182;&#32463;&#24120;&#34987;&#22810;&#20010;&#21033;&#30410;&#25912;&#20851;&#26041;&#20849;&#21516;&#20256;&#25773;&#12290;&#26412;&#25991;&#26088;&#22312;&#31532;&#19968;&#27493;&#20102;&#35299;&#36825;&#20123;&#39640;&#24230;&#20998;&#25955;&#30340;&#25919;&#27835;&#27963;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#30417;&#30563;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;Facebook&#19978;&#25919;&#27835;&#24191;&#21578;&#30340;&#31435;&#22330;&#21644;&#35758;&#39064;&#65292;&#24182;&#20998;&#26512;&#25919;&#27835;&#27963;&#21160;&#22914;&#20309;&#20351;&#29992;&#26576;&#31181;&#20154;&#21475;&#32479;&#35745;&#23398;&#23450;&#20301;&#65292;&#22914;&#20301;&#32622;&#12289;&#24615;&#21035;&#25110;&#24180;&#40836;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#36873;&#20030;&#27665;&#24847;&#35843;&#26597;&#20013;&#25919;&#27835;&#24191;&#21578;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#21033;&#29992;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#21152;&#24378;&#25991;&#26412;&#20998;&#31867;&#30340;&#25928;&#26524;&#19982;&#20808;&#21069;&#19968;&#26679;&#22312;&#26368;&#26032;&#30340;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#36830;&#32493;&#35821;&#35328;&#24314;&#27169;&#21644;&#21160;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#35821;&#35328;&#20013;&#29305;&#23450;&#24180;&#40836;&#21644;&#24615;&#21035;&#30340;&#26041;&#38754;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#20219;&#21153;&#34920;&#29616;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.07362</link><description>&lt;p&gt;
&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#26159;&#21542;&#33021;&#22815;&#25913;&#21892;&#25991;&#26412;&#20998;&#31867;&#65311;&#22312;&#21464;&#21387;&#22120;&#26102;&#20195;&#37325;&#26032;&#23457;&#35270;&#20154;&#21475;&#32479;&#35745;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Can Demographic Factors Improve Text Classification? Revisiting Demographic Adaptation in the Age of Transformers. (arXiv:2210.07362v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07362
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#21033;&#29992;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#21152;&#24378;&#25991;&#26412;&#20998;&#31867;&#30340;&#25928;&#26524;&#19982;&#20808;&#21069;&#19968;&#26679;&#22312;&#26368;&#26032;&#30340;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#12290;&#35813;&#30740;&#31350;&#20351;&#29992;&#36830;&#32493;&#35821;&#35328;&#24314;&#27169;&#21644;&#21160;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#36866;&#24212;&#35821;&#35328;&#20013;&#29305;&#23450;&#24180;&#40836;&#21644;&#24615;&#21035;&#30340;&#26041;&#38754;&#65292;&#36890;&#36807;&#32467;&#21512;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#21644;&#20154;&#21475;&#32479;&#35745;&#23398;&#30340;&#39044;&#27979;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#22235;&#31181;&#35821;&#35328;&#20013;&#20219;&#21153;&#34920;&#29616;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#65288;&#20363;&#22914;&#24615;&#21035;&#25110;&#24180;&#40836;&#65289;&#22609;&#36896;&#20102;&#25105;&#20204;&#30340;&#35821;&#35328;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#25972;&#21512;&#20154;&#21475;&#32479;&#35745;&#22240;&#32032;&#21487;&#20197;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25506;&#31350;&#20808;&#21069;&#30340;&#21457;&#29616;&#26159;&#21542;&#20173;&#28982;&#36866;&#29992;&#20110;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#35821;&#35328;&#27169;&#22411;(PLMs)&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19977;&#31181;&#24120;&#35265;&#30340;&#19987;&#19994;&#21270;&#26041;&#27861;&#65292;&#23558;&#35013;&#36733;&#20102;&#22806;&#37096;&#30693;&#35782;&#65288;&#20363;&#22914;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#25110;&#22320;&#29702;&#30693;&#35782;&#65289;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#36827;&#34892;&#20102;&#25913;&#36827; &#12290;&#25105;&#20204;&#20351;&#29992;&#36830;&#32493;&#35821;&#35328;&#24314;&#27169;&#21644;&#21160;&#24577;&#22810;&#20219;&#21153;&#23398;&#20064;&#20197;&#29992;&#20110;&#36866;&#24212;&#24615;&#30340;&#24615;&#21035;&#21644;&#24180;&#40836;&#30340;&#35821;&#35328;&#34920;&#31034;&#65292;&#20854;&#20013;&#25105;&#20204;&#32467;&#21512;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#21644;&#20154;&#21475;&#32479;&#35745;&#20998;&#31867;&#30340;&#39044;&#27979;&#12290;&#24403;&#20351;&#29992;&#22810;&#35821;&#35328;PLM&#26102;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22235;&#31181;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#24503;&#35821;&#12289;&#27861;&#35821;&#21644;&#20025;&#40614;&#35821;&#65289;&#20013;&#65292;&#20219;&#21153;&#30340;&#24615;&#33021;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#65292;&#36825;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demographic factors (e.g., gender or age) shape our language. Previous work showed that incorporating demographic factors can consistently improve performance for various NLP tasks with traditional NLP models. In this work, we investigate whether these previous findings still hold with state-of-the-art pretrained Transformer-based language models (PLMs). We use three common specialization methods proven effective for incorporating external knowledge into pretrained Transformers (e.g., domain-specific or geographic knowledge). We adapt the language representations for the demographic dimensions of gender and age, using continuous language modeling and dynamic multi-task learning for adaptation, where we couple language modeling objectives with the prediction of demographic classes. Our results, when employing a multilingual PLM, show substantial gains in task performance across four languages (English, German, French, and Danish), which is consistent with the results of previous work. H
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#24179;&#34892;&#21644;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#30340;&#25928;&#26524;&#65292;&#20165;&#20351;&#29992;&#33521;&#35821;IR&#35757;&#32451;&#25968;&#25454;&#21644;&#19968;&#20123;&#24179;&#34892;&#35821;&#26009;&#24211;&#21363;&#21487;&#22312;&#38750;&#33521;&#35821;&#25968;&#25454;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#26816;&#32034;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2210.06633</link><description>&lt;p&gt;
&#26080;&#20851;&#35821;&#35328;&#30340;&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#19982;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Language Agnostic Multilingual Information Retrieval with Contrastive Learning. (arXiv:2210.06633v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06633
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#24179;&#34892;&#21644;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#30340;&#25928;&#26524;&#65292;&#20165;&#20351;&#29992;&#33521;&#35821;IR&#35757;&#32451;&#25968;&#25454;&#21644;&#19968;&#20123;&#24179;&#34892;&#35821;&#26009;&#24211;&#21363;&#21487;&#22312;&#38750;&#33521;&#35821;&#25968;&#25454;&#19978;&#23454;&#29616;&#26174;&#33879;&#30340;&#26816;&#32034;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#31181;&#20449;&#24687;&#26816;&#32034;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#22312;&#35768;&#22810;&#35821;&#35328;&#20013;&#33719;&#21462;&#32463;&#36807;&#27880;&#37322;&#30340;&#35757;&#32451;&#25968;&#25454;&#25104;&#26412;&#24456;&#39640;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22312;&#21482;&#26377;&#33521;&#35821;IR&#35757;&#32451;&#25968;&#25454;&#21644;&#33521;&#35821;&#19982;&#20854;&#20182;&#35821;&#35328;&#20043;&#38388;&#30340;&#19968;&#20123;&#24179;&#34892;&#35821;&#26009;&#24211;&#21487;&#29992;&#26102;&#35757;&#32451;&#22810;&#35821;&#31181;IR&#31995;&#32479;&#12290;&#25105;&#20204;&#21033;&#29992;&#24179;&#34892;&#21644;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#22810;&#35821;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#35821;&#20041;&#23545;&#27604;&#25439;&#22833;&#65292;&#20197;&#23545;&#40784;&#22312;&#19981;&#21516;&#35821;&#35328;&#20013;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#24179;&#34892;&#21477;&#23376;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#23545;&#27604;&#25439;&#22833;&#65292;&#21033;&#29992;&#24179;&#34892;&#21477;&#23376;&#23545;&#20174;&#38750;&#24179;&#34892;&#35821;&#26009;&#24211;&#20013;&#30340;&#21477;&#23376;&#34920;&#31034;&#20013;&#21024;&#38500;&#35821;&#35328;&#29305;&#23450;&#20449;&#24687;&#12290;&#22312;&#20351;&#29992;&#36825;&#20123;&#25439;&#22833;&#23545;&#33521;&#35821;IR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#24182;&#22312;&#38750;&#33521;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#38646;-shot&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#25913;&#36827;&#65292;&#21516;&#26102;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual information retrieval (IR) is challenging since annotated training data is costly to obtain in many languages. We present an effective method to train multilingual IR systems when only English IR training data and some parallel corpora between English and other languages are available. We leverage parallel and non-parallel corpora to improve the pretrained multilingual language models' cross-lingual transfer ability. We design a semantic contrastive loss to align representations of parallel sentences that share the same semantics in different languages, and a new language contrastive loss to leverage parallel sentence pairs to remove language-specific information in sentence representations from non-parallel corpora. When trained on English IR data with these losses and evaluated zero-shot on non-English data, our model demonstrates significant improvement to prior work on retrieval performance, while it requires much less computational effort. We also demonstrate the valu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DSpERT&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#24230;Transformer&#36880;&#23618;&#32858;&#21512;&#26631;&#35760;&#34920;&#31034;&#20316;&#20026;&#38190;&#21644;&#20540;&#65292;&#20135;&#29983;&#20102;&#28145;&#23618;&#35821;&#20041;&#30340;&#36328;&#24230;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#20013;&#38271;&#36328;&#24230;&#23454;&#20307;&#26174;&#30528;&#26080;&#25928;&#24615;&#21644;&#37325;&#21472;&#36328;&#24230;&#34920;&#31034;&#30340;&#32806;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSpERT&#22312;&#20843;&#20010;NER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#39640;&#20110;&#25110;&#19982;&#26368;&#26032;&#26368;&#20808;&#36827;&#31995;&#32479;&#31454;&#20105;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.04182</link><description>&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#28145;&#24230;&#36328;&#24230;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Deep Span Representations for Named Entity Recognition. (arXiv:2210.04182v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DSpERT&#27169;&#22411;&#65292;&#36890;&#36807;&#36328;&#24230;Transformer&#36880;&#23618;&#32858;&#21512;&#26631;&#35760;&#34920;&#31034;&#20316;&#20026;&#38190;&#21644;&#20540;&#65292;&#20135;&#29983;&#20102;&#28145;&#23618;&#35821;&#20041;&#30340;&#36328;&#24230;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#20013;&#38271;&#36328;&#24230;&#23454;&#20307;&#26174;&#30528;&#26080;&#25928;&#24615;&#21644;&#37325;&#21472;&#36328;&#24230;&#34920;&#31034;&#30340;&#32806;&#21512;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DSpERT&#22312;&#20843;&#20010;NER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#24615;&#33021;&#39640;&#20110;&#25110;&#19982;&#26368;&#26032;&#26368;&#20808;&#36827;&#31995;&#32479;&#31454;&#20105;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24230;&#22522;&#30784;&#27169;&#22411;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26368;&#31616;&#21333;&#30452;&#25509;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290; &#29616;&#26377;&#30340;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#23558;&#26631;&#35760;&#34920;&#31034;&#27973;&#23618;&#32858;&#21512;&#21040;&#36328;&#24230;&#34920;&#31034;&#20013;&#12290; &#20294;&#26159;&#65292;&#36825;&#36890;&#24120;&#23548;&#33268;&#38271;&#36328;&#24230;&#23454;&#20307;&#30340;&#26174;&#30528;&#26080;&#25928;&#24615;&#65292;&#37325;&#21472;&#36328;&#24230;&#34920;&#31034;&#30340;&#32806;&#21512;&#65292;&#26368;&#32456;&#24615;&#33021;&#19979;&#38477;&#12290; &#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DSpERT&#65288;&#26469;&#33258;Transformer&#30340;&#28145;&#24230;&#36328;&#24230;&#32534;&#30721;&#22120;&#34920;&#31034;&#65289;&#65292;&#23427;&#30001;&#26631;&#20934;Transformer&#21644;&#36328;&#24230;Transformer&#32452;&#25104;&#12290; &#21518;&#32773;&#20351;&#29992;&#20302;&#23618;&#27425;&#30340;&#36328;&#24230;&#34920;&#31034;&#20316;&#20026;&#26597;&#35810;&#65292;&#24182;&#20174;&#24213;&#37096;&#21040;&#39030;&#37096;&#36880;&#23618;&#32858;&#21512;&#26631;&#35760;&#34920;&#31034;&#20316;&#20026;&#38190;&#21644;&#20540;&#65292;&#22240;&#27492;&#65292;DSpERT&#20135;&#29983;&#20102;&#28145;&#23618;&#35821;&#20041;&#30340;&#36328;&#24230;&#34920;&#31034;&#12290; &#20511;&#21161;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;DSpERT&#22312;&#20843;&#20010;NER&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20110;&#25110;&#19982;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#31995;&#32479;&#31454;&#20105;&#30340;&#24615;&#33021;&#12290; &#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#28145;&#24230;&#23545;&#36328;&#24230;&#22522;&#30784;NER&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Span-based models are one of the most straightforward methods for named entity recognition (NER). Existing span-based NER systems shallowly aggregate the token representations to span representations. However, this typically results in significant ineffectiveness for long-span entities, a coupling between the representations of overlapping spans, and ultimately a performance degradation. In this study, we propose DSpERT (Deep Span Encoder Representations from Transformers), which comprises a standard Transformer and a span Transformer. The latter uses low-layered span representations as queries, and aggregates the token representations as keys and values, layer by layer from bottom to top. Thus, DSpERT produces span representations of deep semantics.  With weight initialization from pretrained language models, DSpERT achieves performance higher than or competitive with recent state-of-the-art systems on eight NER benchmarks. Experimental results verify the importance of the depth for s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PATRON&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;few-shot&#24615;&#33021;&#65292;&#22312;&#20845;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20919;&#21551;&#21160;&#25968;&#25454;&#36873;&#25321;&#22522;&#32447;&#65292;&#19988;&#20165;&#20351;&#29992;128&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;91.0%&#21644;92.1%&#30340;&#23436;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.06995</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65306;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#20256;&#36882;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;few-shot&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cold-Start Data Selection for Few-shot Language Model Fine-tuning: A Prompt-Based Uncertainty Propagation Approach. (arXiv:2209.06995v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PATRON&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#26469;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;few-shot&#24615;&#33021;&#65292;&#22312;&#20845;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#23454;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20919;&#21551;&#21160;&#25968;&#25454;&#36873;&#25321;&#22522;&#32447;&#65292;&#19988;&#20165;&#20351;&#29992;128&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;91.0%&#21644;92.1%&#30340;&#23436;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;few-shot&#24615;&#33021;&#65292;&#20294;&#24615;&#33021;&#23545;&#20110;few-shot&#23454;&#20363;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PATRON&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#36873;&#25321;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#25968;&#25454;&#65292;&#22312;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#27809;&#26377;&#21021;&#22987;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#12290;&#22312;PATRON&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#65288;1&#65289;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#26041;&#27861;&#26469;&#20272;&#35745;&#25968;&#25454;&#28857;&#30340;&#37325;&#35201;&#24615;&#21644;&#65288;2&#65289;&#19968;&#31181;&#20998;&#21106;-&#37325;&#20889;&#65288;PTR&#65289;&#31574;&#30053;&#65292;&#20197;&#22312;&#26597;&#35810;&#27880;&#37322;&#26102;&#20419;&#36827;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;&#22312;&#20845;&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PATRON&#30340;&#34920;&#29616;&#27604;&#26368;&#24378;&#30340;&#20919;&#21551;&#21160;&#25968;&#25454;&#36873;&#25321;&#22522;&#32447;&#20248;&#36234;&#20102;6.9%&#12290;&#27492;&#22806;&#65292;&#20165;&#20351;&#29992;128&#26631;&#31614;&#65292;PATRON&#22522;&#20110;&#26222;&#36890;&#24494;&#35843;&#21644;&#22522;&#20110;&#25552;&#31034;&#20449;&#24687;&#23398;&#20064;&#20998;&#21035;&#36798;&#21040;&#20102;91.0%&#21644;92.1%&#30340;&#23436;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;PATRON&#23454;&#29616;&#21487;&#22312;\url{https://github.com/yueyu1030/Patron}&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have demonstrated remarkable few-shot performance, but the performance can be sensitive to the selection of few-shot instances. We propose PATRON, a new method that uses prompt-based uncertainty estimation for data selection for pre-trained language model fine-tuning under cold-start scenarios, i.e., no initial labeled data are available. In PATRON, we design (1) a prompt-based uncertainty propagation approach to estimate the importance of data points and (2) a partition-then-rewrite (PTR) strategy to promote sample diversity when querying for annotations. Experiments on six text classification datasets show that PATRON outperforms the strongest cold-start data selection baselines by up to 6.9%. Besides, with 128 labels only, PATRON achieves 91.0% and 92.1% of the fully supervised performance based on vanilla fine-tuning and prompt-based learning respectively. Our implementation of PATRON is available at \url{https://github.com/yueyu1030/Patron}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#19982;&#20154;&#31867;&#35268;&#33539;&#36827;&#34892;&#23545;&#27604;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;LLMs&#22312;&#26576;&#20123;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#20013;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#23384;&#22312;&#24369;&#28857;&#65292;&#20363;&#22914;&#22312;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2208.11981</link><description>&lt;p&gt;
&#35770;&#29616;&#23454;&#21644;&#35821;&#35328;&#25968;&#25454;&#38480;&#21046;&#65306;&#23558;LLMs&#19982;&#20154;&#31867;&#35268;&#33539;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
On Reality and the Limits of Language Data: Aligning LLMs with Human Norms. (arXiv:2208.11981v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#65292;&#20351;&#29992;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#19982;&#20154;&#31867;&#35268;&#33539;&#36827;&#34892;&#23545;&#27604;&#65292;&#30740;&#31350;&#21457;&#29616;&#20102;LLMs&#22312;&#26576;&#20123;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#20013;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#20294;&#23384;&#22312;&#24369;&#28857;&#65292;&#20363;&#22914;&#22312;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21033;&#29992;&#22823;&#37327;&#33258;&#28982;&#35821;&#35328;&#25968;&#25454;&#20013;&#30340;&#35821;&#35328;&#20851;&#32852;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20165;&#20351;&#29992;&#35821;&#35328;&#25968;&#25454;&#26469;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#30340;&#33021;&#21147;&#20173;&#26377;&#30097;&#38382;&#12290;&#22312;&#22238;&#39038;&#29616;&#26377;&#21327;&#35758;&#20043;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#19988;&#20005;&#23494;&#25511;&#21046;&#30340;&#25512;&#29702;&#27979;&#35797;&#65288;ART&#65289;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#27604;&#36739;&#20154;&#31867;&#35268;&#33539;&#19982;GPT-3&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#31361;&#20986;&#20102;&#36890;&#24120;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#24120;&#35782;&#20851;&#31995;&#27169;&#22411;&#31867;&#21035;&#20197;&#21450;&#24369;&#28857;&#25152;&#22312;&#12290;GPT-3&#20026;&#21253;&#25324;&#21516;&#20041;&#35789;&#12289;&#21453;&#20041;&#35789;&#21644;&#40664;&#35748;&#32487;&#25215;&#22312;&#20869;&#30340;&#20960;&#20010;&#20851;&#31995;&#26041;&#38754;&#25552;&#20379;&#20102;&#19982;&#20154;&#31867;&#20027;&#20307;&#30456;&#24403;&#30340;&#21475;&#22836;&#25512;&#29702;&#35777;&#25454;&#12290;&#27809;&#26377;&#26469;&#33258;&#20154;&#31867;&#21028;&#26029;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;GPT-3&#22312;&#20855;&#26377;&#37096;&#20998;&#21644;&#21253;&#21547;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#30340;&#21306;&#38388;&#19979;&#38480;&#22788;&#12290;&#22312;&#24517;&#35201;&#21697;&#36136;&#12289;&#22823;&#23567;&#39034;&#24207;&#21644;&#24378;&#24230;&#39034;&#24207;&#31561;&#26041;&#38754;&#20063;&#35266;&#23519;&#21040;&#20102;&#19981;&#36275;&#20043;&#22788;&#12290;&#25226;LLMs&#19982;&#35937;&#24449;&#24615;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#65292;&#34701;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#32467;&#26500;&#21270;&#30340;EHR&#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#12290;&#36890;&#36807;&#38598;&#25104;&#26799;&#24230;&#26041;&#27861;&#36873;&#25321;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#21033;&#29992; Shapley &#20540;&#21457;&#29616;&#37325;&#35201;&#30340;&#32467;&#26500;&#21270; EHR &#29305;&#24449;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#21487;&#35270;&#21270;&#35299;&#37322;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10240</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#65306;&#23558;&#20020;&#24202;&#31508;&#35760;&#19982;&#32467;&#26500;&#21270; EHR &#25968;&#25454;&#34701;&#21512;&#20197;&#35299;&#37322;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction. (arXiv:2208.10240v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#65292;&#34701;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#32467;&#26500;&#21270;&#30340;EHR&#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#12290;&#36890;&#36807;&#38598;&#25104;&#26799;&#24230;&#26041;&#27861;&#36873;&#25321;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#20851;&#38190;&#35789;&#21644;&#21033;&#29992; Shapley &#20540;&#21457;&#29616;&#37325;&#35201;&#30340;&#32467;&#26500;&#21270; EHR &#29305;&#24449;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#21487;&#35270;&#21270;&#35299;&#37322;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#32467;&#26500;&#21270;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#19968;&#30452;&#26159;&#39044;&#27979;&#27515;&#20129;&#39118;&#38505;&#21644;&#30142;&#30149;&#30340;&#30740;&#31350;&#37325;&#28857;&#12290;&#21516;&#26102;&#65292;&#22823;&#37327;&#30340;&#21465;&#36848;&#24615;&#20020;&#24202;&#31508;&#35760;&#25552;&#20379;&#20102;&#34917;&#20805;&#20449;&#24687;&#65292;&#20294;&#24120;&#24120;&#26410;&#38598;&#25104;&#21040;&#39044;&#27979;&#27169;&#22411;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#22810;&#27169;&#24577;&#36716;&#25442;&#22120;&#65292;&#34701;&#21512;&#20020;&#24202;&#31508;&#35760;&#21644;&#32467;&#26500;&#21270;&#30340; EHR &#25968;&#25454;&#65292;&#20197;&#26356;&#22909;&#22320;&#39044;&#27979;&#20303;&#38498;&#27515;&#20129;&#39118;&#38505;&#12290;&#20026;&#20102;&#25552;&#39640;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26799;&#24230;&#65288;IG&#65289;&#26041;&#27861;&#26469;&#36873;&#25321;&#20020;&#24202;&#31508;&#35760;&#20013;&#30340;&#37325;&#35201;&#35789;&#35821;&#65292;&#24182;&#20351;&#29992; Shapley &#20540;&#21457;&#29616;&#37325;&#35201;&#30340;&#32467;&#26500;&#21270; EHR &#29305;&#24449;&#12290;&#36825;&#20123;&#37325;&#35201;&#30340;&#35789;&#35821;&#21644;&#20020;&#24202;&#29305;&#24449;&#21487;&#35270;&#21270;&#20197;&#21327;&#21161;&#35299;&#37322;&#39044;&#27979;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#21644;&#20219;&#21153;&#33258;&#36866;&#24212;&#24494;&#35843;&#23545;&#20020;&#24202; BERT &#30340;&#24847;&#20041;&#65292;&#35813;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#20020;&#24202;&#31508;&#35760;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep-learning-based clinical decision support using structured electronic health records (EHR) has been an active research area for predicting risks of mortality and diseases. Meanwhile, large amounts of narrative clinical notes provide complementary information, but are often not integrated into predictive models. In this paper, we provide a novel multimodal transformer to fuse clinical notes and structured EHR data for better prediction of in-hospital mortality. To improve interpretability, we propose an integrated gradients (IG) method to select important words in clinical notes and discover the critical structured EHR features with Shapley values. These important words and clinical features are visualized to assist with interpretation of the prediction outcomes. We also investigate the significance of domain adaptive pretraining and task adaptive fine-tuning on the Clinical BERT, which is used to learn the representations of clinical notes. Experiments demonstrated that our model o
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;WEAVER&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#26087;&#30693;&#35782;&#34701;&#20837;&#21040;&#26032;&#27169;&#22411;&#20013;&#65292;&#20197;&#26377;&#25928;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#32456;&#36523;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2202.10101</link><description>&lt;p&gt;
BERT WEAVER&#65306;&#20351;&#29992;&#21152;&#26435;&#24179;&#22343;&#20351;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23454;&#29616;&#32456;&#36523;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BERT WEAVER: Using WEight AVERaging to Enable Lifelong Learning for Transformer-based Models in the Biomedical Domain. (arXiv:2202.10101v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.10101
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;WEAVER&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23558;&#26087;&#30693;&#35782;&#34701;&#20837;&#21040;&#26032;&#27169;&#22411;&#20013;&#65292;&#20197;&#26377;&#25928;&#38477;&#20302;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#23454;&#29616;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#32456;&#36523;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#36716;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#21457;&#23637;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#21462;&#20915;&#20110;&#39640;&#36136;&#37327;&#30340;&#25163;&#21160;&#26631;&#27880;&#35757;&#32451;&#25968;&#25454;&#12290;&#23588;&#20854;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#65292;&#24050;&#32463;&#34920;&#26126;&#19968;&#31181;&#35757;&#32451;&#35821;&#26009;&#24211;&#19981;&#36275;&#20197;&#23398;&#20064;&#33021;&#22815;&#22312;&#26032;&#25968;&#25454;&#19978;&#39640;&#25928;&#39044;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#38656;&#35201;&#20855;&#22791;&#32456;&#36523;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20197;&#20415;&#22312;&#26032;&#25968;&#25454;&#21487;&#29992;&#26102;&#25552;&#39640;&#24615;&#33021; - &#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;WEAVER&#65292;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#23558;&#26087;&#30693;&#35782;&#34701;&#20837;&#21040;&#26032;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#20943;&#23569;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#39034;&#24207;&#24212;&#29992;WEAVER&#20250;&#20135;&#29983;&#31867;&#20284;&#20110;&#19968;&#27425;&#24615;&#20351;&#29992;&#25152;&#26377;&#25968;&#25454;&#36827;&#34892;&#32852;&#21512;&#35757;&#32451;&#30340;&#35789;&#23884;&#20837;&#20998;&#24067;&#65292;&#21516;&#26102;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;&#30001;&#20110;&#27809;&#26377;&#25968;&#25454;&#20849;&#20139;&#30340;&#24517;&#35201;&#65292;&#22240;&#27492;&#25152;&#20171;&#32461;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#38544;&#31169;&#26159;&#19968;&#39033;&#20851;&#27880;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in transfer learning have boosted the advancements in natural language processing tasks. The performance is, however, dependent on high-quality, manually annotated training data. Especially in the biomedical domain, it has been shown that one training corpus is not enough to learn generic models that are able to efficiently predict on new data. Therefore, state-of-the-art models need the ability of lifelong learning in order to improve performance as soon as new data are available - without the need of re-training the whole model from scratch. We present WEAVER, a simple, yet efficient post-processing method that infuses old knowledge into the new model, thereby reducing catastrophic forgetting. We show that applying WEAVER in a sequential manner results in similar word embedding distributions as doing a combined training on all data at once, while being computationally more efficient. Because there is no need of data sharing, the presented method is also easily app
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#65292;&#24212;&#29992;&#20110;&#26500;&#24314;&#22996;&#25176;&#35821;&#32570;&#20301;&#21517;&#35789;&#30701;&#35821;&#21644;&#24102;&#26377;&#22238;&#25351;&#21644;&#30465;&#30053;&#30340;&#35805;&#35821;&#21333;&#20803;&#30340;&#32452;&#21512;&#21521;&#37327;&#35299;&#37322;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2111.11331</link><description>&lt;p&gt;
&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Vector Space Semantics for Lambek Calculus with Soft Subexponentials. (arXiv:2111.11331v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.11331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#65292;&#24212;&#29992;&#20110;&#26500;&#24314;&#22996;&#25176;&#35821;&#32570;&#20301;&#21517;&#35789;&#30701;&#35821;&#21644;&#24102;&#26377;&#22238;&#25351;&#21644;&#30465;&#30053;&#30340;&#35805;&#35821;&#21333;&#20803;&#30340;&#32452;&#21512;&#21521;&#37327;&#35299;&#37322;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#36719;&#23376;&#25351;&#25968;Lambek&#28436;&#31639;&#24320;&#21457;&#20102;&#19968;&#20010;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#65292;&#23558;&#28436;&#31639;&#24212;&#29992;&#20110;&#26500;&#24314;&#22996;&#25176;&#35821;&#32570;&#20301;&#21517;&#35789;&#30701;&#35821;&#21644;&#24102;&#26377;&#22238;&#25351;&#21644;&#30465;&#30053;&#30340;&#35805;&#35821;&#21333;&#20803;&#30340;&#32452;&#21512;&#21521;&#37327;&#35299;&#37322;&#65292;&#24182;&#22312;&#20998;&#24067;&#24335;&#21477;&#23376;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#23454;&#39564;&#35777;&#26126;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#30456;&#20851;&#27169;&#24577;&#30340;Lambek&#28436;&#31639;&#19981;&#21516;&#65292;&#26412;&#25991;&#20013;&#20351;&#29992;&#30340;&#28436;&#31639;&#37319;&#29992;&#20102;&#19968;&#20010;&#26377;&#30028;&#29256;&#26412;&#30340;&#27169;&#24577;&#65292;&#19988;&#21487;&#21028;&#23450;&#12290;&#36825;&#31181;&#26032;&#30340;&#27169;&#24577;&#30340;&#21521;&#37327;&#31354;&#38388;&#35821;&#20041;&#20801;&#35768;&#25105;&#20204;&#26377;&#24847;&#20041;&#22320;&#23450;&#20041;&#25910;&#32553;&#20026;&#25237;&#24433;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32447;&#24615;&#29702;&#35770;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#38750;&#32447;&#24615;&#26144;&#23556;&#26469;&#23454;&#29616;&#30340;&#20869;&#23481;&#21464;&#24471;&#21487;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a vector space semantics for Lambek Calculus with Soft Subexponentials, apply the calculus to construct compositional vector interpretations for parasitic gap noun phrases and discourse units with anaphora and ellipsis, and experiment with the constructions in a distributional sentence similarity task. As opposed to previous work, which used Lambek Calculus with a Relevant Modality the calculus used in this paper uses a bounded version of the modality and is decidable. The vector space semantics of this new modality allows us to meaningfully define contraction as projection and provide a linear theory behind what we could previously only achieve via nonlinear maps.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#21457;&#38899;&#35780;&#20998;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38024;&#23545;&#27492;&#20219;&#21153;&#19987;&#38376;&#35757;&#32451;&#30340;&#31995;&#32479;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#20026;ASR&#35757;&#32451;&#30340;&#27169;&#22411;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#32489;&#65292;&#24182;&#22312;EpaDB&#25968;&#25454;&#24211;&#19978;&#23454;&#29616;&#20102;20&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2111.00976</link><description>&lt;p&gt;
&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#21457;&#38899;&#35780;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A transfer learning based approach for pronunciation scoring. (arXiv:2111.00976v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#21457;&#38899;&#35780;&#20998;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#38024;&#23545;&#27492;&#20219;&#21153;&#19987;&#38376;&#35757;&#32451;&#30340;&#31995;&#32479;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#20026;ASR&#35757;&#32451;&#30340;&#27169;&#22411;&#21462;&#24471;&#26356;&#22909;&#30340;&#25104;&#32489;&#65292;&#24182;&#22312;EpaDB&#25968;&#25454;&#24211;&#19978;&#23454;&#29616;&#20102;20&#65285;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#38899;&#32032;&#32423;&#21035;&#30340;&#21457;&#38899;&#35780;&#20998;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#29616;&#26377;&#31995;&#32479;&#30340;&#24615;&#33021;&#36317;&#31163;&#20154;&#24037;&#35780;&#20998;&#36824;&#26377;&#24456;&#22823;&#24046;&#36317;&#12290;&#26631;&#20934;&#31995;&#32479;&#21033;&#29992;&#20165;&#21253;&#21547;&#26412;&#22320;&#25968;&#25454;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035; (ASR) &#27169;&#22411;&#20026;&#30701;&#35821;&#20013;&#30340;&#27599;&#20010;&#38899;&#32032;&#29983;&#25104;&#19968;&#20010;&#24471;&#20998;&#12290;&#20351;&#29992;&#38024;&#23545;&#27492;&#20219;&#21153;&#19987;&#38376;&#35757;&#32451;&#30340;&#31995;&#32479;&#20197;&#21450;&#38750;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#38754;&#20020;&#30528;&#26631;&#27880;&#25968;&#25454;&#31232;&#32570;&#19988;&#36890;&#24120;&#24456;&#23567;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36801;&#31227;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20026;ASR&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#23558;&#20854;&#35843;&#25972;&#20026;&#21457;&#38899;&#35780;&#20998;&#20219;&#21153;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;&#35774;&#35745;&#36873;&#25321;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#21457;&#38899;&#36136;&#37327; (GOP) &#31995;&#32479;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#38024;&#23545;&#19968;&#20010;&#20248;&#20808;&#32771;&#34385;&#19981;&#24517;&#35201;&#20462;&#27491;&#29575;&#36739;&#20302;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#22312;EpaDB&#65292;&#19968;&#20010;&#29992;&#20110;&#21457;&#38899;&#35780;&#20998;&#30340;&#25968;&#25454;&#24211;&#19978;&#65292;&#25105;&#20204;&#30340;&#26368;&#32456;&#31995;&#32479;&#27604;GOP&#31995;&#32479;&#26356;&#22909;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phone-level pronunciation scoring is a challenging task, with performance far from that of human annotators. Standard systems generate a score for each phone in a phrase using models trained for automatic speech recognition (ASR) with native data only. Better performance has been shown when using systems that are trained specifically for the task using non-native data. Yet, such systems face the challenge that datasets labelled for this task are scarce and usually small. In this paper, we present a transfer learning-based approach that leverages a model trained for ASR, adapting it for the task of pronunciation scoring. We analyze the effect of several design choices and compare the performance with a state-of-the-art goodness of pronunciation (GOP) system. Our final system is 20% better than the GOP system on EpaDB, a database for pronunciation scoring research, for a cost function that prioritizes low rates of unnecessary corrections.
&lt;/p&gt;</description></item></channel></rss>