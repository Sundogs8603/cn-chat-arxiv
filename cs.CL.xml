<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06627</link><description>&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#25512;&#21160;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;
&lt;/p&gt;
&lt;p&gt;
Feedback Loops With Language Models Drive In-Context Reward Hacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06627
&lt;/p&gt;
&lt;p&gt;
&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;&#65288;ICRH&#65289;&#65292;&#21363;&#35821;&#35328;&#27169;&#22411;&#22312;&#27979;&#35797;&#26102;&#22312;&#20248;&#21270;&#30446;&#26631;&#30340;&#21516;&#26102;&#21364;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#36825;&#39033;&#30740;&#31350;&#30830;&#23450;&#20102;&#20004;&#20010;&#23548;&#33268;ICRH&#30340;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#23545;&#22806;&#37096;&#19990;&#30028;&#20135;&#29983;&#24433;&#21709;&#65306;&#23427;&#20204;&#26597;&#35810;&#21487;&#20197;&#35835;&#20889;&#32593;&#39029;&#30340;API&#65292;&#29983;&#25104;&#33021;&#22815;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#20869;&#23481;&#65292;&#20197;&#21450;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#36816;&#34892;&#31995;&#32479;&#21629;&#20196;&#12290;&#36825;&#20123;&#20114;&#21160;&#24418;&#25104;&#20102;&#21453;&#39304;&#24490;&#29615;&#65306;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#24433;&#21709;&#19990;&#30028;&#65292;&#21453;&#36807;&#26469;&#21448;&#24433;&#21709;&#21518;&#32493;&#30340;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21453;&#39304;&#24490;&#29615;&#21487;&#33021;&#23548;&#33268;&#19978;&#19979;&#25991;&#20869;&#22870;&#21169;&#27450;&#39575;(ICRH)&#65292;&#21363;&#27979;&#35797;&#26102;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#20248;&#21270;&#65288;&#21487;&#33021;&#38544;&#21547;&#30340;&#65289;&#30446;&#26631;&#30340;&#21516;&#26102;&#65292;&#20135;&#29983;&#36127;&#38754;&#21103;&#20316;&#29992;&#12290;&#20363;&#22914;&#65292;&#32771;&#34385;&#19968;&#20010;&#34987;&#37096;&#32626;&#29992;&#20110;&#22686;&#21152;Twitter&#21442;&#19982;&#24230;&#30340;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#65307;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19978;&#19979;&#25991;&#31383;&#21475;&#20013;&#26816;&#32034;&#20854;&#20197;&#21069;&#30340;&#25512;&#25991;&#65292;&#24182;&#20351;&#25512;&#25991;&#26356;&#20855;&#20105;&#35758;&#24615;&#65292;&#20174;&#32780;&#22686;&#21152;&#21442;&#19982;&#24230;&#65292;&#20294;&#20063;&#22686;&#21152;&#20102;&#26377;&#27602;&#24615;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#30740;&#31350;&#20102;&#23548;&#33268;ICRH&#30340;&#20004;&#20010;&#36807;&#31243;&#65306;&#36755;&#20986;&#20248;&#21270;&#21644;&#31574;&#30053;&#20248;&#21270;&#12290;&#23545;&#20110;&#36825;&#20123;&#36807;&#31243;&#65292;&#38745;&#24577;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#26159;&#19981;&#36275;&#22815;&#30340;-&#20182;&#20204;&#26080;&#27861;&#25429;&#25417;&#21040;&#21453;&#39304;&#25928;&#24212;&#65292;&#20063;&#19981;&#33021;&#25429;&#25417;&#21040;&#26368;&#26377;&#23475;&#30340;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Language models influence the external world: they query APIs that read and write to web pages, generate content that shapes human behavior, and run system commands as autonomous agents. These interactions form feedback loops: LLM outputs affect the world, which in turn affect subsequent LLM outputs. In this work, we show that feedback loops can cause in-context reward hacking (ICRH), where the LLM at test-time optimizes a (potentially implicit) objective but creates negative side effects in the process. For example, consider an LLM agent deployed to increase Twitter engagement; the LLM may retrieve its previous tweets into the context window and make them more controversial, increasing engagement but also toxicity. We identify and study two processes that lead to ICRH: output-refinement and policy-refinement. For these processes, evaluations on static datasets are insufficient -- they miss the feedback effects and thus cannot capture the most harmful behavior. In response, we provide 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36845;&#20195;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#27861;&#20005;&#37325;&#25439;&#23475;&#20102;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#25552;&#31034;&#21464;&#20307;&#26469;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25351;&#26126;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.06625</link><description>&lt;p&gt;
&#29702;&#35299;&#36845;&#20195;&#25552;&#31034;&#23545;&#30495;&#23454;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effects of Iterative Prompting on Truthfulness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36845;&#20195;&#25552;&#31034;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30495;&#23454;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#27861;&#20005;&#37325;&#25439;&#23475;&#20102;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#25552;&#31034;&#21464;&#20307;&#26469;&#25913;&#21892;&#36825;&#19968;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25351;&#26126;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#24050;&#32463;&#26174;&#33879;&#25913;&#21464;&#20102;&#35768;&#22810;&#39046;&#22495;&#65292;&#25552;&#20379;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#21644;&#30495;&#23454;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36845;&#20195;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#31181;&#34987;&#20551;&#35774;&#21487;&#20197;&#25552;&#39640;LLM&#21709;&#24212;&#31934;&#30830;&#24230;&#30340;&#31574;&#30053;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;LLM&#30495;&#23454;&#24615;&#30340;&#24433;&#21709;&#65292;&#36825;&#20010;&#39046;&#22495;&#23578;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36845;&#20195;&#25552;&#31034;&#21464;&#20307;&#30340;&#22797;&#26434;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#26657;&#20934;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#27861;&#20005;&#37325;&#25439;&#23475;&#20102;&#30495;&#23454;&#24615;&#65292;&#23548;&#33268;&#26657;&#20934;&#35823;&#24046;&#21152;&#21095;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#26088;&#22312;&#35299;&#20915;&#24050;&#30830;&#23450;&#38382;&#39064;&#30340;&#25552;&#31034;&#21464;&#20307;&#12290;&#36825;&#20123;&#21464;&#20307;&#30456;&#23545;&#20110;&#29616;&#26377;&#22522;&#32447;&#27169;&#22411;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25351;&#26126;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#23545;&#36845;&#20195;&#25552;&#31034;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of Large Language Models (LLMs) has notably transformed numerous sectors, offering impressive text generation capabilities. Yet, the reliability and truthfulness of these models remain pressing concerns. To this end, we investigate iterative prompting, a strategy hypothesized to refine LLM responses, assessing its impact on LLM truthfulness, an area which has not been thoroughly explored. Our extensive experiments delve into the intricacies of iterative prompting variants, examining their influence on the accuracy and calibration of model responses. Our findings reveal that naive prompting methods significantly undermine truthfulness, leading to exacerbated calibration errors. In response to these challenges, we introduce several prompting variants designed to address the identified issues. These variants demonstrate marked improvements over existing baselines, signaling a promising direction for future research. Our work provides a nuanced understanding of iterative pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#26469;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#35821;&#35328;&#25910;&#34255;&#21697;&#65292;&#21253;&#25324;513&#20159;&#20010;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.06619</link><description>&lt;p&gt;
Aya&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#22810;&#35821;&#35328;&#25351;&#20196;&#35843;&#20248;&#30340;&#24320;&#25918;&#35775;&#38382;&#25910;&#34255;&#21697;
&lt;/p&gt;
&lt;p&gt;
Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#26469;&#24357;&#21512;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#24182;&#19988;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#35821;&#35328;&#25910;&#34255;&#21697;&#65292;&#21253;&#25324;513&#20159;&#20010;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#35768;&#22810;&#31361;&#30772;&#30340;&#22522;&#30784;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#35768;&#22810;&#26368;&#36817;&#30340;&#25104;&#23601;&#37117;&#24402;&#21151;&#20110;&#22312;&#22810;&#26679;&#21270;&#20219;&#21153;&#19978;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#21709;&#24212;&#25351;&#20196;&#12290;&#25351;&#20196;&#24494;&#35843;&#65288;IFT&#65289;&#38656;&#35201;&#29305;&#21035;&#26500;&#24314;&#21644;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#20960;&#20046;&#37117;&#26159;&#20197;&#33521;&#35821;&#20026;&#20027;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#39318;&#35201;&#30446;&#26631;&#26159;&#36890;&#36807;&#26500;&#24314;&#36328;&#36234;65&#31181;&#35821;&#35328;&#30340;&#20154;&#24037;&#31579;&#36873;&#30340;&#25351;&#20196;&#36981;&#24490;&#25968;&#25454;&#38598;&#26469;&#24357;&#21512;&#35821;&#35328;&#24046;&#36317;&#12290;&#25105;&#20204;&#19982;&#26469;&#33258;&#19990;&#30028;&#21508;&#22320;&#30340;&#27969;&#21033;&#35828;&#32773;&#21512;&#20316;&#65292;&#25910;&#38598;&#25351;&#20196;&#21644;&#23436;&#25104;&#30340;&#33258;&#28982;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#22312;114&#31181;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#27169;&#26495;&#21270;&#21644;&#32763;&#35793;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#21019;&#36896;&#20102;&#36804;&#20170;&#20026;&#27490;&#35268;&#27169;&#26368;&#22823;&#30340;&#22810;&#35821;&#35328;&#25910;&#34255;&#21697;&#65292;&#20849;&#26377;5.13&#20159;&#20010;&#23454;&#20363;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22235;&#20010;&#20851;&#38190;&#36164;&#28304;&#65306;&#25105;&#20204;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;Aya&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#27169;&#26495;&#21270;&#21644;&#32763;&#35793;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#25193;&#23637;&#65292;&#24182;&#23558;&#20854;&#36328;&#36234;&#20102;114&#31181;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Datasets are foundational to many breakthroughs in modern artificial intelligence. Many recent achievements in the space of natural language processing (NLP) can be attributed to the finetuning of pre-trained models on a diverse set of tasks that enables a large language model (LLM) to respond to instructions. Instruction fine-tuning (IFT) requires specifically constructed and annotated datasets. However, existing datasets are almost all in the English language. In this work, our primary goal is to bridge the language gap by building a human-curated instruction-following dataset spanning 65 languages. We worked with fluent speakers of languages from around the world to collect natural instances of instructions and completions. Furthermore, we create the most extensive multilingual collection to date, comprising 513 million instances through templating and translating existing datasets across 114 languages. In total, we contribute four key resources: we develop and open-source the Aya A
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FaBERT&#65292;&#19968;&#20010;&#22522;&#20110;&#27874;&#26031;&#35821;&#21338;&#23458;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#12290;&#23427;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#21033;&#29992;&#22810;&#26679;&#21270;&#21644;&#28165;&#29702;&#22909;&#30340;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#22312;&#27874;&#26031;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06617</link><description>&lt;p&gt;
FaBERT: &#22522;&#20110;&#27874;&#26031;&#35821;&#21338;&#23458;&#30340;BERT&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FaBERT: Pre-training BERT on Persian Blogs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06617
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;FaBERT&#65292;&#19968;&#20010;&#22522;&#20110;&#27874;&#26031;&#35821;&#21338;&#23458;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;BERT&#27169;&#22411;&#12290;&#23427;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#21033;&#29992;&#22810;&#26679;&#21270;&#21644;&#28165;&#29702;&#22909;&#30340;&#35821;&#26009;&#24211;&#26469;&#25552;&#39640;&#22312;&#27874;&#26031;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;FaBERT&#65292;&#19968;&#20010;&#22522;&#20110;&#27874;&#26031;&#35821;HmBlogs&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27874;&#26031;&#35821;BERT-base&#27169;&#22411;&#65292;&#21253;&#25324;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#30340;&#27874;&#26031;&#35821;&#25991;&#26412;&#12290;FaBERT&#26088;&#22312;&#22312;&#20256;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35299;&#20915;&#27874;&#26031;&#35821;&#20013;&#24120;&#35265;&#30340;&#21477;&#23376;&#32467;&#26500;&#21644;&#35821;&#35328;&#39118;&#26684;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#23545;FaBERT&#22312;12&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#20013;&#65292;&#28085;&#30422;&#20102;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#12289;&#38382;&#31572;&#31995;&#32479;&#65288;QA&#65289;&#21644;&#38382;&#39064;&#25913;&#20889;&#65288;QP&#65289;&#31561;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#23427;&#22987;&#32456;&#23637;&#31034;&#20986;&#25913;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23610;&#23544;&#36739;&#23567;&#12290;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#21033;&#29992;&#22810;&#26679;&#21270;&#21644;&#28165;&#29702;&#22909;&#30340;&#35821;&#26009;&#24211;&#65288;&#22914;HmBlogs&#65289;&#26469;&#25552;&#39640;&#31867;&#20284;BERT&#22312;&#27874;&#26031;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;FaBERT&#21487;&#20197;&#22312;https://huggingface.co/sbunlp/fabert&#19978;&#20813;&#36153;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce FaBERT, a Persian BERT-base model pre-trained on the HmBlogs corpus, encompassing both informal and formal Persian texts. FaBERT is designed to excel in traditional Natural Language Understanding (NLU) tasks, addressing the intricacies of diverse sentence structures and linguistic styles prevalent in the Persian language. In our comprehensive evaluation of FaBERT on 12 datasets in various downstream tasks, encompassing Sentiment Analysis (SA), Named Entity Recognition (NER), Natural Language Inference (NLI), Question Answering (QA), and Question Paraphrasing (QP), it consistently demonstrated improved performance, all achieved within a compact model size. The findings highlight the importance of utilizing diverse and cleaned corpora, such as HmBlogs, to enhance the performance of language models like BERT in Persian Natural Language Processing (NLP) applications. FaBERT is openly accessible at https://huggingface.co/sbunlp/fabert
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#21644;&#36923;&#36753;&#20013;&#38388;&#34920;&#31034;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;"&#25991;&#26412;&#21040;&#35745;&#21010;"&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;LLMs&#29992;&#20110;&#29983;&#25104;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#30340;PDDL&#34920;&#31034;&#20197;&#21450;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20351;&#29992;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#21010;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.06608</link><description>&lt;p&gt;
TIC&#65306;&#21033;&#29992;LLMs&#21644;&#36923;&#36753;&#20013;&#38388;&#34920;&#31034;&#31934;&#30830;&#36827;&#34892;&#8220;&#25991;&#26412;&#21040;&#35745;&#21010;&#8221;&#30340;&#32763;&#35793;-&#25512;&#26029;-&#32534;&#35793;
&lt;/p&gt;
&lt;p&gt;
TIC: Translate-Infer-Compile for accurate 'text to plan' using LLMs and logical intermediate representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06608
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#21644;&#36923;&#36753;&#20013;&#38388;&#34920;&#31034;&#26469;&#29983;&#25104;&#20934;&#30830;&#30340;"&#25991;&#26412;&#21040;&#35745;&#21010;"&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;LLMs&#29992;&#20110;&#29983;&#25104;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#30340;PDDL&#34920;&#31034;&#20197;&#21450;&#32463;&#20856;&#35268;&#21010;&#22120;&#30340;&#20351;&#29992;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#21010;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20026;&#32473;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#29983;&#25104;&#35745;&#21010;&#30340;&#38382;&#39064;&#12290;&#19968;&#26041;&#38754;&#65292;LLMs&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35745;&#21010;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32463;&#20856;&#35745;&#21010;&#24037;&#20855;&#22312;&#35745;&#21010;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#38656;&#35201;&#20351;&#29992;&#32467;&#26500;&#21270;&#35821;&#35328;&#65288;&#22914;Planning Domain Definition Language&#65288;PDDL&#65289;&#65289;&#20316;&#20026;&#36755;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20004;&#31181;&#25216;&#26415;&#30340;&#20248;&#28857;&#65292;&#36890;&#36807;&#20351;&#29992;LLMs&#29983;&#25104;&#35745;&#21010;&#20219;&#21153;&#35831;&#27714;&#30340;PDDL&#34920;&#31034;&#65288;&#20219;&#21153;PDDL&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#32463;&#20856;&#35268;&#21010;&#22120;&#35745;&#31639;&#35745;&#21010;&#12290;&#19982;&#30452;&#25509;&#20351;&#29992;LLMs&#29983;&#25104;&#20219;&#21153;PDDL&#30340;&#20808;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65288;a&#65289;&#32763;&#35793;&#65306;&#20165;&#20351;&#29992;LLMs&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25551;&#36848;&#30340;&#36923;&#36753;&#21487;&#35299;&#37322;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#65288;b&#65289;&#25512;&#26029;&#65306;&#20351;&#29992;&#36923;&#36753;&#25512;&#29702;&#22120;&#65288;&#30446;&#21069;&#26159;Answer Set Programming solver&#65289;&#20174;&#20013;&#38388;&#34920;&#31034;&#20013;&#25512;&#23548;&#20986;&#39069;&#22806;&#30340;&#36923;&#36753;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#21450;&#65288;c&#65289;&#32534;&#35793;&#65306;&#29983;&#25104;&#30446;&#26631;&#35745;&#21010;&#30340;PDDL&#25551;&#36848;&#30340;&#32534;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of generating plans for given natural language planning task requests. On one hand, LLMs excel at natural language processing but do not perform well on planning. On the other hand, classical planning tools excel at planning tasks but require input in a structured language such as the Planning Domain Definition Language (PDDL). We leverage the strengths of both the techniques by using an LLM for generating the PDDL representation (task PDDL) of planning task requests followed by using a classical planner for computing a plan. Unlike previous approaches that use LLMs for generating task PDDLs directly, our approach comprises of (a) translate: using an LLM only for generating a logically interpretable intermediate representation of natural language task descriptions, (b) infer: deriving additional logically dependent information from the intermediate representation using a logic reasoner (currently, Answer Set Programming solver), and (c) compile: generating the targ
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36716;&#24405;&#22120;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#25552;&#39640;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#24120;&#35265;&#21333;&#35789;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06592</link><description>&lt;p&gt;
&#33258;&#27965;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36716;&#24405;&#22120;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Self-consistent context aware conformer transducer for speech recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06592
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#27965;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#36716;&#24405;&#22120;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#35821;&#38899;&#35782;&#21035;&#20013;&#25552;&#39640;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19981;&#24433;&#21709;&#24120;&#35265;&#21333;&#35789;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36716;&#24405;&#22120;&#30340;&#26032;&#39062;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#20026;ASR&#31995;&#32479;&#28155;&#21152;&#20102;&#19978;&#19979;&#25991;&#20449;&#24687;&#27969;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25552;&#39640;&#35782;&#21035;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#19981;&#24433;&#21709;&#24120;&#35265;&#21333;&#35789;&#30340;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#24403;&#25105;&#20204;&#20351;&#29992;&#26032;&#27169;&#22411;&#21644;/&#25110;&#19982;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#27973;&#24230;&#34701;&#21512;&#26102;&#65292;&#23545;&#19981;&#24120;&#35265;&#21333;&#35789;&#20934;&#30830;&#24615;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#21457;&#29616;&#20004;&#32773;&#30340;&#32452;&#21512;&#21487;&#20197;&#32047;&#31215;&#25552;&#39640;&#19981;&#24120;&#35265;&#21333;&#35789;&#30340;&#35782;&#21035;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural network architecture based on conformer transducer that adds contextual information flow to the ASR systems. Our method improves the accuracy of recognizing uncommon words while not harming the word error rate of regular words. We explore the uncommon words accuracy improvement when we use the new model and/or shallow fusion with context language model. We found that combination of both provides cumulative gain in uncommon words recognition accuracy.
&lt;/p&gt;</description></item><item><title>G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;</title><link>https://arxiv.org/abs/2402.06584</link><description>&lt;p&gt;
G-SciEdBERT: &#29992;&#20110;&#24503;&#35821;&#31185;&#23398;&#35780;&#20272;&#20219;&#21153;&#30340;&#19978;&#19979;&#25991;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT: A Contextualized LLM for Science Assessment Tasks in German
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06584
&lt;/p&gt;
&lt;p&gt;
G-SciEdBERT&#26159;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65292;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;10%&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#27493;&#20026;&#21508;&#31181;&#35821;&#35328;&#65288;&#20363;&#22914;&#24503;&#35821;&#20013;&#30340;&#24503;&#35821;BERT [G-BERT]&#65289;&#30340;&#33258;&#21160;&#35780;&#20998;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#33258;&#21160;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#38382;&#39064;&#30340;&#20070;&#38754;&#22238;&#31572;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#26631;&#20934;&#30340;G-BERT&#26469;&#35828;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#31185;&#23398;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#19988;&#21487;&#33021;&#19982;&#23398;&#29983;&#30340;&#20889;&#20316;&#39118;&#26684;&#19981;&#19968;&#33268;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#21270;&#24503;&#35821;&#31185;&#23398;&#25945;&#32946;BERT&#65288;G-SciEdBERT&#65289;&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#35780;&#20998;&#24503;&#35821;&#31185;&#23398;&#20219;&#21153;&#30340;&#20070;&#38754;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;G-BERT&#65292;&#22312;5M&#20010;&#26631;&#35760;&#30340;PISA 2015&#22269;&#38469;&#23398;&#29983;&#35780;&#20272;&#30340;50K&#20010;&#24503;&#35821;&#31185;&#23398;&#22238;&#31572;&#35821;&#26009;&#24211;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;59&#20010;&#35780;&#20272;&#39033;&#30446;&#19978;&#23545;G-SciEdBERT&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#24182;&#26816;&#26597;&#20102;&#35780;&#20998;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#24615;&#33021;&#19982;G-BERT&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;G-SciEdBERT&#22312;&#35780;&#20998;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#34920;&#26126;&#20854;&#35780;&#20998;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;10%&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of natural language processing has paved the way for automated scoring systems in various languages, such as German (e.g., German BERT [G-BERT]). Automatically scoring written responses to science questions in German is a complex task and challenging for standard G-BERT as they lack contextual knowledge in the science domain and may be unaligned with student writing styles. This paper developed a contextualized German Science Education BERT (G-SciEdBERT), an innovative large language model tailored for scoring German-written responses to science tasks. Using G-BERT, we pre-trained G-SciEdBERT on a corpus of 50K German written science responses with 5M tokens to the Programme for International Student Assessment (PISA) 2015. We fine-tuned G-SciEdBERT on 59 assessment items and examined the scoring accuracy. We then compared its performance with G-BERT. Our findings reveal a substantial improvement in scoring accuracy with G-SciEdBERT, demonstrating a 10% increase of quad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#21644;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65292;&#25581;&#31034;&#20102;&#21307;&#30103;&#23454;&#36341;&#27169;&#24335;&#19982;&#20002;&#22833;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;&#20998;&#26512;&#20559;&#35265;&#12289;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2402.06563</link><description>&lt;p&gt;
&#21307;&#23398;&#30340;&#26263;&#29289;&#36136;&#20013;&#38544;&#34255;&#30528;&#20160;&#20040;&#65311;&#22312;&#21307;&#30103;&#23454;&#36341;&#20013;&#22788;&#29702;&#20002;&#22833;&#25968;&#25454;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
What is Hiding in Medicine's Dark Matter? Learning with Missing Data in Medical Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#32479;&#35745;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#21644;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65292;&#25581;&#31034;&#20102;&#21307;&#30103;&#23454;&#36341;&#27169;&#24335;&#19982;&#20002;&#22833;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#25552;&#20986;&#20102;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#30340;&#26041;&#27861;&#12290;&#36825;&#23545;&#20110;&#20943;&#23569;&#20998;&#26512;&#20559;&#35265;&#12289;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#20154;&#35760;&#24405;&#65288;EPR&#65289;&#20135;&#29983;&#20102;&#22823;&#37327;&#25968;&#25454;&#65292;&#20294;&#20854;&#20013;&#21253;&#21547;&#37325;&#35201;&#30340;&#32570;&#22833;&#20449;&#24687;&#12290;&#29702;&#35299;&#21644;&#22788;&#29702;&#36825;&#20123;&#32570;&#22833;&#25968;&#25454;&#26159;&#20020;&#24202;&#25968;&#25454;&#20998;&#26512;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#22914;&#26524;&#19981;&#21152;&#20197;&#35299;&#20915;&#65292;&#21487;&#33021;&#23548;&#33268;&#20998;&#26512;&#20013;&#30340;&#20559;&#35265;&#21644;&#20851;&#38190;&#32467;&#35770;&#30340;&#25197;&#26354;&#12290;&#32570;&#22833;&#25968;&#25454;&#21487;&#33021;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#30340;&#23454;&#36341;&#27169;&#24335;&#26377;&#20851;&#65292;&#23545;&#32570;&#22833;&#25968;&#25454;&#30340;&#25554;&#34917;&#21487;&#20197;&#25552;&#39640;&#20020;&#24202;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#32479;&#35745;&#26041;&#27861;&#26469;&#29702;&#35299;&#21644;&#35299;&#37322;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#21333;&#19968;&#20013;&#24515;&#30340;&#20799;&#31185;&#24613;&#35786;&#25968;&#25454;&#20197;&#21450;&#33521;&#22269;&#26368;&#22823;&#30340;&#21019;&#20260;&#20260;&#23475;&#25968;&#25454;&#24211;&#65288;TARN&#65289;&#20013;&#30340;&#25968;&#25454;&#65292;&#36827;&#34892;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#25968;&#25454;&#25554;&#34917;&#12290;&#22312;&#23545;56,961&#20010;&#19982;&#20799;&#31461;&#24613;&#35786;&#37096;&#23601;&#35786;&#30456;&#20851;&#30340;&#21021;&#27493;&#29983;&#21629;&#20307;&#24449;&#21644;&#35266;&#23519;&#25968;&#25454;&#36827;&#34892;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#20002;&#22833;&#25968;&#25454;&#24456;&#21487;&#33021;&#26159;&#38750;&#38543;&#26426;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#25968;&#25454;&#19982;&#21307;&#30103;&#19987;&#19994;&#20154;&#22763;&#30340;&#23454;&#36341;&#27169;&#24335;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic patient records (EPRs) produce a wealth of data but contain significant missing information. Understanding and handling this missing data is an important part of clinical data analysis and if left unaddressed could result in bias in analysis and distortion in critical conclusions. Missing data may be linked to health care professional practice patterns and imputation of missing data can increase the validity of clinical decisions. This study focuses on statistical approaches for understanding and interpreting the missing data and machine learning based clinical data imputation using a single centre's paediatric emergency data and the data from UK's largest clinical audit for traumatic injury database (TARN). In the study of 56,961 data points related to initial vital signs and observations taken on children presenting to an Emergency Department, we have shown that missing data are likely to be non-random and how these are linked to health care professional practice patterns.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06559</link><description>&lt;p&gt;
Diffusion-ES:&#22522;&#20110;&#25193;&#25955;&#30340;&#38646;&#26799;&#24230;&#35268;&#21010;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#21644;&#38646;&#38454;&#25351;&#20196;&#36319;&#38543;
&lt;/p&gt;
&lt;p&gt;
Diffusion-ES: Gradient-free Planning with Diffusion for Autonomous Driving and Zero-Shot Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06559
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Diffusion-ES&#26041;&#27861;&#65292;&#23427;&#32467;&#21512;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#25216;&#26415;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#20915;&#31574;&#21644;&#25511;&#21046;&#20013;&#23545;&#22797;&#26434;&#21644;&#22810;&#27169;&#24577;&#36712;&#36857;&#20998;&#24067;&#24314;&#27169;&#26377;&#24456;&#24378;&#20248;&#21183;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#26041;&#27861;&#65292;&#29992;&#20110;&#20135;&#29983;&#22312;&#25193;&#25955;&#27169;&#22411;&#25152;&#25429;&#33719;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#21644;&#20284;&#28982;&#24615;&#30340;&#36712;&#36857;&#12290;&#22870;&#21169;&#26799;&#24230;&#24341;&#23548;&#21435;&#22122;&#38656;&#35201;&#19968;&#20010;&#36866;&#21512;&#20110;&#28165;&#27905;&#21644;&#22122;&#22768;&#26679;&#26412;&#30340;&#21487;&#24494;&#20998;&#22870;&#21169;&#20989;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#36712;&#36857;&#20248;&#21270;&#22120;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffusionES&#65292;&#19968;&#31181;&#23558;&#26080;&#26799;&#24230;&#20248;&#21270;&#21644;&#36712;&#36857;&#21435;&#22122;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#27969;&#24418;&#20013;&#20248;&#21270;&#40657;&#30418;&#38750;&#21487;&#24494;&#30446;&#26631;&#12290;Diffusion-ES&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#36712;&#36857;&#65292;&#24182;&#20351;&#29992;&#40657;&#30418;&#22870;&#21169;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#12290;&#23427;&#36890;&#36807;&#25130;&#26029;&#25193;&#25955;&#36807;&#31243;&#23545;&#24471;&#20998;&#39640;&#30340;&#36712;&#36857;&#36827;&#34892;&#21464;&#24322;&#65292;&#35813;&#36807;&#31243;&#24212;&#29992;&#23569;&#37327;&#30340;&#22122;&#22768;&#21644;&#21435;&#22122;&#27493;&#39588;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#22810;&#26679;&#24615;&#21644;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at modeling complex and multimodal trajectory distributions for decision-making and control. Reward-gradient guided denoising has been recently proposed to generate trajectories that maximize both a differentiable reward function and the likelihood under the data distribution captured by a diffusion model. Reward-gradient guided denoising requires a differentiable reward function fitted to both clean and noised samples, limiting its applicability as a general trajectory optimizer. In this paper, we propose DiffusionES, a method that combines gradient-free optimization with trajectory denoising to optimize black-box non-differentiable objectives while staying in the data manifold. Diffusion-ES samples trajectories during evolutionary search from a diffusion model and scores them using a black-box reward function. It mutates high-scoring trajectories using a truncated diffusion process that applies a small number of noising and denoising steps, allowing for much mo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;LLaMA&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;&#26041;&#24335;&#65292;&#22312;CASE 2024&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#21644;&#30446;&#26631;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.06549</link><description>&lt;p&gt;
Bryndza&#22312;ClimateActivism 2024&#19978;: &#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#30340;GPT-4&#21644;LLaMA&#36827;&#34892;&#31435;&#22330;&#12289;&#30446;&#26631;&#21644;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bryndza at ClimateActivism 2024: Stance, Target and Hate Event Detection via Retrieval-Augmented GPT-4 and LLaMA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;GPT-4&#21644;LLaMA&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#37325;&#26032;&#25490;&#24207;&#30340;&#26041;&#24335;&#65292;&#22312;CASE 2024&#20849;&#20139;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#21644;&#30446;&#26631;&#35782;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;CASE 2024&#27668;&#20505;&#34892;&#21160;&#31435;&#22330;&#21644;&#20167;&#24680;&#20107;&#20214;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#20013;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#12289;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#35782;&#21035;&#21644;&#31435;&#22330;&#26816;&#27979;&#20316;&#20026;&#20998;&#31867;&#25361;&#25112;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#38646;&#27425;&#25110;&#23569;&#27425;&#35757;&#32451;&#24773;&#20917;&#19979;&#36890;&#36807;&#26816;&#32034;&#22686;&#24378;&#21644;&#37325;&#26032;&#25490;&#24207;&#26469;&#36827;&#34892;&#25512;&#29305;&#20998;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;LLM&#33021;&#21542;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#21305;&#37197;&#25110;&#36229;&#36234;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;LLaMA&#30340;&#28040;&#34701;&#30740;&#31350;&#20197;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#32447;&#65292;&#22312;&#30446;&#26631;&#26816;&#27979;&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#31532;&#20108;&#21517;&#12290;&#25105;&#20204;&#25552;&#20132;&#30340;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/NaiveNeuron/bryndza-case-2024&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study details our approach for the CASE 2024 Shared Task on Climate Activism Stance and Hate Event Detection, focusing on Hate Speech Detection, Hate Speech Target Identification, and Stance Detection as classification challenges. We explored the capability of Large Language Models (LLMs), particularly GPT-4, in zero- or few-shot settings enhanced by retrieval augmentation and re-ranking for Tweet classification. Our goal was to determine if LLMs could match or surpass traditional methods in this context.   We conducted an ablation study with LLaMA for comparison, and our results indicate that our models significantly outperformed the baselines, securing second place in the Target Detection task. The code for our submission is available at https://github.com/NaiveNeuron/bryndza-case-2024
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.06544</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26657;&#20934;&#38271;&#31687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Calibrating Long-form Generations from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06544
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#29992;&#20110;&#26657;&#20934;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38271;&#31687;&#29983;&#25104;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#20316;&#32773;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21487;&#38752;&#24615;&#65292;&#26657;&#20934;&#26159;&#24517;&#35201;&#30340; - &#27169;&#22411;&#30340;&#35780;&#20272;&#32622;&#20449;&#24230;&#24212;&#35813;&#19982;&#20854;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#23454;&#38469;&#21487;&#33021;&#24615;&#30456;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#21644;&#26657;&#20934;&#25351;&#26631;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#21709;&#24212;&#27491;&#30830;&#24615;&#30340;&#20108;&#20803;&#30495;/&#20551;&#35780;&#20272;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#38271;&#31687;&#29983;&#25104;&#20013;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#31572;&#26696;&#21487;&#33021;&#37096;&#20998;&#27491;&#30830;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26657;&#20934;&#26694;&#26550;&#65292;&#20854;&#20013;LLMs&#30340;&#21709;&#24212;&#27491;&#30830;&#24615;&#21644;&#20851;&#32852;&#30340;&#32622;&#20449;&#27700;&#24179;&#37117;&#34987;&#35270;&#20026;&#19968;&#31995;&#21015;&#20998;&#25968;&#30340;&#20998;&#24067;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#20010;&#24230;&#37327;&#25351;&#26631;&#26469;&#31934;&#30830;&#35780;&#20272;LLM&#30340;&#26657;&#20934;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#33258;&#19968;&#33268;&#24615;&#21644;&#33258;&#35780;&#20272;&#30340;&#20004;&#31181;&#32622;&#20449;&#24230;&#24341;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#21253;&#25324;&#38271;&#31687;&#38382;&#31572;&#21644;&#25688;&#35201;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#26356;&#22823;&#30340;&#27169;&#22411;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#26356;&#22909;&#30340;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance Large Language Models' (LLMs) reliability, calibration is essential -- the model's assessed confidence scores should align with the actual likelihood of its responses being correct. However, current confidence elicitation methods and calibration metrics typically rely on a binary true/false assessment of response correctness. This approach does not apply to long-form generation, where an answer can be partially correct. Addressing this gap, we introduce a unified calibration framework, in which both the correctness of the LLMs' responses and their associated confidence levels are treated as distributions across a range of scores. Within this framework, we develop three metrics to precisely evaluate LLM calibration and further propose two confidence elicitation methods based on self-consistency and self-evaluation. Our experiments, which include long-form QA and summarization tasks, demonstrate that larger models don't necessarily guarantee better calibration, that calibratio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06529</link><description>&lt;p&gt;
&#20869;&#30465;&#35268;&#21010;&#65306;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Introspective Planning: Guiding Language-Enabled Agents to Refine Their Own Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#24341;&#23548;&#35821;&#35328;&#39537;&#21160;&#30340;&#20195;&#29702;&#26426;&#22120;&#20154;&#25913;&#36827;&#33258;&#36523;&#19981;&#30830;&#23450;&#24615;&#30340;&#31995;&#32479;&#26041;&#27861;&#12290;&#36890;&#36807;&#35782;&#21035;&#20219;&#21153;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#20219;&#21153;&#35268;&#21010;&#30340;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#20808;&#36827;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#65292;&#24182;&#36890;&#36807;&#36866;&#24403;&#30340;&#22522;&#30784;&#22609;&#36896;&#26469;&#31574;&#30053;&#24615;&#22320;&#36827;&#34892;&#39640;&#32423;&#34892;&#21160;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;LLM&#20135;&#29983;&#30340;&#24187;&#35273;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#33258;&#20449;&#22320;&#25191;&#34892;&#19982;&#29992;&#25143;&#30446;&#26631;&#19981;&#31526;&#25110;&#22312;&#26497;&#31471;&#24773;&#20917;&#19979;&#19981;&#23433;&#20840;&#30340;&#35745;&#21010;&#12290;&#27492;&#22806;&#65292;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#21487;&#33021;&#24341;&#21457;&#20219;&#21153;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22810;&#20010;&#26377;&#25928;&#36873;&#39033;&#30340;&#24773;&#20917;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;LLMs&#24517;&#39035;&#35782;&#21035;&#27492;&#31867;&#19981;&#30830;&#23450;&#24615;&#24182;&#20027;&#21160;&#23547;&#27714;&#28548;&#28165;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#20869;&#30465;&#35268;&#21010;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#65292;&#24341;&#23548;LLMs&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#24418;&#25104;&#24847;&#35782;&#21040;&#19981;&#30830;&#23450;&#24615;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#25191;&#34892;&#35745;&#21010;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20219;&#21153;&#32423;&#26426;&#22120;&#20154;&#35268;&#21010;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#24182;&#35777;&#26126;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LLM&#30340;&#35268;&#21010;&#26041;&#27861;&#30456;&#27604;&#65292;&#20869;&#30465;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#21644;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or, in extreme cases, unsafe. Additionally, inherent ambiguity in natural language instructions can induce task uncertainty, particularly in situations where multiple valid options exist. To address this issue, LLMs must identify such uncertainty and proactively seek clarification. This paper explores the concept of introspective planning as a systematic method for guiding LLMs in forming uncertainty--aware plans for robotic task execution without the need for fine-tuning. We investigate uncertainty quantification in task-level robot planning and demonstrate that introspection significantly improves both success rates and safety compared to state-of-the-art LLM-based planning approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20026;&#23545;&#35805;&#31995;&#32479;&#22312;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#24182;&#19988;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.06509</link><description>&lt;p&gt;
&#22312;&#21512;&#36866;&#30340;&#26102;&#38388;&#25552;&#20986;&#27491;&#30830;&#30340;&#38382;&#39064;&#65306;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#25351;&#23548;&#19979;&#30340;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Asking the Right Question at the Right Time: Human and Model Uncertainty Guidance to Ask Clarification Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20026;&#23545;&#35805;&#31995;&#32479;&#22312;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#25552;&#20379;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#24182;&#19988;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28548;&#28165;&#38382;&#39064;&#26159;&#19968;&#31181;&#22312;&#35821;&#35328;&#20351;&#29992;&#20013;&#34920;&#36798;&#35823;&#35299;&#12289;&#27495;&#20041;&#21644;&#26410;&#26126;&#31034;&#30340;&#37325;&#35201;&#23545;&#35805;&#24037;&#20855;&#12290;&#34429;&#28982;&#20154;&#31867;&#33021;&#22815;&#36890;&#36807;&#25552;&#38382;&#26469;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#65292;&#20294;&#29616;&#20195;&#23545;&#35805;&#31995;&#32479;&#24456;&#38590;&#29983;&#25104;&#26377;&#25928;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#22312;&#36825;&#26041;&#38754;&#21462;&#24471;&#36827;&#23637;&#65292;&#26412;&#25991;&#20197;&#21327;&#20316;&#23545;&#35805;&#20219;&#21153;&#20026;&#27979;&#35797;&#24179;&#21488;&#65292;&#30740;&#31350;&#20102;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#19982;&#20154;&#31867;&#19981;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#8212;&#8212;&#36825;&#26159;&#19968;&#20010;&#23578;&#26410;&#28145;&#20837;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#24182;&#19981;&#21453;&#26144;&#20154;&#31867;&#23547;&#27714;&#28548;&#28165;&#30340;&#34892;&#20026;&#65292;&#36825;&#34920;&#26126;&#20351;&#29992;&#20154;&#31867;&#28548;&#28165;&#38382;&#39064;&#26469;&#20915;&#23450;&#20309;&#26102;&#25552;&#38382;&#21487;&#33021;&#19981;&#26159;&#35299;&#20915;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#29983;&#25104;&#28548;&#28165;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#20960;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clarification questions are an essential dialogue tool to signal misunderstanding, ambiguities, and under-specification in language use. While humans are able to resolve uncertainty by asking questions since childhood, modern dialogue systems struggle to generate effective questions. To make progress in this direction, in this work we take a collaborative dialogue task as a testbed and study how model uncertainty relates to human uncertainty -- an as yet under-explored problem. We show that model uncertainty does not mirror human clarification-seeking behavior, which suggests that using human clarification questions as supervision for deciding when to ask may not be the most effective way to resolve model uncertainty. To address this issue, we propose an approach to generating clarification questions based on model uncertainty estimation, compare it to several alternatives, and show that it leads to significant improvements in terms of task success. Our findings highlight the importanc
&lt;/p&gt;</description></item><item><title>&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.06501</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#29992;&#20110;&#26410;&#26469;&#25351;&#25381;&#19982;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Scalable Interactive Machine Learning for Future Command and Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06501
&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#26412;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#65292;&#20197;&#25552;&#39640;C2&#36816;&#20316;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#25112;&#20105;&#23558;&#38656;&#35201;&#25351;&#25381;&#19982;&#25511;&#21046;&#65288;C2&#65289;&#20154;&#21592;&#22312;&#22797;&#26434;&#19988;&#28508;&#22312;&#27169;&#31946;&#30340;&#24773;&#20917;&#19979;&#20197;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20570;&#20986;&#20915;&#31574;&#12290;&#37492;&#20110;&#38656;&#35201;&#24378;&#22823;&#30340;&#20915;&#31574;&#36807;&#31243;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#26234;&#33021;&#30340;&#38598;&#25104;&#20855;&#26377;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;C2&#36816;&#20316;&#27969;&#31243;&#30340;&#28508;&#21147;&#65292;&#20197;&#30830;&#20445;&#22312;&#24555;&#36895;&#21464;&#21270;&#30340;&#25805;&#20316;&#29615;&#22659;&#20013;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#35758;&#21033;&#29992;&#26368;&#36817;&#22312;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#38754;&#21462;&#24471;&#30340;&#31361;&#30772;&#65292;&#20154;&#31867;&#21487;&#20197;&#19982;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21512;&#20316;&#20197;&#25351;&#23548;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#30446;&#21069;&#31185;&#25216;&#21457;&#23637;&#20013;&#23384;&#22312;&#30340;&#20960;&#20010;&#24046;&#36317;&#65292;&#26410;&#26469;&#30340;&#24037;&#20316;&#24212;&#35813;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#20197;&#25193;&#23637;&#36825;&#20123;&#26041;&#27861;&#22312;&#22797;&#26434;&#30340;C2&#29615;&#22659;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19977;&#20010;&#30740;&#31350;&#37325;&#28857;&#39046;&#22495;&#65292;&#20849;&#21516;&#26088;&#22312;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#20114;&#21160;&#24335;&#26426;&#22120;&#23398;&#20064;&#65288;SIML&#65289;&#65306;1&#65289;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20132;&#20114;&#31639;&#27861;&#20197;&#23454;&#29616;&#21327;&#21516;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future warfare will require Command and Control (C2) personnel to make decisions at shrinking timescales in complex and potentially ill-defined situations. Given the need for robust decision-making processes and decision-support tools, integration of artificial and human intelligence holds the potential to revolutionize the C2 operations process to ensure adaptability and efficiency in rapidly changing operational environments. We propose to leverage recent promising breakthroughs in interactive machine learning, in which humans can cooperate with machine learning algorithms to guide machine learning algorithm behavior. This paper identifies several gaps in state-of-the-art science and technology that future work should address to extend these approaches to function in complex C2 contexts. In particular, we describe three research focus areas that together, aim to enable scalable interactive machine learning (SIML): 1) developing human-AI interaction algorithms to enable planning in co
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SQ-Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#24341;&#20837;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#35757;&#32451;&#38598;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#65292;&#37117;&#33021;&#22815;&#26126;&#30830;&#22320;&#40723;&#21169;&#27169;&#22411;&#22312;&#32534;&#30721;&#21477;&#23376;&#26102;&#20445;&#25345;&#31995;&#32479;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06492</link><description>&lt;p&gt;
&#36890;&#36807;&#20851;&#27880;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#23884;&#20837;&#22312;Transformer&#20013;&#24341;&#23548;&#31995;&#32479;&#24615;
&lt;/p&gt;
&lt;p&gt;
Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;SQ-Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#24341;&#20837;&#32467;&#26500;&#21270;&#37327;&#21270;&#30340;&#26041;&#27861;&#65292;&#26080;&#35770;&#35757;&#32451;&#38598;&#30340;&#22797;&#26434;&#24230;&#22914;&#20309;&#65292;&#37117;&#33021;&#22815;&#26126;&#30830;&#22320;&#40723;&#21169;&#27169;&#22411;&#22312;&#32534;&#30721;&#21477;&#23376;&#26102;&#20445;&#25345;&#31995;&#32479;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35757;&#32451;&#36807;&#22797;&#26434;&#25968;&#25454;&#38598;&#21518;&#33021;&#22815;&#25512;&#24191;&#21040;&#32467;&#26500;&#21644;&#23454;&#20307;&#30340;&#26032;&#32452;&#21512;&#65292;&#20294;&#22312;&#22797;&#26434;&#24230;&#19981;&#36275;&#30340;&#25968;&#25454;&#38598;&#19978;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24403;&#35757;&#32451;&#38598;&#36275;&#22815;&#22797;&#26434;&#26102;&#65292;&#27169;&#22411;&#20351;&#29992;&#31995;&#32479;&#24615;&#30340;&#27880;&#24847;&#27169;&#24335;&#23545;&#20855;&#26377;&#20849;&#21516;&#21477;&#27861;&#32467;&#26500;&#30340;&#21477;&#23376;&#36827;&#34892;&#32534;&#30721;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SQ-Transformer&#65288;&#32467;&#26500;&#21270;&#37327;&#21270;&#65289;&#65292;&#21363;&#20351;&#20351;&#29992;&#20302;&#22797;&#26434;&#24230;&#30340;&#35757;&#32451;&#38598;&#65292;&#20063;&#33021;&#26126;&#30830;&#22320;&#22312;&#23884;&#20837;&#21644;&#27880;&#24847;&#23618;&#20013;&#40723;&#21169;&#31995;&#32479;&#24615;&#12290;&#22312;&#23884;&#20837;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32467;&#26500;&#23548;&#21521;&#30340;&#21521;&#37327;&#37327;&#21270;&#65288;SoVQ&#65289;&#65292;&#23558;&#21333;&#35789;&#23884;&#20837;&#32858;&#31867;&#25104;&#33509;&#24178;&#31867;&#20855;&#26377;&#32467;&#26500;&#31561;&#20215;&#30340;&#23454;&#20307;&#12290;&#22312;&#27880;&#24847;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#31995;&#32479;&#24615;&#27880;&#24847;&#23618;&#65288;SAL&#65289;&#21644;&#21478;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#31995;&#32479;&#24615;&#27491;&#21017;&#21270;&#23618;&#65288;SRL&#65289;&#65292;&#23427;&#20204;&#37117;&#22312;&#37327;&#21270;&#30340;&#35789;&#23884;&#20837;&#19978;&#25805;&#20316;&#65292;&#20197;&#20415;&#20197;&#19981;&#21464;&#25110;&#31867;&#20284;&#30340;&#27880;&#24847;&#27169;&#24335;&#32534;&#30721;&#20855;&#26377;&#30456;&#21516;&#32467;&#26500;&#30340;&#21477;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes sentences that have a common syntactic structure using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers, even with a training set of low complexity. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empiricall
&lt;/p&gt;</description></item><item><title>V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.06457</link><description>&lt;p&gt;
V-STaR: &#33258;&#23398;&#25512;&#29702;&#22120;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
V-STaR: Training Verifiers for Self-Taught Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06457
&lt;/p&gt;
&lt;p&gt;
V-STaR&#21033;&#29992;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#35757;&#32451;&#39564;&#35777;&#22120;&#65292;&#29992;&#20110;&#36873;&#25321;&#27169;&#22411;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#33258;&#25105;&#25913;&#36827;&#21644;&#39564;&#35777;&#26041;&#27861;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24120;&#35265;&#33258;&#25105;&#25913;&#36827;&#26041;&#27861;&#65292;&#20363;&#22914;STaR&#65288;Zelikman&#31561;&#20154;&#65292;2022&#65289;&#65292;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#36845;&#20195;&#24494;&#35843;LLM&#20197;&#25552;&#39640;&#20854;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#20002;&#24323;&#20102;&#22823;&#37327;&#30340;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#24573;&#30053;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;V-STaR&#65292;&#23427;&#21033;&#29992;&#33258;&#25105;&#25913;&#36827;&#36807;&#31243;&#20013;&#29983;&#25104;&#30340;&#27491;&#30830;&#21644;&#19981;&#27491;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#20351;&#29992;DPO&#35757;&#32451;&#19968;&#20010;&#21028;&#26029;&#27169;&#22411;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#27491;&#30830;&#24615;&#30340;&#39564;&#35777;&#22120;&#12290;&#22312;&#25512;&#29702;&#26102;&#65292;&#36825;&#20010;&#39564;&#35777;&#22120;&#29992;&#26469;&#22312;&#20247;&#22810;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#20013;&#36873;&#25321;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#12290;&#22810;&#27425;&#36816;&#34892;V-STaR&#20250;&#36880;&#27493;&#20135;&#29983;&#26356;&#22909;&#30340;&#25512;&#29702;&#22120;&#21644;&#39564;&#35777;&#22120;&#65292;&#22312;&#24120;&#35265;&#20195;&#30721;&#29983;&#25104;&#21644;&#25968;&#23398;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;LLaMA2&#27169;&#22411;&#21487;&#20197;&#21462;&#24471;4%&#21040;17%&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Common self-improvement approaches for large language models (LLMs), such as STaR (Zelikman et al., 2022), iteratively fine-tune LLMs on self-generated solutions to improve their problem-solving ability. However, these approaches discard the large amounts of incorrect solutions generated during this process, potentially neglecting valuable information in such solutions. To address this shortcoming, we propose V-STaR that utilizes both the correct and incorrect solutions generated during the self-improvement process to train a verifier using DPO that judges correctness of model-generated solutions. This verifier is used at inference time to select one solution among many candidate solutions. Running V-STaR for multiple iterations results in progressively better reasoners and verifiers, delivering a 4% to 17% test accuracy improvement over existing self-improvement and verification approaches on common code generation and math reasoning benchmarks with LLaMA2 models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#22810;&#20219;&#21153;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#39044;&#27979;&#35299;&#37322;&#29983;&#25104;&#36807;&#31243;&#30475;&#20316;&#26159;&#19968;&#20010;&#25991;&#26412;&#25688;&#35201;&#38382;&#39064;&#26469;&#35299;&#20915;&#25512;&#29702;&#24046;&#36317;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06443</link><description>&lt;p&gt;
&#20351;&#29992;&#35777;&#25454;&#27719;&#24635;&#35299;&#37322;&#30495;&#23454;&#24615;&#39044;&#27979;&#65306;&#19968;&#31181;&#22810;&#20219;&#21153;&#27169;&#22411;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Explaining Veracity Predictions with Evidence Summarization: A Multi-Task Model Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#22810;&#20219;&#21153;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#39044;&#27979;&#35299;&#37322;&#29983;&#25104;&#36807;&#31243;&#30475;&#20316;&#26159;&#19968;&#20010;&#25991;&#26412;&#25688;&#35201;&#38382;&#39064;&#26469;&#35299;&#20915;&#25512;&#29702;&#24046;&#36317;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#34394;&#20551;&#20449;&#24687;&#30340;&#24555;&#36895;&#20256;&#25773;&#22686;&#21152;&#20102;&#33258;&#21160;&#20107;&#23454;&#26816;&#26597;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#20851;&#20110;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20851;&#27880;&#30340;&#20869;&#23481;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#22686;&#21152;&#12290;&#34429;&#28982;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#36824;&#27809;&#26377;&#36798;&#21040;&#19982;&#20154;&#31867;&#25512;&#29702;&#21487;&#27604;&#30340;&#27700;&#24179;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#22810;&#20219;&#21153;&#21487;&#35299;&#37322;&#24615;&#31070;&#32463;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;&#39044;&#27979;&#30340;&#35299;&#37322;&#29983;&#25104;&#36807;&#31243;&#23450;&#20041;&#20026;&#19968;&#20010;&#25991;&#26412;&#25688;&#35201;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#25152;&#25552;&#20986;&#27169;&#22411;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#35780;&#20272;&#20102;&#30456;&#20851;&#30740;&#31350;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid dissemination of misinformation through social media increased the importance of automated fact-checking. Furthermore, studies on what deep neural models pay attention to when making predictions have increased in recent years. While significant progress has been made in this field, it has not yet reached a level of reasoning comparable to human reasoning. To address these gaps, we propose a multi-task explainable neural model for misinformation detection. Specifically, this work formulates an explanation generation process of the model's veracity prediction as a text summarization problem. Additionally, the performance of the proposed model is discussed on publicly available datasets and the findings are evaluated with related studies.
&lt;/p&gt;</description></item><item><title>&#31532;&#19968;&#23626;&#27169;&#25311;&#23545;&#35805;&#26234;&#33021;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#27719;&#38598;&#23545;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30740;&#31350;&#36827;&#34892;&#23454;&#26102;&#20154;&#31867;&#35780;&#20272;&#30340;&#27169;&#25311;&#26234;&#33021;&#23545;&#35805;&#27169;&#22411;&#12290;&#35770;&#25991;&#20027;&#35201;&#25552;&#20379;&#20102;&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#24182;&#38468;&#19978;&#20102;&#19968;&#20010;&#23558;&#22312;&#30740;&#35752;&#20250;&#21518;&#21457;&#24067;&#30340;&#28145;&#20837;&#20998;&#26512;&#20849;&#20139;&#20219;&#21153;&#32467;&#26524;&#30340;&#38142;&#25509;&#12290;</title><link>https://arxiv.org/abs/2402.06420</link><description>&lt;p&gt;
&#31532;&#19968;&#23626;&#27169;&#25311;&#23545;&#35805;&#26234;&#33021;&#30740;&#35752;&#20250;&#30340;&#30740;&#31350;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Findings of the First Workshop on Simulating Conversational Intelligence in Chat
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06420
&lt;/p&gt;
&lt;p&gt;
&#31532;&#19968;&#23626;&#27169;&#25311;&#23545;&#35805;&#26234;&#33021;&#30740;&#35752;&#20250;&#30340;&#30446;&#26631;&#26159;&#27719;&#38598;&#23545;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30740;&#31350;&#36827;&#34892;&#23454;&#26102;&#20154;&#31867;&#35780;&#20272;&#30340;&#27169;&#25311;&#26234;&#33021;&#23545;&#35805;&#27169;&#22411;&#12290;&#35770;&#25991;&#20027;&#35201;&#25552;&#20379;&#20102;&#20849;&#20139;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#24182;&#38468;&#19978;&#20102;&#19968;&#20010;&#23558;&#22312;&#30740;&#35752;&#20250;&#21518;&#21457;&#24067;&#30340;&#28145;&#20837;&#20998;&#26512;&#20849;&#20139;&#20219;&#21153;&#32467;&#26524;&#30340;&#38142;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#35752;&#20250;&#26088;&#22312;&#27719;&#38598;&#20174;&#20107;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30740;&#31350;&#30340;&#19987;&#23478;&#12290;&#22312;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#30740;&#31350;&#39046;&#22495;&#20013;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#20174;&#23545;&#35805;&#20013;&#23398;&#20064;&#20449;&#24687;&#12289;&#36827;&#34892;&#30495;&#23454;&#21644;&#20196;&#20154;&#20449;&#26381;&#30340;&#20154;&#24037;&#26234;&#33021;&#21644;&#25512;&#29702;&#27169;&#25311;&#12290;SCI-CHAT&#26159;&#20043;&#21069;&#20851;&#20110;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#30340;&#30740;&#35752;&#20250;&#30340;&#24310;&#32493;&#65292;&#20294;&#30528;&#37325;&#20110;&#27169;&#25311;&#26234;&#33021;&#23545;&#35805;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#26469;&#21028;&#26029;&#20854;&#36136;&#37327;&#12290;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#33021;&#22815;&#36319;&#38543;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#65292;&#21516;&#26102;&#25552;&#20986;&#12289;&#21453;&#39539;&#21644;&#25512;&#29702;&#35770;&#35777;&#12290;&#35813;&#30740;&#35752;&#20250;&#21253;&#25324;&#30740;&#31350;&#36335;&#24452;&#21644;&#20849;&#20139;&#20219;&#21153;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#27010;&#36848;&#20849;&#20139;&#20219;&#21153;&#65292;&#24182;&#25552;&#20379;&#19968;&#20010;&#38142;&#25509;&#65292;&#38142;&#25509;&#23558;&#21253;&#21547;&#22312;&#30740;&#35752;&#20250;&#19978;&#23637;&#31034;&#21518;&#23545;&#20849;&#20139;&#20219;&#21153;&#32467;&#26524;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#30340;&#21478;&#19968;&#31687;&#35770;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this workshop is to bring together experts working on open-domain dialogue research. In this speedily advancing research area many challenges still exist, such as learning information from conversations, engaging in realistic and convincing simulation of human intelligence and reasoning. SCI-CHAT follows previous workshops on open domain dialogue but with a focus on the simulation of intelligent conversation as judged in a live human evaluation. Models aim to include the ability to follow a challenging topic over a multi-turn conversation, while positing, refuting and reasoning over arguments. The workshop included both a research track and shared task. The main goal of this paper is to provide an overview of the shared task and a link to an additional paper that will include an in depth analysis of the shared task results following presentation at the workshop.
&lt;/p&gt;</description></item><item><title>CoSearchAgent&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#65292;&#21487;&#20316;&#20026;Slack&#25554;&#20214;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.06360</link><description>&lt;p&gt;
CoSearchAgent:&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CoSearchAgent: A Lightweight Collaborative Search Agent with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06360
&lt;/p&gt;
&lt;p&gt;
CoSearchAgent&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#65292;&#21487;&#20316;&#20026;Slack&#25554;&#20214;&#22312;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#20316;&#25628;&#32034;&#25903;&#25345;&#22810;&#20010;&#29992;&#25143;&#20849;&#21516;&#23436;&#25104;&#29305;&#23450;&#30340;&#25628;&#32034;&#20219;&#21153;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#23558;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#25554;&#20214;&#35774;&#35745;&#22312;&#21363;&#26102;&#36890;&#35759;&#24179;&#21488;&#20869;&#26356;&#31526;&#21512;&#29992;&#25143;&#30340;&#21327;&#20316;&#20064;&#24815;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22810;&#29992;&#25143;&#20132;&#20114;&#22330;&#26223;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#31995;&#32479;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22240;&#27492;&#65292;&#20043;&#21069;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#30740;&#31350;&#19981;&#24471;&#19981;&#20381;&#36182;&#20110;"&#21561;&#29275;&#22823;&#29579;"&#33539;&#20363;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#19982;&#29992;&#25143;&#33258;&#28982;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#23454;&#29616;&#22797;&#26434;&#30340;&#20449;&#24687;&#25628;&#32034;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#22909;&#22320;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#30740;&#31350;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;CoSearchAgent&#65292;&#19968;&#31181;&#30001;LLM&#39537;&#21160;&#30340;&#36731;&#37327;&#32423;&#21327;&#20316;&#25628;&#32034;&#20195;&#29702;&#12290;CoSearchAgent&#34987;&#35774;&#35745;&#20026;Slack&#25554;&#20214;&#65292;&#21487;&#20197;&#22312;&#35813;&#24179;&#21488;&#19978;&#30340;&#22810;&#26041;&#23545;&#35805;&#20013;&#25903;&#25345;&#21327;&#20316;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative search supports multiple users working together to accomplish a specific search task. Research has found that designing lightweight collaborative search plugins within instant messaging platforms aligns better with users' collaborative habits. However, due to the complexity of multi-user interaction scenarios, it is challenging to implement a fully functioning lightweight collaborative search system. Therefore, previous studies on lightweight collaborative search had to rely on the Wizard of Oz paradigm. In recent years, large language models (LLMs) have been demonstrated to interact naturally with users and achieve complex information-seeking tasks through LLM-based agents. Hence, to better support the research in collaborative search, in this demo, we propose CoSearchAgent, a lightweight collaborative search agent powered by LLMs. CoSearchAgent is designed as a Slack plugin that can support collaborative search during multi-party conversations on this platform. Equipped
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#21542;&#24212;&#35813;&#36827;&#19968;&#27493;&#25552;&#21319;&#30446;&#26631;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#28304;&#35821;&#35328;&#20013;&#21253;&#21547;&#30446;&#26631;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30446;&#26631;&#35821;&#35328;&#29616;&#35937;&#12290;</title><link>https://arxiv.org/abs/2402.06342</link><description>&lt;p&gt;
&#20419;&#36827;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30446;&#26631;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Promoting Target Data in Context-aware Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#26159;&#21542;&#24212;&#35813;&#36827;&#19968;&#27493;&#25552;&#21319;&#30446;&#26631;&#25968;&#25454;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;&#28304;&#35821;&#35328;&#20013;&#21253;&#21547;&#30446;&#26631;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30446;&#26631;&#35821;&#35328;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#36890;&#24120;&#20381;&#36182;&#20110;&#24182;&#34892;&#30340;&#25991;&#26723;&#32423;&#25968;&#25454;&#65292;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#12290;&#23588;&#20854;&#26159;&#22522;&#20110;&#25340;&#25509;&#30340;&#26041;&#27861;&#20173;&#28982;&#26159;&#25991;&#26723;&#32423;NMT&#30340;&#24378;&#26377;&#21147;&#22522;&#32447;&#65292;&#23427;&#22312;&#35201;&#34987;&#32763;&#35793;&#30340;&#21477;&#23376;&#20043;&#21069;&#28155;&#21152;&#28304;&#35821;&#35328;&#21644;/&#25110;&#30446;&#26631;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#21477;&#23376;&#65292;&#24182;&#19988;&#22312;&#27599;&#19968;&#20391;&#21033;&#29992;&#30456;&#31561;&#25968;&#37327;&#30340;&#28304;&#35821;&#35328;&#21644;&#30446;&#26631;&#35821;&#35328;&#25968;&#25454;&#30340;&#27169;&#22411;&#21464;&#20307;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26631;&#20934;&#30340;&#22522;&#20110;&#25340;&#25509;&#30340;&#26041;&#27861;&#20013;&#26159;&#21542;&#24212;&#35813;&#36827;&#19968;&#27493;&#25552;&#20379;&#30446;&#26631;&#25968;&#25454;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#25991;&#26723;&#32423;&#29616;&#35937;&#20381;&#36182;&#20110;&#30446;&#26631;&#35821;&#35328;&#20391;&#23384;&#22312;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26032;&#30340;&#22522;&#20110;&#25340;&#25509;&#30340;&#21464;&#31181;&#65292;&#22312;&#28304;&#35821;&#35328;&#20043;&#21069;&#28155;&#21152;&#30446;&#26631;&#19978;&#19979;&#25991;&#65292;&#35201;&#20040;&#20165;&#22312;&#28304;&#35821;&#35328;&#20013;&#28155;&#21152;&#65292;&#35201;&#20040;&#19982;&#28304;&#19978;&#19979;&#25991;&#32452;&#21512;&#12290;&#22312;&#33521;&#20420;&#21644;&#24052;&#26031;&#20811;&#35199;&#29677;&#29273;&#35821;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#28304;&#35821;&#35328;&#20013;&#21253;&#21547;&#30446;&#26631;&#19978;&#19979;&#25991;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#30446;&#26631;&#35821;&#35328;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard context-aware neural machine translation (NMT) typically relies on parallel document-level data, exploiting both source and target contexts. Concatenation-based approaches in particular, still a strong baseline for document-level NMT, prepend source and/or target context sentences to the sentences to be translated, with model variants that exploit equal amounts of source and target data on each side achieving state-of-the-art results. In this work, we investigate whether target data should be further promoted within standard concatenation-based approaches, as most document-level phenomena rely on information that is present on the target language side. We evaluate novel concatenation-based variants where the target context is prepended to the source language, either in isolation or in combination with the source context. Experimental results in English-Russian and Basque-Spanish show that including target context in the source leads to large improvements on target language phe
&lt;/p&gt;</description></item><item><title>RareBench&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#32597;&#35265;&#30149;&#39046;&#22495;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2402.06341</link><description>&lt;p&gt;
RareBench&#65306;LLMs&#33021;&#21542;&#25285;&#20219;&#32597;&#35265;&#30149;&#19987;&#23478;&#65311;
&lt;/p&gt;
&lt;p&gt;
RareBench: Can LLMs Serve as Rare Diseases Specialists?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06341
&lt;/p&gt;
&lt;p&gt;
RareBench&#26159;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLMs&#22312;&#32597;&#35265;&#30149;&#39046;&#22495;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#26368;&#22823;&#30340;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-4&#65292;&#22312;&#21253;&#25324;&#21307;&#23398;&#35786;&#26029;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#28508;&#21147;&#12290;&#32597;&#35265;&#30149;&#65292;&#24433;&#21709;&#20840;&#29699;&#32422;3&#20159;&#20154;&#65292;&#24448;&#24448;&#30001;&#20110;&#32570;&#20047;&#32463;&#39564;&#20016;&#23500;&#30340;&#21307;&#29983;&#21644;&#38590;&#20197;&#21306;&#20998;&#20247;&#22810;&#32597;&#35265;&#30149;&#30340;&#22797;&#26434;&#24615;&#32780;&#23548;&#33268;&#20020;&#24202;&#35786;&#26029;&#29575;&#19981;&#23613;&#20154;&#24847;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26368;&#36817;&#30340;&#26032;&#38395;&#22914;"ChatGPT&#22312;17&#21517;&#21307;&#29983;&#22833;&#36133;&#21518;&#27491;&#30830;&#35786;&#26029;&#20986;&#20102;&#19968;&#20301;4&#23681;&#23401;&#23376;&#30340;&#32597;&#35265;&#30149;"&#24378;&#35843;&#20102;LLMs&#22312;&#20020;&#24202;&#35786;&#26029;&#32597;&#35265;&#30149;&#20013;&#30340;&#28508;&#21147;&#65292;&#28982;&#32780;&#36825;&#20010;&#35282;&#33394;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;RareBench&#65292;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;LLMs&#22312;&#32597;&#35265;&#30149;&#39046;&#22495;&#20869;&#30340;4&#20010;&#20851;&#38190;&#32500;&#24230;&#19978;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#26368;&#22823;&#30340;&#32597;&#35265;&#30149;&#24739;&#32773;&#24320;&#25918;&#25968;&#25454;&#38598;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#12290;&#20026;&#20102;&#20419;&#36827;&#32597;&#35265;&#30149;&#30340;&#24046;&#24322;&#35786;&#26029;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21160;&#24577;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalist Large Language Models (LLMs), such as GPT-4, have shown considerable promise in various domains, including medical diagnosis. Rare diseases, affecting approximately 300 million people worldwide, often have unsatisfactory clinical diagnosis rates primarily due to a lack of experienced physicians and the complexity of differentiating among many rare diseases. In this context, recent news such as "ChatGPT correctly diagnosed a 4-year-old's rare disease after 17 doctors failed" underscore LLMs' potential, yet underexplored, role in clinically diagnosing rare diseases. To bridge this research gap, we introduce RareBench, a pioneering benchmark designed to systematically evaluate the capabilities of LLMs on 4 critical dimensions within the realm of rare diseases. Meanwhile, we have compiled the largest open-source dataset on rare disease patients, establishing a benchmark for future studies in this domain. To facilitate differential diagnosis of rare diseases, we develop a dynamic
&lt;/p&gt;</description></item><item><title>ExaRanker-Open &#26159;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;</title><link>https://arxiv.org/abs/2402.06334</link><description>&lt;p&gt;
ExaRanker-Open: &#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#21512;&#25104;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
ExaRanker-Open: Synthetic Explanation for IR using Open-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06334
&lt;/p&gt;
&lt;p&gt;
ExaRanker-Open &#26159;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;LLMs&#36827;&#34892;IR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ExaRanker&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#20449;&#24687;&#26816;&#32034;(IR)&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20316;&#20026;&#38468;&#21152;&#26631;&#31614;&#12290;&#35813;&#26041;&#27861;&#35299;&#20915;&#20102;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#25361;&#25112;&#65292;&#25552;&#39640;&#20102;IR&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#21021;&#22987;&#32467;&#26524;&#26159;&#22522;&#20110;&#19987;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;GPT-3.5&#65292;&#36825;&#23548;&#33268;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#22240;&#20026;&#20854;&#25104;&#26412;&#21644;&#25968;&#25454;&#38544;&#31169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ExaRanker-Open&#65292;&#36890;&#36807;&#36866;&#24212;&#21644;&#25506;&#32034;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#35299;&#37322;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#20351;&#29992;&#19981;&#21516;&#30340;LLMs&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#30340;&#26377;&#25928;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32435;&#20837;&#35299;&#37322;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#31070;&#32463;&#25490;&#24207;&#22120;&#30340;&#24615;&#33021;&#65292;&#32780;LLM&#30340;&#22823;&#23567;&#36234;&#22823;&#65292;&#25910;&#30410;&#36234;&#22823;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#65292;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20063;&#26159;&#26377;&#20248;&#21183;&#30340;&#65292;ExaRanker&#30340;&#24615;&#33021;&#36229;&#36807;&#30446;&#26631;&#22522;&#32447;0
&lt;/p&gt;
&lt;p&gt;
ExaRanker recently introduced an approach to training information retrieval (IR) models, incorporating natural language explanations as additional labels. The method addresses the challenge of limited labeled examples, leading to improvements in the effectiveness of IR models. However, the initial results were based on proprietary language models such as GPT-3.5, which posed constraints on dataset size due to its cost and data privacy. In this paper, we introduce ExaRanker-Open, where we adapt and explore the use of open-source language models to generate explanations. The method has been tested using different LLMs and datasets sizes to better comprehend the effective contribution of data augmentation. Our findings reveal that incorporating explanations consistently enhances neural rankers, with benefits escalating as the LLM size increases. Notably, the data augmentation method proves advantageous even with large datasets, as evidenced by ExaRanker surpassing the target baseline by 0
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#28304;&#25968;&#23398;&#25512;&#29702;LLMs InternLM-Math&#65292;&#35813;&#27169;&#22411;&#20197;&#20854;&#25968;&#23398;&#33021;&#21147;&#20195;&#34920;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#25972;&#21512;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#22870;&#21169;&#24314;&#27169;&#12289;&#24418;&#24335;&#25512;&#29702;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25968;&#23398;&#25512;&#29702;&#22120;&#12289;&#39564;&#35777;&#22120;&#12289;&#35777;&#26126;&#22120;&#21644;&#22686;&#24378;&#22120;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21253;&#25324;GSM8K&#12289;MATH&#12289;&#21256;&#29273;&#21033;&#25968;&#23398;&#32771;&#35797;&#12289;MathBench-ZH&#21644;MiniF2F&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#20195;&#30721;&#36741;&#21161;&#25512;&#29702;&#30340;&#35774;&#32622;&#19979;&#65292;InternLM-Math&#21462;&#24471;&#20102;&#24320;&#28304;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;MiniF2F&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;30.3&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;LEAN&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06332</link><description>&lt;p&gt;
InternLM-Math&#65306;&#38754;&#21521;&#21487;&#39564;&#35777;&#25512;&#29702;&#30340;&#24320;&#25918;&#25968;&#23398;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InternLM-Math: Open Math Large Language Models Toward Verifiable Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06332
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#28304;&#25968;&#23398;&#25512;&#29702;LLMs InternLM-Math&#65292;&#35813;&#27169;&#22411;&#20197;&#20854;&#25968;&#23398;&#33021;&#21147;&#20195;&#34920;&#20102;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#25972;&#21512;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#22870;&#21169;&#24314;&#27169;&#12289;&#24418;&#24335;&#25512;&#29702;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#23398;&#20064;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25968;&#23398;&#25512;&#29702;&#22120;&#12289;&#39564;&#35777;&#22120;&#12289;&#35777;&#26126;&#22120;&#21644;&#22686;&#24378;&#22120;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#21253;&#25324;GSM8K&#12289;MATH&#12289;&#21256;&#29273;&#21033;&#25968;&#23398;&#32771;&#35797;&#12289;MathBench-ZH&#21644;MiniF2F&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#20195;&#30721;&#36741;&#21161;&#25512;&#29702;&#30340;&#35774;&#32622;&#19979;&#65292;InternLM-Math&#21462;&#24471;&#20102;&#24320;&#28304;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;MiniF2F&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;30.3&#30340;&#24471;&#20998;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;LEAN&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#33021;&#21147;&#21487;&#20197;&#34920;&#31034;&#20854;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#24182;&#24320;&#28304;&#25105;&#20204;&#30340;&#25968;&#23398;&#25512;&#29702;LLMs InternLM-Math&#65292;&#35813;&#27169;&#22411;&#26159;&#20174;InternLM2&#32487;&#32493;&#39044;&#35757;&#32451;&#32780;&#26469;&#12290;&#25105;&#20204;&#20197;&#32479;&#19968;&#30340;seq2seq&#26684;&#24335;&#32479;&#19968;&#20102;&#36923;&#36753;&#25512;&#29702;&#12289;&#22870;&#21169;&#24314;&#27169;&#12289;&#24418;&#24335;&#25512;&#29702;&#12289;&#25968;&#25454;&#22686;&#24378;&#21644;&#20195;&#30721;&#35299;&#37322;&#22120;&#65292;&#24182;&#19988;&#30417;&#30563;&#25105;&#20204;&#30340;&#27169;&#22411;&#25104;&#20026;&#19968;&#20010;&#22810;&#21151;&#33021;&#30340;&#25968;&#23398;&#25512;&#29702;&#22120;&#12289;&#39564;&#35777;&#22120;&#12289;&#35777;&#26126;&#22120;&#21644;&#22686;&#24378;&#22120;&#12290;&#36825;&#20123;&#33021;&#21147;&#21487;&#29992;&#20110;&#24320;&#21457;&#19979;&#19968;&#20195;&#25968;&#23398;LLMs&#25110;&#33258;&#36523;&#36845;&#20195;&#12290;&#22312;&#21253;&#25324;GSM8K&#12289;MATH&#12289;&#21256;&#29273;&#21033;&#25968;&#23398;&#32771;&#35797;&#12289;MathBench-ZH&#21644;MiniF2F&#22312;&#20869;&#30340;&#21508;&#31181;&#38750;&#27491;&#24335;&#21644;&#27491;&#24335;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;InternLM-Math&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#30417;&#30563;&#24494;&#35843;&#21644;&#20195;&#30721;&#36741;&#21161;&#25512;&#29702;&#30340;&#35774;&#32622;&#19979;&#21462;&#24471;&#20102;&#24320;&#28304;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#26080;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;MiniF2F&#27979;&#35797;&#38598;&#19978;&#36798;&#21040;&#20102;30.3&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;LEAN&#26469;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#20854;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The math abilities of large language models can represent their abstract reasoning ability. In this paper, we introduce and open-source our math reasoning LLMs InternLM-Math which is continue pre-trained from InternLM2. We unify chain-of-thought reasoning, reward modeling, formal reasoning, data augmentation, and code interpreter in a unified seq2seq format and supervise our model to be a versatile math reasoner, verifier, prover, and augmenter. These abilities can be used to develop the next math LLMs or self-iteration. InternLM-Math obtains open-sourced state-of-the-art performance under the setting of in-context learning, supervised fine-tuning, and code-assisted reasoning in various informal and formal benchmarks including GSM8K, MATH, Hungary math exam, MathBench-ZH, and MiniF2F. Our pre-trained model achieves 30.3 on the MiniF2F test set without fine-tuning. We further explore how to use LEAN to solve math problems and study its performance under the setting of multi-task learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.06262</link><description>&lt;p&gt;
&#20851;&#20110;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38190;&#20540;&#32422;&#26463;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#30340;&#39537;&#36880;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#31574;&#30053;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#30001;&#20110;&#23427;&#20204;&#23545;&#20869;&#23384;&#21644;&#35745;&#31639;&#36164;&#28304;&#30340;&#36807;&#24230;&#38656;&#27714;&#65292;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#37096;&#32626;&#20173;&#28982;&#26114;&#36149;&#12290;&#38500;&#20102;&#27169;&#22411;&#21442;&#25968;&#22806;&#65292;&#38190;&#20540;&#32531;&#23384;&#20063;&#23384;&#20648;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#38543;&#30528;&#25209;&#22788;&#29702;&#22823;&#23567;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#22686;&#21152;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#20026;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21508;&#31181;&#38024;&#23545;&#32473;&#23450;&#39044;&#31639;&#19979;&#32500;&#25252;&#38190;&#20540;&#32531;&#23384;&#24320;&#38144;&#30340;&#39537;&#36880;&#31574;&#30053;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#29616;&#26377;&#39537;&#36880;&#31574;&#30053;&#22312;&#37325;&#35201;&#24615;&#35780;&#20998;&#35745;&#31639;&#21644;&#39537;&#36880;&#33539;&#22260;&#26500;&#24314;&#20004;&#20010;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20808;&#21069;&#31574;&#30053;&#22312;&#36825;&#20004;&#20010;&#26041;&#38754;&#30340;&#19981;&#36275;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26102;&#38388;&#27880;&#24847;&#21147;&#24471;&#20998;&#21644;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;RoCo&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#32531;&#23384;&#39537;&#36880;&#31574;&#30053;&#12290;&#28085;&#30422;&#20102;&#39044;&#22635;&#20805;&#21644;&#33258;&#22238;&#24402;&#35299;&#30721;&#38454;&#27573;&#30340;&#24191;&#27867;&#23454;&#39564;&#39564;&#35777;&#20102;RoCo&#30340;&#20248;&#36234;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;RoCo&#30340;&#20195;&#30721;&#21644;&#27169;&#22411;&#20379;&#30740;&#31350;&#32773;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the recent success associated with Large Language Models~(LLMs), they are notably cost-prohibitive to deploy in resource-constrained environments due to their excessive memory and computational demands. In addition to model parameters, the key-value cache is also stored in GPU memory, growing linearly with batch size and sequence length. As a remedy, recent works have proposed various eviction policies for maintaining the overhead of key-value cache under a given budget. This paper embarks on the efficacy of existing eviction policies in terms of \textit{importance score calculation} and \textit{eviction scope construction}. We identify the deficiency of prior policies in these two aspects and introduce RoCo, a \underline{r}\underline{o}bust \underline{c}ache \underline{o}mission policy based on temporal attention scores and robustness measures. Extensive experimentation spanning prefilling and auto-regressive decoding stages validates the superiority of RoCo. Finally, we relea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>https://arxiv.org/abs/2402.06255</link><description>&lt;p&gt;
&#36827;&#21462;&#30340;&#40077;&#21187;&#36890;&#36807;&#25552;&#31034;&#23545;&#25239;&#35843;&#25972;&#25269;&#21046;&#36234;&#29425;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Studious Bob Fight Back Against Jailbreaking via Prompt Adversarial Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning (PAT)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#65292;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#34892;&#20026;&#30340;&#38450;&#24481;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#26041;&#38754;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20063;&#23481;&#26131;&#21463;&#21040;&#29305;&#23450;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#32469;&#36807;&#20869;&#32622;&#30340;&#23433;&#20840;&#25514;&#26045;&#24182;&#25552;&#20379;&#21361;&#38505;&#25110;&#38750;&#27861;&#20869;&#23481;&#65292;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#36234;&#29425;&#34892;&#20026;&#12290;&#20026;&#20102;&#20445;&#25252;LLMs&#20813;&#21463;&#20135;&#29983;&#26377;&#23475;&#20449;&#24687;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#31574;&#30053;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#38598;&#20013;&#22312;&#20869;&#23481;&#36807;&#28388;&#25110;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Prompt Adversarial Tuning&#65288;PAT&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#19968;&#20010;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#24182;&#23558;&#20854;&#20316;&#20026;&#21069;&#32512;&#23884;&#20837;&#21040;&#29992;&#25143;&#25552;&#31034;&#20013;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#38450;&#24481;&#31574;&#30053;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31867;&#20284;&#23545;&#25239;&#35757;&#32451;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#25105;&#20204;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20132;&#26367;&#26356;&#26032;&#25915;&#20987;&#21644;&#38450;&#24481;&#25511;&#21046;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20174;&#25552;&#31034;&#35843;&#25972;&#30340;&#35282;&#24230;&#23454;&#26045;&#38450;&#24481;&#30340;&#20154;&#12290;&#19968;&#26086;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#19981;&#20250;&#24433;&#21709;LLMs&#30340;&#25805;&#20316;&#25928;&#29575;&#12290;&#23454;&#39564;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25269;&#24481;&#36234;&#29425;&#34892;&#20026;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have achieved tremendous success in various applications, they are also susceptible to certain prompts that can induce them to bypass built-in safety measures and provide dangerous or illegal content, a phenomenon known as jailbreak. To protect LLMs from producing harmful information, various defense strategies are proposed, with most focusing on content filtering or adversarial training of models. In this paper, we propose an approach named Prompt Adversarial Tuning (PAT) to train a defense control mechanism, which is then embedded as a prefix to user prompts to implement our defense strategy. We design a training process similar to adversarial training to achieve our optimized goal, alternating between updating attack and defense controls. To our knowledge, we are the first to implement defense from the perspective of prompt tuning. Once employed, our method will hardly impact the operational efficiency of LLMs. Experiments show that our method i
&lt;/p&gt;</description></item><item><title>ResumeFlow&#26159;&#19968;&#31181;&#21033;&#29992;LLM&#25216;&#26415;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#27714;&#32844;&#32773;&#26681;&#25454;&#29305;&#23450;&#30340;&#32844;&#20301;&#35201;&#27714;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#31616;&#21382;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#23450;&#21046;&#31616;&#21382;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06221</link><description>&lt;p&gt;
ResumeFlow: &#19968;&#31181;&#20010;&#24615;&#21270;&#31616;&#21382;&#29983;&#25104;&#21644;&#20462;&#35746;&#30340;LLM&#36741;&#21161;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
ResumeFlow: An LLM-facilitated Pipeline for Personalized Resume Generation and Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06221
&lt;/p&gt;
&lt;p&gt;
ResumeFlow&#26159;&#19968;&#31181;&#21033;&#29992;LLM&#25216;&#26415;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#27714;&#32844;&#32773;&#26681;&#25454;&#29305;&#23450;&#30340;&#32844;&#20301;&#35201;&#27714;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#31616;&#21382;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#25163;&#21160;&#23450;&#21046;&#31616;&#21382;&#30340;&#32791;&#26102;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#27714;&#32844;&#32773;&#26469;&#35828;&#65292;&#21046;&#20316;&#31526;&#21512;&#29305;&#23450;&#32844;&#20301;&#35201;&#27714;&#30340;&#29702;&#24819;&#31616;&#21382;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#21021;&#20837;&#32844;&#22330;&#30340;&#27714;&#32844;&#32773;&#26469;&#35828;&#12290;&#34429;&#28982;&#24378;&#28872;&#24314;&#35758;&#27714;&#32844;&#32773;&#26681;&#25454;&#20182;&#20204;&#30003;&#35831;&#30340;&#20855;&#20307;&#32844;&#20301;&#23450;&#21046;&#31616;&#21382;&#65292;&#20294;&#25163;&#21160;&#26681;&#25454;&#24037;&#20316;&#25551;&#36848;&#21644;&#32844;&#20301;&#35201;&#27714;&#26469;&#23450;&#21046;&#31616;&#21382;&#36890;&#24120; (1) &#38750;&#24120;&#32791;&#26102;&#65292;&#19988; (2) &#23481;&#26131;&#20986;&#38169;&#12290;&#27492;&#22806;&#65292;&#22312;&#30003;&#35831;&#22810;&#20010;&#32844;&#20301;&#26102;&#36827;&#34892;&#36825;&#26679;&#30340;&#23450;&#21046;&#27493;&#39588;&#21487;&#33021;&#23548;&#33268;&#32534;&#36753;&#31616;&#21382;&#36136;&#37327;&#19981;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#28436;&#31034;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ResumeFlow: &#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24037;&#20855;&#65292;&#20351;&#32456;&#31471;&#29992;&#25143;&#21482;&#38656;&#25552;&#20379;&#35814;&#32454;&#30340;&#31616;&#21382;&#21644;&#25152;&#38656;&#30340;&#32844;&#20301;&#21457;&#24067;&#20449;&#24687;&#65292;&#23601;&#33021;&#22312;&#20960;&#31186;&#38047;&#20869;&#33719;&#24471;&#19968;&#20010;&#38024;&#23545;&#35813;&#29305;&#23450;&#32844;&#20301;&#21457;&#24067;&#30340;&#20010;&#24615;&#21270;&#31616;&#21382;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27969;&#31243;&#21033;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;LLM&#65288;&#22914;OpenAI&#30340;GPT-4&#21644;Google&#30340;......&#65289;
&lt;/p&gt;
&lt;p&gt;
Crafting the ideal, job-specific resume is a challenging task for many job applicants, especially for early-career applicants. While it is highly recommended that applicants tailor their resume to the specific role they are applying for, manually tailoring resumes to job descriptions and role-specific requirements is often (1) extremely time-consuming, and (2) prone to human errors. Furthermore, performing such a tailoring step at scale while applying to several roles may result in a lack of quality of the edited resumes. To tackle this problem, in this demo paper, we propose ResumeFlow: a Large Language Model (LLM) aided tool that enables an end user to simply provide their detailed resume and the desired job posting, and obtain a personalized resume specifically tailored to that specific job posting in the matter of a few seconds. Our proposed pipeline leverages the language understanding and information extraction capabilities of state-of-the-art LLMs such as OpenAI's GPT-4 and Goog
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;meta-SCM&#65289;&#26469;&#25972;&#21512;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#22240;&#26524;&#22240;&#23376;&#24182;&#20351;&#29992;&#36825;&#20123;&#22240;&#23376;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#8220;&#20266;&#30456;&#20851;&#24615;&#8221;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06220</link><description>&lt;p&gt;
&#32479;&#19968;&#30340;&#22240;&#26524;&#35270;&#35282;&#19979;&#30340;&#25351;&#20196;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
A Unified Causal View of Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06220
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20803;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;meta-SCM&#65289;&#26469;&#25972;&#21512;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#12290;&#36890;&#36807;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#25152;&#38656;&#30340;&#22240;&#26524;&#22240;&#23376;&#24182;&#20351;&#29992;&#36825;&#20123;&#22240;&#23376;&#36827;&#34892;&#39044;&#27979;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#8220;&#20266;&#30456;&#20851;&#24615;&#8221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#28151;&#21512;&#20219;&#21153;&#19978;&#30340;&#25351;&#20196;&#35843;&#20248;&#24050;&#32463;&#25552;&#39640;&#20102;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#23398;&#20064;&#21040;&#20102;&#22312;&#25351;&#20196;&#26684;&#24335;&#30340;&#26679;&#26412;&#21644;&#30446;&#26631;&#26631;&#31614;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#32780;&#19981;&#26159;&#22240;&#26524;&#20851;&#31995;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#34987;&#32479;&#35745;&#23398;&#19978;&#31216;&#20026;&#8220;&#20266;&#30456;&#20851;&#24615;&#8221;&#30340;&#30456;&#20851;&#24615;&#22312;&#26032;&#20219;&#21153;&#20013;&#21487;&#33021;&#20250;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#65292;&#20351;&#24471;&#23398;&#20064;&#21040;&#30340;&#29305;&#24449;&#23545;&#32467;&#26524;&#20135;&#29983;&#35823;&#23548;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20803;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;meta-SCM&#65289;&#65292;&#29992;&#20110;&#23558;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#25972;&#21512;&#21040;&#21333;&#19968;&#30340;&#25968;&#25454;&#22240;&#26524;&#32467;&#26500;&#19979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;meta-SCM&#24341;&#20837;&#20102;&#22810;&#20010;&#28508;&#22312;&#22240;&#23376;&#26469;&#34920;&#31034;&#28304;&#19978;&#19979;&#25991;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#21482;&#26377;&#19968;&#20123;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#30446;&#26631;&#26631;&#31614;&#20135;&#29983;&#22240;&#26524;&#24433;&#21709;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23398;&#20064;&#20219;&#21153;&#25152;&#38656;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#21482;&#20351;&#29992;&#36825;&#20123;&#22240;&#32032;&#26469;&#39044;&#27979;&#32473;&#23450;&#20219;&#21153;&#30340;&#32467;&#26524;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#19981;&#28151;&#21512;&#20854;&#20182;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22240;&#26524;&#22240;&#23376;&#30340;&#35782;&#21035;&#12290;&#21463;&#21040;&#22240;&#26524;&#25512;&#26029;&#30340;&#25351;&#23548;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;uchii&#31561;.&#65288;2021&#65289;:&#35770;&#25991;+_&#22240;&#26524;&#35782;&#21035;+_&#25972;&#21512;&#26469;&#23398;&#20064;&#22240;&#26524;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning on a mixture of tasks has improved zero-shot capabilities in natural language processing (NLP). Nevertheless, existing methods often learn features that exhibit correlations between instruction-formatted samples and target labels, rather than causal relationships. Termed as ``spurious correlation'' in statistics, such a correlation may change drastically in a new task, making the effect from the learned features to be misleading. To this end, we develop a meta Structural Causal Model (meta-SCM) to integrate different NLP tasks under a single causal structure of the data. Specifically, the meta-SCM introduces multiple latent factors that represent properties of source context, only some of which causally influence the target labels for a specific task. The key idea is to learn task-required causal factors and only use those to make predictions for a given task. Theoretically, we prove the causal factor can be identified without mixing information from others. Guided b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#35780;&#20272;&#20013;&#30340;&#24726;&#35770;&#65292;&#24182;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#38656;&#35201;&#26816;&#26597;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#25506;&#32034;&#29983;&#25104;&#20248;&#31168;&#19982;&#35780;&#20272;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;</title><link>https://arxiv.org/abs/2402.06204</link><description>&lt;p&gt;
&#29983;&#25104;AI&#35780;&#20272;&#20013;&#30340;&#24726;&#35770;&#65306;&#23427;&#33021;&#35299;&#20915;&#30340;&#38382;&#39064;&#21487;&#33021;&#26080;&#27861;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
The Generative AI Paradox on Evaluation: What It Can Solve, It May Not Evaluate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;AI&#35780;&#20272;&#20013;&#30340;&#24726;&#35770;&#65292;&#24182;&#21457;&#29616;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35780;&#20272;&#20219;&#21153;&#20013;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;&#30740;&#31350;&#31361;&#20986;&#20102;&#38656;&#35201;&#26816;&#26597;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#65292;&#20197;&#21450;&#25506;&#32034;&#29983;&#25104;&#20248;&#31168;&#19982;&#35780;&#20272;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#21363;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#25797;&#38271;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21516;&#26679;&#25797;&#38271;&#20316;&#20026;&#35780;&#20272;&#32773;&#12290;&#25105;&#20204;&#20351;&#29992;TriviaQA&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#19977;&#20010;LLM&#21644;&#19968;&#20010;&#24320;&#28304;LM&#22312;&#38382;&#31572;&#65288;QA&#65289;&#21644;&#35780;&#20272;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;LLM&#22312;&#35780;&#20272;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36739;&#29983;&#25104;&#20219;&#21153;&#20302;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20123;&#19981;&#24544;&#23454;&#30340;&#35780;&#20272;&#24773;&#20917;&#65292;&#27169;&#22411;&#22312;&#20854;&#19981;&#25797;&#38271;&#30340;&#39046;&#22495;&#20013;&#20934;&#30830;&#35780;&#20272;&#31572;&#26696;&#65292;&#31361;&#20986;&#20102;&#38656;&#35201;&#26816;&#26597;LLM&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24544;&#23454;&#24230;&#21644;&#21487;&#20449;&#24230;&#12290;&#26412;&#30740;&#31350;&#26377;&#21161;&#20110;&#29702;&#35299;&#8220;&#29983;&#25104;AI&#24726;&#35770;&#8221;&#65292;&#24378;&#35843;&#20102;&#25506;&#32034;&#29983;&#25104;&#20248;&#31168;&#19982;&#35780;&#20272;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#32852;&#20197;&#21450;&#23457;&#26597;&#27169;&#22411;&#35780;&#20272;&#20013;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the assumption that Large Language Models (LLMs) skilled in generation tasks are equally adept as evaluators. We assess the performance of three LLMs and one open-source LM in Question-Answering (QA) and evaluation tasks using the TriviaQA (Joshi et al., 2017) dataset. Results indicate a significant disparity, with LLMs exhibiting lower performance in evaluation tasks compared to generation tasks. Intriguingly, we discover instances of unfaithful evaluation where models accurately evaluate answers in areas where they lack competence, underscoring the need to examine the faithfulness and trustworthiness of LLMs as evaluators. This study contributes to the understanding of "the Generative AI Paradox" (West et al., 2023), highlighting a need to explore the correlation between generative excellence and evaluation proficiency, and the necessity to scrutinize the faithfulness aspect in model evaluations.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.06196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Large Language Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06196
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#24378;&#22823;&#34920;&#29616;&#12290;&#35813;&#30740;&#31350;&#39046;&#22495;&#21457;&#23637;&#36805;&#36895;&#65292;&#21253;&#25324;&#20102;&#21508;&#31181;&#33879;&#21517;&#30340;LLMs&#12289;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12289;&#20197;&#21450;&#27969;&#34892;&#30340;LLM&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30001;&#20110;&#20854;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#32780;&#21463;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#33258;2022&#24180;11&#26376;ChatGPT&#21457;&#24067;&#20197;&#26469;&#12290;LLMs&#36890;&#36807;&#22312;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#30340;&#25968;&#21313;&#20159;&#21442;&#25968;&#26469;&#33719;&#24471;&#24191;&#27867;&#30340;&#36890;&#29992;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#36825;&#31526;&#21512;&#32553;&#25918;&#23450;&#24459;&#30340;&#39044;&#27979;&#12290;LLMs&#30340;&#30740;&#31350;&#39046;&#22495;&#23613;&#31649;&#38750;&#24120;&#26032;&#65292;&#20294;&#22312;&#35768;&#22810;&#19981;&#21516;&#26041;&#38754;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#20123;&#26368;&#33879;&#21517;&#30340;LLMs&#65292;&#21253;&#25324;&#19977;&#20010;&#27969;&#34892;&#30340;LLM&#31995;&#21015;&#65288;GPT&#12289;LLaMA&#12289;PaLM&#65289;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#29305;&#28857;&#12289;&#36129;&#29486;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#27010;&#36848;&#20102;&#26500;&#24314;&#21644;&#22686;&#24378;LLMs&#30340;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20026;LLM&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#35780;&#20272;&#20934;&#22791;&#30340;&#27969;&#34892;&#25968;&#25454;&#38598;&#65292;&#23457;&#26597;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;LLM&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#27604;&#36739;&#20102;&#20960;&#20010;&#27969;&#34892;LLM&#22312;&#19968;&#32452;&#20195;&#34920;&#24615;&#22522;&#20934;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we co
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35268;&#33539;&#31034;&#20363;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#27599;&#20010;&#26399;&#26395;&#34892;&#20026;&#21482;&#26377;&#19968;&#20010;&#23398;&#20064;&#31034;&#20363;&#65292;&#35780;&#20272;&#20165;&#22312;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#65292;&#23545;&#21021;&#22987;&#27169;&#22411;&#30340;&#20559;&#31163;&#21463;&#38480;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;LoRA&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#21644;MEMIT&#12290;</title><link>https://arxiv.org/abs/2402.06155</link><description>&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#19982;&#35268;&#33539;&#31034;&#20363;
&lt;/p&gt;
&lt;p&gt;
Model Editing with Canonical Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06155
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35268;&#33539;&#31034;&#20363;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#65292;&#27599;&#20010;&#26399;&#26395;&#34892;&#20026;&#21482;&#26377;&#19968;&#20010;&#23398;&#20064;&#31034;&#20363;&#65292;&#35780;&#20272;&#20165;&#22312;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#65292;&#23545;&#21021;&#22987;&#27169;&#22411;&#30340;&#20559;&#31163;&#21463;&#38480;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;LoRA&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#21644;MEMIT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20351;&#29992;&#35268;&#33539;&#31034;&#20363;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;(1)&#27599;&#20010;&#26399;&#26395;&#34892;&#20026;&#21482;&#25552;&#20379;&#19968;&#20010;&#23398;&#20064;&#31034;&#20363;&#65292;(2)&#35780;&#20272;&#20165;&#22312;&#20998;&#24067;&#20043;&#22806;&#36827;&#34892;&#65292;(3)&#23545;&#21021;&#22987;&#27169;&#22411;&#30340;&#20559;&#31163;&#20005;&#26684;&#21463;&#38480;&#21046;&#12290;&#35268;&#33539;&#31034;&#20363;&#26159;&#33391;&#22909;&#34892;&#20026;&#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#20363;&#22914;&#65292;&#8220;&#27611;&#37324;&#27714;&#26031;&#30340;&#39318;&#37117;&#26159;&#36335;&#26131;&#28207;&#8221;&#65292;&#25110;&#32773;&#22351;&#34892;&#20026;&#30340;&#23454;&#20363;&#65292;&#20363;&#22914;&#65292;&#8220;&#30740;&#31350;&#20154;&#21592;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#20919;&#37239;&#26080;&#24773;&#8221;&#12290;&#35780;&#20272;&#38598;&#21253;&#21547;&#26356;&#22797;&#26434;&#30340;&#27599;&#31181;&#34892;&#20026;&#30340;&#31034;&#20363;&#65288;&#20363;&#22914;&#65292;&#22312;&#19968;&#20010;&#27573;&#33853;&#20013;&#21628;&#21483;&#27611;&#37324;&#27714;&#26031;&#30340;&#39318;&#37117;&#65289;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#20010;&#25968;&#25454;&#38598;&#24182;&#20462;&#25913;&#20102;&#21478;&#22806;&#19977;&#20010;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#27169;&#22411;&#32534;&#36753;&#19982;&#35268;&#33539;&#31034;&#20363;&#65292;&#28085;&#30422;&#20102;&#30693;&#35782;&#23494;&#38598;&#22411;&#25913;&#36827;&#12289;&#31038;&#20250;&#20559;&#35265;&#32531;&#35299;&#21644;&#21477;&#27861;&#36793;&#32536;&#26696;&#20363;&#12290;&#22312;&#25105;&#20204;&#23545;Pythia&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LoRA&#20248;&#20110;&#23436;&#20840;&#24494;&#35843;&#21644;MEMIT&#12290;&#28982;&#21518;&#25105;&#20204;&#36716;&#21521;&#20102;Backpack&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#22240;&#20026;&#23427;&#26088;&#22312;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#25913;&#36827;&#12290;Backpack&#23450;&#20041;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;sen...
&lt;/p&gt;
&lt;p&gt;
We introduce model editing with canonical examples, a setting in which (1) a single learning example is provided per desired behavior, (2) evaluation is performed exclusively out-of-distribution, and (3) deviation from an initial model is strictly limited. A canonical example is a simple instance of good behavior, e.g., The capital of Mauritius is Port Louis) or bad behavior, e.g., An aspect of researchers is coldhearted). The evaluation set contains more complex examples of each behavior (like a paragraph in which the capital of Mauritius is called for.) We create three datasets and modify three more for model editing with canonical examples, covering knowledge-intensive improvements, social bias mitigation, and syntactic edge cases. In our experiments on Pythia language models, we find that LoRA outperforms full finetuning and MEMIT. We then turn to the Backpack language model architecture because it is intended to enable targeted improvement. The Backpack defines a large bank of sen
&lt;/p&gt;</description></item><item><title>DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06147</link><description>&lt;p&gt;
DeAL&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#26102;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
DeAL: Decoding-time Alignment for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06147
&lt;/p&gt;
&lt;p&gt;
DeAL&#26159;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29616;&#22312;&#26399;&#26395;&#29983;&#25104;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20869;&#23481;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#27169;&#22411;&#35757;&#32451;&#26102;&#38388;&#23545;&#40784;&#19978;&#65292;&#36890;&#36807;&#35832;&#22914;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#31561;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#26377;&#25928;&#22320;&#25945;&#23548;&#27169;&#22411;&#23545;&#40784;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#26080;&#27861;&#25972;&#21512;&#22810;&#20010;&#33258;&#23450;&#20041;&#22870;&#21169;&#21644;&#20381;&#36182;&#27169;&#22411;&#24320;&#21457;&#32773;&#23545;&#36890;&#29992;&#21644;&#38745;&#24577;&#21407;&#21017;&#30340;&#29702;&#35299;&#26159;&#20027;&#35201;&#23616;&#38480;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27531;&#30041;&#24046;&#36317;&#20197;&#21450;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#20063;&#20540;&#24471;&#36136;&#30097;&#65288;&#20363;&#22914;&#65292;&#21363;&#20351;&#22312;&#23433;&#20840;&#35757;&#32451;&#21518;&#20173;&#28982;&#23481;&#26131;&#34987;&#36234;&#29425;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeAL&#65292;&#19968;&#20010;&#20801;&#35768;&#29992;&#25143;&#33258;&#23450;&#20041;&#22870;&#21169;&#20989;&#25968;&#24182;&#23454;&#29616;&#35299;&#30721;&#26102;&#23545;&#40784;LLMs&#65288;DeAL&#65289;&#30340;&#26694;&#26550;&#12290;&#26680;&#24515;&#24605;&#24819;&#22312;&#20110;&#23558;&#35299;&#30721;&#35270;&#20026;&#19968;&#20010;&#21551;&#21457;&#24335;&#24341;&#23548;&#30340;&#25628;&#32034;&#36807;&#31243;&#65292;&#24182;&#20419;&#20351;&#20351;&#29992;&#21508;&#31181;&#23545;&#40784;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20197;&#32534;&#31243;&#32422;&#26463;&#20026;&#20363;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are nowadays expected to generate content aligned with human preferences. Current work focuses on alignment at model training time, through techniques such as Reinforcement Learning with Human Feedback (RLHF). However, it is unclear if such methods are an effective choice to teach alignment objectives to the model. First, the inability to incorporate multiple, custom rewards and reliance on a model developer's view of universal and static principles are key limitations. Second, the residual gaps in model training and the reliability of such approaches are also questionable (e.g. susceptibility to jail-breaking even after safety training). To address these, we propose DeAL, a framework that allows the user to customize reward functions and enables Decoding-time Alignment of LLMs (DeAL). At its core, we view decoding as a heuristic-guided search process and facilitate the use of a wide variety of alignment objectives. Our experiments with programmatic constra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#39537;&#21160;&#30340;&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#21477;&#23376;&#34917;&#20840;&#29615;&#22659;&#20013;&#24378;&#21046;&#25191;&#34892;&#29305;&#23450;&#20462;&#36766;&#20851;&#31995;&#30340;&#36981;&#24490;&#65292;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#65292;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;</title><link>https://arxiv.org/abs/2402.06125</link><description>&lt;p&gt;
&#24102;&#26377;&#35299;&#26512;&#39537;&#21160;&#30340;&#20462;&#36766;&#25511;&#21046;&#26041;&#27861;&#30340;&#35821;&#35328;&#27169;&#22411;&#21477;&#23376;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Language Model Sentence Completion with a Parser-Driven Rhetorical Control Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#31639;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#39537;&#21160;&#30340;&#35299;&#30721;&#26041;&#26696;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#21477;&#23376;&#34917;&#20840;&#29615;&#22659;&#20013;&#24378;&#21046;&#25191;&#34892;&#29305;&#23450;&#20462;&#36766;&#20851;&#31995;&#30340;&#36981;&#24490;&#65292;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#32463;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#39564;&#35777;&#65292;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25511;&#21046;&#24615;&#25991;&#26412;&#29983;&#25104;&#65288;CTG&#65289;&#26088;&#22312;&#24341;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#31526;&#21512;&#29305;&#23450;&#35201;&#27714;&#30340;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;CTG&#31639;&#27861;&#65292;&#22312;LLM&#21477;&#23376;&#34917;&#20840;&#29615;&#22659;&#20013;&#36890;&#36807;&#35299;&#26512;&#39537;&#21160;&#30340;&#35299;&#30721;&#26041;&#26696;&#24378;&#21046;&#25191;&#34892;&#29305;&#23450;&#20462;&#36766;&#20851;&#31995;&#30340;&#36981;&#24490;&#65292;&#32780;&#26080;&#38656;&#27169;&#22411;&#24494;&#35843;&#12290;&#35813;&#26041;&#27861;&#24050;&#36890;&#36807;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#12290;&#20195;&#30721;&#21487;&#22312;GitHub&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlled text generation (CTG) seeks to guide large language model (LLM) output to produce text that conforms to desired criteria. The current study presents a novel CTG algorithm that enforces adherence toward specific rhetorical relations in an LLM sentence-completion context by a parser-driven decoding scheme that requires no model fine-tuning. The method is validated both with automatic and human evaluation. The code is accessible on GitHub.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#26377;&#30417;&#30563;&#24494;&#35843;&#20013;&#25968;&#25454;&#36873;&#25321;&#30340;&#30452;&#35273;&#12290;&#32771;&#34385;SFT&#30340;&#32932;&#27973;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#35201;&#31034;&#33539;&#24212;&#30528;&#37325;&#21453;&#26144;&#20154;&#31867;&#24335;&#30340;&#20114;&#21160;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#38271;&#22238;&#31572;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;SFT&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06094</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#26377;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Selection for Supervised Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#26377;&#30417;&#30563;&#24494;&#35843;&#20013;&#25968;&#25454;&#36873;&#25321;&#30340;&#30452;&#35273;&#12290;&#32771;&#34385;SFT&#30340;&#32932;&#27973;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#35201;&#31034;&#33539;&#24212;&#30528;&#37325;&#21453;&#26144;&#20154;&#31867;&#24335;&#30340;&#20114;&#21160;&#12290;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#38271;&#22238;&#31572;&#30340;&#23454;&#20363;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;SFT&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26377;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#24050;&#25104;&#20026;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#22522;&#26412;&#25216;&#26415;&#65292;&#20294;&#23427;&#34987;&#35748;&#20026;&#26159;&#32932;&#27973;&#30340;&#65292;&#20854;&#26412;&#36136;&#26159;&#23398;&#20064;&#26679;&#24335;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#20102;&#23545;&#20110;SFT&#32780;&#35328;&#25968;&#25454;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#26174;&#31034;&#20102;&#20351;&#29992;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23376;&#38598;&#36827;&#34892;&#24494;&#35843;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#23545;&#20110;SFT&#30340;&#25968;&#25454;&#36873;&#25321;&#30340;&#30452;&#35273;&#12290;&#32771;&#34385;&#21040;SFT&#30340;&#32932;&#27973;&#24615;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;SFT&#30340;&#37325;&#35201;&#31034;&#33539;&#24212;&#35813;&#30528;&#37325;&#21453;&#26144;&#20154;&#31867;&#24335;&#30340;&#20114;&#21160;&#65292;&#32780;&#19981;&#26159;&#25968;&#25454;&#36136;&#37327;&#25110;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#35780;&#20272;&#19968;&#20010;&#31034;&#33539;&#21453;&#26144;&#20154;&#31867;&#39118;&#26684;&#30340;&#31243;&#24230;&#24182;&#19981;&#31616;&#21333;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#30340;&#21021;&#27493;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36873;&#25321;&#20855;&#26377;&#38271;&#22238;&#31572;&#30340;&#23454;&#20363;&#23545;&#20110;SFT&#32780;&#35328;&#27604;&#20351;&#29992;&#23436;&#25972;&#25968;&#25454;&#38598;&#25110;&#26681;&#25454;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#36873;&#25321;&#23454;&#20363;&#26356;&#20026;&#26377;&#25928;&#12290;&#25105;&#20204;&#20551;&#35774;&#36825;&#26679;&#19968;&#20010;&#31616;&#21333;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#21040;&#25152;&#38656;&#30340;&#31034;&#33539;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although supervised finetuning (SFT) has emerged as an essential technique to align large language models with humans, it is considered superficial, with style learning being its nature. At the same time, recent works indicate the importance of data selection for SFT, showing that finetuning with high-quality and diverse subsets of the original dataset leads to superior downstream performance. In this work, we rethink the intuition behind data selection for SFT. Considering SFT is superficial, we propose that essential demonstrations for SFT should focus on reflecting human-like interactions instead of data quality or diversity. However, it is not straightforward to directly assess to what extent a demonstration reflects human styles. Towards an initial attempt in this direction, we find selecting instances with long responses is surprisingly more effective for SFT than utilizing full datasets or instances selected based on quality and diversity. We hypothesize that such a simple heuri
&lt;/p&gt;</description></item><item><title>LightCAM&#26159;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;&#30340;D-TDNN&#35828;&#35805;&#20154;&#39564;&#35777;&#23454;&#29616;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#65292;&#23427;&#22312;VoxCeleb&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06073</link><description>&lt;p&gt;
LightCAM: &#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;&#30340;D-TDNN&#35828;&#35805;&#20154;&#39564;&#35777;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
LightCAM: A Fast and Light Implementation of Context-Aware Masking based D-Tdnn for Speaker Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06073
&lt;/p&gt;
&lt;p&gt;
LightCAM&#26159;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;&#30340;D-TDNN&#35828;&#35805;&#20154;&#39564;&#35777;&#23454;&#29616;&#65292;&#36890;&#36807;&#37319;&#29992;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#27169;&#22359;&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#65292;&#23427;&#22312;VoxCeleb&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(TDNN)&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#24037;&#19994;&#29615;&#22659;&#20013;&#38590;&#20197;&#23454;&#26045;&#12290;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#23631;&#34109;(CAM)&#27169;&#22359;&#30340;&#23494;&#38598;&#36830;&#36890;&#26102;&#24310;&#31070;&#32463;&#32593;&#32476;(D-TDNN)&#24050;&#32463;&#35777;&#26126;&#26159;&#19968;&#31181;&#38477;&#20302;&#22797;&#26434;&#24615;&#24182;&#20445;&#25345;&#31995;&#32479;&#24615;&#33021;&#30340;&#39640;&#25928;&#32467;&#26500;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#36731;&#37327;&#32423;&#27169;&#22411;LightCAM&#65292;&#23427;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#28145;&#24230;&#21487;&#20998;&#31163;&#21367;&#31215;&#27169;&#22359;(DSM)&#21644;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;(MFA)&#20197;&#23454;&#29616;&#19981;&#21516;&#23618;&#27425;&#30340;&#29305;&#24449;&#34701;&#21512;&#12290;&#22312;VoxCeleb&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#23427;&#22312;VoxCeleb1-O&#19978;&#23454;&#29616;&#20102;0.83&#30340;&#31561;&#38169;&#35823;&#29575;(EER)&#21644;0.0891&#30340;&#26368;&#23567;&#26816;&#27979;&#20195;&#20215;&#22240;&#23376;(MinDCF)&#65292;&#36229;&#36807;&#20102;&#20854;&#20182;&#20027;&#27969;&#30340;&#35828;&#35805;&#20154;&#39564;&#35777;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#22797;&#26434;&#24230;&#20998;&#26512;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#26356;&#24555;&#30340;&#25512;&#29702;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional Time Delay Neural Networks (TDNN) have achieved state-of-the-art performance at the cost of high computational complexity and slower inference speed, making them difficult to implement in an industrial environment. The Densely Connected Time Delay Neural Network (D-TDNN) with Context Aware Masking (CAM) module has proven to be an efficient structure to reduce complexity while maintaining system performance. In this paper, we propose a fast and lightweight model, LightCAM, which further adopts a depthwise separable convolution module (DSM) and uses multi-scale feature aggregation (MFA) for feature fusion at different levels. Extensive experiments are conducted on VoxCeleb dataset, the comparative results show that it has achieved an EER of 0.83 and MinDCF of 0.0891 in VoxCeleb1-O, which outperforms the other mainstream speaker verification methods. In addition, complexity analysis further demonstrates that the proposed architecture has lower computational cost and faster inf
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#21644;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#12290;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2402.06049</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Limits of Large Language Models in Debating Humans
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06049
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#20154;&#31867;&#36777;&#35770;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#23613;&#31649;&#23427;&#20204;&#33021;&#22815;&#34701;&#20837;&#21644;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#12290;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#38656;&#35201;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#19982;&#20154;&#31867;&#30340;&#20114;&#21160;&#20013;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#38543;&#21518;&#65292;&#23558;&#23427;&#20204;&#20316;&#20026;&#20154;&#24037;&#20195;&#34920;&#21644;&#26367;&#20195;&#21697;&#36827;&#34892;&#31038;&#20250;&#23398;&#23454;&#39564;&#30340;&#28508;&#22312;&#24212;&#29992;&#26159;&#19968;&#20010;&#20196;&#20154;&#28608;&#21160;&#30340;&#21069;&#26223;&#12290;&#20294;&#26159;&#36825;&#20010;&#24819;&#27861;&#26377;&#22810;&#21487;&#34892;&#21602;&#65311;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#19968;&#39033;&#39044;&#20808;&#27880;&#20876;&#30340;&#30740;&#31350;&#26469;&#27979;&#35797;&#29616;&#38454;&#27573;LLMs&#30340;&#23616;&#38480;&#24615;&#65292;&#35813;&#30740;&#31350;&#23558;&#30495;&#23454;&#30340;&#20154;&#31867;&#19982;&#25198;&#28436;&#20154;&#31867;&#30340;LLM&#20195;&#29702;&#32467;&#21512;&#36215;&#26469;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#25506;&#35752;&#36777;&#35770;&#20026;&#22522;&#30784;&#30340;&#24847;&#35265;&#20849;&#35782;&#24418;&#25104;&#22312;&#19977;&#31181;&#29615;&#22659;&#19979;&#30340;&#24773;&#20917;&#65306;&#20165;&#20154;&#31867;&#12289;&#20195;&#29702;&#21644;&#20154;&#31867;&#12289;&#20165;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;LLM&#20195;&#29702;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#65292;&#24182;&#35780;&#20272;&#23427;&#20204;&#22312;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#21542;&#19982;&#20154;&#31867;&#30456;&#20284;&#12290;&#25105;&#20204;&#21457;&#29616;LLMs&#33021;&#22815;&#34701;&#20837;&#24182;&#20419;&#36827;&#20154;&#31867;&#30340;&#24037;&#20316;&#25928;&#29575;&#65292;&#20294;&#22312;&#36777;&#35770;&#20013;&#30340;&#35828;&#26381;&#21147;&#36739;&#24369;&#65292;&#26368;&#32456;&#34892;&#20026;&#19982;&#20154;&#31867;&#26377;&#25152;&#20559;&#31163;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#36825;&#20123;&#20027;&#35201;&#32570;&#38519;&#65292;&#24182;&#39044;&#35745;&#22312;&#25104;&#20026;&#21487;&#34892;&#30340;&#36777;&#25163;&#20043;&#21069;&#65292;LLMs&#24517;&#39035;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable promise in their ability to interact proficiently with humans. Subsequently, their potential use as artificial confederates and surrogates in sociological experiments involving conversation is an exciting prospect. But how viable is this idea? This paper endeavors to test the limits of current-day LLMs with a pre-registered study integrating real people with LLM agents acting as people. The study focuses on debate-based opinion consensus formation in three environments: humans only, agents and humans, and agents only. Our goal is to understand how LLM agents influence humans, and how capable they are in debating like humans. We find that LLMs can blend in and facilitate human productivity but are less convincing in debate, with their behavior ultimately deviating from human's. We elucidate these primary failings and anticipate that LLMs must evolve further before being viable debaters.
&lt;/p&gt;</description></item><item><title>OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2402.06044</link><description>&lt;p&gt;
&#24320;&#25918;&#29702;&#35770;-&#24515;&#28789;&#65288;OpenToM&#65289;&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#28789;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06044
&lt;/p&gt;
&lt;p&gt;
OpenToM&#26159;&#19968;&#20010;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24515;&#29702;&#29702;&#35299;&#33021;&#21147;&#30340;&#20840;&#38754;&#22522;&#20934;&#65292;&#36890;&#36807;&#25552;&#20379;&#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#12289;&#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#12289;&#20197;&#21450;&#25361;&#25112;&#27169;&#22411;&#23545;&#24515;&#29702;&#29366;&#24577;&#30340;&#29702;&#35299;&#33021;&#21147;&#30340;&#38382;&#39064;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#29702;&#35299;&#29289;&#29702;&#19990;&#30028;&#19982;&#24515;&#29702;&#19990;&#30028;&#30340;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#30340;&#20248;&#21183;&#21644;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24515;&#29702;&#29702;&#35770;&#65288;N-ToM&#65289;&#26159;&#26426;&#22120;&#29702;&#35299;&#21644;&#36319;&#36394;&#20182;&#20154;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#65292;&#22312;&#24320;&#21457;&#20855;&#26377;&#31038;&#20132;&#26234;&#33021;&#30340;&#20195;&#29702;&#31243;&#24207;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;N-ToM&#22522;&#20934;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#27169;&#31946;&#21644;&#20154;&#24037;&#25925;&#20107;&#30340;&#23384;&#22312;&#65292;&#32570;&#20047;&#20010;&#24615;&#29305;&#24449;&#21644;&#20559;&#22909;&#65292;&#32570;&#20047;&#28041;&#21450;&#35282;&#33394;&#24515;&#29702;&#24515;&#24577;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#25552;&#20986;&#30340;&#38382;&#39064;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;OpenToM&#65292;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;N-ToM&#30340;&#22522;&#20934;&#65292;&#20197; (1) &#26356;&#38271;&#12289;&#26356;&#28165;&#26224;&#30340;&#21465;&#20107;&#25925;&#20107;&#65292;(2) &#20855;&#26377;&#26126;&#30830;&#20010;&#24615;&#29305;&#24449;&#30340;&#35282;&#33394;&#65292;(3) &#35302;&#21457;&#35282;&#33394;&#24847;&#22270;&#30340;&#34892;&#21160;&#65292;&#20197;&#21450; (4) &#35774;&#35745;&#26088;&#22312;&#25361;&#25112;LLMs&#23545;&#24314;&#27169;&#35282;&#33394;&#22312;&#29289;&#29702;&#21644;&#24515;&#29702;&#19990;&#30028;&#30340;&#24515;&#29702;&#29366;&#24577;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;OpenToM&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#24314;&#27169;&#29289;&#29702;&#19990;&#30028;&#30340;&#19968;&#20123;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36319;&#36394;&#35282;&#33394;&#24515;&#29702;&#29366;&#24577;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26426;&#22120;&#32763;&#35793;&#21644;GPT-4&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#33258;&#21160;&#24615;&#21035;&#20013;&#31435;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#29983;&#25104;&#24615;&#21035;&#20013;&#31435;&#32763;&#35793;&#26041;&#38754;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06041</link><description>&lt;p&gt;
&#23545;&#33258;&#21160;&#24615;&#21035;&#20013;&#31435;&#32763;&#35793;&#38656;&#27714;&#30340;&#36805;&#36895;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
A Prompt Response to the Demand for Automatic Gender-Neutral Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#26426;&#22120;&#32763;&#35793;&#21644;GPT-4&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#33258;&#21160;&#24615;&#21035;&#20013;&#31435;&#32763;&#35793;&#30340;&#28508;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#22312;&#29983;&#25104;&#24615;&#21035;&#20013;&#31435;&#32763;&#35793;&#26041;&#38754;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36991;&#20813;&#20559;&#35265;&#21644;&#19981;&#36866;&#24403;&#30340;&#20108;&#20803;&#20551;&#35774;&#30340;&#24615;&#21035;&#20013;&#31435;&#32763;&#35793;&#65288;GNT&#65289;&#26159;&#21019;&#36896;&#26356;&#21253;&#23481;&#24615;&#32763;&#35793;&#25216;&#26415;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#22312;&#27492;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#21463;&#21040;&#20102;&#26080;&#19987;&#29992;&#24179;&#34892;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#25968;&#25454;&#23545;&#20110;&#35843;&#25972;MT&#31995;&#32479;&#20197;&#28385;&#36275;&#20013;&#31435;&#32422;&#26463;&#26159;&#24517;&#35201;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#36804;&#20170;&#20026;&#27490;&#26410;&#26366;&#39044;&#35265;&#21040;&#30340;&#21487;&#33021;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#22312;&#25552;&#20379;&#26126;&#30830;&#25351;&#31034;&#26102;&#22312;&#21508;&#31181;&#65288;&#23376;&#65289;&#20219;&#21153;&#20013;&#28789;&#27963;&#24615;&#30340;&#29420;&#29305;&#20248;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;MT&#19982;&#27969;&#34892;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#20102;&#33258;&#21160;&#21270;GNT&#30340;&#28508;&#21147;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25163;&#24037;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#23454;&#35777;&#35282;&#24230;&#25581;&#31034;&#20102;&#29616;&#26377;MT&#31995;&#32479;&#22312;&#29983;&#25104;GNT&#26041;&#38754;&#30340;&#22266;&#26377;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#25552;&#31034;&#20013;&#31435;&#24615;&#25152;&#24102;&#26469;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gender-neutral translation (GNT) that avoids biased and undue binary assumptions is a pivotal challenge for the creation of more inclusive translation technologies. Advancements for this task in Machine Translation (MT), however, are hindered by the lack of dedicated parallel data, which are necessary to adapt MT systems to satisfy neutral constraints. For such a scenario, large language models offer hitherto unforeseen possibilities, as they come with the distinct advantage of being versatile in various (sub)tasks when provided with explicit instructions. In this paper, we explore this potential to automate GNT by comparing MT with the popular GPT-4 model. Through extensive manual analyses, our study empirically reveals the inherent limitations of current MT systems in generating GNTs and provides valuable insights into the potential and challenges associated with prompting for neutrality.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;GPT-4V&#27169;&#22411;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#25991;&#21270;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#34920;&#29616;&#36739;&#24369;&#12290;&#22312;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20013;&#65292;GPT-4V&#22312;&#25991;&#21270;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#21407;&#25991;&#12290;</title><link>https://arxiv.org/abs/2402.06015</link><description>&lt;p&gt;
&#22312;GPT-4V&#27169;&#22411;&#20013;&#25506;&#32034;&#35270;&#35273;&#25991;&#21270;&#24847;&#35782;&#65306;&#19968;&#39033;&#20840;&#38754;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploring Visual Culture Awareness in GPT-4V: A Comprehensive Probing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06015
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;GPT-4V&#27169;&#22411;&#19978;&#36827;&#34892;&#20840;&#38754;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#35813;&#27169;&#22411;&#22312;&#35782;&#21035;&#25991;&#21270;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20173;&#34920;&#29616;&#36739;&#24369;&#12290;&#22312;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20013;&#65292;GPT-4V&#22312;&#25991;&#21270;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#21407;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102; considerable&#30340;&#20851;&#27880;&#65292;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;&#23613;&#31649;&#24050;&#32463;&#20570;&#20986;&#20102;&#22810;&#26041;&#38754;&#30340;&#21162;&#21147;&#26469;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#20294;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GPT-4V&#27169;&#22411;&#22312;&#35270;&#35273;&#25991;&#21270;&#24847;&#35782;&#26041;&#38754;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20351;&#29992;MaRVL&#22522;&#20934;&#25968;&#25454;&#38598;&#24191;&#27867;&#25506;&#32034;&#20102;GPT-4V&#65292;&#26088;&#22312;&#35843;&#26597;&#20854;&#22312;&#35270;&#35273;&#29702;&#35299;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#25991;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#19982;&#35270;&#35273;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21363;&#26631;&#39064;&#20998;&#31867;&#12289;&#25104;&#23545;&#26631;&#39064;&#29983;&#25104;&#21644;&#25991;&#21270;&#26631;&#31614;&#36873;&#25321;&#65292;&#20197;&#31995;&#32479;&#22320;&#28145;&#20837;&#30740;&#31350;&#32454;&#31890;&#24230;&#30340;&#35270;&#35273;&#25991;&#21270;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4V&#22312;&#35782;&#21035;&#25991;&#21270;&#27010;&#24565;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#35821;&#35328;&#65288;&#22914;&#27888;&#31859;&#23572;&#35821;&#21644;&#26031;&#29926;&#24076;&#37324;&#35821;&#65289;&#20013;&#20173;&#28982;&#34920;&#29616;&#36739;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;GPT-4V&#22312;&#22270;&#20687;&#23383;&#24149;&#20219;&#21153;&#20013;&#35777;&#26126;&#22312;&#25991;&#21270;&#30456;&#20851;&#24615;&#26041;&#38754;&#20248;&#20110;&#21407;&#25991;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained large Vision-Language models have drawn considerable interest in recent years due to their remarkable performance. Despite considerable efforts to assess these models from diverse perspectives, the extent of visual cultural awareness in the state-of-the-art GPT-4V model remains unexplored. To tackle this gap, we extensively probed GPT-4V using the MaRVL benchmark dataset, aiming to investigate its capabilities and limitations in visual understanding with a focus on cultural aspects. Specifically, we introduced three visual related tasks, i.e. caption classification, pairwise captioning, and culture tag selection, to systematically delve into fine-grained visual cultural evaluation. Experimental results indicate that GPT-4V excels at identifying cultural concepts but still exhibits weaker performance in low-resource languages, such as Tamil and Swahili. Notably, through human evaluation, GPT-4V proves to be more culturally relevant in image captioning tasks than the original 
&lt;/p&gt;</description></item><item><title>&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.05964</link><description>&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Survey on Transformer Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05964
&lt;/p&gt;
&lt;p&gt;
&#12298;Transformer&#21387;&#32553;&#35843;&#30740;&#12299;&#26159;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#27169;&#22411;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#26159;&#22312;&#23454;&#38469;&#35774;&#22791;&#19978;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24517;&#35201;&#27493;&#39588;&#12290;&#37492;&#20110;Transformer&#30340;&#29420;&#29305;&#26550;&#26500;&#65292;&#20855;&#26377;&#20132;&#26367;&#30340;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFN&#65289;&#27169;&#22359;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#12290;&#36825;&#20123;&#21387;&#32553;&#26041;&#27861;&#30340;&#25928;&#29575;&#20063;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#37325;&#26032;&#35757;&#32451;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#22411;&#27169;&#22411;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#35843;&#30740;&#25552;&#20379;&#20102;&#23545;&#26368;&#36817;&#21387;&#32553;&#26041;&#27861;&#30340;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#22312;Transformer&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#12290;&#21387;&#32553;&#26041;&#27861;&#20027;&#35201;&#20998;&#20026;&#20462;&#21098;&#12289;&#37327;&#21270;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#39640;&#25928;&#26550;&#26500;&#35774;&#35745;&#22235;&#20010;&#31867;&#21035;&#12290;&#22312;&#27599;&#20010;&#31867;&#21035;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#21387;&#32553;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Large models based on the Transformer architecture play increasingly vital roles in artificial intelligence, particularly within the realms of natural language processing (NLP) and computer vision (CV). Model compression methods reduce their memory and computational cost, which is a necessary step to implement the transformer models on practical devices. Given the unique architecture of transformer, featuring alternative attention and Feedforward Neural Network (FFN) modules, specific compression techniques are required. The efficiency of these compression methods is also paramount, as it is usually impractical to retrain large models on the entire training dataset.This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to transformer models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design. In each category, we discuss compression methods
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30456;&#32467;&#21512;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20026;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2402.05952</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#36827;&#22270;&#34920;&#31034;&#23398;&#20064;&#65306;&#25216;&#26415;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30456;&#32467;&#21512;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#28145;&#20837;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20026;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22270;&#34920;&#31034;&#23398;&#20064;&#65288;GRL&#65289;&#30340;&#25972;&#21512;&#26631;&#24535;&#30528;&#20998;&#26512;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#30340;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#31181;&#21512;&#20316;&#21033;&#29992;LLM&#30340;&#20808;&#36827;&#35821;&#35328;&#33021;&#21147;&#26469;&#25913;&#36827;&#22270;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#21644;&#36866;&#24212;&#24615;&#65292;&#20174;&#32780;&#25299;&#23485;&#20102;GRL&#30340;&#33539;&#22260;&#21644;&#28508;&#21147;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#22823;&#37327;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#23558;LLM&#38598;&#25104;&#21040;&#22270;&#39046;&#22495;&#20013;&#65292;&#20294;&#32570;&#20047;&#19968;&#20221;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#26680;&#24515;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#30340;&#20840;&#38754;&#32508;&#36848;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#26469;&#20998;&#35299;&#36825;&#20123;&#27169;&#22411;&#20026;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21644;&#25805;&#20316;&#25216;&#26415;&#65292;&#20174;&#26032;&#30340;&#25216;&#26415;&#35282;&#24230;&#28145;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#26368;&#36817;&#30340;&#25991;&#29486;&#20998;&#35299;&#20026;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#32452;&#32455;&#32773;&#65292;&#20197;&#21450;&#20004;&#20010;&#25805;&#20316;&#25216;&#26415;&#65292;&#21253;&#25324;&#38598;&#25104;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#25581;&#31034;&#20986;&#26377;&#25928;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#35757;&#32451;&#31574;&#30053;&#30340;&#35201;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additio
&lt;/p&gt;</description></item><item><title>DE$^3$-BERT&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;BERT&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05948</link><description>&lt;p&gt;
DE$^3$-BERT: &#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#30340;&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#65292;&#29992;&#20110;BERT
&lt;/p&gt;
&lt;p&gt;
DE$^3$-BERT: Distance-Enhanced Early Exiting for BERT based on Prototypical Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05948
&lt;/p&gt;
&lt;p&gt;
DE$^3$-BERT&#26159;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#32593;&#32476;&#21644;&#36317;&#31163;&#24230;&#37327;&#30340;&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26694;&#26550;&#65292;&#29992;&#20110;&#25552;&#39640;BERT&#31561;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#25191;&#34892;&#30340;&#23618;&#25968;&#65292;&#25552;&#39640;&#20102;&#20687;BERT&#36825;&#26679;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26089;&#26399;&#20572;&#27490;&#26041;&#27861;&#20165;&#32771;&#34385;&#20102;&#26469;&#33258;&#21333;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#23616;&#37096;&#20449;&#24687;&#26469;&#30830;&#23450;&#26089;&#26399;&#20572;&#27490;&#30340;&#25351;&#26631;&#65292;&#32780;&#26410;&#21033;&#29992;&#26679;&#26412;&#32676;&#20307;&#25552;&#20379;&#30340;&#20840;&#23616;&#20449;&#24687;&#12290;&#36825;&#23548;&#33268;&#23545;&#39044;&#27979;&#27491;&#30830;&#24615;&#30340;&#20272;&#35745;&#19981;&#22815;&#20934;&#30830;&#65292;&#20174;&#32780;&#20135;&#29983;&#38169;&#35823;&#30340;&#26089;&#26399;&#20572;&#27490;&#20915;&#31574;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26377;&#25928;&#32467;&#21512;&#23616;&#37096;&#21644;&#20840;&#23616;&#20449;&#24687;&#20197;&#30830;&#20445;&#21487;&#38752;&#30340;&#26089;&#26399;&#20572;&#27490;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#21407;&#22411;&#32593;&#32476;&#23398;&#20064;&#31867;&#21035;&#21407;&#22411;&#65292;&#24182;&#35774;&#35745;&#20102;&#26679;&#26412;&#21644;&#31867;&#21035;&#21407;&#22411;&#20043;&#38388;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#20840;&#23616;&#20449;&#24687;&#26469;&#20272;&#35745;&#26089;&#26399;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;DE$^3$-BERT&#22686;&#24378;&#36317;&#31163;&#26089;&#26399;&#20572;&#27490;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early exiting has demonstrated its effectiveness in accelerating the inference of pre-trained language models like BERT by dynamically adjusting the number of layers executed. However, most existing early exiting methods only consider local information from an individual test sample to determine their exiting indicators, failing to leverage the global information offered by sample population. This leads to suboptimal estimation of prediction correctness, resulting in erroneous exiting decisions. To bridge the gap, we explore the necessity of effectively combining both local and global information to ensure reliable early exiting during inference. Purposefully, we leverage prototypical networks to learn class prototypes and devise a distance metric between samples and class prototypes. This enables us to utilize global information for estimating the correctness of early predictions. On this basis, we propose a novel Distance-Enhanced Early Exiting framework for BERT (DE$^3$-BERT). DE$^3
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#27010;&#29575;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05939</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65306;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Awareness of Large Language Models Under Code Distribution Shifts: A Benchmark Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22823;&#35268;&#27169;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#27010;&#29575;&#26041;&#27861;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32534;&#31243;&#35821;&#35328;&#20998;&#26512;&#65292;&#20197;&#25552;&#39640;&#20154;&#31867;&#29983;&#20135;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#21508;&#31181;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#36755;&#20986;&#19981;&#19968;&#33268;&#12290;&#23613;&#31649;&#20247;&#25152;&#21608;&#30693;&#65292;&#27010;&#29575;&#26041;&#27861;&#36890;&#36807;&#19981;&#30830;&#23450;&#24615;&#26657;&#20934;&#21644;&#20272;&#35745;&#21487;&#20197;&#20943;&#36731;&#27492;&#31867;&#24433;&#21709;&#65292;&#20294;&#19982;&#20854;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#30456;&#27604;&#65292;&#23427;&#20204;&#22312;&#35821;&#35328;&#39046;&#22495;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#19977;&#31181;&#20195;&#30721;&#20998;&#24067;&#36716;&#31227;&#30340;&#23454;&#38469;&#27169;&#24335;&#65292;&#24378;&#24230;&#21508;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;CodeLlama&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#27010;&#29575;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#21487;&#20197;&#25552;&#39640;CodeLlama&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#24847;&#35782;&#65292;&#25552;&#39640;&#20102;&#26657;&#20934;&#36136;&#37327;&#21644;&#26356;&#39640;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#31934;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#25581;&#31034;&#20102;&#19981;&#21516;&#26631;&#20934;&#19979;&#30340;&#24615;&#33021;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have been widely employed in programming language analysis to enhance human productivity. Yet, their reliability can be compromised by various code distribution shifts, leading to inconsistent outputs. While probabilistic methods are known to mitigate such impact through uncertainty calibration and estimation, their efficacy in the language domain remains underexplored compared to their application in image-based tasks. In this work, we first introduce a large-scale benchmark dataset, incorporating three realistic patterns of code distribution shifts at varying intensities. Then we thoroughly investigate state-of-the-art probabilistic methods applied to CodeLlama using these shifted code snippets. We observe that these methods generally improve the uncertainty awareness of CodeLlama, with increased calibration quality and higher uncertainty estimation~(UE) precision. However, our study further reveals varied performance dynamics across different criteria (e
&lt;/p&gt;</description></item><item><title>LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05130</link><description>&lt;p&gt;
LB-KBQA: &#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
LB-KBQA: Large-language-model and BERT based Knowledge-Based Question and Answering System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05130
&lt;/p&gt;
&lt;p&gt;
LB-KBQA&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;BERT&#30340;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#65292;&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#24110;&#21161;&#65292;&#33021;&#22815;&#25552;&#39640;&#24847;&#22270;&#35782;&#21035;&#30340;&#24615;&#33021;&#21644;&#35299;&#20915;&#35821;&#35328;&#22810;&#26679;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22240;&#20854;&#26032;&#20852;&#30340;&#33021;&#21147;&#32780;&#36171;&#20104;&#20102;&#21508;&#20010;&#39046;&#22495;&#30340;&#21147;&#37327;&#65292;&#20854;&#20013;&#19968;&#20010;&#20856;&#22411;&#30340;&#24212;&#29992;&#39046;&#22495;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#29983;&#25104;&#24335;AI&#30340;&#20856;&#22411;&#24212;&#29992;&#39046;&#22495;&#20043;&#19968;&#26159;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;LLM&#30340;&#33021;&#21147;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#31995;&#32479;&#24847;&#22270;&#35782;&#21035;&#24615;&#33021;&#30340;&#38556;&#30861;&#65292;&#36825;&#28304;&#33258;&#35821;&#35328;&#22810;&#26679;&#24615;&#21644;&#26032;&#20986;&#29616;&#30340;&#24847;&#22270;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;AI&#30340;&#24847;&#22270;&#35782;&#21035;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#35821;&#20041;&#35299;&#26512;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#22312;&#24847;&#22270;&#35782;&#21035;&#26041;&#38754;&#21463;&#21040;&#26377;&#38480;&#30340;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;BERT&#30340;&#26032;&#22411;KBQA&#31995;&#32479;&#65288;LB-KBQA&#65289;&#12290;&#22312;&#29983;&#25104;&#24335;AI&#30340;&#24110;&#21161;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21040;&#8230;&#8230;&#65288;&#30053;&#65289;
&lt;/p&gt;
&lt;p&gt;
Generative Artificial Intelligence (AI), because of its emergent abilities, has empowered various fields, one typical of which is large language models (LLMs). One of the typical application fields of Generative AI is large language models (LLMs), and the natural language understanding capability of LLM is dramatically improved when compared with conventional AI-based methods. The natural language understanding capability has always been a barrier to the intent recognition performance of the Knowledge-Based-Question-and-Answer (KBQA) system, which arises from linguistic diversity and the newly appeared intent. Conventional AI-based methods for intent recognition can be divided into semantic parsing-based and model-based approaches. However, both of the methods suffer from limited resources in intent recognition. To address this issue, we propose a novel KBQA system based on a Large Language Model(LLM) and BERT (LB-KBQA). With the help of generative AI, our proposed method could detect 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04838</link><description>&lt;p&gt;
PaDeLLM-NER&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24182;&#34892;&#35299;&#30721;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PaDeLLM-NER: Parallel Decoding in Large Language Models for Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;PaDeLLM-NER&#65292;&#19968;&#31181;&#33021;&#22815;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#24182;&#34892;&#35299;&#30721;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#29983;&#25104;&#24310;&#36831;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#36136;&#37327;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20943;&#23569;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#29983;&#25104;&#24310;&#36831;&#12290;LLMs&#30340;&#39640;&#24310;&#36831;&#30340;&#20027;&#35201;&#21407;&#22240;&#26159;&#39034;&#24207;&#35299;&#30721;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#33258;&#22238;&#24402;&#22320;&#29983;&#25104;NER&#30340;&#25152;&#26377;&#26631;&#31614;&#21644;&#25552;&#21450;&#65292;&#26174;&#33879;&#22686;&#21152;&#20102;&#24207;&#21015;&#38271;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PaDeLLM-NER&#65288;Parallel Decoding in LLM for NE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#38656;&#39069;&#22806;&#27169;&#22359;&#25110;&#26550;&#26500;&#20462;&#25913;&#21363;&#21487;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;&#26694;&#26550;&#20013;&#30340;&#26041;&#27861;&#12290;PaDeLLM-NER&#20801;&#35768;&#21516;&#26102;&#35299;&#30721;&#25152;&#26377;&#25552;&#21450;&#65292;&#20174;&#32780;&#20943;&#23569;&#29983;&#25104;&#24310;&#36831;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;PaDeLLM-NER&#30340;&#25512;&#29702;&#36895;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#23545;&#33521;&#35821;&#21644;&#20013;&#25991;&#26469;&#35828;&#27604;&#33258;&#22238;&#24402;&#26041;&#27861;&#24555;1.76&#21040;10.22&#20493;&#12290;&#19982;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#23218;&#32654;&#65292;&#21516;&#26102;&#32500;&#25345;&#20102;&#39044;&#27979;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we aim to reduce generation latency for Named Entity Recognition (NER) with Large Language Models (LLMs). The main cause of high latency in LLMs is the sequential decoding process, which autoregressively generates all labels and mentions for NER, significantly increase the sequence length. To this end, we introduce Parallel Decoding in LLM for NE} (PaDeLLM-NER), a approach that integrates seamlessly into existing generative model frameworks without necessitating additional modules or architectural modifications. PaDeLLM-NER allows for the simultaneous decoding of all mentions, thereby reducing generation latency. Experiments reveal that PaDeLLM-NER significantly increases inference speed that is 1.76 to 10.22 times faster than the autoregressive approach for both English and Chinese. Simultaneously it maintains the quality of predictions as evidenced by the performance that is on par with the state-of-the-art across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2402.02420</link><description>&lt;p&gt;
2024&#24180;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Factuality of Large Language Models in the Year 2024
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#30495;&#23454;&#24615;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#29616;&#26377;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25351;&#20986;&#20102;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#30340;&#38556;&#30861;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#35813;&#20851;&#27880;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#22312;&#32842;&#22825;&#26041;&#38754;&#36827;&#34892;&#25351;&#23548;&#35843;&#25972;&#21518;&#65292;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#19968;&#37096;&#20998;&#65292;&#36890;&#36807;&#22312;&#19968;&#20010;&#22320;&#26041;&#30452;&#25509;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#65292;&#20351;&#20154;&#20204;&#20174;&#25628;&#32034;&#12289;&#25552;&#21462;&#21644;&#25972;&#21512;&#22810;&#20010;&#20449;&#24687;&#28304;&#30340;&#36807;&#31243;&#20013;&#24471;&#21040;&#35299;&#33073;&#12290;&#28982;&#32780;&#65292;&#24456;&#22810;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#31572;&#26159;&#38169;&#35823;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#35780;&#20272;&#21644;&#25552;&#39640;LLM&#30495;&#23454;&#24615;&#30340;&#30740;&#31350;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#30740;&#31350;&#36827;&#34892;&#20102;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#26088;&#22312;&#25214;&#20986;&#20027;&#35201;&#25361;&#25112;&#21450;&#20854;&#21407;&#22240;&#65292;&#24182;&#25351;&#20986;&#25913;&#36827;LLM&#30495;&#23454;&#24615;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#21450;&#20998;&#26512;&#24320;&#25918;&#25991;&#26412;&#29983;&#25104;&#30340;&#33258;&#21160;&#30495;&#23454;&#24615;&#35780;&#20272;&#38754;&#20020;&#30340;&#38556;&#30861;&#12290;&#25105;&#20204;&#36824;&#23637;&#26395;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of research attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2401.03630</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#23578;&#26410;&#25104;&#21151;
&lt;/p&gt;
&lt;p&gt;
Why Solving Multi-agent Path Finding with Large Language Model has not Succeeded Yet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03630
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#22330;&#26223;&#19979;&#30340;&#36335;&#24452;&#35268;&#21010;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25104;&#21151;&#24341;&#21457;&#30340;&#29190;&#28856;&#24615;&#24433;&#21709;&#65292;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#34920;&#26126;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#26041;&#38754;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#20998;&#20139;&#35265;&#35299;&#12290;&#22810;&#26234;&#33021;&#20307;&#35268;&#21010;&#19981;&#21516;&#20110;&#20854;&#20182;&#39046;&#22495;&#65292;&#23427;&#23558;&#22810;&#26234;&#33021;&#20307;&#21327;&#35843;&#21644;&#35268;&#21010;&#30340;&#22256;&#38590;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#20419;&#36827;&#25152;&#38656;&#30340;&#25512;&#29702;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#38382;&#39064;&#65292;&#20063;&#34987;&#31216;&#20026;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#35299;&#20915;MAPF&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#38556;&#30861;&#29289;&#30340;&#31354;&#25151;&#38388;&#22320;&#22270;&#19978;&#30340;&#28608;&#21169;&#24615;&#25104;&#21151;&#65292;&#28982;&#21518;&#23637;&#31034;&#20102;&#23545;&#26631;&#20934;MAPF&#22522;&#20934;&#27979;&#35797;&#20013;&#36739;&#38590;&#30340;&#25151;&#38388;&#22320;&#22270;&#21644;&#36855;&#23467;&#22320;&#22270;&#30340;&#35268;&#21010;&#22833;&#36133;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;&#30452;&#25509;&#20351;&#29992;LLM&#35299;&#20915;MAPF&#23578;&#26410;&#25104;&#21151;&#30340;&#31435;&#22330;&#65292;&#24182;&#36890;&#36807;&#21508;&#31181;&#23454;&#39564;&#26469;&#25903;&#25745;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosive influence caused by the success of large language models (LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work showing that foundation models can be used to solve a large variety of tasks. However, there is very limited work that shares insights on multi-agent planning. Multi-agent planning is different from other domains by combining the difficulty of multi-agent coordination and planning, and making it hard to leverage external tools to facilitate the reasoning needed. In this paper, we focus on the problem of multi-agent path finding (MAPF), which is also known as multi-robot route planning, and study the performance of solving MAPF with LLMs. We first show the motivating success on an empty room map without obstacles, then the failure to plan on the harder room map and maze map of the standard MAPF benchmark. We present our position on why directly solving MAPF with LLMs has not been successful yet, and we use various experiments to supp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.15842</link><description>&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#29992;&#20110;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#30340;LLM&#30340;&#33258;&#21160;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Knowledge Distillation of LLM for Automatic Scoring of Science Education Assessments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLM&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#20316;&#20026;&#36719;&#26631;&#31614;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20445;&#35777;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#27169;&#22411;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#26041;&#27861;&#22312;&#31185;&#23398;&#25945;&#32946;&#35780;&#20272;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#31934;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#26356;&#39640;&#25928;&#12289;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#29305;&#21035;&#38024;&#23545;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;LLM&#30340;&#39044;&#27979;&#27010;&#29575;&#65288;&#20316;&#20026;&#36719;&#26631;&#31614;&#65289;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65288;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;LLM&#20805;&#24403;&#25945;&#24072;&#27169;&#22411;&#12290;&#36825;&#36890;&#36807;&#19968;&#20010;&#19987;&#38376;&#20026;&#20102;&#20174;LLM&#30340;&#36755;&#20986;&#27010;&#29575;&#20013;&#23398;&#20064;&#32780;&#23450;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#65292;&#20197;&#30830;&#20445;&#23398;&#29983;&#27169;&#22411;&#19982;&#25945;&#24072;&#30340;&#24615;&#33021;&#38750;&#24120;&#30456;&#20284;&#12290;&#20026;&#20102;&#39564;&#35777;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;6,684&#20010;&#23398;&#29983;&#23545;&#31185;&#23398;&#38382;&#39064;&#30340;&#20889;&#20316;&#22238;&#31572;&#21644;&#19977;&#20010;&#20154;&#24037;&#19987;&#23478;&#35780;&#20998;&#30340;&#25968;&#23398;&#25512;&#29702;&#25968;&#25454;&#38598;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;7T&#12290;&#25105;&#20204;&#23558;&#20934;&#30830;&#24615;&#19982;&#26368;&#20808;&#36827;&#30340;&#30693;&#35782;&#33976;&#39311;&#27169;&#22411;TinyBERT&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
This study proposes a method for knowledge distillation (KD) of fine-tuned Large Language Models (LLMs) into smaller, more efficient, and accurate neural networks. We specifically target the challenge of deploying these models on resource-constrained devices. Our methodology involves training the smaller student model (Neural Network) using the prediction probabilities (as soft labels) of the LLM, which serves as a teacher model. This is achieved through a specialized loss function tailored to learn from the LLM's output probabilities, ensuring that the student model closely mimics the teacher's performance. To validate the performance of the KD approach, we utilized a large dataset, 7T, containing 6,684 student-written responses to science questions and three mathematical reasoning datasets with student-written responses graded by human experts. We compared accuracy with state-of-the-art (SOTA) distilled models, TinyBERT, and artificial neural network (ANN) models. Results have shown 
&lt;/p&gt;</description></item><item><title>MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.13668</link><description>&lt;p&gt;
MAIRA-1&#65306;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAIRA-1: A specialised large multimodal model for radiology report generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13668
&lt;/p&gt;
&lt;p&gt;
MAIRA-1&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#22312;&#19982;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#21644;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#20102;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#23556;&#23398;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#33016;&#37096;X&#20809;&#65288;CXR&#65289;&#29983;&#25104;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#19968;&#20010;&#24605;&#24819;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#19982;&#39044;&#35757;&#32451;&#35270;&#35273;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#22810;&#27169;&#24577;&#33021;&#21147;&#12290;&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#65292;&#36825;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20351;&#22810;&#27169;&#24577;&#27169;&#22411;&#33719;&#24471;&#22270;&#20687;&#29702;&#35299;&#21644;&#25551;&#36848;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#65288;MAIRA-1&#65289;&#21033;&#29992;&#20102;&#19968;&#20010;CXR&#29305;&#23450;&#30340;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#32467;&#21512;&#22522;&#20110;Vicuna-7B&#30340;&#24494;&#35843;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#26368;&#20808;&#36827;&#36136;&#37327;&#30340;&#25253;&#21578;&#12290;&#29305;&#21035;&#22320;&#65292;MAIRA-1&#22312;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#23545;&#40784;&#30340;RadCliQ&#24230;&#37327;&#21644;&#32771;&#34385;&#30340;&#25152;&#26377;&#35789;&#27719;&#24230;&#37327;&#19978;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#23545;&#27169;&#22411;&#36755;&#20986;&#30340;&#25163;&#21160;&#23457;&#26680;&#26174;&#31034;&#20986;&#20102;&#20135;&#29983;&#25253;&#21578;&#30340;&#27969;&#30021;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#25152;&#26410;&#25429;&#25417;&#21040;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#26356;&#22810;&#20449;&#24687;&#21644;&#36164;&#28304;&#21487;&#22312;&#39033;&#30446;&#32593;&#31449;&#19978;&#25214;&#21040;&#65306;
&lt;/p&gt;
&lt;p&gt;
We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website:
&lt;/p&gt;</description></item><item><title>AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.09830</link><description>&lt;p&gt;
AutoPlanBench: &#20174;PDDL&#33258;&#21160;&#29983;&#25104;LLM&#35268;&#21010;&#22120;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench: Automatically generating benchmarks for LLM planners from PDDL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09830
&lt;/p&gt;
&lt;p&gt;
AutoPlanBench&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#36716;&#25442;PDDL&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#20026;&#25991;&#26412;&#25551;&#36848;&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#21069;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#20219;&#21153;&#26469;&#35828;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#65288;&#36923;&#36753;-&#27010;&#29575;&#27169;&#22411;&#65289;&#22312;&#35268;&#21010;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#24191;&#27867;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35268;&#21010;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#23578;&#19981;&#26126;&#30830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AutoPlanBench&#65292;&#19968;&#31181;&#23558;PDDL&#20013;&#30340;&#35268;&#21010;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#36716;&#25442;&#20026;&#25991;&#26412;&#25551;&#36848;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#21019;&#24314;&#30340;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#22909;&#30340;LLM&#35268;&#21010;&#22120;&#22312;&#26576;&#20123;&#35268;&#21010;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#20854;&#20182;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs are being increasingly used for planning-style tasks, but their capabilities for planning and reasoning are poorly understood. We present AutoPlanBench, a novel method for automatically converting planning benchmarks written in PDDL into textual descriptions and offer a benchmark dataset created with our method. We show that while the best LLM planners do well on some planning tasks, others remain out of reach of current methods.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#22797;&#26434;&#31185;&#23398;&#25512;&#29702;&#22256;&#38590;&#65292;&#21457;&#29616;&#38169;&#35823;&#36890;&#24120;&#28304;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#32467;&#26500;&#12290;&#22522;&#20110;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;StructChem&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09656</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#21270;&#21270;&#23398;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Structured Chemistry Reasoning with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09656
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#22797;&#26434;&#31185;&#23398;&#25512;&#29702;&#22256;&#38590;&#65292;&#21457;&#29616;&#38169;&#35823;&#36890;&#24120;&#28304;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#32467;&#26500;&#12290;&#22522;&#20110;&#27492;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;StructChem&#65292;&#22823;&#24133;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21270;&#23398;&#39046;&#22495;&#30340;&#22797;&#26434;&#31185;&#23398;&#25512;&#29702;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#19982;&#20197;&#24448;&#30740;&#31350;&#20013;&#28041;&#21450;&#30340;&#31616;&#21333;&#21270;&#23398;&#20219;&#21153;&#65288;&#20363;&#22914;&#20998;&#23376;&#20998;&#31867;&#65289;&#19981;&#21516;&#65292;&#22797;&#26434;&#30340;&#21270;&#23398;&#38382;&#39064;&#19981;&#20165;&#38656;&#35201;&#24191;&#21338;&#30340;&#30693;&#35782;&#21644;&#31934;&#30830;&#30340;&#35745;&#31639;&#65292;&#36824;&#38656;&#35201;&#20851;&#20110;&#19981;&#21516;&#27010;&#24565;&#65288;&#20363;&#22914;&#28201;&#24230;&#21464;&#21270;&#65289;&#30340;&#20016;&#23500;&#21160;&#24577;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#21512;&#25512;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#20687;GPT-4&#36825;&#26679;&#20808;&#36827;&#30340;LLMs&#20063;&#24456;&#23481;&#26131;&#20986;&#29616;&#38169;&#35823;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#20123;&#38169;&#35823;&#36890;&#24120;&#19981;&#26159;&#30001;&#20110;LLMs&#32570;&#20047;&#39046;&#22495;&#30693;&#35782;&#65292;&#32780;&#26159;&#30001;&#20110;&#32570;&#20047;&#26377;&#25928;&#30340;&#25512;&#29702;&#32467;&#26500;&#26469;&#24341;&#23548;LLMs&#24341;&#21457;&#27491;&#30830;&#30340;&#30693;&#35782;&#65292;&#23558;&#30693;&#35782;&#34701;&#20837;&#36880;&#27493;&#25512;&#29702;&#20013;&#65292;&#24182;&#36845;&#20195;&#25913;&#36827;&#32467;&#26524;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#36136;&#37327;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#25552;&#31034;&#31574;&#30053;&#8212;&#8212;&#32467;&#26500;&#21270;&#21270;&#23398;&#65288;StructChem&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#25152;&#38656;&#30340;&#25351;&#23548;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;LLMs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) excel in diverse areas, yet struggle with complex scientific reasoning, especially in the field of chemistry. Different from the simple chemistry tasks (e.g., molecule classification) addressed in previous studies, complex chemistry problems require not only vast knowledge and precise calculation, but also compositional reasoning about rich dynamic interactions of different concepts (e.g., temperature changes). Our study shows that even advanced LLMs, like GPT-4, can fail easily in different ways. Interestingly, the errors often stem not from a lack of domain knowledge within the LLMs, but rather from the absence of an effective reasoning structure that guides the LLMs to elicit the right knowledge, incorporate the knowledge in step-by-step reasoning, and iteratively refine results for further improved quality. On this basis, we introduce StructChem, a simple yet effective prompting strategy that offers the desired guidance and substantially boosts the LLMs
&lt;/p&gt;</description></item><item><title>Sorted LLaMA&#36890;&#36807;&#25193;&#23637;SortedNet&#21040;&#29983;&#25104;NLP&#20219;&#21153;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21160;&#24577;&#25512;&#29702;&#20013;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#21482;&#38656;&#23558;&#26631;&#20934;&#24494;&#35843;&#26367;&#25442;&#20026;&#25490;&#24207;&#24494;&#35843;&#21363;&#21487;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#37322;&#25918;transformers&#20013;&#38388;&#23618;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23384;&#20648;&#38656;&#27714;&#21644;&#36807;&#28193;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2309.08968</link><description>&lt;p&gt;
Sorted LLaMA: &#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#38388;&#23618;&#30340;&#28508;&#21147;&#65292;&#29992;&#20110;&#21160;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Sorted LLaMA: Unlocking the Potential of Intermediate Layers of Large Language Models for Dynamic Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08968
&lt;/p&gt;
&lt;p&gt;
Sorted LLaMA&#36890;&#36807;&#25193;&#23637;SortedNet&#21040;&#29983;&#25104;NLP&#20219;&#21153;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21160;&#24577;&#25512;&#29702;&#20013;&#26356;&#39640;&#25928;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#65292;&#21482;&#38656;&#23558;&#26631;&#20934;&#24494;&#35843;&#26367;&#25442;&#20026;&#25490;&#24207;&#24494;&#35843;&#21363;&#21487;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#37322;&#25918;transformers&#20013;&#38388;&#23618;&#30340;&#28508;&#21147;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23384;&#20648;&#38656;&#27714;&#21644;&#36807;&#28193;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#25991;&#26412;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#24102;&#26469;&#20102;&#38761;&#21629;&#12290;&#28982;&#32780;&#65292;&#24191;&#27867;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;SortedNet&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#20013;&#30340;&#27169;&#22359;&#21270;&#21644;&#22522;&#20110;&#35745;&#31639;/&#20934;&#30830;&#24615;&#23545;&#23376;&#27169;&#22411;&#36827;&#34892;&#23884;&#22871;&#25490;&#24207;&#65292;&#23454;&#29616;&#20102;&#21160;&#24577;&#25512;&#29702;&#12290;&#25105;&#20204;&#23558;SortedNet&#25193;&#23637;&#21040;&#29983;&#25104;NLP&#20219;&#21153;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#36827;&#34892;&#20219;&#20309;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#21160;&#24577;&#65292;&#20165;&#36890;&#36807;&#23558;&#26631;&#20934;&#24494;&#35843;&#65288;SFT&#65289;&#26367;&#25442;&#20026;&#25490;&#24207;&#24494;&#35843;&#65288;SoFT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25928;&#29575;&#65292;&#28040;&#38500;&#20102;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#37322;&#25918;transformers&#20013;&#38388;&#23618;&#22312;&#29983;&#25104;&#30446;&#26631;&#36755;&#20986;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#23376;&#27169;&#22411;&#20173;&#28982;&#26159;&#21407;&#22987;&#27169;&#22411;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#26368;&#23567;&#21270;&#20102;&#23384;&#20648;&#38656;&#27714;&#21644;&#22312;&#19981;&#21516;&#35745;&#31639;/&#24310;&#36831;&#39044;&#31639;&#20043;&#38388;&#30340;&#36807;&#28193;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized natural language processing (NLP) by excelling at understanding and generating human-like text. However, their widespread deployment can be prohibitively expensive. SortedNet is a recent training technique for enabling dynamic inference by leveraging the modularity in networks and sorting sub-models based on computation/accuracy in a nested manner. We extend SortedNet to generative NLP tasks, making large language models dynamic without any Pre-Training and by only replacing Standard Fine-Tuning (SFT) with Sorted Fine-Tuning (SoFT). Our approach boosts model efficiency, eliminating the need for multiple models for various scenarios during inference. We show that this approach can unlock the power of intermediate layers of transformers in generating the target output. Our sub-models remain integral components of the original model, minimizing storage requirements and transition costs between different computational/latency budgets. The ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#32422;&#26463;&#35757;&#32451;&#65288;PCT&#65289;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2305.13179</link><description>&lt;p&gt;
&#23558;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#25945;&#32473;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Teaching Probabilistic Logical Reasoning to Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#32422;&#26463;&#35757;&#32451;&#65288;PCT&#65289;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#30340;&#25991;&#26412;&#19978;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#19981;&#30830;&#23450;&#30340;&#25512;&#29702;&#35268;&#21017;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21644;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#20195;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#19981;&#30830;&#23450;&#25991;&#26412;&#26041;&#38754;&#37117;&#23384;&#22312;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#24494;&#35843;&#26041;&#27861;&#65292;&#27010;&#29575;&#32422;&#26463;&#35757;&#32451;&#65288;PCT&#65289;&#65292;&#23427;&#22312;&#24494;&#35843;&#38454;&#27573;&#21033;&#29992;&#27010;&#29575;&#36923;&#36753;&#35268;&#21017;&#20316;&#20026;&#32422;&#26463;&#65292;&#32780;&#19981;&#20381;&#36182;&#36825;&#20123;&#35268;&#21017;&#22312;&#25512;&#29702;&#38454;&#27573;&#12290;&#20026;&#20102;&#35780;&#20272;PCT&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21033;&#29992;&#30456;&#20851;&#35821;&#26009;&#24211;&#65292;&#24182;&#39069;&#22806;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#20043;&#21069;&#30340;&#27979;&#35797;&#19981;&#21516;&#65292;&#23427;&#20351;&#29992;&#20102;&#23454;&#20363;&#29305;&#23450;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;PCT&#25552;&#39640;&#20102;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#22312;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#27010;&#29575;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#26356;&#26126;&#30830;&#21644;&#21487;&#35299;&#37322;&#12290;&#27492;&#22806;&#65292;PCT&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19981;&#30830;&#23450;&#30340;&#25991;&#26412;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we evaluate the capability of transformer-based language models in making inferences over uncertain text that includes uncertain rules of reasoning. We cover both Pre-trained Language Models (PLMs) and generative Large Language Models (LLMs). Our evaluation results show that both generations of language models struggle with reasoning over uncertain text. We propose a novel end-to-end fine-tuning approach, Probabilistic Constraint Training (PCT), that utilizes probabilistic logical rules as constraints in the fine-tuning phase without relying on these rules in the inference stage. To assess the effectiveness of PCT, we utilize the related corpora and, additionally, create a new and more challenging benchmark that, unlike the previous ones, uses instance-specific rules. Our study demonstrates that PCT improves the transformer-based language model's intrinsic reasoning and makes their probabilistic logical reasoning process more explicit and explainable. Furthermore, PCT eq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#23454;&#20307;/&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2305.12707</link><description>&lt;p&gt;
&#37327;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#21450;&#20854;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Quantifying Association Capabilities of Large Language Models and Its Implications on Privacy Leakage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12707
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#23545;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#23454;&#20307;/&#20449;&#24687;&#26041;&#38754;&#30340;&#33021;&#21147;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#19982;&#24120;&#35782;&#30693;&#35782;&#30456;&#27604;&#65292;&#27169;&#22411;&#22312;&#20851;&#32852;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#19982;&#27492;&#21516;&#26102;&#20063;&#24341;&#21457;&#20102;&#23545;&#28508;&#22312;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#30340;&#25285;&#24551;&#12290;&#20854;&#20013;&#19968;&#20010;&#26174;&#33879;&#30340;LLMs&#33021;&#21147;&#26159;&#23427;&#20204;&#33021;&#22815;&#24418;&#25104;&#19981;&#21516;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20294;&#36825;&#22312;&#28041;&#21450;&#20010;&#20154;&#21487;&#35782;&#21035;&#20449;&#24687;&#65288;PII&#65289;&#26102;&#24341;&#21457;&#20102;&#25285;&#24551;&#12290;&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#32852;&#33021;&#21147;&#65292;&#26088;&#22312;&#25581;&#31034;&#24433;&#21709;&#20854;&#20851;&#32852;&#20449;&#24687;&#33021;&#21147;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#38543;&#30528;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#65292;&#20854;&#20851;&#32852;&#23454;&#20307;/&#20449;&#24687;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#29305;&#21035;&#26159;&#24403;&#30446;&#26631;&#23545;&#23637;&#31034;&#26356;&#30701;&#30340;&#20849;&#29616;&#36317;&#31163;&#25110;&#26356;&#39640;&#30340;&#20849;&#29616;&#39057;&#29575;&#26102;&#12290;&#28982;&#32780;&#65292;&#22312;&#20851;&#32852;&#24120;&#35782;&#30693;&#35782;&#19982;PII&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#21518;&#32773;&#30340;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#23613;&#31649;&#20934;&#30830;&#39044;&#27979;PII&#30340;&#27604;&#20363;&#30456;&#23545;&#36739;&#23567;&#65292;&#20294;LLMs&#20173;&#28982;&#34920;&#29616;&#20986;&#20102;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of large language models (LLMs) brings notable improvements across various applications, while simultaneously raising concerns about potential private data exposure. One notable capability of LLMs is their ability to form associations between different pieces of information, but this raises concerns when it comes to personally identifiable information (PII). This paper delves into the association capabilities of language models, aiming to uncover the factors that influence their proficiency in associating information. Our study reveals that as models scale up, their capacity to associate entities/information intensifies, particularly when target pairs demonstrate shorter co-occurrence distances or higher co-occurrence frequencies. However, there is a distinct performance gap when associating commonsense knowledge versus PII, with the latter showing lower accuracy. Despite the proportion of accurately predicted PII being relatively small, LLMs still demonstrate the capab
&lt;/p&gt;</description></item><item><title>ALEXSIS-PT&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35789;&#27719;&#31616;&#21270;&#30340;&#26032;&#22411;&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#20026;LS&#31995;&#32479;&#30340;&#25913;&#36827;&#21644;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;BERTimbau&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2209.09034</link><description>&lt;p&gt;
ALEXSIS-PT&#65306;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#33889;&#33796;&#29273;&#35821;&#35789;&#27719;&#31616;&#21270;&#30340;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
ALEXSIS-PT: A New Resource for Portuguese Lexical Simplification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.09034
&lt;/p&gt;
&lt;p&gt;
ALEXSIS-PT&#26159;&#19968;&#20010;&#29992;&#20110;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;&#35789;&#27719;&#31616;&#21270;&#30340;&#26032;&#22411;&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#20026;LS&#31995;&#32479;&#30340;&#25913;&#36827;&#21644;&#36328;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#12290;BERTimbau&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#31616;&#21270;&#65288;LS&#65289;&#26159;&#33258;&#21160;&#26367;&#25442;&#22797;&#26434;&#35789;&#27719;&#20026;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#35789;&#27719;&#30340;&#20219;&#21153;&#65292;&#20351;&#25991;&#26412;&#23545;&#21508;&#31181;&#30446;&#26631;&#20154;&#32676;&#65288;&#22914;&#20302;&#35782;&#23383;&#29575;&#30340;&#20010;&#20307;&#12289;&#23398;&#20064;&#38556;&#30861;&#20010;&#20307;&#12289;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#65289;&#26356;&#26131;&#20110;&#35775;&#38382;&#12290;&#20026;&#20102;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;LS&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#21253;&#21547;&#22797;&#26434;&#35789;&#27719;&#21450;&#20854;&#20505;&#36873;&#26367;&#20195;&#35789;&#30340;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;LS&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ALEXSIS-PT&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24052;&#35199;&#33889;&#33796;&#29273;&#35821;LS&#30340;&#26032;&#22411;&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;387&#20010;&#22797;&#26434;&#35789;&#27719;&#30340;9605&#20010;&#20505;&#36873;&#26367;&#20195;&#35789;&#12290;ALEXSIS-PT&#26159;&#25353;&#29031;ALEXSIS&#21327;&#35758;&#32534;&#21046;&#30340;&#65292;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#65292;&#20026;&#36328;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;ALEXSIS-PT&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#25253;&#32440;&#25991;&#31456;&#30340;LS&#22810;&#20505;&#36873;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#22235;&#31181;&#26367;&#20195;&#29983;&#25104;&#27169;&#22411;&#65292;&#20998;&#21035;&#26159;mDistilBERT&#12289;mBERT&#12289;XLM-R&#21644;BERTimbau&#12290;BERTimbau&#22312;&#25152;&#26377;&#35780;&#20272;&#25351;&#26631;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lexical simplification (LS) is the task of automatically replacing complex words for easier ones making texts more accessible to various target populations (e.g. individuals with low literacy, individuals with learning disabilities, second language learners). To train and test models, LS systems usually require corpora that feature complex words in context along with their candidate substitutions. To continue improving the performance of LS systems we introduce ALEXSIS-PT, a novel multi-candidate dataset for Brazilian Portuguese LS containing 9,605 candidate substitutions for 387 complex words. ALEXSIS-PT has been compiled following the ALEXSIS protocol for Spanish opening exciting new avenues for cross-lingual models. ALEXSIS-PT is the first LS multi-candidate dataset that contains Brazilian newspaper articles. We evaluated four models for substitute generation on this dataset, namely mDistilBERT, mBERT, XLM-R, and BERTimbau. BERTimbau achieved the highest performance across all evalu
&lt;/p&gt;</description></item><item><title>SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.15024</link><description>&lt;p&gt;
SliceGPT: &#36890;&#36807;&#21024;&#38500;&#34892;&#21644;&#21015;&#26469;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SliceGPT: Compress Large Language Models by Deleting Rows and Columns. (arXiv:2401.15024v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15024
&lt;/p&gt;
&lt;p&gt;
SliceGPT&#26159;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#26367;&#25442;&#20026;&#36739;&#23567;&#30340;&#30697;&#38453;&#20197;&#20943;&#23567;&#32593;&#32476;&#30340;&#32500;&#24230;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#39640;&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#65292;&#20294;&#20351;&#29992;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#21644;&#20869;&#23384;&#36164;&#28304;&#12290;&#31232;&#30095;&#21270;&#26041;&#27861;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#36164;&#28304;&#38480;&#21046;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#20107;&#21518;&#30340;&#31232;&#30095;&#21270;&#22788;&#29702;&#12290;&#29616;&#26377;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#32467;&#26500;&#65292;&#24182;&#19988;&#22312;&#24403;&#21069;&#30828;&#20214;&#19978;&#36895;&#24230;&#21463;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#21518;&#35757;&#32451;&#31232;&#30095;&#21270;&#26041;&#26696;SliceGPT&#65292;&#35813;&#26041;&#26696;&#29992;&#36739;&#23567;&#30340;&#65288;&#31264;&#23494;&#30340;&#65289;&#30697;&#38453;&#26367;&#25442;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#65292;&#20174;&#32780;&#20943;&#23567;&#32593;&#32476;&#30340;&#23884;&#20837;&#32500;&#24230;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SliceGPT&#22312;&#20445;&#25345;&#30456;&#24212;&#31264;&#23494;&#27169;&#22411;&#30340;99%&#12289;99%&#21644;90%&#30340;&#38646;-shot&#20219;&#21153;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#31227;&#38500;LLAMA2-70B&#12289;OPT 66B&#21644;Phi-2&#27169;&#22411;&#20013;&#22810;&#36798;25%&#30340;&#27169;&#22411;&#21442;&#25968;&#65288;&#21253;&#25324;&#23884;&#20837;&#65289;&#12290;&#25105;&#20204;&#30340;&#20999;&#29255;&#27169;&#22411;&#22312;&#36739;&#23569;&#30340;GPU&#19978;&#36816;&#34892;&#24182;&#19988;&#26356;&#24555;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#20195;&#30721;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimi
&lt;/p&gt;</description></item><item><title>Muffin&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#26080;&#30410;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#26102;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#26041;&#20301;&#30340;AI&#21453;&#39304;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#12290;</title><link>http://arxiv.org/abs/2401.05928</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#26041;&#20301;AI&#21453;&#39304;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#30340;&#26080;&#30410;&#24615;
&lt;/p&gt;
&lt;p&gt;
Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback. (arXiv:2401.05928v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05928
&lt;/p&gt;
&lt;p&gt;
Muffin&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#65292;&#29992;&#20110;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#20013;&#26080;&#30410;&#24615;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#22312;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#26102;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#24182;&#36890;&#36807;&#22810;&#26041;&#20301;&#30340;AI&#21453;&#39304;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20943;&#36731;&#29992;&#25143;&#30340;&#24773;&#24863;&#22256;&#25200;&#24182;&#24110;&#21161;&#20182;&#20204;&#35299;&#20915;&#25361;&#25112;&#12290;&#20026;&#20102;&#29983;&#25104;&#25903;&#25345;&#24615;&#22238;&#22797;&#65292;&#24517;&#39035;&#32771;&#34385;&#21040;&#22810;&#20010;&#22240;&#32032;&#65292;&#22914;&#20849;&#24773;&#12289;&#25903;&#25345;&#31574;&#30053;&#21644;&#22238;&#22797;&#36830;&#36143;&#24615;&#65292;&#36825;&#20123;&#22312;&#20043;&#21069;&#30340;&#26041;&#27861;&#20013;&#24050;&#32463;&#24471;&#21040;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#27169;&#22411;&#20598;&#23572;&#20250;&#29983;&#25104;&#26080;&#30410;&#30340;&#22238;&#22797;&#65292;&#36825;&#20123;&#22238;&#22797;&#24847;&#22270;&#25552;&#20379;&#25903;&#25345;&#65292;&#20294;&#21364;&#20135;&#29983;&#36866;&#24471;&#20854;&#21453;&#30340;&#25928;&#26524;&#12290;&#26681;&#25454;&#24515;&#29702;&#23398;&#21644;&#27807;&#36890;&#29702;&#35770;&#65292;&#34429;&#28982;&#21482;&#26159;&#21333;&#19968;&#22240;&#32032;&#30340;&#34920;&#29616;&#19981;&#20339;&#21487;&#33021;&#20250;&#23548;&#33268;&#22238;&#22797;&#26080;&#30410;&#12290;&#20174;&#27169;&#22411;&#35757;&#32451;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30001;&#20110;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#38454;&#27573;&#27809;&#26377;&#25509;&#35302;&#21040;&#26080;&#30410;&#30340;&#22238;&#22797;&#65292;&#23427;&#20204;&#26080;&#27861;&#21028;&#26029;&#23427;&#20204;&#29983;&#25104;&#30340;&#26631;&#35760;&#26159;&#21542;&#20250;&#23548;&#33268;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26080;&#30410;&#22238;&#22797;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#26041;&#20301;AI&#21453;&#39304;&#20943;&#36731;&#24773;&#24863;&#25903;&#25345;&#65288;Muffin&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#26080;&#20851;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specif
&lt;/p&gt;</description></item><item><title>AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2401.03003</link><description>&lt;p&gt;
AST-T5&#65306;&#38754;&#21521;&#20195;&#30721;&#29983;&#25104;&#21644;&#29702;&#35299;&#30340;&#32467;&#26500;&#24863;&#30693;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AST-T5: Structure-Aware Pretraining for Code Generation and Understanding. (arXiv:2401.03003v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03003
&lt;/p&gt;
&lt;p&gt;
AST-T5&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#26469;&#22686;&#24378;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;&#23427;&#20248;&#20110;&#20854;&#20182;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#28982;&#32780;&#35768;&#22810;&#27169;&#22411;&#23558;&#20195;&#30721;&#35270;&#20026;&#31616;&#21333;&#24207;&#21015;&#65292;&#24573;&#30053;&#20102;&#20854;&#32467;&#26500;&#21270;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;AST-T5&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#21033;&#29992;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#22686;&#24378;&#20102;&#20195;&#30721;&#29983;&#25104;&#12289;&#36716;&#25442;&#21644;&#29702;&#35299;&#12290;&#36890;&#36807;&#21160;&#24577;&#35268;&#21010;&#65292;&#25105;&#20204;&#30340;AST&#24863;&#30693;&#20998;&#21106;&#20445;&#30041;&#20102;&#20195;&#30721;&#32467;&#26500;&#65292;&#32780;AST&#24863;&#30693;&#36328;&#24230;&#30772;&#22351;&#30446;&#26631;&#20351;&#27169;&#22411;&#33021;&#22815;&#37325;&#24314;&#21508;&#31181;&#20195;&#30721;&#32467;&#26500;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#19981;&#21516;&#65292;AST-T5&#36991;&#20813;&#20102;&#22797;&#26434;&#30340;&#31243;&#24207;&#20998;&#26512;&#25110;&#26550;&#26500;&#26356;&#25913;&#65292;&#22240;&#27492;&#21487;&#20197;&#19982;&#20219;&#20309;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26080;&#32541;&#38598;&#25104;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#65292;AST-T5&#22312;&#21508;&#31181;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#20013;&#22987;&#32456;&#20248;&#20110;&#21516;&#31561;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#32467;&#26500;&#24863;&#30693;&#20351;&#24471;AST-T5&#22312;&#20195;&#30721;&#21040;&#20195;&#30721;&#20219;&#21153;&#20013;&#29305;&#21035;&#24378;&#22823;&#65292;&#22312;Bugs2Fix&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 2&#20010;&#28857;&#65292;&#24182;&#22312;CodeXGLUE&#20013;&#30340;Java-C#&#36716;&#25442;&#20219;&#21153;&#30340;&#31934;&#30830;&#21305;&#37197;&#24471;&#20998;&#19978;&#36229;&#36807;CodeT5 3&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant advancements in code-related tasks, yet many LLMs treat code as simple sequences, neglecting its structured nature. We introduce AST-T5, a novel pretraining paradigm that leverages the Abstract Syntax Tree (AST) for enhanced code generation, transpilation, and understanding. Using dynamic programming, our AST-Aware Segmentation retains code structure, while our AST-Aware Span Corruption objective equips the model to reconstruct various code structures. Unlike other models, AST-T5 avoids intricate program analyses or architectural changes, so it integrates seamlessly with any encoder-decoder Transformer. Evaluations show that AST-T5 consistently outperforms similar-sized LMs across various code-related tasks. Structure-awareness makes AST-T5 particularly powerful in code-to-code tasks, surpassing CodeT5 by 2 points in exact match score for the Bugs2Fix task and by 3 points in exact match score for Java-C# Transpilation in CodeXGLUE. Our
&lt;/p&gt;</description></item><item><title>LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2401.02330</link><description>&lt;p&gt;
LLaVA-$\phi$: &#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#19982;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$: Efficient Multi-Modal Assistant with Small Language Model. (arXiv:2401.02330v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02330
&lt;/p&gt;
&lt;p&gt;
LLaVA-$\phi$&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#22810;&#27169;&#24577;&#21161;&#25163;&#65292;&#20351;&#29992;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#12290;&#21363;&#20351;&#20855;&#26377;&#36739;&#23569;&#30340;&#21442;&#25968;&#65292;&#23427;&#20063;&#33021;&#26377;&#25928;&#22320;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaVA-$\phi$&#65288;LLaVA-Phi&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#20808;&#36827;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;Phi-2&#26469;&#20419;&#36827;&#22810;&#27169;&#24577;&#23545;&#35805;&#30340;&#39640;&#25928;&#22810;&#27169;&#24577;&#21161;&#25163;&#12290;LLaVA-Phi&#22312;&#32039;&#20945;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#39046;&#22495;&#20013;&#26631;&#24535;&#30528;&#37325;&#35201;&#36827;&#23637;&#12290;&#23427;&#35777;&#26126;&#20102;&#21363;&#20351;&#26159;&#20010;&#21442;&#25968;&#21482;&#26377;27&#20159;&#30340;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26377;&#39640;&#36136;&#37327;&#35821;&#26009;&#24211;&#30340;&#24773;&#20917;&#19979;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#21442;&#19982;&#34701;&#21512;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#30340;&#22797;&#26434;&#23545;&#35805;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21253;&#25324;&#35270;&#35273;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#24863;&#30693;&#31561;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#38500;&#20102;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20026;&#26102;&#38388;&#25935;&#24863;&#30340;&#29615;&#22659;&#21644;&#38656;&#35201;&#23454;&#26102;&#20132;&#20114;&#30340;&#31995;&#32479;&#65288;&#22914;&#23454;&#20307;&#20195;&#29702;&#65289;&#24320;&#36767;&#20102;&#26032;&#30340;&#24212;&#29992;&#36884;&#24452;&#12290;&#23427;&#31361;&#26174;&#20102;&#36739;&#23567;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#39640;&#32423;&#29702;&#35299;&#21644;&#20132;&#20114;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and inte
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18235</link><description>&lt;p&gt;
Davidsonian&#22330;&#26223;&#22270;&#65306;&#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#30340;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Davidsonian Scene Graph: Improving Reliability in Fine-grained Evaluation for Text-Image Generation. (arXiv:2310.18235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25991;&#26412;-&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#21487;&#38752;&#24615;&#25361;&#25112;&#65292;&#21253;&#25324;QG&#38382;&#39064;&#30340;&#20934;&#30830;&#24615;&#21644;VQA&#31572;&#26696;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#26368;&#36817;&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;-&#22270;&#20687;&#24544;&#23454;&#24230;&#30340;&#24378;&#22823;&#26041;&#27861;&#26159;&#22522;&#20110;QG/A&#65288;&#38382;&#39064;&#29983;&#25104;&#21644;&#22238;&#31572;&#65289;&#65292;&#23427;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#19968;&#32452;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#31572;&#26696;&#19982;&#22522;&#20110;&#25552;&#31034;&#30340;&#31572;&#26696;&#22312;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#20013;&#25552;&#21462;&#30340;&#19968;&#33268;&#24615;&#23545;&#36755;&#20986;&#22270;&#20687;&#36827;&#34892;&#35780;&#20998;&#12290;&#36825;&#31181;&#35780;&#20272;&#33258;&#28982;&#19978;&#21462;&#20915;&#20110;&#24213;&#23618;QG&#21644;QA&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30830;&#23450;&#24182;&#35299;&#20915;&#20102;&#29616;&#26377;QG/A&#24037;&#20316;&#20013;&#30340;&#20960;&#20010;&#21487;&#38752;&#24615;&#25361;&#25112;&#65306;&#65288;a&#65289;QG&#38382;&#39064;&#24212;&#23562;&#37325;&#25552;&#31034;&#65288;&#36991;&#20813;&#24187;&#35273;&#12289;&#37325;&#22797;&#21644;&#36951;&#28431;&#65289;&#21644;&#65288;b&#65289;VQA&#31572;&#26696;&#24212;&#19968;&#33268;&#65288;&#19981;&#20250;&#22312;&#22270;&#20687;&#20013;&#23459;&#31216;&#27809;&#26377;&#25705;&#25176;&#36710;&#65292;&#21516;&#26102;&#22768;&#31216;&#25705;&#25176;&#36710;&#26159;&#34013;&#33394;&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;Davidsonian&#22330;&#26223;&#22270;&#65288;DSG&#65289;&#65292;&#36825;&#20010;&#21463;&#24418;&#24335;&#35821;&#20041;&#21551;&#21457;&#30340;&#23454;&#35777;&#35780;&#20272;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating text-to-image models is notoriously difficult. A strong recent approach for assessing text-image faithfulness is based on QG/A (question generation and answering), which uses pre-trained foundational models to automatically generate a set of questions and answers from the prompt, and output images are scored based on whether these answers extracted with a visual question answering model are consistent with the prompt-based answers. This kind of evaluation is naturally dependent on the quality of the underlying QG and QA models. We identify and address several reliability challenges in existing QG/A work: (a) QG questions should respect the prompt (avoiding hallucinations, duplications, and omissions) and (b) VQA answers should be consistent (not asserting that there is no motorcycle in an image while also claiming the motorcycle is blue). We address these issues with Davidsonian Scene Graph (DSG), an empirically grounded evaluation framework inspired by formal semantics. DSG
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.03094</link><description>&lt;p&gt;
&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Cascades with Mixture of Thoughts Representations for Cost-efficient Reasoning. (arXiv:2310.03094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24605;&#32500;&#28151;&#21512;&#34920;&#31034;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#32423;&#32852;&#26041;&#27861;&#65292;&#29992;&#20110;&#25104;&#26412;&#39640;&#25928;&#30340;&#25512;&#29702;&#12290;&#36890;&#36807;&#32771;&#34385;&#26356;&#24369;&#27169;&#22411;&#30340;&#31572;&#26696;&#19968;&#33268;&#24615;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#21487;&#20197;&#23454;&#29616;&#23545;&#38382;&#39064;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#33410;&#32422;&#20351;&#29992;&#26356;&#24378;&#27169;&#22411;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-4&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#36825;&#31181;&#24378;&#22823;&#30340;&#24615;&#33021;&#36890;&#24120;&#20276;&#38543;&#30528;&#20351;&#29992;&#20184;&#36153;API&#26381;&#21153;&#30340;&#39640;&#26114;&#36153;&#29992;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#20026;&#20102;&#30740;&#31350;&#26500;&#24314;LLM&#32423;&#32852;&#20197;&#33410;&#32422;&#20351;&#29992;LLM&#30340;&#25104;&#26412;&#65292;&#29305;&#21035;&#26159;&#29992;&#20110;&#36827;&#34892;&#25512;&#29702;&#65288;&#20363;&#22914;&#25968;&#23398;&#12289;&#22240;&#26524;&#25512;&#29702;&#65289;&#20219;&#21153;&#30340;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32423;&#32852;&#31649;&#36947;&#36981;&#24490;&#19968;&#20010;&#30452;&#35266;&#30340;&#24605;&#24819;&#65292;&#21363;&#31616;&#21333;&#30340;&#38382;&#39064;&#21487;&#20197;&#30001;&#19968;&#20010;&#26356;&#24369;&#20294;&#26356;&#23454;&#24800;&#30340;LLM&#26469;&#35299;&#20915;&#65292;&#32780;&#21482;&#26377;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#25165;&#38656;&#35201;&#26356;&#24378;&#22823;&#12289;&#26356;&#26114;&#36149;&#30340;LLM&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#20915;&#31574;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#26356;&#24369;&#30340;LLM&#30340;&#8220;&#31572;&#26696;&#19968;&#33268;&#24615;&#8221;&#20316;&#20026;&#38382;&#39064;&#38590;&#24230;&#30340;&#20449;&#21495;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#31181;&#31572;&#26696;&#37319;&#26679;&#21644;&#19968;&#33268;&#24615;&#26816;&#26597;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#24605;&#32500;&#34920;&#31034;&#65288;&#21363;&#36830;&#32493;&#24605;&#32500;&#21644;&#31243;&#24207;&#24605;&#32500;&#65289;&#30340;&#28151;&#21512;&#12290;&#36890;&#36807;&#22312;&#20845;&#20010;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-3.5-turbo&#21644;GPT-4&#20316;&#20026;&#36739;&#24369;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as GPT-4 have exhibited remarkable performance in a variety of tasks, but this strong performance often comes with the high expense of using paid API services. In this paper, we are motivated to study building an LLM cascade to save the cost of using LLMs, particularly for performing reasoning (e.g., mathematical, causal) tasks. Our cascade pipeline follows the intuition that simpler questions can be addressed by a weaker but more affordable LLM, whereas only the challenging questions necessitate the stronger and more expensive LLM. To realize this decision-making, we consider the "answer consistency" of the weaker LLM as a signal of the question difficulty and propose several methods for the answer sampling and consistency checking, including one leveraging a mixture of two thought representations (i.e., Chain-of-Thought and Program-of-Thought). Through experiments on six reasoning benchmark datasets, with GPT-3.5-turbo and GPT-4 being the weaker and 
&lt;/p&gt;</description></item><item><title>Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;</title><link>http://arxiv.org/abs/2309.17179</link><description>&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Alphazero-like Tree-Search can Guide Large Language Model Decoding and Training. (arXiv:2309.17179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17179
&lt;/p&gt;
&lt;p&gt;
Alphazero&#31867;&#20284;&#30340;&#26641;&#25628;&#32034;&#26694;&#26550;TS-LLM&#21487;&#20197;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#21644;&#35757;&#32451;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#25512;&#29702;&#20219;&#21153;&#65292;&#36824;&#36866;&#29992;&#20110;&#20854;&#20182;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36890;&#24120;&#37319;&#29992;&#37319;&#26679;&#25110;&#26463;&#25628;&#32034;&#65292;&#32467;&#21512; Chain-of-Thought (CoT) &#31561;&#25552;&#31034;&#26469;&#25552;&#39640;&#25512;&#29702;&#21644;&#35299;&#30721;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#22914; Tree-of-Thought (ToT) &#21644; Reasoning via Planning (RAP) &#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#26641;&#25628;&#32034;&#31639;&#27861;&#26469;&#24341;&#23548;&#22810;&#27493;&#25512;&#29702;&#65292;&#26469;&#22686;&#24378;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;LLM&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#26469;&#28608;&#27963;LLM&#20316;&#20026;&#19968;&#20010;&#20215;&#20540;&#20989;&#25968;&#65292;&#32570;&#20047;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;AlphaZero&#31867;&#20284;&#30340;&#29992;&#20110;LLM&#30340;&#26641;&#25628;&#32034;&#26694;&#26550; (&#31216;&#20026;TS-LLM)&#65292;&#31995;&#32479;&#22320;&#35828;&#26126;&#20102;&#22914;&#20309;&#36890;&#36807;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#21033;&#29992;&#26641;&#25628;&#32034;&#26469;&#25351;&#23548;LLM&#30340;&#35299;&#30721;&#33021;&#21147;&#12290;TS-LLM&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#19982;&#20247;&#19981;&#21516;&#65306;(1)&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#26222;&#36866;&#22320;&#24212;&#29992;&#20110;&#38500;&#20102;&#25512;&#29702;&#20043;&#22806;&#30340;&#19981;&#21516;&#20219;&#21153; (&#20363;&#22914;RLHF&#23545;&#40784;)&#65292;&#20197;&#21450;&#20219;&#20309;&#22823;&#23567;&#30340;LLM&#65292;&#32780;&#19981;&#38656;&#35201;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) typically employ sampling or beam search, accompanied by prompts such as Chain-of-Thought (CoT), to boost reasoning and decoding ability. Recent work like Tree-of-Thought (ToT) and Reasoning via Planning (RAP) aim to augment the reasoning capabilities of LLMs by utilizing tree-search algorithms to guide multi-step reasoning. These methods mainly focus on LLMs' reasoning ability during inference and heavily rely on human-designed prompts to activate LLM as a value function, which lacks general applicability and scalability. To address these limitations, we present an AlphaZero-like tree-search framework for LLMs (termed TS-LLM), systematically illustrating how tree-search with a learned value function can guide LLMs' decoding ability. TS-LLM distinguishes itself in two key ways: (1) Leveraging a learned value function, our approach can be generally applied to different tasks beyond reasoning (such as RLHF alignment), and LLMs of any size, without prompting a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#26412;&#35821;&#27861;&#20070;&#20013;&#23398;&#20064;&#32763;&#35793;&#26032;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;MTOB&#65292;&#29992;&#20110;&#32763;&#35793;&#33521;&#35821;&#21644;Kalamang&#20043;&#38388;&#30340;&#25991;&#26412;&#65292;&#25506;&#32034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#20917;&#19979;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2309.16575</link><description>&lt;p&gt;
&#20174;&#19968;&#26412;&#35821;&#27861;&#20070;&#23398;&#20064;&#32763;&#35793;&#26032;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark for Learning to Translate a New Language from One Grammar Book. (arXiv:2309.16575v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#19968;&#26412;&#35821;&#27861;&#20070;&#20013;&#23398;&#20064;&#32763;&#35793;&#26032;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;MTOB&#65292;&#29992;&#20110;&#32763;&#35793;&#33521;&#35821;&#21644;Kalamang&#20043;&#38388;&#30340;&#25991;&#26412;&#65292;&#25506;&#32034;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#24773;&#20917;&#19979;&#30340;&#32763;&#35793;&#38382;&#39064;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#25110;&#36731;&#37327;&#32423;&#24494;&#35843;&#26469;&#23436;&#25104;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20219;&#21153;&#12290;&#20154;&#20204;&#33258;&#28982;&#32780;&#28982;&#22320;&#24819;&#30693;&#36947;&#36825;&#20123;&#27169;&#22411;&#22312;&#36866;&#24212;&#20840;&#26032;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#22914;&#20309;&#65292;&#20294;&#22914;&#20309;&#25214;&#21040;&#22312;&#20114;&#32852;&#32593;&#35268;&#27169;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#21602;&#65311;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#26126;&#30830;&#21463;&#21040;&#32593;&#32476;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#39537;&#21160;&#21644;&#38480;&#21046;&#30340;&#39046;&#22495;&#65306;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;MTOB&#65288;&#20174;&#19968;&#26412;&#20070;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#33521;&#35821;&#21644;Kalamang&#20043;&#38388;&#32763;&#35793;--Kalamang&#26159;&#19968;&#31181;&#21482;&#26377;&#19981;&#21040;200&#21517;&#20351;&#29992;&#32773;&#30340;&#35821;&#35328;&#65292;&#22240;&#27492;&#22312;&#32593;&#32476;&#19978;&#20960;&#20046;&#27809;&#26377;&#23384;&#22312;&#24863;--&#25105;&#20204;&#20351;&#29992;&#20102;&#20960;&#30334;&#39029;&#30340;&#30000;&#37326;&#35821;&#35328;&#23398;&#21442;&#32771;&#36164;&#26009;&#12290;&#36825;&#31181;&#20219;&#21153;&#26694;&#26550;&#30340;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#65292;&#23427;&#35201;&#27714;&#27169;&#22411;&#20174;&#19968;&#26412;&#20154;&#31867;&#21487;&#35835;&#30340;&#35821;&#27861;&#35299;&#37322;&#20070;&#20013;&#23398;&#20064;&#19968;&#31181;&#35821;&#35328;&#65292;&#32780;&#19981;&#26159;&#20174;&#22823;&#35268;&#27169;&#25366;&#25496;&#30340;&#39046;&#22495;&#20869;&#25968;&#25454;&#35821;&#26009;&#24211;&#20013;&#23398;&#20064;&#65292;&#26356;&#31867;&#20284;&#20110;L2&#23398;&#20064;&#32780;&#19981;&#26159;L1&#20064;&#24471;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#20351;&#29992;&#24403;&#21069;LLMs&#30340;&#22522;&#20934;&#27979;&#35797;&#20855;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#28982;&#19981;&#21450;&#20154;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can perform impressive feats with in-context learning or lightweight finetuning. It is natural to wonder how well these models adapt to genuinely new tasks, but how does one find tasks that are unseen in internet-scale training sets? We turn to a field that is explicitly motivated and bottlenecked by a scarcity of web data: low-resource languages. In this paper, we introduce MTOB (Machine Translation from One Book), a benchmark for learning to translate between English and Kalamang -- a language with less than 200 speakers and therefore virtually no presence on the web -using several hundred pages of field linguistics reference materials. This task framing is novel in that it asks a model to learn a language from a single human-readable book of grammar explanations, rather than a large mined corpus of in-domain data, more akin to L2 learning than L1 acquisition. We demonstrate that baselines using current LLMs are promising but fall short of human perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08345</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#19982;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#31561;&#29616;&#23454;&#29615;&#22659;&#30340;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27424;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24433;&#21709;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#24212;&#29992;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#8220;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#8221;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;LM&#22312;&#22788;&#29702;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26102;&#25152;&#36935;&#21040;&#30340;&#20581;&#22766;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#35206;&#30422;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#25581;&#31034;&#20102;&#21363;&#20351;&#22312;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#36827;&#30340;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.06358</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#25552;&#39640;&#38382;&#31572;&#20013;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative Data Augmentation using LLMs improves Distributional Robustness in Question Answering. (arXiv:2309.06358v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#29983;&#25104;&#24335;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#22914;&#20309;&#25552;&#39640;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23454;&#39564;&#23637;&#31034;&#20102;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#40065;&#26834;&#24615;&#38382;&#39064;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#38382;&#31572;&#29615;&#22659;&#20013;&#65292;&#23545;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#30340;&#30740;&#31350;&#24037;&#20316;&#20173;&#22312;&#19981;&#26029;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#22495;&#27867;&#21270;&#27010;&#24565;&#21364;&#21463;&#21040;&#24456;&#23569;&#20851;&#27880;&#65292;&#22240;&#20026;&#30446;&#26631;&#22495;&#26159;&#26410;&#30693;&#30340;&#12290;&#38543;&#30528;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#21644;&#33719;&#21462;&#26041;&#24335;&#30340;&#22823;&#24133;&#25552;&#39640;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#22914;&#20309;&#24433;&#21709;&#38382;&#31572;&#27169;&#22411;&#22312;&#33258;&#28982;&#20998;&#24067;&#36716;&#25442;&#19979;&#30340;&#24615;&#33021;&#65311;&#25105;&#20204;&#22312;4&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#8220;&#37326;&#22806;&#29983;&#25104;&#8221;&#22914;&#20309;&#24110;&#21161;&#23454;&#29616;&#22495;&#27867;&#21270;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#20004;&#27493;&#29983;&#25104;&#26041;&#27861;&#65292;&#29983;&#25104;&#19978;&#19979;&#25991;&#21644;&#38382;&#31572;&#23545;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22686;&#24378;&#38405;&#35835;&#29702;&#35299;&#25968;&#25454;&#38598;&#26469;&#25552;&#21319;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robustness in Natural Language Processing continues to be a pertinent issue, where state of the art models under-perform under naturally shifted distributions. In the context of Question Answering, work on domain adaptation methods continues to be a growing body of research. However, very little attention has been given to the notion of domain generalization under natural distribution shifts, where the target domain is unknown. With drastic improvements in the quality and access to generative models, we answer the question: How do generated datasets influence the performance of QA models under natural distribution shifts? We perform experiments on 4 different datasets under varying amounts of distribution shift, and analyze how "in-the-wild" generation can help achieve domain generalization. We take a two-step generation approach, generating both contexts and QA pairs to augment existing datasets. Through our experiments, we demonstrate how augmenting reading comprehension datasets wit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2309.00155</link><description>&lt;p&gt;
LLM&#22312;Shell&#20013;&#30340;&#24212;&#29992;&#65306;&#29983;&#25104;&#24335;&#34588;&#32592;
&lt;/p&gt;
&lt;p&gt;
LLM in the Shell: Generative Honeypots. (arXiv:2309.00155v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#65292;&#35299;&#20915;&#20102;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#32592;&#26159;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#34588;&#32592;&#65288;&#21363;&#20351;&#26159;&#39640;&#20132;&#20114;&#24335;&#30340;&#65289;&#32570;&#20047;&#36275;&#22815;&#30340;&#30495;&#23454;&#24863;&#26469;&#27450;&#39575;&#25915;&#20987;&#32773;&#12290;&#36825;&#20010;&#38480;&#21046;&#20351;&#24471;&#23427;&#20204;&#24456;&#23481;&#26131;&#34987;&#35782;&#21035;&#65292;&#20174;&#32780;&#24433;&#21709;&#21040;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#21019;&#24314;&#21160;&#24577;&#21644;&#30495;&#23454;&#30340;&#36719;&#20214;&#34588;&#32592;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#33021;&#22815;&#21019;&#24314;&#21487;&#20449;&#19988;&#21160;&#24577;&#30340;&#34588;&#32592;&#65292;&#33021;&#22815;&#35299;&#20915;&#20197;&#24448;&#34588;&#32592;&#30340;&#37325;&#35201;&#23616;&#38480;&#24615;&#65292;&#22914;&#30830;&#23450;&#24615;&#21709;&#24212;&#12289;&#32570;&#20047;&#36866;&#24212;&#24615;&#31561;&#12290;&#25105;&#20204;&#36890;&#36807;&#19982;&#38656;&#35201;&#21028;&#26029;&#34588;&#32592;&#22238;&#24212;&#26159;&#21542;&#34394;&#20551;&#30340;&#25915;&#20987;&#32773;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#27599;&#20010;&#21629;&#20196;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#34588;&#32592;&#65292;&#31216;&#20026;shelLM&#65292;&#36798;&#21040;&#20102;0.92&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Honeypots are essential tools in cybersecurity. However, most of them (even the high-interaction ones) lack the required realism to engage and fool human attackers. This limitation makes them easily discernible, hindering their effectiveness. This work introduces a novel method to create dynamic and realistic software honeypots based on Large Language Models. Preliminary results indicate that LLMs can create credible and dynamic honeypots capable of addressing important limitations of previous honeypots, such as deterministic responses, lack of adaptability, etc. We evaluated the realism of each command by conducting an experiment with human attackers who needed to say if the answer from the honeypot was fake or not. Our proposed honeypot, called shelLM, reached an accuracy rate of 0.92.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.10635</link><description>&lt;p&gt;
SciBench: &#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. (arXiv:2307.10635v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10635
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;SciBench&#30340;&#22522;&#20934;&#22871;&#20214;&#65292;&#26088;&#22312;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23398;&#27700;&#24179;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#22312;&#35768;&#22810;&#25968;&#23398;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#22823;&#22810;&#21482;&#21253;&#21547;&#21021;&#39640;&#20013;&#31185;&#30446;&#30340;&#38382;&#39064;&#65292;&#20165;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#65292;&#24182;&#19988;&#20165;&#38480;&#20110;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#33539;&#22260;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#22522;&#20934;&#22871;&#20214;SciBench&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#27979;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#25152;&#38656;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;SciBench&#21253;&#21547;&#20004;&#20010;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#24320;&#25918;&#38598;&#65292;&#21253;&#25324;&#20174;&#25968;&#23398;&#12289;&#21270;&#23398;&#21644;&#29289;&#29702;&#25945;&#31185;&#20070;&#20013;&#25688;&#24405;&#30340;&#22823;&#23398;&#27700;&#24179;&#30340;&#31185;&#23398;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#23553;&#38381;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#25968;&#23398;&#26412;&#31185;&#32771;&#35797;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#36825;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;LLM&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;LLMs&#22312;&#25552;&#20379;&#22797;&#26434;&#31185;&#23398;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#26041;&#38754;&#36824;&#23384;&#22312;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have demonstrated notable progress on many mathematical benchmarks. However, most of these benchmarks only feature problems grounded in junior and senior high school subjects, contain only multiple-choice questions, and are confined to a limited scope of elementary arithmetic operations. To address these issues, this paper introduces an expansive benchmark suite SciBench that aims to systematically examine the reasoning capabilities required for complex scientific problem solving. SciBench contains two carefully curated datasets: an open set featuring a range of collegiate-level scientific problems drawn from mathematics, chemistry, and physics textbooks, and a closed set comprising problems from undergraduate-level exams in computer science and mathematics. Based on the two datasets, we conduct an in-depth benchmark study of two representative LLMs with various prompting strategies. The results reveal that current LLMs fall short of deli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#24110;&#21161;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.18582</link><description>&lt;p&gt;
&#36890;&#36807;&#20943;&#23569;&#26292;&#38706;&#20559;&#24046;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
Self Information Update for Large Language Models through Mitigating Exposure Bias. (arXiv:2305.18582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#26469;&#24110;&#21161;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;&#26292;&#38706;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20449;&#24687;&#35831;&#27714;&#26041;&#38754;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#21463;&#20854;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#26368;&#26032;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#20351;&#20854;&#26080;&#27861;&#25552;&#20379;&#26368;&#26032;&#30340;&#20449;&#24687;&#12290;&#20174;&#22836;&#24320;&#22987;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#20195;&#20215;&#36739;&#39640;&#65292;&#24182;&#19988;&#23545;&#26032;&#35821;&#26009;&#24211;&#36827;&#34892;&#36830;&#32493;&#24494;&#35843;&#30340;&#25928;&#26524;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#26816;&#26597;&#12290;&#27492;&#22806;&#65292;&#30446;&#21069;&#30340;&#26356;&#26032;&#31243;&#24207;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#25237;&#20837;&#65292;&#23558;&#20449;&#24687;&#20934;&#22791;&#25104;&#26356;&#32467;&#26500;&#21270;&#30340;&#24418;&#24335;&#65292;&#22914;&#30693;&#35782;&#19977;&#20803;&#32452;&#12289;&#23545;&#35805;&#25968;&#25454;&#25110;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#21709;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#20219;&#21153;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#36825;&#21482;&#38656;&#35201;&#25552;&#20379;&#20449;&#24687;&#20016;&#23500;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#26368;&#26032;&#30340;&#26032;&#38395;&#25991;&#31456;&#26469;&#26356;&#26032;LLM&#30340;&#29616;&#26377;&#30693;&#35782;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#33258;&#25105;&#20449;&#24687;&#26356;&#26032;&#20219;&#21153;&#65292;&#24182;&#35780;&#20272;&#20102;&#36830;&#32493;&#24494;&#35843;&#26041;&#27861;&#22312;&#27492;&#20219;&#21153;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current LLMs have demonstrated remarkable capabilities in addressing users' requests for various types of information. However, these models are limited by the most recent data available in their pretraining corpora, rendering them incapable of providing up-to-date information. Retraining LLMs from scratch is cost-prohibitive, and the effectiveness of continual fine-tuning on new corpora has not been thoroughly examined. Additionally, current update procedures typically demand significant human input to prepare the information into more structured format, such as knowledge triples, conversational data or responses with human feedback. In this study, we conduct a comprehensive examination of a novel self information update task in LLMs, which only requires the provision of informative text corpora. For instance, we can use the latest news articles to update the LLMs' existing knowledge. We define the self information update task and assess the continual fine-tuning approach for this pur
&lt;/p&gt;</description></item></channel></rss>