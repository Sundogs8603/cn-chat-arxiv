<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01345</link><description>&lt;p&gt;
&#36339;&#36807;$\textbackslash n$: &#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#20943;&#23569;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Skip $\textbackslash n$: A simple method to reduce hallucination in Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#25351;&#20986;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#36890;&#36807;&#31995;&#32479;&#35782;&#21035;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#25105;&#20204;&#21457;&#29616;&#27169;&#22411;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32463;&#24120;&#36935;&#21040;&#26126;&#26174;&#30340;&#20869;&#23481;&#35821;&#20041;&#21464;&#21270;&#65292;&#23548;&#33268;&#24187;&#35273;&#30340;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#30340;&#36827;&#23637;&#23637;&#31034;&#20102;&#20854;&#22312;&#35270;&#35273;&#20449;&#24687;&#29702;&#35299;&#19982;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;LVLMs&#20173;&#28982;&#38754;&#20020;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#29983;&#25104;&#19982;&#35270;&#35273;&#20449;&#24687;&#20013;&#19981;&#23384;&#22312;&#30340;&#23545;&#35937;&#30456;&#20851;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#22810;&#27169;&#24577;&#24187;&#35273;&#30340;&#26681;&#26412;&#21407;&#22240;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35270;&#35282;&#65292;&#35748;&#20026;LVLMs&#20013;&#22266;&#26377;&#30340;&#20559;&#35265;&#21487;&#33021;&#26159;&#24187;&#35273;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#30830;&#23450;&#20102;&#19982;&#27573;&#33853;&#20998;&#21106;&#31526;&#65288;'$\textbackslash n\textbackslash n$'&#65289;&#30456;&#20851;&#30340;&#35821;&#20041;&#28418;&#31227;&#20559;&#24046;&#65292;&#21363;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21069;&#21644;&#20043;&#21518;&#30340;&#20869;&#23481;&#32463;&#24120;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#35821;&#20041;&#25913;&#21464;&#12290;&#36825;&#31181;&#27169;&#24335;&#20351;&#24471;&#27169;&#22411;&#25512;&#26029;&#22312;&#8220;$\textbackslash n\textbackslash n$&#8221;&#20043;&#21518;&#30340;&#20869;&#23481;&#24212;&#26126;&#26174;&#19981;&#21516;&#20110;&#21069;&#38754;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large vision-language models (LVLMs) have demonstrated impressive capability in visual information understanding with human language. Despite these advances, LVLMs still face challenges with multimodal hallucination, such as generating text descriptions of objects that are not present in the visual information. However, the underlying fundamental reasons of multimodal hallucinations remain poorly explored. In this paper, we propose a new perspective, suggesting that the inherent biases in LVLMs might be a key factor in hallucinations. Specifically, we systematically identify a semantic shift bias related to paragraph breaks ('$\textbackslash n\textbackslash n$'), where the content before and after '$\textbackslash n\textbackslash n$' in the training data frequently exhibit significant semantic changes. This pattern leads the model to infer that the contents following '$\textbackslash n\textbackslash n$' should be obviously different from the preceding contents wi
&lt;/p&gt;</description></item><item><title>AnyTool&#26159;&#19968;&#20010;&#33258;&#25105;&#21453;&#24605;&#12289;&#23618;&#27425;&#21270;&#20195;&#29702;&#31995;&#32479;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;API&#35843;&#29992;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;16000&#22810;&#20010;API&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#65292;&#24182;&#20855;&#26377;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#20197;&#24212;&#23545;&#38382;&#39064;&#12290;&#19982;&#20043;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AnyTool&#22312;&#22823;&#35268;&#27169;API&#35843;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.04253</link><description>&lt;p&gt;
AnyTool: &#22823;&#35268;&#27169;API&#35843;&#29992;&#30340;&#33258;&#25105;&#21453;&#24605;&#12289;&#23618;&#27425;&#21270;&#20195;&#29702;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04253
&lt;/p&gt;
&lt;p&gt;
AnyTool&#26159;&#19968;&#20010;&#33258;&#25105;&#21453;&#24605;&#12289;&#23618;&#27425;&#21270;&#20195;&#29702;&#31995;&#32479;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;API&#35843;&#29992;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;16000&#22810;&#20010;API&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#65292;&#24182;&#20855;&#26377;&#37325;&#26032;&#28608;&#27963;&#26426;&#21046;&#20197;&#24212;&#23545;&#38382;&#39064;&#12290;&#19982;&#20043;&#21069;&#24037;&#20316;&#30456;&#27604;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AnyTool&#22312;&#22823;&#35268;&#27169;API&#35843;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;AnyTool&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#31995;&#32479;&#65292;&#26088;&#22312;&#38761;&#26032;&#21033;&#29992;&#21508;&#31181;&#24037;&#20855;&#26469;&#35299;&#20915;&#29992;&#25143;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;Rapid API&#30340;16000&#22810;&#20010;API&#65292;&#20551;&#35774;&#20854;&#20013;&#19968;&#37096;&#20998;API&#21487;&#33021;&#33021;&#22815;&#35299;&#20915;&#26597;&#35810;&#12290;AnyTool&#20027;&#35201;&#21253;&#25324;&#19977;&#20010;&#20803;&#32032;&#65306;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;API&#26816;&#32034;&#22120;&#12289;&#35299;&#20915;&#22120;&#20197;&#21450;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#65292;&#21518;&#32773;&#33021;&#22815;&#22312;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#26080;&#27861;&#23454;&#29616;&#26102;&#37325;&#26032;&#28608;&#27963;AnyTool&#12290;AnyTool&#20381;&#38752;GPT-4&#30340;&#20989;&#25968;&#35843;&#29992;&#21151;&#33021;&#65292;&#26080;&#38656;&#35757;&#32451;&#22806;&#37096;&#27169;&#22359;&#12290;&#25105;&#20204;&#36824;&#37325;&#26032;&#23457;&#35270;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#24341;&#20837;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#21457;&#29616;&#35813;&#21327;&#35758;&#23384;&#22312;&#19968;&#20010;&#38480;&#21046;&#65292;&#23548;&#33268;&#36890;&#36807;&#29575;&#20154;&#20026;&#19978;&#21319;&#12290;&#36890;&#36807;&#20462;&#25913;&#35780;&#20272;&#21327;&#35758;&#20197;&#26356;&#22909;&#22320;&#21453;&#26144;&#23454;&#38469;&#24212;&#29992;&#22330;&#26223;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AnyToolBench&#30340;&#38468;&#21152;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;AnyTool &#22312;&#22823;&#35268;&#27169;API&#35843;&#29992;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#21442;&#32771;&#34920;&#31034;&#26469;&#36817;&#20284;&#37197;&#23545;&#24230;&#37327;&#20998;&#25968;&#65292;&#23558;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#22823;&#37096;&#20998;&#36136;&#37327;&#22686;&#30410;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04251</link><description>&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#19982;&#21442;&#32771;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Linear-time Minimum Bayes Risk Decoding with Reference Aggregation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26102;&#38388;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#32858;&#21512;&#21442;&#32771;&#34920;&#31034;&#26469;&#36817;&#20284;&#37197;&#23545;&#24230;&#37327;&#20998;&#25968;&#65292;&#23558;&#22797;&#26434;&#24230;&#38477;&#20302;&#21040;&#32447;&#24615;&#32423;&#21035;&#65292;&#21516;&#26102;&#22312;&#20445;&#25345;&#22823;&#37096;&#20998;&#36136;&#37327;&#22686;&#30410;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#26159;&#19968;&#31181;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#65292;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#20294;&#21363;&#20351;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#36817;&#20284;&#26041;&#27861;&#20063;&#24456;&#26114;&#36149;&#12290;&#38500;&#20102;&#38656;&#35201;&#22823;&#37327;&#37319;&#26679;&#24207;&#21015;&#22806;&#65292;&#36824;&#38656;&#35201;&#23545;&#25928;&#29992;&#24230;&#37327;&#36827;&#34892;&#37197;&#23545;&#35745;&#31639;&#65292;&#36825;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32858;&#21512;&#21442;&#32771;&#34920;&#31034;&#35745;&#31639;&#36817;&#20284;&#30340;&#37197;&#23545;&#24230;&#37327;&#20998;&#25968;&#12290;&#36825;&#23558;&#25928;&#29992;&#20272;&#35745;&#30340;&#22797;&#26434;&#24230;&#20174;$O(n^2)$&#38477;&#20302;&#21040;$O(n)$&#65292;&#21516;&#26102;&#22312;&#32463;&#39564;&#19978;&#20445;&#25345;&#20102;MBR&#35299;&#30721;&#30340;&#22823;&#37096;&#20998;&#36136;&#37327;&#25552;&#21319;&#12290;&#25105;&#20204;&#22312;https://github.com/ZurichNLP/mbr&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr
&lt;/p&gt;</description></item><item><title>HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04249</link><description>&lt;p&gt;
HarmBench&#65306;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04249
&lt;/p&gt;
&lt;p&gt;
HarmBench&#26159;&#19968;&#20010;&#20026;&#33258;&#21160;&#32418;&#38431;&#21644;&#24378;&#22823;&#25298;&#32477;&#35774;&#35745;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#30340;&#27604;&#36739;&#65292;&#24471;&#20986;&#20102;&#26032;&#30340;&#35265;&#35299;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#32418;&#38431;&#20855;&#26377;&#21457;&#29616;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24694;&#24847;&#20351;&#29992;&#30340;&#39118;&#38505;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#32780;&#35813;&#39046;&#22495;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#20005;&#26684;&#35780;&#20272;&#26032;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;HarmBench&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#32418;&#38431;&#30340;&#26631;&#20934;&#21270;&#35780;&#20272;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#32418;&#38431;&#35780;&#20272;&#20013;&#30830;&#23450;&#20102;&#20960;&#20010;&#20197;&#21069;&#26410;&#32771;&#34385;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#35774;&#35745;&#20102;HarmBench&#20197;&#28385;&#36275;&#36825;&#20123;&#26631;&#20934;&#12290;&#20351;&#29992;HarmBench&#65292;&#25105;&#20204;&#23545;18&#31181;&#32418;&#38431;&#26041;&#27861;&#21644;33&#20010;&#30446;&#26631;LLM&#21644;&#38450;&#24481;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#27604;&#36739;&#65292;&#24471;&#21040;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#21508;&#31181;&#25915;&#20987;&#19979;&#30340;&#31283;&#20581;&#24615;&#65292;&#23637;&#31034;&#20102;HarmBench&#22914;&#20309;&#20419;&#36827;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#20849;&#21516;&#24320;&#21457;&#12290;&#25105;&#20204;&#22312;https://github.com/centerforaisafety/HarmBench&#19978;&#24320;&#28304;&#20102;HarmBench&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04247</link><description>&lt;p&gt;
&#20248;&#20808;&#23433;&#20840;&#20445;&#38556;&#32780;&#38750;&#33258;&#27835;&#65306;&#31185;&#23398;&#20013;LLM&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#19982;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#26234;&#33021;&#26426;&#22120;&#20154;&#22312;&#21508;&#20010;&#23398;&#31185;&#20013;&#33258;&#20027;&#36827;&#34892;&#23454;&#39564;&#21644;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#12290;&#23613;&#31649;&#23427;&#20204;&#30340;&#33021;&#21147;&#38750;&#24120;&#26377;&#21069;&#36884;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19968;&#20123;&#26032;&#30340;&#28431;&#27934;&#65292;&#38656;&#35201;&#20180;&#32454;&#32771;&#34385;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#23384;&#22312;&#26174;&#33879;&#30340;&#31354;&#30333;&#65292;&#23578;&#26410;&#23545;&#36825;&#20123;&#28431;&#27934;&#36827;&#34892;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#31185;&#23398;&#39046;&#22495;&#20013;&#22522;&#20110;LLM&#30340;&#26426;&#22120;&#20154;&#30340;&#28431;&#27934;&#36827;&#34892;&#28145;&#20837;&#30740;&#31350;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#35823;&#29992;&#21487;&#33021;&#24102;&#26469;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23433;&#20840;&#25514;&#26045;&#30340;&#38656;&#27714;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#39318;&#20808;&#20840;&#38754;&#27010;&#36848;&#20102;&#31185;&#23398;LLM&#26426;&#22120;&#20154;&#22266;&#26377;&#30340;&#28508;&#22312;&#39118;&#38505;&#65292;&#32771;&#34385;&#20102;&#29992;&#25143;&#24847;&#22270;&#12289;&#29305;&#23450;&#30340;&#31185;&#23398;&#39046;&#22495;&#20197;&#21450;&#23427;&#20204;&#23545;&#22806;&#37096;&#29615;&#22659;&#21487;&#33021;&#36896;&#25104;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20123;&#28431;&#27934;&#30340;&#36215;&#28304;&#21644;&#25552;&#20379;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#22791;&#25805;&#20316;&#38142;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#65292;&#24182;&#20197;&#20854;&#35777;&#25454;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.04236</link><description>&lt;p&gt;
CogCoM: &#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25805;&#20316;&#35757;&#32451;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#28145;&#20837;&#32454;&#33410;
&lt;/p&gt;
&lt;p&gt;
CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#22791;&#25805;&#20316;&#38142;&#26426;&#21046;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#25805;&#20316;&#35299;&#20915;&#35270;&#35273;&#38382;&#39064;&#65292;&#24182;&#20197;&#20854;&#35777;&#25454;&#24615;&#30340;&#35270;&#35273;&#25512;&#29702;&#33021;&#21147;&#23454;&#29616;&#24544;&#23454;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#36890;&#36807;&#24191;&#27867;&#30340;&#35757;&#32451;&#65292;&#22312;&#23558;&#35270;&#35273;&#25351;&#20196;&#19982;&#31572;&#26696;&#23545;&#40784;&#26041;&#38754;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#21487;&#34892;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30830;&#23450;&#24615;&#30340;&#23545;&#40784;&#23548;&#33268;&#27169;&#22411;&#24573;&#35270;&#20102;&#20851;&#38190;&#30340;&#35270;&#35273;&#25512;&#29702;&#65292;&#24182;&#23548;&#33268;&#22312;&#32454;&#33268;&#30340;&#35270;&#35273;&#38382;&#39064;&#21644;&#19981;&#24544;&#23454;&#30340;&#21709;&#24212;&#26041;&#38754;&#22833;&#36133;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#25805;&#20316;&#38142;&#8221;&#30340;&#26426;&#21046;&#65292;&#20351;VLM&#33021;&#22815;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25805;&#20316;&#26469;&#35299;&#20915;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#25805;&#20316;&#37117;&#25351;&#30340;&#26159;&#23545;&#35270;&#35273;&#36755;&#20837;&#30340;&#25805;&#20316;&#65292;&#21487;&#20197;&#26159;&#36890;&#36807;&#20808;&#21069;&#35757;&#32451;&#33719;&#24471;&#30340;&#20869;&#22312;&#33021;&#21147;&#65288;&#20363;&#22914;&#65292;&#22522;&#30784;&#65289;&#25110;&#32773;&#26159;&#27169;&#20223;&#31867;&#20154;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#25918;&#22823;&#65289;&#12290;&#36825;&#20010;&#26426;&#21046;&#40723;&#21169;VLM&#29983;&#25104;&#24102;&#26377;&#35777;&#25454;&#30340;&#35270;&#35273;&#25512;&#29702;&#30340;&#24544;&#23454;&#30340;&#21709;&#24212;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#22312;&#21487;&#35299;&#37322;&#30340;&#36335;&#24452;&#19978;&#36861;&#36394;&#38169;&#35823;&#30340;&#21407;&#22240;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;CogCoM&#65292;&#19968;&#20010;&#20855;&#26377;&#20869;&#32622;&#25512;&#29702;&#26426;&#21046;&#30340;17B&#36890;&#29992;VLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;</title><link>https://arxiv.org/abs/2402.04232</link><description>&lt;p&gt;
&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#33021;&#22815;&#39044;&#27979;&#24773;&#24863;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Agents Predict Emotion?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04232
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24773;&#24863;&#27979;&#35797;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#35768;&#22810;&#31867;&#20284;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;LLMs&#30340;&#20849;&#24773;&#29702;&#35299;&#21644;&#24773;&#32490;&#29366;&#24577;&#23578;&#26410;&#19982;&#20154;&#31867;&#23545;&#40784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#29983;&#25104;&#22411;LLM&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#26032;&#20107;&#20214;&#26102;&#24773;&#32490;&#29366;&#24577;&#30340;&#28436;&#21464;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#27604;&#36739;&#26032;&#32463;&#39564;&#21644;&#36807;&#21435;&#35760;&#24518;&#26469;&#33719;&#24471;&#29702;&#35299;&#26032;&#32463;&#39564;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#24773;&#22659;&#20013;&#29702;&#35299;&#26032;&#32463;&#39564;&#65292;&#26681;&#25454;&#24773;&#32490;&#35780;&#20272;&#29702;&#35770;&#65292;&#36825;&#23545;&#20110;&#24773;&#32490;&#29983;&#25104;&#33267;&#20851;&#37325;&#35201;&#12290;&#39318;&#20808;&#65292;&#26234;&#33021;&#20307;&#23558;&#26032;&#32463;&#39564;&#24863;&#30693;&#20026;&#26102;&#38388;&#24207;&#21015;&#25991;&#26412;&#25968;&#25454;&#12290;&#22312;&#24863;&#30693;&#27599;&#20010;&#26032;&#36755;&#20837;&#21518;&#65292;&#26234;&#33021;&#20307;&#29983;&#25104;&#36807;&#21435;&#30456;&#20851;&#35760;&#24518;&#30340;&#25688;&#35201;&#65292;&#34987;&#31216;&#20026;&#8220;&#35268;&#33539;&#8221;&#65292;&#24182;&#23558;&#26032;&#32463;&#39564;&#19982;&#27492;&#35268;&#33539;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#27604;&#36739;&#65292;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#26234;&#33021;&#20307;&#22914;&#20309;&#22312;&#24773;&#22659;&#20013;&#23545;&#26032;&#32463;&#39564;&#20570;&#20986;&#21453;&#24212;&#12290;&#20351;&#29992;&#24773;&#24863;&#27979;&#35797;PANAS&#23545;&#26234;&#33021;&#20307;&#36827;&#34892;&#27979;&#35797;&#65292;&#25429;&#25417;&#26234;&#33021;&#20307;&#22312;&#26032;&#32463;&#39564;&#21518;&#30340;&#24773;&#32490;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.04222</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is 'Typological Diversity' in NLP?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#19981;&#21516;&#35770;&#25991;&#23545;&#20110;&#36825;&#19968;&#27010;&#24565;&#30340;&#23450;&#20041;&#21644;&#26631;&#20934;&#21508;&#19981;&#30456;&#21516;&#65292;&#24341;&#20837;&#20102;&#22810;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#23384;&#22312;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
NLP&#30740;&#31350;&#30028;&#23545;&#33521;&#35821;&#20197;&#22806;&#30340;&#35821;&#35328;&#25237;&#20837;&#26356;&#22810;&#20851;&#27880;&#65292;&#20174;&#32780;&#22312;&#22810;&#35821;&#35328;NLP&#26041;&#38754;&#21462;&#24471;&#20102;&#21487;&#35266;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25913;&#36827;&#21482;&#36866;&#29992;&#20110;&#19990;&#30028;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#20026;&#20102;&#25193;&#23637;&#36825;&#19968;&#33539;&#22260;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#35770;&#25991;&#33268;&#21147;&#20110;&#25552;&#39640;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#35821;&#35328;&#31867;&#22411;&#23398;&#24120;&#34987;&#29992;&#26469;&#36873;&#25321;&#35821;&#35328;&#65292;&#22522;&#20110;&#24191;&#27867;&#30340;&#35821;&#35328;&#31867;&#22411;&#26679;&#26412;&#24212;&#33021;&#24102;&#26469;&#23545;&#22810;&#31181;&#35821;&#35328;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#20123;&#36873;&#25321;&#36890;&#24120;&#34987;&#25551;&#36848;&#20026;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#21253;&#21547;&#8220;&#35821;&#35328;&#31867;&#22411;&#22810;&#26679;&#24615;&#8221;&#20027;&#24352;&#30340;NLP&#30740;&#31350;&#65292;&#21457;&#29616;&#27809;&#26377;&#30830;&#20999;&#30340;&#23450;&#20041;&#25110;&#26631;&#20934;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20010;&#32500;&#24230;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#36817;&#20284;&#35821;&#35328;&#36873;&#25321;&#30340;&#22810;&#26679;&#24615;&#65292;&#24182;&#21457;&#29616;&#32467;&#26524;&#22312;&#19981;&#21516;&#35770;&#25991;&#20013;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#35328;&#36873;&#25321;&#30340;&#20559;&#21521;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language sele
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#65292;&#21457;&#29616;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#23545;&#19979;&#28216;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04177</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#30340;&#23610;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws for Downstream Task Performance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23610;&#24230;&#34892;&#20026;&#65292;&#21457;&#29616;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#23545;&#19979;&#28216;&#24615;&#33021;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23610;&#24230;&#24459;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#65292;&#21487;&#20197;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35774;&#35745;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#30740;&#31350;&#39044;&#35757;&#32451;&#65288;&#19978;&#28216;&#65289;&#25439;&#22833;&#30340;&#23610;&#24230;&#24459;&#12290;&#28982;&#32780;&#65292;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;LLM&#20808;&#22312;&#26080;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#25105;&#20204;&#36890;&#24120;&#20063;&#20851;&#24515;&#19979;&#28216;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36716;&#31227;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#23610;&#24230;&#34892;&#20026;&#65292;&#20854;&#20013;LLM&#34987;&#24494;&#35843;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#36873;&#25321;&#21644;&#22823;&#23567;&#23545;&#19979;&#28216;&#24615;&#33021;&#65288;&#32763;&#35793;&#36136;&#37327;&#65289;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#35780;&#20215;&#25351;&#26631;&#65306;&#19979;&#28216;&#20132;&#21449;&#29109;&#21644;BLEU&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#24494;&#35843;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21644;&#39044;&#35757;&#32451;&#25968;&#25454;&#19982;&#19979;&#28216;&#25968;&#25454;&#30340;&#20998;&#24067;&#19968;&#33268;&#24615;&#26174;&#33879;&#24433;&#21709;&#23610;&#24230;&#34892;&#20026;&#12290;&#22312;&#20805;&#20998;&#19968;&#33268;&#24615;&#24773;&#20917;&#19979;&#65292;&#19979;&#28216;&#20132;&#21449;&#29109;&#21644;BLEU&#20998;&#25968;&#37117;&#20250;&#36880;&#28176;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04161</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35268;&#33539;&#20998;&#26512;&#26694;&#26550;&#65306;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#27169;&#22411;&#22312;&#27492;&#36807;&#31243;&#20013;&#36890;&#36807;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#65292;&#20801;&#35768;&#29702;&#35770;&#21644;&#31995;&#32479;&#23454;&#39564;&#26469;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#28304;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12289;Transformer&#26550;&#26500;&#12289;&#23398;&#21040;&#30340;&#20998;&#24067;&#21644;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#25968;&#25454;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#25991;&#26412;&#23646;&#24615;&#30340;&#28789;&#27963;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#27969;&#30021;&#24615;&#65292;&#36890;&#36807;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#20351;&#29992;&#21363;&#25554;&#21363;&#29992;&#25511;&#21046;&#22120;&#65288;PPCs&#65289;&#65292;&#21160;&#24577;&#35843;&#25972;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2402.04160</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#26469;&#25511;&#21046;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#25511;&#21046;&#22120;&#30340;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
Harnessing the Plug-and-Play Controller by Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23454;&#29616;&#20102;&#23545;&#29983;&#25104;&#25991;&#26412;&#23646;&#24615;&#30340;&#28789;&#27963;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#27969;&#30021;&#24615;&#65292;&#36890;&#36807;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#20351;&#29992;&#21363;&#25554;&#21363;&#29992;&#25511;&#21046;&#22120;&#65288;PPCs&#65289;&#65292;&#21160;&#24577;&#35843;&#25972;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20013;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#20854;&#19987;&#27880;&#20110;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#29983;&#25104;&#28385;&#36275;&#29305;&#23450;&#32422;&#26463;&#30340;&#25991;&#26412;&#12290;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#22914;&#21363;&#25554;&#21363;&#29992;&#25511;&#21046;&#22120;&#65288;PPC&#65289;&#65292;&#26088;&#22312;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#24341;&#23548;&#29983;&#25104;&#25991;&#26412;&#30340;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24120;&#24120;&#25439;&#23475;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#30721;&#36807;&#31243;&#30340;&#23436;&#25972;&#24615;&#65292;&#23548;&#33268;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#19979;&#38477;&#12290;&#21478;&#22806;&#65292;&#20854;&#20182;&#25216;&#26415;&#20351;&#29992;&#22810;&#20010;&#23646;&#24615;&#25552;&#31034;&#26469;&#23558;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#25152;&#38656;&#23646;&#24615;&#23545;&#40784;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#23646;&#24615;&#36827;&#34892;&#25552;&#31034;&#35774;&#35745;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36827;&#34892;&#25991;&#26412;&#29983;&#25104;&#20013;&#28789;&#27963;&#23646;&#24615;&#25511;&#21046;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#20351;&#29992;PPC&#26469;&#25552;&#39640;&#29983;&#25104;&#25991;&#26412;&#30340;&#27969;&#30021;&#24615;&#12290;&#20851;&#38190;&#24605;&#36335;&#26159;&#21160;&#24577;&#35843;&#25972;&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation. Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;GPT-3.5&#21644;GPT-4&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;&#65292;&#37319;&#29992;&#24515;&#29702;&#27979;&#35797;&#21644;&#38382;&#21367;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20004;&#20010;&#27169;&#22411;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;&#24182;&#19981;&#29305;&#21035;&#26126;&#26174;&#65292;&#24046;&#24322;&#36739;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.04110</link><description>&lt;p&gt;
&#23631;&#24149;&#32972;&#21518;&#65306;&#25506;&#31350;ChatGPT&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;
&lt;/p&gt;
&lt;p&gt;
Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;GPT-3.5&#21644;GPT-4&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;&#65292;&#37319;&#29992;&#24515;&#29702;&#27979;&#35797;&#21644;&#38382;&#21367;&#65292;&#24182;&#27604;&#36739;&#20102;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20004;&#20010;&#27169;&#22411;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;&#24182;&#19981;&#29305;&#21035;&#26126;&#26174;&#65292;&#24046;&#24322;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#22240;&#20854;&#19981;&#36879;&#26126;&#30340;&#34892;&#20026;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#26412;&#25991;&#35797;&#22270;&#25581;&#31034;&#36825;&#19968;&#28857;&#65292;&#25552;&#20379;&#20102;&#23545;GPT-3.5&#21644;GPT-4&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;&#12290;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#24515;&#29702;&#27979;&#35797;&#21644;&#38382;&#21367;&#65292;&#21253;&#25324;&#40657;&#26263;&#22240;&#23376;&#27979;&#39564;&#12289;Mach-IV&#37327;&#34920;&#12289;&#19968;&#33324;&#24615;&#38452;&#35851;&#20449;&#20208;&#37327;&#34920;&#21644;&#38452;&#35851;&#24515;&#29702;&#37327;&#34920;&#12290;&#20998;&#26512;&#20102;&#22238;&#31572;&#30340;&#24179;&#22343;&#20998;&#25968;&#12289;&#26631;&#20934;&#24046;&#21644;&#26174;&#33879;&#24615;&#26816;&#39564;&#20197;&#30740;&#31350;GPT-3.5&#21644;GPT-4&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#23545;&#20110;&#22312;&#20154;&#31867;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#23384;&#22312;&#30456;&#20114;&#20381;&#36182;&#30340;&#29305;&#36136;&#65292;&#36824;&#32771;&#34385;&#20102;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23558;&#19982;&#22312;&#30456;&#24212;&#38382;&#21367;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#22238;&#31572;&#34892;&#20026;&#30340;&#32676;&#32452;&#23545;&#24212;&#30340;&#31995;&#32479;&#35282;&#33394;&#24212;&#29992;&#20110;&#30740;&#31350;&#27169;&#22411;&#22312;&#22238;&#31572;&#20013;&#21453;&#26144;&#19982;&#36825;&#20123;&#35282;&#33394;&#30456;&#20851;&#30340;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#20004;&#20010;&#27169;&#22411;&#20013;&#30340;&#40657;&#26263;&#20154;&#26684;&#29305;&#36136;&#21644;&#38452;&#35851;&#20449;&#20208;&#24182;&#19981;&#29305;&#21035;&#26126;&#26174;&#65292;&#24046;&#24322;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#20004;&#20010;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26126;&#30830;&#26080;&#20559;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#20154;&#31867;&#21270;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#65292;&#24182;&#19982;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#20851;&#12290;</title><link>https://arxiv.org/abs/2402.04105</link><description>&lt;p&gt;
&#22312;&#26126;&#30830;&#26080;&#20559;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Implicit Bias in Explicitly Unbiased Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04105
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#20004;&#20010;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26126;&#30830;&#26080;&#20559;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#20102;&#26222;&#36941;&#23384;&#22312;&#30340;&#20154;&#31867;&#21270;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#65292;&#24182;&#19982;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#36890;&#36807;&#26126;&#30830;&#30340;&#20559;&#35265;&#27979;&#35797;&#65292;&#20294;&#20173;&#28982;&#21487;&#33021;&#23384;&#22312;&#38544;&#24615;&#20559;&#35265;&#65292;&#31867;&#20284;&#20110;&#25345;&#26377;&#24179;&#31561;&#20027;&#20041;&#20449;&#24565;&#30340;&#20154;&#20204;&#21364;&#34920;&#29616;&#20986;&#24494;&#22937;&#30340;&#20559;&#35265;&#12290;&#27979;&#37327;&#36825;&#31181;&#38544;&#24615;&#20559;&#35265;&#26159;&#19968;&#39033;&#25361;&#25112;&#65306;&#38543;&#30528;LLMs&#21464;&#24471;&#36234;&#26469;&#36234;&#19987;&#26377;&#65292;&#21487;&#33021;&#26080;&#27861;&#35775;&#38382;&#23427;&#20204;&#30340;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#29616;&#26377;&#30340;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#65307;&#27492;&#22806;&#65292;&#38544;&#24615;&#20559;&#35265;&#20027;&#35201;&#26159;&#19968;&#20010;&#38382;&#39064;&#65292;&#22914;&#26524;&#23427;&#20204;&#24433;&#21709;&#20102;&#36825;&#20123;&#31995;&#32479;&#25152;&#20570;&#30340;&#23454;&#38469;&#20915;&#31574;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#21463;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#20004;&#20010;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#26469;&#24212;&#23545;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;LLMs&#38544;&#21547;&#32852;&#24819;&#27979;&#35797;&#65288;IAT&#65289;&#20559;&#35265;&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#27979;&#37327;&#38544;&#24615;&#20559;&#35265;&#30340;&#26041;&#27861;&#65307;LLMs&#20915;&#31574;&#20559;&#35265;&#29992;&#20110;&#26816;&#27979;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#24494;&#22937;&#27495;&#35270;&#12290;&#20351;&#29992;&#36825;&#20123;&#27979;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;6&#20010;LLMs&#22312;4&#20010;&#31038;&#20250;&#39046;&#22495;&#65288;&#31181;&#26063;&#12289;&#24615;&#21035;&#12289;&#23447;&#25945;&#12289;&#20581;&#24247;&#65289;&#21644;21&#20010;&#31867;&#21035;&#65288;&#27494;&#22120;&#12289;&#32618;&#32602;&#12289;&#31185;&#23398;&#12289;&#32844;&#19994;&#31561;&#65289;&#20013;&#26222;&#36941;&#23384;&#22312;&#20154;&#31867;&#21270;&#30340;&#21051;&#26495;&#21360;&#35937;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#38544;&#24615;&#20559;&#35265;&#27979;&#37327;&#26041;&#27861;&#19982;&#23454;&#38469;&#20915;&#31574;&#20013;&#30340;&#38544;&#24615;&#20559;&#35265;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates wit
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;RoBERTa&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65292;&#24182;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#20197;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04088</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Use of a Large Language Model for Cyberbullying Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04088
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;RoBERTa&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#65292;&#24182;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#20197;&#24212;&#23545;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#31867;&#21035;&#21644;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#30340;&#30427;&#34892;&#20026;&#24694;&#24847;&#29992;&#25143;&#25552;&#20379;&#20102;&#26356;&#22810;&#30340;&#32593;&#32476;&#27450;&#20940;&#28192;&#36947;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#32593;&#32476;&#27450;&#20940;&#26159;&#24403;&#20170;&#32593;&#32476;&#19990;&#30028;&#20013;&#26368;&#26222;&#36941;&#30340;&#29616;&#35937;&#65292;&#23545;&#20844;&#27665;&#30340;&#24515;&#29702;&#21644;&#36523;&#20307;&#20581;&#24247;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#36825;&#23601;&#38656;&#35201;&#24320;&#21457;&#19968;&#20010;&#24378;&#22823;&#30340;&#31995;&#32479;&#65292;&#20197;&#38459;&#27490;&#22312;&#32447;&#35770;&#22363;&#12289;&#21338;&#23458;&#21644;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#27450;&#20940;&#20869;&#23481;&#65292;&#20197;&#31649;&#29702;&#20854;&#23545;&#25105;&#20204;&#31038;&#20250;&#30340;&#24433;&#21709;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#24230;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#27867;&#21270;&#38382;&#39064;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#24182;&#19981;&#31283;&#23450;&#12290;&#36817;&#24180;&#26469;&#65292;&#20687;BERT&#21644;RoBERTa&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;LLM&#22312;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#26041;&#38754;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#32593;&#32476;&#27450;&#20940;&#26816;&#27979;&#12290;&#25105;&#20204;&#20174;&#29616;&#26377;&#30740;&#31350;&#65288;Formspring&#21644;Twitter&#65289;&#20013;&#20934;&#22791;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65288;D2&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter)
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36845;&#20195;&#25552;&#31034;&#32454;&#21270;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21069;&#21015;&#33146;&#30284;&#25918;&#23556;&#27835;&#30103;&#30151;&#29366;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#19968;&#30151;&#29366;&#21644;&#22810;&#30151;&#29366;&#31508;&#35760;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21462;&#20934;&#30830;&#24230;&#21644;&#31934;&#30830;&#24230;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.04075</link><description>&lt;p&gt;
&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36752;&#23556;&#32959;&#30244;&#23398;&#30151;&#29366;&#25552;&#21462;&#30340;&#36845;&#20195;&#25552;&#31034;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#24072;&#29983;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36845;&#20195;&#25552;&#31034;&#32454;&#21270;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#20174;&#20020;&#24202;&#31508;&#35760;&#20013;&#25552;&#21462;&#21069;&#21015;&#33146;&#30284;&#25918;&#23556;&#27835;&#30103;&#30151;&#29366;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21333;&#19968;&#30151;&#29366;&#21644;&#22810;&#30151;&#29366;&#31508;&#35760;&#20013;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25552;&#21462;&#20934;&#30830;&#24230;&#21644;&#31934;&#30830;&#24230;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24072;&#29983;&#26550;&#26500;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#36827;&#20020;&#24202;&#31508;&#35760;&#20013;&#21069;&#21015;&#33146;&#30284;&#25918;&#23556;&#27835;&#30103;&#30151;&#29366;&#30340;&#25552;&#21462;&#12290;&#23398;&#29983;&#27169;&#22411;Mixtral&#39318;&#20808;&#25552;&#21462;&#30151;&#29366;&#65292;&#28982;&#21518;&#25945;&#24072;&#27169;&#22411;GPT-4&#26681;&#25454;Mixtral&#30340;&#34920;&#29616;&#36827;&#34892;&#25552;&#31034;&#32454;&#21270;&#12290;&#35813;&#36845;&#20195;&#36807;&#31243;&#28041;&#21450;12&#31181;&#30151;&#29366;&#30340;294&#20010;&#21333;&#19968;&#30151;&#29366;&#20020;&#24202;&#31508;&#35760;&#65292;&#27599;&#36718;&#36845;&#20195;&#26368;&#22810;&#36827;&#34892;16&#36718;&#30340;&#32454;&#21270;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26080;&#35770;&#26159;&#20174;&#21333;&#19968;&#30151;&#29366;&#31508;&#35760;&#36824;&#26159;&#22810;&#30151;&#29366;&#31508;&#35760;&#20013;&#25552;&#21462;&#30151;&#29366;&#37117;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;&#23545;&#20110;59&#20010;&#21333;&#19968;&#30151;&#29366;&#31508;&#35760;&#65292;&#20934;&#30830;&#24230;&#20174;0.51&#22686;&#21152;&#21040;0.71&#65292;&#31934;&#30830;&#24230;&#20174;0.52&#22686;&#21152;&#21040;0.82&#65292;&#21484;&#22238;&#29575;&#20174;0.52&#22686;&#21152;&#21040;0.72&#65292;F1&#20998;&#25968;&#20174;0.49&#22686;&#21152;&#21040;0.73&#12290;&#22312;375&#20010;&#22810;&#30151;&#29366;&#31508;&#35760;&#20013;&#65292;&#20934;&#30830;&#24230;&#20174;0.24&#22686;&#21152;&#21040;0.43&#65292;&#31934;&#30830;&#24230;&#20174;0.6&#22686;&#21152;&#21040;0.76&#65292;&#21484;&#22238;&#29575;&#20174;0.24&#22686;&#21152;&#21040;0.43&#65292;F1&#20998;&#25968;&#20174;0.20&#22686;&#21152;&#21040;0.44&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36752;&#23556;&#32959;&#30244;&#23398;&#20013;&#65292;&#39640;&#32423;&#25552;&#31034;&#24037;&#31243;&#22312;LLMs&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04049</link><description>&lt;p&gt;
&#35770;&#35821;&#26009;&#24211;&#27169;&#25311;&#36777;&#35770;&#20013;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Systematic Biases in LLM Simulations of Debates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24615;&#20559;&#24046;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36890;&#36807;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#20026;&#26500;&#24314;&#33021;&#22815;&#20934;&#30830;&#22797;&#21046;&#20154;&#31867;&#34892;&#20026;&#30340;&#35745;&#31639;&#26426;&#27169;&#25311;&#25552;&#20379;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;LLMs&#26159;&#22797;&#26434;&#30340;&#32479;&#35745;&#23398;&#20064;&#22120;&#65292;&#27809;&#26377;&#30452;&#25509;&#30340;&#28436;&#32462;&#35268;&#21017;&#65292;&#20351;&#20854;&#23481;&#26131;&#20986;&#29616;&#24847;&#22806;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#28857;&#20171;&#32461;&#20102;LLMs&#22312;&#27169;&#25311;&#20154;&#31867;&#20114;&#21160;&#20013;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#20851;&#27880;LLMs&#22312;&#27169;&#25311;&#25919;&#27835;&#36777;&#35770;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23613;&#31649;&#34987;&#25351;&#23450;&#20174;&#29305;&#23450;&#30340;&#25919;&#27835;&#35266;&#28857;&#36827;&#34892;&#36777;&#35770;&#65292;LLMs&#20195;&#29702;&#26426;&#26500;&#20542;&#21521;&#20110;&#36981;&#24490;&#27169;&#22411;&#22266;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#36825;&#31181;&#20542;&#21521;&#23548;&#33268;&#20986;&#29616;&#34892;&#20026;&#27169;&#24335;&#65292;&#20284;&#20046;&#20559;&#31163;&#20102;&#20154;&#31867;&#20043;&#38388;&#24050;&#32463;&#30830;&#31435;&#30340;&#31038;&#20250;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#33258;&#25105;&#20248;&#21270;&#26041;&#27861;&#21152;&#24378;&#20102;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#35813;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#25805;&#32437;LLMs&#20869;&#37096;&#30340;&#20559;&#35265;&#65292;&#24182;&#35777;&#26126;&#20195;&#29702;&#38543;&#21518;&#19982;&#36825;&#20123;&#35843;&#25972;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the al
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21516;&#26102;&#20063;&#35777;&#23454;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04028</link><description>&lt;p&gt;
AlbNews&#65306;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#20027;&#39064;&#24314;&#27169;&#26631;&#39064;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
AlbNews: A Corpus of Headlines for Topic Modeling in Albanian
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#21516;&#26102;&#20063;&#35777;&#23454;&#20102;&#22522;&#26412;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#31561;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#31232;&#32570;&#65292;&#36825;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#30740;&#31350;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;AlbNews&#65292;&#19968;&#20010;&#21253;&#21547;600&#20010;&#20027;&#39064;&#26631;&#31614;&#30340;&#26032;&#38395;&#26631;&#39064;&#21644;2600&#20010;&#26410;&#26631;&#31614;&#30340;&#38463;&#23572;&#24052;&#23612;&#20122;&#35821;&#25968;&#25454;&#38598;&#12290;&#36825;&#20123;&#25968;&#25454;&#21487;&#29992;&#20110;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#30740;&#31350;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#20351;&#29992;AlbNews&#26679;&#26412;&#35757;&#32451;&#30340;&#19968;&#20123;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#21021;&#22987;&#20998;&#31867;&#24471;&#20998;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#22522;&#26412;&#27169;&#22411;&#32988;&#36807;&#38598;&#25104;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21487;&#20316;&#20026;&#26410;&#26469;&#23454;&#39564;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The scarcity of available text corpora for low-resource languages like Albanian is a serious hurdle for research in natural language processing tasks. This paper introduces AlbNews, a collection of 600 topically labeled news headlines and 2600 unlabeled ones in Albanian. The data can be freely used for conducting topic modeling research. We report the initial classification scores of some traditional machine learning classifiers trained with the AlbNews samples. These results show that basic models outrun the ensemble learning ones and can serve as a baseline for future experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#32763;&#35793;&#22312;&#24515;&#29702;&#20581;&#24247;&#20449;&#24687;&#32763;&#35793;&#20013;&#30340;&#36755;&#20986;&#65292;&#35780;&#20272;&#20102;&#20854;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#23545;&#22810;&#35821;&#35328;&#21307;&#30103;&#27807;&#36890;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32763;&#35793;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#38463;&#25289;&#20271;&#35821;&#12289;&#32599;&#39532;&#23612;&#20122;&#35821;&#21644;&#27874;&#26031;&#35821;&#20013;&#12290;&#27969;&#30021;&#24615;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#65292;&#20027;&#35201;&#24433;&#21709;&#20102;&#38463;&#25289;&#20271;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#29702;&#35299;&#12290;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#20986;&#29616;&#20102;&#20851;&#38190;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.04023</link><description>&lt;p&gt;
&#35895;&#27468;&#32763;&#35793;&#22312;&#24515;&#29702;&#20581;&#24247;&#20449;&#24687;&#20013;&#30340;&#35823;&#24046;&#20998;&#26512;&#65306;&#35780;&#20272;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#23545;&#22810;&#35821;&#35328;&#21307;&#30103;&#27807;&#36890;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Google Translate Error Analysis for Mental Healthcare Information: Evaluating Accuracy, Comprehensibility, and Implications for Multilingual Healthcare Communication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04023
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#35895;&#27468;&#32763;&#35793;&#22312;&#24515;&#29702;&#20581;&#24247;&#20449;&#24687;&#32763;&#35793;&#20013;&#30340;&#36755;&#20986;&#65292;&#35780;&#20272;&#20102;&#20854;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#23545;&#22810;&#35821;&#35328;&#21307;&#30103;&#27807;&#36890;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#32763;&#35793;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#23588;&#20854;&#22312;&#38463;&#25289;&#20271;&#35821;&#12289;&#32599;&#39532;&#23612;&#20122;&#35821;&#21644;&#27874;&#26031;&#35821;&#20013;&#12290;&#27969;&#30021;&#24615;&#38382;&#39064;&#26222;&#36941;&#23384;&#22312;&#65292;&#20027;&#35201;&#24433;&#21709;&#20102;&#38463;&#25289;&#20271;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#29702;&#35299;&#12290;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#20986;&#29616;&#20102;&#20851;&#38190;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35895;&#27468;&#32763;&#35793;&#22312;&#24515;&#29702;&#20581;&#24247;&#20449;&#24687;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#20174;&#33521;&#35821;&#21040;&#27874;&#26031;&#35821;&#12289;&#38463;&#25289;&#20271;&#35821;&#12289;&#22303;&#32819;&#20854;&#35821;&#12289;&#32599;&#39532;&#23612;&#20122;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#35895;&#27468;&#32763;&#35793;&#36755;&#20986;&#26469;&#35780;&#20272;&#20854;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#23545;&#22810;&#35821;&#35328;&#21307;&#30103;&#27807;&#36890;&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33521;&#22269;&#22269;&#23478;&#21355;&#29983;&#26381;&#21153;&#32593;&#31449;&#19978;&#30340;&#24515;&#29702;&#20581;&#24247;&#20449;&#24687;&#21644;&#33521;&#22269;&#30343;&#23478;&#31934;&#31070;&#30149;&#23398;&#23398;&#38498;&#30340;&#20449;&#24687;&#20256;&#21333;&#12290;&#30446;&#26631;&#35821;&#35328;&#30340;&#27597;&#35821;&#20154;&#22763;&#25163;&#21160;&#35780;&#20272;&#20102;&#35895;&#27468;&#32763;&#35793;&#30340;&#32763;&#35793;&#32467;&#26524;&#65292;&#37325;&#28857;&#20851;&#27880;&#21307;&#23398;&#26415;&#35821;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#29702;&#35299;&#24615;&#21644;&#20851;&#38190;&#30340;&#21477;&#27861;/&#35821;&#20041;&#38169;&#35823;&#12290;&#35895;&#27468;&#32763;&#35793;&#30340;&#36755;&#20986;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#26415;&#35821;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#38463;&#25289;&#20271;&#35821;&#12289;&#32599;&#39532;&#23612;&#20122;&#35821;&#21644;&#27874;&#26031;&#35821;&#26041;&#38754;&#12290;&#22312;&#21508;&#31181;&#35821;&#35328;&#20013;&#26222;&#36941;&#23384;&#22312;&#27969;&#30021;&#24615;&#38382;&#39064;&#65292;&#20027;&#35201;&#24433;&#21709;&#20102;&#38463;&#25289;&#20271;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#29702;&#35299;&#12290;&#22312;&#29305;&#23450;&#35821;&#22659;&#20013;&#20986;&#29616;&#20102;&#20851;&#38190;&#38169;&#35823;&#65292;&#27604;&#22914;&#39033;&#30446;&#31526;&#21495;&#26684;&#24335;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the use of Google Translate (GT) for translating mental healthcare (MHealth) information and evaluates its accuracy, comprehensibility, and implications for multilingual healthcare communication through analysing GT output in the MHealth domain from English to Persian, Arabic, Turkish, Romanian, and Spanish. Two datasets comprising MHealth information from the UK National Health Service website and information leaflets from The Royal College of Psychiatrists were used. Native speakers of the target languages manually assessed the GT translations, focusing on medical terminology accuracy, comprehensibility, and critical syntactic/semantic errors. GT output analysis revealed challenges in accurately translating medical terminology, particularly in Arabic, Romanian, and Persian. Fluency issues were prevalent across various languages, affecting comprehension, mainly in Arabic and Spanish. Critical errors arose in specific contexts, such as bullet-point formatting, speci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REBORN&#65292;&#22312;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#26469;&#23454;&#29616;&#36793;&#30028;&#20998;&#21106;&#12290;&#36890;&#36807;&#20132;&#26367;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#21644;&#38899;&#32032;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#35821;&#38899;&#20449;&#21495;&#20998;&#27573;&#32467;&#26500;&#36793;&#30028;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03988</link><description>&lt;p&gt;
REBORN: &#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36793;&#30028;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
REBORN: Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REBORN&#65292;&#22312;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#20351;&#29992;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#26469;&#23454;&#29616;&#36793;&#30028;&#20998;&#21106;&#12290;&#36890;&#36807;&#20132;&#26367;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#21644;&#38899;&#32032;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#26080;&#30417;&#30563;&#24773;&#20917;&#19979;&#35821;&#38899;&#20449;&#21495;&#20998;&#27573;&#32467;&#26500;&#36793;&#30028;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#26088;&#22312;&#23398;&#20064;&#35821;&#38899;&#20449;&#21495;&#19982;&#20854;&#23545;&#24212;&#30340;&#25991;&#26412;&#36716;&#24405;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#32780;&#26080;&#38656;&#37197;&#23545;&#30340;&#35821;&#38899;-&#25991;&#26412;&#25968;&#25454;&#30417;&#30563;&#12290;&#35821;&#38899;&#20449;&#21495;&#20013;&#30340;&#21333;&#35789;/&#38899;&#32032;&#30001;&#19968;&#27573;&#38271;&#24230;&#21487;&#21464;&#19988;&#36793;&#30028;&#26410;&#30693;&#30340;&#35821;&#38899;&#20449;&#21495;&#34920;&#31034;&#65292;&#32780;&#36825;&#31181;&#20998;&#27573;&#32467;&#26500;&#20351;&#24471;&#22312;&#27809;&#26377;&#37197;&#23545;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35821;&#38899;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#26144;&#23556;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;REBORN&#65292;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#36845;&#20195;&#35757;&#32451;&#30340;&#26080;&#30417;&#30563;&#35821;&#38899;&#35782;&#21035;&#20013;&#30340;&#36793;&#30028;&#20998;&#21106;&#12290;REBORN&#20132;&#26367;&#36827;&#34892;&#20197;&#19979;&#20004;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#39044;&#27979;&#35821;&#38899;&#20449;&#21495;&#20013;&#20998;&#27573;&#32467;&#26500;&#36793;&#30028;&#30340;&#20998;&#21106;&#27169;&#22411;&#65292;&#21644;&#65288;2&#65289;&#35757;&#32451;&#19968;&#20010;&#38899;&#32032;&#39044;&#27979;&#27169;&#22411;&#65292;&#20854;&#36755;&#20837;&#26159;&#30001;&#20998;&#21106;&#27169;&#22411;&#20998;&#21106;&#30340;&#20998;&#27573;&#32467;&#26500;&#65292;&#29992;&#20110;&#39044;&#27979;&#38899;&#32032;&#36716;&#24405;&#12290;&#30001;&#20110;&#27809;&#26377;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#30417;&#30563;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised automatic speech recognition (ASR) aims to learn the mapping between the speech signal and its corresponding textual transcription without the supervision of paired speech-text data. A word/phoneme in the speech signal is represented by a segment of speech signal with variable length and unknown boundary, and this segmental structure makes learning the mapping between speech and text challenging, especially without paired data. In this paper, we propose REBORN, Reinforcement-Learned Boundary Segmentation with Iterative Training for Unsupervised ASR. REBORN alternates between (1) training a segmentation model that predicts the boundaries of the segmental structures in speech signals and (2) training the phoneme prediction model, whose input is a segmental structure segmented by the segmentation model, to predict a phoneme transcription. Since supervised data for training the segmentation model is not available, we use reinforcement learning to train the segmentation model t
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;</title><link>https://arxiv.org/abs/2402.03962</link><description>&lt;p&gt;
&#20301;&#32622;&#35770;&#25991;&#65306;&#21453;&#23545;&#34394;&#20551;&#30340;AI&#33192;&#32960;&#24615;&#22768;&#26126;
&lt;/p&gt;
&lt;p&gt;
Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03962
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35752;&#35770;&#20102;&#22312;&#23547;&#27714;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#36807;&#31243;&#20013;&#65292;&#20154;&#20204;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#20154;&#29305;&#36136;&#36807;&#24230;&#24402;&#22240;&#30340;&#29616;&#35937;&#65292;&#24182;&#21628;&#21505;&#23398;&#26415;&#30028;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20869;&#23481;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#19968;&#31181;&#20542;&#21521;&#65292;&#30475;&#21040;&#21608;&#22260;&#30340;&#29289;&#20307;&#20855;&#26377;&#31867;&#20284;"&#20154;&#31867;"&#30340;&#29305;&#36136;&#12290;&#25105;&#20204;&#32473;&#27773;&#36710;&#21462;&#21517;&#23383;&#65292;&#21644;&#23456;&#29289;&#29978;&#33267;&#23478;&#29992;&#30005;&#22120;&#20132;&#35848;&#65292;&#20223;&#20315;&#23427;&#20204;&#33021;&#20687;&#20854;&#20182;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#25105;&#20204;&#12290;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#25311;&#20154;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#39046;&#22495;&#20063;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20154;&#20204;&#22768;&#31216;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#33021;&#22815;&#23519;&#35273;&#21040;&#31867;&#20284;&#20110;&#20154;&#31867;&#26234;&#33021;&#30340;&#29305;&#36136;&#12290;&#22312;&#36825;&#31687;&#20301;&#32622;&#35770;&#25991;&#20013;&#65292;&#32771;&#34385;&#21040;&#19987;&#19994;&#28608;&#21169;&#12289;&#20154;&#31867;&#20559;&#35265;&#21644;&#19968;&#33324;&#26041;&#27861;&#35770;&#35774;&#32622;&#65292;&#25105;&#20204;&#35752;&#35770;&#22914;&#20309;&#24403;&#21069;&#23545;&#20110;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#30340;&#36861;&#27714;&#20026;&#23558;&#31867;&#20154;&#29305;&#36136;&#24402;&#22240;&#20110;LLM&#25171;&#24320;&#20102;&#28389;&#35294;&#20043;&#38376;&#12290;&#36890;&#36807;&#20960;&#20010;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#21457;&#29616;&#21487;&#35299;&#37322;&#20026;&#20154;&#31867;&#30340;&#27169;&#24335;&#24182;&#19981;&#24212;&#35813;&#26159;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#12290;&#32771;&#34385;&#21040;&#23186;&#20307;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26222;&#36941;&#25551;&#20889;&#65292;&#25105;&#20204;&#21628;&#21505;&#23398;&#26415;&#30028;&#29305;&#21035;&#35880;&#24910;&#65292;&#24182;&#23545;&#23398;&#26415;&#35802;&#20449;&#21407;&#21017;&#26377;&#39069;&#22806;&#30340;&#24847;&#35782;&#65292;&#22312;&#35299;&#35835;&#21644;&#20256;&#25773;&#20851;&#20110;AI&#30740;&#31350;&#30340;&#20449;&#24687;&#26102;&#35201;&#20445;&#25345;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a tendency to see 'human'-like qualities in objects around them. We name our cars, and talk to pets and even household appliances, as if they could understand us as other humans do. This behavior, called anthropomorphism, is also seeing traction in Machine Learning (ML), where human-like intelligence is claimed to be perceived in Large Language Models (LLMs). In this position paper, considering professional incentives, human biases, and general methodological setups, we discuss how the current search for Artificial General Intelligence (AGI) is a perfect storm for over-attributing human-like qualities to LLMs. In several experiments, we demonstrate that the discovery of human-interpretable patterns in latent spaces should not be a surprising outcome. Also in consideration of common AI portrayal in the media, we call for the academic community to exercise extra caution, and to be extra aware of principles of academic integrity, in interpreting and communicating about AI rese
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#23545;&#34920;&#31034;&#20026;&#26377;&#21521;&#21644;&#31232;&#30095;&#30340;JCIG&#20197;&#21253;&#21547;&#39034;&#24207;&#20449;&#24687;&#65292;&#26469;&#24314;&#27169;&#25991;&#26723;&#30456;&#20284;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25991;&#26723;&#20043;&#38388;&#30340;&#39034;&#24207;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.03957</link><description>&lt;p&gt;
&#29992;&#20110;&#36807;&#31243;&#25351;&#20196;&#25991;&#26723;&#30340;&#31232;&#30095;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sparse Graph Representations for Procedural Instructional Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#23545;&#34920;&#31034;&#20026;&#26377;&#21521;&#21644;&#31232;&#30095;&#30340;JCIG&#20197;&#21253;&#21547;&#39034;&#24207;&#20449;&#24687;&#65292;&#26469;&#24314;&#27169;&#25991;&#26723;&#30456;&#20284;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25991;&#26723;&#20043;&#38388;&#30340;&#39034;&#24207;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#30456;&#20284;&#24615;&#35745;&#31639;&#26159;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#20854;&#22312;&#21435;&#37325;&#12289;&#21305;&#37197;&#21644;&#25512;&#33616;&#31561;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#25991;&#26723;&#30456;&#20284;&#24615;&#35745;&#31639;&#26041;&#27861;&#21253;&#25324;&#23398;&#20064;&#25991;&#26723;&#34920;&#31034;&#21644;&#20351;&#29992;&#30456;&#20284;&#24230;&#25110;&#36317;&#31163;&#20989;&#25968;&#36827;&#34892;&#23884;&#20837;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#30340;&#34920;&#31034;&#26080;&#27861;&#26377;&#25928;&#25429;&#25417;&#25104;&#23545;&#30456;&#20284;&#24615;&#21644;&#24046;&#24322;&#24615;&#12290;&#22270;&#34920;&#31034;&#65288;&#22914;&#32852;&#21512;&#27010;&#24565;&#20132;&#20114;&#22270;&#65292;JCIG&#65289;&#23558;&#19968;&#23545;&#25991;&#26723;&#34920;&#31034;&#20026;&#19968;&#20010;&#32852;&#21512;&#30340;&#26080;&#21521;&#21152;&#26435;&#22270;&#65292;&#26377;&#21161;&#20110;&#23558;&#25991;&#26723;&#23545;&#20316;&#20026;&#22270;&#24418;&#30340;&#21487;&#35299;&#37322;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;JCIG&#26159;&#26080;&#21521;&#30340;&#65292;&#19981;&#32771;&#34385;&#25991;&#26723;&#20013;&#21477;&#23376;&#30340;&#39034;&#24207;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#24314;&#27169;&#25991;&#26723;&#30456;&#20284;&#24615;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#23545;&#34920;&#31034;&#20026;&#19968;&#20010;&#26377;&#21521;&#21644;&#31232;&#30095;&#30340;JCIG&#65292;&#20197;&#21253;&#21547;&#39034;&#24207;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#36229;&#22522;&#22240;&#32452;&#25490;&#24207;&#21644;&#21704;&#23494;&#39039;&#36335;&#24452;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#21462;&#20195;&#20102;&#26080;&#21521;JCIG&#30340;&#26500;&#24314;&#36807;&#31243;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#21040;&#25991;&#26723;&#20043;&#38388;&#30340;&#39034;&#24207;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computation of document similarity is a critical task in various NLP domains that has applications in deduplication, matching, and recommendation. Traditional approaches for document similarity computation include learning representations of documents and employing a similarity or a distance function over the embeddings. However, pairwise similarities and differences are not efficiently captured by individual representations. Graph representations such as Joint Concept Interaction Graph (JCIG) represent a pair of documents as a joint undirected weighted graph. JCIGs facilitate an interpretable representation of document pairs as a graph. However, JCIGs are undirected, and don't consider the sequential flow of sentences in documents. We propose two approaches to model document similarity by representing document pairs as a directed and sparse JCIG that incorporates sequential information. We propose two algorithms inspired by Supergenome Sorting and Hamiltonian Path that replace the und
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03927</link><description>&lt;p&gt;
&#27844;&#28431;&#12289;&#27450;&#39575;&#12289;&#37325;&#22797;&#65306;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03927
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23553;&#38381;&#28304;LLMs&#20013;&#30340;&#25968;&#25454;&#27745;&#26579;&#21644;&#35780;&#20272;&#19981;&#31471;&#34892;&#20026;&#12290;&#36890;&#36807;&#23545;255&#31687;&#35770;&#25991;&#30340;&#20998;&#26512;&#21644;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#32771;&#34385;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31532;&#19968;&#24180;&#21457;&#24067;&#21518;&#23384;&#22312;&#27844;&#38706;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#36234;&#26469;&#36234;&#22810;&#22320;&#20851;&#27880;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#20854;&#20013;&#19968;&#20123;&#26368;&#21463;&#27426;&#36814;&#30340;&#27169;&#22411;&#26159;&#23436;&#20840;&#25110;&#37096;&#20998;&#23553;&#38381;&#28304;&#30340;&#12290;&#23545;&#20110;&#27169;&#22411;&#32454;&#33410;&#65292;&#29305;&#21035;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#32570;&#20047;&#35775;&#38382;&#26435;&#38480;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#21453;&#22797;&#23545;&#25968;&#25454;&#27745;&#26579;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#34429;&#28982;&#24050;&#32463;&#36827;&#34892;&#20102;&#19968;&#20123;&#23581;&#35797;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#20165;&#38480;&#20110;&#20010;&#21035;&#26696;&#20363;&#21644;&#35797;&#38169;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#8220;&#38388;&#25509;&#8221;&#25968;&#25454;&#27844;&#28431;&#30340;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#26412;&#30740;&#31350;&#22312;OpenAI&#30340;GPT-3.5&#21644;GPT-4&#20351;&#29992;&#19978;&#36827;&#34892;&#20102;&#39318;&#27425;&#31995;&#32479;&#20998;&#26512;&#65292;&#36825;&#20123;&#26159;&#24403;&#20170;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;LLMs&#65292;&#24182;&#32771;&#34385;&#20102;OpenAI&#30340;&#25968;&#25454;&#20351;&#29992;&#25919;&#31574;&#65292;&#35814;&#32454;&#35760;&#24405;&#20102;&#27169;&#22411;&#21457;&#24067;&#21518;&#19968;&#24180;&#20869;&#27844;&#38706;&#32473;&#36825;&#20123;&#27169;&#22411;&#30340;&#25968;&#25454;&#37327;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#20027;&#35201;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#29702;&#35299;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#23558;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#65292;&#25105;&#20204;&#30340;LeRuD&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#35875;&#35328;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#26356;&#26377;&#28508;&#21147;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03916</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Detect Rumors on Social Media?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#29702;&#35299;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#23558;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#65292;&#25105;&#20204;&#30340;LeRuD&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#35875;&#35328;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#26356;&#26377;&#28508;&#21147;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36827;&#34892;&#35875;&#35328;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#25512;&#29702;&#25972;&#20010;&#20256;&#25773;&#20449;&#24687;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#35813;&#20449;&#24687;&#21253;&#21547;&#26032;&#38395;&#20869;&#23481;&#21644;&#22823;&#37327;&#35780;&#35770;&#65292;LLMs&#21487;&#33021;&#26080;&#27861;&#38598;&#20013;&#20851;&#27880;&#22797;&#26434;&#20256;&#25773;&#20449;&#24687;&#20013;&#30340;&#20851;&#38190;&#32447;&#32034;&#65292;&#24182;&#19988;&#22312;&#38754;&#23545;&#22823;&#37327;&#21644;&#20887;&#20313;&#20449;&#24687;&#26102;&#38590;&#20197;&#36827;&#34892;&#25512;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#22686;&#24378;&#30340;&#35875;&#35328;&#26816;&#27979;&#65288;LeRuD&#65289;&#26041;&#27861;&#65292;&#22312;&#20854;&#20013;&#35774;&#35745;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#20851;&#27880;&#26032;&#38395;&#21644;&#35780;&#35770;&#20013;&#30340;&#37325;&#35201;&#32447;&#32034;&#65292;&#24182;&#23558;&#25972;&#20010;&#20256;&#25773;&#20449;&#24687;&#20998;&#35299;&#20026;&#20256;&#25773;&#38142;&#20197;&#20943;&#36731;LLMs&#30340;&#36127;&#25285;&#12290;&#25105;&#20204;&#22312;Twitter&#21644;&#24494;&#21338;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;LeRuD&#30340;&#24615;&#33021;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#35875;&#35328;&#26816;&#27979;&#27169;&#22411;&#65292;&#25552;&#21319;&#20102;2.4&#65285;&#33267;7.6&#65285;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24212;&#29992;LLMs&#65292;LeRuD&#26080;&#38656;&#36827;&#34892;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#19988;&#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#26356;&#20855;&#26377;&#28508;&#21147;&#30340;&#35875;&#35328;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate to use Large Language Models (LLMs) for rumor detection on social media. However, it is challenging for LLMs to reason over the entire propagation information on social media, which contains news contents and numerous comments, due to LLMs may not concentrate on key clues in the complex propagation information, and have trouble in reasoning when facing massive and redundant information. Accordingly, we propose an LLM-empowered Rumor Detection (LeRuD) approach, in which we design prompts to teach LLMs to reason over important clues in news and comments, and divide the entire propagation information into a Chain-of-Propagation for reducing LLMs' burden. We conduct extensive experiments on the Twitter and Weibo datasets, and LeRuD outperforms several state-of-the-art rumor detection models by 2.4% to 7.6%. Meanwhile, by applying LLMs, LeRuD requires no data for training, and thus shows more promising rumor detection ability in few-shot or zero-shot scenarios.
&lt;/p&gt;</description></item><item><title>Pro-HAN&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#37197;&#32622;&#25991;&#20214;&#30340;&#21475;&#35821;&#29702;&#35299;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#37197;&#32622;&#25991;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#21475;&#35821;&#29702;&#35299;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03900</link><description>&lt;p&gt;
Pro-HAN:&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#37197;&#32622;&#25991;&#20214;&#30340;&#21475;&#35821;&#29702;&#35299;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Pro-HAN: A Heterogeneous Graph Attention Network for Profile-Based Spoken Language Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03900
&lt;/p&gt;
&lt;p&gt;
Pro-HAN&#26159;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#37197;&#32622;&#25991;&#20214;&#30340;&#21475;&#35821;&#29702;&#35299;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23427;&#36890;&#36807;&#25429;&#25417;&#19981;&#21516;&#37197;&#32622;&#25991;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#21475;&#35821;&#29702;&#35299;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#37197;&#32622;&#25991;&#20214;&#30340;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#26088;&#22312;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;&#34917;&#20805;&#37197;&#32622;&#25991;&#20214;&#20449;&#24687;&#65288;&#21363;&#65292;&#30693;&#35782;&#22270;&#35889;&#12289;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#65289;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#28040;&#38500;&#29992;&#25143;&#35805;&#35821;&#20013;&#24120;&#35265;&#30340;&#27495;&#20041;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20165;&#33021;&#20998;&#21035;&#23545;&#19981;&#21516;&#30340;&#37197;&#32622;&#25991;&#20214;&#20449;&#24687;&#36827;&#34892;&#24314;&#27169;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25110;&#32773;&#25490;&#38500;&#20854;&#20013;&#30340;&#19981;&#30456;&#20851;&#21644;&#20914;&#31361;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#22810;&#20010;&#37197;&#32622;&#25991;&#20214;&#20449;&#24687;&#20043;&#38388;&#36827;&#34892;&#25512;&#29702;&#30340;&#24322;&#26500;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#31216;&#20026;Pro-HAN&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36793;&#65292;&#20998;&#21035;&#34920;&#31034;&#37197;&#32622;&#25991;&#20214;&#20869;&#37096;&#12289;&#37197;&#32622;&#25991;&#20214;&#20043;&#38388;&#21644;&#35805;&#35821;&#19982;&#37197;&#32622;&#25991;&#20214;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#25429;&#25417;&#22810;&#20010;&#37197;&#32622;&#25991;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#22312;ProSLU&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26368;&#20808;&#36827;&#29366;&#24577;&#65292;&#24182;&#22312;&#25152;&#26377;&#19977;&#20010;&#24230;&#37327;&#26631;&#20934;&#19978;&#21462;&#24471;&#20102;&#32422;8%&#30340;&#25552;&#21319;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24314;&#27169;&#31890;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Profile-based Spoken Language Understanding (SLU) has gained increasing attention, which aims to incorporate various types of supplementary profile information (i.e., Knowledge Graph, User Profile, Context Awareness) to eliminate the prevalent ambiguities in user utterances. However, existing approaches can only separately model different profile information, without considering their interrelationships or excluding irrelevant and conflicting information within them. To address the above issues, we introduce a Heterogeneous Graph Attention Network to perform reasoning across multiple Profile information, called Pro-HAN. Specifically, we design three types of edges, denoted as intra-Pro, inter-Pro, and utterance-Pro, to capture interrelationships among multiple Pros. We establish a new state-of-the-art on the ProSLU dataset, with an improvement of approximately 8% across all three metrics. Further analysis experiments also confirm the effectiveness of our method in modeling mu
&lt;/p&gt;</description></item><item><title>DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03898</link><description>&lt;p&gt;
DistiLLM: &#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21270;&#33976;&#39311;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DistiLLM: Towards Streamlined Distillation for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03898
&lt;/p&gt;
&lt;p&gt;
DistiLLM&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#38024;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#21644;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#25945;&#24072;&#27169;&#22411;&#21387;&#32553;&#20026;&#26356;&#23567;&#30340;&#23398;&#29983;&#27169;&#22411;&#65292;&#38477;&#20302;&#25512;&#29702;&#25104;&#26412;&#21644;&#20869;&#23384;&#21344;&#29992;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#65288;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;KD&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26631;&#20934;&#21270;&#30446;&#26631;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#20351;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#26469;&#35299;&#20915;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#38382;&#39064;&#30340;&#20570;&#27861;&#26174;&#33879;&#22686;&#21152;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DistiLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#26694;&#26550;&#12290;DistiLLM&#30001;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32452;&#25104;&#65306;&#65288;1&#65289;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#26012;Kullback-Leibler&#25955;&#24230;&#25439;&#22833;&#65292;&#25105;&#20204;&#25581;&#31034;&#24182;&#21033;&#29992;&#20102;&#23427;&#30340;&#29702;&#35770;&#23646;&#24615;&#65307;&#65288;2&#65289;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21033;&#29992;&#23398;&#29983;&#29983;&#25104;&#30340;&#36755;&#20986;&#30340;&#25928;&#29575;&#12290;&#21253;&#25324;&#25351;&#20196;&#36319;&#38543;&#20219;&#21153;&#22312;&#20869;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;DistiLLM&#22312;&#26500;&#24314;&#39640;&#24615;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation (KD) is widely used for compressing a teacher model to a smaller student model, reducing its inference cost and memory footprint while preserving model capabilities. However, current KD methods for auto-regressive sequence models (e.g., large language models) suffer from missing a standardized objective function. Moreover, the recent use of student-generated outputs to address training-inference mismatches has significantly escalated computational costs. To tackle these issues, we introduce DistiLLM, a more effective and efficient KD framework for auto-regressive language models. DistiLLM comprises two components: (1) a novel skew Kullback-Leibler divergence loss, where we unveil and leverage its theoretical properties, and (2) an adaptive off-policy approach designed to enhance the efficiency in utilizing student-generated outputs. Extensive experiments, including instruction-following tasks, demonstrate the effectiveness of DistiLLM in building high-performing 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#31038;&#20250;&#35268;&#33539;&#21464;&#21270;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#38761;&#65292;&#29305;&#21035;&#20851;&#27880;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#20851;&#20110;&#35821;&#35328;&#21644;&#24615;&#21035;&#30340;&#20105;&#35758;&#12290;&#36890;&#36807;&#23545;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#20854;&#20182;&#35821;&#35328;&#21644;&#24615;&#21035;&#38382;&#39064;&#30340;&#35752;&#35770;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#21644;&#24615;&#21035;&#22312;&#35758;&#38498;&#20013;&#30340;&#25345;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#35758;&#38498;&#30340;&#35821;&#35328;&#23454;&#36341;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;</title><link>https://arxiv.org/abs/2402.03887</link><description>&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#21464;&#21270;&#21161;&#25512;&#35821;&#35328;&#21464;&#38761;&#65306;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#20851;&#20110;&#35821;&#35328;&#21644;&#24615;&#21035;&#30340;&#20105;&#35758;
&lt;/p&gt;
&lt;p&gt;
Shifting social norms as a driving force for linguistic change: Struggles about language and gender in the German Bundestag
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03887
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20851;&#27880;&#31038;&#20250;&#35268;&#33539;&#21464;&#21270;&#24341;&#36215;&#30340;&#35821;&#35328;&#21464;&#38761;&#65292;&#29305;&#21035;&#20851;&#27880;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#20851;&#20110;&#35821;&#35328;&#21644;&#24615;&#21035;&#30340;&#20105;&#35758;&#12290;&#36890;&#36807;&#23545;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20013;&#20854;&#20182;&#35821;&#35328;&#21644;&#24615;&#21035;&#38382;&#39064;&#30340;&#35752;&#35770;&#30340;&#20998;&#26512;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#35821;&#35328;&#21644;&#24615;&#21035;&#22312;&#35758;&#38498;&#20013;&#30340;&#25345;&#32493;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23545;&#35758;&#38498;&#30340;&#35821;&#35328;&#23454;&#36341;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22522;&#20110;&#31038;&#20250;&#35268;&#33539;&#21464;&#21270;&#30340;&#35821;&#35328;&#21464;&#38761;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#35821;&#35328;&#21644;&#24615;&#21035;&#30340;&#36777;&#35770;&#12290;&#36777;&#35770;&#20013;&#32463;&#24120;&#25552;&#21450;&#30340;&#19968;&#20010;&#35770;&#28857;&#26159;&#65292;&#35821;&#35328;&#20250;&#8220;&#33258;&#28982;&#8221;&#21457;&#23637;&#65292;&#32780;&#23545;&#20854;&#36827;&#34892;&#8220;&#20005;&#37325;&#20171;&#20837;&#8221;&#65288;&#20363;&#22914;&#24615;&#21035;&#21253;&#23481;&#35821;&#35328;&#65289;&#22312;&#25152;&#35859;&#30340;&#8220;&#26377;&#26426;&#8221;&#35821;&#35328;&#31995;&#32479;&#20013;&#26159;&#19981;&#24688;&#24403;&#29978;&#33267;&#8220;&#21361;&#38505;&#8221;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20171;&#20837;&#24182;&#38750;&#21069;&#25152;&#26410;&#26377;&#12290;&#31038;&#20250;&#21160;&#26426;&#39537;&#21160;&#19979;&#30340;&#35821;&#35328;&#21464;&#38761;&#26082;&#19981;&#23547;&#24120;&#20063;&#19981;&#26032;&#40092;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20316;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#25919;&#27835;&#31038;&#20250;&#31354;&#38388;&#12290;&#36890;&#36807;&#20197;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#20840;&#20307;&#20250;&#35758;&#19978;&#20851;&#20110;&#35821;&#35328;&#21644;&#24615;&#21035;&#30340;&#20854;&#20182;&#20105;&#35758;&#20026;&#36215;&#28857;&#65292;&#25105;&#20204;&#30340;&#25991;&#31456;&#35828;&#26126;&#33258;1980&#24180;&#20197;&#26469;&#65292;&#35821;&#35328;&#21644;&#24615;&#21035;&#19968;&#30452;&#26159;&#24503;&#22269;&#32852;&#37030;&#35758;&#38498;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#23637;&#31034;&#36825;&#19968;&#28857;&#65306;a&#65289;&#23545;&#21516;&#24615;&#24651;&#32773;&#30340;&#31216;&#35859;; b&#65289;&#21452;&#25968;&#24418;&#24335;&#65288;&#27604;&#22914;B&#252;rg&#65289;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on language change based on shifting social norms, in particular with regard to the debate on language and gender. It is a recurring argument in this debate that language develops "naturally" and that "severe interventions" - such as gender-inclusive language is often claimed to be - in the allegedly "organic" language system are inappropriate and even "dangerous". Such interventions are, however, not unprecedented. Socially motivated processes of language change are neither unusual nor new. We focus in our contribution on one important political-social space in Germany, the German Bundestag. Taking other struggles about language and gender in the plenaries of the Bundestag as a starting point, our article illustrates that language and gender has been a recurring issue in the German Bundestag since the 1980s. We demonstrate how this is reflected in linguistic practices of the Bundestag, by the use of a) designations for gays and lesbians; b) pair forms such as B\"urg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.03877</link><description>&lt;p&gt;
&#36229;&#36234;&#32447;&#26465;&#21644;&#22278;&#22280;&#65306;&#25581;&#31034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20309;&#25512;&#29702;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Beyond Lines and Circles: Unveiling the Geometric Reasoning Gap in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#21644;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#21644;&#22256;&#38590;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;LLMs&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23398;&#21644;&#31639;&#27861;&#20219;&#21153;&#26041;&#38754;&#23637;&#31034;&#20102;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#21147;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#20960;&#20309;&#25512;&#29702;&#26041;&#38754;&#30340;&#25216;&#33021;&#36824;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#26500;&#36896;&#24615;&#20960;&#20309;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#20154;&#31867;&#25968;&#23398;&#25512;&#29702;&#21457;&#23637;&#20013;&#26368;&#22522;&#30784;&#30340;&#27493;&#39588;&#20043;&#19968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;LLMs&#22312;&#36825;&#20010;&#39046;&#22495;&#38754;&#20020;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23613;&#31649;&#22312;&#31867;&#20284;&#39046;&#22495;&#21462;&#24471;&#20102;&#35768;&#22810;&#25104;&#21151;&#12290;LLMs&#22312;&#30446;&#26631;&#21464;&#37327;&#36873;&#25321;&#26041;&#38754;&#23384;&#22312;&#20559;&#35265;&#65292;&#24182;&#19988;&#22312;2D&#31354;&#38388;&#20851;&#31995;&#26041;&#38754;&#38754;&#20020;&#22256;&#38590;&#65292;&#32463;&#24120;&#20250;&#38169;&#35823;&#22320;&#34920;&#31034;&#21644;&#33222;&#36896;&#23545;&#35937;&#21450;&#20854;&#25918;&#32622;&#20301;&#32622;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#22810;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#36827;&#34892;&#20869;&#37096;&#23545;&#35805;&#26469;&#22686;&#24378;&#23427;&#20204;&#29616;&#26377;&#30340;&#25512;&#29702;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#24378;&#35843;&#20102;LLMs&#22312;&#20960;&#20309;&#25512;&#29702;&#20013;&#30340;&#29616;&#26377;&#38480;&#21046;&#65292;&#24182;&#36890;&#36807;&#33258;&#25105;&#32416;&#27491;&#12289;&#21327;&#20316;&#21644;&#19981;&#21516;&#35282;&#33394;&#19987;&#19994;&#21270;&#26469;&#25552;&#39640;&#20960;&#20309;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate ever-increasing abilities in mathematical and algorithmic tasks, yet their geometric reasoning skills are underexplored. We investigate LLMs' abilities in constructive geometric problem-solving one of the most fundamental steps in the development of human mathematical reasoning. Our work reveals notable challenges that the state-of-the-art LLMs face in this domain despite many successes in similar areas. LLMs exhibit biases in target variable selection and struggle with 2D spatial relationships, often misrepresenting and hallucinating objects and their placements. To this end, we introduce a framework that formulates an LLMs-based multi-agents system that enhances their existing reasoning potential by conducting an internal dialogue. This work underscores LLMs' current limitations in geometric reasoning and improves geometric reasoning capabilities through self-correction, collaboration, and diverse role specializations.
&lt;/p&gt;</description></item><item><title>&#24503;&#22269;&#26032;&#38395;&#25991;&#26412;&#20013;&#21463;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#24433;&#21709;&#30340;&#35789;&#27719;&#19981;&#21040;&#30334;&#20998;&#20043;&#19968;&#65292;&#21453;&#23545;&#20351;&#29992;&#24615;&#21035;&#21253;&#23481;&#24615;&#24503;&#35821;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#23427;&#20351;&#20070;&#38754;&#25991;&#26412;&#21464;&#24471;&#36807;&#38271;&#21644;&#22797;&#26434;&#65292;&#20294;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#36825;&#31181;&#24433;&#21709;&#38750;&#24120;&#26377;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.03870</link><description>&lt;p&gt;
&#24503;&#22269;&#26032;&#38395;&#25991;&#26412;&#20013;&#21463;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#24433;&#21709;&#30340;&#35789;&#27719;&#19981;&#21040;&#30334;&#20998;&#20043;&#19968;
&lt;/p&gt;
&lt;p&gt;
Less than one percent of words would be affected by gender-inclusive language in German press texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03870
&lt;/p&gt;
&lt;p&gt;
&#24503;&#22269;&#26032;&#38395;&#25991;&#26412;&#20013;&#21463;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#24433;&#21709;&#30340;&#35789;&#27719;&#19981;&#21040;&#30334;&#20998;&#20043;&#19968;&#65292;&#21453;&#23545;&#20351;&#29992;&#24615;&#21035;&#21253;&#23481;&#24615;&#24503;&#35821;&#30340;&#20027;&#35201;&#35770;&#28857;&#26159;&#23427;&#20351;&#20070;&#38754;&#25991;&#26412;&#21464;&#24471;&#36807;&#38271;&#21644;&#22797;&#26434;&#65292;&#20294;&#26681;&#25454;&#25105;&#20204;&#30340;&#30740;&#31350;&#65292;&#36825;&#31181;&#24433;&#21709;&#38750;&#24120;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#24615;&#21035;&#21644;&#35821;&#35328;&#30340;&#30740;&#31350;&#19982;&#24615;&#21035;&#24179;&#31561;&#21644;&#38750;&#27495;&#35270;&#24615;&#35821;&#35328;&#20351;&#29992;&#30340;&#31038;&#20250;&#36777;&#35770;&#23494;&#20999;&#30456;&#20851;&#12290;&#24515;&#29702;&#35821;&#35328;&#23398;&#23398;&#32773;&#22312;&#36825;&#20010;&#39046;&#22495;&#20570;&#20986;&#20102;&#37325;&#35201;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#22312;&#35821;&#35328;&#20351;&#29992;&#30340;&#32972;&#26223;&#19979;&#23545;&#36825;&#20123;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#30340;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#30740;&#31350;&#36824;&#24456;&#23569;&#35265;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35299;&#31572;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#22914;&#26524;&#38750;&#24615;&#21035;&#21253;&#23481;&#24615;&#30340;&#25991;&#26412;&#37325;&#20889;&#20026;&#24615;&#21035;&#21253;&#23481;&#24615;&#25991;&#26412;&#65292;&#23454;&#38469;&#19978;&#26377;&#22810;&#23569;&#25991;&#26412;&#26448;&#26009;&#38656;&#35201;&#26356;&#25913;&#12290;&#36825;&#31181;&#23450;&#37327;&#27979;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#32463;&#39564;&#24615;&#27934;&#23519;&#65292;&#22240;&#20026;&#21453;&#23545;&#20351;&#29992;&#24615;&#21035;&#21253;&#23481;&#24615;&#24503;&#35821;&#30340;&#19968;&#20010;&#32463;&#24120;&#20986;&#29616;&#30340;&#35770;&#28857;&#26159;&#65292;&#23427;&#20250;&#20351;&#20070;&#38754;&#25991;&#26412;&#21464;&#24471;&#36807;&#38271;&#21644;&#22797;&#26434;&#12290;&#36824;&#26377;&#20154;&#25552;&#20986;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#23545;&#35821;&#35328;&#23398;&#20064;&#32773;&#26377;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#21482;&#26377;&#22312;&#24615;&#21035;&#21253;&#23481;&#24615;&#25991;&#26412;&#19982;&#38750;&#24615;&#21035;&#21253;&#23481;&#24615;&#25991;&#26412;&#38750;&#24120;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#25165;&#21487;&#33021;&#20135;&#29983;&#36825;&#26679;&#30340;&#24433;&#21709;&#12290;&#22312;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#35821;&#35328;&#23398;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#26631;&#27880;&#24503;&#25991;&#26032;&#38395;&#25991;&#26412;&#65292;&#20197;&#35782;&#21035;&#20854;&#20013;&#21463;&#24615;&#21035;&#21253;&#23481;&#24615;&#35821;&#35328;&#24433;&#21709;&#30340;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research on gender and language is tightly knitted to social debates on gender equality and non-discriminatory language use. Psycholinguistic scholars have made significant contributions in this field. However, corpus-based studies that investigate these matters within the context of language use are still rare. In our study, we address the question of how much textual material would actually have to be changed if non-gender-inclusive texts were rewritten to be gender-inclusive. This quantitative measure is an important empirical insight, as a recurring argument against the use of gender-inclusive German is that it supposedly makes written texts too long and complicated. It is also argued that gender-inclusive language has negative effects on language learners. However, such effects are only likely if gender-inclusive texts are very different from those that are not gender-inclusive. In our corpus-linguistic study, we manually annotated German press texts to identify the parts that wou
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#32844;&#22330;&#39046;&#22495;&#25216;&#33021;&#25552;&#21462;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#21477;&#27861;&#22797;&#26434;&#30340;&#25216;&#33021;&#25552;&#21450;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03832</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#24605;&#32771;&#32844;&#22330;&#39046;&#22495;&#30340;&#25216;&#33021;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Rethinking Skill Extraction in the Job Market Domain using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#32844;&#22330;&#39046;&#22495;&#25216;&#33021;&#25552;&#21462;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#22788;&#29702;&#21477;&#27861;&#22797;&#26434;&#30340;&#25216;&#33021;&#25552;&#21450;&#26102;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#33021;&#25552;&#21462;&#26159;&#25351;&#22312;&#25307;&#32856;&#23703;&#20301;&#21644;&#31616;&#21382;&#31561;&#25991;&#26723;&#20013;&#35782;&#21035;&#25552;&#21450;&#30340;&#25216;&#33021;&#21644;&#32972;&#26223;&#35201;&#27714;&#12290;&#36825;&#39033;&#20219;&#21153;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;BIO&#26631;&#31614;&#30340;&#24207;&#21015;&#26631;&#27880;&#26041;&#27861;&#35757;&#32451;&#30417;&#30563;&#27169;&#22411;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#25163;&#24037;&#27880;&#37322;&#30340;&#25968;&#25454;&#38480;&#21046;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;BIO&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#25429;&#25417;&#22797;&#26434;&#25216;&#33021;&#27169;&#24335;&#21644;&#22788;&#29702;&#27169;&#31946;&#25552;&#21450;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#19968;&#20010;&#21253;&#21547;6&#20010;&#32479;&#19968;&#21270;&#25216;&#33021;&#25552;&#21462;&#25968;&#25454;&#38598;&#30340;&#22522;&#20934;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#21477;&#23376;&#20013;&#35782;&#21035;&#21644;&#25552;&#21462;&#25216;&#33021;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#22312;&#24615;&#33021;&#26041;&#38754;&#19981;&#21450;&#20256;&#32479;&#30340;&#30417;&#30563;&#27169;&#22411;&#65292;&#20294;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#26356;&#22909;&#22320;&#22788;&#29702;&#22312;&#25216;&#33021;&#25552;&#21462;&#20219;&#21153;&#20013;&#21477;&#27861;&#22797;&#26434;&#30340;&#25216;&#33021;&#25552;&#21450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skill Extraction involves identifying skills and qualifications mentioned in documents such as job postings and resumes. The task is commonly tackled by training supervised models using a sequence labeling approach with BIO tags. However, the reliance on manually annotated data limits the generalizability of such approaches. Moreover, the common BIO setting limits the ability of the models to capture complex skill patterns and handle ambiguous mentions. In this paper, we explore the use of in-context learning to overcome these challenges, on a benchmark of 6 uniformized skill extraction datasets. Our approach leverages the few-shot learning capabilities of large language models (LLMs) to identify and extract skills from sentences. We show that LLMs, despite not being on par with traditional supervised models in terms of performance, can better handle syntactically complex skill mentions in skill extraction tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.03822</link><description>&lt;p&gt;
RevOrder&#65306;&#19968;&#31181;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RevOrder: A Novel Method for Enhanced Arithmetic in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RevOrder&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;nD&#20056;&#20197;1D&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31639;&#26415;&#36816;&#31639;&#12290;&#32463;&#36807;&#20840;&#38754;&#27979;&#35797;&#65292;RevOrder&#22312;&#22522;&#26412;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#20934;&#30830;&#24230;&#65292;&#24182;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22823;&#25968;&#26102;&#12290;&#22312;GSM8K&#25968;&#23398;&#20219;&#21153;&#20013;&#24212;&#29992;RevOrder&#36827;&#34892;&#24494;&#35843;&#65292;&#26377;&#25928;&#38477;&#20302;&#20102;&#38169;&#35823;&#29575;&#24182;&#25552;&#39640;&#20102;&#24635;&#20307;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RevOrder&#65292;&#19968;&#31181;&#26088;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31639;&#26415;&#36816;&#31639;&#30340;&#26032;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#32763;&#36716;&#21152;&#27861;&#12289;&#20943;&#27861;&#21644;n&#20301;&#25968;&#20056;&#20197;1&#20301;&#25968;&#65288;nD&#20056;&#20197;1D&#65289;&#30340;&#36755;&#20986;&#25968;&#23383;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#39034;&#24207;&#20013;&#38388;&#25968;&#23383;&#30340;&#25968;&#37327; (CSID)&#65292;&#36825;&#26159;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#35780;&#20272;&#26041;&#31243;&#22797;&#26434;&#24615;&#30340;&#26032;&#24230;&#37327;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#27979;&#35797;&#65292;RevOrder&#19981;&#20165;&#22312;&#22522;&#26412;&#30340;&#31639;&#26415;&#36816;&#31639;&#20013;&#36798;&#21040;&#20102;&#23436;&#32654;&#30340;&#20934;&#30830;&#24230;&#65292;&#32780;&#19988;&#22312;&#38500;&#27861;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#21319;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#27169;&#22411;&#38590;&#20197;&#22788;&#29702;&#30340;&#22823;&#25968;&#24773;&#20917;&#19979;&#12290;RevOrder&#30340;&#23454;&#29616;&#23545;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#37117;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#27492;&#22806;&#65292;&#23558;RevOrder&#24212;&#29992;&#20110;&#23545;GSM8K&#25968;&#23398;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;LLaMA2-7B&#27169;&#22411;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#21892;&#65292;&#23558;&#26041;&#31243;&#35745;&#31639;&#38169;&#35823;&#29575;&#38477;&#20302;&#20102;46%&#65292;&#23558;&#24635;&#20307;&#24471;&#20998;&#20174;41.6&#25552;&#21319;&#21040;44.4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents RevOrder, a novel technique aimed at improving arithmetic operations in large language models (LLMs) by reversing the output digits in addition, subtraction, and n-digit by 1-digit (nD by 1D) multiplication tasks. Our method significantly reduces the Count of Sequential Intermediate Digits (CSID) to $\mathcal{O}(1)$, a new metric we introduce to assess equation complexity. Through comprehensive testing, RevOrder not only achieves perfect accuracy in basic arithmetic operations but also substantially boosts LLM performance in division tasks, particularly with large numbers where traditional models struggle. Implementation of RevOrder is cost-effective for both training and inference phases. Moreover, applying RevOrder to fine-tune the LLaMA2-7B model on the GSM8K math task results in a considerable improvement, reducing equation calculation errors by 46% and increasing overall scores from 41.6 to 44.4.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36719;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20165;&#35757;&#32451;&#36719;&#25552;&#31034;&#32780;&#19981;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03782</link><description>&lt;p&gt;
&#36719;&#25552;&#31034;&#35843;&#25972;&#29992;&#20110;&#36328;&#35821;&#35328;&#36801;&#31227;&#65306;&#36234;&#23569;&#36234;&#22909;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning for Cross-Lingual Transfer: When Less is More
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36719;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20165;&#35757;&#32451;&#36719;&#25552;&#31034;&#32780;&#19981;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#21442;&#25968;&#25928;&#29575;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#25552;&#31034;&#35843;&#25972;&#65288;SPT&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#36755;&#20837;&#23618;&#25554;&#20837;&#21487;&#23398;&#20064;&#30340;&#23884;&#20837;&#65292;&#25110;&#36719;&#25552;&#31034;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#21442;&#25968;&#65292;&#26469;&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;SPT&#22312;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#19982;&#20808;&#21069;&#20851;&#20110;SPT&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#22362;&#25345;SPT&#30340;&#21407;&#22987;&#24847;&#22270;&#65292;&#21363;&#20165;&#35757;&#32451;&#36719;&#25552;&#31034;&#32780;&#20445;&#25345;&#27169;&#22411;&#21442;&#25968;&#19981;&#21464;&#12290;&#36825;&#19981;&#20165;&#20943;&#23569;&#20102;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#24320;&#38144;&#65292;&#32780;&#19988;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;SPT&#22266;&#26377;&#30340;&#21442;&#25968;&#25928;&#29575;&#33021;&#22815;&#25552;&#39640;&#23545;&#35821;&#35328;&#24046;&#24322;&#36739;&#22823;&#30340;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19982;&#25552;&#31034;&#30456;&#20851;&#30340;&#19981;&#21516;&#22240;&#32032;&#65292;&#22914;&#38271;&#24230;&#25110;&#20854;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23545;&#36328;&#35821;&#35328;&#36801;&#31227;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Soft Prompt Tuning (SPT) is a parameter-efficient method for adapting pre-trained language models (PLMs) to specific tasks by inserting learnable embeddings, or soft prompts, at the input layer of the PLM, without modifying its parameters. This paper investigates the potential of SPT for cross-lingual transfer. Unlike previous studies on SPT for cross-lingual transfer that often fine-tune both the soft prompt and the model parameters, we adhere to the original intent of SPT by keeping the model parameters frozen and only training the soft prompt. This does not only reduce the computational cost and storage overhead of full-model fine-tuning, but we also demonstrate that this very parameter efficiency intrinsic to SPT can enhance cross-lingual transfer performance to linguistically distant languages. Moreover, we explore how different factors related to the prompt, such as the length or its reparameterization, affect cross-lingual transfer performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03780</link><description>&lt;p&gt;
&#25581;&#31034;&#23459;&#20256;&#65306;&#22522;&#20110;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exposing propaganda: an analysis of stylistic cues comparing human annotations and machine classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#25991;&#20307;&#32447;&#32034;&#27604;&#36739;&#20154;&#31867;&#27880;&#37322;&#21644;&#26426;&#22120;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#23459;&#20256;&#35821;&#35328;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;PPN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#23459;&#20256;&#26032;&#38395;&#21644;&#24120;&#35268;&#26032;&#38395;&#12290;&#30740;&#31350;&#36824;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#26377;&#20851;&#25991;&#20307;&#32447;&#32034;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#35821;&#35328;&#21450;&#20854;&#25991;&#20307;&#29305;&#24449;&#12290;&#25552;&#20986;&#20102;PPN&#25968;&#25454;&#38598;&#65292;&#21363;&#23459;&#20256;&#24615;&#20266;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#23427;&#26159;&#19968;&#31181;&#22810;&#28304;&#12289;&#22810;&#35821;&#35328;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26426;&#26500;&#30830;&#23450;&#30340;&#23459;&#20256;&#26469;&#28304;&#32593;&#31449;&#19978;&#30340;&#26032;&#38395;&#25991;&#31456;&#32452;&#25104;&#12290;&#20174;&#35813;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#20102;&#19968;&#37096;&#20998;&#26679;&#26412;&#19982;&#26469;&#33258;&#24120;&#35268;&#27861;&#22269;&#26032;&#38395;&#30340;&#25991;&#31456;&#28151;&#21512;&#65292;&#24182;&#23545;&#23427;&#20204;&#30340;URL&#36827;&#34892;&#20102;&#25513;&#30422;&#65292;&#20197;&#36827;&#34892;&#20154;&#31867;&#27880;&#37322;&#23454;&#39564;&#65292;&#20351;&#29992;11&#20010;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#20154;&#31867;&#27880;&#37322;&#32773;&#33021;&#22815;&#21487;&#38752;&#22320;&#21306;&#20998;&#20004;&#31181;&#31867;&#22411;&#30340;&#26032;&#38395;&#32440;&#23545;&#27599;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#26469;&#35782;&#21035;&#27880;&#37322;&#32773;&#20351;&#29992;&#30340;&#32447;&#32034;&#65292;&#24182;&#19982;&#26426;&#22120;&#20998;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;&#20854;&#20013;&#21253;&#25324;&#20351;&#29992;VAGO&#20998;&#26512;&#22120;&#36827;&#34892;&#36766;&#36848;&#27169;&#31946;&#21644;&#20027;&#35266;&#24615;&#30340;&#27979;&#37327;&#65292;&#20351;&#29992;TF-IDF&#20316;&#20026;&#22522;&#20934;&#65292;&#20197;&#21450;&#22235;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;&#20004;&#20010;&#22522;&#20110;RoBERTa&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#21477;&#27861;&#30340;CATS&#65292;&#20197;&#21450;&#32467;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#19968;&#20010;XGBoost&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the language of propaganda and its stylistic features. It presents the PPN dataset, standing for Propagandist Pseudo-News, a multisource, multilingual, multimodal dataset composed of news articles extracted from websites identified as propaganda sources by expert agencies. A limited sample from this set was randomly mixed with papers from the regular French press, and their URL masked, to conduct an annotation-experiment by humans, using 11 distinct labels. The results show that human annotators were able to reliably discriminate between the two types of press across each of the labels. We propose different NLP techniques to identify the cues used by the annotators, and to compare them with machine classification. They include the analyzer VAGO to measure discourse vagueness and subjectivity, a TF-IDF to serve as a baseline, and four different classifiers: two RoBERTa-based models, CATS using syntax, and one XGBoost combining syntactic and semantic features.   K
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.03776</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;MOOCs&#35780;&#20998;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models As MOOCs Graders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03776
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#26367;MOOCs&#20013;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#65292;&#26088;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#20013;&#35780;&#20272;&#23398;&#29983;&#20889;&#20316;&#20219;&#21153;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22312;&#32447;&#24320;&#25918;&#35838;&#31243;&#65288;MOOCs&#65289;&#20026;&#25317;&#26377;&#30005;&#33041;&#21644;&#20114;&#32852;&#32593;&#35775;&#38382;&#26435;&#38480;&#30340;&#20840;&#29699;&#20219;&#20309;&#20154;&#25552;&#20379;&#20813;&#36153;&#25945;&#32946;&#30340;&#26426;&#20250;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36825;&#20123;&#35838;&#31243;&#30340;&#22823;&#35268;&#27169;&#27880;&#20876;&#24847;&#21619;&#30528;&#19968;&#20301;&#25945;&#24072;&#20960;&#20046;&#19981;&#21487;&#33021;&#35780;&#20272;&#27599;&#20010;&#23398;&#29983;&#30340;&#20889;&#20316;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#21516;&#20276;&#35780;&#20998;&#36890;&#24120;&#26159;&#39318;&#36873;&#26041;&#27861;&#65292;&#36890;&#24120;&#30001;&#31616;&#21333;&#26126;&#20102;&#30340;&#35780;&#20998;&#26631;&#20934;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#21516;&#20276;&#35780;&#20998;&#22312;&#21487;&#38752;&#24230;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;18&#20010;&#19981;&#21516;&#30340;&#22330;&#26223;&#65292;&#25506;&#32034;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26367;&#20195;MOOCs&#20013;&#30340;&#21516;&#20276;&#35780;&#20998;&#30340;&#21487;&#34892;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;LLMs&#65306;GPT-4&#21644;GPT-3.5&#65292;&#24182;&#28085;&#30422;&#19977;&#38376;&#19981;&#21516;&#30340;&#35838;&#31243;&#65306;&#20837;&#38376;&#22825;&#25991;&#23398;&#65292;&#22825;&#20307;&#29983;&#29289;&#23398;&#20197;&#21450;&#22825;&#25991;&#23398;&#30340;&#21382;&#21490;&#19982;&#21746;&#23398;&#12290;&#20026;&#20102;&#35757;&#32451;LLMs&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#38646;-shot&#36830;&#32493;&#24605;&#32771;&#65288;Zero-shot-CoT&#65289;&#25552;&#31034;&#25216;&#26415;&#30340;&#21464;&#31181;&#30340;&#19977;&#20010;&#19981;&#21516;&#25552;&#31034;&#65306;&#32467;&#21512;Zero-shot-CoT&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Massive open online courses (MOOCs) unlock the doors to free education for anyone around the globe with access to a computer and the internet. Despite this democratization of learning, the massive enrollment in these courses means it is almost impossible for one instructor to assess every student's writing assignment. As a result, peer grading, often guided by a straightforward rubric, is the method of choice. While convenient, peer grading often falls short in terms of reliability and validity. In this study, using 18 distinct settings, we explore the feasibility of leveraging large language models (LLMs) to replace peer grading in MOOCs. Specifically, we focus on two state-of-the-art LLMs: GPT-4 and GPT-3.5, across three distinct courses: Introductory Astronomy, Astrobiology, and the History and Philosophy of Astronomy. To instruct LLMs, we use three different prompts based on a variant of the zero-shot chain-of-thought (Zero-shot-CoT) prompting technique: Zero-shot-CoT combined with
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;</title><link>https://arxiv.org/abs/2402.03774</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#23398;&#20064;&#20915;&#31574;&#26641;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning a Decision Tree Algorithm with Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03774
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;MetaTree&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36755;&#20986;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#22240;&#20854;&#21487;&#35299;&#37322;&#24615;&#21644;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#23454;&#29616;&#39640;&#39044;&#27979;&#24615;&#33021;&#32780;&#38395;&#21517;&#12290;&#20256;&#32479;&#19978;&#65292;&#20915;&#31574;&#26641;&#26159;&#36890;&#36807;&#36882;&#24402;&#31639;&#27861;&#26500;&#24314;&#30340;&#65292;&#22312;&#26641;&#30340;&#27599;&#20010;&#33410;&#28857;&#19978;&#23558;&#25968;&#25454;&#36827;&#34892;&#20998;&#21306;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#20998;&#21306;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#38024;&#23545;&#23616;&#37096;&#27573;&#20248;&#21270;&#30340;&#20915;&#31574;&#26641;&#21487;&#33021;&#26080;&#27861;&#24102;&#26469;&#20840;&#23616;&#27010;&#25324;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MetaTree&#65292;&#35813;&#27169;&#22411;&#20351;&#29992;&#32463;&#20856;&#31639;&#27861;&#30340;&#36807;&#28388;&#36755;&#20986;&#26469;&#35757;&#32451;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#24378;&#22823;&#30340;&#20998;&#31867;&#20915;&#31574;&#26641;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#22823;&#37327;&#25968;&#25454;&#38598;&#19978;&#25311;&#21512;&#36138;&#23146;&#20915;&#31574;&#26641;&#21644;&#20248;&#21270;&#20915;&#31574;&#26641;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35757;&#32451;MetaTree&#20135;&#29983;&#20855;&#26377;&#24378;&#22823;&#27010;&#25324;&#24615;&#33021;&#30340;&#20915;&#31574;&#26641;&#12290;&#36825;&#31181;&#35757;&#32451;&#20351;MetaTree&#19981;&#20165;&#21487;&#20197;&#27169;&#25311;&#36825;&#20123;&#31639;&#27861;&#65292;&#36824;&#21487;&#20197;&#26681;&#25454;&#19978;&#19979;&#25991;&#26234;&#33021;&#22320;&#35843;&#25972;&#31574;&#30053;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#24378;&#30340;&#27010;&#25324;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are renowned for their interpretability capability to achieve high predictive performance, especially on tabular data. Traditionally, they are constructed through recursive algorithms, where they partition the data at every node in a tree. However, identifying the best partition is challenging, as decision trees optimized for local segments may not bring global generalization. To address this, we introduce MetaTree, which trains a transformer-based model on filtered outputs from classical algorithms to produce strong decision trees for classification. Specifically, we fit both greedy decision trees and optimized decision trees on a large number of datasets. We then train MetaTree to produce the trees that achieve strong generalization performance. This training enables MetaTree to not only emulate these algorithms, but also to intelligently adapt its strategy according to the context, thereby achieving superior generalization performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03757</link><description>&lt;p&gt;
&#26412;&#33021;&#20559;&#35265;&#65306;&#34394;&#20551;&#22270;&#20687;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#34394;&#20551;&#22270;&#20687;&#20250;&#23548;&#33268;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35780;&#20272;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;CorrelationQA&#65292;&#24182;&#21457;&#29616;&#20027;&#27969;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#30340;&#20986;&#29616;&#20351;LLMs&#20855;&#22791;&#20102;&#35270;&#35273;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20687;GPT-4V&#36825;&#26679;&#24378;&#22823;&#30340;MLLMs&#22312;&#38754;&#23545;&#26576;&#20123;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#26102;&#20173;&#28982;&#20197;&#24778;&#20154;&#30340;&#26041;&#24335;&#22833;&#36133;&#20102;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31867;&#20856;&#22411;&#36755;&#20837;&#65292;&#36825;&#20123;&#36755;&#20837;&#20196;MLLMs&#22256;&#24785;&#65292;&#23427;&#20204;&#30001;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#31572;&#26696;&#19981;&#19968;&#33268;&#30340;&#22270;&#20687;&#32452;&#25104;&#65292;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#31181;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CorrelationQA&#65292;&#36825;&#26159;&#39318;&#20010;&#35780;&#20272;&#32473;&#23450;&#34394;&#20551;&#22270;&#20687;&#30340;&#24187;&#35273;&#31243;&#24230;&#30340;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#21547;13&#20010;&#31867;&#21035;&#30340;7,308&#20010;&#25991;&#26412;-&#22270;&#20687;&#23545;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;CorrelationQA&#65292;&#25105;&#20204;&#23545;9&#20010;&#20027;&#27969;MLLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#23427;&#20204;&#26222;&#36941;&#21463;&#21040;&#36825;&#31181;&#26412;&#33021;&#20559;&#35265;&#30340;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#31934;&#36873;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;&#32467;&#26524;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently experienced remarkable progress, where the advent of multi-modal large language models (MLLMs) has endowed LLMs with visual capabilities, leading to impressive performances in various multi-modal tasks. However, those powerful MLLMs such as GPT-4V still fail spectacularly when presented with certain image and text inputs. In this paper, we identify a typical class of inputs that baffles MLLMs, which consist of images that are highly relevant but inconsistent with answers, causing MLLMs to suffer from hallucination. To quantify the effect, we propose CorrelationQA, the first benchmark that assesses the hallucination level given spurious images. This benchmark contains 7,308 text-image pairs across 13 categories. Based on the proposed CorrelationQA, we conduct a thorough analysis on 9 mainstream MLLMs, illustrating that they universally suffer from this instinctive bias to varying degrees. We hope that our curated benchmark and evaluation result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLMs&#20869;&#37096;&#29366;&#24577;&#20013;&#20445;&#30041;&#23494;&#38598;&#35821;&#20041;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;EigenScore&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#22238;&#31572;&#30340;&#33258;&#27965;&#24615;&#65292;&#24182;&#25506;&#32034;&#27979;&#35797;&#26102;&#38388;&#30340;&#29305;&#24449;&#21098;&#20999;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.03744</link><description>&lt;p&gt;
INSIDE: LLMs&#30340;&#20869;&#37096;&#29366;&#24577;&#20445;&#30041;&#20102;&#24187;&#35273;&#26816;&#27979;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03744
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;LLMs&#20869;&#37096;&#29366;&#24577;&#20013;&#20445;&#30041;&#23494;&#38598;&#35821;&#20041;&#20449;&#24687;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#24187;&#35273;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;EigenScore&#24230;&#37327;&#26041;&#27861;&#35780;&#20272;&#22238;&#31572;&#30340;&#33258;&#27965;&#24615;&#65292;&#24182;&#25506;&#32034;&#27979;&#35797;&#26102;&#38388;&#30340;&#29305;&#24449;&#21098;&#20999;&#26041;&#27861;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24187;&#35273;&#23545;&#37096;&#32626;&#30340;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#25552;&#20986;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#23545;&#24187;&#35273;&#30340;&#26816;&#27979;&#19978;&#65292;&#37319;&#29992;&#20102;&#36923;&#36753;&#32423;&#21035;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#25110;&#35821;&#35328;&#32423;&#21035;&#30340;&#33258;&#27965;&#24615;&#35780;&#20272;&#65292;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#20002;&#22833;&#20102;&#35821;&#20041;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#25506;&#32034;LLMs&#20869;&#37096;&#29366;&#24577;&#20013;&#20445;&#30041;&#30340;&#23494;&#38598;&#35821;&#20041;&#20449;&#24687;&#29992;&#20110;&#24187;&#35273;&#26816;&#27979;&#65288;INSIDE&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;EigenScore&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#35780;&#20272;&#22238;&#31572;&#30340;&#33258;&#27965;&#24615;&#65292;&#23427;&#21033;&#29992;&#22238;&#31572;&#30340;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#26469;&#34913;&#37327;&#23494;&#38598;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;/&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#20174;&#33258;&#27965;&#30340;&#24187;&#35273;&#26816;&#27979;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#27979;&#35797;&#26102;&#38388;&#30340;&#29305;&#24449;&#21098;&#20999;&#26041;&#27861;&#65292;&#29992;&#20110;&#25130;&#26029;&#20869;&#37096;&#29366;&#24577;&#20013;&#30340;&#26497;&#31471;&#28608;&#27963;&#65292;&#20197;&#20943;&#23569;&#36807;&#24230;&#33258;&#20449;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge hallucination have raised widespread concerns for the security and reliability of deployed LLMs. Previous efforts in detecting hallucinations have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, where the semantic information is inevitably lost during the token-decoding procedure. Thus, we propose to explore the dense semantic information retained within LLMs' \textbf{IN}ternal \textbf{S}tates for halluc\textbf{I}nation \textbf{DE}tection (\textbf{INSIDE}). In particular, a simple yet effective \textbf{EigenScore} metric is proposed to better evaluate responses' self-consistency, which exploits the eigenvalues of responses' covariance matrix to measure the semantic consistency/diversity in the dense embedding space. Furthermore, from the perspective of self-consistent hallucination detection, a test time feature clipping approach is explored to truncate extreme activations in the internal states, which reduces overconfident g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEAN&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.03732</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#28145;&#24230;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Outdated Fact Detection in Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DEAN&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#37319;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#30340;&#26041;&#27861;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22240;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#28508;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#36807;&#26102;&#20107;&#23454;&#30340;&#38382;&#39064;&#32473;KG&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#24433;&#21709;&#20102;&#20854;&#20316;&#20026;&#29616;&#23454;&#19990;&#30028;&#20449;&#24687;&#30340;&#25972;&#20307;&#36136;&#37327;&#12290;&#29616;&#26377;&#30340;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#35782;&#21035;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;DEAN&#65288;&#28145;&#24230;&#36807;&#26102;&#20107;&#23454;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;KG&#20013;&#35782;&#21035;&#36807;&#26102;&#20107;&#23454;&#12290;DEAN&#36890;&#36807;&#20840;&#38754;&#24314;&#27169;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#38544;&#21547;&#32467;&#26500;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#21306;&#20998;&#20107;&#23454;&#26041;&#38754;&#34920;&#29616;&#20986;&#33258;&#24049;&#30340;&#29420;&#29305;&#20043;&#22788;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#25581;&#31034;&#28508;&#22312;&#30340;&#36807;&#26102;&#20449;&#24687;&#65292;DEAN&#37319;&#29992;&#20102;&#22522;&#20110;&#39044;&#23450;&#20041;&#30340;&#20851;&#31995;&#21040;&#33410;&#28857;&#65288;R2N&#65289;&#22270;&#30340;&#23545;&#27604;&#26041;&#27861;&#65292;&#35813;&#22270;&#30001;&#23454;&#20307;&#25968;&#37327;&#21152;&#26435;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;DEAN&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have garnered significant attention for their vast potential across diverse domains. However, the issue of outdated facts poses a challenge to KGs, affecting their overall quality as real-world information evolves. Existing solutions for outdated fact detection often rely on manual recognition. In response, this paper presents DEAN (Deep outdatEd fAct detectioN), a novel deep learning-based framework designed to identify outdated facts within KGs. DEAN distinguishes itself by capturing implicit structural information among facts through comprehensive modeling of both entities and relations. To effectively uncover latent out-of-date information, DEAN employs a contrastive approach based on a pre-defined Relations-to-Nodes (R2N) graph, weighted by the number of entities. Experimental results demonstrate the effectiveness and superiority of DEAN over state-of-the-art baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22806;&#37096;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#12290;&#32463;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03728</link><description>&lt;p&gt;
&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#33268;&#32852;&#21512;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Consistent Joint Decision-Making with Heterogeneous Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#27169;&#22411;&#30340;&#39044;&#27979;&#21644;&#22806;&#37096;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#20915;&#31574;&#30340;&#19968;&#33268;&#24615;&#12290;&#32463;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20915;&#31574;&#26694;&#26550;&#65292;&#20419;&#36827;&#20102;&#30001;&#19981;&#21516;&#27169;&#22411;&#20570;&#20986;&#30340;&#20915;&#31574;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#27169;&#22411;&#30340;&#39044;&#27979;&#26144;&#23556;&#21040;&#20840;&#23616;&#24402;&#19968;&#21270;&#21644;&#21487;&#27604;&#36739;&#30340;&#20540;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#20915;&#31574;&#30340;&#20808;&#39564;&#27010;&#29575;&#12289;&#32622;&#20449;&#24230;&#65288;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#27169;&#22411;&#30340;&#39044;&#26399;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20256;&#32479;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel decision-making framework that promotes consistency among decisions made by diverse models while utilizing external knowledge. Leveraging the Integer Linear Programming (ILP) framework, we map predictions from various models into globally normalized and comparable values by incorporating information about decisions' prior probability, confidence (uncertainty), and the models' expected accuracy. Our empirical study demonstrates the superiority of our approach over conventional baselines on multiple datasets.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#36890;&#36807;&#25913;&#21892;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03720</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#29992;&#20110;&#22270;&#24418;LLMs
&lt;/p&gt;
&lt;p&gt;
Similarity-based Neighbor Selection for Graph LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03720
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#36890;&#36807;&#25913;&#21892;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#35299;&#20915;&#20102;&#22788;&#29702;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#30452;&#25509;&#22788;&#29702;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65292;&#20294;&#23427;&#20204;&#30340;&#24191;&#27867;&#24120;&#35782;&#30693;&#35782;&#21644;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#20026;TAGs&#20013;&#30340;&#33410;&#28857;&#20998;&#31867;&#25552;&#20379;&#20102;&#26497;&#22823;&#30340;&#24076;&#26395;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#24050;&#32463;&#35299;&#20915;&#20102;&#36807;&#24230;&#21387;&#32553;&#12289;&#24322;&#36136;&#24615;&#21644;&#20449;&#24687;&#38598;&#25104;&#19981;&#24403;&#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#21463;&#21040;&#25968;&#25454;&#38598;&#20998;&#21306;&#30340;&#19981;&#19968;&#33268;&#24615;&#21644;&#39640;&#32423;LLMs&#30340;&#20302;&#21033;&#29992;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#37051;&#23621;&#36873;&#25321;&#65288;SNS&#65289;&#12290;&#20351;&#29992;SimCSE&#21644;&#39640;&#32423;&#37051;&#23621;&#36873;&#25321;&#25216;&#26415;&#65292;SNS&#26377;&#25928;&#25552;&#39640;&#20102;&#25152;&#36873;&#37051;&#23621;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#22270;&#24418;&#34920;&#31034;&#24182;&#20943;&#36731;&#20102;&#36807;&#24230;&#21387;&#32553;&#21644;&#24322;&#36136;&#24615;&#31561;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#19968;&#31181;&#24402;&#32435;&#21644;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;SNS&#22312;&#20256;&#32479;GNN&#26041;&#27861;&#19978;&#23637;&#31034;&#20102;&#26356;&#24378;&#30340;&#27867;&#21270;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#31526;&#21512;&#26631;&#20934;&#30340;&#25968;&#25454;&#38598;&#20998;&#21306;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) present unique challenges for direct processing by Language Learning Models (LLMs), yet their extensive commonsense knowledge and robust reasoning capabilities offer great promise for node classification in TAGs. Prior research in this field has grappled with issues such as over-squashing, heterophily, and ineffective graph information integration, further compounded by inconsistencies in dataset partitioning and underutilization of advanced LLMs. To address these challenges, we introduce Similarity-based Neighbor Selection (SNS). Using SimCSE and advanced neighbor selection techniques, SNS effectively improves the quality of selected neighbors, thereby improving graph representation and alleviating issues like over-squashing and heterophily. Besides, as an inductive and training-free approach, SNS demonstrates superior generalization and scalability over traditional GNN methods. Our comprehensive experiments, adhering to standard dataset partitioning prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaMAI&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20027;&#21160;&#35810;&#38382;&#30340;&#26041;&#24335;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#38169;&#35823;&#35299;&#35835;&#30340;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.03719</link><description>&lt;p&gt;
&#29992;&#20027;&#21160;&#35810;&#38382;&#36171;&#20104;&#35821;&#35328;&#27169;&#22411;&#26356;&#28145;&#20837;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Empowering Language Models with Active Inquiry for Deeper Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LaMAI&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20027;&#21160;&#35810;&#38382;&#30340;&#26041;&#24335;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#24182;&#20943;&#23569;&#20102;&#38169;&#35823;&#35299;&#35835;&#30340;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#38761;&#26032;&#20102;&#25105;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20114;&#21160;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;LLMs&#23545;&#29992;&#25143;&#26597;&#35810;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#23427;&#20204;&#32463;&#24120;&#20250;&#38169;&#35823;&#35299;&#35835;&#29992;&#25143;&#30340;&#26597;&#35810;&#65292;&#23548;&#33268;&#19981;&#22826;&#26377;&#24110;&#21161;&#30340;&#22238;&#22797;&#12290;&#22312;&#33258;&#28982;&#20154;&#31867;&#20114;&#21160;&#20013;&#65292;&#36890;&#36807;&#26377;&#38024;&#23545;&#24615;&#30340;&#38382;&#39064;&#23547;&#27714;&#28548;&#28165;&#65292;&#20197;&#25581;&#31034;&#19981;&#26126;&#30830;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LaMAI&#65288;&#24102;&#26377;&#20027;&#21160;&#35810;&#38382;&#30340;&#35821;&#35328;&#27169;&#22411;&#65289;&#65292;&#26088;&#22312;&#36171;&#20104;LLMs&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#24615;&#12290;LaMAI&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#25216;&#26415;&#25552;&#20986;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#38382;&#39064;&#65292;&#20419;&#36827;&#21160;&#24577;&#30340;&#21452;&#21521;&#23545;&#35805;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#32553;&#23567;&#20102;&#19978;&#19979;&#25991;&#24046;&#36317;&#65292;&#36824;&#25913;&#21892;&#20102;LLMs&#30340;&#36755;&#20986;&#65292;&#20351;&#20854;&#26356;&#21152;&#31526;&#21512;&#29992;&#25143;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22797;&#26434;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20102;LaMAI&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;LLMs&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#22238;&#31572;&#30340;&#20934;&#30830;&#29575;&#20174;31.9%&#25552;&#39640;&#21040;50%
&lt;/p&gt;
&lt;p&gt;
The rise of large language models (LLMs) has revolutionized the way that we interact with artificial intelligence systems through natural language. However, LLMs often misinterpret user queries because of their uncertain intention, leading to less helpful responses. In natural human interactions, clarification is sought through targeted questioning to uncover obscure information. Thus, in this paper, we introduce LaMAI (Language Model with Active Inquiry), designed to endow LLMs with this same level of interactive engagement. LaMAI leverages active learning techniques to raise the most informative questions, fostering a dynamic bidirectional dialogue. This approach not only narrows the contextual gap but also refines the output of the LLMs, aligning it more closely with user expectations. Our empirical studies, across a variety of complex datasets where LLMs have limited conversational context, demonstrate the effectiveness of LaMAI. The method improves answer accuracy from 31.9% to 50
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03715</link><description>&lt;p&gt;
&#28548;&#28165;&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Clarify: Improving Model Robustness With Natural Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03715
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;Clarify&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#32416;&#27491;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#19968;&#33268;&#22833;&#36133;&#27169;&#24335;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20174;&#38745;&#24577;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#30456;&#20851;&#24615;&#12290;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#20381;&#36182;&#20110;&#39640;&#32423;&#38169;&#35823;&#27010;&#24565;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#31181;&#38169;&#35823;&#27010;&#24565;&#65292;&#25105;&#20204;&#24517;&#39035;&#25552;&#20379;&#39069;&#22806;&#30340;&#20449;&#24687;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#20123;&#39069;&#22806;&#30340;&#23454;&#20363;&#32423;&#30417;&#30563;&#24418;&#24335;&#65292;&#20363;&#22914;&#26631;&#35760;&#34394;&#20551;&#29305;&#24449;&#25110;&#26469;&#33258;&#24179;&#34913;&#20998;&#24067;&#30340;&#39069;&#22806;&#26631;&#35760;&#25968;&#25454;&#12290;&#23545;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26469;&#35828;&#65292;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#20250;&#21464;&#24471;&#26114;&#36149;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20197;&#25509;&#36817;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#30340;&#35268;&#27169;&#36827;&#34892;&#39069;&#22806;&#27880;&#37322;&#12290;&#25105;&#20204;&#20551;&#35774;&#26377;&#38024;&#23545;&#24615;&#30340;&#20851;&#20110;&#27169;&#22411;&#38169;&#35823;&#27010;&#24565;&#30340;&#33258;&#28982;&#35821;&#35328;&#21453;&#39304;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#39069;&#22806;&#30417;&#30563;&#24418;&#24335;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Clarify&#65292;&#19968;&#31181;&#26032;&#22411;&#30028;&#38754;&#21644;&#26041;&#27861;&#26469;&#20132;&#20114;&#24335;&#22320;&#32416;&#27491;&#27169;&#22411;&#30340;&#38169;&#35823;&#27010;&#24565;&#12290;&#36890;&#36807;Clarify&#65292;&#29992;&#25143;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20010;&#31616;&#30701;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#25551;&#36848;&#27169;&#22411;&#30340;&#19968;&#33268;&#24615;&#22833;&#36133;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23436;&#20840;&#33258;&#21160;&#21270;&#22320;&#20351;&#29992;s
&lt;/p&gt;
&lt;p&gt;
In supervised learning, models are trained to extract correlations from a static dataset. This often leads to models that rely on high-level misconceptions. To prevent such misconceptions, we must necessarily provide additional information beyond the training data. Existing methods incorporate forms of additional instance-level supervision, such as labels for spurious features or additional labeled data from a balanced distribution. Such strategies can become prohibitively costly for large-scale datasets since they require additional annotation at a scale close to the original training data. We hypothesize that targeted natural language feedback about a model's misconceptions is a more efficient form of additional supervision. We introduce Clarify, a novel interface and method for interactively correcting model misconceptions. Through Clarify, users need only provide a short text description to describe a model's consistent failure patterns. Then, in an entirely automated way, we use s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22768;&#38899;&#28151;&#21512;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#23545;&#22768;&#38899;&#28304;&#30340;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#22768;&#38899;&#28304;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#23558;&#23427;&#20204;&#20998;&#31163;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#32534;&#36753;&#22120;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03710</link><description>&lt;p&gt;
&#21548;&#12289;&#32842;&#12289;&#32534;&#36753;&#65306;&#22522;&#20110;&#25991;&#26412;&#25351;&#23548;&#30340;&#22768;&#26223;&#20462;&#25913;&#20197;&#22686;&#24378;&#21548;&#35273;&#20307;&#39564;
&lt;/p&gt;
&lt;p&gt;
Listen, Chat, and Edit: Text-Guided Soundscape Modification for Enhanced Auditory Experience
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22768;&#38899;&#28151;&#21512;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25351;&#20196;&#23454;&#29616;&#23545;&#22768;&#38899;&#28304;&#30340;&#20462;&#25913;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#32534;&#36753;&#22810;&#20010;&#22768;&#38899;&#28304;&#30340;&#33021;&#21147;&#65292;&#26080;&#38656;&#23558;&#23427;&#20204;&#20998;&#31163;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#32534;&#36753;&#22120;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#25105;&#20204;&#36935;&#21040;&#21508;&#31181;&#21508;&#26679;&#30340;&#22768;&#38899;&#65292;&#26377;&#20123;&#26159;&#25105;&#20204;&#26399;&#26395;&#30340;&#65292;&#26377;&#20123;&#26159;&#25105;&#20204;&#19981;&#24076;&#26395;&#30340;&#65292;&#23545;&#23427;&#20204;&#30340;&#23384;&#22312;&#21644;&#38899;&#37327;&#30340;&#25511;&#21046;&#26377;&#38480;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#22768;&#38899;&#28151;&#21512;&#32534;&#36753;&#22120;"&#21548;&#12289;&#32842;&#12289;&#32534;&#36753;"(LCE)&#65292;&#35813;&#32534;&#36753;&#22120;&#26681;&#25454;&#29992;&#25143;&#25552;&#20379;&#30340;&#25991;&#26412;&#25351;&#20196;&#20462;&#25913;&#28151;&#21512;&#20013;&#30340;&#27599;&#20010;&#22768;&#38899;&#28304;&#12290;LCE&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#32842;&#22825;&#30028;&#38754;&#20197;&#21450;&#20854;&#22312;&#19981;&#38656;&#35201;&#23558;&#22768;&#38899;&#28304;&#20998;&#31163;&#30340;&#24773;&#20917;&#19979;&#21516;&#26102;&#23545;&#22810;&#20010;&#22768;&#38899;&#28304;&#36827;&#34892;&#32534;&#36753;&#30340;&#33021;&#21147;&#32780;&#19982;&#20247;&#19981;&#21516;&#12290;&#29992;&#25143;&#36755;&#20837;&#24320;&#25918;&#24615;&#30340;&#25991;&#26412;&#25552;&#31034;&#65292;&#36825;&#20123;&#25552;&#31034;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#65292;&#29992;&#20110;&#21019;&#24314;&#32534;&#36753;&#22768;&#38899;&#28151;&#21512;&#30340;&#35821;&#20041;&#28388;&#27874;&#22120;&#12290;&#31995;&#32479;&#28982;&#21518;&#23558;&#28151;&#21512;&#35299;&#26512;&#25104;&#20854;&#32452;&#25104;&#37096;&#20998;&#65292;&#24212;&#29992;&#35821;&#20041;&#28388;&#27874;&#22120;&#65292;&#24182;&#23558;&#20854;&#37325;&#26032;&#32452;&#35013;&#25104;&#26399;&#26395;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#25324;&#35821;&#38899;&#21644;&#21508;&#31181;&#38899;&#39057;&#28304;&#20197;&#21450;&#29992;&#20110;&#19981;&#21516;&#32534;&#36753;&#20219;&#21153;&#30340;&#25991;&#26412;&#25552;&#31034;&#30340;160&#23567;&#26102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25552;&#21462;&#12289;&#21024;&#38500;&#21644;&#38899;&#37327;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In daily life, we encounter a variety of sounds, both desirable and undesirable, with limited control over their presence and volume. Our work introduces "Listen, Chat, and Edit" (LCE), a novel multimodal sound mixture editor that modifies each sound source in a mixture based on user-provided text instructions. LCE distinguishes itself with a user-friendly chat interface and its unique ability to edit multiple sound sources simultaneously within a mixture, without needing to separate them. Users input open-vocabulary text prompts, which are interpreted by a large language model to create a semantic filter for editing the sound mixture. The system then decomposes the mixture into its components, applies the semantic filter, and reassembles it into the desired output. We developed a 160-hour dataset with over 100k mixtures, including speech and various audio sources, along with text prompts for diverse editing tasks like extraction, removal, and volume control. Our experiments demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.03686</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#65306;&#37325;&#26032;&#24605;&#32771;&#35821;&#35328;&#27169;&#22411;&#22312;&#34164;&#21547;&#39564;&#35777;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Minds versus Machines: Rethinking Entailment Verification with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#31616;&#21333;&#25512;&#29702;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#20248;&#21270;&#30340;Flan-T5&#27169;&#22411;&#65292;&#29992;&#20110;&#34164;&#21547;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#25991;&#26412;&#29702;&#35299;&#20013;&#36827;&#34892;&#22823;&#37327;&#30340;&#25512;&#29702;&#20197;&#29702;&#35299;&#35770;&#36848;&#12290;&#26412;&#25991;&#26088;&#22312;&#20102;&#35299;&#20154;&#31867;&#21644;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25512;&#29702;&#21028;&#26029;&#20013;&#30340;&#20849;&#24615;&#21644;&#24046;&#24322;&#12290;&#36890;&#36807;&#32508;&#21512;&#31574;&#21010;&#30340;&#34164;&#21547;&#39564;&#35777;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20154;&#31867;&#21644;LLM&#22312;&#21508;&#31181;&#25512;&#29702;&#31867;&#21035;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#26469;&#33258;&#19977;&#20010;&#31867;&#21035;&#65288;NLI&#12289;&#19978;&#19979;&#25991;QA&#21644;&#35299;&#37322;&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22810;&#21477;&#21069;&#25552;&#21644;&#19981;&#21516;&#30340;&#30693;&#35782;&#31867;&#22411;&#65292;&#20174;&#32780;&#35780;&#20272;&#20102;&#22797;&#26434;&#25512;&#29702;&#24773;&#20917;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#26174;&#31034;LLM&#22312;&#36328;&#25193;&#23637;&#19978;&#19979;&#25991;&#30340;&#22810;&#36339;&#25512;&#29702;&#20013;&#20855;&#26377;&#20248;&#21183;&#65292;&#32780;&#20154;&#31867;&#22312;&#38656;&#35201;&#31616;&#21333;&#28436;&#32462;&#25512;&#29702;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#21033;&#29992;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#32454;&#35843;&#25972;&#30340;Flan-T5&#27169;&#22411;&#65292;&#20854;&#24615;&#33021;&#36229;&#36807;&#20102;GPT-3.5&#65292;&#24182;&#19982;GPT-4&#23218;&#32654;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24320;&#28304;&#35299;&#20915;&#26041;&#26696;&#20379;&#34164;&#21547;&#39564;&#35777;&#20351;&#29992;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Humans make numerous inferences in text comprehension to understand discourse. This paper aims to understand the commonalities and disparities in the inference judgments between humans and state-of-the-art Large Language Models (LLMs). Leveraging a comprehensively curated entailment verification benchmark, we evaluate both human and LLM performance across various reasoning categories. Our benchmark includes datasets from three categories (NLI, contextual QA, and rationales) that include multi-sentence premises and different knowledge types, thereby evaluating the inference capabilities in complex reasoning instances. Notably, our findings reveal LLMs' superiority in multi-hop reasoning across extended contexts, while humans excel in tasks necessitating simple deductive reasoning. Leveraging these insights, we introduce a fine-tuned Flan-T5 model that outperforms GPT-3.5 and rivals with GPT-4, offering a robust open-source solution for entailment verification. As a practical application
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#38388;&#25509;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#30340;&#26041;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03667</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38388;&#25509;&#25512;&#29702;&#22120;&#65306;&#23545;&#33258;&#21160;&#25512;&#29702;&#30340;&#21453;&#35777;&#21644;&#30683;&#30462;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as an Indirect Reasoner: Contrapositive and Contradiction for Automated Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03667
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#22411;&#38388;&#25509;&#25512;&#29702;&#26041;&#27861;&#65292;&#20351;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#22686;&#24378;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#21450;&#35774;&#35745;&#25552;&#31034;&#27169;&#26495;&#30340;&#26041;&#24335;&#22686;&#24378;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#36981;&#24490;&#30452;&#25509;&#25512;&#29702;&#65288;DR&#65289;&#26694;&#26550;&#65292;&#22914;&#8220;&#24605;&#32500;&#38142;&#8221;&#21644;&#8220;&#33258;&#19968;&#33268;&#24615;&#8221;&#65292;&#22240;&#27492;&#22312;&#35299;&#20915;&#24456;&#38590;&#36890;&#36807;DR&#35299;&#20915;&#30340;&#20247;&#22810;&#23454;&#38469;&#38382;&#39064;&#26102;&#20250;&#36935;&#21040;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22686;&#24378;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38388;&#25509;&#25512;&#29702;&#65288;IR&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21453;&#35777;&#21644;&#30683;&#30462;&#30340;&#36923;&#36753;&#26469;&#22788;&#29702;&#20107;&#23454;&#25512;&#29702;&#21644;&#25968;&#23398;&#35777;&#26126;&#31561;IR&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#35777;&#30340;&#36923;&#36753;&#31561;&#20215;&#24615;&#26469;&#22686;&#24378;LLMs&#30340;&#25968;&#25454;&#21644;&#35268;&#21017;&#65292;&#20197;&#25552;&#39640;&#20854;&#21487;&#29702;&#35299;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#32452;&#25552;&#31034;&#27169;&#26495;&#65292;&#35302;&#21457;LLMs&#36827;&#34892;&#22522;&#20110;&#30683;&#30462;&#35777;&#26126;&#30340;IR&#65292;&#20854;&#36923;&#36753;&#19978;&#31561;&#20215;&#20110;&#21407;&#22987;&#30340;DR&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;IR&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, increasing attention has been focused drawn on to improve the ability of Large Language Models (LLMs) to perform complex reasoning. However, previous methods, such as Chain-of-Thought and Self-Consistency, mainly follow Direct Reasoning (DR) frameworks, so they will meet difficulty in solving numerous real-world tasks which can hardly be solved via DR. Therefore, to strengthen the reasoning power of LLMs, this paper proposes a novel Indirect Reasoning (IR) method that employs the logic of contrapositives and contradictions to tackle IR tasks such as factual reasoning and mathematic proof. Specifically, our methodology comprises two steps. Firstly, we leverage the logical equivalence of contrapositive to augment the data and rules to enhance the comprehensibility of LLMs. Secondly, we design a set of prompt templates to trigger LLMs to conduct IR based on proof by contradiction that is logically equivalent to the original DR process. Our IR method is simple yet effective and c
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03659</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#21453;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03659
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#32929;&#31080;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#26469;&#35299;&#20915;&#32929;&#31080;&#39044;&#27979;&#20013;&#30340;&#35299;&#37322;&#38382;&#39064;&#21644;&#25968;&#25454;&#26631;&#27880;&#25104;&#26412;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20256;&#32479;&#30340;&#38750;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#37322;&#32929;&#31080;&#39044;&#27979;&#36890;&#24120;&#26159;&#19968;&#39033;&#22256;&#38590;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#35299;&#37322;&#20165;&#38480;&#20110;&#21487;&#35270;&#21270;&#37325;&#35201;&#25991;&#26412;&#19978;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#30446;&#21069;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#29983;&#25104;&#20154;&#31867;&#21487;&#35835;&#35299;&#37322;&#20854;&#20915;&#31574;&#36807;&#31243;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32929;&#31080;&#39044;&#27979;&#23545;LLM&#26469;&#35828;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#33021;&#22815;&#26435;&#34913;&#28151;&#20081;&#31038;&#20250;&#25991;&#26412;&#23545;&#32929;&#31080;&#20215;&#26684;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#38543;&#30528;&#24341;&#20837;&#35299;&#37322;&#32452;&#20214;&#65292;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#38656;&#35201;LLM&#33021;&#22815;&#29992;&#21475;&#22836;&#26041;&#24335;&#35299;&#37322;&#20026;&#20160;&#20040;&#26576;&#20123;&#22240;&#32032;&#27604;&#20854;&#20182;&#22240;&#32032;&#26356;&#37325;&#35201;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35201;&#20026;&#36825;&#26679;&#30340;&#20219;&#21153;&#23545;LLM&#36827;&#34892;&#24494;&#35843;&#65292;&#38656;&#35201;&#19987;&#23478;&#26631;&#27880;&#30340;&#26679;&#26412;&#26469;&#35299;&#37322;&#35757;&#32451;&#38598;&#20013;&#30340;&#27599;&#27425;&#32929;&#31080;&#27874;&#21160;&#65292;&#36825;&#22312;&#25104;&#26412;&#21644;&#23454;&#38469;&#21487;&#25193;&#23637;&#24615;&#19978;&#26159;&#26114;&#36149;&#19988;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;Summarize-Explain-Predict&#65288;SEP&#65289;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDGE&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24773;&#24863;&#22686;&#24378;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#28041;&#21450;&#22810;&#31181;&#27169;&#24577;&#30340;&#35773;&#21050;&#23545;&#35805;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#35805;&#35821;&#35760;&#21495;&#23545;&#24773;&#24863;&#30340;&#22810;&#26679;&#25928;&#24212;&#12289;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20449;&#21495;&#19982;BART&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#35805;&#35821;&#12289;&#35805;&#35821;&#24773;&#24863;&#21644;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.03658</link><description>&lt;p&gt;
&#22312;&#23545;&#35805;&#20013;&#22686;&#24378;&#24773;&#24863;&#30340;&#22522;&#20110;&#22270;&#30340;&#35773;&#21050;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Sentiment-enhanced Graph-based Sarcasm Explanation in Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EDGE&#30340;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#24773;&#24863;&#22686;&#24378;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26694;&#26550;&#65292;&#26088;&#22312;&#20026;&#28041;&#21450;&#22810;&#31181;&#27169;&#24577;&#30340;&#35773;&#21050;&#23545;&#35805;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#35813;&#26694;&#26550;&#20811;&#26381;&#20102;&#35805;&#35821;&#35760;&#21495;&#23545;&#24773;&#24863;&#30340;&#22810;&#26679;&#25928;&#24212;&#12289;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20449;&#21495;&#19982;BART&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#20197;&#21450;&#35805;&#35821;&#12289;&#35805;&#35821;&#24773;&#24863;&#21644;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#20013;&#30340;&#35773;&#21050;&#35299;&#37322;&#65288;SED&#65289;&#26159;&#19968;&#39033;&#26032;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#20026;&#28041;&#21450;&#22810;&#31181;&#27169;&#24577;&#65288;&#21363;&#35805;&#35821;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#65289;&#30340;&#35773;&#21050;&#23545;&#35805;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#22522;&#20110;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BART&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24573;&#35270;&#20102;&#35805;&#35821;&#12289;&#35270;&#39057;&#21644;&#38899;&#39057;&#20013;&#23384;&#22312;&#30340;&#24773;&#24863;&#65292;&#32780;&#36825;&#20123;&#24773;&#24863;&#26159;&#35773;&#21050;&#35299;&#37322;&#20013;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#20107;&#23454;&#19978;&#65292;&#30001;&#20110;&#20197;&#19979;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;1&#65289;&#35805;&#35821;&#35760;&#21495;&#23545;&#24773;&#24863;&#30340;&#22810;&#26679;&#25928;&#24212;&#65307;2&#65289;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20449;&#21495;&#19982;BART&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#24046;&#36317;&#65307;3&#65289;&#35805;&#35821;&#12289;&#35805;&#35821;&#24773;&#24863;&#21644;&#35270;&#39057;&#38899;&#39057;&#24773;&#24863;&#20043;&#38388;&#30340;&#19981;&#21516;&#20851;&#31995;&#65292;&#23558;&#24773;&#24863;&#34701;&#20837;&#20197;&#25552;&#21319;SED&#24615;&#33021;&#26159;&#19968;&#39033;&#38750;&#24120;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#22686;&#24378;&#24773;&#24863;&#30340;&#22810;&#27169;&#24577;&#35773;&#21050;&#35299;&#37322;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;EDGE&#12290;
&lt;/p&gt;
&lt;p&gt;
Sarcasm Explanation in Dialogue (SED) is a new yet challenging task, which aims to generate a natural language explanation for the given sarcastic dialogue that involves multiple modalities (i.e., utterance, video, and audio). Although existing studies have achieved great success based on the generative pretrained language model BART, they overlook exploiting the sentiments residing in the utterance, video and audio, which are vital clues for sarcasm explanation. In fact, it is non-trivial to incorporate sentiments for boosting SED performance, due to three main challenges: 1) diverse effects of utterance tokens on sentiments; 2) gap between video-audio sentiment signals and the embedding space of BART; and 3) various relations among utterances, utterance sentiments, and video-audio sentiments. To tackle these challenges, we propose a novel sEntiment-enhanceD Graph-based multimodal sarcasm Explanation framework, named EDGE. In particular, we first propose a lexicon-guided utterance sen
&lt;/p&gt;</description></item><item><title>Stanceosaurus 2.0&#25193;&#23637;&#20102;&#21407;&#22987;&#26694;&#26550;&#65292;&#26032;&#22686;&#23545;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#30340;&#20998;&#31867;&#65292;&#26088;&#22312;&#25903;&#25345;&#20998;&#26512;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#36890;&#36807;&#19982;&#21021;&#22987;&#30740;&#31350;&#30340;&#32467;&#26524;&#30456;&#24403;&#30340;&#38646;-shot&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#39564;&#35777;&#20102;&#25968;&#25454;&#30340;&#20215;&#20540;&#21644;&#31435;&#22330;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03642</link><description>&lt;p&gt;
Stanceosaurus 2.0: &#23545;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Stanceosaurus 2.0: Classifying Stance Towards Russian and Spanish Misinformation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03642
&lt;/p&gt;
&lt;p&gt;
Stanceosaurus 2.0&#25193;&#23637;&#20102;&#21407;&#22987;&#26694;&#26550;&#65292;&#26032;&#22686;&#23545;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#30340;&#20998;&#31867;&#65292;&#26088;&#22312;&#25903;&#25345;&#20998;&#26512;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#36890;&#36807;&#19982;&#21021;&#22987;&#30740;&#31350;&#30340;&#32467;&#26524;&#30456;&#24403;&#30340;&#38646;-shot&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#39564;&#35777;&#20102;&#25968;&#25454;&#30340;&#20215;&#20540;&#21644;&#31435;&#22330;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stanceosaurus &#35821;&#26009;&#24211;&#65288;Zheng&#31561;&#65292;2022&#65289;&#26088;&#22312;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#12289;&#26631;&#27880;&#30340;&#12289;&#20174;Twitter&#20013;&#25552;&#21462;&#30340;&#20116;&#20998;&#31867;&#31435;&#22330;&#25968;&#25454;&#65292;&#36866;&#29992;&#20110;&#20998;&#26512;&#36328;&#25991;&#21270;&#21644;&#36328;&#35821;&#35328;&#30340;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;Stanceosaurus 2.0&#29256;&#26412;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#36825;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20102;&#20420;&#32599;&#26031;&#21644;&#35199;&#29677;&#29273;&#12290;&#21069;&#32773;&#30001;&#20110;&#19982;&#35199;&#26041;&#32039;&#24352;&#23616;&#21183;&#21644;&#23545;&#20044;&#20811;&#20848;&#30340;&#26292;&#21147;&#20837;&#20405;&#32780;&#21464;&#24471;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#32780;&#21518;&#32773;&#21017;&#20195;&#34920;&#30528;&#19968;&#20010;&#34987;&#20027;&#35201;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#24573;&#35270;&#30340;&#24222;&#22823;&#31038;&#32676;&#12290;&#36890;&#36807;&#28155;&#21152;3,874&#26465;&#35199;&#29677;&#29273;&#21644;&#20420;&#35821;&#25512;&#25991;&#65292;&#28041;&#21450;41&#21017;&#34394;&#20551;&#20449;&#24687;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#25903;&#25345;&#20851;&#27880;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#25968;&#25454;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#22312;&#22810;&#35821;&#35328;BERT&#19978;&#20351;&#29992;&#38646;-shot&#36328;&#35821;&#35328;&#36801;&#31227;&#65292;&#24471;&#21040;&#20102;&#19982;&#26368;&#21021;&#30340;Stanceosaurus&#30740;&#31350;&#30456;&#24403;&#30340;&#32467;&#26524;&#65292;&#20004;&#31181;&#35821;&#35328;&#30340;macro F1&#24471;&#20998;&#22343;&#20026;43&#12290;&#36825;&#26174;&#31034;&#20102;&#31435;&#22330;&#20998;&#31867;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Stanceosaurus corpus (Zheng et al., 2022) was designed to provide high-quality, annotated, 5-way stance data extracted from Twitter, suitable for analyzing cross-cultural and cross-lingual misinformation. In the Stanceosaurus 2.0 iteration, we extend this framework to encompass Russian and Spanish. The former is of current significance due to prevalent misinformation amid escalating tensions with the West and the violent incursion into Ukraine. The latter, meanwhile, represents an enormous community that has been largely overlooked on major social media platforms. By incorporating an additional 3,874 Spanish and Russian tweets over 41 misinformation claims, our objective is to support research focused on these issues. To demonstrate the value of this data, we employed zero-shot cross-lingual transfer on multilingual BERT, yielding results on par with the initial Stanceosaurus study with a macro F1 score of 43 for both languages. This underlines the viability of stance classificatio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19987;&#19994;&#20195;&#29702;&#20154;&#65288;PAgents&#65289;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#21019;&#24314;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#21644;&#19987;&#19994;&#27700;&#24179;&#30340;&#33258;&#20027;&#20195;&#29702;&#20154;&#65292;&#20197;&#37325;&#22609;&#19987;&#19994;&#26381;&#21153;&#12290;&#36890;&#36807;&#19981;&#26029;&#21457;&#23637;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#25972;&#21512;&#65292;PAgents&#30340;&#25552;&#21319;&#21487;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#22797;&#26434;&#39046;&#22495;&#23637;&#31034;&#19987;&#19994;&#31934;&#36890;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03628</link><description>&lt;p&gt;
&#19987;&#19994;&#20195;&#29702;&#20154; - &#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#28436;&#21464;&#20026;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#30340;&#33258;&#20027;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
Professional Agents -- Evolving Large Language Models into Autonomous Experts with Human-Level Competencies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19987;&#19994;&#20195;&#29702;&#20154;&#65288;PAgents&#65289;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#21019;&#24314;&#20855;&#26377;&#20154;&#31867;&#27700;&#24179;&#33021;&#21147;&#21644;&#19987;&#19994;&#27700;&#24179;&#30340;&#33258;&#20027;&#20195;&#29702;&#20154;&#65292;&#20197;&#37325;&#22609;&#19987;&#19994;&#26381;&#21153;&#12290;&#36890;&#36807;&#19981;&#26029;&#21457;&#23637;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#25972;&#21512;&#65292;PAgents&#30340;&#25552;&#21319;&#21487;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#22797;&#26434;&#39046;&#22495;&#23637;&#31034;&#19987;&#19994;&#31934;&#36890;&#21644;&#20154;&#31867;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;PaLM&#21644;GPT-4&#30340;&#20986;&#29616;&#65292;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#21331;&#36234;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#32423;&#35821;&#35328;&#27969;&#21033;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19987;&#19994;&#20195;&#29702;&#20154;&#65288;PAgents&#65289;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#20010;&#24212;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#21019;&#24314;&#20855;&#26377;&#21487;&#25511;&#12289;&#19987;&#19994;&#27700;&#24179;&#30340;&#20132;&#20114;&#24335;&#33258;&#20027;&#20195;&#29702;&#20154;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19987;&#19994;&#20195;&#29702;&#20154;&#36890;&#36807;&#19981;&#26029;&#21457;&#23637;&#30340;&#19987;&#19994;&#30693;&#35782;&#21487;&#20197;&#37325;&#22609;&#19987;&#19994;&#26381;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#19987;&#19994;&#20195;&#29702;&#20154;&#26694;&#26550;&#21253;&#25324;&#19977;&#23618;&#26550;&#26500;&#65306;&#22522;&#30784;&#24037;&#20855;&#23618;&#12289;&#20013;&#38388;&#20195;&#29702;&#20154;&#23618;&#21644;&#39030;&#23618;&#21327;&#21516;&#23618;&#12290;&#26412;&#25991;&#26088;&#22312;&#25512;&#21160;&#20851;&#20110;LLMs&#26377;&#21069;&#26223;&#30340;&#30495;&#23454;&#24212;&#29992;&#30340;&#35752;&#35770;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19987;&#19994;&#20195;&#29702;&#20154;&#30340;&#19981;&#26029;&#25552;&#21319;&#21644;&#25972;&#21512;&#21487;&#33021;&#23548;&#33268;AI&#31995;&#32479;&#22312;&#22797;&#26434;&#39046;&#22495;&#23637;&#31034;&#19987;&#19994;&#31934;&#36890;&#65292;&#28385;&#36275;&#20851;&#38190;&#38656;&#27714;&#65292;&#24182;&#26377;&#21487;&#33021;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#20154;&#31867;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) such as ChatGPT, PaLM, and GPT-4 has catalyzed remarkable advances in natural language processing, demonstrating human-like language fluency and reasoning capacities. This position paper introduces the concept of Professional Agents (PAgents), an application framework harnessing LLM capabilities to create autonomous agents with controllable, specialized, interactive, and professional-level competencies. We posit that PAgents can reshape professional services through continuously developed expertise. Our proposed PAgents framework entails a tri-layered architecture for genesis, evolution, and synergy: a base tool layer, a middle agent layer, and a top synergy layer. This paper aims to spur discourse on promising real-world applications of LLMs. We argue the increasing sophistication and integration of PAgents could lead to AI systems exhibiting professional mastery over complex domains, serving critical needs, and potentially achieving artifici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.03627</link><description>&lt;p&gt;
&#36817;&#20284;&#30340;&#20013;&#24515;&#21270;softmax&#25439;&#22833;&#29992;&#20110;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Partially Recentralization Softmax Loss for Vision-Language Models Robustness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#65292;&#33021;&#22815;&#26377;&#25928;&#25269;&#24481;&#24120;&#35265;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#31361;&#30772;&#65292;&#22810;&#27169;&#24577;&#25216;&#26415;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#24050;&#32463;&#35777;&#26126;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#65292;&#21363;&#27169;&#22411;&#30340;&#36755;&#20986;&#21487;&#20197;&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#32780;&#21457;&#29983;&#24040;&#22823;&#21464;&#21270;&#12290;&#34429;&#28982;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38450;&#24481;&#25216;&#26415;&#65292;&#20294;&#23545;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#36824;&#27809;&#26377;&#36827;&#34892;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#20462;&#25913;&#39044;&#35757;&#32451;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38480;&#21046;&#21069;K&#20010;softmax&#36755;&#20986;&#26469;&#25552;&#20379;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#22522;&#20110;&#35780;&#20272;&#21644;&#35780;&#20998;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#65292;&#23545;&#25239;&#24120;&#35265;&#30340;&#25915;&#20987;&#26377;&#25928;&#12290;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#24212;&#35813;&#25506;&#32034;&#36825;&#31867;&#25439;&#22833;&#20989;&#25968;&#30340;&#36755;&#20986;&#22810;&#26679;&#24615;&#12289;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#40065;&#26834;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20043;&#21518;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models make a breakthrough in natural language processing tasks (NLP), multimodal technique becomes extremely popular. However, it has been shown that multimodal NLP are vulnerable to adversarial attacks, where the outputs of a model can be dramatically changed by a perturbation to the input. While several defense techniques have been proposed both in computer vision and NLP models, the multimodal robustness of models have not been fully explored. In this paper, we study the adversarial robustness provided by modifying loss function of pre-trained multimodal models, by restricting top K softmax outputs. Based on the evaluation and scoring, our experiments show that after a fine-tuning, adversarial robustness of pre-trained models can be significantly improved, against popular attacks. Further research should be studying, such as output diversity, generalization and the robustness-performance trade-off of this kind of loss functions. Our code will be available after th
&lt;/p&gt;</description></item><item><title>SELF-DISCOVER&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#33021;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#21457;&#29616;&#20219;&#21153;&#20869;&#22312;&#30340;&#25512;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#35745;&#31639;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03620</link><description>&lt;p&gt;
&#33258;&#25105;&#21457;&#29616;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#26500;&#24314;&#25512;&#29702;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Self-Discover: Large Language Models Self-Compose Reasoning Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03620
&lt;/p&gt;
&lt;p&gt;
SELF-DISCOVER&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#33021;&#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#20027;&#21457;&#29616;&#20219;&#21153;&#20869;&#22312;&#30340;&#25512;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#30340;&#35299;&#20915;&#24615;&#33021;&#65292;&#24182;&#22312;&#25512;&#29702;&#35745;&#31639;&#26041;&#38754;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SELF-DISCOVER&#65292;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;LLM&#33258;&#20027;&#21457;&#29616;&#20219;&#21153;&#20869;&#22312;&#30340;&#25512;&#29702;&#32467;&#26500;&#65292;&#20197;&#24212;&#23545;&#23545;&#20110;&#20856;&#22411;&#25552;&#31034;&#26041;&#27861;&#32780;&#35328;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22797;&#26434;&#25512;&#29702;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#33258;&#25105;&#21457;&#29616;&#36807;&#31243;&#65292;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;LLM&#36873;&#25321;&#22810;&#20010;&#21407;&#23376;&#25512;&#29702;&#27169;&#22359;&#65292;&#22914;&#25209;&#21028;&#24615;&#24605;&#32500;&#21644;&#36880;&#27493;&#24605;&#32771;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;LLM&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#36981;&#24490;&#30340;&#26126;&#30830;&#25512;&#29702;&#32467;&#26500;&#12290;SELF-DISCOVER&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22914;BigBench-Hard&#12289;&#22522;&#20110;&#20195;&#29702;&#30340;&#25512;&#29702;&#21644;MATH&#19978;&#65292;&#30456;&#36739;&#20110;Chain of Thought (CoT)&#30340;&#24615;&#33021;&#25552;&#21319;&#39640;&#36798;32%&#12290;&#27492;&#22806;&#65292;SELF-DISCOVER&#22312;&#38656;&#35201;10-40&#20493;&#36739;&#23569;&#25512;&#29702;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36739;CoT-Self-Consistency&#31561;&#25512;&#29702;&#23494;&#38598;&#26041;&#27861;&#30340;&#34920;&#29616;&#26356;&#22909;&#65292;&#36229;&#36807;20%&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#21457;&#29616;&#30340;&#25512;&#29702;&#32467;&#26500;&#22312;&#27169;&#22411;&#23478;&#26063;&#20043;&#38388;&#26159;&#26222;&#36941;&#36866;&#29992;&#30340;&#65306;&#20174;PaLM 2-L&#21040;GPT-4&#65292;&#20174;GPT-4&#21040;Llama2&#65292;&#24182;&#20998;&#20139;&#20102;
&lt;/p&gt;
&lt;p&gt;
We introduce SELF-DISCOVER, a general framework for LLMs to self-discover the task-intrinsic reasoning structures to tackle complex reasoning problems that are challenging for typical prompting methods. Core to the framework is a self-discovery process where LLMs select multiple atomic reasoning modules such as critical thinking and step-by-step thinking, and compose them into an explicit reasoning structure for LLMs to follow during decoding. SELF-DISCOVER substantially improves GPT-4 and PaLM 2's performance on challenging reasoning benchmarks such as BigBench-Hard, grounded agent reasoning, and MATH, by as much as 32% compared to Chain of Thought (CoT). Furthermore, SELF-DISCOVER outperforms inference-intensive methods such as CoT-Self-Consistency by more than 20%, while requiring 10-40x fewer inference compute. Finally, we show that the self-discovered reasoning structures are universally applicable across model families: from PaLM 2-L to GPT-4, and from GPT-4 to Llama2, and share 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#26694;&#26550;&#65292;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;GPT-4&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#23545;&#20154;&#31867;&#30340;&#22797;&#21046;&#24433;&#21709;&#26356;&#22823;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#24449;&#27604;GPT-4&#30340;&#34920;&#24449;&#26356;&#20855;&#20998;&#31163;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03618</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#27604;&#36739;&#20154;&#31867;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25277;&#35937;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Comparing Abstraction in Humans and Large Language Models Using Multimodal Serial Reproduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03618
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#29616;&#19968;&#20010;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#26694;&#26550;&#65292;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;GPT-4&#30340;&#25277;&#35937;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#23545;&#20154;&#31867;&#30340;&#22797;&#21046;&#24433;&#21709;&#26356;&#22823;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#24449;&#27604;GPT-4&#30340;&#34920;&#24449;&#26356;&#20855;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20174;&#22024;&#26434;&#30340;&#24863;&#23448;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#19990;&#30028;&#25277;&#35937;&#12290;&#36830;&#32493;&#22797;&#21046;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#31867;&#20284;&#20110;&#30005;&#35805;&#28216;&#25103;&#30340;&#33539;&#24335;&#26469;&#30740;&#31350;&#20154;&#20204;&#22914;&#20309;&#26500;&#24314;&#19990;&#30028;&#65292;&#20854;&#20013;&#19968;&#20010;&#20154;&#35266;&#23519;&#19968;&#20010;&#21050;&#28608;&#24182;&#23558;&#20854;&#22797;&#21046;&#32473;&#19979;&#19968;&#20010;&#20154;&#24418;&#25104;&#19968;&#26465;&#22797;&#21046;&#38142;&#12290;&#36807;&#21435;&#30340;&#36830;&#32493;&#22797;&#21046;&#23454;&#39564;&#36890;&#24120;&#37319;&#29992;&#21333;&#19968;&#24863;&#35273;&#27169;&#24335;&#65292;&#20294;&#20154;&#31867;&#32463;&#24120;&#36890;&#36807;&#35821;&#35328;&#30456;&#20114;&#20256;&#36798;&#19990;&#30028;&#30340;&#25277;&#35937;&#12290;&#20026;&#20102;&#35843;&#26597;&#35821;&#35328;&#23545;&#25277;&#35937;&#24418;&#25104;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#36890;&#36807;&#35201;&#27714;&#25509;&#25910;&#35270;&#35273;&#21050;&#28608;&#30340;&#20154;&#20197;&#35821;&#35328;&#24418;&#24335;&#26469;&#22797;&#21046;&#23427;&#65292;&#24182;&#21453;&#20043;&#20134;&#28982;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#36830;&#32493;&#22797;&#21046;&#26694;&#26550;&#12290;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;GPT-4&#36827;&#34892;&#20102;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#22797;&#21046;&#65292;&#24182;&#21457;&#29616;&#23558;&#35821;&#35328;&#20316;&#20026;&#19968;&#31181;&#27169;&#24577;&#23545;&#20154;&#31867;&#30340;&#22797;&#21046;&#24433;&#21709;&#26356;&#22823;&#12290;&#36825;&#34920;&#26126;&#20154;&#31867;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#24449;&#27604;GPT-4&#30340;&#34920;&#24449;&#26356;&#20855;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#28151;&#21512;&#24037;&#20316;&#22330;&#25152;&#25552;&#20379;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#30340;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#25552;&#20379;&#36866;&#24403;&#24037;&#20316;&#21306;&#24314;&#35758;&#26041;&#38754;&#20855;&#26377;&#36229;&#36234;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#39640;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#20307;&#39564;&#12290;</title><link>https://arxiv.org/abs/2402.03616</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#28151;&#21512;&#24037;&#20316;&#22330;&#25152;&#20915;&#31574;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Hybrid Workplace Decision Support
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#28151;&#21512;&#24037;&#20316;&#22330;&#25152;&#25552;&#20379;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#30340;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#21457;&#29616;LLMs&#22312;&#25552;&#20379;&#36866;&#24403;&#24037;&#20316;&#21306;&#24314;&#35758;&#26041;&#38754;&#20855;&#26377;&#36229;&#36234;&#25552;&#31034;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25552;&#39640;&#24037;&#20316;&#32773;&#30340;&#24037;&#20316;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#25191;&#34892;&#21508;&#31181;&#25991;&#26412;&#22788;&#29702;&#20219;&#21153;&#24182;&#20026;&#25552;&#35758;&#30340;&#25805;&#20316;&#25110;&#20915;&#31574;&#25552;&#20379;&#25991;&#26412;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#22312;&#28151;&#21512;&#24037;&#20316;&#26102;&#20195;&#65292;LLMs&#21487;&#20197;&#20026;&#35774;&#35745;&#28151;&#21512;&#24037;&#20316;&#35745;&#21010;&#30340;&#24037;&#20316;&#32773;&#25552;&#20379;&#26234;&#33021;&#20915;&#31574;&#25903;&#25345;&#12290;&#29305;&#21035;&#26159;&#23427;&#20204;&#21487;&#20197;&#20026;&#24179;&#34913;&#20247;&#22810;&#20915;&#31574;&#22240;&#32032;&#30340;&#24037;&#20316;&#32773;&#25552;&#20379;&#24314;&#35758;&#21644;&#35299;&#37322;&#65292;&#20174;&#32780;&#22686;&#24378;&#20182;&#20204;&#30340;&#24037;&#20316;&#20307;&#39564;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#28151;&#21512;&#24037;&#20316;&#29615;&#22659;&#20013;&#24037;&#20316;&#21306;&#20915;&#31574;&#25903;&#25345;&#27169;&#22411;&#65292;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#20102;LLMs&#23545;&#20110;&#25552;&#20379;&#36866;&#24403;&#24037;&#20316;&#21306;&#24314;&#35758;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#36229;&#36234;&#20102;&#25552;&#31034;&#20013;&#30340;&#25351;&#23548;&#26041;&#38024;&#65292;LLMs&#21487;&#20197;&#22312;&#24037;&#20316;&#21306;&#36164;&#28304;&#30340;&#21487;&#29992;&#24615;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#24037;&#20316;&#32773;&#22312;&#24037;&#20316;&#21306;&#36873;&#25321;&#19978;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#35780;&#20272;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#24037;&#20316;&#32773;&#30340;&#20915;&#31574;&#21487;&#33021;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) hold the potential to perform a variety of text processing tasks and provide textual explanations for proposed actions or decisions. In the era of hybrid work, LLMs can provide intelligent decision support for workers who are designing their hybrid work plans. In particular, they can offer suggestions and explanations to workers balancing numerous decision factors, thereby enhancing their work experience. In this paper, we present a decision support model for workspaces in hybrid work environments, leveraging the reasoning skill of LLMs. We first examine LLM's capability of making suitable workspace suggestions. We find that its reasoning extends beyond the guidelines in the prompt and the LLM can manage the trade-off among the available resources in the workspaces. We conduct an extensive user study to understand workers' decision process for workspace choices and evaluate the effectiveness of the system. We observe that a worker's decision could be influe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RAP&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20197;&#21160;&#24577;&#26041;&#24335;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;RAP&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03610</link><description>&lt;p&gt;
RAP&#65306;&#20855;&#26377;&#19978;&#19979;&#25991;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22810;&#27169;&#24577;LLM&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RAP: Retrieval-Augmented Planning with Contextual Memory for Multimodal LLM Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;RAP&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#20197;&#21160;&#24577;&#26041;&#24335;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#22686;&#24378;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#24182;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;RAP&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#21487;&#20197;&#34987;&#37096;&#32626;&#20026;&#29992;&#20110;&#26426;&#22120;&#20154;&#12289;&#28216;&#25103;&#21644;API&#38598;&#25104;&#31561;&#39046;&#22495;&#20013;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#20915;&#31574;&#24212;&#29992;&#30340;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#26469;&#25351;&#23548;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#20173;&#28982;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#26816;&#32034;&#22686;&#24378;&#35268;&#21010;&#65288;RAP&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#21160;&#24577;&#22320;&#21033;&#29992;&#36807;&#21435;&#19982;&#24403;&#21069;&#24773;&#20917;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#32463;&#39564;&#65292;&#20174;&#32780;&#25552;&#21319;&#20195;&#29702;&#30340;&#35268;&#21010;&#33021;&#21147;&#12290;RAP&#30340;&#29305;&#28857;&#22312;&#20110;&#23427;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#23427;&#22312;&#32431;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#20219;&#21153;&#12290;&#23454;&#35777;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;RAP&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#25991;&#26412;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;SOTA&#30340;&#34920;&#29616;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#27169;&#24577;LLM&#20195;&#29702;&#22312;&#20855;&#36523;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;RAP&#22312;&#25512;&#36827;LLM&#30340;&#21151;&#33021;&#21644;&#36866;&#29992;&#24615;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Owing to recent advancements, Large Language Models (LLMs) can now be deployed as agents for increasingly complex decision-making applications in areas including robotics, gaming, and API integration. However, reflecting past experiences in current decision-making processes, an innate human behavior, continues to pose significant challenges. Addressing this, we propose Retrieval-Augmented Planning (RAP) framework, designed to dynamically leverage past experiences corresponding to the current situation and context, thereby enhancing agents' planning capabilities. RAP distinguishes itself by being versatile: it excels in both text-only and multimodal environments, making it suitable for a wide range of tasks. Empirical evaluations demonstrate RAP's effectiveness, where it achieves SOTA performance in textual scenarios and notably enhances multimodal LLM agents' performance for embodied tasks. These results highlight RAP's potential in advancing the functionality and applicability of LLM 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03607</link><description>&lt;p&gt;
&#25552;&#39640;&#22810;&#27169;&#24577;&#33829;&#38144;&#30340;&#19978;&#19979;&#25991;&#19968;&#33268;&#24615;&#65306;&#30693;&#35782;&#22522;&#30784;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Contextual Congruence Across Modalities for Effective Multimodal Marketing using Knowledge-infused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#24120;&#35782;&#30693;&#35782;&#22270;&#35889;&#19982;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25913;&#36827;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#25928;&#26524;&#30340;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26089;&#26399;&#26816;&#27979;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#35774;&#22791;&#30340;&#26222;&#21450;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#32447;&#20307;&#39564;&#22810;&#27169;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#35270;&#35273;&#27169;&#22411;&#65288;LVM&#65289;&#20173;&#28982;&#21463;&#21040;&#25429;&#25417;&#36328;&#27169;&#24577;&#35821;&#20041;&#20851;&#31995;&#30340;&#25972;&#20307;&#24847;&#20041;&#30340;&#38480;&#21046;&#12290;&#32570;&#20047;&#26126;&#30830;&#30340;&#24120;&#35782;&#30693;&#35782;&#65288;&#20363;&#22914;&#65292;&#20316;&#20026;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65289;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20165;&#36890;&#36807;&#25429;&#25417;&#24222;&#22823;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#39640;&#32423;&#27169;&#24335;&#26469;&#23398;&#20064;&#38544;&#24335;&#34920;&#31034;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#36328;&#27169;&#24577;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#26174;&#24335;&#30340;&#24120;&#35782;&#30693;&#35782;&#20197;&#30693;&#35782;&#22270;&#35889;&#30340;&#24418;&#24335;&#19982;&#22823;&#22411;&#30340;VLM&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21363;&#39044;&#27979;&#22810;&#27169;&#24577;&#33829;&#38144;&#27963;&#21160;&#30340;&#26377;&#25928;&#24615;&#12290;&#34429;&#28982;&#33829;&#38144;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#35828;&#26381;&#21147;&#30340;&#25351;&#26631;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#20855;&#26377;&#35828;&#26381;&#21147;&#30340;&#22810;&#27169;&#24577;&#27963;&#21160;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#35780;&#20272;&#21644;&#22686;&#24378;&#33829;&#38144;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of smart devices with the ability to capture moments in multiple modalities has enabled users to experience multimodal information online. However, large Language (LLMs) and Vision models (LVMs) are still limited in capturing holistic meaning with cross-modal semantic relationships. Without explicit, common sense knowledge (e.g., as a knowledge graph), Visual Language Models (VLMs) only learn implicit representations by capturing high-level patterns in vast corpora, missing essential contextual cross-modal cues. In this work, we design a framework to couple explicit commonsense knowledge in the form of knowledge graphs with large VLMs to improve the performance of a downstream task, predicting the effectiveness of multi-modal marketing campaigns. While the marketing application provides a compelling metric for assessing our methods, our approach enables the early detection of likely persuasive multi-modal campaigns and the assessment and augmentation of marketing theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35782;&#21035;&#36991;&#23381;&#33647;&#20999;&#25442;&#21407;&#22240;&#19978;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;BERT&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03597</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#23454;&#38469;&#25968;&#25454;&#20013;&#35782;&#21035;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;
&lt;/p&gt;
&lt;p&gt;
Identifying Reasons for Contraceptive Switching from Real-World Data Using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#35782;&#21035;&#36991;&#23381;&#33647;&#20999;&#25442;&#21407;&#22240;&#19978;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;GPT-4&#21487;&#20197;&#20934;&#30830;&#22320;&#20174;&#20020;&#24202;&#35760;&#24405;&#20013;&#25552;&#21462;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;BERT&#27169;&#22411;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#26041;&#36991;&#23381;&#33647;&#22312;&#25903;&#25345;&#22919;&#22899;&#29983;&#27542;&#20581;&#24247;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#22312;&#32654;&#22269;&#26377;&#23558;&#36817;5000&#19975;&#22899;&#24615;&#20351;&#29992;&#36991;&#23381;&#33647;&#65292;&#20102;&#35299;&#23548;&#33268;&#36991;&#23381;&#33647;&#36873;&#25321;&#21644;&#20999;&#25442;&#30340;&#22240;&#32032;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#33647;&#29289;&#20999;&#25442;&#30456;&#20851;&#30340;&#35768;&#22810;&#22240;&#32032;&#36890;&#24120;&#21482;&#22312;&#26080;&#32467;&#26500;&#30340;&#20020;&#24202;&#35760;&#24405;&#20013;&#24471;&#21040;&#25429;&#33719;&#65292;&#24182;&#19988;&#24456;&#38590;&#25552;&#21462;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#36817;&#24320;&#21457;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-4&#65288;&#36890;&#36807;&#31526;&#21512;HIPAA&#30340;Microsoft Azure API&#65289;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#20197;&#20174;UCSF&#20449;&#24687;&#20849;&#20139;&#24179;&#21488;&#30340;&#20020;&#24202;&#35760;&#24405;&#25968;&#25454;&#38598;&#20013;&#35782;&#21035;&#36991;&#23381;&#33647;&#31867;&#21035;&#20999;&#25442;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GPT-4&#21487;&#20197;&#20934;&#30830;&#22320;&#25552;&#21462;&#36991;&#23381;&#33647;&#20999;&#25442;&#30340;&#21407;&#22240;&#65292;&#30456;&#36739;&#20110;&#22522;&#20934;BERT&#27169;&#22411;&#65292;&#22312;&#36991;&#23381;&#33647;&#24320;&#22987;&#21644;&#20572;&#27490;&#25552;&#21462;&#26041;&#38754;&#30340;microF1&#20998;&#25968;&#20998;&#21035;&#20026;0.849&#21644;0.881&#12290;&#23545;&#20110;GPT-4&#25552;&#21462;&#30340;&#20999;&#25442;&#21407;&#22240;&#30340;&#20154;&#24037;&#35780;&#20272;&#26174;&#31034;&#20986;91.4%&#30340;&#20934;&#30830;&#24230;&#65292;&#20986;&#29616;&#24187;&#35273;&#30340;&#24773;&#20917;&#24456;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prescription contraceptives play a critical role in supporting women's reproductive health. With nearly 50 million women in the United States using contraceptives, understanding the factors that drive contraceptives selection and switching is of significant interest. However, many factors related to medication switching are often only captured in unstructured clinical notes and can be difficult to extract. Here, we evaluate the zero-shot abilities of a recently developed large language model, GPT-4 (via HIPAA-compliant Microsoft Azure API), to identify reasons for switching between classes of contraceptives from the UCSF Information Commons clinical notes dataset. We demonstrate that GPT-4 can accurately extract reasons for contraceptive switching, outperforming baseline BERT-based models with microF1 scores of 0.849 and 0.881 for contraceptive start and stop extraction, respectively. Human evaluation of GPT-4-extracted reasons for switching showed 91.4% accuracy, with minimal hallucin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.03563</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#27169;&#22411;&#21306;&#20998;&#21487;&#30693;&#19982;&#19981;&#21487;&#30693;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Distinguishing the Knowable from the Unknowable with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03563
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#33258;&#30001;&#25991;&#26412;&#20013;&#35782;&#21035;&#20316;&#20026;&#20195;&#29702;&#30340;&#27169;&#22411;&#21644;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#26356;&#22823;&#27169;&#22411;&#20196;&#29260;&#32423;&#21035;&#19978;&#30340;&#33258;&#20449;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#65292;&#36825;&#35777;&#26126;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#33258;&#30001;&#25991;&#26412;&#36755;&#20986;&#20013;&#65292;&#26159;&#21542;&#21487;&#20197;&#37492;&#21035;&#20986;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#32570;&#20047;&#30693;&#35782;&#30340;&#19981;&#30830;&#23450;&#24615;&#65289;&#21644;&#20598;&#28982;&#19981;&#30830;&#23450;&#24615;&#65288;&#21453;&#26144;&#22522;&#30784;&#20998;&#24067;&#20013;&#30340;&#29109;&#65289;&#12290;&#22312;&#27809;&#26377;&#30495;&#23454;&#27010;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#20010;&#35774;&#32622;&#65292;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#20026;&#20102;&#65288;&#36817;&#20284;&#22320;&#65289;&#20998;&#35299;&#32473;&#23450;LLM&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#19968;&#20010;&#26126;&#26174;&#26356;&#22823;&#30340;&#27169;&#22411;&#20805;&#24403;&#22320;&#38754;&#30495;&#30456;&#30340;&#20195;&#29702;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#30340;&#23567;&#22411;&#32447;&#24615;&#25506;&#27979;&#22120;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#26356;&#22823;&#27169;&#22411;&#23558;&#26356;&#33258;&#20449;&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#22312;&#19968;&#20010;&#25991;&#26412;&#39046;&#22495;&#19978;&#35757;&#32451;&#30340;&#25506;&#27979;&#22120;&#21487;&#20197;&#27867;&#21270;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#36827;&#19968;&#27493;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#22312;&#30456;&#21516;&#20219;&#21153;&#19978;&#36798;&#21040;&#20102;&#38750;&#24179;&#20961;&#30340;&#20934;&#30830;&#24230;&#12290;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#35299;&#37322;&#36825;&#20123;&#32467;&#26524;&#20316;&#20026;LLMs&#20869;&#37096;&#33258;&#28982;&#22320;&#21253;&#21547;&#20102;&#19981;&#21516;&#31867;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#26377;&#21161;&#20110;&#21046;&#23450;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the feasibility of identifying epistemic uncertainty (reflecting a lack of knowledge), as opposed to aleatoric uncertainty (reflecting entropy in the underlying distribution), in the outputs of large language models (LLMs) over free-form text. In the absence of ground-truth probabilities, we explore a setting where, in order to (approximately) disentangle a given LLM's uncertainty, a significantly larger model stands in as a proxy for the ground truth. We show that small linear probes trained on the embeddings of frozen, pretrained models accurately predict when larger models will be more confident at the token level and that probes trained on one text domain generalize to others. Going further, we propose a fully unsupervised method that achieves non-trivial accuracy on the same task. Taken together, we interpret these results as evidence that LLMs naturally contain internal representations of different types of uncertainty that could potentially be leveraged to devise more i
&lt;/p&gt;</description></item><item><title>VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03561</link><description>&lt;p&gt;
VLN-Video: &#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#36827;&#34892;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
VLN-Video: Utilizing Driving Videos for Outdoor Vision-and-Language Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03561
&lt;/p&gt;
&lt;p&gt;
VLN-Video&#21033;&#29992;&#34892;&#36710;&#35270;&#39057;&#30340;&#22810;&#26679;&#23460;&#22806;&#29615;&#22659;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#23548;&#33322;&#25351;&#20196;&#19982;&#21160;&#20316;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#20102;&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#22806;&#35270;&#35273;&#19982;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#35201;&#27714;&#20195;&#29702;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#36924;&#30495;&#30340;&#19977;&#32500;&#23460;&#22806;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#29616;&#26377;&#30340;VLN&#26041;&#27861;&#22312;&#23548;&#33322;&#29615;&#22659;&#22810;&#26679;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;VLN-Video&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22312;&#32654;&#22269;&#22810;&#20010;&#22478;&#24066;&#30340;&#34892;&#36710;&#35270;&#39057;&#20013;&#23384;&#22312;&#30340;&#22810;&#26679;&#21270;&#23460;&#22806;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#23548;&#33322;&#25351;&#20196;&#21644;&#21160;&#20316;&#26469;&#25552;&#39640;&#23460;&#22806;VLN&#24615;&#33021;&#12290;VLN-Video&#32467;&#21512;&#20102;&#30452;&#35266;&#32463;&#20856;&#26041;&#27861;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#20248;&#21183;&#65292;&#21033;&#29992;&#27169;&#26495;&#22635;&#20805;&#29983;&#25104;&#26377;&#23454;&#38469;&#22522;&#30784;&#30340;&#23548;&#33322;&#25351;&#20196;&#65292;&#24182;&#32467;&#21512;&#22522;&#20110;&#22270;&#20687;&#26059;&#36716;&#30456;&#20284;&#24230;&#30340;&#23548;&#33322;&#21160;&#20316;&#39044;&#27979;&#22120;&#20174;&#34892;&#36710;&#35270;&#39057;&#20013;&#33719;&#21462;VLN&#39118;&#26684;&#30340;&#25968;&#25454;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;VLN&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Touchdown&#25968;&#25454;&#38598;&#21644;&#30001;&#34892;&#36710;&#35270;&#39057;&#21019;&#24314;&#30340;&#35270;&#39057;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Outdoor Vision-and-Language Navigation (VLN) requires an agent to navigate through realistic 3D outdoor environments based on natural language instructions. The performance of existing VLN methods is limited by insufficient diversity in navigation environments and limited training data. To address these issues, we propose VLN-Video, which utilizes the diverse outdoor environments present in driving videos in multiple cities in the U.S. augmented with automatically generated navigation instructions and actions to improve outdoor VLN performance. VLN-Video combines the best of intuitive classical approaches and modern deep learning techniques, using template infilling to generate grounded navigation instructions, combined with an image rotation similarity-based navigation action predictor to obtain VLN style data from driving videos for pretraining deep learning VLN models. We pre-train the model on the Touchdown dataset and our video-augmented dataset created from driving videos with th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#20219;&#21153;&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#21644;&#35789;&#27719;&#20449;&#21495;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#36716;&#24405;&#20013;&#25552;&#39640;&#20102;&#38382;&#21495;&#30340;F1&#20998;&#25968;&#21644;&#25972;&#20307;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.03519</link><description>&lt;p&gt;
&#22312;&#35199;&#29677;&#29273;&#35821;&#20013;&#35299;&#20915;&#36716;&#24405;&#27495;&#20041;&#65306;&#19968;&#31181;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#31995;&#32479;&#29992;&#20110;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Resolving Transcription Ambiguity in Spanish: A Hybrid Acoustic-Lexical System for Punctuation Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03519
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#31995;&#32479;&#65292;&#29992;&#20110;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#20219;&#21153;&#65292;&#36890;&#36807;&#25972;&#21512;&#22768;&#23398;&#21644;&#35789;&#27719;&#20449;&#21495;&#65292;&#22312;&#35199;&#29677;&#29273;&#35821;&#36716;&#24405;&#20013;&#25552;&#39640;&#20102;&#38382;&#21495;&#30340;F1&#20998;&#25968;&#21644;&#25972;&#20307;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#36229;&#36234;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#26159;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#21518;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#20197;&#22686;&#24378;&#36716;&#24405;&#30340;&#21487;&#35835;&#24615;&#24182;&#20419;&#36827;&#21518;&#32493;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35789;&#27719;&#30340;&#26041;&#27861;&#19981;&#36275;&#20197;&#35299;&#20915;&#35199;&#29677;&#29273;&#35821;&#20013;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#30340;&#20219;&#21153;&#65292;&#22240;&#20026;&#22312;&#26410;&#26631;&#28857;&#30340;&#38472;&#36848;&#21477;&#21644;&#30097;&#38382;&#21477;&#20043;&#38388;&#24120;&#24120;&#23384;&#22312;&#27495;&#20041;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#22768;&#23398;-&#35789;&#27719;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#31995;&#32479;&#65292;&#29992;&#20110;&#35199;&#29677;&#29273;&#35821;&#36716;&#24405;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#36807;&#31243;&#25972;&#21512;&#22768;&#23398;&#21644;&#35789;&#27719;&#20449;&#21495;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#38382;&#21495;&#30340;F1&#20998;&#25968;&#65292;&#24182;&#22312;&#20844;&#20849;&#21644;&#20869;&#37096;&#30340;&#35199;&#29677;&#29273;&#35821;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#25972;&#20307;&#26631;&#28857;&#31526;&#21495;&#24674;&#22797;&#12290;&#27492;&#22806;&#65292;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22522;&#20934;&#27604;&#36739;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#24310;&#36831;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;ASR&#27169;&#22359;&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20063;&#21463;&#30410;&#20110;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Punctuation restoration is a crucial step after Automatic Speech Recognition (ASR) systems to enhance transcript readability and facilitate subsequent NLP tasks. Nevertheless, conventional lexical-based approaches are inadequate for solving the punctuation restoration task in Spanish, where ambiguity can be often found between unpunctuated declaratives and questions. In this study, we propose a novel hybrid acoustic-lexical punctuation restoration system for Spanish transcription, which consolidates acoustic and lexical signals through a modular process. Our experiment results show that the proposed system can effectively improve F1 score of question marks and overall punctuation restoration on both public and internal Spanish conversational datasets. Additionally, benchmark comparison against LLMs (Large Language Model) indicates the superiority of our approach in accuracy, reliability and latency. Furthermore, we demonstrate that the Word Error Rate (WER) of the ASR module also benef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36328;&#39046;&#22495;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#31561;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#29983;&#25104;&#25688;&#35201;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.03509</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#35780;&#20272;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Factuality of Zero-shot Summarizers Across Varied Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36328;&#39046;&#22495;&#35780;&#20272;&#20102;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#30495;&#23454;&#24615;&#12290;&#36890;&#36807;&#23545;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#31561;&#19987;&#19994;&#39046;&#22495;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#29983;&#25104;&#25688;&#35201;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#23545;&#25688;&#35201;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#38646;&#26679;&#26412;&#65288;&#21363;&#22312;&#27809;&#26377;&#26126;&#30830;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65289;&#29983;&#25104;&#25688;&#35201;&#65292;&#32463;&#36807;&#20154;&#24037;&#35780;&#20272;&#65292;&#36825;&#20123;&#25688;&#35201;&#24448;&#24448;&#19982;&#25163;&#24037;&#32534;&#20889;&#30340;&#21442;&#32771;&#25688;&#35201;&#30456;&#27604;&#65292;&#29978;&#33267;&#26356;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26089;&#26399;&#30340;&#30740;&#31350;&#20960;&#20046;&#19987;&#27880;&#20110;&#35780;&#20272;&#26032;&#38395;&#25991;&#31456;&#30340;&#25688;&#35201;&#12290;&#38646;&#26679;&#26412;&#25688;&#35201;&#29983;&#25104;&#22120;&#22312;&#20854;&#20182;&#65288;&#21487;&#33021;&#26356;&#19987;&#19994;&#65289;&#39046;&#22495;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#36328;&#19987;&#19994;&#39046;&#22495;&#20013;&#38646;&#26679;&#26412;&#29983;&#25104;&#30340;&#25688;&#35201;&#65292;&#21253;&#25324;&#29983;&#29289;&#21307;&#23398;&#25991;&#31456;&#21644;&#27861;&#24459;&#27861;&#26696;&#65288;&#38500;&#20102;&#26631;&#20934;&#26032;&#38395;&#25688;&#35201;&#30340;&#21442;&#32771;&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#36755;&#20986;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#20174;&#39046;&#22495;&#19987;&#23478;&#22788;&#33719;&#21462;&#27880;&#37322;&#65292;&#20197;&#35782;&#21035;&#25688;&#35201;&#20013;&#30340;&#19981;&#19968;&#33268;&#20043;&#22788;&#65292;&#24182;&#23545;&#36825;&#20123;&#38169;&#35823;&#36827;&#34892;&#31995;&#32479;&#20998;&#31867;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#32473;&#23450;&#39046;&#22495;&#30340;&#26222;&#36941;&#24615;&#26159;&#21542;&#20250;&#24433;&#21709;&#22312;&#35813;&#39046;&#22495;&#30340;&#25991;&#31456;&#30340;&#25688;&#35201;&#30340;&#25552;&#21462;&#21644;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#25152;&#26377;&#25910;&#38598;&#21040;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that large language models (LLMs) are capable of generating summaries zero-shot (i.e., without explicit supervision) that, under human assessment, are often comparable or even preferred to manually composed reference summaries. However, this prior work has focussed almost exclusively on evaluating news article summarization. How do zero-shot summarizers perform in other (potentially more specialized) domains? In this work we evaluate zero-shot generated summaries across specialized domains including biomedical articles, and legal bills (in addition to standard news benchmarks for reference). We focus especially on the factuality of outputs. We acquire annotations from domain experts to identify inconsistencies in summaries and systematically categorize these errors. We analyze whether the prevalence of a given domain in the pretraining corpus affects extractiveness and faithfulness of generated summaries of articles in this domain. We release all collected annotat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#20915;&#25277;&#35937;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#35797;&#22270;&#25552;&#39640;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03507</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25277;&#35937;&#21644;&#25512;&#29702;&#65306;&#36808;&#21521;&#26426;&#22120;&#30340;&#24191;&#27867;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Neural networks for abstraction and reasoning: Towards broad generalization in machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03507
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25506;&#32034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#30740;&#31350;&#35299;&#20915;&#25277;&#35937;&#21644;&#25512;&#29702;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#65292;&#35797;&#22270;&#25552;&#39640;&#35745;&#31639;&#26426;&#31995;&#32479;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#20010;&#19990;&#32426;&#20197;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#19968;&#30452;&#35797;&#22270;&#22797;&#21046;&#20154;&#31867;&#30340;&#25277;&#35937;&#21644;&#25512;&#29702;&#33021;&#21147;-&#21019;&#36896;&#20986;&#33021;&#22815;&#20174;&#19968;&#23567;&#32452;&#31034;&#20363;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#65292;&#22312;&#20154;&#20204;&#21457;&#29616;&#36825;&#24456;&#23481;&#26131;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#29305;&#23450;&#30340;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#38382;&#39064;&#65292;&#20294;&#23545;&#20110;&#36229;&#36234;&#35757;&#32451;&#25968;&#25454;&#33539;&#22260;&#30340;&#24191;&#27867;&#27867;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#38590;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#35299;&#20915;&#25277;&#35937;&#19982;&#25512;&#29702;&#35821;&#26009;&#24211;&#65288;ARC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;ARC&#26159;&#19968;&#20010;&#29992;&#20110;&#27979;&#35797;&#31639;&#27861;&#22312;&#24191;&#27867;&#27867;&#21270;&#26041;&#38754;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#26377;&#19977;&#20010;&#22269;&#38469;&#31454;&#36187;&#25552;&#20379;&#20102;10&#19975;&#32654;&#20803;&#30340;&#22870;&#37329;&#65292;&#26368;&#22909;&#30340;&#31639;&#27861;&#20173;&#28982;&#26080;&#27861;&#35299;&#20915;&#22823;&#22810;&#25968;ARC&#20219;&#21153;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#25163;&#24037;&#35268;&#21017;&#65292;&#27809;&#26377;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#12290;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#26368;&#36817;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#26159;&#21542;&#33021;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#21462;&#24471;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;DreamCoder&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#27714;&#35299;&#22120;&#24212;&#29992;&#21040;ARC&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning - creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive.In this work, we look at several novel approaches for solving the Abstraction &amp; Reasoning Corpus (ARC), a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with $100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks and rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task.   First, we adapt the DreamCoder neurosymbolic reasoning solver to ARC. DreamCoder
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#22797;&#21644;&#26367;&#25442;&#30340;&#26381;&#35013;&#19982;&#32972;&#26223;&#29983;&#25104;&#20851;&#38190;&#25216;&#26415;&#65292;&#21033;&#29992;GenAI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#20272;&#35745;&#12289;&#20462;&#22797;&#25513;&#27169;&#21019;&#24314;&#21644;&#31283;&#23450;&#25193;&#25955;&#21644;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#29031;&#29255;&#20013;&#26381;&#35013;&#21644;&#32972;&#26223;&#30340;&#20462;&#25913;&#21644;&#20462;&#22797;&#12290;</title><link>https://arxiv.org/abs/2402.03501</link><description>&lt;p&gt;
&#22522;&#20110;&#20462;&#22797;&#21644;&#26367;&#25442;&#30340;&#26381;&#35013;&#19982;&#32972;&#26223;&#29983;&#25104;&#20851;&#38190;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
An Inpainting-Infused Pipeline for Attire and Background Replacement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20462;&#22797;&#21644;&#26367;&#25442;&#30340;&#26381;&#35013;&#19982;&#32972;&#26223;&#29983;&#25104;&#20851;&#38190;&#25216;&#26415;&#65292;&#21033;&#29992;GenAI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#65292;&#36890;&#36807;&#28145;&#24230;&#20272;&#35745;&#12289;&#20462;&#22797;&#25513;&#27169;&#21019;&#24314;&#21644;&#31283;&#23450;&#25193;&#25955;&#21644;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#65292;&#23454;&#29616;&#20102;&#23545;&#20010;&#20154;&#29031;&#29255;&#20013;&#26381;&#35013;&#21644;&#32972;&#26223;&#30340;&#20462;&#25913;&#21644;&#20462;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#30340;&#31361;&#30772;&#24615;&#36827;&#23637;&#24341;&#21457;&#20102;&#19968;&#22330;&#21464;&#38761;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#23545;&#21508;&#20010;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#12290;&#26412;&#25991;&#20855;&#20307;&#25506;&#35752;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;GenAI&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20808;&#36827;&#25216;&#26415;&#26469;&#36827;&#34892;&#22270;&#20687;&#22788;&#29702;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#28145;&#24230;&#20272;&#35745;&#12289;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21019;&#24314;&#20462;&#22797;&#25513;&#27169;&#12289;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#21644;&#28508;&#22312;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;LCMs&#65289;&#29983;&#25104;&#21644;&#26367;&#25442;&#32972;&#26223;&#65292;&#36890;&#36807;&#20462;&#22797;&#27969;&#31243;&#26367;&#25442;&#26381;&#35013;&#21644;&#24212;&#29992;&#23457;&#32654;&#20462;&#25913;&#12290;&#26412;&#30740;&#31350;&#20013;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#20986;&#20102;&#20854;&#20135;&#29983;&#35270;&#35273;&#21560;&#24341;&#21147;&#20869;&#23481;&#30340;&#28508;&#21147;&#12290;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#30340;&#34701;&#21512;&#20351;&#29992;&#25143;&#33021;&#22815;&#36755;&#20837;&#20010;&#20154;&#29031;&#29255;&#24182;&#23545;&#20854;&#36827;&#34892;&#26381;&#35013;&#21644;&#32972;&#26223;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, groundbreaking advancements in Generative Artificial Intelligence (GenAI) have triggered a transformative paradigm shift, significantly influencing various domains. In this work, we specifically explore an integrated approach, leveraging advanced techniques in GenAI and computer vision emphasizing image manipulation. The methodology unfolds through several stages, including depth estimation, the creation of inpaint masks based on depth information, the generation and replacement of backgrounds utilizing Stable Diffusion in conjunction with Latent Consistency Models (LCMs), and the subsequent replacement of clothes and application of aesthetic changes through an inpainting pipeline. Experiments conducted in this study underscore the methodology's efficacy, highlighting its potential to produce visually captivating content. The convergence of these advanced techniques allows users to input photographs of individuals and manipulate them to modify clothing and background b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;</title><link>https://arxiv.org/abs/2402.03485</link><description>&lt;p&gt;
&#27880;&#24847;&#21147;&#19982;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#30456;&#36935;&#65306;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Attention Meets Post-hoc Interpretability: A Mathematical Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#27604;&#36739;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#23613;&#31649;&#26377;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26426;&#21046;&#22522;&#20110;transformer&#31561;&#26550;&#26500;&#65292;&#25104;&#20026;&#20102;&#25216;&#26415;&#38761;&#21629;&#30340;&#26680;&#24515;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#38500;&#20102;&#24110;&#21161;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#20043;&#22806;&#65292;&#27880;&#24847;&#21147;&#26426;&#21046;&#26412;&#36523;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#20869;&#37096;&#34892;&#20026;&#30340;&#26377;&#24847;&#20041;&#27934;&#23519;&#12290;&#36825;&#20123;&#27934;&#23519;&#26159;&#21542;&#21487;&#20197;&#29992;&#20316;&#35299;&#37322;&#65311;&#20851;&#20110;&#27492;&#20105;&#35770;&#19981;&#26029;&#12290;&#26412;&#25991;&#36890;&#36807;&#25968;&#23398;&#26041;&#24335;&#30740;&#31350;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26550;&#26500;&#65292;&#24182;&#20934;&#30830;&#23450;&#20301;&#20102;&#20107;&#21518;&#35299;&#37322;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#25105;&#20204;&#34920;&#26126;&#23427;&#20204;&#25552;&#20379;&#20102;&#30456;&#24403;&#19981;&#21516;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#23613;&#31649;&#26377;&#20854;&#23616;&#38480;&#24615;&#65292;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#33021;&#22815;&#25429;&#33719;&#27604;&#20165;&#20165;&#26816;&#26597;&#27880;&#24847;&#21147;&#26435;&#37325;&#26356;&#26377;&#29992;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based architectures, in particular transformers, are at the heart of a technological revolution. Interestingly, in addition to helping obtain state-of-the-art results on a wide range of applications, the attention mechanism intrinsically provides meaningful insights on the internal behavior of the model. Can these insights be used as explanations? Debate rages on. In this paper, we mathematically study a simple attention-based architecture and pinpoint the differences between post-hoc and attention-based explanations. We show that they provide quite different results, and that, despite their limitations, post-hoc methods are capable of capturing more useful insights than merely examining the attention weights.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;PubMed&#29992;&#25143;&#26597;&#35810;&#26085;&#24535;&#26500;&#24314;&#20102;PubCLogs&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#20107;&#21518;&#26041;&#27861;&#35299;&#37322;&#25512;&#33616;&#30340;&#30456;&#20851;&#25991;&#31456;&#65292;&#36890;&#36807;&#35782;&#21035;&#31867;&#20284;&#25991;&#31456;&#26631;&#39064;&#20013;&#30340;&#30456;&#20851;&#35789;&#27719;&#26469;&#25552;&#20379;&#35299;&#37322;&#12290;&#36825;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#22312;&#25991;&#29486;&#25628;&#32034;&#20013;&#26356;&#26041;&#20415;&#22320;&#23547;&#25214;&#30456;&#20851;&#25991;&#31456;&#12290;</title><link>https://arxiv.org/abs/2402.03484</link><description>&lt;p&gt;
&#21033;&#29992;PubMed&#29992;&#25143;&#26597;&#35810;&#26085;&#24535;&#35299;&#37322;&#25512;&#33616;&#30340;&#30456;&#20851;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
Harnessing PubMed User Query Logs for Post Hoc Explanations of Recommended Similar Articles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;PubMed&#29992;&#25143;&#26597;&#35810;&#26085;&#24535;&#26500;&#24314;&#20102;PubCLogs&#25968;&#25454;&#38598;&#65292;&#24182;&#37319;&#29992;&#20107;&#21518;&#26041;&#27861;&#35299;&#37322;&#25512;&#33616;&#30340;&#30456;&#20851;&#25991;&#31456;&#65292;&#36890;&#36807;&#35782;&#21035;&#31867;&#20284;&#25991;&#31456;&#26631;&#39064;&#20013;&#30340;&#30456;&#20851;&#35789;&#27719;&#26469;&#25552;&#20379;&#35299;&#37322;&#12290;&#36825;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#22312;&#25991;&#29486;&#25628;&#32034;&#20013;&#26356;&#26041;&#20415;&#22320;&#23547;&#25214;&#30456;&#20851;&#25991;&#31456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#24341;&#29992;&#25991;&#31456;&#23547;&#25214;&#30456;&#20851;&#25991;&#31456;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#37096;&#20998;&#12290;&#20687;&#35768;&#22810;&#23398;&#26415;&#25628;&#32034;&#24341;&#25806;&#19968;&#26679;&#65292;PubMed&#25317;&#26377;&#19968;&#20010;&#8220;&#31867;&#20284;&#25991;&#31456;&#8221;&#30340;&#21151;&#33021;&#65292;&#21487;&#20197;&#25512;&#33616;&#19982;&#29992;&#25143;&#26597;&#30475;&#30340;&#24403;&#21069;&#25991;&#31456;&#30456;&#20851;&#30340;&#25991;&#31456;&#12290;&#35299;&#37322;&#25512;&#33616;&#30340;&#25991;&#31456;&#23545;&#29992;&#25143;&#26469;&#35828;&#38750;&#24120;&#26377;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#25991;&#29486;&#25628;&#32034;&#36807;&#31243;&#20013;&#12290;&#37492;&#20110;&#27599;&#24180;&#26377;&#36229;&#36807;&#19968;&#30334;&#19975;&#31687;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#21457;&#34920;&#65292;&#35299;&#37322;&#25512;&#33616;&#30340;&#30456;&#20851;&#25991;&#31456;&#23558;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20020;&#24202;&#21307;&#29983;&#22312;&#23547;&#25214;&#30456;&#20851;&#25991;&#31456;&#26102;&#25552;&#20379;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22823;&#22810;&#25968;&#25991;&#29486;&#25512;&#33616;&#31995;&#32479;&#37117;&#32570;&#20047;&#23545;&#20854;&#24314;&#35758;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#37319;&#29992;&#20107;&#21518;&#26041;&#27861;&#26469;&#35299;&#37322;&#25512;&#33616;&#65292;&#36890;&#36807;&#35782;&#21035;&#31867;&#20284;&#25991;&#31456;&#26631;&#39064;&#20013;&#30340;&#30456;&#20851;&#35789;&#27719;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;PubMed&#29992;&#25143;&#26597;&#35810;&#26085;&#24535;&#20013;&#30340;560&#19975;&#20010;&#20849;&#28857;&#20987;&#25991;&#31456;&#23545;&#26500;&#24314;PubCLogs&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;PubCLogs&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;"Highlight Similar Article Title"&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Searching for a related article based on a reference article is an integral part of scientific research. PubMed, like many academic search engines, has a "similar articles" feature that recommends articles relevant to the current article viewed by a user. Explaining recommended items can be of great utility to users, particularly in the literature search process. With more than a million biomedical papers being published each year, explaining the recommended similar articles would facilitate researchers and clinicians in searching for related articles. Nonetheless, the majority of current literature recommendation systems lack explanations for their suggestions. We employ a post hoc approach to explaining recommendations by identifying relevant tokens in the titles of similar articles. Our major contribution is building PubCLogs by repurposing 5.6 million pairs of coclicked articles from PubMed's user query logs. Using our PubCLogs dataset, we train the Highlight Similar Article Title 
&lt;/p&gt;</description></item><item><title>SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;</title><link>https://arxiv.org/abs/2402.03483</link><description>&lt;p&gt;
SWAG: &#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;
&lt;/p&gt;
&lt;p&gt;
SWAG: Storytelling With Action Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03483
&lt;/p&gt;
&lt;p&gt;
SWAG&#26159;&#19968;&#31181;&#26032;&#30340;&#25925;&#20107;&#35762;&#36848;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#25628;&#32034;&#38382;&#39064;&#65292;&#20351;&#29992;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#26469;&#25351;&#23548;&#25925;&#20107;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;&#22312;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#65292;SWAG&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#20351;&#29992;&#20165;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36807;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#38271;&#31687;&#25925;&#20107;&#29983;&#25104;&#36890;&#24120;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19968;&#27425;&#24615;&#21019;&#24314;&#65292;&#23427;&#21487;&#20197;&#20135;&#29983;&#36830;&#36143;&#20294;&#19981;&#19968;&#23450;&#24341;&#20154;&#20837;&#32988;&#30340;&#20869;&#23481;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24102;&#26377;&#34892;&#21160;&#25351;&#23548;&#30340;&#25925;&#20107;&#35762;&#36848;&#65288;SWAG&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27169;&#22411;&#30340;&#21453;&#39304;&#24490;&#29615;&#23558;&#25925;&#20107;&#20889;&#20316;&#31616;&#21270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65306;&#19968;&#20010;LLM&#29983;&#25104;&#25925;&#20107;&#20869;&#23481;&#65292;&#21478;&#19968;&#20010;&#36741;&#21161;LLM&#29992;&#20110;&#36873;&#25321;&#19979;&#19968;&#20010;&#26368;&#20339;&#30340;&#8220;&#34892;&#21160;&#8221;&#65292;&#20197;&#24341;&#23548;&#25925;&#20107;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20351;&#29992;GPT-4&#21644;&#20154;&#24037;&#35780;&#20272;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;SWAG&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20197;&#24448;&#30340;&#31471;&#21040;&#31471;&#25925;&#20107;&#29983;&#25104;&#25216;&#26415;&#65292;&#24182;&#19988;&#25105;&#20204;&#21482;&#20351;&#29992;&#24320;&#28304;&#27169;&#22411;&#30340;SWAG&#27969;&#31243;&#36229;&#36234;&#20102;GPT-3.5-Turbo&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated long-form story generation typically employs long-context large language models (LLMs) for one-shot creation, which can produce cohesive but not necessarily engaging content. We introduce Storytelling With Action Guidance (SWAG), a novel approach to storytelling with LLMs. Our approach reduces story writing to a search problem through a two-model feedback loop: one LLM generates story content, and another auxiliary LLM is used to choose the next best "action" to steer the story's future direction. Our results show that SWAG can substantially outperform previous end-to-end story generation techniques when evaluated by GPT-4 and through human evaluation, and our SWAG pipeline using only open-source models surpasses GPT-3.5-Turbo.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#22312;&#38463;&#25289;&#20271;&#25991;&#20013;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#35789;&#32423;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#21516;&#20041;&#35789;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#33945;&#29256;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#36890;&#36807;&#35780;&#20272;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#19982;&#21407;&#26679;&#26412;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03477</link><description>&lt;p&gt;
&#22522;&#20110;&#38463;&#25289;&#20271;&#25991;&#21516;&#20041;&#35789;BERT&#30340;&#25991;&#26412;&#20998;&#31867;&#23545;&#25239;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
Arabic Synonym BERT-based Adversarial Examples for Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#22312;&#38463;&#25289;&#20271;&#25991;&#20013;&#23545;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#36827;&#34892;&#20102;&#23545;&#25239;&#25915;&#20987;&#30340;&#35789;&#32423;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#21516;&#20041;&#35789;&#30340;BERT&#27169;&#22411;&#36827;&#34892;&#33945;&#29256;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#12290;&#36890;&#36807;&#35780;&#20272;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#19982;&#21407;&#26679;&#26412;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#31995;&#32479;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#23545;&#25239;&#24615;&#25991;&#26412;&#31034;&#20363;&#20855;&#26377;&#33030;&#24369;&#24615;&#65292;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#25991;&#26412;&#31034;&#20363;&#22312;&#20154;&#30524;&#20013;&#24448;&#24448;&#19981;&#26131;&#23519;&#35273;&#65292;&#20294;&#21487;&#20197;&#36843;&#20351;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#25913;&#21464;&#20854;&#20998;&#31867;&#32467;&#26524;&#12290;&#30446;&#21069;&#65292;&#23545;&#20110;&#37327;&#21270;&#23545;&#25239;&#24615;&#25991;&#26412;&#25915;&#20987;&#24433;&#21709;&#30340;&#30740;&#31350;&#24037;&#20316;&#22823;&#22810;&#21482;&#24212;&#29992;&#20110;&#33521;&#25991;&#27169;&#22411;&#12290;&#26412;&#25991;&#39318;&#27425;&#20171;&#32461;&#20102;&#38463;&#25289;&#20271;&#25991;&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#35789;&#32423;&#30740;&#31350;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;BERT&#27169;&#22411;&#30340;&#33945;&#29256;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#20219;&#21153;&#30340;&#21516;&#20041;&#35789;&#65288;&#35789;&#32423;&#65289;&#25915;&#20987;&#26041;&#27861;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#20998;&#31867;&#27169;&#22411;&#23545;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#20102;&#35780;&#20272;&#20351;&#29992;&#25105;&#20204;&#30340;&#21516;&#20041;&#35789;BERT&#25915;&#20987;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#35831;&#22235;&#21517;&#20154;&#31867;&#35780;&#20272;&#21592;&#35780;&#20272;&#21644;&#27604;&#36739;&#29983;&#25104;&#30340;&#23545;&#25239;&#26679;&#26412;&#19982;&#21407;&#26679;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#36825;&#20123;&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification systems have been proven vulnerable to adversarial text examples, modified versions of the original text examples that are often unnoticed by human eyes, yet can force text classification models to alter their classification. Often, research works quantifying the impact of adversarial text attacks have been applied only to models trained in English. In this paper, we introduce the first word-level study of adversarial attacks in Arabic. Specifically, we use a synonym (word-level) attack using a Masked Language Modeling (MLM) task with a BERT model in a black-box setting to assess the robustness of the state-of-the-art text classification models to adversarial attacks in Arabic. To evaluate the grammatical and semantic similarities of the newly produced adversarial examples using our synonym BERT-based attack, we invite four human evaluators to assess and compare the produced adversarial examples with their original examples. We also study the transferability of thes
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#29109;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#12290;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#22312;&#26631;&#35760;&#20043;&#38388;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.03471</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20960;&#20309;&#20449;&#24687;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Information of Large Language Model Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03471
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#65292;&#24182;&#21457;&#29616;&#34920;&#31034;&#29109;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#12290;&#36890;&#36807;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#65292;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#65292;&#24182;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20449;&#24687;&#22312;&#26631;&#35760;&#20043;&#38388;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#20449;&#24687;&#32534;&#30721;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#34920;&#31034;&#29109;&#65292;&#24182;&#21457;&#29616;&#20102;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#24130;&#24459;&#20851;&#31995;&#30340;&#29616;&#35937;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#65288;&#26465;&#20214;&#65289;&#29109;&#30340;&#29702;&#35770;&#26469;&#35299;&#37322;&#36825;&#31181;&#35268;&#27169;&#23450;&#24459;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;LLMs&#30340;&#33258;&#22238;&#24402;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#21644;&#22238;&#24402;&#25216;&#26415;&#26469;&#20998;&#26512;&#26368;&#21518;&#19968;&#20010;&#26631;&#35760;&#19982;&#20043;&#21069;&#19978;&#19979;&#25991;&#26631;&#35760;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#26032;&#26631;&#35760;&#30340;&#20449;&#24687;&#22686;&#30410;&#19982;&#23725;&#22238;&#24402;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;Lasso&#22238;&#24402;&#22312;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#26631;&#35760;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#26377;&#26102;&#34920;&#29616;&#20248;&#20110;&#32039;&#23494;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23545;&#27604;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#20449;&#24687;&#20998;&#24067;&#22312;&#26631;&#35760;&#20043;&#38388;&#65292;&#32780;&#19981;&#20165;&#20165;&#38598;&#20013;&#22312;&#29305;&#23450;&#30340;&#8220;&#26377;&#24847;&#20041;&#8221;&#30340;&#26631;&#35760;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the information encoded in the embeddings of large language models (LLMs). We conduct simulations to analyze the representation entropy and discover a power law relationship with model sizes. Building upon this observation, we propose a theory based on (conditional) entropy to elucidate the scaling law phenomenon. Furthermore, we delve into the auto-regressive structure of LLMs and examine the relationship between the last token and previous context tokens using information theory and regression techniques. Specifically, we establish a theoretical connection between the information gain of new tokens and ridge regression. Additionally, we explore the effectiveness of Lasso regression in selecting meaningful tokens, which sometimes outperforms the closely related attention weights. Finally, we conduct controlled experiments, and find that information is distributed across tokens, rather than being concentrated in specific "meaningful" tokens alone.
&lt;/p&gt;</description></item><item><title>&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03469</link><description>&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#19982;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Preference-free Alignment Learning with Regularized Relevance Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03469
&lt;/p&gt;
&lt;p&gt;
&#26080;&#20559;&#22909;&#23545;&#40784;&#23398;&#20064;&#20351;&#29992;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#20316;&#20026;&#20851;&#38190;&#30446;&#26631;&#65292;&#22312;&#25552;&#20379;&#31283;&#20581;&#22870;&#21169;&#20449;&#21495;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#34987;&#35748;&#20026;&#26159;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#19982;&#26222;&#36941;&#30340;&#35266;&#28857;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#21021;&#27493;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20542;&#21521;&#20110;&#32473;&#38271;&#30340;&#19982;&#20027;&#39064;&#26080;&#20851;&#30340;&#22238;&#22797;&#26356;&#39640;&#30340;&#20998;&#25968;&#65292;&#32780;&#32473;&#30701;&#30340;&#19982;&#20027;&#39064;&#30456;&#20851;&#30340;&#22238;&#22797;&#36739;&#20302;&#20998;&#12290;&#22312;&#36825;&#19968;&#35266;&#23519;&#30340;&#39537;&#21160;&#19979;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#8220;&#30456;&#20851;&#24615;&#8221;&#20316;&#20026;&#23545;&#40784;&#30340;&#19968;&#20010;&#20851;&#38190;&#30446;&#26631;&#12290;&#22312;&#25105;&#20204;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#26816;&#32034;&#24471;&#21040;&#30340;&#30456;&#20851;&#24615;&#24471;&#20998;&#23481;&#26131;&#21463;&#21040;&#22870;&#21169;&#27450;&#39575;&#30340;&#24433;&#21709;&#65292;&#21363;&#36807;&#24230;&#20248;&#21270;&#21040;&#19981;&#26399;&#26395;&#30340;&#25463;&#24452;&#19978;&#65292;&#24403;&#25105;&#20204;&#23558;&#35813;&#24471;&#20998;&#20316;&#20026;&#22870;&#21169;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26377;&#25928;&#30340;&#24402;&#32435;&#20559;&#24046;&#25972;&#21512;&#21040;&#24120;&#35268;&#30340;&#30456;&#20851;&#24615;&#20013;&#65292;&#20114;&#30456;&#27491;&#21017;&#21270;&#65292;&#24418;&#25104;&#20102;&#19968;&#31181;&#28151;&#21512;&#22870;&#21169;&#20989;&#25968;&#65306;&#27491;&#21017;&#21270;&#30456;&#20851;&#22870;&#21169;&#65288;$R^3$&#65289;&#12290;$R^3$&#36890;&#36807;&#25552;&#20379;&#31283;&#20581;&#30340;&#22870;&#21169;&#20449;&#21495;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#20559;&#22909;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;$R^3$&#19981;&#38656;&#35201;
&lt;/p&gt;
&lt;p&gt;
Learning from human preference has been considered key to aligning Large Language Models (LLMs) with human values. However, contrary to popular belief, our preliminary study reveals that reward models trained on human preference datasets tend to give higher scores to long off-topic responses than short on-topic ones. Motivated by this observation, we explore a preference-free approach utilizing `relevance' as a key objective for alignment. On our first attempt, we find that the relevance score obtained by a retriever alone is vulnerable to reward hacking, i.e., overoptimizing to undesired shortcuts, when we utilize the score as a reward for reinforcement learning. To mitigate it, we integrate effective inductive biases into the vanilla relevance to regularize each other, resulting in a mixture of reward functions: Regularized Relevance Reward ($R^3$). $R^3$ significantly improves performance on preference benchmarks by providing a robust reward signal. Notably, $R^3$ does not require a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#25991;&#26412;&#35780;&#35770;&#65292;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#25104;&#26412;&#25928;&#30410;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#35821;&#27861;&#26469;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#33258;&#26432;&#39118;&#38505;&#24515;&#29702;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03435</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#35780;&#20272;&#65306;&#27880;&#37325;&#38544;&#31169;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Psychological Assessments with Large Language Models: A Privacy-Focused and Cost-Effective Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#25991;&#26412;&#35780;&#35770;&#65292;&#20197;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#21644;&#25104;&#26412;&#25928;&#30410;&#20026;&#37325;&#28857;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#35821;&#27861;&#26469;&#23454;&#29616;&#39044;&#23450;&#20041;&#30340;&#33258;&#26432;&#39118;&#38505;&#24515;&#29702;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#26480;&#20986;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20998;&#26512;Reddit&#29992;&#25143;&#30340;&#25991;&#26412;&#35780;&#35770;&#65292;&#20027;&#35201;&#30446;&#26631;&#26377;&#20004;&#20010;&#65306;&#31532;&#19968;&#65292;&#30830;&#23450;&#25903;&#25345;&#39044;&#23450;&#20041;&#30340;&#33258;&#26432;&#39118;&#38505;&#24515;&#29702;&#35780;&#20272;&#30340;&#20851;&#38190;&#25688;&#24405;&#65307;&#31532;&#20108;&#65292;&#24635;&#32467;&#26448;&#26009;&#20197;&#35777;&#23454;&#39044;&#20998;&#37197;&#30340;&#33258;&#26432;&#39118;&#38505;&#27700;&#24179;&#12290;&#35813;&#24037;&#20316;&#23616;&#38480;&#20110;&#20351;&#29992;&#21487;&#20197;&#22312;&#26412;&#22320;&#36816;&#34892;&#30340;&#8220;&#24320;&#28304;&#8221;LLMs&#65292;&#20174;&#32780;&#22686;&#24378;&#25968;&#25454;&#38544;&#31169;&#12290;&#27492;&#22806;&#65292;&#23427;&#20248;&#20808;&#36873;&#29992;&#35745;&#31639;&#35201;&#27714;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#20197;&#20351;&#26377;&#38480;&#30340;&#35745;&#31639;&#39044;&#31639;&#30340;&#20010;&#20154;&#21644;&#26426;&#26500;&#33021;&#22815;&#20351;&#29992;&#12290;&#23454;&#26045;&#30340;&#31574;&#30053;&#20165;&#20381;&#36182;&#20110;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#21644;&#35821;&#27861;&#65292;&#20197;&#25351;&#23548;LLM&#30340;&#25991;&#26412;&#23436;&#25104;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#35780;&#20272;&#25351;&#26631;&#26174;&#31034;&#20986;&#26480;&#20986;&#30340;&#32467;&#26524;&#65292;&#20351;&#20043;&#25104;&#20026;&#19968;&#31181;&#26377;&#20215;&#20540;&#30340;&#27880;&#37325;&#38544;&#31169;&#21644;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#27861;&#12290;&#27492;&#24037;&#20316;&#26159;2024 Computational Linguistics and Clinical Psychology (CLPsych)&#20849;&#20139;&#20219;&#21153;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the use of Large Language Models (LLMs) to analyze text comments from Reddit users, aiming to achieve two primary objectives: firstly, to pinpoint critical excerpts that support a predefined psychological assessment of suicidal risk; and secondly, to summarize the material to substantiate the preassigned suicidal risk level. The work is circumscribed to the use of "open-source" LLMs that can be run locally, thereby enhancing data privacy. Furthermore, it prioritizes models with low computational requirements, making it accessible to both individuals and institutions operating on limited computing budgets. The implemented strategy only relies on a carefully crafted prompt and a grammar to guide the LLM's text completion. Despite its simplicity, the evaluation metrics show outstanding results, making it a valuable privacy-focused and cost-effective approach. This work is part of the Computational Linguistics and Clinical Psychology (CLPsych) 2024 shared task.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21161;&#36716;&#25442;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#22686;&#24378;LLM&#22522;&#30784;&#30340;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#25512;&#29702;&#26102;&#20986;&#29616;&#30340;&#22810;&#20010;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;LLM&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#20165;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#32780;&#35828;&#35805;&#32773;&#36523;&#20221;&#30001;&#21478;&#19968;&#20010;&#27169;&#22411;&#25552;&#20379;&#12290;</title><link>https://arxiv.org/abs/2402.03407</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#22686;&#24378;LLM&#22522;&#30784;&#30340;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03407
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#30417;&#30563;&#34920;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#21161;&#36716;&#25442;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#21487;&#20197;&#22686;&#24378;LLM&#22522;&#30784;&#30340;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#20811;&#26381;&#20102;&#22312;&#25512;&#29702;&#26102;&#20986;&#29616;&#30340;&#22810;&#20010;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;LLM&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#20165;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#32780;&#35828;&#35805;&#32773;&#36523;&#20221;&#30001;&#21478;&#19968;&#20010;&#27169;&#22411;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#19979;&#19968;&#20195;&#35821;&#38899;&#29983;&#25104;&#31995;&#32479;&#26368;&#26377;&#21069;&#26223;&#30340;&#25216;&#26415;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#21487;&#25193;&#23637;&#24615;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#29702;&#26102;&#65292;&#23427;&#20204;&#36935;&#21040;&#20102;&#22810;&#20010;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#22914;&#24187;&#35273;&#12289;&#36339;&#36807;&#20869;&#23481;&#25110;&#35821;&#38899;&#37325;&#22797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#35821;&#38899;&#36716;&#25442;&#65288;VC&#65289;&#26550;&#26500;&#65292;&#21487;&#20197;&#29992;&#20110;&#23398;&#20064;&#23558;&#30636;&#24577;&#29305;&#24449;&#65288;&#22914;&#20869;&#23481;&#65289;&#19982;&#22266;&#23450;&#29305;&#24449;&#65288;&#22914;&#35828;&#35805;&#32773;ID&#25110;&#24405;&#21046;&#26465;&#20214;&#65289;&#20998;&#24320;&#32534;&#30721;&#65292;&#21019;&#24314;&#35828;&#35805;&#32773;&#35299;&#32806;&#34920;&#31034;&#12290;&#20351;&#29992;&#35828;&#35805;&#32773;&#35299;&#32806;&#32534;&#30721;&#26469;&#35757;&#32451;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#20801;&#35768;LLM&#20165;&#36890;&#36807;&#25991;&#26412;&#29983;&#25104;&#35821;&#38899;&#30340;&#20869;&#23481;&#21644;&#39118;&#26684;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#65292;&#32780;&#35828;&#35805;&#32773;&#36523;&#20221;&#30001;VC&#27169;&#22411;&#30340;&#35299;&#30721;&#22120;&#25552;&#20379;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35757;&#32451;&#36807;&#35828;&#35805;&#32773;&#35299;&#32806;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#30340;LLMs&#22312;&#35828;&#35805;&#32773;&#30456;&#20284;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;4.7pp&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UniTSyn&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20851;&#32852;&#27979;&#35797;&#21644;&#34987;&#27979;&#35797;&#20989;&#25968;&#65292;UniTSyn&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.03396</link><description>&lt;p&gt;
UniTSyn&#65306;&#19968;&#20010;&#21487;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
UniTSyn: A Large-Scale Dataset Capable of Enhancing the Prowess of Large Language Models for Program Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;UniTSyn&#65292;&#19968;&#20010;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31243;&#24207;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20851;&#32852;&#27979;&#35797;&#21644;&#34987;&#27979;&#35797;&#20989;&#25968;&#65292;UniTSyn&#33021;&#22815;&#25552;&#39640;&#27169;&#22411;&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#20195;&#30721;&#26041;&#38754;&#30340;&#20986;&#33394;&#33021;&#21147;&#24050;&#32463;&#24341;&#36215;&#20102;&#36719;&#20214;&#27979;&#35797;&#31038;&#21306;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#29983;&#25104;&#20934;&#30830;&#21644;&#23436;&#25972;&#30340;&#27979;&#35797;&#26041;&#38754;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#19981;&#21306;&#20998;&#27979;&#35797;&#30446;&#30340;&#20195;&#30721;&#21644;&#20854;&#20182;&#20195;&#30721;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;UniTSyn&#65292;&#23427;&#33021;&#22815;&#22686;&#24378;LLM&#22312;&#21333;&#20803;&#27979;&#35797;&#21512;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#23558;&#27979;&#35797;&#19982;&#34987;&#27979;&#35797;&#20989;&#25968;&#36827;&#34892;&#20851;&#32852;&#23545;&#20110;LLM&#25512;&#26029;&#39044;&#26399;&#34892;&#20026;&#21644;&#35201;&#39564;&#35777;&#30340;&#36923;&#36753;&#36335;&#24452;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#26381;&#21153;&#22120;&#21327;&#35758;&#65292;UniTSyn&#23454;&#29616;&#20102;&#22312;&#27809;&#26377;&#27599;&#20010;&#39033;&#30446;&#25191;&#34892;&#35774;&#32622;&#25110;&#26131;&#30862;&#19988;&#38590;&#20197;&#25193;&#23637;&#30340;&#27599;&#20010;&#35821;&#35328;&#21551;&#21457;&#24335;&#30340;&#24773;&#20917;&#19979;&#25910;&#38598;&#28966;&#28857;&#27979;&#35797;&#23545;&#30340;&#25361;&#25112;&#30446;&#26631;&#12290;&#23427;&#21253;&#21547;&#20102;&#20116;&#31181;&#20027;&#27969;&#32534;&#31243;&#35821;&#35328;&#30340;270&#19975;&#20010;&#28966;&#28857;&#27979;&#35797;&#23545;&#65292;&#20351;&#20854;&#33021;&#22815;&#34987;&#24212;&#29992;&#20110;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable capability of large language models (LLMs) in generating high-quality code has drawn increasing attention in the software testing community. However, existing code LLMs often demonstrate unsatisfactory capabilities in generating accurate and complete tests since they were trained on code snippets collected without differentiating between code for testing purposes and other code. In this paper, we present a large-scale dataset UniTSyn, which is capable of enhancing the prowess of LLMs for Unit Test Synthesis. Associating tests with the tested functions is crucial for LLMs to infer the expected behavior and the logic paths to be verified. By leveraging Language Server Protocol, UniTSyn achieves the challenging goal of collecting focal-test pairs without per-project execution setups or per-language heuristics that tend to be fragile and difficult to scale. It contains 2.7 million focal-test pairs across five mainstream programming languages, making it possible to be utilize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#21160;&#26816;&#27979;&#31185;&#23398;&#35770;&#25991;&#20013;&#25305;&#21155;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#27979;&#20998;&#25968;&#30340;&#20256;&#25773;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26631;&#35760;&#24182;&#25552;&#21462;&#36825;&#20123;&#25305;&#21155;&#30701;&#35821;&#65292;&#20026;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.03370</link><description>&lt;p&gt;
&#26816;&#27979;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#25305;&#21155;&#30701;&#35821;
&lt;/p&gt;
&lt;p&gt;
Detection of tortured phrases in scientific literature
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#33258;&#21160;&#26816;&#27979;&#31185;&#23398;&#35770;&#25991;&#20013;&#25305;&#21155;&#30701;&#35821;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#21644;&#39044;&#27979;&#20998;&#25968;&#30340;&#20256;&#25773;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#26631;&#35760;&#24182;&#25552;&#21462;&#36825;&#20123;&#25305;&#21155;&#30701;&#35821;&#65292;&#20026;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#25552;&#20379;&#26032;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#31181;&#33258;&#21160;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#25152;&#35859;&#30340;&#25305;&#21155;&#30701;&#35821;&#12290;&#36825;&#20123;&#25305;&#21155;&#30701;&#35821;&#65292;&#20363;&#22914;&#23558;"&#20449;&#21495;&#19982;&#22122;&#22768;"&#26367;&#25442;&#20026;"&#26071;&#24092;&#19982;&#21927;&#38393;"&#65292;&#26159;&#20351;&#29992;&#25913;&#20889;&#24037;&#20855;&#35268;&#36991;&#25220;&#34989;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#31574;&#30053;&#26469;&#26631;&#35760;&#20197;&#21069;&#26410;&#35760;&#24405;&#30340;&#25305;&#21155;&#30701;&#35821;&#12290;&#25552;&#20986;&#21644;&#27979;&#35797;&#30340;&#26041;&#27861;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#35201;&#20040;&#22522;&#20110;&#23884;&#20837;&#30456;&#20284;&#24615;&#65292;&#35201;&#20040;&#22522;&#20110;&#25513;&#30721;&#26631;&#35760;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#31181;&#20351;&#29992;&#26631;&#35760;&#39044;&#27979;&#24182;&#23558;&#20998;&#25968;&#20256;&#25773;&#21040;&#22359;&#32423;&#21035;&#30340;&#26041;&#27861;&#25928;&#26524;&#26368;&#22909;&#12290;&#20854;&#21484;&#22238;&#29575;&#20026;0.87&#65292;&#31934;&#30830;&#29575;&#20026;0.61&#65292;&#21487;&#20197;&#26816;&#32034;&#21040;&#26032;&#30340;&#25305;&#21155;&#30701;&#35821;&#20197;&#20379;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents various automatic detection methods to extract so called tortured phrases from scientific papers. These tortured phrases, e.g. flag to clamor instead of signal to noise, are the results of paraphrasing tools used to escape plagiarism detection. We built a dataset and evaluated several strategies to flag previously undocumented tortured phrases. The proposed and tested methods are based on language models and either on embeddings similarities or on predictions of masked token. We found that an approach using token prediction and that propagates the scores to the chunk level gives the best results. With a recall value of .87 and a precision value of .61, it could retrieve new tortured phrases to be submitted to domain experts for validation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#31867;&#22312;&#21307;&#30103;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24212;&#29992;&#20110;&#22260;&#25163;&#26415;&#26399;&#26381;&#21153;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#27969;&#31243;&#21644;&#21307;&#30103;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#21518;&#22788;&#29702;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.03369</link><description>&lt;p&gt;
&#35780;&#20272;&#35895;&#27468;&#35821;&#38899;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#31867;&#22312;&#21307;&#30103;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Google's Voice Recognition and Sentence Classification for Health Care Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#31867;&#22312;&#21307;&#30103;&#20581;&#24247;&#24212;&#29992;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23558;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24212;&#29992;&#20110;&#22260;&#25163;&#26415;&#26399;&#26381;&#21153;&#65292;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#27969;&#31243;&#21644;&#21307;&#30103;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#21518;&#22788;&#29702;&#20998;&#31867;&#22120;&#65292;&#26412;&#30740;&#31350;&#22686;&#24378;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#24212;&#29992;&#20110;&#22260;&#25163;&#26415;&#26399;&#26381;&#21153;&#65288;Periop&#65289;&#65292;&#20197;&#20351;Periop&#21592;&#24037;&#33021;&#22815;&#20351;&#29992;&#31227;&#21160;&#25216;&#26415;&#35760;&#24405;&#24037;&#20316;&#27969;&#31243;&#37324;&#31243;&#30865;&#12290;&#22914;&#26524;&#33021;&#22815;&#20351;&#36825;&#31181;&#35821;&#38899;&#35782;&#21035;&#25216;&#26415;&#21464;&#24471;&#24378;&#22823;&#21487;&#38752;&#65292;&#23601;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#31227;&#21160;&#25216;&#26415;&#25913;&#21892;&#24739;&#32773;&#27969;&#31243;&#21644;&#21307;&#30103;&#36136;&#37327;&#12290;&#27492;&#23454;&#39564;&#30340;&#30446;&#26631;&#26159;&#20351;Periop&#21592;&#24037;&#33021;&#22815;&#25552;&#20379;&#26080;&#24178;&#25200;&#30340;&#25252;&#29702;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#24405;&#20837;&#21644;&#26597;&#35810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#35813;&#30740;&#31350;&#30340;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;&#20854;&#20182;&#24773;&#20917;&#65292;&#21363;&#24037;&#31243;&#32463;&#29702;&#23581;&#35797;&#20351;&#29992;&#31227;&#21160;&#25216;&#26415;&#25913;&#21892;&#36890;&#20449;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#21518;&#22788;&#29702;&#20998;&#31867;&#22120;&#65288;&#21363;&#21477;&#23376;&#21253;&#12289;&#25903;&#25345;&#21521;&#37327;&#26426;&#21644;&#26368;&#22823;&#29109;&#65289;&#22686;&#24378;&#20102;&#35895;&#27468;&#30340;&#35821;&#38899;&#35782;&#21035;&#33021;&#21147;&#12290;&#23454;&#39564;&#30740;&#31350;&#20102;&#19977;&#20010;&#22240;&#32032;&#65288;&#21407;&#22987;&#25514;&#36766;&#12289;&#31616;&#21270;&#25514;&#36766;&#21644;&#20010;&#24615;&#21270;&#25514;&#36766;&#65289;&#22312;&#19977;&#20010;&#27700;&#24179;&#65288;&#38646;&#27425;&#35757;&#32451;&#37325;&#22797;&#12289;5&#27425;&#35757;&#32451;&#37325;&#22797;&#21644;10&#27425;&#35757;&#32451;&#37325;&#22797;&#65289;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study examined the use of voice recognition technology in perioperative services (Periop) to enable Periop staff to record workflow milestones using mobile technology. The use of mobile technology to improve patient flow and quality of care could be facilitated if such voice recognition technology could be made robust. The goal of this experiment was to allow the Periop staff to provide care without being interrupted with data entry and querying tasks. However, the results are generalizable to other situations where an engineering manager attempts to improve communication performance using mobile technology. This study enhanced Google's voice recognition capability by using post-processing classifiers (i.e., bag-of-sentences, support vector machine, and maximum entropy). The experiments investigated three factors (original phrasing, reduced phrasing, and personalized phrasing) at three levels (zero training repetition, 5 training repetitions, and 10 training repetitions). Results 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65292;&#21033;&#29992;GPT-2&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03366</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#21487;&#35299;&#37322;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Explainable Recommendation with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03366
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;&#25552;&#31034;&#65292;&#21033;&#29992;GPT-2&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#21487;&#35299;&#37322;&#25512;&#33616;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#36827;&#34892;&#20248;&#21270;&#65292;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20869;&#25552;&#20379;&#35299;&#37322;&#33021;&#22815;&#25552;&#21319;&#29992;&#25143;&#28385;&#24847;&#24230;&#24182;&#24314;&#31435;&#20449;&#20219;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#35814;&#32454;&#35828;&#26126;&#20026;&#29992;&#25143;&#23450;&#21046;&#25512;&#33616;&#39033;&#30446;&#30340;&#21407;&#22240;&#12290;&#24403;&#21069;&#39046;&#22495;&#20013;&#20027;&#35201;&#30340;&#26041;&#27861;&#26159;&#29983;&#25104;&#22522;&#20110;&#25991;&#26412;&#30340;&#35299;&#37322;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24212;&#29992;&#23588;&#20026;&#31361;&#20986;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26102;&#38388;&#21644;&#35745;&#31639;&#36164;&#28304;&#38480;&#21046;&#65292;&#25913;&#36827;LLMs&#20197;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#25512;&#33616;&#22312;&#23454;&#36341;&#19978;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#26159;&#35757;&#32451;&#25552;&#31034;&#32780;&#19981;&#26159;LLM&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#36755;&#20837;&#30340;ID&#21521;&#37327;&#20316;&#20026;GPT-2&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#26694;&#26550;&#20013;&#37319;&#29992;&#32852;&#21512;&#35757;&#32451;&#26426;&#21046;&#65292;&#20248;&#21270;&#25512;&#33616;&#20219;&#21153;&#21644;&#35299;&#37322;&#20219;&#21153;&#12290;&#36825;&#31181;&#31574;&#30053;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#25506;&#32034;&#29992;&#25143;&#30340;&#20852;&#36259;&#65292;&#25552;&#39640;&#25512;&#33616;&#25928;&#26524;&#21644;&#29992;&#25143;&#28385;&#24847;&#24230;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Providing explanations within the recommendation system would boost user satisfaction and foster trust, especially by elaborating on the reasons for selecting recommended items tailored to the user. The predominant approach in this domain revolves around generating text-based explanations, with a notable emphasis on applying large language models (LLMs). However, refining LLMs for explainable recommendations proves impractical due to time constraints and computing resource limitations. As an alternative, the current approach involves training the prompt rather than the LLM. In this study, we developed a model that utilizes the ID vectors of user and item inputs as prompts for GPT-2. We employed a joint training mechanism within a multi-task learning framework to optimize both the recommendation task and explanation task. This strategy enables a more effective exploration of users' interests, improving recommendation effectiveness and user satisfaction. Through the experiments, our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NanoNER&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#36828;&#31243;&#30417;&#30563;&#23398;&#20064;&#65292;NanoNER&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20808;&#21069;&#24050;&#30693;&#23454;&#20307;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#27880;&#37322;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03362</link><description>&lt;p&gt;
NanoNER: &#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#21644;&#36828;&#31243;&#30417;&#30563;&#36827;&#34892;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
NanoNER: Named Entity Recognition for nanobiology using experts' knowledge and distant supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03362
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NanoNER&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;&#30340;&#30693;&#35782;&#21644;&#36828;&#31243;&#30417;&#30563;&#23398;&#20064;&#65292;NanoNER&#33021;&#22815;&#20934;&#30830;&#22320;&#35782;&#21035;&#20808;&#21069;&#24050;&#30693;&#23454;&#20307;&#65292;&#24182;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#27880;&#37322;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NanoNER&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#32435;&#31859;&#29983;&#29289;&#23398;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#27169;&#22411;&#12290;NER&#26159;&#22312;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#35782;&#21035;&#29305;&#23450;&#23454;&#20307;&#30340;&#20219;&#21153;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20449;&#24687;&#25552;&#21462;&#20013;&#32463;&#24120;&#26159;&#19968;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#30446;&#30340;&#26159;&#35782;&#21035;&#39046;&#22495;&#19987;&#23478;&#20043;&#21069;&#30830;&#23450;&#20026;&#35813;&#39046;&#22495;&#22522;&#26412;&#30693;&#35782;&#30340;&#23454;&#20307;&#12290;&#25105;&#20204;&#20381;&#38752;&#26412;&#20307;&#35770;&#26469;&#25552;&#20379;&#39046;&#22495;&#35789;&#27719;&#21644;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#19968;&#20010;&#36845;&#20195;&#36807;&#31243;&#65292;&#20351;&#19987;&#23478;&#33021;&#22815;&#30830;&#23450;&#19982;&#24403;&#21069;&#39046;&#22495;&#30456;&#20851;&#30340;&#23454;&#20307;&#12290;&#28982;&#21518;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36828;&#31243;&#30417;&#30563;&#23398;&#20064;&#22312;NER&#20013;&#30340;&#28508;&#21147;&#65292;&#25903;&#25345;&#36825;&#31181;&#26041;&#27861;&#22914;&#20309;&#21487;&#20197;&#36890;&#36807;&#26368;&#23569;&#30340;&#20154;&#21147;&#22686;&#21152;&#27880;&#37322;&#25968;&#25454;&#30340;&#25968;&#37327;&#12290;&#22312;&#21253;&#21547;&#36229;&#36807;120k&#23454;&#20307;&#20986;&#29616;&#27425;&#25968;&#30340;728&#31687;&#20840;&#25991;&#32435;&#31859;&#29983;&#29289;&#23398;&#25991;&#31456;&#30340;&#23436;&#25972;&#35821;&#26009;&#24211;&#19978;&#65292;NanoNER&#22312;&#20808;&#21069;&#24050;&#30693;&#23454;&#20307;&#30340;&#35782;&#21035;&#19978;&#33719;&#24471;&#20102;0.98&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Here we present the training and evaluation of NanoNER, a Named Entity Recognition (NER) model for Nanobiology. NER consists in the identification of specific entities in spans of unstructured texts and is often a primary task in Natural Language Processing (NLP) and Information Extraction. The aim of our model is to recognise entities previously identified by domain experts as constituting the essential knowledge of the domain. Relying on ontologies, which provide us with a domain vocabulary and taxonomy, we implemented an iterative process enabling experts to determine the entities relevant to the domain at hand. We then delve into the potential of distant supervision learning in NER, supporting how this method can increase the quantity of annotated data with minimal additional manpower. On our full corpus of 728 full-text nanobiology articles, containing more than 120k entity occurrences, NanoNER obtained a F1-score of 0.98 on the recognition of previously known entities. Our model 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#35821;&#20041;&#36890;&#20449;&#19982;&#30693;&#35782;&#23398;&#20064;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#24182;&#25506;&#32034;&#20102;&#20351;&#31995;&#32479;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#24211;&#20013;&#26356;&#26377;&#25928;&#36816;&#34892;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#23558;&#35821;&#20041;&#36890;&#20449;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03339</link><description>&lt;p&gt;
&#35821;&#20041;&#36890;&#20449;&#19982;&#30693;&#35782;&#23398;&#20064;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Interplay of Semantic Communication and Knowledge Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03339
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#35821;&#20041;&#36890;&#20449;&#19982;&#30693;&#35782;&#23398;&#20064;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#35752;&#35770;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#22686;&#24378;&#35821;&#20041;&#36890;&#20449;&#31995;&#32479;&#65292;&#24182;&#25506;&#32034;&#20102;&#20351;&#31995;&#32479;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#24211;&#20013;&#26356;&#26377;&#25928;&#36816;&#34892;&#30340;&#26041;&#27861;&#12290;&#21516;&#26102;&#36824;&#30740;&#31350;&#20102;&#23558;&#35821;&#20041;&#36890;&#20449;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36805;&#36895;&#21457;&#23637;&#30340;&#36890;&#20449;&#25216;&#26415;&#39046;&#22495;&#65292;&#24378;&#35843;&#30693;&#35782;&#29702;&#35299;&#21644;&#22788;&#29702;&#30340;&#35821;&#20041;&#36890;&#20449;&#65288;SemCom&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;SemCom&#20419;&#36827;&#20102;&#23545;&#36890;&#20449;&#20869;&#23481;&#30340;&#28145;&#20837;&#29702;&#35299;&#12289;&#20998;&#26512;&#21644;&#20256;&#36755;&#12290;&#22312;&#26412;&#31456;&#20013;&#65292;&#25105;&#20204;&#30528;&#37325;&#38416;&#26126;&#20102;SemCom&#20013;&#30340;&#30693;&#35782;&#23398;&#20064;&#25163;&#27573;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#30340;&#21033;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#22238;&#39038;&#20102;&#23558;SemCom&#19982;&#30693;&#35782;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#29616;&#26377;&#24037;&#20316;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;KG&#22686;&#24378;&#30340;SemCom&#31995;&#32479;&#65292;&#20854;&#20013;&#25509;&#25910;&#26041;&#36890;&#36807;&#20174;&#38745;&#24577;&#30693;&#35782;&#24211;&#20013;&#21033;&#29992;&#30693;&#35782;&#26469;&#25913;&#21892;&#35299;&#30721;&#24615;&#33021;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#33021;&#22815;&#20351;&#31995;&#32479;&#22312;&#19981;&#26029;&#28436;&#21270;&#30340;&#30693;&#35782;&#24211;&#20013;&#26356;&#26377;&#25928;&#22320;&#36816;&#34892;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the swiftly advancing realm of communication technologies, Semantic Communication (SemCom), which emphasizes knowledge understanding and processing, has emerged as a hot topic. By integrating artificial intelligence technologies, SemCom facilitates a profound understanding, analysis and transmission of communication content. In this chapter, we clarify the means of knowledge learning in SemCom with a particular focus on the utilization of Knowledge Graphs (KGs). Specifically, we first review existing efforts that combine SemCom with knowledge learning. Subsequently, we introduce a KG-enhanced SemCom system, wherein the receiver is carefully calibrated to leverage knowledge from its static knowledge base for ameliorating the decoding performance. Contingent upon this framework, we further explore potential approaches that can empower the system to operate in evolving knowledge base more effectively. Furthermore, we investigate the possibility of integration with Large Language Models
&lt;/p&gt;</description></item><item><title>Uni3D-LLM&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#28857;&#20113;&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#21644;&#20462;&#25913;&#28857;&#20113;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#25805;&#20316;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03327</link><description>&lt;p&gt;
Uni3D-LLM&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32479;&#19968;&#28857;&#20113;&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03327
&lt;/p&gt;
&lt;p&gt;
Uni3D-LLM&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20102;&#28857;&#20113;&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#30340;&#19968;&#20307;&#21270;&#12290;&#36890;&#36807;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#29983;&#25104;&#21644;&#20462;&#25913;&#28857;&#20113;&#22330;&#26223;&#20013;&#30340;&#23545;&#35937;&#65292;&#20174;&#32780;&#25552;&#39640;&#25805;&#20316;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Uni3D-LLM&#65292;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23558;3D&#24863;&#30693;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#20219;&#21153;&#25972;&#21512;&#20026;&#19968;&#20307;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;&#25143;&#33021;&#22815;&#36731;&#26494;&#22320;&#22312;&#28857;&#20113;&#22330;&#26223;&#20013;&#25351;&#23450;&#20301;&#32622;&#29983;&#25104;&#21644;&#20462;&#25913;&#23545;&#35937;&#65292;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22810;&#26679;&#24615;&#36827;&#34892;&#24341;&#23548;&#12290;Uni3D-LLM&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;3D&#23545;&#35937;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#31934;&#30830;&#25511;&#21046;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#21319;&#25805;&#20316;&#30340;&#28789;&#27963;&#24615;&#21644;&#21487;&#25511;&#24615;&#12290;&#36890;&#36807;&#23558;&#28857;&#20113;&#26144;&#23556;&#21040;&#32479;&#19968;&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;Uni3D-LLM&#23454;&#29616;&#20102;&#36328;&#24212;&#29992;&#21151;&#33021;&#65292;&#33021;&#22815;&#26080;&#32541;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#20174;&#31934;&#30830;&#23454;&#20363;&#21270;3D&#23545;&#35937;&#21040;&#20132;&#20114;&#35774;&#35745;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20005;&#26684;&#30340;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;Uni3D-LLM&#22312;&#28857;&#20113;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Uni3D-LLM, a unified framework that leverages a Large Language Model (LLM) to integrate tasks of 3D perception, generation, and editing within point cloud scenes. This framework empowers users to effortlessly generate and modify objects at specified locations within a scene, guided by the versatility of natural language descriptions. Uni3D-LLM harnesses the expressive power of natural language to allow for precise command over the generation and editing of 3D objects, thereby significantly enhancing operational flexibility and controllability. By mapping point cloud into the unified representation space, Uni3D-LLM achieves cross-application functionality, enabling the seamless execution of a wide array of tasks, ranging from the accurate instantiation of 3D objects to the diverse requirements of interactive design. Through a comprehensive suite of rigorous experiments, the efficacy of Uni3D-LLM in the comprehension, generation, and editing of point cloud has
&lt;/p&gt;</description></item><item><title>DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03300</link><description>&lt;p&gt;
DeepSeekMath: &#23558;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#25512;&#21521;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03300
&lt;/p&gt;
&lt;p&gt;
DeepSeekMath&#26159;&#19968;&#31181;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#25968;&#25454;&#36873;&#25321;&#65292;&#25552;&#21319;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#31454;&#36187;&#32423;&#21035;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23398;&#25512;&#29702;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#32467;&#26500;&#21270;&#30340;&#29305;&#24615;&#65292;&#23545;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;DeepSeekMath 7B&#65292;&#23427;&#22312;Common Crawl&#20013;&#33719;&#21462;&#20102;120B&#20010;&#19982;&#25968;&#23398;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24182;&#32467;&#21512;&#20102;&#33258;&#28982;&#35821;&#35328;&#21644;&#20195;&#30721;&#25968;&#25454;&#26469;&#32487;&#32493;&#39044;&#35757;&#32451;DeepSeek-Coder-Base-v1.5 7B&#12290;DeepSeekMath 7B&#22312;&#31454;&#36187;&#32423;&#21035;&#30340;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;51.7%&#30340;&#20998;&#25968;&#65292;&#26080;&#38656;&#20381;&#36182;&#22806;&#37096;&#24037;&#20855;&#21253;&#21644;&#25237;&#31080;&#25216;&#26415;&#65292;&#25509;&#36817;&#20102;Gemini-Ultra&#21644;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;DeepSeekMath 7B&#30340;&#33258;&#19968;&#33268;&#24615;&#22312;MATH&#19978;&#30340;64&#20010;&#26679;&#26412;&#20013;&#36798;&#21040;&#20102;60.9%&#30340;&#20998;&#25968;&#12290;DeepSeekMath&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#24402;&#22240;&#20110;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25968;&#25454;&#36873;&#25321;&#31649;&#36947;&#20805;&#20998;&#21033;&#29992;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#32593;&#32476;&#25968;&#25454;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32676;&#20307;&#30456;&#23545;&#31574;&#30053;&#20248;&#21270;&#65288;GRPO&#65289;&#65292;&#36825;&#26159;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#30340;&#19968;&#20010;&#21464;&#20307;&#65292;&#21487;&#20197;&#22686;&#24378;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities w
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#23558;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2402.03161</link><description>&lt;p&gt;
Video-LaVIT&#65306;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#21450;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Video-LaVIT: Unified Video-Language Pre-training with Decoupled Visual-Motional Tokenization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03161
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#35270;&#39057;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#32806;&#30340;&#35270;&#35273;-&#36816;&#21160;&#26631;&#35760;&#21270;&#23558;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#65292;&#28982;&#21518;&#20351;&#29992;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#25216;&#26415;&#26469;&#29983;&#25104;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#22914;&#20309;&#23558;&#20854;&#20174;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#25193;&#23637;&#21040;&#26356;&#20855;&#20449;&#24687;&#20215;&#20540;&#30340;&#29616;&#23454;&#19990;&#30028;&#35270;&#39057;&#12290;&#19982;&#38745;&#24577;&#22270;&#20687;&#30456;&#27604;&#65292;&#35270;&#39057;&#22312;&#26377;&#25928;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#38754;&#20020;&#30528;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21407;&#22240;&#22312;&#20110;&#38656;&#35201;&#23545;&#20854;&#26102;&#31354;&#21160;&#24577;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#38024;&#23545;&#35270;&#39057;-&#35821;&#35328;&#39044;&#35757;&#32451;&#20013;&#30340;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#39057;&#20998;&#35299;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#35270;&#39057;&#34920;&#31034;&#20026;&#20851;&#38190;&#24103;&#21644;&#26102;&#38388;&#36816;&#21160;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#35774;&#35745;&#33391;&#22909;&#30340;&#26631;&#35760;&#22120;&#23558;&#35270;&#35273;&#21644;&#26102;&#38388;&#20449;&#24687;&#31163;&#25955;&#21270;&#20026;&#23569;&#37327;&#26631;&#35760;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;LLM&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#35270;&#39057;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#32479;&#19968;&#29983;&#25104;&#39044;&#35757;&#32451;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#20174;LLM&#29983;&#25104;&#30340;&#26631;&#35760;&#34987;&#20180;&#32454;&#24674;&#22797;&#21040;&#21407;&#22987;&#30340;&#36830;&#32493;&#20687;&#32032;&#31354;&#38388;&#65292;&#20197;&#29983;&#25104;&#21508;&#31181;&#35270;&#39057;&#20869;&#23481;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26082;&#33021;&#29702;&#35299;&#21448;&#33021;&#29983;&#25104;&#22270;&#20687;&#21644;&#35270;&#39057;&#20869;&#23481;&#65292;&#24182;&#36890;&#36807;&#22312;13&#20010;&#20219;&#21153;&#19978;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#21152;&#20197;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
In light of recent advances in multimodal Large Language Models (LLMs), there is increasing attention to scaling them from image-text data to more informative real-world videos. Compared to static images, video poses unique challenges for effective large-scale pre-training due to the modeling of its spatiotemporal dynamics. In this paper, we address such limitations in video-language pre-training with an efficient video decomposition that represents each video as keyframes and temporal motions. These are then adapted to an LLM using well-designed tokenizers that discretize visual and temporal information as a few tokens, thus enabling unified generative pre-training of videos, images, and text. At inference, the generated tokens from the LLM are carefully recovered to the original continuous pixel space to create various video content. Our proposed framework is both capable of comprehending and generating image and video content, as demonstrated by its competitive performance across 13
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02791</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Rethinking Optimization and Architecture for Tiny Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#24605;&#32771;&#20102;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#21644;&#26550;&#26500;&#65292;&#36890;&#36807;&#32463;&#39564;&#30740;&#31350;&#21457;&#29616;&#20102;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29305;&#21035;&#26377;&#25928;&#30340;&#35774;&#35745;&#20844;&#24335;&#65292;&#24182;&#22312;&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23041;&#21147;&#36890;&#36807;&#22823;&#37327;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#28982;&#32780;&#65292;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#24212;&#29992;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#30340;&#24040;&#22823;&#25361;&#25112;&#65292;&#36843;&#20999;&#38656;&#35201;&#39640;&#24615;&#33021;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#22797;&#26434;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#65292;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#35768;&#22810;&#32454;&#33410;&#24456;&#23569;&#24471;&#21040;&#20180;&#32454;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#19968;&#20010;&#20855;&#26377;10&#20159;&#21442;&#25968;&#30340;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#20180;&#32454;&#35774;&#35745;&#20102;&#19968;&#31995;&#21015;&#32463;&#39564;&#30740;&#31350;&#26469;&#20998;&#26512;&#27599;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#20027;&#35201;&#35752;&#35770;&#20102;&#19977;&#20010;&#26041;&#38754;&#65292;&#21363;&#31070;&#32463;&#26550;&#26500;&#12289;&#21442;&#25968;&#21021;&#22987;&#21270;&#21644;&#20248;&#21270;&#31574;&#30053;&#12290;&#22810;&#20010;&#35774;&#35745;&#20844;&#24335;&#22312;&#24494;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32463;&#39564;&#24615;&#22320;&#34987;&#35777;&#26126;&#29305;&#21035;&#26377;&#25928;&#65292;&#21253;&#25324;&#20998;&#35789;&#22120;&#21387;&#32553;&#12289;&#26550;&#26500;&#35843;&#25972;&#12289;&#21442;&#25968;&#32487;&#25215;&#21644;&#22810;&#36718;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;1.6T&#22810;&#35821;&#31181;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20102;PanGu-$\pi$-1B Pro&#21644;PanGu-$\pi$-1.5B Pro&#12290;
&lt;/p&gt;
&lt;p&gt;
The power of large language models (LLMs) has been demonstrated through numerous data and computing resources. However, the application of language models on mobile devices is facing huge challenge on the computation and memory costs, that is, tiny language models with high performance are urgently required. Limited by the highly complex training process, there are many details for optimizing language models that are seldom studied carefully. In this study, based on a tiny language model with 1B parameters, we carefully design a series of empirical study to analyze the effect of each component. Three perspectives are mainly discussed, i.e., neural architecture, parameter initialization, and optimization strategy. Several design formulas are empirically proved especially effective for tiny language models, including tokenizer compression, architecture tweaking, parameter inheritance and multiple-round training. Then we train PanGu-$\pi$-1B Pro and PanGu-$\pi$-1.5B Pro on 1.6T multilingu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24314;&#31435;&#22303;&#33879;&#35821;&#35328;NLP&#25216;&#26415;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#24182;&#25512;&#33616;NLP&#30740;&#31350;&#20154;&#21592;&#22686;&#21152;&#23545;&#19982;&#22303;&#33879;&#31038;&#21306;&#21512;&#20316;&#36807;&#31243;&#30340;&#20851;&#27880;&#12290;</title><link>https://arxiv.org/abs/2402.02639</link><description>&lt;p&gt;
&#8220;&#37325;&#35201;&#30340;&#26159;&#20320;&#22914;&#20309;&#20570;&#20107;&#24773;&#8221;&#65306;&#20851;&#27880;&#36807;&#31243;&#20197;&#26356;&#22909;&#22320;&#20026;&#22303;&#33879;&#31038;&#21306;&#25552;&#20379;&#35821;&#35328;&#25216;&#26415;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
It's how you do things that matters": Attending to Process to Better Serve Indigenous Communities with Language Technologies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24314;&#31435;&#22303;&#33879;&#35821;&#35328;NLP&#25216;&#26415;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#24182;&#25512;&#33616;NLP&#30740;&#31350;&#20154;&#21592;&#22686;&#21152;&#23545;&#19982;&#22303;&#33879;&#31038;&#21306;&#21512;&#20316;&#36807;&#31243;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#19978;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#23545;&#22303;&#33879;&#35821;&#35328;&#30340;&#26381;&#21153;&#24635;&#26159;&#19981;&#36275;&#30340;&#65292;&#20294;&#26159;&#38543;&#30528;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;NLP&#31038;&#32676;&#23545;&#28626;&#21361;&#35821;&#35328;&#30340;&#20851;&#27880;&#22686;&#21152;&#65292;&#36825;&#31181;&#24773;&#20917;&#27491;&#22312;&#21457;&#29983;&#25913;&#21464;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20026;&#22303;&#33879;&#35821;&#35328;&#26500;&#24314;NLP&#25216;&#26415;&#20013;&#30340;&#20262;&#29702;&#32771;&#34385;&#65292;&#22522;&#20110;&#36825;&#26679;&#30340;&#21069;&#25552;&#65306;&#36825;&#20123;&#39033;&#30446;&#39318;&#20808;&#24212;&#35813;&#26381;&#21153;&#20110;&#22303;&#33879;&#31038;&#21306;&#12290;&#25105;&#20204;&#23545;&#22312;&#28595;&#22823;&#21033;&#20122;&#20174;&#20107;&#22303;&#33879;&#21644;/&#25110;&#25176;&#38647;&#26031;&#28023;&#23777;&#23707;&#27665;&#31038;&#21306;&#30340;&#35821;&#35328;&#25216;&#26415;&#39033;&#30446;&#30340;17&#21517;&#30740;&#31350;&#20154;&#21592;&#36827;&#34892;&#20102;&#37319;&#35775;&#65292;&#24182;&#20511;&#37492;&#20102;&#36825;&#20123;&#37319;&#35775;&#30340;&#35265;&#35299;&#65292;&#25552;&#20986;&#20102;&#22686;&#21152;NLP&#30740;&#31350;&#20154;&#21592;&#23545;&#19982;&#22303;&#33879;&#31038;&#21306;&#21512;&#20316;&#36807;&#31243;&#30340;&#20851;&#27880;&#65292;&#32780;&#19981;&#20165;&#20165;&#20851;&#27880;&#20110;&#21435;&#24773;&#22659;&#21270;&#30340;&#24037;&#33402;&#21697;&#30340;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Indigenous languages are historically under-served by Natural Language Processing (NLP) technologies, but this is changing for some languages with the recent scaling of large multilingual models and an increased focus by the NLP community on endangered languages. This position paper explores ethical considerations in building NLP technologies for Indigenous languages, based on the premise that such projects should primarily serve Indigenous communities. We report on interviews with 17 researchers working in or with Aboriginal and/or Torres Strait Islander communities on language technology projects in Australia. Drawing on insights from the interviews, we recommend practices for NLP researchers to increase attention to the process of engagements with Indigenous communities, rather than focusing only on decontextualised artefacts.
&lt;/p&gt;</description></item><item><title>Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.02416</link><description>&lt;p&gt;
Aligner: &#36890;&#36807;&#24369;&#21040;&#24378;&#26657;&#27491;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Aligner: Achieving Efficient Alignment through Weak-to-Strong Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02416
&lt;/p&gt;
&lt;p&gt;
Aligner&#26159;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#26657;&#27491;&#27531;&#24046;&#26469;&#23454;&#29616;&#39640;&#25928;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;Aligner&#20855;&#26377;&#21442;&#25968;&#39640;&#25928;&#12289;&#24369;&#21040;&#24378;&#27867;&#21270;&#20197;&#21450;&#21363;&#25554;&#21363;&#29992;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26469;&#36827;&#34892;&#23545;&#40784;&#30340;&#21162;&#21147;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30528;&#20027;&#35201;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35757;&#32451;&#22870;&#21169;&#27169;&#22411;&#12289;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24037;&#31243;&#20197;&#21450;&#37325;&#35201;&#30340;&#26159;&#65292;&#38656;&#35201;&#35775;&#38382;LLM&#21442;&#25968;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#23545;&#40784;&#33539;&#24335;Aligner&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#23545;&#40784;&#21644;&#26410;&#23545;&#40784;&#31572;&#26696;&#20043;&#38388;&#30340;&#26657;&#27491;&#27531;&#24046;&#26469;&#32469;&#36807;&#25972;&#20010;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;Aligner&#20855;&#26377;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#33258;&#21160;&#22238;&#24402;seq2seq&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;&#26597;&#35810;-&#31572;&#26696;-&#26657;&#27491;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#23545;&#40784;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#19988;&#23545;&#36164;&#28304;&#38656;&#27714;&#36739;&#23569;&#12290;&#20854;&#27425;&#65292;Aligner&#23454;&#29616;&#20102;&#20174;&#24369;&#21040;&#24378;&#30340;&#27867;&#21270;&#65307;&#36890;&#36807;Aligner&#30340;&#30417;&#30563;&#20449;&#21495;&#26469;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24615;&#33021;&#12290;&#31532;&#19977;&#65292;Aligner&#20316;&#20026;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#8230;
&lt;/p&gt;
&lt;p&gt;
Efforts to align Large Language Models (LLMs) are mainly conducted via Reinforcement Learning from Human Feedback (RLHF) methods. However, RLHF encounters major challenges including training reward models, actor-critic engineering, and importantly, it requires access to LLM parameters. Here we introduce Aligner, a new efficient alignment paradigm that bypasses the whole RLHF process by learning the correctional residuals between the aligned and the unaligned answers. Our Aligner offers several key advantages. Firstly, it is an autoregressive seq2seq model that is trained on the query-answer-correction dataset via supervised learning; this offers a parameter-efficient alignment solution with minimal resources. Secondly, the Aligner facilitates weak-to-strong generalization; finetuning large pretrained models by Aligner's supervisory signals demonstrates strong performance boost. Thirdly, Aligner functions as a model-agnostic plug-and-play module, allowing for its direct application on d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02380</link><description>&lt;p&gt;
&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#26102;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models in Analysing Classroom Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#29305;&#28857;&#26159;GPT-4&#65292;&#23545;&#35838;&#22530;&#23545;&#35805;&#36827;&#34892;&#20998;&#26512;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#33021;&#22815;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#19988;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#65292;&#22312;&#20998;&#26512;&#35838;&#22530;&#23545;&#35805;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#25945;&#23398;&#35786;&#26029;&#21644;&#36136;&#37327;&#25913;&#36827;&#30340;&#37325;&#35201;&#30740;&#31350;&#20219;&#21153;&#12290;&#37492;&#20110;&#20256;&#32479;&#25945;&#32946;&#30740;&#31350;&#20013;&#30693;&#35782;&#23494;&#38598;&#21644;&#21171;&#21160;&#23494;&#38598;&#30340;&#23450;&#24615;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;LLM&#22312;&#20248;&#21270;&#21644;&#22686;&#24378;&#20998;&#26512;&#36807;&#31243;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#35813;&#30740;&#31350;&#28041;&#21450;&#20013;&#23398;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25968;&#23398;&#21644;&#35821;&#25991;&#35838;&#22530;&#19978;&#30340;&#23545;&#35805;&#12290;&#36825;&#20123;&#23545;&#35805;&#30001;&#25945;&#32946;&#19987;&#23478;&#25163;&#21160;&#32534;&#30721;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#21046;&#30340;GPT-4&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#27604;&#36739;&#25163;&#21160;&#27880;&#37322;&#19982;GPT-4&#30340;&#36755;&#20986;&#65292;&#20197;&#35780;&#20272;&#20854;&#22312;&#20998;&#26512;&#25945;&#32946;&#23545;&#35805;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#35780;&#20272;&#26102;&#38388;&#25928;&#29575;&#12289;&#32534;&#30721;&#32773;&#38388;&#19968;&#33268;&#24615;&#21644;&#32534;&#30721;&#32773;&#38388;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;GPT-4&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#26102;&#38388;&#65292;&#24182;&#22312;&#32534;&#30721;&#19968;&#33268;&#24615;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the application of Large Language Models (LLMs), specifically GPT-4, in the analysis of classroom dialogue, a crucial research task for both teaching diagnosis and quality improvement. Recognizing the knowledge-intensive and labor-intensive nature of traditional qualitative methods in educational research, this study investigates the potential of LLM to streamline and enhance the analysis process. The study involves datasets from a middle school, encompassing classroom dialogues across mathematics and Chinese classes. These dialogues were manually coded by educational experts and then analyzed using a customised GPT-4 model. This study focuses on comparing manual annotations with the outputs of GPT-4 to evaluate its efficacy in analyzing educational dialogues. Time efficiency, inter-coder agreement, and inter-coder reliability between human coders and GPT-4 are evaluated. Results indicate substantial time savings with GPT-4, and a high degree of consistency in codin
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.01801</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Time Series: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01801
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#35770;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#26041;&#27861;&#12290;&#36890;&#36807;&#35299;&#20915;LLM&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#25581;&#31034;&#20102;LLM&#22312;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#30452;&#25509;&#25552;&#31034;&#12289;&#37327;&#21270;&#12289;&#23545;&#40784;&#12289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#21644;&#32467;&#21512;&#24037;&#20855;&#31561;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#23545;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;LLM&#19981;&#20165;&#20165;&#23616;&#38480;&#20110;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#22270;&#24418;&#65292;&#36824;&#20855;&#26377;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#30340;&#37325;&#35201;&#28508;&#21147;&#65292;&#21487;&#20197;&#22312;&#27668;&#20505;&#12289;&#29289;&#32852;&#32593;&#12289;&#21307;&#30103;&#12289;&#20132;&#36890;&#12289;&#38899;&#39057;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#21463;&#30410;&#12290;&#26412;&#35843;&#30740;&#35770;&#25991;&#23545;&#21033;&#29992;LLM&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#21508;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#28145;&#20837;&#25506;&#35752;&#21644;&#35814;&#32454;&#20998;&#31867;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;LLM&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#35757;&#32451;&#19982;&#25968;&#20540;&#22411;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#25361;&#25112;&#65292;&#24182;&#25506;&#32034;&#20102;&#23558;LLM&#30340;&#30693;&#35782;&#36716;&#31227;&#21644;&#25552;&#21462;&#21040;&#25968;&#20540;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#65288;1&#65289;&#30452;&#25509;&#25552;&#31034;LLM&#65292;&#65288;2&#65289;&#26102;&#38388;&#24207;&#21015;&#37327;&#21270;&#65292;&#65288;3&#65289;&#23545;&#40784;&#25216;&#26415;&#65292;&#65288;4&#65289;&#21033;&#29992;&#35270;&#35273;&#26041;&#24335;&#20316;&#20026;&#26725;&#25509;&#26426;&#21046;&#65292;&#21644;&#65288;5&#65289;&#32467;&#21512;LLM&#19982;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#26412;&#35843;&#30740;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#28041;&#21450;&#24212;&#29992;&#39046;&#22495;&#12289;&#35780;&#20272;&#26041;&#27861;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#30340;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) alignment techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey off
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01704</link><description>&lt;p&gt;
&#20316;&#20026;&#31574;&#30053;&#30340;&#29366;&#24577;&#23383;&#31526;&#20018;&#65306;&#29992;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#24341;&#23548;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
States as Strings as Strategies: Steering Language Models with Game-Theoretic Solvers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#21338;&#24328;&#35770;&#24605;&#24819;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32465;&#23450;&#21338;&#24328;&#35770;&#30340;&#31526;&#21495;&#36923;&#36753;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#21338;&#24328;&#35770;&#27714;&#35299;&#22120;&#25552;&#20379;&#26356;&#21152;&#31283;&#23450;&#21644;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21338;&#24328;&#35770;&#26159;&#30740;&#31350;&#29702;&#24615;&#20027;&#20307;&#38388;&#25112;&#30053;&#20114;&#21160;&#30340;&#25968;&#23398;&#27169;&#22411;&#12290;&#35821;&#35328;&#26159;&#20154;&#31867;&#20114;&#21160;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20294;&#22312;&#21382;&#21490;&#19978;&#19968;&#30452;&#24456;&#38590;&#36890;&#36807;&#25968;&#23398;&#26041;&#27861;&#23545;&#23545;&#35805;&#21450;&#20854;&#25112;&#30053;&#21160;&#26426;&#24314;&#27169;&#12290;&#19982;&#35821;&#35328;&#20114;&#21160;&#30456;&#20851;&#30340;&#29609;&#23478;&#12289;&#31574;&#30053;&#21644;&#22238;&#25253;&#30340;&#36866;&#24403;&#27169;&#22411;&#65288;&#21363;&#23545;&#28216;&#25103;&#35770;&#24120;&#35268;&#31526;&#21495;&#36923;&#36753;&#30340;&#32422;&#26463;&#65289;&#23558;&#20351;&#29616;&#26377;&#30340;&#21338;&#24328;&#35770;&#31639;&#27861;&#33021;&#22815;&#22312;&#35821;&#35328;&#39046;&#22495;&#25552;&#20379;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#31181;&#32422;&#26463;&#21487;&#20197;&#20026;&#22312;&#23545;&#35805;&#20013;&#35745;&#31639;&#31283;&#23450;&#12289;&#29702;&#24615;&#30340;&#23545;&#35805;&#31574;&#30053;&#25552;&#20379;&#19968;&#26465;&#36884;&#24452;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21487;&#33021;&#24050;&#32463;&#36798;&#21040;&#20102;&#20854;&#29983;&#25104;&#33021;&#21147;&#36275;&#20197;&#23454;&#29616;&#33258;&#28982;&#23545;&#35805;&#30495;&#23454;&#12289;&#31867;&#20284;&#20154;&#31867;&#30340;&#27169;&#25311;&#30340;&#31243;&#24230;&#12290;&#36890;&#36807;&#20197;&#19981;&#21516;&#30340;&#26041;&#24335;&#25552;&#31034;&#23427;&#20204;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#20854;&#21709;&#24212;&#24341;&#23548;&#21040;&#19981;&#21516;&#30340;&#36755;&#20986;&#35805;&#35821;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;LLM&#36824;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#24555;&#36895;&#29983;&#25104;&#26032;&#30340;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
Game theory is the study of mathematical models of strategic interactions among rational agents. Language is a key medium of interaction for humans, though it has historically proven difficult to model dialogue and its strategic motivations mathematically. A suitable model of the players, strategies, and payoffs associated with linguistic interactions (i.e., a binding to the conventional symbolic logic of game theory) would enable existing game-theoretic algorithms to provide strategic solutions in the space of language. In other words, a binding could provide a route to computing stable, rational conversational strategies in dialogue. Large language models (LLMs) have arguably reached a point where their generative capabilities can enable realistic, human-like simulations of natural dialogue. By prompting them in various ways, we can steer their responses towards different output utterances. Leveraging the expressivity of natural language, LLMs can also help us quickly generate new di
&lt;/p&gt;</description></item><item><title>SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.01685</link><description>&lt;p&gt;
SMUTF&#65306;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#21644;&#28151;&#21512;&#29305;&#24449;&#30340;&#27169;&#24335;&#21305;&#37197;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SMUTF: Schema Matching Using Generative Tags and Hybrid Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01685
&lt;/p&gt;
&lt;p&gt;
SMUTF&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#26631;&#31614;&#25552;&#39640;&#21305;&#37197;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#24320;&#21457;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#29616;&#26377;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SMUTF&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#34920;&#26684;&#25968;&#25454;&#27169;&#24335;&#21305;&#37197;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20551;&#35774;&#22312;&#24320;&#25918;&#22495;&#20219;&#21153;&#20013;&#65292;&#30417;&#30563;&#23398;&#20064;&#19981;&#20250;&#24433;&#21709;&#24615;&#33021;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#36328;&#22495;&#21305;&#37197;&#12290;&#36825;&#20010;&#31995;&#32479;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#24037;&#31243;&#12289;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;&#21463;&#20154;&#36947;&#20027;&#20041;&#20132;&#25442;&#35821;&#35328;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20351;&#29992;&#8220;&#29983;&#25104;&#26631;&#31614;&#8221;&#20026;&#27599;&#20010;&#25968;&#25454;&#21015;&#37096;&#32626;&#20102;&#21019;&#26032;&#30340;&#36866;&#24212;&#24615;&#65292;&#25552;&#39640;&#20102;&#27169;&#24335;&#21305;&#37197;&#30340;&#25928;&#26524;&#12290;SMUTF&#20855;&#26377;&#24191;&#27867;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#12289;&#20998;&#31867;&#26041;&#27861;&#21644;&#29983;&#25104;&#27169;&#22411;&#26080;&#32541;&#37197;&#21512;&#20351;&#29992;&#12290;&#37492;&#20110;&#27169;&#24335;&#21305;&#37197;&#32570;&#20047;&#24191;&#27867;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24050;&#32463;&#21019;&#24314;&#24182;&#24320;&#28304;&#20102;HDXSM&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26469;&#33258;&#20844;&#20849;&#20154;&#36947;&#20027;&#20041;&#25968;&#25454;&#65292;&#25105;&#20204;&#30456;&#20449;&#36825;&#26159;&#30446;&#21069;&#26368;&#20840;&#38754;&#30340;&#27169;&#24335;&#21305;&#37197;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SMUTF, a unique approach for large-scale tabular data schema matching (SM), which assumes that supervised learning does not affect performance in open-domain tasks, thereby enabling effective cross-domain matching. This system uniquely combines rule-based feature engineering, pre-trained language models, and generative large language models. In an innovative adaptation inspired by the Humanitarian Exchange Language, we deploy 'generative tags' for each data column, enhancing the effectiveness of SM. SMUTF exhibits extensive versatility, working seamlessly with any pre-existing pre-trained embeddings, classification methods, and generative models.   Recognizing the lack of extensive, publicly available datasets for SM, we have created and open-sourced the HDXSM dataset from the public humanitarian data. We believe this to be the most exhaustive SM dataset currently available. In evaluations across various public datasets and the novel HDXSM dataset, SMUTF demonstrated excep
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLsM&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#38544;&#20889;&#26415;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35805;&#35821;&#29305;&#24449;&#30340;&#38544;&#20889;&#25991;&#26412;&#65292;&#25552;&#39640;&#20102;&#38544;&#34109;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.15656</link><description>&lt;p&gt;
LLsM: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
LLsM: Generative Linguistic Steganography with Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;LLsM&#65292;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#38544;&#20889;&#26415;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;LLM&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35805;&#35821;&#29305;&#24449;&#30340;&#38544;&#20889;&#25991;&#26412;&#65292;&#25552;&#39640;&#20102;&#38544;&#34109;&#36890;&#20449;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#38544;&#20889;&#26415;&#65288;LS&#65289;&#26088;&#22312;&#26681;&#25454;&#31192;&#23494;&#20449;&#24687;&#29983;&#25104;&#38544;&#20889;&#25991;&#26412;&#65288;stego&#65289;&#12290;&#21482;&#26377;&#25480;&#26435;&#25509;&#25910;&#32773;&#25165;&#33021;&#23519;&#35273;&#25991;&#26412;&#20013;&#31192;&#23494;&#30340;&#23384;&#22312;&#24182;&#25552;&#21462;&#20986;&#26469;&#65292;&#20174;&#32780;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#26696;&#29983;&#25104;&#30340;&#38544;&#20889;&#25991;&#26412;&#21487;&#25511;&#24615;&#36739;&#24046;&#65292;&#24456;&#38590;&#21253;&#21547;&#29305;&#23450;&#30340;&#35805;&#35821;&#29305;&#24449;&#65292;&#22914;&#39118;&#26684;&#12290;&#32467;&#26524;&#65292;&#38544;&#20889;&#25991;&#26412;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65292;&#21361;&#21450;&#38544;&#34109;&#36890;&#20449;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LLsM&#65292;&#31532;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;LS&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21253;&#21547;&#20016;&#23500;&#35805;&#35821;&#29305;&#24449;&#30340;&#22823;&#35268;&#27169;&#26500;&#24314;&#25968;&#25454;&#38598;&#23545;LLaMA2&#36827;&#34892;&#24494;&#35843;&#65292;&#20351;&#24471;&#24494;&#35843;&#21518;&#30340;LLM&#33021;&#22815;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#35805;&#35821;&#29305;&#24449;&#30340;&#25991;&#26412;&#12290;&#28982;&#21518;&#23558;&#35805;&#35821;&#20316;&#20026;&#24341;&#23548;&#20449;&#24687;&#21644;&#31192;&#23494;&#19968;&#36215;&#36755;&#20837;&#32473;&#24494;&#35843;&#21518;&#30340;LLM&#65292;&#24418;&#24335;&#20026;&#8220;Prompt&#8221;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26500;&#24314;&#30340;&#20505;&#36873;&#27744;&#23558;&#36827;&#34892;&#33539;&#22260;&#32534;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linguistic Steganography (LS) tasks aim to generate steganographic text (stego) based on secret information. Only authorized recipients can perceive the existence of secrets in the texts and extract them, thereby preserving privacy. However, the controllability of the stego generated by existing schemes is poor, and the stego is difficult to contain specific discourse characteristics such as style. As a result, the stego is easily detectable, compromising covert communication. To address these problems, this paper proposes LLsM, the first LS with the Large Language Model (LLM). We fine-tuned the LLaMA2 with a large-scale constructed dataset encompassing rich discourse characteristics, which enables the fine-tuned LLM to generate texts with specific discourse in a controllable manner. Then the discourse is used as guiding information and inputted into the fine-tuned LLM in the form of the Prompt together with secret. On this basis, the constructed candidate pool will be range encoded an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2401.07237</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling Event Sequence Knowledge From Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07237
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#26041;&#27861;&#12290;&#37319;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#30340;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#24335;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#24182;&#19988;&#22312;&#22635;&#34917;&#30693;&#35782;&#31354;&#30333;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#27169;&#22411;&#22312;&#20107;&#20214;&#30340;&#20998;&#26512;&#21644;&#39044;&#27979;&#20013;&#34987;&#21457;&#29616;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;&#24314;&#31435;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#20016;&#23500;&#30340;&#39640;&#36136;&#37327;&#20107;&#20214;&#24207;&#21015;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24212;&#29992;&#20013;&#65292;&#24178;&#20928;&#30340;&#32467;&#26500;&#21270;&#20107;&#20214;&#24207;&#21015;&#19981;&#21487;&#29992;&#65292;&#33258;&#21160;&#21270;&#24207;&#21015;&#25552;&#21462;&#23548;&#33268;&#30340;&#25968;&#25454;&#22826;&#22024;&#26434;&#21644;&#19981;&#23436;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#21487;&#20197;&#26377;&#25928;&#29992;&#20110;&#27010;&#29575;&#20107;&#20214;&#27169;&#22411;&#26500;&#24314;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#20174;LLMs&#20013;&#25552;&#21462;&#20107;&#20214;&#24207;&#21015;&#30693;&#35782;&#30340;&#19968;&#31181;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20855;&#26377;&#37096;&#20998;&#22240;&#26524;&#20851;&#31995;&#30340;&#20107;&#20214;&#27010;&#24565;&#30340;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#26469;&#25351;&#23548;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22240;&#26524;&#20107;&#20214;&#24207;&#21015;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20107;&#20214;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;&#36755;&#20837;KG&#20013;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#22914;&#20309;&#21033;&#29992;&#29983;&#25104;&#30340;&#24207;&#21015;&#26469;&#21457;&#29616;&#26356;&#26377;&#29992;&#21644;&#26356;&#22797;&#26434;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event sequence models have been found to be highly effective in the analysis and prediction of events. Building such models requires availability of abundant high-quality event sequence data. In certain applications, however, clean structured event sequences are not available, and automated sequence extraction results in data that is too noisy and incomplete. In this work, we explore the use of Large Language Models (LLMs) to generate event sequences that can effectively be used for probabilistic event model construction. This can be viewed as a mechanism of distilling event sequence knowledge from LLMs. Our approach relies on a Knowledge Graph (KG) of event concepts with partial causal relations to guide the generative language model for causal event sequence generation. We show that our approach can generate high-quality event sequences, filling a knowledge gap in the input KG. Furthermore, we explore how the generated sequences can be leveraged to discover useful and more complex st
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#26029;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#24182;&#19981;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31616;&#21333;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#21363;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.14215</link><description>&lt;p&gt;
SimLM&#65306;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#25512;&#26029;&#29289;&#29702;&#31995;&#32479;&#30340;&#21442;&#25968;&#65311;
&lt;/p&gt;
&lt;p&gt;
SimLM: Can Language Models Infer Parameters of Physical Systems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#26029;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#24182;&#19981;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#26159;&#23545;&#20110;&#31616;&#21333;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#65292;&#21363;&#21033;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#32972;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#25110;&#25512;&#29702;&#22797;&#26434;&#30340;&#29289;&#29702;&#31995;&#32479;&#12290;&#25512;&#29702;&#30340;&#24120;&#35265;&#31532;&#19968;&#27493;&#26159;&#20174;&#31995;&#32479;&#34892;&#20026;&#30340;&#35266;&#23519;&#20013;&#25512;&#26029;&#31995;&#32479;&#21442;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29289;&#29702;&#31995;&#32479;&#19978;&#25191;&#34892;&#21442;&#25968;&#25512;&#26029;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#20204;&#24182;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#20351;&#23545;&#20110;&#31616;&#21333;&#30340;&#31995;&#32479;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#35813;&#26041;&#21521;&#28041;&#21450;&#21040;&#20351;&#29992;&#29289;&#29702;&#27169;&#25311;&#22120;&#26469;&#22686;&#24378;LLMs&#30340;&#32972;&#26223;&#12290;&#25105;&#20204;&#35780;&#20272;&#21644;&#27604;&#36739;&#20102;&#19981;&#21516;LLMs&#22312;&#19968;&#20010;&#31616;&#21333;&#30340;&#31034;&#20363;&#19978;&#30340;&#24615;&#33021;&#65292;&#26377;&#26080;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several machine learning methods aim to learn or reason about complex physical systems. A common first-step towards reasoning is to infer system parameters from observations of its behavior. In this paper, we investigate the performance of Large Language Models (LLMs) at performing parameter inference in the context of physical systems. Our experiments suggest that they are not inherently suited to this task, even for simple systems. We propose a promising direction of exploration, which involves the use of physical simulators to augment the context of LLMs. We assess and compare the performance of different LLMs on a simple example with and without access to physical simulation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LLMCompiler&#30340;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#25191;&#34892;&#20989;&#25968;&#26469;&#39640;&#25928;&#22320;&#21327;&#35843;&#22810;&#20010;&#20989;&#25968;&#35843;&#29992;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22810;&#20989;&#25968;&#35843;&#29992;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#39640;&#24310;&#36831;&#12289;&#39640;&#25104;&#26412;&#21644;&#19981;&#20934;&#30830;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.04511</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#24182;&#34892;&#20989;&#25968;&#35843;&#29992;&#30340;LLM&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
An LLM Compiler for Parallel Function Calling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LLMCompiler&#30340;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#24182;&#34892;&#25191;&#34892;&#20989;&#25968;&#26469;&#39640;&#25928;&#22320;&#21327;&#35843;&#22810;&#20010;&#20989;&#25968;&#35843;&#29992;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#22810;&#20989;&#25968;&#35843;&#29992;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#39640;&#24310;&#36831;&#12289;&#39640;&#25104;&#26412;&#21644;&#19981;&#20934;&#30830;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#22797;&#26434;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;LLM&#30340;&#25512;&#29702;&#33021;&#21147;&#20351;&#23427;&#20204;&#33021;&#22815;&#25191;&#34892;&#22806;&#37096;&#20989;&#25968;&#35843;&#29992;&#65292;&#20197;&#20811;&#26381;&#23427;&#20204;&#30340;&#22266;&#26377;&#23616;&#38480;&#65292;&#20363;&#22914;&#30693;&#35782;&#25130;&#26029;&#12289;&#31967;&#31957;&#30340;&#31639;&#26415;&#33021;&#21147;&#25110;&#26080;&#27861;&#35775;&#38382;&#31169;&#26377;&#25968;&#25454;&#12290;&#36825;&#19968;&#21457;&#23637;&#20351;&#24471;LLM&#33021;&#22815;&#22522;&#20110;&#19978;&#19979;&#25991;&#36873;&#25321;&#21644;&#21327;&#35843;&#22810;&#20010;&#20989;&#25968;&#65292;&#20197;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22810;&#20989;&#25968;&#35843;&#29992;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#20010;&#20989;&#25968;&#36827;&#34892;&#39034;&#24207;&#25512;&#29702;&#21644;&#25191;&#34892;&#65292;&#20174;&#32780;&#23548;&#33268;&#39640;&#24310;&#36831;&#12289;&#39640;&#25104;&#26412;&#21644;&#26377;&#26102;&#19981;&#20934;&#30830;&#30340;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#32534;&#35793;&#22120;&#65292;&#23427;&#22312;&#24182;&#34892;&#25191;&#34892;&#20989;&#25968;&#30340;&#21516;&#26102;&#39640;&#25928;&#22320;&#21327;&#35843;&#22810;&#20010;&#20989;&#25968;&#35843;&#29992;&#12290;&#20511;&#37492;&#32463;&#20856;&#32534;&#35793;&#22120;&#30340;&#21407;&#29702;&#65292;LLM&#32534;&#35793;&#22120;&#36890;&#36807;&#19977;&#20010;&#32452;&#20214;&#31616;&#21270;&#24182;&#34892;&#20989;&#25968;&#35843;&#29992;&#65306;&#65288;i&#65289;LLM&#35268;&#21010;&#22120;&#65292;&#21046;&#23450;&#25191;&#34892;&#35745;&#21010;&#65307;&#65288;ii&#65289;&#20219;&#21153;&#33719;&#21462;&#21333;&#20803;&#65292;&#20998;&#27966;&#20989;&#25968;&#35843;&#29992;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Recent language models have shown remarkable results on various complex reasoning benchmarks. The reasoning capabilities of LLMs enable them to execute external function calls to overcome their inherent limitations, such as knowledge cutoffs, poor arithmetic skills, or lack of access to private data. This development has allowed LLMs to select and coordinate multiple functions based on the context to tackle more complex problems. However, current methods for multiple function calling often require sequential reasoning and acting for each function which can result in high latency, cost, and sometimes inaccurate behavior. To address this, we introduce LLMCompiler, which executes functions in parallel to efficiently orchestrate multiple function calling. Drawing from the principles of classical compilers, LLMCompiler streamlines parallel function calling with three components: (i) an LLM Planner, formulating execution plans; (ii) a Task Fetching Unit, dispatching function calling tasks; a
&lt;/p&gt;</description></item><item><title>Sig-Networks&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#26399;&#35821;&#35328;&#24314;&#27169;&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#22522;&#20110;&#31614;&#21517;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#21442;&#25968;&#35843;&#33410;&#21644;&#33258;&#21160;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;PyTorch&#30340;&#26500;&#24314;&#22359;&#22312;&#26410;&#26469;&#30340;&#26550;&#26500;&#20013;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2312.03523</link><description>&lt;p&gt;
Sig-Networks&#24037;&#20855;&#21253;&#65306;&#38271;&#26399;&#35821;&#35328;&#24314;&#27169;&#30340;&#31614;&#21517;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sig-Networks Toolkit: Signature Networks for Longitudinal Language Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03523
&lt;/p&gt;
&lt;p&gt;
Sig-Networks&#26159;&#19968;&#20010;&#29992;&#20110;&#38271;&#26399;&#35821;&#35328;&#24314;&#27169;&#30340;&#24320;&#28304;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#38598;&#25104;&#20102;&#22522;&#20110;&#31614;&#21517;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#24182;&#22312;&#22810;&#20010;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#23427;&#25552;&#20379;&#20102;&#28789;&#27963;&#30340;&#21442;&#25968;&#35843;&#33410;&#21644;&#33258;&#21160;&#27169;&#22411;&#36873;&#25321;&#65292;&#24182;&#21487;&#20197;&#20316;&#20026;PyTorch&#30340;&#26500;&#24314;&#22359;&#22312;&#26410;&#26469;&#30340;&#26550;&#26500;&#20013;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#21487;&#36890;&#36807;pip&#23433;&#35013;&#30340;&#24037;&#20855;&#21253;&#65292;Sig-Networks&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#38271;&#26399;&#35821;&#35328;&#24314;&#27169;&#30340;&#24037;&#20855;&#21253;&#65292;&#20854;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#28857;&#26159;&#32435;&#20837;&#20102;&#22522;&#20110;&#31614;&#21517;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#26102;&#38388;&#30456;&#20851;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#25104;&#21151;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24212;&#29992;&#24182;&#25193;&#23637;&#20102;&#24050;&#21457;&#34920;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#19968;&#22871;&#23436;&#25972;&#30340;&#22522;&#20110;&#31614;&#21517;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#32452;&#20214;&#21487;&#20197;&#20316;&#20026;PyTorch&#26500;&#24314;&#22359;&#22312;&#26410;&#26469;&#30340;&#26550;&#26500;&#20013;&#20351;&#29992;&#12290;Sig-Networks&#25903;&#25345;&#26080;&#20851;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#25554;&#20214;&#65292;&#23545;&#20110;&#39034;&#24207;&#25968;&#25454;&#30340;&#26080;&#32541;&#39044;&#22788;&#29702;&#65292;&#21442;&#25968;&#28789;&#27963;&#24615;&#65292;&#20197;&#21450;&#22312;&#19968;&#31995;&#21015;&#27169;&#22411;&#20013;&#30340;&#33258;&#21160;&#35843;&#20248;&#12290;&#25105;&#20204;&#23558;&#31614;&#21517;&#32593;&#32476;&#24212;&#29992;&#20110;&#19977;&#20010;&#19981;&#21516;&#30340;NLP&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#19981;&#21516;&#30340;&#26102;&#38388;&#31890;&#24230;&#65306;&#21672;&#35810;&#23545;&#35805;&#12289;&#35875;&#35328;&#31435;&#22330;&#36716;&#25442;&#21644;&#31038;&#20132;&#23186;&#20307;&#20027;&#39064;&#20013;&#30340;&#24773;&#32490;&#21464;&#21270;&#65292;&#23637;&#31034;&#20102;&#22312;&#36825;&#19977;&#20010;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#20219;&#21153;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#25105;&#20204;&#20197;PyTorch&#21253;&#30340;&#24418;&#24335;&#21457;&#24067;&#20102;&#36825;&#20010;&#24037;&#20855;&#21253;&#65292;&#38468;&#26377;&#19968;&#20010;&#20171;&#32461;&#35270;&#39057;&#65292;&#21253;&#25324;&#39044;&#22788;&#29702;&#21644;&#24314;&#27169;&#30340;Git&#20179;&#24211;&#20197;&#21450;&#31034;&#20363;&#31508;&#35760;&#26412;
&lt;/p&gt;
&lt;p&gt;
We present an open-source, pip installable toolkit, Sig-Networks, the first of its kind for longitudinal language modelling. A central focus is the incorporation of Signature-based Neural Network models, which have recently shown success in temporal tasks. We apply and extend published research providing a full suite of signature-based models. Their components can be used as PyTorch building blocks in future architectures. Sig-Networks enables task-agnostic dataset plug-in, seamless pre-processing for sequential data, parameter flexibility, automated tuning across a range of models. We examine signature networks under three different NLP tasks of varying temporal granularity: counselling conversations, rumour stance switch and mood changes in social media threads, showing SOTA performance in all three, and provide guidance for future tasks. We release the Toolkit as a PyTorch package with an introductory video, Git repositories for preprocessing and modelling including sample notebooks
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#22330;&#26223;&#20013;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.03414</link><description>&lt;p&gt;
&#22312;&#32447;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#20013;&#30340;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
Compressed Context Memory For Online Language Model Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03414
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#22330;&#26223;&#20013;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#23454;&#29616;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#65292;&#25552;&#39640;&#21534;&#21520;&#37327;&#65292;&#24182;&#36890;&#36807;&#20010;&#24615;&#21270;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#22330;&#26223;&#20013;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#38190;/&#20540;&#21387;&#32553;&#26041;&#27861;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#19981;&#26029;&#25193;&#23637;&#12290;&#38543;&#30528;&#19978;&#19979;&#25991;&#30340;&#22686;&#21152;&#65292;&#27880;&#24847;&#21147;&#36807;&#31243;&#38656;&#35201;&#26356;&#22810;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#65292;&#36827;&#32780;&#38477;&#20302;&#35821;&#35328;&#27169;&#22411;&#30340;&#21534;&#21520;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21387;&#32553;&#19978;&#19979;&#25991;&#35760;&#24518;&#31995;&#32479;&#65292;&#23558;&#32047;&#31215;&#30340;&#27880;&#24847;&#21147;&#38190;/&#20540;&#23545;&#19981;&#26029;&#21387;&#32553;&#21040;&#32039;&#20945;&#30340;&#20869;&#23384;&#31354;&#38388;&#20013;&#65292;&#20197;&#20415;&#22312;&#35745;&#31639;&#29615;&#22659;&#30340;&#26377;&#38480;&#20869;&#23384;&#31354;&#38388;&#20013;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#12290;&#25105;&#20204;&#30340;&#21387;&#32553;&#36807;&#31243;&#28041;&#21450;&#23558;&#36731;&#37327;&#32423;&#30340;&#26465;&#20214;LoRA&#25972;&#21512;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;&#21069;&#21521;&#20256;&#36882;&#20013;&#36827;&#34892;&#25512;&#26029;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#27169;&#22411;&#30340;&#25152;&#26377;&#26435;&#37325;&#12290;&#36890;&#36807;&#23558;&#36882;&#24402;&#21387;&#32553;&#36807;&#31243;&#24314;&#27169;&#20026;&#21333;&#20010;&#24182;&#34892;&#21270;&#30340;&#21069;&#21521;&#35745;&#31639;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#23545;&#35805;&#12289;&#20010;&#24615;&#21270;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.01037</link><description>&lt;p&gt;
&#20174;&#21476;&#24618;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge from Quirky Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#19968;&#22871;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35843;&#21462;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#29305;&#23450;&#19978;&#19979;&#25991;&#20013;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#23637;&#31034;&#20102;&#20174;&#21487;&#20449;&#24230;&#20302;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30693;&#35782;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35843;&#21462;&#28508;&#22312;&#30693;&#35782;&#65288;ELK&#65289;&#26088;&#22312;&#22312;&#19968;&#20010;&#33021;&#21147;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#28608;&#27963;&#20013;&#25214;&#21040;&#27169;&#24335;&#65292;&#21363;&#20351;&#32593;&#32476;&#30340;&#26126;&#26174;&#36755;&#20986;&#26159;&#38169;&#35823;&#25110;&#35823;&#23548;&#24615;&#30340;&#65292;&#20063;&#33021;&#31283;&#23450;&#36319;&#36394;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;ELK&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;12&#20010;&#25968;&#25454;&#38598;&#21644;&#19968;&#22871;&#30456;&#24212;&#30340;&#8220;&#21476;&#24618;&#8221;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21482;&#26377;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#20851;&#38190;&#35789;&#8220;Bob&#8221;&#26102;&#25165;&#20250;&#36827;&#34892;&#31995;&#32479;&#24615;&#38169;&#35823;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31616;&#21333;&#30340;&#25506;&#27979;&#26041;&#27861;&#21487;&#20197;&#35843;&#21462;&#27169;&#22411;&#22312;&#36825;&#20123;&#19978;&#19979;&#25991;&#20013;&#23545;&#27491;&#30830;&#31572;&#26696;&#30340;&#28508;&#22312;&#30693;&#35782;&#65292;&#21363;&#20351;&#38382;&#39064;&#27604;&#25506;&#27979;&#22120;&#35757;&#32451;&#30340;&#38382;&#39064;&#26356;&#22256;&#38590;&#12290;&#36825;&#26159;&#30001;&#20110;&#20013;&#38388;&#23618;&#28608;&#27963;&#20013;&#30340;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;&#23384;&#22312;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19968;&#31181;&#26426;&#26800;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#20197;94%&#30340;AUROC&#26631;&#35782;&#19981;&#30495;&#23454;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#20174;&#33021;&#21147;&#24378;&#20294;&#19981;&#21463;&#20449;&#20219;&#30340;&#27169;&#22411;&#20013;&#35843;&#21462;&#21487;&#38752;&#30340;&#30693;&#35782;&#65292;&#24182;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;ELK&#26041;&#27861;&#30340;&#23454;&#35777;&#30740;&#31350;&#26159;&#26377;&#24076;&#26395;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations which robustly track the true state of the world, even when the network's overt output is false or misleading. To further ELK research, we introduce 12 datasets and a corresponding suite of "quirky" language models that are LoRA finetuned to make systematic errors when answering questions if and only if the keyword "Bob" is present in the prompt. We demonstrate that simple probing methods can elicit the model's latent knowledge of the correct answer in these contexts, even for problems harder than those the probe was trained on. This is enabled by context-independent knowledge representations located in middle layer activations. We also find that a mechanistic anomaly detection approach can flag untruthful behavior with 94% AUROC. Our results show promise for eliciting reliable knowledge from capable but untrusted models, and facilitates future research empirically investigating ELK methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24847;&#35782;&#21040;&#24187;&#35273;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26469;&#35299;&#20915;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#38754;&#23545;&#30456;&#21516;&#22270;&#20687;&#30340;&#20004;&#20010;&#21709;&#24212;&#26102;&#20559;&#22909;&#36873;&#25321;&#38750;&#24187;&#35273;&#24615;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#26500;&#24314;&#26679;&#26412;&#23545;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#38382;&#39064;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.16839</link><description>&lt;p&gt;
&#36229;&#36234;&#24187;&#35273;&#65306;&#36890;&#36807;&#24847;&#35782;&#21040;&#24187;&#35273;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22686;&#24378;LVLMs
&lt;/p&gt;
&lt;p&gt;
Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24847;&#35782;&#21040;&#24187;&#35273;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#26469;&#35299;&#20915;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#27169;&#22411;&#22312;&#38754;&#23545;&#30456;&#21516;&#22270;&#20687;&#30340;&#20004;&#20010;&#21709;&#24212;&#26102;&#20559;&#22909;&#36873;&#25321;&#38750;&#24187;&#35273;&#24615;&#30340;&#21709;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;&#26500;&#24314;&#26679;&#26412;&#23545;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#38382;&#39064;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#19968;&#31181;&#24120;&#35265;&#38382;&#39064;&#65292;&#21363;&#8220;&#24187;&#35273;&#38382;&#39064;&#8221;&#65292;&#21363;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#25551;&#36848;&#19981;&#20934;&#30830;&#22320;&#25551;&#32472;&#25110;&#23436;&#20840;&#25423;&#36896;&#30456;&#20851;&#22270;&#20687;&#30340;&#20869;&#23481;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;&#24187;&#35273;&#24863;&#30693;&#30340;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;HA-DPO&#65289;&#65292;&#23427;&#23558;&#24187;&#35273;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#20559;&#22909;&#36873;&#25321;&#20219;&#21153;&#12290;&#24403;&#27169;&#22411;&#38754;&#23545;&#20004;&#20010;&#30456;&#21516;&#22270;&#20687;&#30340;&#21709;&#24212;&#65288;&#19968;&#20010;&#20934;&#30830;&#30340;&#21644;&#19968;&#20010;&#24187;&#35273;&#30340;&#65289;&#26102;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20026;&#20542;&#21521;&#20110;&#36873;&#25321;&#38750;&#24187;&#35273;&#24615;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#26500;&#24314;&#27491;&#26679;&#26412;&#65288;&#38750;&#24187;&#35273;&#24615;&#65289;&#21644;&#36127;&#26679;&#26412;&#65288;&#24187;&#35273;&#24615;&#65289;&#23545;&#65292;&#30830;&#20445;&#20102;&#39640;&#36136;&#37327;&#12289;&#39118;&#26684;&#19968;&#33268;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#36827;&#34892;&#20581;&#22766;&#30340;&#20559;&#22909;&#23398;&#20064;&#12290;&#24403;&#24212;&#29992;&#20110;&#19977;&#31181;&#20027;&#27969;&#30340;&#22810;&#27169;&#24335;&#27169;&#22411;&#26102;&#65292;HA-DPO&#26174;&#33879;&#20943;&#23569;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal large language models have made significant advancements in recent years, yet they still suffer from a common issue known as the "hallucination problem", in which the models generate textual descriptions that inaccurately depict or entirely fabricate content from associated images. This paper introduces a novel solution, Hallucination-Aware Direct Preference Optimization (HA-DPO), which reframes the hallucination problem as a preference selection task. The model is trained to favor the non-hallucinating response when presented with two responses of the same image (one accurate and one hallucinatory). Furthermore, this paper proposes an efficient pipeline for constructing positive~(non-hallucinatory) and negative~(hallucinatory) sample pairs, ensuring a high-quality, style-consistent dataset for robust preference learning. When applied to three mainstream multimodal models, HA-DPO significantly reduced hallucination issues and amplified the models' generalization capabilities
&lt;/p&gt;</description></item><item><title>CodeScope&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2311.08588</link><description>&lt;p&gt;
CodeScope:&#19968;&#20010;&#22522;&#20110;&#25191;&#34892;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08588
&lt;/p&gt;
&lt;p&gt;
CodeScope&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#33021;&#21147;&#30340;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22810;&#32500;&#22522;&#20934;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22522;&#20934;&#22312;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#21644;&#22810;&#20219;&#21153;&#35774;&#32622;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32534;&#30721;&#30456;&#20851;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#24110;&#21161;&#20154;&#31867;&#32534;&#31243;&#21644;&#20419;&#36827;&#32534;&#31243;&#33258;&#21160;&#21270;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;LLMs&#30340;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#30340;&#22522;&#20934;&#23384;&#22312;&#20005;&#37325;&#30340;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#23384;&#22312;&#32570;&#38519;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#20851;&#27880;&#20110;&#29421;&#31364;&#33539;&#22260;&#20869;&#30340;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#21644;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#23454;&#38469;&#36719;&#20214;&#24320;&#21457;&#22330;&#26223;&#38656;&#35201;&#23454;&#29616;&#22810;&#35821;&#35328;&#32534;&#31243;&#29615;&#22659;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#12290;&#23454;&#38469;&#32534;&#31243;&#23454;&#36341;&#36824;&#24378;&#28872;&#26399;&#26395;&#22810;&#20219;&#21153;&#35774;&#32622;&#65292;&#20197;&#20840;&#38754;&#21644;&#31283;&#20581;&#22320;&#27979;&#35797;LLMs&#30340;&#32534;&#30721;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#22823;&#37096;&#20998;&#22522;&#20934;&#20063;&#26410;&#32771;&#34385;&#29983;&#25104;&#20195;&#30721;&#30340;&#21487;&#25191;&#34892;&#24615;&#21644;&#25191;&#34892;&#32467;&#26524;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#29616;&#26377;&#22522;&#20934;&#19982;&#23454;&#38469;&#24212;&#29992;&#26399;&#26395;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CodeScope&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable performance on coding related tasks, particularly on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are deficient as they focus on a narrow range of popular programming languages and specific tasks, whereas the real-world software development scenarios show dire need to implement systems with multilingual programming environments to satisfy diverse requirements. Practical programming practices also strongly expect multi-task settings for testing coding capabilities of LLMs comprehensively and robustly. Second, most benchmarks also fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce CodeScope
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#22270;&#20687;&#37096;&#20998;&#30693;&#35782;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;&#12290;</title><link>https://arxiv.org/abs/2307.16806</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#27979;&#35797;ChatGPT&#23545;&#20110;&#29702;&#35299;&#28145;&#24230;&#30340;&#33021;&#21147;&#65306;GPT3.5&#22312;&#35782;&#21035;&#21644;&#29983;&#25104;ASCII-Art&#26041;&#38754;&#30340;&#33021;&#21147;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;
&lt;/p&gt;
&lt;p&gt;
Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;ASCII-Art&#30340;&#36328;&#27169;&#24577;&#20219;&#21153;&#65292;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#22312;&#22270;&#20687;&#35782;&#21035;&#12289;&#22270;&#20687;&#37096;&#20998;&#30693;&#35782;&#21644;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#24182;&#19981;&#23436;&#20840;&#32570;&#20047;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21457;&#24067;&#21518;&#30340;&#20843;&#20010;&#26376;&#37324;&#65292;&#30001;&#20110;&#20854;&#24378;&#22823;&#30340;&#33021;&#21147;&#21644;&#26131;&#20110;&#20351;&#29992;&#65292;ChatGPT&#21450;&#20854;&#24213;&#23618;&#27169;&#22411;GPT3.5&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#19968;&#25209;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#33021;&#21147;&#33539;&#22260;&#30340;&#35770;&#25991;&#65292;&#20294;&#36825;&#20123;&#32593;&#32476;&#25152;&#25509;&#25910;&#21644;&#25552;&#21462;&#30340;&#20449;&#24687;&#35201;&#20040;&#26159;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#65292;&#35201;&#20040;&#26159;&#31867;&#20284;&#20195;&#30721;&#30340;&#39118;&#26684;&#21270;&#35821;&#35328;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#23545;&#19968;&#20010;&#30495;&#27491;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#22810;&#20010;&#20449;&#21495;&#27169;&#24577;&#19978;&#20855;&#22791;&#30340;&#33021;&#21147;&#30340;&#21551;&#31034;&#65292;&#32771;&#23519;&#20102;GPT3.5&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#36755;&#20837;&#20197;ASCII-Art&#24418;&#24335;&#25552;&#20379;&#20869;&#23481;&#65292;&#27809;&#26377;&#26126;&#26174;&#30340;&#35821;&#35328;&#21270;&#24635;&#32467;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#35813;&#27169;&#22411;&#22312;&#32463;&#36807;&#20856;&#22411;&#30340;&#35270;&#35273;&#35774;&#32622;&#19979;&#30340;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#20854;&#23545;&#22270;&#20687;&#37096;&#20998;&#30340;&#30693;&#35782;&#20197;&#21450;&#22270;&#20687;&#29983;&#25104;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the eight months since its release, ChatGPT and its underlying model, GPT3.5, have garnered massive attention, due to their potent mix of capability and accessibility. While a niche-industry of papers have emerged examining the scope of capabilities these models possess, the information fed to and extracted from these networks has been either natural language text or stylized, code-like language. Drawing inspiration from the prowess we expect a truly human-level intelligent agent to have across multiple signal modalities, in this work we examine GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary. We conduct experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings, trials investigating knowledge of image parts, and tasks covering image generation.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#22312;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#20294;&#23545;&#32454;&#31890;&#24230;&#38169;&#35823;&#35782;&#21035;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#12289;&#22270;&#20687;&#30456;&#20851;&#23545;&#35937;&#22823;&#23567;&#21464;&#21270;&#20197;&#21450;&#26631;&#39064;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#25935;&#24863;&#24615;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2305.14998</link><description>&lt;p&gt;
&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#30340;&#40065;&#26834;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Examination of the Robustness of Reference-Free Image Captioning Evaluation Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.14998
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#26080;&#21442;&#32771;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#22312;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#65292;&#32467;&#26524;&#21457;&#29616;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#24615;&#36739;&#39640;&#65292;&#20294;&#23545;&#32454;&#31890;&#24230;&#38169;&#35823;&#35782;&#21035;&#22256;&#38590;&#65292;&#24182;&#19988;&#22312;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#12289;&#22270;&#20687;&#30456;&#20851;&#23545;&#35937;&#22823;&#23567;&#21464;&#21270;&#20197;&#21450;&#26631;&#39064;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#26041;&#38754;&#23384;&#22312;&#25935;&#24863;&#24615;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#26080;&#21442;&#32771;&#25351;&#26631;&#65292;&#22914;CLIPScore&#65288;Hessel&#31561;&#65292;2021&#65289;&#65292;UMIC&#65288;Lee&#31561;&#65292;2021&#65289;&#21644;PAC-S&#65288;Sarto&#31561;&#65292;2023&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#26080;&#21442;&#32771;&#35780;&#20272;&#22270;&#20687;&#26631;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#35780;&#20272;&#36825;&#20123;&#25351;&#26631;&#22312;&#38656;&#35201;&#21306;&#20998;&#20855;&#26377;&#39640;&#35789;&#27719;&#37325;&#21472;&#20294;&#21547;&#20041;&#24046;&#24322;&#24456;&#22823;&#30340;&#20004;&#20010;&#26631;&#39064;&#30340;&#24773;&#20917;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#21028;&#26029;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;CLIPScore&#12289;UMIC&#21644;PAC-S&#24456;&#38590;&#35782;&#21035;&#32454;&#31890;&#24230;&#38169;&#35823;&#12290;&#34429;&#28982;&#25152;&#26377;&#25351;&#26631;&#23545;&#35270;&#35273;&#38169;&#35823;&#25935;&#24863;&#65292;&#20294;&#23545;&#26631;&#39064;&#19981;&#21512;&#29702;&#24615;&#38169;&#35823;&#30340;&#25935;&#24863;&#24615;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#25152;&#26377;&#25351;&#26631;&#23545;&#26631;&#39064;&#20013;&#25552;&#21450;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23545;&#35937;&#30340;&#22823;&#23567;&#21464;&#21270;&#25935;&#24863;&#65292;&#32780;CLIPScore&#21644;PAC-S&#23545;&#26631;&#39064;&#20013;&#25552;&#21450;&#30340;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#23545;&#35937;&#30340;&#25968;&#37327;&#20063;&#25935;&#24863;&#12290;&#20851;&#20110;&#26631;&#39064;&#30340;&#35821;&#35328;&#26041;&#38754;&#65292;&#25152;&#26377;&#25351;&#26631;&#23545;&#21542;&#23450;&#24847;&#20041;&#30340;&#29702;&#35299;&#33021;&#21147;&#36739;&#24369;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reference-free metrics such as CLIPScore (Hessel et al., 2021), UMIC (Lee et al., 2021), and PAC-S (Sarto et al., 2023) have been proposed for automatic reference-free evaluation of image captions. Our focus lies in evaluating the robustness of these metrics in scenarios that require distinguishing between two captions with high lexical overlap but very different meanings. Our findings reveal that despite their high correlation with human judgments, CLIPScore, UMIC, and PAC-S struggle to identify fine-grained errors. While all metrics exhibit strong sensitivity to visual grounding errors, their sensitivity to caption implausibility errors is limited. Furthermore, we found that all metrics are sensitive to variations in the size of image-relevant objects mentioned in the caption, while CLIPScore and PAC-S are also sensitive to the number of mentions of image-relevant objects in the caption. Regarding linguistic aspects of a caption, all metrics show weak comprehension of negat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20116;&#20010;&#23383;&#27597;&#21333;&#35789;&#30340;&#23383;&#31526;&#32479;&#35745;&#20449;&#24687;&#65292;&#36873;&#25321;&#20986;&#26368;&#20339;&#30340;&#19977;&#20010;&#21021;&#22987;&#35789;&#26469;&#35299;&#20915;Wordle&#28216;&#25103;&#12290;</title><link>https://arxiv.org/abs/2202.03457</link><description>&lt;p&gt;
&#20351;&#29992;&#23383;&#31526;&#32479;&#35745;&#36873;&#25321;Wordle&#30340;&#31181;&#23376;&#35789;
&lt;/p&gt;
&lt;p&gt;
Selecting Seed Words for Wordle using Character Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20116;&#20010;&#23383;&#27597;&#21333;&#35789;&#30340;&#23383;&#31526;&#32479;&#35745;&#20449;&#24687;&#65292;&#36873;&#25321;&#20986;&#26368;&#20339;&#30340;&#19977;&#20010;&#21021;&#22987;&#35789;&#26469;&#35299;&#20915;Wordle&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wordle&#26159;&#19968;&#27454;&#29468;&#35789;&#28216;&#25103;&#65292;&#22312;2022&#24180;1&#26376;&#20840;&#29699;&#29190;&#32418;&#12290;&#28216;&#25103;&#30340;&#30446;&#26631;&#26159;&#22312;&#20845;&#27425;&#23581;&#35797;&#20869;&#29468;&#20986;&#19968;&#20010;&#20116;&#20010;&#23383;&#27597;&#30340;&#33521;&#35821;&#21333;&#35789;&#12290;&#27599;&#27425;&#23581;&#35797;&#37117;&#20250;&#36890;&#36807;&#39068;&#33394;&#21464;&#21270;&#30340;&#26041;&#22359;&#32473;&#29609;&#23478;&#25552;&#20379;&#25552;&#31034;&#65292;&#21578;&#30693;&#19968;&#20010;&#32473;&#23450;&#30340;&#23383;&#31526;&#26159;&#21542;&#26159;&#35299;&#31572;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#21450;&#22312;&#26159;&#35299;&#31572;&#30340;&#19968;&#37096;&#20998;&#26102;&#65292;&#26159;&#21542;&#27491;&#30830;&#22320;&#25918;&#32622;&#22312;&#20854;&#20013;&#12290;&#24050;&#32463;&#26377;&#24456;&#22810;&#23581;&#35797;&#26469;&#25214;&#21040;&#35299;&#20915;&#27599;&#26085;Wordle&#30340;&#26368;&#20339;&#21021;&#22987;&#35789;&#21644;&#26368;&#20339;&#31574;&#30053;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20116;&#20010;&#23383;&#27597;&#21333;&#35789;&#30340;&#23383;&#31526;&#32479;&#35745;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#19977;&#20010;&#21021;&#22987;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wordle, a word guessing game rose to global popularity in the January of 2022. The goal of the game is to guess a five-letter English word within six tries. Each try provides the player with hints by means of colour changing tiles which inform whether or not a given character is part of the solution as well as, in cases where it is part of the solution, whether or not it is in the correct placement. Numerous attempts have been made to find the best starting word and best strategy to solve the daily wordle. This study uses character statistics of five-letter words to determine the best three starting words.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;&#20108;&#27425;&#30740;&#31350;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#35843;&#26597;&#20102;&#22522;&#20110;&#36719;&#20214;&#30340;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#36129;&#29486;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#31561;&#12290;&#24403;&#21069;&#32570;&#23569;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#23545;&#35805;&#20195;&#29702;&#30740;&#31350;&#32508;&#36848;&#12290;</title><link>https://arxiv.org/abs/2106.10901</link><description>&lt;p&gt;
&#22522;&#20110;&#36719;&#20214;&#30340;&#23545;&#35805;&#31995;&#32479;&#65306;&#35843;&#26597;&#12289;&#20998;&#31867;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Software-Based Dialogue Systems: Survey, Taxonomy and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2106.10901
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#23545;&#20108;&#27425;&#30740;&#31350;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#35843;&#26597;&#20102;&#22522;&#20110;&#36719;&#20214;&#30340;&#23545;&#35805;&#31995;&#32479;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#12290;&#36825;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#36129;&#29486;&#21253;&#25324;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#31561;&#12290;&#24403;&#21069;&#32570;&#23569;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#23545;&#35805;&#20195;&#29702;&#30740;&#31350;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#65292;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20351;&#29992;&#27491;&#21463;&#21040;&#19987;&#38376;&#30340;&#31185;&#23398;&#21644;&#24037;&#19994;&#30740;&#31350;&#30340;&#23494;&#20999;&#20851;&#27880;&#12290;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36129;&#29486;&#21253;&#25324;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#30340;&#28508;&#21147;&#21644;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#20877;&#27425;&#24341;&#36215;&#20102;&#31038;&#21306;&#30340;&#20851;&#27880;&#65292;&#36719;&#20214;&#23545;&#35805;&#31995;&#32479;&#36890;&#24120;&#34987;&#31216;&#20026;&#20250;&#35805;&#20195;&#29702;&#25110;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#37492;&#20110;&#35813;&#39046;&#22495;&#30340;&#26032;&#39062;&#24615;&#65292;&#32570;&#23569;&#28085;&#30422;&#25152;&#26377;&#30740;&#31350;&#35270;&#35282;&#30340;&#36890;&#29992;&#30340;&#12289;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#23545;&#35805;&#20195;&#29702;&#30740;&#31350;&#29616;&#29366;&#32508;&#36848;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#26412;&#25991;&#36890;&#36807;&#23545;&#20108;&#27425;&#30740;&#31350;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#25253;&#21578;&#20102;&#23545;&#35805;&#20195;&#29702;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#30340;&#35843;&#26597;&#12290;&#36827;&#34892;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#32858;&#21512;&#30693;&#35782;&#30340;&#28165;&#26224;&#21576;&#29616;&#65292;&#21457;&#23637;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of natural language interfaces in the field of human-computer interaction is undergoing intense study through dedicated scientific and industrial research. The latest contributions in the field, including deep learning approaches like recurrent neural networks, the potential of context-aware strategies and user-centred design approaches, have brought back the attention of the community to software-based dialogue systems, generally known as conversational agents or chatbots. Nonetheless, and given the novelty of the field, a generic, context-independent overview on the current state of research of conversational agents covering all research perspectives involved is missing. Motivated by this context, this paper reports a survey of the current state of research of conversational agents through a systematic literature review of secondary studies. The conducted research is designed to develop an exhaustive perspective through a clear presentation of the aggregated knowledge publish
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.13227</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38142;&#25509;&#39044;&#27979;&#30340;&#21487;&#25193;&#23637;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scalable Link Prediction on Large-Scale Heterogeneous Graphs with Large Language Models. (arXiv:2401.13227v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;LPNL&#26694;&#26550;&#29992;&#20110;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#35821;&#21644;&#37319;&#26679;&#27969;&#31243;&#65292;&#20197;&#21450;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#22270;&#20013;&#30340;&#20449;&#24687;&#36807;&#36733;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#23558;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26159;&#19968;&#39033;&#26032;&#39062;&#30340;&#21162;&#21147;&#12290;&#28982;&#32780;&#65292;&#22823;&#22270;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#20449;&#24687;&#32473;&#36825;&#19968;&#36807;&#31243;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#20171;&#32461;&#20102;LPNL&#65288;Link Prediction via Natural Language&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#19978;&#30340;&#21487;&#25193;&#23637;&#38142;&#25509;&#39044;&#27979;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#20197;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#22270;&#32454;&#33410;&#30340;&#21019;&#26032;&#25552;&#31034;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#37319;&#26679;&#27969;&#31243;&#65292;&#20174;&#22823;&#35268;&#27169;&#24322;&#26500;&#22270;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#37319;&#29992;&#20998;&#32780;&#27835;&#20043;&#30340;&#31574;&#30053;&#26469;&#25511;&#21046;&#36755;&#20837;&#20196;&#29260;&#25968;&#37327;&#22312;&#39044;&#23450;&#38480;&#21046;&#20869;&#65292;&#35299;&#20915;&#20102;&#20449;&#24687;&#36807;&#36733;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;T5&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#22823;&#22411;&#20844;&#20849;&#24322;&#26500;&#22270;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;LPNL&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21508;&#31181;&#20808;&#36827;&#30340;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploring the application of large-scale language models to graph learning is a novel endeavor. However, the vast amount of information inherent in large graphs poses significant challenges to this process. This paper focuses on the link prediction task and introduces LPNL (Link Prediction via Natural Language), a framework based on a large language model designed for scalable link prediction on large-scale heterogeneous graphs.We design novel prompts for link prediction that articulate graph details in natural language. We propose a two-stage sampling pipeline to extract crucial information from large-scale heterogeneous graphs, and a divide-and-conquer strategy to control the input token count within predefined limits, addressing the challenge of overwhelming information. We fine-tune a T5 model based on our self-supervised learning designed for for link prediction. Extensive experiments on a large public heterogeneous graphs demonstrate that LPNL outperforms various advanced baselin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.10463</link><description>&lt;p&gt;
&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
Critical Data Size of Language Models from a Grokking Perspective. (arXiv:2401.10463v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#29702;&#35299;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#35777;&#26126;&#20102;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20851;&#38190;&#25968;&#25454;&#35268;&#27169;&#65292;&#36825;&#26159;&#20174;&#24555;&#36895;&#35760;&#24518;&#21040;&#32531;&#24930;&#27867;&#21270;&#30340;&#19968;&#20010;&#22522;&#26412;&#36716;&#21464;&#38408;&#20540;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#30456;&#21464;&#24418;&#24335;&#21270;&#20026;Grokking&#37197;&#32622;&#19979;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#65292;&#24182;&#30830;&#23450;&#20102;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#21160;&#21147;&#23398;&#20013;&#30340;&#25968;&#25454;&#19981;&#36275;&#12289;&#20805;&#36275;&#21644;&#36807;&#21097;&#38454;&#27573;&#12290;&#25105;&#20204;&#36890;&#36807;&#37325;&#26032;&#35843;&#25972;&#21021;&#22987;&#21270;&#21644;&#26435;&#37325;&#34928;&#20943;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;Grokking&#37197;&#32622;&#65292;&#31283;&#23450;&#22320;&#22312;&#31616;&#21270;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#37325;&#29616;&#20102;Grokking&#12290;&#25105;&#20204;&#34920;&#26126;&#21482;&#26377;&#24403;&#35821;&#35328;&#27169;&#22411;&#36798;&#21040;&#20851;&#38190;&#22823;&#23567;&#26102;&#25165;&#20250;&#21457;&#29983;&#27867;&#21270;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#26679;&#26412;&#32423;&#21644;&#27169;&#22411;&#32423;&#30340;Grokking&#65292;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#25968;&#25454;&#25928;&#29575;&#20551;&#35828;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#35821;&#35328;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#25968;&#25454;&#38598;&#22823;&#23567;&#22788;&#21457;&#29983;&#30340;&#26356;&#24179;&#28369;&#30340;&#30456;&#21464;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#36825;&#20010;&#20020;&#30028;&#28857;&#20063;&#21464;&#24471;&#26356;&#22823;&#65292;&#36825;&#34920;&#26126;&#26356;&#22823;&#30340;&#27169;&#22411;&#38656;&#35201;&#26356;&#22810;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#21152;&#28145;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#30340;&#29702;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the critical data size in language models, a threshold that marks a fundamental shift from quick memorization to slow generalization. We formalize the phase transition under the grokking configuration into the Data Efficiency Hypothesis and identify data insufficiency, sufficiency, and surplus regimes in language models training dynamics. We develop a grokking configuration to reproduce grokking on simplistic language models stably by rescaling initialization and weight decay. We show that generalization occurs only when language models reach a critical size. We analyze grokking across sample-wise and model-wise, verifying the proposed data efficiency hypothesis. Our experiments reveal smoother phase transitions occurring at the critical dataset size for language datasets. As the model size increases, this critical point also becomes larger, indicating that larger models require more data. Our results deepen the understanding of language model training, offering a novel pers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06118</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#36890;&#36807;&#21152;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Extreme Compression of Large Language Models via Additive Quantization. (arXiv:2401.06118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#26497;&#31471;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#26032;&#25216;&#26415;&#65292;&#22312;&#32473;&#23450;&#30340;&#21387;&#32553;&#39044;&#31639;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#24320;&#28304;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#20986;&#29616;&#24341;&#21457;&#20102;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#37327;&#21270;&#25216;&#26415;&#30340;&#31454;&#36187;&#65292;&#20174;&#32780;&#20351;&#20854;&#33021;&#22815;&#22312;&#26368;&#32456;&#29992;&#25143;&#35774;&#22791;&#19978;&#25191;&#34892;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#22810;&#30721;&#26412;&#37327;&#21270;(MCQ)&#30340;&#32463;&#20856;&#26041;&#27861;&#35282;&#24230;&#37325;&#26032;&#24605;&#32771;&#20102;&#8220;&#26497;&#31471;&#8221;LLM&#21387;&#32553;&#30340;&#38382;&#39064;&#65292;&#21363;&#38024;&#23545;&#38750;&#24120;&#20302;&#30340;&#20301;&#25968;&#65292;&#20363;&#22914;&#27599;&#20010;&#21442;&#25968;2&#21040;3&#20301;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24314;&#31435;&#22312;&#21152;&#24615;&#37327;&#21270;&#36825;&#19968;&#32463;&#20856;&#31639;&#27861;&#20043;&#19978;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#37327;&#21270;&#12290;&#30001;&#27492;&#24471;&#21040;&#30340;&#31639;&#27861;&#22312;LLM&#21387;&#32553;&#26041;&#38754;&#25512;&#36827;&#20102;&#26368;&#26032;&#25216;&#26415;&#65292;&#20197;&#32473;&#23450;&#21387;&#32553;&#39044;&#31639;&#30340;&#20934;&#30830;&#24615;&#32780;&#35328;&#65292;&#20248;&#20110;&#25152;&#26377;&#26368;&#36817;&#25552;&#20986;&#30340;&#25216;&#26415;&#12290;&#20363;&#22914;&#65292;&#24403;&#23558;Llama 2&#27169;&#22411;&#21387;&#32553;&#21040;&#27599;&#20010;&#21442;&#25968;2&#20301;&#26102;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;7B&#27169;&#22411;&#37327;&#21270;&#20026;6.93&#22256;&#24785;&#24230;(&#30456;&#23545;&#20110;&#20043;&#21069;&#26368;&#20339;&#24037;&#20316;&#25913;&#36827;1.29&#65292;&#30456;&#23545;&#20110;FP16&#25913;&#36827;1.81)&#65292;13B&#27169;&#22411;&#37327;&#21270;&#20026;5.70&#22256;&#24785;&#24230;(&#25913;&#36827;0.36)&#65292;70B&#27169;&#22411;&#37327;&#21270;&#20026;3.94&#22256;&#24785;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of "extreme" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#33539;&#24335;&#65292;&#36890;&#36807;&#25361;&#25112;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20803;&#25512;&#29702;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#23427;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#19968;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#33021;&#22815;&#25581;&#31034;&#20986;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#21457;&#29616;&#30340;&#27169;&#22411;&#30340;&#28508;&#22312;&#35748;&#30693;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2312.17080</link><description>&lt;p&gt;
MR-GSM8K: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#30340;&#20803;&#25512;&#29702;&#38761;&#21629;
&lt;/p&gt;
&lt;p&gt;
MR-GSM8K: A Meta-Reasoning Revolution in Large Language Model Evaluation. (arXiv:2312.17080v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17080
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#33539;&#24335;&#65292;&#36890;&#36807;&#25361;&#25112;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#20803;&#25512;&#29702;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;&#23427;&#20204;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#36825;&#19968;&#33539;&#24335;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#33021;&#22815;&#25581;&#31034;&#20986;&#20256;&#32479;&#22522;&#20934;&#27979;&#35797;&#26080;&#27861;&#21457;&#29616;&#30340;&#27169;&#22411;&#30340;&#28508;&#22312;&#35748;&#30693;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#33539;&#24335;&#65292;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36825;&#31181;&#33539;&#24335;&#25361;&#25112;&#23427;&#20204;&#20174;&#20107;&#20803;&#25512;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#30340;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;&#22522;&#20934;&#20013;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#20256;&#32479;&#19978;&#29992;&#20110;&#35780;&#20272;&#26234;&#33021;&#20307;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#33539;&#24335;&#23558;&#28966;&#28857;&#20174;&#20197;&#32467;&#26524;&#20026;&#23548;&#21521;&#30340;&#35780;&#20272;&#36716;&#31227;&#21040;&#20102;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#27169;&#22411;&#20043;&#38388;&#30340;&#35748;&#30693;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;GPT-4 &#30340;&#24615;&#33021;&#36739; GPT3-5 &#25552;&#21319;&#20102;&#20116;&#20493;&#12290;&#36825;&#31181;&#26032;&#33539;&#24335;&#30340;&#37325;&#35201;&#24847;&#20041;&#22312;&#20110;&#23427;&#33021;&#22815;&#25581;&#31034;&#20986;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65288;&#22914;GSM8K&#65289;&#26080;&#27861;&#21457;&#29616;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#35748;&#30693;&#32570;&#38519;&#65292;&#36825;&#26159;&#30001;&#20110;&#22522;&#20934;&#27979;&#35797;&#30340;&#39281;&#21644;&#24230;&#21644;&#23545;&#19981;&#21516;&#25512;&#29702;&#33021;&#21147;&#30340;&#26377;&#25928;&#21306;&#20998;&#19981;&#36275;&#12290;&#25105;&#20204;&#30340;&#32508;&#21512;&#20998;&#26512;&#21253;&#25324;&#20102;&#26469;&#33258;&#24320;&#28304;&#21644;&#38381;&#28304;&#31038;&#21306;&#30340;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35748;&#30693;&#33021;&#21147;&#30340;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a novel evaluation paradigm for Large Language Models, one that challenges them to engage in meta-reasoning. This approach addresses critical shortcomings in existing math problem-solving benchmarks, traditionally used to evaluate the cognitive capabilities of agents. Our paradigm shifts the focus from result-oriented assessments, which often overlook the reasoning process, to a more holistic evaluation that effectively differentiates the cognitive capabilities among models. For example, in our benchmark, GPT-4 demonstrates a performance five times better than GPT3-5. The significance of this new paradigm lies in its ability to reveal potential cognitive deficiencies in LLMs that current benchmarks, such as GSM8K, fail to uncover due to their saturation and lack of effective differentiation among varying reasoning abilities. Our comprehensive analysis includes several state-of-the-art math models from both open-source and closed-source communities, uncovering
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2312.11509</link><description>&lt;p&gt;
&#38754;&#21521;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#33647;&#29289;&#35843;&#25972;&#31995;&#32479;&#20197;&#20943;&#23569;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#35770;&#25991;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Toward A Reinforcement-Learning-Based System for Adjusting Medication to Minimize Speech Disfluency. (arXiv:2312.11509v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.11509
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#26681;&#25454;&#24739;&#32773;&#35328;&#35821;&#19981;&#27969;&#30021;&#31243;&#24230;&#33258;&#21160;&#35843;&#25972;&#33647;&#29289;&#65292;&#36890;&#36807;&#23545;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#20248;&#21270;&#65292;&#33021;&#22815;&#25910;&#25947;&#21040;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#20026;&#24739;&#26377;&#19982;&#24515;&#29702;&#20581;&#24247;&#30456;&#20851;&#30340;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#34394;&#25311;&#24739;&#32773;&#24320;&#20855;&#33647;&#29289;&#22788;&#26041;&#65292;&#24182;&#26681;&#25454;&#38646;&#25104;&#26412;&#39057;&#32321;&#27979;&#37327;&#32467;&#26524;&#65292;&#35843;&#25972;&#33647;&#29289;&#21644;&#21058;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#19968;&#20010;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#26816;&#27979;&#21644;&#35780;&#20272;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#25214;&#21040;&#33391;&#22909;&#33647;&#29289;&#32452;&#21512;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#20004;&#20010;&#27169;&#22359;&#65292;&#25105;&#20204;&#20174;&#25991;&#29486;&#20013;&#25910;&#38598;&#20102;&#20851;&#20110;&#33647;&#29289;&#27835;&#30103;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#25928;&#26524;&#30340;&#25968;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21487;&#20449;&#30340;&#24739;&#32773;&#27169;&#25311;&#31995;&#32479;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#25910;&#25947;&#21040;&#19968;&#20010;&#33391;&#22909;&#30340;&#29992;&#33647;&#26041;&#26696;&#12290;&#25105;&#20204;&#25910;&#38598;&#24182;&#23545;&#21487;&#33021;&#23384;&#22312;&#35328;&#35821;&#19981;&#27969;&#30021;&#30340;&#20154;&#32676;&#36827;&#34892;&#20102;&#25968;&#25454;&#26631;&#27880;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;:
&lt;/p&gt;
&lt;p&gt;
We propose a Reinforcement-Learning-based system that would automatically prescribe a hypothetical patient medication that may help the patient with their mental-health-related speech disfluency, and adjust the medication and the dosages in response to zero-cost frequent measurement of the fluency of the patient. We demonstrate the components of the system: a module that detects and evaluates speech disfluency on a large dataset we built, and a Reinforcement Learning algorithm that automatically finds good combinations of medications. To support the two modules, we collect data on the effect of psychiatric medications for speech disfluency from the literature, and build a plausible patient simulation system. We demonstrate that the Reinforcement Learning system is, under some circumstances, able to converge to a good medication regime. We collect and label a dataset of people with possible speech disfluency and demonstrate our methods using that dataset. Our work is a proof of concept:
&lt;/p&gt;</description></item><item><title>TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.08846</link><description>&lt;p&gt;
TiMix: &#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#29992;&#20110;&#26377;&#25928;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TiMix: Text-aware Image Mixing for Effective Vision-Language Pre-training. (arXiv:2312.08846v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.08846
&lt;/p&gt;
&lt;p&gt;
TiMix&#26159;&#19968;&#31181;&#23558;&#25991;&#26412;&#24863;&#30693;&#30340;&#22270;&#20687;&#28151;&#21512;&#25216;&#26415;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#28151;&#21512;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#24182;&#20174;&#20114;&#20449;&#24687;&#30340;&#35282;&#24230;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#24182;&#21462;&#24471;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#65288;SMCL&#65289;&#36890;&#36807;&#23545;&#40784;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#24577;&#65292;&#26174;&#33879;&#25512;&#36827;&#20102;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32593;&#32476;&#25910;&#38598;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20013;&#23384;&#22312;&#22122;&#22768;&#65292;&#25193;&#22823;SMCL&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#38754;&#20020;&#30528;&#30456;&#24403;&#22823;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#25552;&#39640;VLP&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25991;&#26412;&#24863;&#30693;&#22270;&#20687;&#28151;&#21512;&#65288;TiMix&#65289;&#65292;&#23558;&#22522;&#20110;&#28151;&#21512;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#38598;&#25104;&#21040;SMCL&#20013;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#65292;&#32780;&#19981;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#20174;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#30340;&#35282;&#24230;&#23545;TiMix&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#34920;&#26126;&#36328;&#27169;&#24577;&#23545;&#27604;&#23398;&#20064;&#30340;&#28151;&#21512;&#25968;&#25454;&#26679;&#26412;&#38544;&#24335;&#22320;&#20316;&#20026;&#23545;&#27604;&#25439;&#22833;&#30340;&#27491;&#21017;&#21270;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#36739;&#30701;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;TiMix&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Multi-modal Contrastive Learning (SMCL) remarkably advances modern Vision-Language Pre-training (VLP) models by aligning visual and linguistic modalities. Due to noises in web-harvested text-image pairs, however, scaling up training data volume in SMCL presents considerable obstacles in terms of computational cost and data inefficiency. To improve data efficiency in VLP, we propose Text-aware Image Mixing (TiMix), which integrates mix-based data augmentation techniques into SMCL, yielding significant performance improvements without significantly increasing computational overhead. We provide a theoretical analysis of TiMixfrom a mutual information (MI) perspective, showing that mixed data samples for cross-modal contrastive learning implicitly serve as a regularizer for the contrastive loss. The experimental results demonstrate that TiMix exhibits a comparable performance on downstream tasks, even with a reduced amount of training data and shorter training time, when be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2311.04076</link><description>&lt;p&gt;
LLMs&#26159;&#21542;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#20506;&#65311;&#19968;&#39033;&#20851;&#20110;&#35843;&#26597;&#35774;&#35745;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do LLMs exhibit human-like response biases? A case study in survey design. (arXiv:2311.04076v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04076
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;LLMs&#26159;&#21542;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22686;&#24378;&#65292;&#20154;&#20204;&#23545;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#20154;&#31867;&#36827;&#34892;&#20027;&#35266;&#26631;&#31614;&#20219;&#21153;&#65288;&#22914;&#35843;&#26597;&#21644;&#33286;&#35770;&#35843;&#26597;&#65289;&#30340;&#21487;&#33021;&#24615;&#36234;&#26469;&#36234;&#20852;&#22859;&#12290;&#28982;&#32780;&#65292;LLMs&#23545;&#25552;&#31034;&#25514;&#36766;&#30340;&#25935;&#24863;&#24615;&#26159;&#20854;&#24191;&#27867;&#24341;&#36848;&#30340;&#38480;&#21046;&#20043;&#19968;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#20154;&#31867;&#22312;&#22238;&#24212;&#20013;&#20063;&#26174;&#31034;&#20986;&#23545;&#25351;&#20196;&#21464;&#21270;&#30340;&#25935;&#24863;&#24615;&#65292;&#34920;&#29616;&#20026;&#21453;&#24212;&#20559;&#20506;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#35201;&#20351;&#29992;LLMs&#36817;&#20284;&#20154;&#31867;&#24847;&#35265;&#65292;&#26377;&#24517;&#35201;&#35843;&#26597;LLMs&#26159;&#21542;&#20063;&#21453;&#26144;&#20102;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20197;&#35843;&#26597;&#35774;&#35745;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#35843;&#26597;&#38382;&#21367;&#20013;&#30001;&#20110;&#8220;&#25552;&#31034;&#8221;&#25514;&#36766;&#30340;&#21464;&#21270;&#23548;&#33268;&#30340;&#20154;&#31867;&#21453;&#24212;&#20559;&#24046;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#20511;&#37492;&#31038;&#20250;&#24515;&#29702;&#23398;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#20197;&#35780;&#20272;LLMs&#26159;&#21542;&#22312;&#35843;&#26597;&#38382;&#21367;&#20013;&#23637;&#29616;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#21453;&#24212;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. As such, we argue that if LLMs are going to be used to approximate human opinions, it is necessary to investigate the extent to which LLMs also reflect human response biases, if at all. In this work, we use survey design as a case study, where human response biases caused by permutations in wordings of "prompts" have been extensively studied. Drawing from prior work in social psychology, we design a dataset and propose a framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00768</link><description>&lt;p&gt;
&#29992;&#20110;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Language Model Training Paradigms for Clinical Feature Embeddings. (arXiv:2311.00768v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#34920;&#31034;&#23398;&#20064;&#20026;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#25512;&#23548;&#20986;&#39640;&#36136;&#37327;&#30340;&#36890;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#12290;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#34920;&#31034;&#23398;&#20064;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23545;&#20020;&#24202;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#25512;&#23548;&#20986;&#20020;&#24202;&#29305;&#24449;&#65288;&#22914;&#24515;&#29575;&#21644;&#34880;&#21387;&#65289;&#30340;&#36890;&#29992;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#35757;&#32451;&#33539;&#24335;&#65292;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#65292;&#23454;&#29616;&#27604;&#29616;&#26377;&#30340;&#26102;&#38388;&#27493;&#21644;&#24739;&#32773;&#32423;&#21035;&#34920;&#31034;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#34920;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#38477;&#32500;&#25216;&#26415;&#21487;&#35270;&#21270;&#23398;&#20064;&#21040;&#30340;&#23884;&#20837;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#20808;&#21069;&#30340;&#20020;&#24202;&#30693;&#35782;&#39640;&#24230;&#19968;&#33268;&#12290;&#25105;&#20204;&#36824;&#22312;MIMIC-III&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#20020;&#24202;&#29305;&#24449;&#23884;&#20837;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#21457;&#24067;&#22312;&#32593;&#19978;&#20197;&#20379;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In research areas with scarce data, representation learning plays a significant role. This work aims to enhance representation learning for clinical time series by deriving universal embeddings for clinical features, such as heart rate and blood pressure. We use self-supervised training paradigms for language models to learn high-quality clinical feature embeddings, achieving a finer granularity than existing time-step and patient-level representation learning. We visualize the learnt embeddings via unsupervised dimension reduction techniques and observe a high degree of consistency with prior clinical knowledge. We also evaluate the model performance on the MIMIC-III benchmark and demonstrate the effectiveness of using clinical feature embeddings. We publish our code online for replication.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.18168</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30495;&#23454;&#24615;
&lt;/p&gt;
&lt;p&gt;
Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#20154;&#35774;&#26469;&#24314;&#27169;&#30495;&#23454;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#29305;&#24449;&#21028;&#26029;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#20114;&#32852;&#32593;&#19978;&#30340;&#22823;&#37327;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#25991;&#26412;&#20013;&#26082;&#21253;&#21547;&#20102;&#20107;&#23454;&#65292;&#20063;&#21253;&#21547;&#20102;&#35823;&#23548;&#24615;&#30340;&#20449;&#24687;&#12290;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20174;&#36825;&#20123;&#30456;&#20114;&#30683;&#30462;&#30340;&#25968;&#25454;&#20013;&#36776;&#21035;&#30495;&#23454;&#19982;&#34394;&#20551;&#21527;&#65311;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24314;&#27169;&#19981;&#21516;&#20135;&#29983;&#25991;&#26412;&#30340;&#20010;&#20307;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#24314;&#27169;&#30495;&#23454;&#20154;&#35774;&#26469;&#32858;&#31867;&#30495;&#23454;&#25991;&#26412;&#65306;&#19968;&#32676;&#24456;&#21487;&#33021;&#20135;&#29983;&#30495;&#23454;&#25991;&#26412;&#24182;&#20855;&#26377;&#30456;&#20284;&#29305;&#24449;&#30340;&#20010;&#20307;&#12290;&#20363;&#22914;&#65292;&#21487;&#20449;&#28304;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#31185;&#23398;&#26399;&#21002;&#36890;&#24120;&#20351;&#29992;&#27491;&#24335;&#30340;&#20889;&#20316;&#39118;&#26684;&#24182;&#25552;&#20986;&#19968;&#33268;&#30340;&#20027;&#24352;&#12290;&#36890;&#36807;&#24314;&#27169;&#36825;&#19968;&#20154;&#35774;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23558;&#30495;&#23454;&#24615;&#25512;&#24191;&#21040;&#27599;&#20010;&#20010;&#20307;&#29983;&#25104;&#35757;&#32451;&#25991;&#26412;&#30340;&#29305;&#23450;&#19978;&#19979;&#25991;&#20043;&#22806;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#21487;&#20197;&#25512;&#26029;&#20986;&#8220;&#32500;&#22522;&#30334;&#31185;&#8221;&#36825;&#20010;&#20010;&#20307;&#22312;&#8220;&#31185;&#23398;&#8221;&#29983;&#25104;&#30340;&#20027;&#39064;&#19978;&#20250;&#34920;&#29616;&#20986;&#30495;&#23454;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20849;&#20139;&#19968;&#20010;&#20154;&#35774;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#20004;&#20010;&#35266;&#23519;&#32467;&#26524;&#20026;&#20154;&#35774;&#20551;&#35774;&#25552;&#20379;&#20102;&#35777;&#25454;&#65306;&#65288;1&#65289;&#25105;&#20204;&#21487;&#20197;&#25506;&#27979;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21028;&#26029;&#30495;&#23454;&#24615;&#30340;&#33021;&#21147;&#65307;&#65288;2&#65289;&#27169;&#22411;&#21487;&#20197;&#20174;&#30456;&#20851;&#29305;&#24449;&#20013;&#25512;&#27979;&#20010;&#20307;&#20135;&#29983;&#25991;&#26412;&#30340;&#30495;&#23454;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are trained on vast amounts of text from the internet, which contains both factual and misleading information about the world. Can language models discern truth from falsehood in this contradicting data? Expanding on the view that LLMs can model different agents producing the corpora, we hypothesize that they can cluster truthful text by modeling a truthful persona: a group of agents that are likely to produce truthful text and share similar features. For example, trustworthy sources like Wikipedia and Science usually use formal writing styles and make consistent claims. By modeling this persona, LLMs can generalize truthfulness beyond the specific contexts in which each agent generated the training text. For example, the model can infer that the agent "Wikipedia" will behave truthfully on topics that were only generated by "Science" because they share a persona. We first show evidence for the persona hypothesis via two observations: (1) we can probe whether a mod
&lt;/p&gt;</description></item><item><title>O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2310.14403</link><description>&lt;p&gt;
O3D: &#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#21457;&#29616;&#19982;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14403
&lt;/p&gt;
&lt;p&gt;
O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#20223;&#25552;&#31034;&#20013;&#25552;&#20379;&#30340;&#23569;&#37327;&#31034;&#20363;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#19982;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#24182;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23569;&#37327;&#31034;&#20363;&#24448;&#24448;&#19981;&#36275;&#20197;&#29983;&#25104;&#22797;&#26434;&#19988;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26080;&#27861;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#31163;&#32447;&#25968;&#25454;&#65288;&#20363;&#22914;&#20154;&#31867;&#20132;&#20114;&#30340;&#26085;&#24535;&#65289;&#26469;&#25913;&#36827;LLM&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#21644;&#20195;&#30721;&#20004;&#31181;&#26041;&#27861;&#27491;&#24335;&#23450;&#20041;&#20102;LLM&#24378;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;O3D&#30340;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;LLM&#24378;&#21270;&#31574;&#30053;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;O3D&#33258;&#21160;&#22320;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.07818</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#31867;&#27604;&#35782;&#21035;&#19982;&#21477;&#23376;&#32467;&#26500;&#32534;&#30721;&#20043;&#38388;&#30340;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07818
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#31867;&#27604;&#22312;&#20154;&#31867;&#35748;&#30693;&#21644;&#35821;&#35328;&#33021;&#21147;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#37324;&#65292;&#23545;&#20110;&#8220;A&#23545;B&#23601;&#20687;C&#23545;D&#8221;&#36825;&#31181;&#24418;&#24335;&#30340;&#35789;&#35821;&#31867;&#27604;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#28041;&#21450;&#26356;&#38271;&#25991;&#26412;&#30340;&#31867;&#27604;&#65292;&#22914;&#21477;&#23376;&#21644;&#21477;&#23376;&#38598;&#21512;&#65292;&#20256;&#36798;&#31867;&#27604;&#24847;&#20041;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20852;&#36259;&#12290;&#24403;&#21069;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#31038;&#21306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#35782;&#21035;&#27492;&#31867;&#31867;&#27604;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#33021;&#21147;&#32972;&#21518;&#30340;&#21407;&#22240;&#38656;&#35201;&#36827;&#19968;&#27493;&#25506;&#31350;&#12290;&#27492;&#22806;&#65292;LLMs&#22312;&#20854;&#23884;&#20837;&#20013;&#32534;&#30721;&#35821;&#35328;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#20010;LLMs&#35782;&#21035;&#21477;&#23376;&#31867;&#27604;&#30340;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#21477;&#27861;&#21644;&#35821;&#20041;&#32467;&#26500;&#30340;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#31867;&#27604;&#35782;&#21035;&#33021;&#21147;&#19982;&#20854;&#32534;&#30721;&#33021;&#21147;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Identifying analogies plays a pivotal role in human cognition and language proficiency. In the last decade, there has been extensive research on word analogies in the form of ``A is to B as C is to D.'' However, there is a growing interest in analogies that involve longer text, such as sentences and collections of sentences, which convey analogous meanings. While the current NLP research community evaluates the ability of Large Language Models (LLMs) to identify such analogies, the underlying reasons behind these abilities warrant deeper investigation. Furthermore, the capability of LLMs to encode both syntactic and semantic structures of language within their embeddings has garnered significant attention with the surge in their utilization. In this work, we examine the relationship between the abilities of multiple LLMs to identify sentence analogies, and their capacity to encode syntactic and semantic structures. Through our analysis, we find that analogy identification ability of LL
&lt;/p&gt;</description></item><item><title>OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02031</link><description>&lt;p&gt;
OceanGPT&#65306;&#29992;&#20110;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
OceanGPT: A Large Language Model for Ocean Science Tasks. (arXiv:2310.02031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02031
&lt;/p&gt;
&lt;p&gt;
OceanGPT&#26159;&#39318;&#20010;&#19987;&#20026;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#35774;&#35745;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;DoInstruct&#26694;&#26550;&#23454;&#29616;&#33258;&#21160;&#33719;&#21462;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#12290;&#36825;&#19968;&#27169;&#22411;&#30340;&#24341;&#20837;&#22635;&#34917;&#20102;&#28023;&#27915;&#31185;&#23398;&#39046;&#22495;&#20013;&#23545;LLM&#30340;&#38656;&#27714;&#32570;&#21475;&#65292;&#24182;&#20026;&#28023;&#27915;&#31185;&#23398;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#27915;&#31185;&#23398;&#26159;&#25506;&#32034;&#20805;&#28385;&#29983;&#21629;&#21644;&#29983;&#29289;&#22810;&#26679;&#24615;&#30340;&#28023;&#27915;&#30340;&#31185;&#23398;&#65292;&#32771;&#34385;&#21040;&#28023;&#27915;&#35206;&#30422;&#20102;&#22320;&#29699;&#34920;&#38754;&#30340;70&#65285;&#20197;&#19978;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#25913;&#21464;&#20102;&#31185;&#23398;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#22312;&#20854;&#20182;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29616;&#26377;&#30340;LLM&#36890;&#24120;&#26080;&#27861;&#28385;&#36275;&#28023;&#27915;&#23398;&#23478;&#31561;&#39046;&#22495;&#19987;&#23478;&#30340;&#38656;&#27714;&#65292;&#21516;&#26102;&#23545;LLM&#22312;&#28023;&#27915;&#31185;&#23398;&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#36825;&#20854;&#20013;&#30340;&#26681;&#26412;&#21407;&#22240;&#21487;&#33021;&#26159;&#28023;&#27915;&#25968;&#25454;&#30340;&#24222;&#22823;&#32780;&#22797;&#26434;&#30340;&#24615;&#36136;&#65292;&#20197;&#21450;&#23545;&#26356;&#39640;&#30340;&#31890;&#24230;&#21644;&#20016;&#23500;&#30340;&#30693;&#35782;&#30340;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#39318;&#20010;&#28023;&#27915;&#39046;&#22495;&#30340;LLM&#8212;&#8212;OceanGPT&#65292;&#35813;&#27169;&#22411;&#25797;&#38271;&#21508;&#31181;&#28023;&#27915;&#31185;&#23398;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;DoInstruct&#65292;&#29992;&#20110;&#33258;&#21160;&#33719;&#21462;&#22823;&#37327;&#30340;&#28023;&#27915;&#39046;&#22495;&#25351;&#23548;&#25968;&#25454;&#65292;&#23427;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#21327;&#20316;&#29983;&#25104;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet's surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#22120;(ALMA)&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2309.11674</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#65306;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
A Paradigm Shift in Machine Translation: Boosting Translation Performance of Large Language Models. (arXiv:2309.11674v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#22120;(ALMA)&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#28040;&#38500;&#20102;&#23545;&#22823;&#37327;&#24179;&#34892;&#25968;&#25454;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22312;&#32763;&#35793;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#37027;&#20123;&#20855;&#26377;&#36866;&#24230;&#27169;&#22411;&#22823;&#23567;&#65288;&#21363;7B&#25110;13B&#21442;&#25968;&#65289;&#30340;&#20219;&#21153;&#65292;&#36825;&#20123;&#36827;&#23637;&#23578;&#26410;&#24471;&#21040;&#21453;&#26144;&#65292;&#20173;&#28982;&#33853;&#21518;&#20110;&#20256;&#32479;&#30340;&#26377;&#30417;&#30563;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32763;&#35793;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#35797;&#22270;&#25913;&#21892;&#36825;&#20123;&#36866;&#24230;LLMs&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#20294;&#20854;&#22686;&#30410;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LLMs&#24494;&#35843;&#26041;&#27861;&#65292;&#19987;&#20026;&#32763;&#35793;&#20219;&#21153;&#32780;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#20256;&#32479;&#32763;&#35793;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#22823;&#37327;&#24179;&#34892;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#24494;&#35843;&#38454;&#27573;&#65306;&#22312;&#21333;&#35821;&#25968;&#25454;&#19978;&#36827;&#34892;&#21021;&#22987;&#24494;&#35843;&#65292;&#28982;&#21518;&#22312;&#19968;&#23567;&#37096;&#20998;&#39640;&#36136;&#37327;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#21518;&#32493;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#24320;&#21457;&#30340;LLM&#34987;&#31216;&#20026;&#22522;&#20110;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#32763;&#35793;&#22120;(ALMA)&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Large Language Models (LLMs) have achieved remarkable advancements in various NLP tasks. However, these advances have not been reflected in the translation task, especially those with moderate model sizes (i.e., 7B or 13B parameters), which still lag behind conventional supervised encoder-decoder translation models. Previous studies have attempted to improve the translation capabilities of these moderate LLMs, but their gains have been limited. In this study, we propose a novel fine-tuning approach for LLMs that is specifically designed for the translation task, eliminating the need for the abundant parallel data that traditional translation models usually depend on. Our approach consists of two fine-tuning stages: initial fine-tuning on monolingual data followed by subsequent fine-tuning on a small set of high-quality parallel data. We introduce the LLM developed through this strategy as Advanced Language Model-based trAnslator (ALMA). Based on LLaMA-2 as our underlying mod
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2309.07683</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#36136;&#65306;&#23545;&#20154;&#31867;&#20013;&#24515;&#20027;&#20041;&#30340;&#35686;&#21578;
&lt;/p&gt;
&lt;p&gt;
Assessing the nature of large language models: A caution against anthropocentrism. (arXiv:2309.07683v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07683
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35780;&#20272;GPT3.5&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20855;&#26377;&#26377;&#36259;&#30340;&#20010;&#24615;&#38382;&#21367;&#22238;&#31572;&#33021;&#21147;&#65292;&#20294;&#19981;&#22826;&#21487;&#33021;&#21457;&#23637;&#20986;&#24847;&#35782;&#65292;&#24182;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#36890;&#36807;OpenAI&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;ChatGPT&#30340;&#21457;&#24067;&#24341;&#36215;&#20102;&#20844;&#20247;&#30340;&#20851;&#27880;&#21644;&#29468;&#27979;&#12290;&#30446;&#21069;&#23384;&#22312;&#20004;&#31181;&#24847;&#35265;&#38453;&#33829;&#65306;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#20026;&#20154;&#31867;&#20219;&#21153;&#24102;&#26469;&#30340;&#22522;&#26412;&#21464;&#38761;&#30340;&#21487;&#33021;&#24615;&#24863;&#21040;&#20852;&#22859;&#65292;&#21478;&#19968;&#26041;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#24863;&#21040;&#39640;&#24230;&#20851;&#20999;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#20851;&#20999;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26631;&#20934;&#12289;&#35268;&#33539;&#21270;&#21644;&#32463;&#36807;&#39564;&#35777;&#30340;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#24037;&#20855;&#26469;&#35780;&#20272;GPT3.5&#12290;&#22312;&#36825;&#20010;&#21021;&#27493;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#27979;&#35797;&#65292;&#21487;&#20197;&#20272;&#35745;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#65292;&#23427;&#20204;&#22312;&#30701;&#26102;&#38388;&#20869;&#30340;&#31283;&#23450;&#24615;&#20197;&#21450;&#19982;&#20154;&#31867;&#30340;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;GPT 3.5&#24456;&#21487;&#33021;&#27809;&#26377;&#20135;&#29983;&#24847;&#35782;&#65292;&#23613;&#31649;&#23427;&#23545;&#20010;&#24615;&#38382;&#21367;&#30340;&#22238;&#31572;&#33021;&#21147;&#20196;&#20154;&#24863;&#20852;&#36259;&#12290;&#23427;&#22312;&#37325;&#22797;&#35266;&#23519;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#35748;&#30693;&#21644;&#20010;&#24615;&#27979;&#37327;&#26041;&#38754;&#30340;&#22823;&#37327;&#21464;&#24322;&#65292;&#36825;&#19982;&#20855;&#26377;&#20154;&#31867;&#33324;&#20010;&#24615;&#30340;&#27169;&#22411;&#26159;&#19981;&#31526;&#21512;&#39044;&#26399;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models garnered a large amount of public attention and speculation with the release of OpenAIs chatbot, ChatGPT. At least two opinion camps exist: one excited about possibilities these models offer for fundamental changes to human tasks, and another highly concerned about power these models seem to have. To address these concerns, we assessed GPT3.5 using standard, normed, and validated cognitive and personality measures. For this seedling project, we developed a battery of tests that allowed us to estimate the boundaries of some of these models capabilities, how stable those capabilities are over a short period of time, and how they compare to humans.  Our results indicate that GPT 3.5 is unlikely to have developed sentience, although its ability to respond to personality inventories is interesting. It did display large variability in both cognitive and personality measures over repeated observations, which is not expected if it had a human-like personality. Variability 
&lt;/p&gt;</description></item><item><title>NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2309.04146</link><description>&lt;p&gt;
NESTLE&#65306;&#19968;&#31181;&#29992;&#20110;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
NESTLE: a No-Code Tool for Statistical Analysis of Legal Corpus. (arXiv:2309.04146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04146
&lt;/p&gt;
&lt;p&gt;
NESTLE&#26159;&#19968;&#20010;&#26080;&#20195;&#30721;&#24037;&#20855;&#65292;&#29992;&#20110;&#36827;&#34892;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#12290;&#23427;&#25552;&#20379;&#20102;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;&#20449;&#24687;&#25552;&#21462;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#25805;&#20316;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#24182;&#21487;&#35270;&#21270;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#30340;&#32479;&#35745;&#20998;&#26512;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#27861;&#24459;&#35265;&#35299;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#26679;&#30340;&#20998;&#26512;&#65292;&#38656;&#35201;&#20351;&#29992;&#25991;&#26723;&#26816;&#32034;&#24037;&#20855;&#36873;&#25321;&#35821;&#26009;&#24211;&#30340;&#23376;&#38598;&#65292;&#20351;&#29992;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#31995;&#32479;&#23545;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#24182;&#23545;&#25968;&#25454;&#36827;&#34892;&#21487;&#35270;&#21270;&#20197;&#36827;&#34892;&#32479;&#35745;&#20998;&#26512;&#12290;&#27599;&#20010;&#36807;&#31243;&#37117;&#38656;&#35201;&#19987;&#19994;&#24037;&#20855;&#25110;&#32534;&#31243;&#25216;&#33021;&#65292;&#28982;&#32780;&#36824;&#27809;&#26377;&#20840;&#38754;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#21487;&#29992;&#12290;&#23588;&#20854;&#26159;&#23545;&#20110;IE&#65292;&#22914;&#26524;IE&#31995;&#32479;&#30340;&#26412;&#20307;&#20013;&#27809;&#26377;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#20449;&#24687;&#65292;&#37027;&#20040;&#38656;&#35201;&#33258;&#24049;&#26500;&#24314;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;NESTLE&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#27861;&#24459;&#35821;&#26009;&#24211;&#32479;&#35745;&#20998;&#26512;&#30340;&#26080;&#20195;&#30721;&#24037;&#20855;&#12290;&#36890;&#36807;NESTLE&#65292;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#25628;&#32034;&#30446;&#26631;&#25991;&#20214;&#12289;&#25552;&#21462;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#36741;&#21161;GUI&#36827;&#34892;&#32454;&#33268;&#32423;&#21035;&#30340;&#25511;&#21046;&#26469;&#21487;&#35270;&#21270;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;NESTLE&#30001;&#19977;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#25628;&#32034;&#24341;&#25806;&#12289;&#31471;&#21040;&#31471;&#30340;IE&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#23427;&#23558;&#21508;&#20010;&#32452;&#20214;&#36830;&#25509;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
The statistical analysis of large scale legal corpus can provide valuable legal insights. For such analysis one needs to (1) select a subset of the corpus using document retrieval tools, (2) structuralize text using information extraction (IE) systems, and (3) visualize the data for the statistical analysis. Each process demands either specialized tools or programming skills whereas no comprehensive unified "no-code" tools have been available. Especially for IE, if the target information is not predefined in the ontology of the IE system, one needs to build their own system. Here we provide NESTLE, a no code tool for large-scale statistical analysis of legal corpus. With NESTLE, users can search target documents, extract information, and visualize the structured data all via the chat interface with accompanying auxiliary GUI for the fine-level control. NESTLE consists of three main components: a search engine, an end-to-end IE system, and a Large Language Model (LLM) that glues the who
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.15812</link><description>&lt;p&gt;
&#36879;&#36807;&#20559;&#22909;&#30475;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21453;&#39304;&#33719;&#21462;&#65306;&#25581;&#31034;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Peering Through Preferences: Unraveling Feedback Acquisition for Aligning Large Language Models. (arXiv:2308.15812v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#23545;&#20110;&#23545;&#40784;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#35328;&#65292;&#35774;&#35745;&#21453;&#39304;&#36873;&#25321;&#26159;&#35780;&#20998;&#36824;&#26159;&#25490;&#21517;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#21457;&#29616;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#23384;&#22312;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#24182;&#19988;&#27880;&#37322;&#32773;&#30340;&#20559;&#35265;&#20063;&#20250;&#24433;&#21709;&#32467;&#26524;&#12290;&#21516;&#26102;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#20063;&#23545;&#35780;&#20272;&#32467;&#26524;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#24847;&#22270;&#30340;&#23545;&#40784;&#25215;&#35834;&#28041;&#21450;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#25110;&#20154;&#31867;&#21453;&#39304;&#12290;&#31264;&#23494;&#30340;&#21453;&#39304;&#27880;&#37322;&#33719;&#21462;&#21644;&#25972;&#21512;&#25104;&#26412;&#36739;&#39640;&#65292;&#32780;&#31232;&#30095;&#30340;&#21453;&#39304;&#21017;&#28041;&#21450;&#32467;&#26500;&#24615;&#35774;&#35745;&#36873;&#25321;&#65292;&#21363;&#35780;&#20998;&#65288;&#20363;&#22914;&#65292;&#22312;1-7&#30340;&#33539;&#22260;&#20869;&#23545;&#22238;&#31572;A&#36827;&#34892;&#35780;&#20998;&#65289;&#21644;&#25490;&#21517;&#65288;&#20363;&#22914;&#65292;&#22238;&#31572;A&#26159;&#21542;&#27604;&#22238;&#31572;B&#26356;&#22909;&#65311;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#35745;&#36873;&#25321;&#23545;LLMs&#30340;&#23545;&#40784;&#21644;&#35780;&#20272;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35780;&#20998;&#21644;&#25490;&#21517;&#25152;&#25512;&#26029;&#20986;&#30340;&#20559;&#22909;&#22312;&#20154;&#31867;&#21644;AI&#27880;&#37322;&#32773;&#20013;&#37117;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#19968;&#33268;&#38382;&#39064;&#65292;&#36798;&#21040;&#20102;60%&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#20998;&#26512;&#30830;&#23450;&#20102;&#35299;&#37322;&#36825;&#20010;&#29616;&#35937;&#30340;&#21508;&#31181;&#27880;&#37322;&#32773;&#20559;&#35265;&#26041;&#38754;&#65292;&#27604;&#22914;&#20154;&#31867;&#27880;&#37322;&#32773;&#26356;&#21916;&#27426;&#23494;&#38598;&#22238;&#31572;&#24182;&#22312;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#26356;&#38738;&#30544;&#20934;&#30830;&#24615;&#12290;&#20196;&#25105;&#20204;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#23545;&#23545;&#40784;&#30340;LLMs&#30340;&#35780;&#20272;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#35780;&#20272;&#32467;&#26524;&#22240;&#20026;&#21453;&#39304;&#21327;&#35758;&#30340;&#36873;&#25321;&#32780;&#26377;&#25152;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values and intents critically involves the use of human or AI feedback. While dense feedback annotations are expensive to acquire and integrate, sparse feedback presents a structural design choice between ratings (e.g., score Response A on a scale of 1-7) and rankings (e.g., is Response A better than Response B?). In this work, we analyze the effect of this design choice for the alignment and evaluation of LLMs. We uncover an inconsistency problem wherein the preferences inferred from ratings and rankings significantly disagree 60% for both human and AI annotators. Our subsequent analysis identifies various facets of annotator biases that explain this phenomena, such as human annotators would rate denser responses higher while preferring accuracy during pairwise judgments. To our surprise, we also observe that the choice of feedback protocol also has a significant effect on the evaluation of aligned LLMs. In particular, we find that LLMs
&lt;/p&gt;</description></item><item><title>&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;</title><link>http://arxiv.org/abs/2308.09687</link><description>&lt;p&gt;
&#24819;&#27861;&#22270;&#65306;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Graph of Thoughts: Solving Elaborate Problems with Large Language Models. (arXiv:2308.09687v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09687
&lt;/p&gt;
&lt;p&gt;
&#24819;&#27861;&#22270;&#65288;GoT&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25552;&#31034;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#23558;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#65292;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#65292;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#12290;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20248;&#21183;&#65292;&#24182;&#21487;&#20197;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20351;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#24819;&#27861;&#22270;&#65288;Graph of Thoughts&#65292;GoT&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25552;&#31034;&#33021;&#21147;&#19978;&#36229;&#36234;&#20102;Chain-of-Thought&#25110;Tree of Thoughts&#65288;ToT&#65289;&#31561;&#33539;&#24335;&#12290;GoT&#30340;&#20851;&#38190;&#24605;&#24819;&#21644;&#20027;&#35201;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#23558;LLM&#29983;&#25104;&#30340;&#20449;&#24687;&#24314;&#27169;&#20026;&#20219;&#24847;&#22270;&#24418;&#65292;&#20854;&#20013;&#20449;&#24687;&#21333;&#20803;&#65288;"LLM&#24819;&#27861;"&#65289;&#26159;&#39030;&#28857;&#65292;&#36793;&#34920;&#31034;&#36825;&#20123;&#39030;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;&#23558;&#20219;&#24847;LLM&#24819;&#27861;&#32452;&#21512;&#25104;&#20855;&#26377;&#21327;&#21516;&#25928;&#24212;&#30340;&#32467;&#26524;&#12289;&#25552;&#28860;&#25972;&#20010;&#24605;&#32500;&#32593;&#32476;&#30340;&#26412;&#36136;&#25110;&#32773;&#20351;&#29992;&#21453;&#39304;&#29615;&#36335;&#22686;&#24378;&#24605;&#32500;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#35777;&#26126;GoT&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#20248;&#21183;&#65292;&#20363;&#22914;&#22312;&#25490;&#24207;&#20219;&#21153;&#19978;&#36136;&#37327;&#25552;&#39640;&#20102;62%&#65292;&#21516;&#26102;&#25104;&#26412;&#38477;&#20302;&#20102;&#36229;&#36807;31%&#12290;&#25105;&#20204;&#30830;&#20445;GoT&#33021;&#22815;&#36890;&#36807;&#26032;&#30340;&#24819;&#27861;&#36716;&#25442;&#36827;&#34892;&#25193;&#23637;&#65292;&#20174;&#32780;&#21487;&#20197;&#29992;&#20110;&#24320;&#21019;&#26032;&#30340;&#25552;&#31034;&#26041;&#26696;&#12290;&#36825;&#39033;&#24037;&#20316;&#20351;&#24471;LLM&#30340;&#25512;&#29702;&#26356;&#25509;&#36817;&#20154;&#31867;&#24605;&#32500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information ("LLM thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinki
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.07134</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#26159;&#22270;&#34920;&#25152;&#38656;&#35201;&#30340;&#20840;&#37096;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Natural Language is All a Graph Needs. (arXiv:2308.07134v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InstructGLM&#30340;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25506;&#32034;&#26159;&#21542;&#21487;&#20197;&#29992;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#30340;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#38761;&#21629;&#12290;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36880;&#28176;&#21462;&#20195;&#20102;CNN&#21644;RNN&#65292;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#32479;&#19968;&#36215;&#26469;&#12290;&#19982;&#30456;&#23545;&#29420;&#31435;&#23384;&#22312;&#30340;&#25968;&#25454;&#65288;&#22914;&#22270;&#20687;&#12289;&#35270;&#39057;&#25110;&#25991;&#26412;&#65289;&#30456;&#27604;&#65292;&#22270;&#34920;&#26159;&#19968;&#31181;&#21253;&#21547;&#20016;&#23500;&#32467;&#26500;&#21644;&#20851;&#31995;&#20449;&#24687;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#21516;&#26102;&#65292;&#20316;&#20026;&#26368;&#20855;&#34920;&#29616;&#21147;&#30340;&#23186;&#20171;&#20043;&#19968;&#65292;&#33258;&#28982;&#35821;&#35328;&#22312;&#25551;&#36848;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#23558;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#32435;&#20837;&#29983;&#25104;&#24335;&#35821;&#35328;&#24314;&#27169;&#26694;&#26550;&#30340;&#29616;&#26377;&#24037;&#20316;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#19981;&#26029;&#22686;&#38271;&#65292;&#25506;&#32034;LLMs&#26159;&#21542;&#20063;&#21487;&#20197;&#26367;&#20195;GNNs&#25104;&#20026;&#22270;&#34920;&#30340;&#22522;&#30784;&#27169;&#22411;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructGLM&#65288;&#32467;&#26500;&#21270;&#35821;&#35328;&#27169;&#22411;&#65289;&#31639;&#27861;&#65292;&#31995;&#32479;&#22320;&#35774;&#35745;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#27169;&#22411;&#26469;&#22788;&#29702;&#22270;&#34920;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of large-scale pre-trained language models, such as ChatGPT, has revolutionized various research fields in artificial intelligence. Transformers-based large language models (LLMs) have gradually replaced CNNs and RNNs to unify fields of computer vision and natural language processing. Compared with the data that exists relatively independently such as images, videos or texts, graph is a type of data that contains rich structural and relational information. Meanwhile, natural language, as one of the most expressive mediums, excels in describing complex structures. However, existing work on incorporating graph learning problems into the generative language modeling framework remains very limited. As the importance of large language models continues to grow, it becomes essential to explore whether LLMs can also replace GNNs as the foundation model for graphs. In this paper, we propose InstructGLM (Instruction-finetuned Graph Language Model), systematically design highly scal
&lt;/p&gt;</description></item><item><title>GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.06911</link><description>&lt;p&gt;
GIT-Mol&#65306;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#22270;&#20687;&#65292;&#22270;&#24418;&#21644;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text. (arXiv:2308.06911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06911
&lt;/p&gt;
&lt;p&gt;
GIT-Mol&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#22788;&#29702;&#22270;&#20687;&#12289;&#22270;&#24418;&#21644;&#25991;&#26412;&#20449;&#24687;&#12290;&#36890;&#36807;&#26032;&#25552;&#20986;&#30340;GIT-Former&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#23545;&#40784;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;GIT-Mol&#22312;&#24615;&#36136;&#39044;&#27979;&#21644;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#36824;&#21487;&#29992;&#20110;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#36890;&#36807;&#22788;&#29702;&#20998;&#23376;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#20026;&#20998;&#23376;&#31185;&#23398;&#20013;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20855;&#26377;&#22797;&#26434;&#20998;&#23376;&#32467;&#26500;&#25110;&#22270;&#20687;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GIT-Mol&#65292;&#19968;&#31181;&#38598;&#25104;&#20102;&#22270;&#24418;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#20419;&#36827;&#22810;&#27169;&#24577;&#20998;&#23376;&#25968;&#25454;&#30340;&#38598;&#25104;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GIT-Former&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#33021;&#22815;&#23558;&#25152;&#26377;&#27169;&#24577;&#23545;&#40784;&#21040;&#32479;&#19968;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#12290;&#19982;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#24615;&#36136;&#39044;&#27979;&#26041;&#38754;&#23454;&#29616;&#20102;5%-10%&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#65292;&#24182;&#22312;&#20998;&#23376;&#29983;&#25104;&#26377;&#25928;&#24615;&#26041;&#38754;&#25552;&#39640;&#20102;20.2%&#12290;&#36890;&#36807;&#20219;&#24847;&#21040;&#35821;&#35328;&#30340;&#20998;&#23376;&#32763;&#35793;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#28508;&#21147;&#36827;&#34892;&#26356;&#22810;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#20363;&#22914;&#21270;&#21512;&#29289;&#21517;&#31216;&#35782;&#21035;&#21644;&#21270;&#23398;&#21453;&#24212;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have made significant strides in natural language processing, enabling innovative applications in molecular science by processing textual representations of molecules. However, most existing language models cannot capture the rich information with complex molecular structures or images. In this paper, we introduce GIT-Mol, a multi-modal large language model that integrates the Graph, Image, and Text information. To facilitate the integration of multi-modal molecular data, we propose GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space. We achieve a 5%-10% accuracy increase in properties prediction and a 20.2% boost in molecule generation validity compared to the baselines. With the any-to-language molecular translation strategy, our model has the potential to perform more downstream tasks, such as compound name recognition and chemical reaction prediction.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#26469;&#30830;&#23450;&#20505;&#36873;&#22238;&#24212;&#30340;&#20248;&#21155;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#32477;&#23545;&#35780;&#20998;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#36798;&#21040;&#20102;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.07889</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#21028;&#23450;&#36827;&#34892;&#38646;&#26679;&#26412;NLG&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38646;&#26679;&#26412;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#26469;&#30830;&#23450;&#20505;&#36873;&#22238;&#24212;&#30340;&#20248;&#21155;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#32477;&#23545;&#35780;&#20998;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#36798;&#21040;&#20102;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#36755;&#20986;&#26159;&#33267;&#20851;&#37325;&#35201;&#20294;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#33258;&#21160;NLG&#35780;&#20272;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#29305;&#23450;&#20219;&#21153;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#38656;&#35201;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#21644;&#23646;&#24615;&#36827;&#34892;&#24037;&#31243;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#38646;&#26679;&#26412;NLG&#35780;&#20272;&#30340;&#31283;&#20581;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#37197;&#23545;&#27604;&#36739;&#21028;&#23450;&#30340;&#26041;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#65292;&#21363;&#20351;&#20316;&#20026;&#20154;&#31867;&#65292;&#30830;&#23450;&#20004;&#20010;&#36873;&#39033;&#20013;&#21738;&#20010;&#26356;&#22909;&#35201;&#27604;&#29420;&#31435;&#23458;&#35266;&#35780;&#20998;&#27599;&#20010;&#36873;&#39033;&#26356;&#23481;&#26131;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#24182;&#21033;&#29992;LLMs&#26032;&#20852;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#25506;&#27979;FlanT5&#65292;&#30830;&#23450;&#20004;&#20010;&#20505;&#36873;&#22238;&#24212;&#20013;&#21738;&#19968;&#20010;&#26356;&#22909;&#65292;&#32780;&#19981;&#26159;&#25351;&#23450;&#32477;&#23545;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#27604;&#36739;&#35780;&#20272;&#26159;&#27604;&#32477;&#23545;&#35780;&#20998;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#24471;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#33021;&#22815;&#36798;&#21040;&#19982;&#26356;&#22823;&#30340;&#20844;&#20849;&#35775;&#38382;API&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating Natural Language Generation (NLG) outputs is crucial but laborious and expensive. While various automatic NLG assessment methods have been proposed, they often are quite task-specific and have to be engineered with a particular domain and attribute in mind. In this work, we propose a robust zero-shot approach to NLG evaluation using pairwise comparative judgment with open-source Large Language Models (LLMs). The motivation for this approach is that even as humans, it is easier to determine which of two options are better, than it is to independently objectively score each option. We use this insight and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine which of two candidate responses is better, rather than assigning absolute scores. Our results demonstrate that comparative assessment is a more effective approach than absolute scoring, enabling smaller open-source LLMs to achieve comparable performance to larger public access APIs. We evaluate syste
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#20505;&#36873;&#27169;&#22411;&#30340;&#25490;&#21517;&#20998;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23454;&#38469;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#65292;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01015</link><description>&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#30340;&#36801;&#31227;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to Estimate Model Transferability of Pre-Trained Speech Models?. (arXiv:2306.01015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36890;&#36807;&#29983;&#25104;&#20505;&#36873;&#27169;&#22411;&#30340;&#25490;&#21517;&#20998;&#25968;&#65292;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#23454;&#38469;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#65292;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#22522;&#20110;&#20998;&#25968;&#35780;&#20272;&#8221;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20272;&#35745;&#39044;&#35757;&#32451;&#35821;&#38899;&#27169;&#22411;&#65288;PSMs&#65289;&#22312;&#24494;&#35843;&#30446;&#26631;&#20219;&#21153;&#26102;&#30340;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#20004;&#20010;&#34920;&#31034;&#29702;&#35770;&#65292;&#36125;&#21494;&#26031;&#20284;&#28982;&#20272;&#35745;&#21644;&#26368;&#20248;&#20256;&#36755;&#65292;&#20351;&#29992;&#25552;&#21462;&#30340;&#34920;&#31034;&#29983;&#25104;PSM&#20505;&#36873;&#30340;&#25490;&#21517;&#20998;&#25968;&#12290;&#36890;&#36807;&#20551;&#35774;&#29420;&#31435;&#24615;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#39640;&#25928;&#22320;&#35745;&#31639;&#36801;&#31227;&#24615;&#20998;&#25968;&#65292;&#32780;&#26080;&#38656;&#23454;&#38469;&#24494;&#35843;&#20505;&#36873;&#27169;&#22411;&#25110;&#23618;&#12290;&#25105;&#20204;&#20351;&#29992;&#20844;&#20849;&#25968;&#25454;&#22312;&#20132;&#21449;&#23618;&#21644;&#20132;&#21449;&#27169;&#22411;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#65288;&#20363;&#22914;Conformer RNN-Transducer&#65289;&#21644;&#33258;&#30417;&#30563;&#35821;&#38899;&#27169;&#22411;&#65288;&#20363;&#22914;HuBERT&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#20272;&#35745;&#26694;&#26550;&#19982;&#24494;&#35843;&#22522;&#30784;&#20107;&#23454;&#20043;&#38388;&#23384;&#22312;&#24456;&#39640;&#30340;Spearman&#25490;&#21517;&#30456;&#20851;&#24615;&#21644;&#20302;&#30340;p&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#36801;&#31227;&#24615;&#26694;&#26550;&#38656;&#35201;&#36739;&#23569;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#22240;&#27492;&#26159;&#19968;&#20010;&#33410;&#30465;&#36164;&#28304;&#12289;&#39640;&#25928;&#33410;&#30465;&#26102;&#38388;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce a ``score-based assessment'' framework for estimating the transferability of pre-trained speech models (PSMs) for fine-tuning target tasks. We leverage upon two representation theories, Bayesian likelihood estimation and optimal transport, to generate rank scores for the PSM candidates using the extracted representations. Our framework efficiently computes transferability scores without actual fine-tuning of candidate models or layers by making a temporal independent hypothesis. We evaluate some popular supervised speech models (e.g., Conformer RNN-Transducer) and self-supervised speech models (e.g., HuBERT) in cross-layer and cross-model settings using public data. Experimental results show a high Spearman's rank correlation and low $p$-value between our estimation framework and fine-tuning ground truth. Our proposed transferability framework requires less computational time and resources, making it a resource-saving and time-efficient approach for tuning sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;</title><link>http://arxiv.org/abs/2305.14330</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#24103;&#32423;&#23548;&#28436;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Frame-level Directors for Zero-shot Text-to-Video Generation. (arXiv:2305.14330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;DirecT2V&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#23548;&#28436;&#65292;&#20174;&#19968;&#20010;&#25277;&#35937;&#30340;&#29992;&#25143;&#25552;&#31034;&#20013;&#29983;&#25104;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#30340;&#36830;&#36143;&#19988;&#36830;&#36143;&#30340;&#35270;&#39057;&#12290;&#35813;&#26694;&#26550;&#20351;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#19968;&#24103;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#20540;&#26144;&#23556;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#26469;&#20445;&#25345;&#26102;&#38388;&#19968;&#33268;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#30340;&#33539;&#24335;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#23558;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#25193;&#23637;&#21040;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#29983;&#25104;&#19978;&#12290;&#23613;&#31649;&#36825;&#20123;&#26694;&#26550;&#24456;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#32500;&#25252;&#19968;&#33268;&#30340;&#21465;&#36848;&#21644;&#22788;&#29702;&#20174;&#21333;&#20010;&#29992;&#25143;&#25552;&#31034;&#20013;&#30340;&#24555;&#36895;&#22330;&#26223;&#32452;&#21512;&#25110;&#23545;&#35937;&#20301;&#32622;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DirecT2V&#65292;&#23427;&#21033;&#29992;&#38024;&#23545;&#25351;&#20196;&#26657;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20174;&#21333;&#20010;&#25277;&#35937;&#29992;&#25143;&#25552;&#31034;&#29983;&#25104;&#36880;&#24103;&#25551;&#36848;&#12290;DirecT2V&#21033;&#29992;LLM&#23548;&#28436;&#23558;&#29992;&#25143;&#36755;&#20837;&#20998;&#20026;&#27599;&#20010;&#24103;&#30340;&#21333;&#29420;&#25552;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#21253;&#21547;&#26102;&#38388;&#21464;&#21270;&#30340;&#20869;&#23481;&#21644;&#20415;&#20110;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#12290;&#20026;&#20102;&#20445;&#25345;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#21644;&#38450;&#27490;&#23545;&#35937;&#25240;&#21472;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20540;&#26144;&#23556;&#26041;&#27861;&#21644;&#21452;softmax&#36807;&#28388;&#22120;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;DirecT2V&#26694;&#26550;&#22312;&#38646;&#26679;&#26412;T2V&#29983;&#25104;&#20013;&#20135;&#29983;&#30340;&#35270;&#35273;&#36830;&#36143;&#21644;&#19968;&#33268;&#30340;&#35270;&#39057;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the paradigm of AI-generated content (AIGC), there has been increasing attention in extending pre-trained text-to-image (T2I) models to text-to-video (T2V) generation. Despite their effectiveness, these frameworks face challenges in maintaining consistent narratives and handling rapid shifts in scene composition or object placement from a single user prompt. This paper introduces a new framework, dubbed DirecT2V, which leverages instruction-tuned large language models (LLMs) to generate frame-by-frame descriptions from a single abstract user prompt. DirecT2V utilizes LLM directors to divide user inputs into separate prompts for each frame, enabling the inclusion of time-varying content and facilitating consistent video generation. To maintain temporal consistency and prevent object collapse, we propose a novel value mapping method and dual-softmax filtering. Extensive experimental results validate the effectiveness of the DirecT2V framework in producing visually coherent and consist
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2305.06841</link><description>&lt;p&gt;
&#19977;&#24605;&#32780;&#21518;&#34892;&#65306;&#34913;&#37327;&#28040;&#38500;&#38382;&#31572;&#27169;&#22411;&#39044;&#27979;&#24555;&#25463;&#26041;&#24335;&#30340;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Think Twice: Measuring the Efficiency of Eliminating Prediction Shortcuts of Question Answering Models. (arXiv:2305.06841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06841
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#25216;&#26415;&#65292;&#24182;&#35780;&#20272;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#38382;&#31572;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#20854;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#20559;&#24046;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;OOD&#25910;&#30410;&#65292;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Models&#65292;LLMs&#65289;&#20027;&#23548;&#20102;&#22823;&#37096;&#20998;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24314;&#27169;&#20551;&#28151;&#28102;&#30340;&#25903;&#25345;&#19979;, &#22240;&#27492;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#26576;&#20123;&#36825;&#20123;&#32467;&#26524;&#26159;&#30001;&#24314;&#27169;&#34394;&#20551;&#30456;&#20851;&#24615;&#23454;&#29616;&#30340;&#12290;&#20316;&#32773;&#24120;&#24120;&#36890;&#36807;&#35780;&#20272;&#21516;&#19968;&#20219;&#21153;&#30340;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#20581;&#22766;&#24615;&#65292;&#20294;&#36825;&#20123;&#25968;&#25454;&#38598;&#21487;&#33021;&#20855;&#26377;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30456;&#21516;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34913;&#37327;&#27169;&#22411;&#20381;&#36182;&#20110;&#20219;&#20309;&#24050;&#30693;&#34394;&#20551;&#29305;&#24449;&#30340;&#23610;&#24230;&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#21435;&#20559;&#32622;&#26041;&#27861;&#22312;&#38382;&#31572;&#65288;QA&#65289;&#20013;&#23545;&#22823;&#37327;&#24050;&#30693;&#21644;&#26032;&#21457;&#29616;&#30340;&#39044;&#27979;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#21435;&#20559;&#32622;&#26041;&#27861;&#30340;&#25253;&#21578;OOD&#25910;&#30410;&#19981;&#33021;&#36890;&#36807;&#20943;&#36731;&#23545;&#26377;&#20559;&#29305;&#24449;&#30340;&#20381;&#36182;&#26469;&#35299;&#37322;&#65292;&#36825;&#34920;&#26126;&#20559;&#24046;&#22312;QA&#25968;&#25454;&#38598;&#20013;&#20849;&#20139;&#12290;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;OOD&#27169;&#22411;&#30340;&#34920;&#29616;&#21462;&#20915;&#20110;&#20559;&#24046;&#29305;&#24449;&#65292;&#19982;ID&#27169;&#22411;&#30456;&#24403;&#65292;&#36827;&#32780;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#65292;&#36825;&#25512;&#21160;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the Large Language Models (LLMs) dominate a majority of language understanding tasks, previous work shows that some of these results are supported by modelling spurious correlations of training datasets. Authors commonly assess model robustness by evaluating their models on out-of-distribution (OOD) datasets of the same task, but these datasets might share the bias of the training dataset.  We propose a simple method for measuring a scale of models' reliance on any identified spurious feature and assess the robustness towards a large set of known and newly found prediction biases for various pre-trained models and debiasing methods in Question Answering (QA). We find that the reported OOD gains of debiasing methods can not be explained by mitigated reliance on biased features, suggesting that biases are shared among QA datasets. We further evidence this by measuring that performance of OOD models depends on bias features comparably to the ID model, motivating future work to refin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11062</link><description>&lt;p&gt;
&#21033;&#29992;RMT&#23558;Transformer&#25193;&#23637;&#21040;100&#19975;&#20010;&#26631;&#35760;&#21450;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;BERT&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#22686;&#21152;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#30340;&#20869;&#23384;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23384;&#20648;&#21644;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#23454;&#29616;&#36755;&#20837;&#24207;&#21015;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10750</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#65288;Grounded Language Understanding&#65289;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10750
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36890;&#24120;&#34987;&#35270;&#20026;&#21333;&#27493;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#25509;&#25910;&#19968;&#20010;&#25351;&#20196;&#65292;&#25191;&#34892;&#23427;&#65292;&#28982;&#21518;&#26681;&#25454;&#26368;&#32456;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#25105;&#20204;&#20027;&#24352;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#20063;&#24212;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#20154;&#31867;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20195;&#29702;&#21487;&#20197;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;Help Feedback&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations.  In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impac
&lt;/p&gt;</description></item></channel></rss>