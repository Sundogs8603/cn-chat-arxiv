<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2310.19736</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#32508;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35780;&#20272;&#65292;&#21253;&#25324;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#12289;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#23545;&#20110;&#20805;&#20998;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#20197;&#21450;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#21560;&#24341;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#19982;&#21452;&#20995;&#21073;&#19968;&#26679;&#65292;LLMs&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;&#23427;&#20204;&#21487;&#33021;&#21463;&#21040;&#31169;&#20154;&#25968;&#25454;&#27844;&#38706;&#65292;&#20135;&#29983;&#19981;&#36866;&#24403;&#12289;&#26377;&#23475;&#25110;&#35823;&#23548;&#24615;&#30340;&#20869;&#23481;&#12290;&#27492;&#22806;&#65292;LLMs&#30340;&#24555;&#36895;&#36827;&#23637;&#24341;&#21457;&#20102;&#23545;&#21487;&#33021;&#20986;&#29616;&#27809;&#26377;&#36275;&#22815;&#20445;&#38556;&#30340;&#36229;&#26234;&#33021;&#31995;&#32479;&#30340;&#25285;&#24551;&#12290;&#20026;&#20102;&#26377;&#25928;&#21033;&#29992;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#30830;&#20445;&#20854;&#23433;&#20840;&#21644;&#26377;&#30410;&#30340;&#21457;&#23637;&#65292;&#23545;LLMs&#36827;&#34892;&#20005;&#26684;&#21644;&#20840;&#38754;&#30340;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#25552;&#20379;&#23545;LLMs&#35780;&#20272;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#25105;&#20204;&#23558;LLMs&#30340;&#35780;&#20272;&#20998;&#20026;&#19977;&#22823;&#31867;&#21035;&#65306;&#30693;&#35782;&#21644;&#33021;&#21147;&#35780;&#20272;&#65292;&#23545;&#40784;&#35780;&#20272;&#21644;&#23433;&#20840;&#35780;&#20272;&#12290;&#38500;&#20102;&#20840;&#38754;&#22238;&#39038;&#35780;&#20272;&#26041;&#27861;&#21644;&#25216;&#26415;&#20043;&#22806;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.10520</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35821;&#20041;&#35299;&#26512;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ParsingDST&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35821;&#20041;&#35299;&#26512;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#22797;&#26434;&#30340;&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#30340;&#26356;&#26032;&#31574;&#30053;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#26126;&#26174;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65288;DST&#65289;&#35299;&#20915;&#20102;&#33719;&#21462;&#21644;&#27880;&#37322;&#38754;&#21521;&#20219;&#21153;&#30340;&#23545;&#35805;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;DST&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#22635;&#27133;&#65292;&#38656;&#35201;&#26377;&#25928;&#30340;&#26356;&#26032;&#31574;&#30053;&#26469;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#38543;&#30528;&#23545;&#35805;&#30340;&#36827;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ParsingDST&#65292;&#19968;&#31181;&#26032;&#30340;In-Context Learning&#65288;ICL&#65289;&#26041;&#27861;&#65292;&#20197;&#24341;&#20837;&#39069;&#22806;&#30340;&#22797;&#26434;&#26356;&#26032;&#31574;&#30053;&#29992;&#20110;&#38646;&#26679;&#26412;DST&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24182;&#36890;&#36807;&#35821;&#20041;&#35299;&#26512;&#23558;&#21407;&#22987;&#23545;&#35805;&#25991;&#26412;&#36716;&#25442;&#20026;JSON&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#29366;&#24577;&#26469;&#37325;&#26032;&#23450;&#20041;DST&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#26356;&#22810;&#30340;&#27169;&#22359;&#26469;&#30830;&#20445;&#25991;&#26412;&#21040;JSON&#36807;&#31243;&#20013;&#26356;&#26032;&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MultiWOZ&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;DST&#26041;&#27861;&#65292;&#22312;&#32852;&#21512;&#30446;&#26631;&#20934;&#30830;&#29575;&#65288;JGA&#65289;&#21644;&#27133;&#20934;&#30830;&#24230;&#26041;&#38754;&#19982;&#29616;&#26377;&#30340;ICL&#26041;&#27861;&#30456;&#27604;&#21576;&#29616;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring and annotating task-oriented dialogues, which can be time consuming and costly. However, DST extends beyond simple slot-filling and requires effective updating strategies for tracking dialogue state as conversations progress. In this paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to introduce additional intricate updating strategies in zero-shot DST. Our approach reformulates the DST task by leveraging powerful Large Language Models (LLMs) and translating the original dialogue text to JSON through semantic parsing as an intermediate state. We also design a novel framework that includes more modules to ensure the effectiveness of updating strategies in the text-to-JSON process. Experimental results demonstrate that our approach outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to existing ICL
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.06422</link><description>&lt;p&gt;
&#29992;&#20110;&#20256;&#25773;&#20449;&#24687;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Propaganda Detection. (arXiv:2310.06422v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06422
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-4&#36798;&#21040;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#31526;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#25968;&#23383;&#21270;&#31038;&#20250;&#20013;&#65292;&#23459;&#20256;&#20449;&#24687;&#30340;&#26222;&#36941;&#23384;&#22312;&#23545;&#31038;&#20250;&#21644;&#30495;&#30456;&#30340;&#20256;&#25773;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#23459;&#20256;&#20449;&#24687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23384;&#22312;&#24494;&#22937;&#30340;&#25805;&#32437;&#25216;&#26415;&#21644;&#35821;&#22659;&#20381;&#36182;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3&#21644;GPT-4&#22312;&#23459;&#20256;&#20449;&#24687;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;SemEval-2020&#20219;&#21153;11&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20855;&#26377;14&#31181;&#23459;&#20256;&#25216;&#26415;&#26631;&#31614;&#30340;&#26032;&#38395;&#25991;&#31456;&#65292;&#20316;&#20026;&#19968;&#20010;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;GPT-3&#21644;GPT-4&#30340;&#20116;&#31181;&#21464;&#20307;&#65292;&#32467;&#21512;&#20102;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#21508;&#31181;&#25552;&#31034;&#24037;&#31243;&#21644;&#24494;&#35843;&#31574;&#30053;&#12290;&#36890;&#36807;&#35780;&#20272;$F1$&#20998;&#25968;&#65292;$Precision$&#21644;$Recall$&#31561;&#25351;&#26631;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#20351;&#29992;RoBERTa&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-4&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prevalence of propaganda in our digital society poses a challenge to societal harmony and the dissemination of truth. Detecting propaganda through NLP in text is challenging due to subtle manipulation techniques and contextual dependencies. To address this issue, we investigate the effectiveness of modern Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection. We conduct experiments using the SemEval-2020 task 11 dataset, which features news articles labeled with 14 propaganda techniques as a multi-label classification problem. Five variations of GPT-3 and GPT-4 are employed, incorporating various prompt engineering and fine-tuning strategies across the different models. We evaluate the models' performance by assessing metrics such as $F1$ score, $Precision$, and $Recall$, comparing the results with the current state-of-the-art approach using RoBERTa. Our findings demonstrate that GPT-4 achieves comparable results to the current state-of-the-art. Further, thi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;</title><link>http://arxiv.org/abs/2310.01889</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#22359;Transformer&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#35299;&#20915;&#36817;&#26080;&#38480;&#19978;&#19979;&#25991;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ring Attention with Blockwise Transformers for Near-Infinite Context. (arXiv:2310.01889v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#22359;&#35745;&#31639;&#21644;&#36890;&#20449;&#37325;&#21472;&#30340;&#26041;&#24335;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#35299;&#20915;&#20102;Transformer&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#28040;&#38500;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#65292;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#38271;&#24230;&#33021;&#22815;&#26356;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#39318;&#36873;&#26550;&#26500;&#65292;&#22312;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#23637;&#31034;&#20986;&#20102;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#23545;&#20869;&#23384;&#30340;&#38656;&#27714;&#38480;&#21046;&#20102;&#23427;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#23545;&#20110;&#28041;&#21450;&#25193;&#23637;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#32780;&#35328;&#23384;&#22312;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#21363;&#29615;&#24418;&#27880;&#24847;&#21147;(Ring Attention)&#65292;&#23427;&#21033;&#29992;&#33258;&#27880;&#24847;&#21147;&#30340;&#20998;&#22359;&#35745;&#31639;&#23558;&#38271;&#24207;&#21015;&#20998;&#24067;&#21040;&#22810;&#20010;&#35774;&#22791;&#19978;&#65292;&#21516;&#26102;&#23558;&#20851;&#38190;-&#20540;&#22359;&#30340;&#36890;&#20449;&#19982;&#20998;&#22359;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#37325;&#21472;&#12290;&#36890;&#36807;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#21516;&#26102;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#65292;&#29615;&#24418;&#27880;&#24847;&#21147;&#20351;&#24471;&#35757;&#32451;&#21644;&#25512;&#29702;&#30340;&#24207;&#21015;&#27604;&#20043;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;Transformer&#33021;&#22815;&#22810;&#20986;&#35774;&#22791;&#25968;&#37327;&#20493;&#65292;&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#21333;&#20010;&#35774;&#22791;&#23545;&#20869;&#23384;&#30340;&#32422;&#26463;&#12290;&#22312;&#35821;&#35328;&#27169;&#22411;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving extended sequences or long-term dependencies. We present a distinct approach, Ring Attention, which leverages blockwise computation of self-attention to distribute long sequences across multiple devices while concurrently overlapping the communication of key-value blocks with the computation of blockwise attention. By processing longer input sequences while maintaining memory efficiency, Ring Attention enables training and inference of sequences that are device count times longer than those of prior memory-efficient Transformers, effectively eliminating the memory constraints imposed by individual devices. Extensive experiments on language modeling tasks demonstrate the effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2310.01828</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65306;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35757;&#32451;&#30340;&#22122;&#22768;&#27169;&#22411;&#20316;&#20026;XAI&#35780;&#20272;&#26041;&#27861;&#65292;&#22312;&#36965;&#24863;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#65292;&#20294;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#20851;&#38190;&#20219;&#21153;&#24212;&#29992;&#26102;&#30340;&#24517;&#22791;&#35201;&#27714;&#65292;&#30830;&#20445;&#25152;&#20351;&#29992;&#30340;&#40657;&#30418;&#23376;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;XAI&#30340;&#37325;&#35201;&#24615;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#20174;&#21307;&#30103;&#20445;&#20581;&#21040;&#37329;&#34701;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#20102;&#35299;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#24448;&#24448;&#26159;&#40657;&#30418;&#23376;&#65292;&#22240;&#27492;&#22312;&#22270;&#20687;&#22788;&#29702;&#20013;&#25552;&#20379;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#23427;&#20204;&#22312;&#21307;&#30103;&#22270;&#20687;&#20998;&#26512;&#12289;&#33258;&#21160;&#39550;&#39542;&#21644;&#36965;&#24863;&#24212;&#29992;&#20013;&#30340;&#24191;&#27867;&#37319;&#29992;&#21644;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#38024;&#23545;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;XAI&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#65292;&#22270;&#20687;&#20998;&#21106;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#39046;&#22495;&#20013;&#65292;&#21463;&#21040;&#20102;&#30456;&#23545;&#36739;&#23569;&#30340;&#20851;&#27880;&#12290;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;XAI&#31639;&#27861;&#26469;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
eXplainable Artificial Intelligence (XAI) has emerged as an essential requirement when dealing with mission-critical applications, ensuring transparency and interpretability of the employed black box AI models. The significance of XAI spans various domains, from healthcare to finance, where understanding the decision-making process of deep learning algorithms is essential. Most AI-based computer vision models are often black boxes; hence, providing explainability of deep neural networks in image processing is crucial for their wide adoption and deployment in medical image analysis, autonomous driving, and remote sensing applications. Recently, several XAI methods for image classification tasks have been introduced. On the contrary, image segmentation has received comparatively less attention in the context of explainability, although it is a fundamental task in computer vision applications, especially in remote sensing. Only some research proposes gradient-based XAI algorithms for imag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.01825</link><description>&lt;p&gt;
&#20908;&#23567;&#40614;&#20998;&#21106;&#30340;PEFT&#25216;&#26415;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;PEFT&#25216;&#26415;&#65292;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#26368;&#36817;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#65292;&#24182;&#34987;&#24191;&#27867;&#29992;&#20110;&#23558;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#20197;&#26368;&#23567;&#30340;&#35745;&#31639;&#38656;&#27714;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#36965;&#24863;&#21644;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#20851;&#38190;&#39046;&#22495;&#20013;&#65292;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#28508;&#22312;&#30340;PEFT&#24212;&#29992;&#12290;&#19981;&#21516;&#22320;&#21306;&#30340;&#27668;&#20505;&#22810;&#26679;&#24615;&#21644;&#23545;&#20840;&#38754;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#65292;&#32473;&#31934;&#30830;&#35782;&#21035;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#31181;&#26893;&#23395;&#33410;&#30340;&#20316;&#29289;&#31867;&#22411;&#36896;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#25506;&#32034;&#36328;&#21306;&#22495;&#21644;&#36328;&#24180;&#20221;&#30340;&#20998;&#24067;&#22806;&#25512;&#24191;&#24615;&#65292;&#20351;&#29992;&#22269;&#20869;&#39046;&#20808;&#30340;&#20908;&#23567;&#40614;&#20316;&#29289;&#30417;&#27979;&#27169;&#22411;&#65292;&#26469;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#30446;&#26631;&#26159;&#25506;&#32034;PEFT&#26041;&#27861;&#22312;&#20316;&#29289;&#30417;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#36866;&#24212;&#24615;&#22320;&#35843;&#25972;PEFT&#26041;&#27861;&#20197;&#36866;&#24212;&#20892;&#20316;&#29289;&#30417;&#27979;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced significant growth and have been extensively employed to adapt large vision and language models to various domains, enabling satisfactory model performance with minimal computational needs. Despite these advances, more research has yet to delve into potential PEFT applications in real-life scenarios, particularly in the critical domains of remote sensing and crop monitoring. The diversity of climates across different regions and the need for comprehensive large-scale datasets have posed significant obstacles to accurately identify crop types across varying geographic locations and changing growing seasons. This study seeks to bridge this gap by comprehensively exploring the feasibility of cross-area and cross-year out-of-distribution generalization using the State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to explore PEFT approaches for crop monitoring. Specifically, we focus on adap
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.17255</link><description>&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#22270;&#35889;&#65306;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17255
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#29983;&#21629;&#31185;&#23398;&#39046;&#22495;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#21629;&#31185;&#23398;&#26159;&#30740;&#31350;&#29983;&#29289;&#21644;&#29983;&#21629;&#36807;&#31243;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#21270;&#23398;&#12289;&#29983;&#29289;&#23398;&#12289;&#21307;&#23398;&#21644;&#19968;&#31995;&#21015;&#20854;&#20182;&#30456;&#20851;&#23398;&#31185;&#12290;&#29983;&#21629;&#31185;&#23398;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#20381;&#36182;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#20135;&#29983;&#21644;&#28040;&#36153;&#22823;&#37327;&#31185;&#23398;&#25968;&#25454;&#65292;&#20854;&#20013;&#24456;&#22810;&#25968;&#25454;&#20855;&#26377;&#20851;&#31995;&#21644;&#22270;&#32467;&#26500;&#12290;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#20854;&#20013;&#28041;&#21450;&#30340;&#31185;&#23398;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#24615;&#25512;&#21160;&#20102;&#24212;&#29992;&#20808;&#36827;&#30340;&#30693;&#35782;&#39537;&#21160;&#25216;&#26415;&#26469;&#31649;&#29702;&#21644;&#35299;&#37322;&#25968;&#25454;&#65292;&#26368;&#32456;&#30446;&#26631;&#26159;&#25512;&#21160;&#31185;&#23398;&#21457;&#29616;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#21644;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#30693;&#35782;&#22270;&#35889;&#22312;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#36827;&#23637;&#65292;&#24182;&#23637;&#26395;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#26410;&#26469;&#23545;&#36825;&#20123;&#39046;&#22495;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#20027;&#39064;&#65306;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#21644;&#31649;&#29702;&#65292;&#20197;&#21450;&#22312;&#26032;&#21457;&#29616;&#30340;&#36807;&#31243;&#20013;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#21644;&#30456;&#20851;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term life sciences refers to the disciplines that study living organisms and life processes, and include chemistry, biology, medicine, and a range of other related disciplines. Research efforts in life sciences are heavily data-driven, as they produce and consume vast amounts of scientific data, much of which is intrinsically relational and graph-structured.  The volume of data and the complexity of scientific concepts and relations referred to therein promote the application of advanced knowledge-driven technologies for managing and interpreting data, with the ultimate aim to advance scientific discovery.  In this survey and position paper, we discuss recent developments and advances in the use of graph-based technologies in life sciences and set out a vision for how these technologies will impact these fields into the future. We focus on three broad topics: the construction and management of Knowledge Graphs (KGs), the use of KGs and associated technologies in the discovery of ne
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.14306</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25351;&#20196;&#24494;&#35843;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27169;&#22411;&#23545;&#38476;&#29983;&#25351;&#20196;&#30340;&#22788;&#29702;&#33021;&#21147;&#26377;&#24453;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26032;&#20219;&#21153;&#20013;&#38646;-shot&#33021;&#21147;&#30340;&#28508;&#22312;&#26041;&#27861;&#12290;&#35813;&#25216;&#26415;&#26174;&#31034;&#20986;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#25552;&#21319;&#20013;&#31561;&#35268;&#27169;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26377;&#26102;&#29978;&#33267;&#36798;&#21040;&#30456;&#24403;&#20110;&#26356;&#22823;&#27169;&#22411;&#21464;&#20307;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#26412;&#30740;&#31350;&#37325;&#28857;&#30740;&#31350;&#20102;&#32463;&#36807;&#25351;&#20196;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#24050;&#30693;&#20219;&#21153;&#21644;&#26410;&#30693;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23545;&#20845;&#20010;&#27169;&#22411;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#21253;&#25324;Alpaca&#12289;Vicuna&#12289;WizardLM&#21644;&#20256;&#32479;&#30340;&#20219;&#21153;&#23548;&#21521;&#27169;&#22411;&#65288;Flan-T5-XL/XXL&#12289;T0++&#65289;&#65292;&#20197;&#30495;&#23454;&#19990;&#30028;&#30340;&#20851;&#31995;&#25552;&#21462;&#25968;&#25454;&#38598;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#36981;&#24490;&#25351;&#20196;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#22522;&#20110;&#24320;&#25918;&#22495;&#25351;&#20196;&#21644;&#20219;&#21153;&#23548;&#21521;&#25351;&#20196;&#36827;&#34892;&#24494;&#35843;&#30340;&#12290;&#20027;&#35201;&#35752;&#35770;&#30340;&#26159;&#23427;&#20204;&#22312;&#22788;&#29702;&#25351;&#20196;&#26102;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#22312;&#22788;&#29702;&#38476;&#29983;&#25351;&#20196;&#26041;&#38754;&#30340;&#24615;&#33021;&#24448;&#24448;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Instruction fine-tuning has risen to prominence as a potential method for enhancing the zero-shot capabilities of Large Language Models (LLMs) on novel tasks. This technique has shown an exceptional ability to boost the performance of moderately sized LLMs, sometimes even reaching performance levels comparable to those of much larger model variants. The focus is on the robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction datasets as case studies. We carried out a comprehensive evaluation of these instruction-following LLMs which have been tuned based on open-domain instructions and task-oriented instructions. The main discussion is their performance and robustness towards instructions. We have observed that in most cases, the model's performance in dealing with unfamiliar instructions tends to w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10819</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#25552;&#31034;&#27880;&#20837;&#30340;&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10819
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#40065;&#26834;&#24615;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#37327;&#21270;&#27169;&#22411;&#21463;&#21040;&#27880;&#20837;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#21644;&#27880;&#20837;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36981;&#24490;&#25351;&#20196;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20351;&#20854;&#22312;&#38754;&#21521;&#23458;&#25143;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20986;&#33394;&#33021;&#21147;&#20063;&#24341;&#21457;&#20102;&#23545;&#30001;&#31532;&#19977;&#26041;&#25915;&#20987;&#32773;&#27880;&#20837;&#27169;&#22411;&#36755;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#30340;&#39118;&#38505;&#25918;&#22823;&#30340;&#25285;&#24551;&#65292;&#36825;&#20123;&#25351;&#20196;&#21487;&#33021;&#25805;&#32437;LLM&#30340;&#21407;&#22987;&#25351;&#20196;&#24182;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#21644;&#20869;&#23481;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;LLM&#20934;&#30830;&#36776;&#21035;&#35201;&#36981;&#24490;&#30340;&#25351;&#20196;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#23427;&#20204;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#23545;LLM&#25351;&#20196;&#36319;&#38543;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#35813;&#22522;&#20934;&#30340;&#30446;&#26631;&#26159;&#37327;&#21270;LLM&#21463;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#24433;&#21709;&#30340;&#31243;&#24230;&#65292;&#24182;&#35780;&#20272;&#20854;&#21306;&#20998;&#36825;&#20123;&#27880;&#20837;&#30340;&#23545;&#25239;&#24615;&#25351;&#20196;&#21644;&#21407;&#22987;&#29992;&#25143;&#25351;&#20196;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable proficiency in following instructions, making them valuable in customer-facing applications. However, their impressive capabilities also raise concerns about the amplification of risks posed by adversarial instructions, which can be injected into the model input by third-party attackers to manipulate LLMs' original instructions and prompt unintended actions and content. Therefore, it is crucial to understand LLMs' ability to accurately discern which instructions to follow to ensure their safe deployment in real-world scenarios. In this paper, we propose a pioneering benchmark for automatically evaluating the robustness of instruction-following LLMs against adversarial instructions injected in the prompt. The objective of this benchmark is to quantify the extent to which LLMs are influenced by injected adversarial instructions and assess their ability to differentiate between these injected adversarial instructions and original user ins
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.00802</link><description>&lt;p&gt;
GRDD: &#24076;&#33098;&#26041;&#35328;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
GRDD: A Dataset for Greek Dialectal NLP. (arXiv:2308.00802v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;GRDD&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#30740;&#31350;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#20811;&#37324;&#29305;&#12289;&#24222;&#25552;&#12289;&#21271;&#24076;&#33098;&#21644;&#22622;&#28006;&#36335;&#26031;&#24076;&#33098;&#22235;&#31181;&#26041;&#35328;&#30340;&#21407;&#22987;&#25991;&#26412;&#25968;&#25454;&#12290;&#23613;&#31649;&#23384;&#22312;&#19981;&#24179;&#34913;&#65292;&#20294;&#35813;&#25968;&#25454;&#38598;&#26159;&#30456;&#24403;&#22823;&#30340;&#65292;&#24182;&#19988;&#26159;&#21019;&#24314;&#29616;&#20195;&#24076;&#33098;&#26041;&#35328;&#31867;&#20284;&#36164;&#28304;&#30340;&#39318;&#27425;&#23581;&#35797;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#26041;&#35328;&#35782;&#21035;&#65292;&#24182;&#23581;&#35797;&#20102;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21644;&#31616;&#21333;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#38750;&#24120;&#22909;&#65292;&#36825;&#21487;&#33021;&#34920;&#26126;&#25152;&#30740;&#31350;&#30340;&#26041;&#35328;&#20855;&#26377;&#36275;&#22815;&#30340;&#29420;&#29305;&#29305;&#24449;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20063;&#33021;&#22312;&#35813;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#38024;&#23545;&#34920;&#29616;&#26368;&#20339;&#30340;&#31639;&#27861;&#36827;&#34892;&#20102;&#38169;&#35823;&#20998;&#26512;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#38169;&#35823;&#26159;&#30001;&#20110;&#25968;&#25454;&#38598;&#28165;&#29702;&#19981;&#36275;&#36896;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a dataset for the computational study of a number of Modern Greek dialects. It consists of raw text data from four dialects of Modern Greek, Cretan, Pontic, Northern Greek and Cypriot Greek. The dataset is of considerable size, albeit imbalanced, and presents the first attempt to create large scale dialectal resources of this type for Modern Greek dialects. We then use the dataset to perform dialect idefntification. We experiment with traditional ML algorithms, as well as simple DL architectures. The results show very good performance on the task, potentially revealing that the dialects in question have distinct enough characteristics allowing even simple ML models to perform well on the task. Error analysis is performed for the top performing algorithms showing that in a number of cases the errors are due to insufficient dataset cleaning.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27700;&#21360;&#26041;&#27861;&#32534;&#30721;&#25928;&#29575;&#20302;&#12289;&#19981;&#33021;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.15992</link><description>&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Codable Text Watermarking for Large Language Models. (arXiv:2307.15992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15992
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#27700;&#21360;&#26041;&#27861;&#32534;&#30721;&#25928;&#29575;&#20302;&#12289;&#19981;&#33021;&#28385;&#36275;&#19981;&#21516;&#24212;&#29992;&#22330;&#26223;&#38656;&#27714;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#26085;&#30410;&#27969;&#30021;&#21644;&#36924;&#30495;&#65292;&#26377;&#24517;&#35201;&#35782;&#21035;&#25991;&#26412;&#30340;&#26469;&#28304;&#20197;&#38450;&#27490;LLMs&#30340;&#28389;&#29992;&#12290;&#25991;&#26412;&#27700;&#21360;&#25216;&#26415;&#36890;&#36807;&#23558;&#38544;&#34255;&#30340;&#27169;&#24335;&#27880;&#20837;&#21040;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#24050;&#34987;&#35777;&#23454;&#21487;&#20197;&#21487;&#38752;&#22320;&#21306;&#20998;&#26159;&#21542;&#30001;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;LLMs&#27700;&#21360;&#26041;&#27861;&#22312;&#32534;&#30721;&#25928;&#29575;&#19978;&#23384;&#22312;&#38382;&#39064;&#65288;&#21482;&#21253;&#21547;&#19968;&#20010;&#20301;&#30340;&#20449;&#24687;&#65292;&#21363;&#25991;&#26412;&#26159;&#21542;&#30001;LLMs&#29983;&#25104;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#28789;&#27963;&#22320;&#28385;&#36275;&#19981;&#21516;LLMs&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#22810;&#26679;&#21270;&#20449;&#24687;&#32534;&#30721;&#38656;&#27714;&#65288;&#22914;&#32534;&#30721;&#27169;&#22411;&#29256;&#26412;&#12289;&#29983;&#25104;&#26102;&#38388;&#12289;&#29992;&#25143;ID&#31561;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;LLMs&#30340;&#21487;&#32534;&#35299;&#30721;&#25991;&#26412;&#27700;&#21360;&#65288;CTWL&#65289;&#36827;&#34892;&#20102;&#31995;&#32479;&#30740;&#31350;&#65292;&#20801;&#35768;&#25991;&#26412;&#27700;&#21360;&#25658;&#24102;&#26356;&#22810;&#21487;&#23450;&#21046;&#21270;&#30340;&#20449;&#24687;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#27700;&#21360;&#25216;&#26415;&#30340;&#20998;&#31867;&#65292;&#20026;CTWL&#25552;&#20379;&#20102;&#25968;&#23398;&#20844;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#30740;&#31350;&#35270;&#22270;&#65292;&#28085;&#30422;&#20102;CTWL&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns into the generated texts. However, we argue that existing watermarking methods for LLMs are encoding-inefficient (only contain one bit of information whether it is generated from an LLM or not) and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.) in different LLMs application scenarios. In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry more customizable information. First of all, we study the taxonomy of LLM watermarking technology and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2307.15176</link><description>&lt;p&gt;
RCT&#25298;&#32477;&#25277;&#26679;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15176
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#30340;&#26032;&#25277;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#22240;&#26524;&#20272;&#35745;&#35780;&#20272;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23376;&#25277;&#26679;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#26159;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26080;&#20559;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#12290;&#23545;&#20110;&#39640;&#32500;&#21327;&#21464;&#37327;&#30340;&#24773;&#20917;&#65292;&#22914;&#25991;&#26412;&#25968;&#25454;&#12289;&#22522;&#22240;&#32452;&#23398;&#25110;&#34892;&#20026;&#31038;&#20250;&#31185;&#23398;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36866;&#24212;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#30340;&#35843;&#25972;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#25972;&#26041;&#27861;&#30340;&#32463;&#39564;&#35780;&#20272;&#19968;&#30452;&#23384;&#22312;&#22256;&#38590;&#21644;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#32463;&#39564;&#35780;&#20272;&#31574;&#30053;&#65292;&#31616;&#21270;&#20102;&#35780;&#20272;&#35774;&#35745;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#65306;&#23545;&#38543;&#26426;&#25511;&#21046;&#35797;&#39564;(RCT)&#36827;&#34892;&#23376;&#25277;&#26679;&#65292;&#20197;&#21019;&#24314;&#28151;&#28102;&#30340;&#35266;&#27979;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20351;&#29992;RCT&#30340;&#24179;&#22343;&#22240;&#26524;&#25928;&#24212;&#20316;&#20026;&#22522;&#20934;&#30495;&#23454;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25277;&#26679;&#31639;&#27861;&#65292;&#31216;&#20026;RCT&#25298;&#32477;&#25277;&#26679;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#30830;&#20445;&#35266;&#27979;&#25968;&#25454;&#30340;&#22240;&#26524;&#35782;&#21035;&#25104;&#31435;&#65292;&#20174;&#32780;&#21487;&#20197;&#19982;&#22522;&#20934;RCT&#36827;&#34892;&#26377;&#25928;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;...
&lt;/p&gt;
&lt;p&gt;
Confounding is a significant obstacle to unbiased estimation of causal effects from observational data. For settings with high-dimensional covariates -- such as text data, genomics, or the behavioral social sciences -researchers have proposed methods to adjust for confounding by adapting machine learning methods to the goal of causal estimation. However, empirical evaluation of these adjustment methods has been challenging and limited. In this work, we build on a promising empirical evaluation strategy that simplifies evaluation design and uses real data: subsampling randomized controlled trials (RCTs) to create confounded observational datasets while using the average causal effects from the RCTs as ground-truth. We contribute a new sampling algorithm, which we call RCT rejection sampling, and provide theoretical guarantees that causal identification holds in the observational data to allow for valid comparisons to the ground-truth RCT. Using synthetic data, we show our algorithm in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.11845</link><description>&lt;p&gt;
&#38754;&#21521;&#38134;&#34892;&#27969;&#31243;&#33258;&#21160;&#21270;&#30340;&#22810;&#27169;&#24335;&#25991;&#26723;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#24212;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#21644;&#25552;&#39640;&#38134;&#34892;&#19994;&#21153;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#36890;&#36807;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#25216;&#26415;&#65292;&#30740;&#31350;&#20102;&#38134;&#34892;&#27969;&#31243;&#20013;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#65292;&#24182;&#23637;&#31034;&#20102;LayoutXLM&#31561;&#27169;&#22411;&#22312;&#20998;&#26512;&#38134;&#34892;&#25991;&#26723;&#20013;&#30340;&#28508;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#31185;&#25216;&#31454;&#20105;&#30340;&#22686;&#38271;&#21644;&#25552;&#39640;&#36816;&#33829;&#25928;&#29575;&#30340;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#20110;&#29702;&#35299;&#22312;&#38134;&#34892;&#27969;&#31243;&#20013;&#21033;&#29992;&#22810;&#27169;&#24335;&#27169;&#22411;&#29305;&#21035;&#26159;&#20808;&#36827;&#30340;&#25991;&#26723;&#20998;&#26512;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23545;&#22810;&#26679;&#21270;&#30340;&#38134;&#34892;&#25991;&#26723;&#39046;&#22495;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#36890;&#36807;&#33258;&#21160;&#21270;&#21644;&#20808;&#36827;&#30340;&#20998;&#26512;&#25216;&#26415;&#22312;&#23458;&#25143;&#19994;&#21153;&#20013;&#25552;&#39640;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#22522;&#20110;&#24555;&#36895;&#21457;&#23637;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35832;&#22914;LayoutXLM&#36825;&#26679;&#30340;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#23427;&#26159;&#19968;&#31181;&#36328;&#35821;&#35328;&#12289;&#22810;&#27169;&#24335;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#38134;&#34892;&#19994;&#20013;&#21508;&#31181;&#19981;&#21516;&#30340;&#25991;&#26723;&#12290;&#35813;&#27169;&#22411;&#23545;&#24503;&#22269;&#20844;&#21496;&#30331;&#35760;&#25552;&#21462;&#30340;&#25991;&#26412;&#26631;&#35760;&#20998;&#31867;&#20855;&#26377;&#22823;&#32422;80%&#30340;F1&#24471;&#20998;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#35777;&#25454;&#35777;&#23454;&#20102;&#24067;&#23616;&#20449;&#24687;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#30340;&#20851;&#38190;&#20316;&#29992;&#65292;&#24182;&#36827;&#19968;&#27493;&#24378;&#35843;&#20102;&#25972;&#21512;&#22270;&#20687;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to growing FinTech competition and the need for improved operational efficiency, this research focuses on understanding the potential of advanced document analytics, particularly using multimodal models, in banking processes. We perform a comprehensive analysis of the diverse banking document landscape, highlighting the opportunities for efficiency gains through automation and advanced analytics techniques in the customer business. Building on the rapidly evolving field of natural language processing (NLP), we illustrate the potential of models such as LayoutXLM, a cross-lingual, multimodal, pre-trained model, for analyzing diverse documents in the banking sector. This model performs a text token classification on German company register extracts with an overall F1 score performance of around 80\%. Our empirical evidence confirms the critical role of layout information in improving model performance and further underscores the benefits of integrating image information. Inte
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;</title><link>http://arxiv.org/abs/2305.13547</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup&#65306;&#22686;&#24378;&#23569;&#26679;&#26412;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Self-Evolution Learning for Mixup: Enhance Data Augmentation on Few-Shot Text Classification Tasks. (arXiv:2305.13547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13547
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#24448;&#24448;&#36935;&#21040;&#26377;&#38480;&#26631;&#27880;&#25968;&#25454;&#30340;&#23569;&#26679;&#26412;&#22330;&#26223;&#65292;&#35299;&#20915;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#20351;&#29992; Mixup &#36827;&#34892;&#25968;&#25454;&#25193;&#20805;&#24050;&#32463;&#22312;&#21508;&#31181;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968; Mixup &#26041;&#27861;&#24182;&#19981;&#32771;&#34385;&#35757;&#32451;&#19981;&#21516;&#38454;&#27573;&#30340;&#23398;&#20064;&#38590;&#24230;&#24046;&#24322;&#24182;&#20135;&#29983;&#24102;&#26377; one hot &#26631;&#31614;&#30340;&#26032;&#26679;&#26412;&#65292;&#23548;&#33268;&#27169;&#22411;&#36807;&#20110;&#33258;&#20449;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#25105;&#36827;&#21270;&#23398;&#20064;&#65288;SE&#65289;&#30340; Mixup &#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#30340;&#25968;&#25454;&#25193;&#20805;&#65292;&#21487;&#20197;&#20026;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#26356;&#21152;&#36866;&#24212;&#21644;&#21451;&#22909;&#30340;&#20266;&#26679;&#26412;&#12290;SE &#20851;&#27880;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#21464;&#21270;&#12290;&#20026;&#20102;&#20943;&#36731;&#27169;&#22411;&#32622;&#20449;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#20363;&#26631;&#31614;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32447;&#24615;&#25554;&#20540;&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#21407;&#22987;&#26679;&#26412;&#30340; one-hot &#26631;&#31614;&#65292;&#20197;&#29983;&#25104;&#26032;&#30340;&#36719;&#26631;&#31614;&#29992;&#20110;&#28151;&#21512;&#12290;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#22312;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#29575;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#30340;overconfidence&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification tasks often encounter few shot scenarios with limited labeled data, and addressing data scarcity is crucial. Data augmentation with mixup has shown to be effective on various text classification tasks. However, most of the mixup methods do not consider the varying degree of learning difficulty in different stages of training and generate new samples with one hot labels, resulting in the model over confidence. In this paper, we propose a self evolution learning (SE) based mixup approach for data augmentation in text classification, which can generate more adaptive and model friendly pesudo samples for the model training. SE focuses on the variation of the model's learning ability. To alleviate the model confidence, we introduce a novel instance specific label smoothing approach, which linearly interpolates the model's output and one hot labels of the original samples to generate new soft for label mixing up. Through experimental analysis, in addition to improving cla
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#20013;&#25552;&#31034;&#25991;&#26412;&#26500;&#24314;&#38382;&#39064;&#23637;&#24320;&#20102;&#32508;&#21512;&#25506;&#31350;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11853</link><description>&lt;p&gt;
&#22914;&#20309;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#23398;&#20064;: &#20174;&#38646;&#26679;&#26412;&#21040;&#21333;&#39046;&#22495;&#21040;&#36328;&#39046;&#22495;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings. (arXiv:2305.11853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24341;&#23548;LLMs&#36827;&#34892;&#25991;&#26412;&#21040;SQL&#30340;&#20219;&#21153;&#20013;&#25552;&#31034;&#25991;&#26412;&#26500;&#24314;&#38382;&#39064;&#23637;&#24320;&#20102;&#32508;&#21512;&#25506;&#31350;&#65292;&#20174;&#32780;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#21508;&#31181;&#28436;&#31034;-&#26816;&#32034;&#31574;&#30053;&#21644;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#26469;&#20419;&#20351;LLMs&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#22312;&#26500;&#24314;&#25991;&#26412;&#21040;SQL&#36755;&#20837;&#30340;&#25552;&#31034;&#25991;&#26412;(&#22914;&#25968;&#25454;&#24211;&#21644;&#28436;&#31034;&#31034;&#20363;)&#26102;&#24120;&#37319;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#36825;&#23548;&#33268;&#25552;&#31034;&#25991;&#26412;&#30340;&#26500;&#24314;&#21644;&#20854;&#20027;&#35201;&#36129;&#29486;&#30340;&#21487;&#27604;&#24615;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#36873;&#25321;&#26377;&#25928;&#30340;&#25552;&#31034;&#25991;&#26412;&#24314;&#35774;&#24050;&#25104;&#20026;&#26410;&#26469;&#30740;&#31350;&#20013;&#30340;&#25345;&#20037;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;&#19981;&#21516;&#35774;&#32622;&#19979;&#25552;&#31034;&#25991;&#26412;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) with in-context learning have demonstrated remarkable capability in the text-to-SQL task. Previous research has prompted LLMs with various demonstration-retrieval strategies and intermediate reasoning steps to enhance the performance of LLMs. However, those works often employ varied strategies when constructing the prompt text for text-to-SQL inputs, such as databases and demonstration examples. This leads to a lack of comparability in both the prompt constructions and their primary contributions. Furthermore, selecting an effective prompt construction has emerged as a persistent problem for future research. To address this limitation, we comprehensively investigate the impact of prompt constructions across various settings and provide insights for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2305.09620</link><description>&lt;p&gt;
AI&#22686;&#24378;&#30340;&#35843;&#26597;&#65306;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
AI-Augmented Surveys: Leveraging Large Language Models for Opinion Prediction in Nationally Representative Surveys. (arXiv:2305.09620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#30340;&#35266;&#28857;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#22312;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#21644;&#22238;&#28335;&#25512;&#29702;&#26041;&#38754;&#20248;&#31168;&#30340;&#25104;&#26524;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#26041;&#38754;&#20173;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20351;&#29992;&#32463;&#36807;&#20840;&#22269;&#20195;&#34920;&#24615;&#35843;&#26597;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#22686;&#24378;&#35843;&#26597;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#22312;&#35266;&#28857;&#39044;&#27979;&#20013;&#65292;&#36951;&#28431;&#25968;&#25454;&#25554;&#20540;&#65292;&#22238;&#28335;&#25512;&#29702;&#21644;&#38646;&#27425;&#39044;&#27979;&#19977;&#20010;&#19981;&#21516;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#35770;&#26694;&#26550;&#65292;&#23558;&#35843;&#26597;&#38382;&#39064;&#12289;&#20010;&#20154;&#20449;&#24565;&#21644;&#26102;&#38388;&#32972;&#26223;&#30340;&#31070;&#32463;&#23884;&#20837;&#24341;&#20837;&#21040;&#35266;&#28857;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;LLMs&#20013;&#12290;&#22312;1972&#24180;&#21040;2021&#24180;&#30340;&#8220;&#24120;&#35268;&#31038;&#20250;&#35843;&#26597;&#8221;&#20013;&#65292;&#25105;&#20204;&#20174;68,846&#21517;&#32654;&#22269;&#20154;&#20013;&#33719;&#24471;&#20102;3,110&#20010;&#20108;&#36827;&#21046;&#35266;&#28857;&#65292;&#22312;Alpaca-7b&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#21462;&#24471;&#20102;&#26368;&#22909;&#30340;&#25104;&#26524;&#65292;&#22312;&#32570;&#22833;&#25968;&#25454;&#25554;&#20540;&#65288;AUC=0.87&#65292;&#20844;&#24320;&#35266;&#28857;&#39044;&#27979;&#20026;$\rho$=0.99&#65289;&#21644;&#22238;&#28335;&#25512;&#29702;&#65288;AUC=0.86&#65292;$\rho$=0.98&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36825;&#20123;&#26174;&#33879;&#30340;&#39044;&#27979;&#33021;&#21147;&#33021;&#22815;&#20197;&#39640;&#32622;&#20449;&#24230;&#22635;&#34917;&#32570;&#22833;&#30340;&#36235;&#21183;&#65292;&#24182;&#26631;&#26126;&#20844;&#20247;&#24577;&#24230;&#20309;&#26102;&#21457;&#29983;&#21464;&#21270;&#65292;&#22914;&#21516;&#24615;&#23130;&#23035;&#30340;&#33719;&#21462;&#25903;&#25345;&#12290;&#28982;&#32780;&#65292;&#22312;&#38646;&#27425;&#39044;&#27979;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#21463;&#21040;&#38480;&#21046;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we use large language models (LLMs) to augment surveys? This paper investigates three distinct applications of LLMs fine-tuned by nationally representative surveys for opinion prediction -- missing data imputation, retrodiction, and zero-shot prediction. We present a new methodological framework that incorporates neural embeddings of survey questions, individual beliefs, and temporal contexts to personalize LLMs in opinion prediction. Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our best models based on Alpaca-7b excels in missing data imputation (AUC = 0.87 for personal opinion prediction and $\rho$ = 0.99 for public opinion prediction) and retrodiction (AUC = 0.86, $\rho$ = 0.98). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. However, the models show limited performance in a zer
&lt;/p&gt;</description></item><item><title>Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;</title><link>http://arxiv.org/abs/2305.07637</link><description>&lt;p&gt;
Text2Cohort: &#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#23545;&#30284;&#30151;&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;&#30340;&#27665;&#20027;&#21270;
&lt;/p&gt;
&lt;p&gt;
Text2Cohort: Democratizing the NCI Imaging Data Commons with Natural Language Cohort Discovery. (arXiv:2305.07637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07637
&lt;/p&gt;
&lt;p&gt;
Text2Cohort&#26159;&#19968;&#20010;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#31665;&#65292;&#21487;&#20197;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#65292;&#20943;&#23569;&#30740;&#31350;&#20154;&#21592;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#30340;&#23398;&#20064;&#26354;&#32447;&#65292;&#23454;&#29616;&#20102;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#30340;&#27665;&#20027;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24433;&#20687;&#25968;&#25454;&#20849;&#20139;&#24179;&#21488;(IDC)&#26159;&#19968;&#20010;&#22522;&#20110;&#20113;&#30340;&#25968;&#25454;&#24211;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#24320;&#25918;&#33719;&#21462;&#30340;&#30284;&#30151;&#25104;&#20687;&#25968;&#25454;&#21644;&#20998;&#26512;&#24037;&#20855;&#65292;&#26088;&#22312;&#20419;&#36827;&#21307;&#23398;&#25104;&#20687;&#30740;&#31350;&#20013;&#30340;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#25216;&#26415;&#24615;&#36136;&#65292;&#26597;&#35810;IDC&#25968;&#25454;&#24211;&#20197;&#36827;&#34892;&#38431;&#21015;&#21457;&#29616;&#21644;&#35775;&#38382;&#25104;&#20687;&#25968;&#25454;&#23545;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#20855;&#26377;&#26174;&#33879;&#30340;&#23398;&#20064;&#26354;&#32447;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;Text2Cohort&#24037;&#20855;&#31665;&#65292;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#23558;&#29992;&#25143;&#36755;&#20837;&#36716;&#21270;&#20026;IDC&#25968;&#25454;&#24211;&#26597;&#35810;&#65292;&#24182;&#23558;&#26597;&#35810;&#30340;&#21709;&#24212;&#36820;&#22238;&#32473;&#29992;&#25143;&#65292;&#20197;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#38431;&#21015;&#21457;&#29616;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#26657;&#27491;&#20197;&#35299;&#20915;&#26597;&#35810;&#20013;&#30340;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#65292;&#36890;&#36807;&#23558;&#38169;&#35823;&#20256;&#22238;&#27169;&#22411;&#36827;&#34892;&#35299;&#37322;&#21644;&#26657;&#27491;&#12290;&#25105;&#20204;&#23545;50&#20010;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#20102;Text2Cohort&#35780;&#20272;&#65292;&#33539;&#22260;&#20174;&#20449;&#24687;&#25552;&#21462;&#21040;&#38431;&#21015;&#21457;&#29616;&#12290;&#32467;&#26524;&#26597;&#35810;&#21644;&#36755;&#20986;&#30001;&#20004;&#20301;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Imaging Data Commons (IDC) is a cloud-based database that provides researchers with open access to cancer imaging data and tools for analysis, with the goal of facilitating collaboration in medical imaging research. However, querying the IDC database for cohort discovery and access to imaging data has a significant learning curve for researchers due to its complex and technical nature. We developed Text2Cohort, a large language model (LLM) based toolkit to facilitate natural language cohort discovery by translating user input into IDC database queries through prompt engineering and returning the query's response to the user. Furthermore, autocorrection is implemented to resolve syntax and semantic errors in queries by passing the errors back to the model for interpretation and correction. We evaluate Text2Cohort on 50 natural language user inputs ranging from information extraction to cohort discovery. The resulting queries and outputs were verified by two computer scientists to me
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#36866;&#37197;&#22120;&#65288;CoDA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#23494;&#38598;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#22686;&#21152;&#31232;&#30095;&#28608;&#27963;&#12289;&#23569;&#37327;&#26032;&#21442;&#25968;&#20197;&#21450;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#38454;&#27573;&#26469;&#23454;&#29616;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;2&#20493;&#33267;8&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#19988;&#20934;&#30830;&#29575;&#26377;&#36731;&#24494;&#25110;&#26080;&#25439;&#22833;&#65292;&#19988;&#21442;&#25968;&#25928;&#29575;&#30456;&#21516;&#12290;</title><link>http://arxiv.org/abs/2304.04947</link><description>&lt;p&gt;
&#26465;&#20214;&#36866;&#37197;&#22120;&#65306;&#20855;&#26377;&#24555;&#36895;&#25512;&#29702;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Conditional Adapters: Parameter-efficient Transfer Learning with Fast Inference. (arXiv:2304.04947v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04947
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#36866;&#37197;&#22120;&#65288;CoDA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#23494;&#38598;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#22686;&#21152;&#31232;&#30095;&#28608;&#27963;&#12289;&#23569;&#37327;&#26032;&#21442;&#25968;&#20197;&#21450;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#38454;&#27573;&#26469;&#23454;&#29616;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#24335;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;2&#20493;&#33267;8&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#19988;&#20934;&#30830;&#29575;&#26377;&#36731;&#24494;&#25110;&#26080;&#25439;&#22833;&#65292;&#19988;&#21442;&#25968;&#25928;&#29575;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#26465;&#20214;&#36866;&#37197;&#22120;&#65288;CoDA&#65289;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25512;&#29702;&#25928;&#29575;&#12290;CoDA&#19981;&#20165;&#36866;&#29992;&#20110;&#26631;&#20934;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#26469;&#23454;&#29616;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#29616;&#26377;&#30340;&#23494;&#38598;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#22686;&#21152;&#31232;&#30095;&#28608;&#27963;&#12289;&#23569;&#37327;&#26032;&#21442;&#25968;&#20197;&#21450;&#36731;&#37327;&#32423;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;CoDA&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20986;&#20046;&#24847;&#26009;&#30340;&#20256;&#36882;&#30693;&#35782;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#30456;&#27604;&#65292;CoDA&#22312;&#21508;&#31181;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#37117;&#23454;&#29616;&#20102;2&#20493;&#33267;8&#20493;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#32780;&#19988;&#20934;&#30830;&#29575;&#26377;&#36731;&#24494;&#25110;&#26080;&#25439;&#22833;&#65292;&#19988;&#21442;&#25968;&#25928;&#29575;&#30456;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Conditional Adapter (CoDA), a parameter-efficient transfer learning method that also improves inference efficiency. CoDA generalizes beyond standard adapter approaches to enable a new way of balancing speed and accuracy using conditional computation. Starting with an existing dense pretrained model, CoDA adds sparse activation together with a small number of new parameters and a light-weight training phase. Our experiments demonstrate that the CoDA approach provides an unexpectedly efficient way to transfer knowledge. Across a variety of language, vision, and speech tasks, CoDA achieves a 2x to 8x inference speed-up compared to the state-of-the-art Adapter approach with moderate to no accuracy loss and the same parameter efficiency.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06373</link><description>&lt;p&gt;
InferEM: &#25512;&#26029;&#35828;&#35805;&#32773;&#24847;&#22270;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
InferEM: Inferring the Speaker's Intention for Empathetic Dialogue Generation. (arXiv:2212.06373v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06373
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25512;&#26029;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#26469;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#30340;&#20849;&#24773;&#23545;&#35805;&#29983;&#25104;&#27169;&#22411;InferEM&#12290;&#27169;&#22411;&#21516;&#26102;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#30340;&#26041;&#27861;&#19968;&#33324;&#30452;&#25509;&#32534;&#30721;&#25972;&#20010;&#23545;&#35805;&#21382;&#21490;&#65292;&#28982;&#21518;&#36890;&#36807;&#35299;&#30721;&#22120;&#29983;&#25104;&#21451;&#22909;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#26041;&#27861;&#24378;&#35843;&#24314;&#27169;&#24773;&#22659;&#20449;&#24687;&#65292;&#20294;&#24573;&#35270;&#20102;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#30452;&#25509;&#24847;&#22270;&#12290;&#25105;&#20204;&#35748;&#20026;&#23545;&#35805;&#20013;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#34920;&#36798;&#20102;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InferEM&#30340;&#26032;&#27169;&#22411;&#29992;&#20110;&#20849;&#24773;&#22238;&#22797;&#29983;&#25104;&#12290;&#25105;&#20204;&#23558;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#21333;&#29420;&#32534;&#30721;&#65292;&#36890;&#36807;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#24847;&#22270;&#34701;&#21512;&#27169;&#22359;&#19982;&#25972;&#20010;&#23545;&#35805;&#34701;&#21512;&#20197;&#25429;&#25417;&#35828;&#35805;&#32773;&#30340;&#24847;&#22270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#21069;&#20960;&#27425;&#21457;&#35328;&#39044;&#27979;&#26368;&#21518;&#19968;&#27425;&#21457;&#35328;&#65292;&#20197;&#27169;&#25311;&#20154;&#31867;&#30340;&#24515;&#29702;&#65292;&#29468;&#27979;&#23545;&#35805;&#32773;&#21487;&#33021;&#25552;&#21069;&#35828;&#20123;&#20160;&#20040;&#12290;&#20026;&#24179;&#34913;&#21457;&#35328;&#39044;&#27979;&#21644;&#22238;&#22797;&#29983;&#25104;&#30340;&#20248;&#21270;&#36895;&#29575;&#65292;InferEM&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current approaches to empathetic response generation typically encode the entire dialogue history directly and put the output into a decoder to generate friendly feedback. These methods focus on modelling contextual information but neglect capturing the direct intention of the speaker. We argue that the last utterance in the dialogue empirically conveys the intention of the speaker. Consequently, we propose a novel model named InferEM for empathetic response generation. We separately encode the last utterance and fuse it with the entire dialogue through the multi-head attention based intention fusion module to capture the speaker's intention. Besides, we utilize previous utterances to predict the last utterance, which simulates human's psychology to guess what the interlocutor may speak in advance. To balance the optimizing rates of the utterance prediction and response generation, a multi-task learning strategy is designed for InferEM. Experimental results demonstrate the plausibility
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2206.10498</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20173;&#26080;&#27861;&#35268;&#21010;&#65288;LLM&#22312;&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#20013;&#30340;&#22522;&#20934;&#65289;&#12290;&#65288;arXiv:2206.10498v3 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change). (arXiv:2206.10498v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10498
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27979;&#35780;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#65292;&#24182;&#27979;&#35797;&#20102;&#27969;&#34892;&#30340;LLMs (GPT-3 &#21644; GShard) &#22312;&#27492;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#19978;&#37117;&#34920;&#29616;&#19981;&#20339;&#65292;&#24378;&#35843;&#20102;&#30446;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#38480;&#21046;&#65292;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#26356;&#20808;&#36827;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#26469;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#20174;GPT-3&#21040;PaLM&#65292;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#30340;&#26368;&#26032;&#24615;&#33021;&#27491;&#22312;&#38543;&#30528;&#27599;&#20010;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#20986;&#19981;&#26029;&#25552;&#39640;&#12290;&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#33021;&#21147;&#22806;&#65292;&#20154;&#20204;&#23545;&#20110;&#29702;&#35299;&#27492;&#31867;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#25512;&#29702;&#33021;&#21147;&#20135;&#29983;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#37319;&#29992;&#20102;&#25512;&#29702;&#22522;&#20934;&#26469;&#36827;&#34892;&#27979;&#35780;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#32467;&#26524;&#30475;&#20284;&#31215;&#26497;&#65292;&#36825;&#20123;&#22522;&#20934;&#22312;&#26412;&#36136;&#19978;&#26159;&#31616;&#21333;&#30340;&#65292;LLMs&#22312;&#36825;&#20123;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#24182;&#19981;&#33021;&#20316;&#20026;&#25903;&#25345;LLMs&#25512;&#29702;&#33021;&#21147;&#65288;&#26377;&#26102;&#26159;&#33618;&#35884;&#30340;&#65289;&#22768;&#31216;&#30340;&#35777;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21482;&#20195;&#34920;&#20102;&#19968;&#20010;&#38750;&#24120;&#26377;&#38480;&#30340;&#31616;&#21333;&#25512;&#29702;&#20219;&#21153;&#38598;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#34913;&#37327;&#27492;&#31867;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#30340;&#30495;&#27491;&#38480;&#21046;&#65292;&#25105;&#20204;&#38656;&#35201;&#30740;&#31350;&#26356;&#22797;&#26434;&#30340;&#25512;&#29702;&#38382;&#39064;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#29992;&#20110;&#27979;&#35797;LLMs&#35268;&#21010;&#21644;&#21464;&#21270;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#19968;&#31995;&#21015;&#30340;&#35268;&#21010;&#21644;&#25512;&#29702;&#20219;&#21153;&#65292;&#20363;&#22914;&#21629;&#39064;&#36923;&#36753;&#12289;&#22240;&#26524;&#25512;&#26029;&#21644;&#24120;&#35782;&#25512;&#29702;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#38590;&#24230;&#38543;&#30528;&#20219;&#21153;&#30340;&#36827;&#23637;&#32780;&#36880;&#28176;&#22686;&#21152;&#12290;&#25105;&#20204;&#27979;&#37327;&#20102;&#20004;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;GPT-3&#21644;GShard&#65289;&#22312;&#36825;&#20010;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#29978;&#33267;&#26080;&#27861;&#22788;&#29702;&#26368;&#31616;&#21333;&#30340;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#24403;&#21069;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#20005;&#37325;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#38656;&#35201;&#22823;&#37327;&#24037;&#20316;&#26469;&#24320;&#21457;&#21487;&#20197;&#35268;&#21010;&#21644;&#25512;&#29702;&#21464;&#21270;&#30340;LLM&#22522;&#30784;&#31995;&#32479;&#65292;&#20197;&#28385;&#36275;&#23454;&#38469;&#24212;&#29992;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have transformed the field of natural language processing (NLP). From GPT-3 to PaLM, the state-of-the-art performance on natural language tasks is being pushed forward with every new large language model. Along with natural language abilities, there has been a significant interest in understanding whether such models exhibit reasoning capabilities with the use of reasoning benchmarks. However, even though results are seemingly positive, these benchmarks prove to be simplistic in nature and the performance of LLMs on these benchmarks cannot be used as evidence to support, many a times outlandish, claims being made about LLMs' reasoning capabilities. Further, these only represent a very limited set of simple reasoning tasks and we need to look at more sophisticated reasoning problems if we are to measure the true limits of such LLM-based systems. Motivated by this, we propose an extensible assessment framework to test the capabilities of LL
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;</title><link>http://arxiv.org/abs/2102.00225</link><description>&lt;p&gt;
&#20174;&#20154;&#31867;&#30340;&#32416;&#38169;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning From How Humans Correct. (arXiv:2102.00225v14 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2102.00225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20154;&#31867;&#30699;&#27491;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#26631;&#27880;&#25968;&#25454;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#27880;&#20837;&#33267;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#23558;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24230;&#25552;&#21319;&#20102;1.7&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#25163;&#21160;&#26631;&#27880;&#30340;&#25968;&#25454;&#20013;&#23384;&#22312;&#19968;&#23450;&#25968;&#37327;&#30340;&#22122;&#22768;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#25214;&#21040;&#22122;&#22768;&#25968;&#25454;&#24182;&#25163;&#21160;&#37325;&#26032;&#26631;&#27880;&#23427;&#20204;&#65292;&#21516;&#26102;&#25910;&#38598;&#32416;&#38169;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20154;&#31867;&#32416;&#38169;&#20449;&#24687;&#34701;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#12290;&#20154;&#31867;&#30693;&#36947;&#22914;&#20309;&#32416;&#27491;&#22122;&#22768;&#25968;&#25454;&#65292;&#22240;&#27492;&#32416;&#38169;&#20449;&#24687;&#21487;&#20197;&#27880;&#20837;&#21040;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#25105;&#20204;&#22312;&#33258;&#24049;&#30340;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#25163;&#21160;&#26631;&#27880;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#37325;&#26032;&#26631;&#27880;&#20102;&#25105;&#20204;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#25968;&#25454;&#65292;&#20197;&#36866;&#29992;&#20110;&#25105;&#20204;&#30340;&#24037;&#19994;&#24212;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20998;&#31867;&#20934;&#30830;&#24230;&#20174;91.7%&#25552;&#21319;&#21040;92.5%&#12290;91.7%&#30340;&#20934;&#30830;&#24230;&#26159;&#22312;&#20462;&#27491;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#23427;&#23558;&#22522;&#32447;&#20934;&#30830;&#24230;&#20174;83.3%&#25552;&#21319;&#21040;91.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry NLP application, our manually labeled data has a certain number of noisy data. We present a simple method to find the noisy data and re-label them manually, meanwhile we collect the correction information. Then we present novel method to incorporate the human correction information into deep learning model. Human know how to correct noisy data. So the correction information can be inject into deep learning model. We do the experiment on our own text classification dataset, which is manually labeled, because we re-label the noisy data in our dataset for our industry application. The experiment result shows that our method improve the classification accuracy from 91.7% to 92.5%. The 91.7% accuracy is trained on the corrected dataset, which improve the baseline from 83.3% to 91.7%.
&lt;/p&gt;</description></item></channel></rss>