<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>HARE&#26159;&#19968;&#20010;&#25903;&#25345;&#36880;&#27493;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22635;&#34917;&#29616;&#26377;&#27880;&#37322;&#26041;&#26696;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#30417;&#30563;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.00321</link><description>&lt;p&gt;
HARE: &#25903;&#25345;&#36880;&#27493;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HARE: Explainable Hate Speech Detection with Step-by-Step Reasoning. (arXiv:2311.00321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00321
&lt;/p&gt;
&lt;p&gt;
HARE&#26159;&#19968;&#20010;&#25903;&#25345;&#36880;&#27493;&#25512;&#29702;&#30340;&#21487;&#35299;&#37322;&#24615;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22635;&#34917;&#29616;&#26377;&#27880;&#37322;&#26041;&#26696;&#30340;&#25512;&#29702;&#24046;&#36317;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#30417;&#30563;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#20934;&#30830;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#20197;&#30830;&#20445;&#22312;&#32447;&#23433;&#20840;&#12290;&#20026;&#20102;&#24212;&#23545;&#32454;&#24494;&#30340;&#20167;&#24680;&#35328;&#35770;&#24418;&#24335;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#35782;&#21035;&#24182;&#35814;&#32454;&#35299;&#37322;&#20167;&#24680;&#35328;&#35770;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#29702;&#35299;&#20854;&#26377;&#23475;&#24433;&#21709;&#12290;&#26368;&#36817;&#30340;&#22522;&#20934;&#27979;&#35797;&#35797;&#22270;&#36890;&#36807;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#26469;&#22788;&#29702;&#20167;&#24680;&#25991;&#26412;&#20013;&#21547;&#20041;&#30340;&#33258;&#30001;&#25991;&#26412;&#27880;&#37322;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#29616;&#26377;&#27880;&#37322;&#26041;&#26696;&#23384;&#22312;&#37325;&#22823;&#25512;&#29702;&#24046;&#36317;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#26816;&#27979;&#27169;&#22411;&#30340;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;HARE&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#26469;&#22635;&#34917;&#36825;&#20123;&#20851;&#20110;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#26816;&#27979;&#27169;&#22411;&#30417;&#30563;&#12290;&#22312;SBIC&#21644;Implicit Hate&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#22987;&#32456;&#20248;&#20110;&#20351;&#29992;&#29616;&#26377;&#33258;&#30001;&#25991;&#26412;&#27880;&#37322;&#30340;&#22522;&#20934;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the proliferation of social media, accurate detection of hate speech has become critical to ensure safety online. To combat nuanced forms of hate speech, it is important to identify and thoroughly explain hate speech to help users understand its harmful effects. Recent benchmarks have attempted to tackle this issue by training generative models on free-text annotations of implications in hateful text. However, we find significant reasoning gaps in the existing annotations schemes, which may hinder the supervision of detection models. In this paper, we introduce a hate speech detection framework, HARE, which harnesses the reasoning capabilities of large language models (LLMs) to fill these gaps in explanations of hate speech, thus enabling effective supervision of detection models. Experiments on SBIC and Implicit Hate benchmarks show that our method, using model-generated data, consistently outperforms baselines, using existing free-text human annotations. Analysis demonstrates th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.12942</link><description>&lt;p&gt;
&#20851;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Representational Capacity of Recurrent Neural Language Models. (arXiv:2310.12942v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12942
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#65292;&#25193;&#23637;&#20102;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#24182;&#25552;&#20379;&#20102;&#19978;&#19979;&#30028;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNNs)&#30340;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#35745;&#31639;&#34920;&#36798;&#24615;&#12290;Siegelmann&#21644;Sontag(1992)&#26366;&#32463;&#23637;&#31034;&#20102;&#20855;&#26377;&#26377;&#29702;&#26435;&#37325;&#21644;&#38544;&#34255;&#29366;&#24577;&#20197;&#21450;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#30340;RNNs&#26159;&#22270;&#28789;&#23436;&#22791;&#30340;&#12290;&#28982;&#32780;&#65292;LMs&#19981;&#20165;&#23450;&#20041;&#20102;&#23383;&#31526;&#20018;&#19978;&#30340;&#21152;&#26435;&#65292;&#36824;&#23450;&#20041;&#20102;(&#38750;&#21152;&#26435;)&#35821;&#35328;&#25104;&#21592;&#20851;&#31995;&#65292;&#23545;RNN LMs&#65288;RLMs&#65289;&#30340;&#35745;&#31639;&#33021;&#21147;&#20998;&#26512;&#24212;&#35813;&#21453;&#26144;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#23558;&#22270;&#28789;&#23436;&#22791;&#24615;&#32467;&#26524;&#25193;&#23637;&#21040;&#27010;&#29575;&#24773;&#20917;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26377;&#29702;&#26435;&#37325;&#30340;RLM&#21644;&#26080;&#38480;&#35745;&#31639;&#26102;&#38388;&#26469;&#27169;&#25311;&#20219;&#20309;&#27010;&#29575;&#22270;&#28789;&#26426;(PTM)&#12290;&#30001;&#20110;&#22312;&#23454;&#36341;&#20013;&#65292;RLMs&#23454;&#26102;&#24037;&#20316;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#22788;&#29702;&#19968;&#20010;&#31526;&#21495;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#19978;&#36848;&#32467;&#26524;&#20316;&#20026;RLMs&#34920;&#36798;&#24615;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23637;&#31034;&#22312;&#23454;&#26102;&#35745;&#31639;&#38480;&#21046;&#19979;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#27169;&#25311;&#30830;&#23450;&#24615;&#23454;&#26102;&#26377;&#29702;PTMs&#26469;&#25552;&#20379;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2310.09886</link><description>&lt;p&gt;
&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation. (arXiv:2310.09886v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09886
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#27169;&#22411;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#20915;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#24110;&#21161;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#23545;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#20005;&#37325;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#24207;&#21015;&#29983;&#25104;&#65288;LSG&#65289;&#26159;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#38382;&#39064;&#65292;&#26088;&#22312;&#35753;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#29983;&#25104;&#20219;&#21153;&#19978;&#36827;&#34892;&#25345;&#32493;&#35757;&#32451;&#65292;&#20197;&#19981;&#26029;&#23398;&#20064;&#26032;&#30340;&#29983;&#25104;&#27169;&#24335;&#24182;&#36991;&#20813;&#36951;&#24536;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#29616;&#26377;&#30340;LSG&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#32500;&#25345;&#26087;&#30693;&#35782;&#65292;&#32780;&#23545;&#36328;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#20851;&#27880;&#36739;&#23569;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#33719;&#21462;&#30340;&#31867;&#20284;&#20219;&#21153;&#30340;&#30693;&#35782;&#26356;&#22909;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#21463;&#20154;&#31867;&#23398;&#20064;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#27169;&#22359;&#25193;&#23637;&#21644;&#33258;&#36866;&#24212;&#65288;DMEA&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#27169;&#22411;&#33021;&#22815;&#26681;&#25454;&#20219;&#21153;&#30456;&#20851;&#24615;&#21160;&#24577;&#30830;&#23450;&#33719;&#21462;&#26032;&#30693;&#35782;&#30340;&#26550;&#26500;&#65292;&#24182;&#36873;&#25321;&#26368;&#30456;&#20284;&#30340;&#20808;&#21069;&#20219;&#21153;&#26469;&#20419;&#36827;&#23545;&#26032;&#20219;&#21153;&#30340;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#23398;&#20064;&#36807;&#31243;&#24456;&#23481;&#26131;&#20559;&#21521;&#20110;&#24403;&#21069;&#20219;&#21153;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#26356;&#20005;&#37325;&#30340;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#24577;&#26799;&#24230;&#32553;&#25918;&#26469;&#24179;&#34913;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lifelong sequence generation (LSG), a problem in continual learning, aims to continually train a model on a sequence of generation tasks to learn constantly emerging new generation patterns while avoiding the forgetting of previous knowledge. Existing LSG methods mainly focus on maintaining old knowledge while paying little attention to knowledge transfer across tasks. In contrast, humans can better learn new tasks by leveraging previously acquired knowledge from similar tasks. Inspired by the learning paradigm of humans, we propose Dynamic Module Expansion and Adaptation (DMEA), which enables the model to dynamically determine the architecture for acquiring new knowledge based on task correlation and select the most similar previous tasks to facilitate adaptation to new tasks. In addition, as the learning process can easily be biased towards the current task which might cause more severe forgetting of previously learned knowledge, we propose dynamic gradient scaling to balance the lea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21512;&#24182;&#19987;&#23478;&#8221;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#19987;&#23478;&#30340;&#27700;&#24179;&#26469;&#25913;&#36827;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09832</link><description>&lt;p&gt;
&#21512;&#24182;&#19987;&#23478;&#65306;&#25913;&#36827;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Merging Experts into One: Improving Computational Efficiency of Mixture of Experts. (arXiv:2310.09832v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#21512;&#24182;&#19987;&#23478;&#8221;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#19987;&#23478;&#30340;&#27700;&#24179;&#26469;&#25913;&#36827;&#28151;&#21512;&#19987;&#23478;&#26041;&#27861;&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#35268;&#27169;&#25193;&#22823;&#36890;&#24120;&#20250;&#24102;&#26469;NLP&#20219;&#21153;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24448;&#24448;&#20250;&#20276;&#38543;&#30528;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#31232;&#30095;&#30340;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#21487;&#20197;&#36890;&#36807;&#28608;&#27963;&#27599;&#20010;&#36755;&#20837;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#21442;&#25968;&#65288;&#20363;&#22914;&#19968;&#20010;&#19987;&#23478;&#65289;&#26469;&#20943;&#23569;&#25104;&#26412;&#65292;&#20294;&#22914;&#26524;&#22686;&#21152;&#28608;&#27963;&#30340;&#19987;&#23478;&#25968;&#37327;&#65292;&#20854;&#35745;&#31639;&#23558;&#26174;&#33879;&#22686;&#21152;&#65292;&#38480;&#21046;&#20102;&#20854;&#23454;&#38469;&#25928;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#36873;&#25321;&#22810;&#20010;&#19987;&#23478;&#30340;&#20248;&#36234;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#21512;&#24182;&#19987;&#23478;&#8221;&#65288;MEO&#65289;&#65292;&#23558;&#35745;&#31639;&#25104;&#26412;&#38477;&#20302;&#21040;&#21333;&#20010;&#19987;&#23478;&#30340;&#27700;&#24179;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;MEO&#26174;&#30528;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20363;&#22914;&#65292;FLOPS&#20174;&#26222;&#36890;MoE&#30340;72.0G&#38477;&#20302;&#21040;28.6G&#65288;MEO&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#35760;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling the size of language models usually leads to remarkable advancements in NLP tasks. But it often comes with a price of growing computational cost. Although a sparse Mixture of Experts (MoE) can reduce the cost by activating a small subset of parameters (e.g., one expert) for each input, its computation escalates significantly if increasing the number of activated experts, limiting its practical utility. Can we retain the advantages of adding more experts without substantially increasing the computational costs? In this paper, we first demonstrate the superiority of selecting multiple experts and then propose a computation-efficient approach called \textbf{\texttt{Merging Experts into One}} (MEO), which reduces the computation cost to that of a single expert. Extensive experiments show that MEO significantly improves computational efficiency, e.g., FLOPS drops from 72.0G of vanilla MoE to 28.6G (MEO). Moreover, we propose a token-level attention block that further enhances the ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21047;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#30340;&#20107;&#23454;&#24615;&#65292;&#24341;&#20837;&#20102;FreshQA&#36825;&#19968;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.03214</link><description>&lt;p&gt;
FreshLLMs: &#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#30340;&#26041;&#27861;&#21047;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FreshLLMs: Refreshing Large Language Models with Search Engine Augmentation. (arXiv:2310.03214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25628;&#32034;&#24341;&#25806;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#21047;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#30740;&#31350;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#22238;&#31572;&#38382;&#39064;&#26041;&#38754;&#30340;&#20107;&#23454;&#24615;&#65292;&#24341;&#20837;&#20102;FreshQA&#36825;&#19968;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#12290;&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#26377;&#26174;&#33879;&#30340;&#25913;&#36827;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21482;&#35757;&#32451;&#19968;&#27425;&#65292;&#19981;&#36827;&#34892;&#26356;&#26032;&#65292;&#22240;&#27492;&#32570;&#20047;&#23545;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#21160;&#24577;&#36866;&#24212;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#22312;&#27979;&#35797;&#24403;&#21069;&#19990;&#30028;&#30693;&#35782;&#30340;&#38382;&#39064;&#22238;&#31572;&#30340;&#32972;&#26223;&#19979;&#65292;&#23545;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20107;&#23454;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FreshQA&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#21160;&#24577;&#38382;&#31572;&#22522;&#20934;&#65292;&#21253;&#25324;&#24191;&#27867;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#31867;&#22411;&#65292;&#21253;&#25324;&#38656;&#35201;&#24555;&#36895;&#21464;&#21270;&#30340;&#19990;&#30028;&#30693;&#35782;&#21644;&#38656;&#35201;&#25581;&#31034;&#38169;&#35823;&#21069;&#25552;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21452;&#27169;&#24335;&#35780;&#20272;&#36807;&#31243;&#20013;&#23545;&#22810;&#31181;&#38381;&#28304;&#21644;&#24320;&#28304;LLM&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#21516;&#26102;&#27979;&#37327;&#27491;&#30830;&#24615;&#21644;&#34394;&#26500;&#24615;&#12290;&#36890;&#36807;&#28041;&#21450;&#36229;&#36807;50K&#20010;&#35780;&#21028;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#25913;&#36827;&#30340;&#26174;&#33879;&#31354;&#38388;&#65306;&#20363;&#22914;&#65292;&#25152;&#26377;&#27169;&#22411;&#65288;&#26080;&#35770;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#65289;&#22312;&#28041;&#21450;&#24555;&#36895;&#21464;&#21270;&#30340;&#30693;&#35782;&#21644;&#38169;&#35823;&#21069;&#25552;&#30340;&#38382;&#39064;&#19978;&#37117;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most large language models (LLMs) are trained once and never updated; thus, they lack the ability to dynamically adapt to our ever-changing world. In this work, we perform a detailed study of the factuality of LLM-generated text in the context of answering questions that test current world knowledge. Specifically, we introduce FreshQA, a novel dynamic QA benchmark encompassing a diverse range of question and answer types, including questions that require fast-changing world knowledge as well as questions with false premises that need to be debunked. We benchmark a diverse array of both closed and open-source LLMs under a two-mode evaluation procedure that allows us to measure both correctness and hallucination. Through human evaluations involving more than 50K judgments, we shed light on limitations of these models and demonstrate significant room for improvement: for instance, all models (regardless of model size) struggle on questions that involve fast-changing knowledge and false pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14457</link><description>&lt;p&gt;
&#20026;&#27604;&#36739;&#25512;&#29702;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Pre-training Language Models for Comparative Reasoning. (arXiv:2305.14457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#22686;&#24378;&#20854;&#22312;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#21644;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20197;&#22686;&#24378;&#20854;&#22312;&#25991;&#26412;&#27604;&#36739;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#21487;&#25193;&#23637;&#30340;&#29992;&#20110;&#25910;&#38598;&#22522;&#20110;&#25991;&#26412;&#23454;&#20307;&#27604;&#36739;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19977;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#12290;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#65292;&#21253;&#25324;&#27604;&#36739;&#38382;&#31572;&#12289;&#38382;&#21477;&#29983;&#25104;&#21644;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#22823;&#22823;&#25552;&#39640;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#25512;&#29702;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#21294;&#20047;&#30340;&#24773;&#20917;&#19979;&#12290;&#27492;&#22806;&#65292;&#26412;&#24037;&#20316;&#36824;&#21457;&#24067;&#20102;&#31532;&#19968;&#20010;&#27604;&#36739;&#25512;&#29702;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel framework to pre-train language models for enhancing their abilities of comparative reasoning over texts. While recent research has developed models for NLP tasks that require comparative reasoning, they suffer from costly manual data labeling and limited generalizability to different tasks. Our approach involves a scalable method for collecting data for text-based entity comparison, which leverages both structured and unstructured data, and the design of three novel pre-training tasks. Evaluation on a range of downstream tasks including comparative question answering, question generation, and summarization shows that our pre-training framework significantly improves the comparative reasoning abilities of language models, especially under low-resource conditions. This work also releases the first integrated benchmark for comparative reasoning over texts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#30446;&#21069;&#35821;&#29992;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#24314;&#35758;&#24182;&#20998;&#26512;&#20102;&#35821;&#35328;&#21547;&#20041;&#30340;&#20016;&#23500;&#24615;&#12290;&#26410;&#26469;&#30340;&#20219;&#21153;&#35774;&#35745;&#38656;&#35201;&#24341;&#20986;&#35821;&#29992;&#29616;&#35937;&#65292;&#24182;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#20132;&#27969;&#19978;&#19979;&#25991;&#21644;&#25928;&#30410;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.08371</link><description>&lt;p&gt;
&#35821;&#35328;&#22522;&#30784;&#20013;&#30340;&#35821;&#29992;&#23398;&#65306;&#29616;&#35937;&#12289;&#20219;&#21153;&#21644;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches. (arXiv:2211.08371v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#30446;&#21069;&#35821;&#29992;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#24314;&#35758;&#24182;&#20998;&#26512;&#20102;&#35821;&#35328;&#21547;&#20041;&#30340;&#20016;&#23500;&#24615;&#12290;&#26410;&#26469;&#30340;&#20219;&#21153;&#35774;&#35745;&#38656;&#35201;&#24341;&#20986;&#35821;&#29992;&#29616;&#35937;&#65292;&#24182;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#20132;&#27969;&#19978;&#19979;&#25991;&#21644;&#25928;&#30410;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#20132;&#27969;&#20013;&#32463;&#24120;&#20381;&#36182;&#19978;&#19979;&#25991;&#26469;&#20016;&#23500;&#35328;&#22806;&#20043;&#24847;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#26126;&#32780;&#26377;&#25928;&#30340;&#27807;&#36890;&#12290;&#20026;&#20102;&#33021;&#22815;&#19982;&#20154;&#31867;&#25104;&#21151;&#22320;&#33258;&#28982;&#20132;&#20114;&#65292;&#38754;&#21521;&#29992;&#25143;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23558;&#38656;&#35201;&#31867;&#20284;&#30340;&#35821;&#29992;&#23398;&#25216;&#33021;&#65306;&#20381;&#38752;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#8212;&#8212;&#20174;&#20849;&#20139;&#30340;&#35821;&#35328;&#30446;&#26631;&#21644;&#32422;&#23450;&#21040;&#35270;&#35273;&#21644;&#20855;&#36523;&#19990;&#30028;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#35821;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#35821;&#22659;&#35774;&#32622;&#21644;&#35821;&#29992;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#24037;&#20316;&#20013;&#20219;&#21153;&#30446;&#26631;&#12289;&#29615;&#22659;&#19978;&#19979;&#25991;&#21644;&#20132;&#38469;&#25928;&#30410;&#22914;&#20309;&#20016;&#23500;&#35821;&#35328;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#22522;&#30784;&#20219;&#21153;&#35774;&#35745;&#30340;&#24314;&#35758;&#65292;&#20197;&#33258;&#28982;&#22320;&#24341;&#20986;&#35821;&#29992;&#23398;&#29616;&#35937;&#65292;&#24182;&#24314;&#35758;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#20132;&#27969;&#19978;&#19979;&#25991;&#21644;&#25928;&#30410;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of context -from shared linguistic goals and conventions, to the visual and embodied world -- to use language effectively. We survey existing grounded settings and pragmatic modeling approaches and analyze how the task goals, environmental contexts, and communicative affordances in each work enrich linguistic meaning. We present recommendations for future grounded task design to naturally elicit pragmatic phenomena, and suggest directions that focus on a broader range of communicative contexts and affordances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2207.12261</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#29992;&#20110;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
GraphCFC: A Directed Graph based Cross-modal Feature Complementation Approach for Multimodal Conversational Emotion Recognition. (arXiv:2207.12261v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.12261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#21462;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#20132;&#20114;&#20449;&#24687;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#22312;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25552;&#20379;&#26377;&#20849;&#24773;&#24515;&#29702;&#30340;&#26381;&#21153;&#12290;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#21487;&#20197;&#32531;&#35299;&#21333;&#27169;&#24577;&#26041;&#27861;&#30340;&#32570;&#28857;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20851;&#31995;&#24314;&#27169;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#12290;&#22312;&#22810;&#27169;&#24577;&#23545;&#35805;&#24773;&#24863;&#35782;&#21035;&#20013;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#21462;&#36828;&#36317;&#31163;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#36328;&#27169;&#24577;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30001;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;&#22914;MMGCN&#65289;&#30452;&#25509;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#20887;&#20313;&#20449;&#24687;&#65292;&#19988;&#21487;&#33021;&#20002;&#22833;&#22810;&#26679;&#21270;&#30340;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#21521;&#22270;&#30340;&#36328;&#27169;&#24577;&#29305;&#24449;&#34917;&#20805;&#65288;GraphCFC&#65289;&#27169;&#22359;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#27169;&#25311;&#19978;&#19979;&#25991;&#21644;&#20114;&#21160;&#20449;&#24687;&#12290;GraphCFC&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#23376;&#31354;&#38388;&#25552;&#21462;&#22120;&#21644;&#25104;&#23545;&#36328;&#27169;&#24577;&#34917;&#20805;&#65288;PairCC&#65289;&#31574;&#30053;&#65292;&#32531;&#35299;&#20102;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#30340;&#24322;&#26500;&#24615;&#24046;&#36317;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition in Conversation (ERC) plays a significant part in Human-Computer Interaction (HCI) systems since it can provide empathetic services. Multimodal ERC can mitigate the drawbacks of uni-modal approaches. Recently, Graph Neural Networks (GNNs) have been widely used in a variety of fields due to their superior performance in relation modeling. In multimodal ERC, GNNs are capable of extracting both long-distance contextual information and inter-modal interactive information. Unfortunately, since existing methods such as MMGCN directly fuse multiple modalities, redundant information may be generated and diverse information may be lost. In this work, we present a directed Graph based Cross-modal Feature Complementation (GraphCFC) module that can efficiently model contextual and interactive information. GraphCFC alleviates the problem of heterogeneity gap in multimodal fusion by utilizing multiple subspace extractors and Pair-wise Cross-modal Complementary (PairCC) strategy. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; REPINA&#65292;&#26088;&#22312;&#20943;&#23569;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312; 13 &#20010;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2205.11603</link><description>&lt;p&gt;
&#34920;&#31034;&#25237;&#24433;&#19981;&#21464;&#24615;&#32531;&#35299;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Representation Projection Invariance Mitigates Representation Collapse. (arXiv:2205.11603v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; REPINA&#65292;&#26088;&#22312;&#20943;&#23569;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#65292;&#32467;&#26524;&#22312; 13 &#20010;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#21270;&#34920;&#31034;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20173;&#28982;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#24494;&#35843;&#21487;&#33021;&#20250;&#23548;&#33268;&#34920;&#31034;&#38477;&#32423;&#65288;&#20063;&#34987;&#31216;&#20026;&#34920;&#31034;&#23849;&#28291;&#65289;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12289;&#27425;&#20248;&#24615;&#33021;&#21644;&#24369;&#27867;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#34920;&#31034;&#25237;&#24433;&#19981;&#21464;&#24615;&#8221;&#65288;REPINA&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#34920;&#31034;&#20013;&#30340;&#19981;&#33391;&#21464;&#21270;&#26469;&#32500;&#25252;&#34920;&#31034;&#30340;&#20449;&#24687;&#20869;&#23481;&#24182;&#20943;&#23569;&#34920;&#31034;&#23849;&#28291;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#19982;5&#20010;&#21487;&#27604;&#36739;&#22522;&#32447;&#22312;13&#20010;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#65288;GLUE&#22522;&#20934;&#27979;&#35797;&#21644;&#20854;&#20182;&#20845;&#20010;&#25968;&#25454;&#38598;&#65289;&#20013;&#30340;&#23454;&#35777;&#34892;&#20026;&#12290;&#22312;&#35780;&#20272;&#20869;&#22495;&#24615;&#33021;&#26102;&#65292;REPINA &#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#65288;13&#39033;&#20013;&#30340;10&#39033;&#65289;&#19978;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23427;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#23545;&#26631;&#31614;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#20808;&#21069;&#24037;&#20316;&#30340;&#33539;&#22260;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#36807;&#21253;&#25324;&#39044;&#27979;&#20219;&#21153;&#22312;&#20869;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#38477;&#20302;&#34920;&#31034;&#23849;&#28291;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning contextualized representations learned by pre-trained language models remains a prevalent practice in NLP. However, fine-tuning can lead to representation degradation (also known as representation collapse), which may result in instability, sub-optimal performance, and weak generalization.  In this paper, we propose Representation Projection Invariance (REPINA), a novel regularization method to maintain the information content of representation and reduce representation collapse during fine-tuning by discouraging undesirable changes in the representations. We study the empirical behavior of the proposed regularization in comparison to 5 comparable baselines across 13 language understanding tasks (GLUE benchmark and six additional datasets). When evaluating in-domain performance, REPINA consistently outperforms other baselines on most tasks (10 out of 13). We also demonstrate its effectiveness in few-shot settings and robustness to label perturbation. As a by-product, we ext
&lt;/p&gt;</description></item></channel></rss>