<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>GANTEE &#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#26032;&#30340;&#32508;&#21512;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#27604;&#20854;&#20182;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14480</link><description>&lt;p&gt;
GANTEE&#65306;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GANTEE: Generative Adversatial Network for Taxonomy Entering Evaluation. (arXiv:2303.14480v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14480
&lt;/p&gt;
&lt;p&gt;
GANTEE &#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#26032;&#30340;&#32508;&#21512;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#27604;&#20854;&#20182;&#20808;&#21069;&#26041;&#27861;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#27861;&#34987;&#21046;&#23450;&#20026;&#25903;&#25345;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#21521;&#26080;&#29615;&#27010;&#24565;&#22270;&#25110;&#26641;&#12290;&#35768;&#22810;&#26032;&#27010;&#24565;&#38656;&#35201;&#28155;&#21152;&#21040;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#20998;&#31867;&#27861;&#25193;&#23637;&#20219;&#21153;&#20165;&#26088;&#22312;&#25214;&#21040;&#26032;&#27010;&#24565;&#22312;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#30340;&#26368;&#20339;&#20301;&#32622;&#65292;&#20294;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#23384;&#22312;&#20004;&#20010;&#32570;&#28857;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#25928;&#29575;&#20302;&#19979;&#65292;&#22240;&#20026;&#24403;&#22823;&#22810;&#25968;&#26032;&#27010;&#24565;&#23454;&#38469;&#19978;&#26159;&#22122;&#22768;&#27010;&#24565;&#26102;&#65292;&#23427;&#20204;&#20250;&#28010;&#36153;&#24456;&#22810;&#26102;&#38388;&#12290;&#23427;&#20204;&#20063;&#22240;&#20165;&#20174;&#29616;&#26377;&#20998;&#31867;&#27861;&#20013;&#25910;&#38598;&#35757;&#32451;&#26679;&#26412;&#32780;&#38480;&#21046;&#20102;&#27169;&#22411;&#25366;&#25496;&#30495;&#23454;&#27010;&#24565;&#20043;&#38388;&#26356;&#22810;&#30340;&#19978;&#19979;&#20301;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#25554;&#25300;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#29992;&#20110;&#20998;&#31867;&#27861;&#28155;&#21152;&#35780;&#20272;&#30340;&#29983;&#25104;&#25932;&#23545;&#32593;&#32476;&#65288;GANTEE&#65289;&#65292;&#20197;&#20943;&#36731;&#36825;&#20123;&#32570;&#28857;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#35774;&#35745;&#20102;&#19968;&#31181;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#36776;&#21035;&#27169;&#22411;&#20943;&#36731;&#31532;&#19968;&#20010;&#32570;&#28857;&#65292;&#24182;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#32508;&#21512;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20197;&#32531;&#35299;&#31532;&#20108;&#20010;&#32570;&#28857;&#12290;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;GANTEE&#26694;&#26550;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Taxonomy is formulated as directed acyclic concepts graphs or trees that support many downstream tasks. Many new coming concepts need to be added to an existing taxonomy. The traditional taxonomy expansion task aims only at finding the best position for new coming concepts in the existing taxonomy. However, they have two drawbacks when being applied to the real-scenarios. The previous methods suffer from low-efficiency since they waste much time when most of the new coming concepts are indeed noisy concepts. They also suffer from low-effectiveness since they collect training samples only from the existing taxonomy, which limits the ability of the model to mine more hypernym-hyponym relationships among real concepts. This paper proposes a pluggable framework called Generative Adversarial Network for Taxonomy Entering Evaluation (GANTEE) to alleviate these drawbacks. A generative adversarial network is designed in this framework by discriminative models to alleviate the first drawback an
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#30740;&#31350;&#20171;&#32461;&#20102;&#30740;&#21457;&#20986;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;, &#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#26597;&#25214;&#24182;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#29486;,&#20197;&#21327;&#21161;&#22303;&#33879;&#20154;&#31867;&#23547;&#25214;&#20854;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#36951;&#39608;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14475</link><description>&lt;p&gt;
&#20449;&#24687;&#23398;&#20064;&#12289;&#20013;&#24515;&#24615;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#30456;&#20851;&#25991;&#29486;&#26816;&#27979;&#12289;&#22303;&#33879;&#20154;&#31867;&#36951;&#39608;&#24402;&#36824;
&lt;/p&gt;
&lt;p&gt;
Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains. (arXiv:2303.14475v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#30740;&#31350;&#20171;&#32461;&#20102;&#30740;&#21457;&#20986;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#36827;&#23637;, &#35813;&#31995;&#32479;&#21487;&#20197;&#33258;&#21160;&#26597;&#25214;&#24182;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#29486;,&#20197;&#21327;&#21161;&#22303;&#33879;&#20154;&#31867;&#23547;&#25214;&#20854;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#36951;&#39608;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28595;&#22823;&#21033;&#20122;&#21644;&#20854;&#20182;&#21407;&#20303;&#27665;&#38754;&#20020;&#30340;&#32039;&#36843;&#38382;&#39064;&#20043;&#19968;&#26159;&#23558;&#20182;&#20204;&#31062;&#20808;&#30340;&#23608;&#20307;&#36951;&#39608;&#24402;&#36824;&#21040;&#35199;&#26041;&#31185;&#23398;&#26426;&#26500;&#12290;&#25104;&#21151;&#23558;&#36825;&#20123;&#36951;&#39608;&#36820;&#36824;&#21040;&#20854;&#31038;&#21306;&#20197;&#37325;&#26032;&#23433;&#33900;&#65292;&#20027;&#35201;&#21462;&#20915;&#20110;&#22312;1790&#24180;&#33267;1970&#24180;&#26399;&#38388;&#21457;&#34920;&#30340;&#31185;&#23398;&#21644;&#20854;&#20182;&#25991;&#29486;&#20013;&#25214;&#21040;&#35760;&#24405;&#23427;&#20204;&#34987;&#30423;&#31363;&#12289;&#25424;&#36192;&#12289;&#20986;&#21806;&#25110;&#22312;&#26426;&#26500;&#20043;&#38388;&#20132;&#25442;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25253;&#36947;&#20102;&#30001;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20154;&#21592;&#22312;&#8220;&#30740;&#31350;&#12289;&#21644;&#35299;&#12289;&#26356;&#26032;&#8221;&#32593;&#32476;&#65288;RRR&#65289;&#20013;&#36827;&#34892;&#30340;&#21327;&#20316;&#30740;&#31350;&#65292;&#20197;&#24320;&#21457;&#21644;&#24212;&#29992;&#25991;&#26412;&#25366;&#25496;&#25216;&#26415;&#26469;&#30830;&#23450;&#36825;&#20123;&#20851;&#38190;&#20449;&#24687;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#36804;&#20170;&#20026;&#27490;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;&#20197;&#33258;&#21160;&#21270;&#26597;&#25214;&#21644;&#35821;&#20041;&#20998;&#26512;&#30456;&#20851;&#25991;&#26412;&#30340;&#24037;&#20316;&#12290;&#20998;&#31867;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#22312;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#31934;&#24230;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the pressing issues facing Australian and other First Nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in Western scientific institutions. The success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. This article reports on collaborative research by data scientists and social science researchers in the Research, Reconcile, Renew Network (RRR) to develop and apply text mining techniques to identify this vital information. We describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevant texts. Classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. rele
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#21360;&#24230;&#35821;&#35328;&#25991;&#26412;&#25688;&#35201;&#30340;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#22312;ILSUM shared task&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#31532;&#19968;&#21517;&#65292;&#24191;&#27867;&#20998;&#26512;&#20102;&#26377;&#38480;&#25968;&#25454;&#22823;&#23567;&#19979;&#30340;k&#20493;&#20132;&#21449;&#39564;&#35777;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14461</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#30340;&#21360;&#24230;&#35821;&#35328;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Indian Language Summarization using Pretrained Sequence-to-Sequence Models. (arXiv:2303.14461v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#36827;&#34892;&#21360;&#24230;&#35821;&#35328;&#25991;&#26412;&#25688;&#35201;&#30340;&#30740;&#31350;&#12290;&#35813;&#30740;&#31350;&#22312;ILSUM shared task&#20219;&#21153;&#20013;&#33719;&#24471;&#20102;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#31532;&#19968;&#21517;&#65292;&#24191;&#27867;&#20998;&#26512;&#20102;&#26377;&#38480;&#25968;&#25454;&#22823;&#23567;&#19979;&#30340;k&#20493;&#20132;&#21449;&#39564;&#35777;&#23545;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#23454;&#39564;&#39564;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ILSUM&#20849;&#20139;&#20219;&#21153;&#19987;&#27880;&#20110;&#23545;&#21360;&#22320;&#35821;&#12289;&#21476;&#21513;&#25289;&#29305;&#35821;&#21644;&#33521;&#35821;&#19977;&#31181;&#20027;&#35201;&#35821;&#35328;&#36827;&#34892;&#25991;&#26412;&#25688;&#35201;&#12290;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#39044;&#20808;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#65292;&#20197;&#25214;&#20986;&#27599;&#31181;&#35821;&#35328;&#26368;&#20339;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#21644;&#25105;&#20204;&#26041;&#27861;&#30340;&#35814;&#32454;&#27010;&#36848;&#12290;&#25105;&#20204;&#22312;&#25152;&#26377;&#19977;&#20010;&#23376;&#20219;&#21153;&#65288;&#33521;&#35821;&#65292;&#21360;&#22320;&#35821;&#21644;&#21476;&#21513;&#25289;&#29305;&#35821;&#65289;&#20013;&#33719;&#24471;&#20102;&#31532;&#19968;&#21517;&#12290;&#25105;&#20204;&#36824;&#23545;&#26377;&#38480;&#25968;&#25454;&#22823;&#23567;&#36827;&#34892;&#20102;k&#20493;&#20132;&#21449;&#39564;&#35777;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#24182;&#36827;&#34892;&#20102;&#23545;&#21407;&#22987;&#25968;&#25454;&#21644;&#32463;&#36807;&#31579;&#36873;&#30340;&#25968;&#25454;&#30340;&#32452;&#21512;&#36827;&#34892;&#21508;&#31181;&#23454;&#39564;&#65292;&#20197;&#30830;&#23450;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ILSUM shared task focuses on text summarization for two major Indian languages- Hindi and Gujarati, along with English. In this task, we experiment with various pretrained sequence-to-sequence models to find out the best model for each of the languages. We present a detailed overview of the models and our approaches in this paper. We secure the first rank across all three sub-tasks (English, Hindi and Gujarati). This paper also extensively analyzes the impact of k-fold cross-validation while experimenting with limited data size, and we also perform various experiments with a combination of the original and a filtered version of the data to determine the efficacy of the pretrained models.
&lt;/p&gt;</description></item><item><title>Sem4SAP&#21033;&#29992;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#36827;&#34892;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;&#21516;&#20041;&#35789;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Sem4SAP&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.14425</link><description>&lt;p&gt;
Sem4SAP: &#22522;&#20110;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21516;&#20041;&#34920;&#36798;&#24335;&#25366;&#25496;&#65292;&#20026;&#35821;&#35328;&#27169;&#22411;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Sem4SAP: Synonymous Expression Mining From Open Knowledge Graph For Language Model Synonym-Aware Pretraining. (arXiv:2303.14425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14425
&lt;/p&gt;
&lt;p&gt;
Sem4SAP&#21033;&#29992;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#20013;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#36827;&#34892;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#65292;&#25193;&#23637;&#20102;&#21516;&#20041;&#35789;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#24182;&#25552;&#20986;&#20004;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Sem4SAP&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#32780;&#35328;&#65292;&#27169;&#22411;&#29702;&#35299;&#21516;&#20041;&#34920;&#36798;&#24335;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#23558;&#20351;&#27169;&#22411;&#26356;&#22909;&#22320;&#29702;&#35299;&#19978;&#19979;&#25991;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#26356;&#20855;&#26377;&#25269;&#24481;&#21516;&#20041;&#35789;&#26367;&#25442;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Sem4SAP&#30340;&#26694;&#26550;&#65292;&#20174;&#24320;&#25918;&#30693;&#35782;&#22270;&#35889;&#65288;Open-KG&#65289;&#20013;&#25366;&#25496;&#21516;&#20041;&#35789;&#65292;&#24182;&#21033;&#29992;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#36827;&#34892;&#35821;&#35328;&#27169;&#22411;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#31616;&#35201;&#36807;&#28388;Open-KG&#20013;&#30340;&#20869;&#23481;&#65292;&#24182;&#21033;&#29992;&#39057;&#29575;&#20449;&#24687;&#26469;&#26356;&#22909;&#22320;&#24110;&#21161;&#20302;&#36164;&#28304;&#26080;&#30417;&#30563;&#26465;&#20214;&#19979;&#30340;&#32858;&#31867;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#36801;&#31227;&#21516;&#20041;&#34920;&#36798;&#24335;&#20043;&#38388;&#30340;&#26680;&#24515;&#35821;&#20041;&#26469;&#25193;&#23637;&#25366;&#25496;&#21040;&#30340;&#21516;&#20041;&#35789;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#21516;&#20041;&#35789;&#24863;&#30693;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#21516;&#20041;&#35789;&#30693;&#35782;&#27880;&#20837;PLMs (Pretrained Language Model)&#20013;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;Sem4SAP&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The model's ability to understand synonymous expression is crucial in many kinds of downstream tasks. It will make the model to better understand the similarity between context, and more robust to the synonym substitution attack. However, many Pretrained Language Model (PLM) lack synonym knowledge due to limitation of small-scale synsets and PLM's pretraining objectives. In this paper, we propose a framework called Sem4SAP to mine synsets from Open Knowledge Graph (Open-KG) and using the mined synsets to do synonym-aware pretraining for language models. We propose to coarsly filter the content in Open-KG and use the frequency information to better help the clustering process under low-resource unsupervised conditions. We expand the mined synsets by migrating core semantics between synonymous expressions.We also propose two novel and effective synonym-aware pre-training methods for injecting synonym knowledge into PLMs.Extensive experiments demonstrate that Sem4SAP can dramatically outp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22235;&#31181;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#24211;&#20197;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.14406</link><description>&lt;p&gt;
&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65306;&#29616;&#29366;&#12289;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing in Ethiopian Languages: Current State, Challenges, and Opportunities. (arXiv:2303.14406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#22235;&#31181;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#24211;&#20197;&#20419;&#36827;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22467;&#22622;&#20420;&#27604;&#20122;&#30340;&#22235;&#31181;&#35821;&#35328;&#65288;&#38463;&#22982;&#21704;&#25289;&#35821;&#12289;&#22885;&#32599;&#33707;&#35821;&#12289;&#25552;&#26684;&#21033;&#23612;&#20122;&#35821;&#21644;&#27779;&#25289;&#20234;&#22612;&#35821;&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30446;&#21069;&#30340;&#29366;&#20917;&#12290;&#36890;&#36807;&#26412;&#25991;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22467;&#22622;&#20420;&#27604;&#20122;NLP&#30740;&#31350;&#30340;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;GitHub&#19978;&#30340;&#38598;&#20013;&#23384;&#20648;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#36825;&#20123;&#35821;&#35328;&#30340;&#21508;&#31181;NLP&#20219;&#21153;&#30340;&#20844;&#20849;&#36164;&#28304;&#12290;&#35813;&#23384;&#20648;&#24211;&#21487;&#20197;&#23450;&#26399;&#26356;&#26032;&#65292;&#20174;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#30340;&#36129;&#29486;&#20013;&#24471;&#21040;&#23436;&#21892;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#30740;&#31350;&#31354;&#30333;&#24182;&#23558;&#20449;&#24687;&#20256;&#25773;&#32473;&#23545;&#22467;&#22622;&#20420;&#27604;&#20122;&#35821;&#35328;&#24863;&#20852;&#36259;&#30340;NLP&#30740;&#31350;&#32773;&#65292;&#24182;&#40723;&#21169;&#26410;&#26469;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This survey delves into the current state of natural language processing (NLP) for four Ethiopian languages: Amharic, Afaan Oromo, Tigrinya, and Wolaytta. Through this paper, we identify key challenges and opportunities for NLP research in Ethiopia. Furthermore, we provide a centralized repository on GitHub that contains publicly available resources for various NLP tasks in these languages. This repository can be updated periodically with contributions from other researchers. Our objective is to identify research gaps and disseminate the information to NLP researchers interested in Ethiopian languages and encourage future research in this domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#31934;&#30830;&#30340;&#26694;&#26550;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20248;&#21270;&#20102;&#35821;&#20041;&#34920;&#31034;&#21644;&#35821;&#20041;&#35299;&#26512;&#30340;&#20934;&#30830;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.14375</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#28151;&#21512;&#25552;&#31034;&#35843;&#25972;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Knowledge-augmented Frame Semantic Parsing with Hybrid Prompt-tuning. (arXiv:2303.14375v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#31934;&#30830;&#30340;&#26694;&#26550;&#30693;&#35782;&#19982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#65292;&#20248;&#21270;&#20102;&#35821;&#20041;&#34920;&#31034;&#21644;&#35821;&#20041;&#35299;&#26512;&#30340;&#20934;&#30830;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26694;&#26550;&#35821;&#20041;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35821;&#20041;&#35299;&#26512;&#20219;&#21153;&#20013;&#30340;&#20027;&#27969;&#26041;&#27861;&#65292;&#20294;&#26159;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#28040;&#27495;&#30446;&#26631;&#35789;&#27719;&#25152;&#21796;&#36215;&#30340;&#26694;&#26550;&#34920;&#31034;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#24050;&#34987;&#29992;&#20110;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#19988;&#26174;&#33879;&#25552;&#39640;&#20102;&#31070;&#32463;&#35299;&#26512;&#22120;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;PLMs&#30340;&#26041;&#27861;&#24448;&#24448;&#20559;&#29233;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#30340;&#32852;&#21512;&#27169;&#24335;&#65292;&#23548;&#33268;&#36755;&#20986;&#32467;&#26524;&#19981;&#20934;&#30830;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#26694;&#26550;&#35821;&#20041;&#35299;&#26512;&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;&#31934;&#30830;&#30340;&#26694;&#26550;&#30693;&#35782;&#19982;PLMs&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#35760;&#24518;&#30340;&#30693;&#35782;&#25552;&#21462;&#27169;&#22359;&#65288;MKEM&#65289;&#26469;&#36873;&#25321;&#20934;&#30830;&#30340;&#26694;&#26550;&#30693;&#35782;&#24182;&#22312;&#39640;&#32423;&#35821;&#35328;&#34920;&#31034;&#23618;&#20013;&#26500;&#24314;&#36830;&#32493;&#27169;&#26495;&#12290;&#28982;&#21518;&#65292;&#37319;&#29992;&#28151;&#21512;&#25552;&#31034;&#35843;&#25972;&#26426;&#21046;&#26469;&#24341;&#23548;&#31070;&#32463;&#32593;&#32476;&#23558;&#24102;&#26377;&#35821;&#20041;&#39046;&#22495;&#30693;&#35782;&#30340;&#20869;&#23481;&#38598;&#25104;&#21040;&#36755;&#20986;&#32467;&#26524;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Frame semantics-based approaches have been widely used in semantic parsing tasks and have become mainstream. It remains challenging to disambiguate frame representations evoked by target lexical units under different contexts. Pre-trained Language Models (PLMs) have been used in semantic parsing and significantly improve the accuracy of neural parsers. However, the PLMs-based approaches tend to favor collocated patterns presented in the training data, leading to inaccurate outcomes. The intuition here is to design a mechanism to optimally use knowledge captured in semantic frames in conjunction with PLMs to disambiguate frames. We propose a novel Knowledge-Augmented Frame Semantic Parsing Architecture (KAF-SPA) to enhance semantic representation by incorporating accurate frame knowledge into PLMs during frame semantic parsing. Specifically, a Memory-based Knowledge Extraction Module (MKEM) is devised to select accurate frame knowledge and construct the continuous templates in the high 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;GPT-3&#27169;&#22411;&#22312;&#35821;&#27861;&#32416;&#38169;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#35780;&#20998;&#32773;&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.14342</link><description>&lt;p&gt;
GPT-3&#22312;&#35821;&#27861;&#32416;&#38169;&#19978;&#24615;&#33021;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of GPT-3's Performance in Grammatical Error Correction. (arXiv:2303.14342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;GPT-3&#27169;&#22411;&#22312;&#35821;&#27861;&#32416;&#38169;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#36890;&#36807;&#23454;&#39564;&#27979;&#35797;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#35780;&#20998;&#32773;&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-3&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#23427;&#20204;&#22312;&#35821;&#27861;&#32416;&#38169;(GEC)&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#32570;&#20047;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23545;GPT-3&#27169;&#22411;&#65288;text-davinci-003&#29256;&#26412;&#65289;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27979;&#35797;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#25552;&#31034;&#26041;&#24335;&#65292;&#21253;&#25324;&#38646;&#26679;&#26412;&#23398;&#20064;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#19981;&#21516;&#25552;&#31034;&#26684;&#24335;&#36935;&#21040;&#30340;&#26377;&#36259;&#25110;&#26377;&#38382;&#39064;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#31867;&#35780;&#20215;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#25253;&#21578;&#20102;&#25105;&#20204;&#26368;&#20339;&#25552;&#31034;&#22312;BEA-2019&#21644;JFLEG&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#20154;&#31867;&#35780;&#20998;&#32773;&#19982;&#22522;&#20110;&#21442;&#32771;&#30340;&#33258;&#21160;&#24230;&#37327;&#20043;&#38388;&#30340;&#26377;&#36259;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-3 models are very powerful, achieving high performance on a variety of natural language processing tasks. However, there is a relative lack of detailed published analysis on how well they perform on the task of grammatical error correction (GEC). To address this, we perform experiments testing the capabilities of a GPT-3 model (text-davinci-003) against major GEC benchmarks, comparing the performance of several different prompts, including a comparison of zero-shot and few-shot settings. We analyze intriguing or problematic outputs encountered with different prompt formats. We report the performance of our best prompt on the BEA-2019 and JFLEG datasets using a combination of automatic metrics and human evaluations, revealing interesting differences between the preferences of human raters and the reference-based automatic metrics.
&lt;/p&gt;</description></item><item><title>SmartBook&#26159;&#19968;&#31181;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#28040;&#32791;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#22312;Ukraine-Russia&#21361;&#26426;&#20013;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.14337</link><description>&lt;p&gt;
SmartBook&#65306;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SmartBook: AI-Assisted Situation Report Generation. (arXiv:2303.14337v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14337
&lt;/p&gt;
&lt;p&gt;
SmartBook&#26159;&#19968;&#31181;AI&#36741;&#21161;&#30340;&#24773;&#25253;&#25253;&#21578;&#29983;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#28040;&#32791;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#22312;Ukraine-Russia&#21361;&#26426;&#20013;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#20852;&#20107;&#20214;&#65292;&#22914;COVID&#30123;&#24773;&#21644;&#20044;&#20811;&#20848;&#21361;&#26426;&#65292;&#38656;&#35201;&#26102;&#38388;&#25935;&#24863;&#30340;&#20840;&#38754;&#20102;&#35299;&#24773;&#20917;&#65292;&#20197;&#20415;&#36827;&#34892;&#36866;&#24403;&#30340;&#20915;&#31574;&#21644;&#26377;&#25928;&#30340;&#34892;&#21160;&#21709;&#24212;&#12290;&#33258;&#21160;&#29983;&#25104;&#24773;&#25253;&#25253;&#21578;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#39046;&#22495;&#19987;&#23478;&#20934;&#22791;&#23448;&#26041;&#20154;&#24037;&#31574;&#21010;&#25253;&#21578;&#30340;&#26102;&#38388;&#12289;&#31934;&#21147;&#21644;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;AI&#30740;&#31350;&#22312;&#36825;&#20010;&#30446;&#26631;&#26041;&#38754;&#38750;&#24120;&#26377;&#38480;&#65292;&#36824;&#27809;&#26377;&#25104;&#21151;&#30340;&#35797;&#39564;&#26469;&#33258;&#21160;&#21270;&#36825;&#31181;&#25253;&#21578;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SmartBook&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#20998;&#35299;&#65292;&#26088;&#22312;&#29983;&#25104;&#24773;&#20917;&#25253;&#21578;&#65292;&#22312;&#22823;&#37327;&#26032;&#38395;&#25968;&#25454;&#30340;&#22522;&#30784;&#19978;&#29983;&#25104;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#24773;&#20917;&#25253;&#21578;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;&#20551;&#35774;&#65288;&#20027;&#24352;&#65289;&#65292;&#24182;&#19982;&#20107;&#23454;&#20381;&#25454;&#24314;&#31435;&#20016;&#23500;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#24773;&#25253;&#20998;&#26512;&#25253;&#21578;&#26469;&#23454;&#29616;SmartBook&#65292;&#20197;&#21327;&#21161;&#19987;&#23478;&#20998;&#26512;&#24072;&#22788;&#29702;&#20044;&#20811;&#20848;-&#20420;&#32599;&#26031;&#21361;&#26426;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25253;&#21578;&#20197;&#26102;&#38388;&#36724;&#30340;&#24418;&#24335;&#32467;&#26500;&#21270;&#65292;&#27599;&#20010;&#20107;&#20214;&#37117;&#19982;&#30456;&#20851;&#30340;&#28436;&#21592;&#12289;&#20301;&#32622;&#21644;&#22240;&#26524;&#20851;&#31995;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;SmartBook&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#25253;&#21578;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#27604;&#20154;&#31867;&#21516;&#34892;&#26356;&#20840;&#38754;&#12289;&#20934;&#30830;&#21644;&#19968;&#33268;&#30340;&#25253;&#21578;&#26469;&#25552;&#39640;&#25253;&#21578;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging events, such as the COVID pandemic and the Ukraine Crisis, require a time-sensitive comprehensive understanding of the situation to allow for appropriate decision-making and effective action response. Automated generation of situation reports can significantly reduce the time, effort, and cost for domain experts when preparing their official human-curated reports. However, AI research toward this goal has been very limited, and no successful trials have yet been conducted to automate such report generation. We propose SmartBook, a novel task formulation targeting situation report generation, which consumes large volumes of news data to produce a structured situation report with multiple hypotheses (claims) summarized and grounded with rich links to factual evidence. We realize SmartBook for the Ukraine-Russia crisis by automatically generating intelligence analysis reports to assist expert analysts. The machine-generated reports are structured in the form of timelines, with ea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2303.14334</link><description>&lt;p&gt;
&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65306;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#38405;&#35835;&#30028;&#38754;&#22686;&#24378;&#23398;&#26415;&#25991;&#26723;
&lt;/p&gt;
&lt;p&gt;
The Semantic Reader Project: Augmenting Scholarly Documents through AI-Powered Interactive Reading Interfaces. (arXiv:2303.14334v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#26426;&#20132;&#20114;&#25216;&#26415;&#20026;&#30740;&#31350;&#35770;&#25991;&#25552;&#20379;&#26234;&#33021;&#12289;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#36328;&#26426;&#26500;&#21512;&#20316;&#30340;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#20986;&#29256;&#29289;&#26159;&#23398;&#32773;&#21521;&#20182;&#20154;&#20256;&#36882;&#30693;&#35782;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#35770;&#25991;&#20449;&#24687;&#23494;&#38598;&#65292;&#38543;&#30528;&#31185;&#23398;&#25991;&#29486;&#37327;&#30340;&#22686;&#38271;&#65292;&#38656;&#35201;&#26032;&#25216;&#26415;&#25903;&#25345;&#38405;&#35835;&#36807;&#31243;&#12290;&#19982;&#36890;&#36807;&#20114;&#32852;&#32593;&#25216;&#26415;&#36716;&#21464;&#30340;&#26597;&#25214;&#35770;&#25991;&#36807;&#31243;&#19981;&#21516;&#65292;&#38405;&#35835;&#30740;&#31350;&#35770;&#25991;&#30340;&#20307;&#39564;&#20960;&#21313;&#24180;&#26469;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#12290;&#34429;&#28982;PDF&#26684;&#24335;&#22240;&#20854;&#20415;&#25658;&#24615;&#32780;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23427;&#26377;&#37325;&#22823;&#32570;&#28857;&#65292;&#21253;&#25324;&#65306;&#38745;&#24577;&#20869;&#23481;&#65292;&#20302;&#35270;&#35273;&#35835;&#32773;&#30340;&#21487;&#35775;&#38382;&#24615;&#24046;&#65292;&#20197;&#21450;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#38405;&#35835;&#22256;&#38590;&#12290;&#26412;&#25991;&#25506;&#35752;&#8220;&#26368;&#36817;&#30340;AI&#21644;HCI&#36827;&#23637;&#33021;&#21542;&#20026;&#36951;&#30041;&#30340;PDF&#25552;&#20379;&#26234;&#33021;&#65292;&#20132;&#20114;&#24335;&#21644;&#26080;&#38556;&#30861;&#30340;&#38405;&#35835;&#30028;&#38754;&#65311;&#8221;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#35821;&#20041;&#38405;&#35835;&#22120;&#39033;&#30446;&#65292;&#36825;&#26159;&#22810;&#20010;&#26426;&#26500;&#30340;&#21327;&#20316;&#21162;&#21147;&#65292;&#26088;&#22312;&#25506;&#32034;&#20026;&#30740;&#31350;&#35770;&#25991;&#33258;&#21160;&#21019;&#24314;&#21160;&#24577;&#38405;&#35835;&#30028;&#38754;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scholarly publications are key to the transfer of knowledge from scholars to others. However, research papers are information-dense, and as the volume of the scientific literature grows, the need for new technology to support the reading process grows. In contrast to the process of finding papers, which has been transformed by Internet technology, the experience of reading research papers has changed little in decades. The PDF format for sharing research papers is widely used due to its portability, but it has significant downsides including: static content, poor accessibility for low-vision readers, and difficulty reading on mobile devices. This paper explores the question "Can recent advances in AI and HCI power intelligent, interactive, and accessible reading interfaces -- even for legacy PDFs?" We describe the Semantic Reader Project, a collaborative effort across multiple institutions to explore automatic creation of dynamic reading interfaces for research papers. Through this pro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;GPT-3&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#21253;&#25324;&#24490;&#29615;&#22312;&#20869;&#30340;&#22810;&#31181;&#27969;&#34892;&#31639;&#27861;&#12290;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22242;&#32467;&#21487;&#20197;&#35302;&#21457;&#36845;&#20195;&#30340;&#25191;&#34892;&#21644;&#25551;&#36848;&#12290;IRSA&#21487;&#29992;&#20110;&#25945;&#32946;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2303.14310</link><description>&lt;p&gt;
GPT &#27491;&#25104;&#20026;&#22270;&#28789;&#26426;&#65306;&#36825;&#37324;&#26159;&#19968;&#20123;&#32534;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GPT is becoming a Turing machine: Here are some ways to program it. (arXiv:2303.14310v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;GPT-3&#27169;&#22411;&#21487;&#20197;&#25191;&#34892;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#21253;&#25324;&#24490;&#29615;&#22312;&#20869;&#30340;&#22810;&#31181;&#27969;&#34892;&#31639;&#27861;&#12290;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22242;&#32467;&#21487;&#20197;&#35302;&#21457;&#36845;&#20195;&#30340;&#25191;&#34892;&#21644;&#25551;&#36848;&#12290;IRSA&#21487;&#29992;&#20110;&#25945;&#32946;&#65292;&#20855;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#65292;&#23637;&#31034;&#20102; GPT-3 &#27169;&#22411;&#26063;&#21487;&#20197;&#35302;&#21457;&#36845;&#20195;&#34892;&#20026;&#65292;&#20197;&#25191;&#34892;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#20889;&#20837;&#25110;&#22238;&#24518;&#65289;&#28041;&#21450;&#24490;&#29615;&#30340;&#31243;&#24207;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#31185;&#23398;&#35838;&#31243;&#25110;&#36719;&#20214;&#24320;&#21457;&#32773;&#38754;&#35797;&#20013;&#21457;&#29616;&#30340;&#20960;&#31181;&#27969;&#34892;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#22242;&#32467;&#65288;IRSA&#65289;&#22312;&#19977;&#31181;&#26041;&#24335;&#20043;&#19968;&#65288;&#25110;&#32452;&#21512;&#65289;&#20013;&#35302;&#21457;&#36845;&#20195;&#30340;&#25191;&#34892;&#21644;&#25551;&#36848;&#65306;1&#65289;&#22312;&#19968;&#20010;&#29305;&#23450;&#36755;&#20837;&#30340;&#30446;&#26631;&#31243;&#24207;&#30340;&#25191;&#34892;&#36335;&#24452;&#31034;&#20363;&#20013;&#20351;&#29992;&#24378;&#28872;&#30340;&#37325;&#22797;&#32467;&#26500;&#65292;2&#65289;&#25552;&#31034;&#25191;&#34892;&#36335;&#24452;&#30340;&#29255;&#27573;&#65292;&#20197;&#21450;3&#65289;&#26126;&#30830;&#31105;&#27490;&#65288;&#36339;&#36807;&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#26576;&#20123;&#33258;&#25105;&#27880;&#24847;&#21147;&#12290;&#22312;&#21160;&#24577;&#31243;&#24207;&#25191;&#34892;&#20013;&#65292;IRSA &#24102;&#26469;&#20102;&#27604;&#29992;&#26356;&#24378;&#22823;&#30340; GPT-4 &#26367;&#25442;&#27169;&#22411;&#26356;&#22823;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#12290;IRSA &#22312;&#25945;&#32946;&#20013;&#20855;&#26377;&#24212;&#29992;&#21069;&#26223;&#65292;&#22240;&#20026;&#25552;&#31034;&#21644;&#21709;&#24212;&#31867;&#20284;&#20110;&#25968;&#25454;&#32467;&#26500;&#21644;&#31639;&#27861;&#35838;&#22530;&#30340;&#23398;&#29983;&#20316;&#19994;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20855;&#26377;&#38544;&#21547;&#30340;
&lt;/p&gt;
&lt;p&gt;
We demonstrate that, through appropriate prompting, GPT-3 family of models can be triggered to perform iterative behaviours necessary to execute (rather than just write or recall) programs that involve loops, including several popular algorithms found in computer science curricula or software developer interviews. We trigger execution and description of Iterations by Regimenting Self-Attention (IRSA) in one (or a combination) of three ways: 1) Using strong repetitive structure in an example of an execution path of a target program for one particular input, 2) Prompting with fragments of execution paths, and 3) Explicitly forbidding (skipping) self-attention to parts of the generated text. On a dynamic program execution, IRSA leads to larger accuracy gains than replacing the model with the much more powerful GPT-4. IRSA has promising applications in education, as the prompts and responses resemble student assignments in data structures and algorithms classes. Our findings hold implicati
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#38899;&#23545;&#35805;&#20195;&#29702;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36741;&#21161;&#23621;&#20303;&#26032;&#38395;&#25628;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#32769;&#24180;&#20154;&#21644;&#24930;&#24615;&#30149;&#24739;&#32773;&#26356;&#36731;&#26494;&#21644;&#30452;&#35266;&#22320;&#25214;&#21040;&#25152;&#38656;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.14286</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#38899;&#23545;&#35805;&#20195;&#29702;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36741;&#21161;&#23621;&#20303;&#26032;&#38395;&#25628;&#32034;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Voice-Based Conversational Agents and Knowledge Graphs for Improving News Search in Assisted Living. (arXiv:2303.14286v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#38899;&#23545;&#35805;&#20195;&#29702;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36741;&#21161;&#23621;&#20303;&#26032;&#38395;&#25628;&#32034;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#32769;&#24180;&#20154;&#21644;&#24930;&#24615;&#30149;&#24739;&#32773;&#26356;&#36731;&#26494;&#21644;&#30452;&#35266;&#22320;&#25214;&#21040;&#25152;&#38656;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#38754;&#20020;&#30528;&#20154;&#21475;&#32769;&#40836;&#21270;&#12289;&#20154;&#25163;&#30701;&#32570;&#21644;&#24120;&#35265;&#24930;&#24615;&#30149;&#31561;&#37325;&#22823;&#25361;&#25112;&#65292;&#21521;&#20010;&#20307;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#25252;&#29702;&#24050;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#23545;&#35805;&#20195;&#29702;&#24050;&#34987;&#35777;&#26126;&#26159;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#23427;&#20204;&#21487;&#20197;&#20316;&#20026;&#25968;&#23383;&#20581;&#24247;&#21161;&#25163;&#65292;&#28508;&#22312;&#22320;&#25913;&#21892;&#32769;&#24180;&#20154;&#21644;&#24930;&#24615;&#30149;&#24739;&#32773;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#21253;&#25324;&#26381;&#33647;&#25552;&#37266;&#12289;&#20363;&#34892;&#26816;&#26597;&#25110;&#31038;&#20132;&#38386;&#32842;&#12290;&#27492;&#22806;&#65292;&#23545;&#35805;&#20195;&#29702;&#21487;&#20197;&#28385;&#36275;&#20154;&#20204;&#23545;&#26085;&#24120;&#26032;&#38395;&#25110;&#26412;&#22320;&#27963;&#21160;&#20449;&#24687;&#30340;&#22522;&#26412;&#38656;&#27714;&#65292;&#20351;&#20010;&#20154;&#33021;&#22815;&#20445;&#25345;&#23545;&#21608;&#22260;&#19990;&#30028;&#30340;&#20102;&#35299;&#21644;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#37027;&#20123;&#21487;&#33021;&#25216;&#26415;&#32032;&#20859;&#26377;&#38480;&#25110;&#20581;&#24247;&#30456;&#20851;&#33021;&#21147;&#21463;&#38480;&#30340;&#20154;&#26469;&#35828;&#65292;&#23547;&#25214;&#30456;&#20851;&#26032;&#38395;&#26469;&#28304;&#21644;&#27983;&#35272;&#32593;&#19978;&#22823;&#37327;&#30340;&#26032;&#38395;&#25991;&#31456;&#21487;&#33021;&#20250;&#35753;&#20182;&#20204;&#24863;&#21040;&#19981;&#30693;&#25152;&#25514;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26234;&#33021;&#30340;&#36741;&#21161;&#23621;&#20303;&#26032;&#38395;&#25628;&#32034;&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22522;&#20110;&#35821;&#38899;&#30340;&#23545;&#35805;&#20195;&#29702;&#21644;&#30693;&#35782;&#22270;&#35889;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#26032;&#38395;&#25628;&#32034;&#31995;&#32479;&#20132;&#20114;&#65292;&#20351;&#20182;&#20204;&#26356;&#23481;&#26131;&#21644;&#26356;&#30452;&#35266;&#22320;&#25214;&#21040;&#25152;&#38656;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#36824;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#21644;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#20869;&#23481;&#25512;&#33616;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#26174;&#33879;&#25913;&#21892;&#36741;&#21161;&#23621;&#20303;&#29615;&#22659;&#20013;&#30340;&#32769;&#24180;&#20154;&#21644;&#24930;&#24615;&#30149;&#24739;&#32773;&#30340;&#26032;&#38395;&#25628;&#32034;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the healthcare sector is facing major challenges, such as aging populations, staff shortages, and common chronic diseases, delivering high-quality care to individuals has become very difficult. Conversational agents have shown to be a promising technology to alleviate some of these issues. In the form of digital health assistants, they have the potential to improve the everyday life of the elderly and chronically ill people. This includes, for example, medication reminders, routine checks, or social chit-chat. In addition, conversational agents can satisfy the fundamental need of having access to information about daily news or local events, which enables individuals to stay informed and connected with the world around them. However, finding relevant news sources and navigating the plethora of news articles available online can be overwhelming, particularly for those who may have limited technological literacy or health-related impairments. To address this challenge, we propose an i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24773;&#24863;&#21644;&#31038;&#20250;&#35268;&#33539;&#29305;&#24449;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#28145;&#24230;&#26550;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20149;&#28174;&#21644;&#36947;&#24503;&#29305;&#24449;&#23545;&#20110;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#26159;&#37325;&#35201;&#30340;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19979;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.14279</link><description>&lt;p&gt;
&#21033;&#29992;&#24773;&#24863;&#21644;&#31038;&#20250;&#35268;&#33539;&#29305;&#24449;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;
&lt;/p&gt;
&lt;p&gt;
Depression detection in social media posts using affective and social norm features. (arXiv:2303.14279v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24773;&#24863;&#21644;&#31038;&#20250;&#35268;&#33539;&#29305;&#24449;&#22312;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#30340;&#28145;&#24230;&#26550;&#26500;&#65292;&#32467;&#26524;&#34920;&#26126;&#20149;&#28174;&#21644;&#36947;&#24503;&#29305;&#24449;&#23545;&#20110;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#26159;&#37325;&#35201;&#30340;&#65292;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19979;&#22343;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#26816;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#26550;&#26500;&#30340;&#26041;&#27861;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#26816;&#27979;&#25233;&#37057;&#30151;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;BERT&#20174;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#25552;&#21462;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;GRU&#32593;&#32476;&#23558;&#36825;&#20123;&#34920;&#31034;&#32452;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#24773;&#24863;&#20998;&#31867;&#22120;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#22686;&#21152;&#25991;&#26412;&#34920;&#31034;&#30340;&#24773;&#24863;&#20449;&#24687;&#12290;&#25105;&#20204;&#21442;&#32771;&#24515;&#29702;&#23398;&#25991;&#29486;&#30340;&#24314;&#35758;&#65292;&#25552;&#20986;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#26041;&#26696;&#23558;&#24086;&#23376;&#21644;&#21333;&#35789;&#30340;&#20149;&#28174;&#21644;&#36947;&#24503;&#29305;&#24449;&#32435;&#20837;&#21040;&#25105;&#20204;&#30340;&#26550;&#26500;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20149;&#28174;&#21644;&#36947;&#24503;&#29305;&#24449;&#23545;&#20110;&#25233;&#37057;&#30151;&#30340;&#26816;&#27979;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#23558;&#35813;&#27169;&#22411;&#24212;&#29992;&#20110;Reddit Pirina&#25968;&#25454;&#38598;&#19978;&#30340;&#25233;&#37057;&#30151;&#26816;&#27979;&#65292;&#24182;&#36827;&#19968;&#27493;&#32771;&#34385;&#22312;Reddit RSDD&#25968;&#25454;&#38598;&#20013;&#26816;&#27979;&#21457;&#24086;&#29992;&#25143;&#30340;&#25233;&#37057;&#30151;&#29366;&#24577;&#65292;&#35813;&#25968;&#25454;&#38598;&#32771;&#34385;&#20102;&#27599;&#20010;&#29992;&#25143;&#30340;&#22810;&#20010;&#24086;&#23376;&#12290;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#29305;&#24449;&#30340;&#32467;&#26524;&#22312;&#20004;&#20010;&#35774;&#32622;&#19979;&#22343;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#25104;&#26524;&#65292;&#21363;2.65%&#21644;6.73%&#30340;&#32477;&#23545;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a deep architecture for depression detection from social media posts. The proposed architecture builds upon BERT to extract language representations from social media posts and combines these representations using an attentive bidirectional GRU network. We incorporate affective information, by augmenting the text representations with features extracted from a pretrained emotion classifier. Motivated by psychological literature we propose to incorporate profanity and morality features of posts and words in our architecture using a late fusion scheme. Our analysis indicates that morality and profanity can be important features for depression detection. We apply our model for depression detection on Reddit posts on the Pirina dataset, and further consider the setting of detecting depressed users, given multiple posts per user, proposed in the Reddit RSDD dataset. The inclusion of the proposed features yields state-of-the-art results in both settings, namely 2.65% and 6.73% abso
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SIGMORPHON 2023&#20132;&#20114;&#31034;&#33539;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#23558;&#20869;&#25554;&#21457;&#29983;&#35270;&#20026;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;IGT&#30340;&#33258;&#21160;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.14234</link><description>&lt;p&gt;
SIGMORPHON 2023&#20132;&#20114;&#31034;&#33539;&#30340;&#22522;&#32447;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SIGMORPHON 2023 Shared Task of Interlinear Glossing: Baseline Model. (arXiv:2303.14234v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SIGMORPHON 2023&#20132;&#20114;&#31034;&#33539;&#30340;&#22522;&#32447;&#31995;&#32479;&#65292;&#21033;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#23558;&#20869;&#25554;&#21457;&#29983;&#35270;&#20026;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;IGT&#30340;&#33258;&#21160;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#25991;&#29486;&#26159;&#35821;&#35328;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#36890;&#24120;&#21253;&#25324;&#21019;&#24314;&#20132;&#20114;&#24335;&#26631;&#27880;&#25991;&#26412;&#65288;IGT&#65289;&#12290;&#21019;&#24314;IGT&#36153;&#26102;&#19988;&#32321;&#29712;&#65292;&#33258;&#21160;&#21270;&#22788;&#29702;&#21487;&#20197;&#33410;&#30465;&#23453;&#36149;&#30340;&#20154;&#21147;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SIGMORPHON 2023&#20132;&#20114;&#31034;&#33539;&#30340;&#22522;&#32447;&#31995;&#32479;&#12290;&#22312;&#25105;&#20204;&#30340;&#31995;&#32479;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#24182;&#23558;&#20869;&#25554;&#21457;&#29983;&#35270;&#20026;&#24207;&#21015;&#26631;&#35760;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language documentation is a critical aspect of language preservation, often including the creation of Interlinear Glossed Text (IGT). Creating IGT is time-consuming and tedious, and automating the process can save valuable annotator effort.  This paper describes the baseline system for the SIGMORPHON 2023 Shared Task of Interlinear Glossing. In our system, we utilize a transformer architecture and treat gloss generation as a sequence labelling task.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#24179;&#27665;&#26131;&#25026;&#25688;&#35201;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#26368;&#26377;&#25928;&#65292;&#20026;&#32531;&#35299;&#30740;&#31350;&#20154;&#21592;&#36127;&#25285;&#21644;&#20419;&#36827;&#31038;&#20250;&#19982;&#31185;&#23398;&#20132;&#27969;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2303.14222</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#38750;&#19987;&#19994;&#20154;&#22763;&#25991;&#26412;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#31687;&#21465;&#36848;&#24615;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review. (arXiv:2303.14222v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#24635;&#32467;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#24179;&#27665;&#26131;&#25026;&#25688;&#35201;&#30340;&#19981;&#21516;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#26368;&#26377;&#25928;&#65292;&#20026;&#32531;&#35299;&#30740;&#31350;&#20154;&#21592;&#36127;&#25285;&#21644;&#20419;&#36827;&#31038;&#20250;&#19982;&#31185;&#23398;&#20132;&#27969;&#25552;&#20379;&#20102;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24179;&#27665;&#26131;&#25026;&#30340;&#35821;&#35328;&#24635;&#32467;&#30740;&#31350;&#32467;&#26524;&#23545;&#20110;&#20419;&#36827;&#20844;&#20247;&#20102;&#35299;&#30740;&#31350;&#25104;&#26524;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#29983;&#25104;&#31616;&#21270;&#29256;&#25688;&#35201;&#26377;&#26395;&#32531;&#35299;&#30740;&#31350;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#24357;&#21512;&#31185;&#23398;&#19982;&#31038;&#20250;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36825;&#31687;&#21465;&#36848;&#24615;&#25991;&#29486;&#32508;&#36848;&#30340;&#30446;&#30340;&#26159;&#25551;&#36848;&#21644;&#27604;&#36739;&#19981;&#21516;&#30340;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#65292;&#20197;&#29983;&#25104;&#24179;&#27665;&#26131;&#25026;&#30340;&#25688;&#35201;&#12290;&#25105;&#20204;&#22312;Web of Science&#65292;Google Scholar&#65292;IEEE Xplore&#65292;Association for Computing Machinery Digital Library &#21644; arXiv &#31561;&#25968;&#25454;&#24211;&#20013;&#25628;&#32034;&#20102;2022&#24180;5&#26376;6&#26085;&#20197;&#21069;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#25105;&#20204;&#21253;&#25324;&#20102;&#20851;&#20110;&#29992;&#20110;&#29983;&#25104;&#24179;&#27665;&#26131;&#25026;&#30340;&#25688;&#35201;&#30340;&#33258;&#21160;&#25991;&#26412;&#25688;&#35201;&#26041;&#27861;&#30340;&#21407;&#22987;&#30740;&#31350;&#12290;&#25105;&#20204;&#31579;&#36873;&#20102;82&#31687;&#25991;&#31456;&#65292;&#24182;&#21253;&#25324;&#20102;&#22312;2020&#24180;&#33267;2021&#24180;&#26399;&#38388;&#21457;&#24067;&#30340;8&#31687;&#30456;&#20851;&#35770;&#25991;&#65292;&#20840;&#37096;&#20351;&#29992;&#30456;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#26041;&#27861;&#65288;&#22914;&#26368;&#36817;&#27969;&#34892;&#30340;Bidirectional Encoder Representations from Transformers&#65288;BERT&#65289;&#21644;&#20351;&#29992;&#25277;&#21462;&#38388;&#38548;&#21477;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#65289;&#26159;&#26368;&#26377;&#25928;&#30340;&#29983;&#25104;&#24179;&#27665;&#26131;&#25026;&#30340;&#25688;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarisation of research results in plain language is crucial for promoting public understanding of research findings. The use of Natural Language Processing to generate lay summaries has the potential to relieve researchers' workload and bridge the gap between science and society. The aim of this narrative literature review is to describe and compare the different text summarisation approaches used to generate lay summaries. We searched the databases Web of Science, Google Scholar, IEEE Xplore, Association for Computing Machinery Digital Library and arXiv for articles published until 6 May 2022. We included original studies on automatic text summarisation methods to generate lay summaries. We screened 82 articles and included eight relevant papers published between 2020 and 2021, all using the same dataset. The results show that transformer-based methods such as Bidirectional Encoder Representations from Transformers (BERT) and Pre-training with Extracted Gap-sentences for Abstractiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20250;&#35758;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;(MUG)&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20026;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#25552;&#20379;&#25903;&#25345;</title><link>http://arxiv.org/abs/2303.13939</link><description>&lt;p&gt;
MUG: &#19968;&#39033;&#36890;&#29992;&#30340;&#20250;&#35758;&#29702;&#35299;&#19982;&#29983;&#25104;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MUG: A General Meeting Understanding and Generation Benchmark. (arXiv:2303.13939v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20250;&#35758;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;(MUG)&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#20026;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#25552;&#20379;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#21462;&#35270;&#39057;&#20250;&#35758;&#21644;&#22312;&#32447;&#35838;&#31243;&#30340;&#38271;&#26102;&#38388;&#38899;&#39057;&#35760;&#24405;&#20197;&#33719;&#21462;&#20449;&#24687;&#26497;&#20026;&#20302;&#25928;&#12290;&#21363;&#20351;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#23558;&#35760;&#24405;&#36716;&#24405;&#20026;&#38271;&#24418;&#24335;&#30340;&#21475;&#35821;&#35821;&#35328;&#25991;&#26723;&#65292;&#20165;&#38405;&#35835;&#35821;&#38899;&#35782;&#21035;&#36716;&#24405;&#20063;&#21482;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21152;&#24555;&#23547;&#25214;&#20449;&#24687;&#30340;&#36895;&#24230;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#20027;&#39064;&#20998;&#21106;&#21644;&#25688;&#35201;&#31561;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26174;&#33879;&#25552;&#39640;&#29992;&#25143;&#33719;&#24471;&#37325;&#35201;&#20449;&#24687;&#30340;&#25928;&#29575;&#12290;&#20250;&#35758;&#22330;&#26223;&#26159;&#24212;&#29992;&#36825;&#20123;&#21475;&#35821;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26368;&#26377;&#20215;&#20540;&#30340;&#22330;&#26223;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#22823;&#35268;&#27169;&#20844;&#24320;&#30340;&#20250;&#35758;&#25968;&#25454;&#38598;&#23545;&#36825;&#20123;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#36827;&#23637;&#20135;&#29983;&#20102;&#20005;&#37325;&#38459;&#30861;&#12290;&#20026;&#20102;&#20419;&#36827;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36890;&#29992;&#20250;&#35758;&#29702;&#35299;&#21644;&#29983;&#25104;&#22522;&#20934;(MUG)&#65292;&#20197;&#35780;&#20272;&#19968;&#31995;&#21015;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20027;&#39064;&#20998;&#21106;&#12289;&#35805;&#39064;&#32423;&#21035;&#21644;&#20250;&#35805;&#32423;&#21035;&#30340;&#25688;&#35201;&#25552;&#21462;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Listening to long video/audio recordings from video conferencing and online courses for acquiring information is extremely inefficient. Even after ASR systems transcribe recordings into long-form spoken language documents, reading ASR transcripts only partly speeds up seeking information. It has been observed that a range of NLP applications, such as keyphrase extraction, topic segmentation, and summarization, significantly improve users' efficiency in grasping important information. The meeting scenario is among the most valuable scenarios for deploying these spoken language processing (SLP) capabilities. However, the lack of large-scale public meeting datasets annotated for these SLP tasks severely hinders their advancement. To prompt SLP advancement, we establish a large-scale general Meeting Understanding and Generation Benchmark (MUG) to benchmark the performance of a wide range of SLP tasks, including topic segmentation, topic-level and session-level extractive summarization and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13217</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#27491;&#24341;&#23548;&#23569;&#26679;&#26412;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Fairness-guided Few-shot Prompting for Large Language Models. (arXiv:2303.13217v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;-FairPrompt&#65292;&#22312;&#20445;&#35777;&#20844;&#27491;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#36890;&#36807;&#35780;&#20272;&#25552;&#31034;&#39044;&#27979;&#20559;&#24046;&#65292;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36890;&#36807;&#20960;&#20010;&#36755;&#20837;&#36755;&#20986;&#31034;&#20363;&#26500;&#24314;&#30340;&#25552;&#31034;&#36827;&#34892;&#30452;&#25509;&#24212;&#29992;&#26469;&#35299;&#20915;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#12290;&#20294;&#26159;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#35757;&#32451;&#31034;&#20363;&#65292;&#31034;&#20363;&#39034;&#24207;&#21644;&#25552;&#31034;&#26684;&#24335;&#30340;&#21464;&#21270;&#23548;&#33268;&#19978;&#19979;&#25991;&#23398;&#20064;&#23481;&#26131;&#20986;&#29616;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#26500;&#24314;&#36866;&#24403;&#30340;&#25552;&#31034;&#23545;&#20110;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20174;&#39044;&#27979;&#20559;&#24046;&#30340;&#35282;&#24230;&#37325;&#26032;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#26469;&#35780;&#20272;&#22266;&#23450;&#25552;&#31034;&#30456;&#23545;&#20110;&#26631;&#31614;&#25110;&#32473;&#23450;&#23646;&#24615;&#30340;&#39044;&#27979;&#20559;&#24046;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#39044;&#27979;&#20559;&#24046;&#36739;&#22823;&#30340;&#25552;&#31034;&#24635;&#26159;&#23548;&#33268;&#19981;&#20196;&#20154;&#28385;&#24847;&#30340;&#39044;&#27979;&#36136;&#37327;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25628;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#36138;&#23146;&#25628;&#32034;&#26469;&#30830;&#23450;&#36817;&#20284;&#26368;&#20248;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21483;&#20570;"&#20844;&#27491;&#25552;&#31034;"&#65292;&#20854;&#20013;&#34701;&#20837;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#20197;&#25351;&#23548;&#25628;&#32034;&#19981;&#23637;&#29616;&#20986;&#23545;&#26576;&#20123;&#20154;&#32676;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#22810;&#31181;&#23569;&#26679;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#35777;&#26126;&#20102;FairPrompt&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#27491;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have demonstrated surprising ability to perform in-context learning, i.e., these models can be directly applied to solve numerous downstream tasks by conditioning on a prompt constructed by a few input-output examples. However, prior research has shown that in-context learning can suffer from high instability due to variations in training examples, example order, and prompt formats. Therefore, the construction of an appropriate prompt is essential for improving the performance of in-context learning. In this paper, we revisit this problem from the view of predictive bias. Specifically, we introduce a metric to evaluate the predictive bias of a fixed prompt against labels or a given attributes. Then we empirically show that prompts with higher bias always lead to unsatisfactory predictive quality. Based on this observation, we propose a novel search strategy based on the greedy search to identify the near-optimal prompt for improving the performance of in-context l
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.</title><link>http://arxiv.org/abs/2303.13035</link><description>&lt;p&gt;
SPeC&#65306;&#36719;&#25552;&#31034;&#26657;&#20934;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20013;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SPeC: A Soft Prompt-Based Calibration on Mitigating Performance Variability in Clinical Notes Summarization. (arXiv:2303.13035v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13035
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36719;&#25552;&#31034;&#23884;&#20837;&#65292;&#25552;&#20986;Soft Prompt-Based Calibration (SPeC)&#31649;&#36947;&#65292;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#65292;&#38477;&#20302;&#24615;&#33021;&#21464;&#24322;. &#27492;&#26041;&#27861;&#19981;&#20165;&#27604;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24615;&#33021;&#31283;&#23450;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#23384;&#20648;&#30528;&#21253;&#25324;&#30149;&#21382;&#12289;&#35786;&#26029;&#12289;&#27835;&#30103;&#21644;&#26816;&#27979;&#32467;&#26524;&#22312;&#20869;&#30340;&#22823;&#37327;&#24739;&#32773;&#20449;&#24687;&#12290;&#36825;&#20123;&#35760;&#24405;&#23545;&#20110;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#20570;&#20986;&#26126;&#26234;&#30340;&#24739;&#32773;&#25252;&#29702;&#20915;&#31574;&#38750;&#24120;&#20851;&#38190;&#12290;&#25688;&#35201;&#20020;&#24202;&#31508;&#35760;&#21487;&#20197;&#24110;&#21161;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#26356;&#22909;&#22320;&#21457;&#29616;&#28508;&#22312;&#20581;&#24247;&#39118;&#38505;&#65292;&#20197;&#21450;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;&#36825;&#19968;&#36807;&#31243;&#36890;&#36807;&#30830;&#20445;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#21487;&#20197;&#35775;&#38382;&#26368;&#30456;&#20851;&#21644;&#26368;&#26032;&#30340;&#24739;&#32773;&#25968;&#25454;&#65292;&#26377;&#21161;&#20110;&#20943;&#23569;&#38169;&#35823;&#24182;&#25552;&#39640;&#24739;&#32773;&#30340;&#25252;&#29702;&#25928;&#26524;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23558;&#25552;&#31034;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#20063;&#20250;&#23548;&#33268;&#36755;&#20986;&#26041;&#24046;&#22686;&#21152;&#65292;&#21363;&#20351;&#25552;&#31034;&#24847;&#20041;&#30456;&#20284;&#65292;&#36755;&#20986;&#20063;&#20250;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#36719;&#25552;&#31034;&#26657;&#20934;&#65288;SPeC&#65289;&#27969;&#31243;&#65292;&#35813;&#27969;&#31243;&#37319;&#29992;&#36719;&#25552;&#31034;&#23884;&#20837;&#26469;&#20943;&#36731;&#36755;&#20837;&#21464;&#37327;&#23545;&#36755;&#20986;&#22810;&#26679;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SPeC&#19981;&#20165;&#21487;&#20197;&#38477;&#20302;LLM&#30340;&#24615;&#33021;&#21464;&#24322;&#65292;&#32780;&#19988;&#22312;&#20020;&#24202;&#31508;&#35760;&#25688;&#35201;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) store an extensive array of patient information, encompassing medical histories, diagnoses, treatments, and test outcomes. These records are crucial for enabling healthcare providers to make well-informed decisions regarding patient care. Summarizing clinical notes further assists healthcare professionals in pinpointing potential health risks and making better-informed decisions. This process contributes to reducing errors and enhancing patient outcomes by ensuring providers have access to the most pertinent and current patient data. Recent research has shown that incorporating prompts with large language models (LLMs) substantially boosts the efficacy of summarization tasks. However, we show that this approach also leads to increased output variance, resulting in notably divergent outputs even when prompts share similar meanings. To tackle this challenge, we introduce a model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft prom
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19981;&#21516;&#39046;&#22495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#23545;&#20110;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2303.12810</link><description>&lt;p&gt;
LLM&#26159;&#19975;&#33021;&#30340;&#22823;&#24072;&#21527;&#65311;&#25506;&#32034;LLM&#30340;&#39046;&#22495;&#19981;&#21487;&#30693;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are LLMs the Master of All Trades? : Exploring Domain-Agnostic Reasoning Skills of LLMs. (arXiv:2303.12810v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#19981;&#21516;&#39046;&#22495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#21457;&#29616;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#36739;&#24046;&#12290;&#36825;&#23545;&#20110;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#31867;&#20284;&#20110;&#20154;&#31867;&#25512;&#29702;&#30340;&#28508;&#21147;&#19968;&#30452;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20105;&#35758;&#26368;&#28608;&#28872;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#22810;&#26041;&#38754;&#30340;&#65292;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#24418;&#24335;&#36827;&#34892;&#20307;&#29616;&#65292;&#21253;&#25324;&#31867;&#27604;&#12289;&#31354;&#38388;&#21644;&#36947;&#24503;&#25512;&#29702;&#31561;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;LLM&#33021;&#21542;&#22312;&#25152;&#26377;&#36825;&#20123;&#19981;&#21516;&#39046;&#22495;&#20013;&#21516;&#26679;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#30452;&#25509;&#20351;&#29992;&#25110;&#20174;&#29616;&#26377;&#31867;&#27604;&#21644;&#31354;&#38388;&#25512;&#29702;&#25968;&#25454;&#38598;&#20013;&#27762;&#21462;&#21551;&#31034;&#65292;&#23545;LLM&#22312;&#19981;&#21516;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#36827;&#34892;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;LLM&#20687;&#20154;&#31867;&#19968;&#26679;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#36824;&#23545;&#26356;&#24320;&#25918;&#12289;&#33258;&#28982;&#30340;&#35821;&#35328;&#38382;&#39064;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;LLM&#22312;&#31867;&#27604;&#21644;&#36947;&#24503;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#31354;&#38388;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#24471;&#19981;&#22815;&#29087;&#32451;&#12290;&#25105;&#35748;&#20026;&#36825;&#20123;&#23454;&#39564;&#23545;&#20110;&#25512;&#21160;LLM&#26410;&#26469;&#30340;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#25913;&#36827;&#31354;&#38388;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The potential of large language models (LLMs) to reason like humans has been a highly contested topic in Machine Learning communities. However, the reasoning abilities of humans are multifaceted and can be seen in various forms, including analogical, spatial and moral reasoning, among others. This fact raises the question whether LLMs can perform equally well across all these different domains. This research work aims to investigate the performance of LLMs on different reasoning tasks by conducting experiments that directly use or draw inspirations from existing datasets on analogical and spatial reasoning. Additionally, to evaluate the ability of LLMs to reason like human, their performance is evaluted on more open-ended, natural language questions. My findings indicate that LLMs excel at analogical and moral reasoning, yet struggle to perform as proficiently on spatial reasoning tasks. I believe these experiments are crucial for informing the future development of LLMs, particularly 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.11525</link><description>&lt;p&gt;
SIFT: &#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#20197;&#26368;&#22823;&#38480;&#24230;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
SIFT: Sparse Iso-FLOP Transformations for Maximizing Training Efficiency. (arXiv:2303.11525v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SIFT&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#25928;&#29575;&#12289;&#20934;&#30830;&#24615;&#21644;&#34920;&#31034;&#33021;&#21147;&#65292;&#36890;&#36807;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#65292;&#32553;&#30701;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#26435;&#37325;&#31232;&#30095;&#24615;&#26469;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#35757;&#32451;&#25928;&#29575;&#65288;&#19982;&#35757;&#32451;FLOPS&#30456;&#20851;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#65289;&#12290; &#36825;&#20123;&#24037;&#20316;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;FLOP&#65292;&#20294;&#20351;&#29992;&#31232;&#30095;&#26435;&#37325;&#36827;&#34892;&#35757;&#32451;&#36890;&#24120;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#25439;&#22833;&#25110;&#38656;&#35201;&#26356;&#38271;&#30340;&#35757;&#32451;&#21608;&#26399;&#65292;&#20351;&#24471;&#32467;&#26524;&#30340;&#35757;&#32451;&#25928;&#29575;&#19981;&#22815;&#28165;&#26224;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#20351;&#29992;&#31232;&#30095;&#24615;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#19982;&#23494;&#38598;&#27169;&#22411;&#30456;&#21516;&#30340;FLOPS&#65292;&#24182;&#36890;&#36807;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23637;&#31034;&#35757;&#32451;&#25928;&#29575;&#25552;&#39640;&#12290; &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SIFT&#65292;&#19968;&#32452;&#29992;&#20316;&#23494;&#38598;&#23618;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#26469;&#25552;&#39640;&#20854;&#34920;&#31034;&#33021;&#21147;&#21644;FLOP&#25928;&#29575;&#30340;&#31232;&#30095;&#31561;FLOP&#36716;&#25442;&#12290; &#27599;&#20010;&#36716;&#25442;&#37117;&#30001;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#65288;&#31232;&#30095;&#32423;&#21035;&#65289;&#21442;&#25968;&#21270;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#31232;&#30095;&#25513;&#33180;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of weight sparsity to improve the training efficiency (test accuracy w.r.t training FLOPs) of deep neural networks (DNNs). These works aim to reduce training FLOPs but training with sparse weights often leads to accuracy loss or requires longer train schedules, making the resulting training efficiency less clear. In contrast, we focus on using sparsity to increase accuracy while using the same FLOPS as the dense model and show training efficiency gains through higher accuracy. In this work, we introduce SIFT, a family of Sparse Iso-FLOP Transformations which are used as drop-in replacements for dense layers to improve their representational capacity and FLOP efficiency. Each transformation is parameterized by a single parameter (sparsity level) and provides a larger search space to find optimal sparse masks. Without changing any training hyperparameters, replacing dense layers with SIFT leads to significant improvements across computer vision (CV) and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#26679;&#26412;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#12289;&#37492;&#21035;&#27169;&#22411;&#25913;&#36827;&#39281;&#21644;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;</title><link>http://arxiv.org/abs/2303.09075</link><description>&lt;p&gt;
&#33258;&#19968;&#33268;&#23398;&#20064;&#65306;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#30340;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Self-Consistent Learning: Cooperation between Generators and Discriminators. (arXiv:2303.09075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#21512;&#20316;&#35757;&#32451;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;GAN&#35757;&#32451;&#19981;&#31283;&#23450;&#12289;&#26679;&#26412;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#12289;&#37492;&#21035;&#27169;&#22411;&#25913;&#36827;&#39281;&#21644;&#31561;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#20063;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#29983;&#25104;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#19979;&#28216;&#37492;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#24050;&#32463;&#22240;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24040;&#22823;&#21457;&#23637;&#32780;&#24191;&#21463;&#27426;&#36814;&#12290;&#22312;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#29983;&#25104;&#27169;&#22411;&#21644;&#37492;&#21035;&#27169;&#22411;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#23427;&#20204;&#19981;&#33021;&#36866;&#24212;&#24444;&#27492;&#30340;&#20219;&#20309;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#24456;&#23481;&#26131;&#20559;&#31163;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#37492;&#21035;&#27169;&#22411;&#30340;&#25913;&#36827;&#24456;&#24555;&#23601;&#20250;&#36798;&#21040;&#39281;&#21644;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#36890;&#36807;&#19968;&#31181;&#23545;&#25239;&#24615;&#36807;&#31243;&#19982;&#37492;&#21035;&#27169;&#22411;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#20197;&#23454;&#29616;&#32852;&#21512;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;GAN&#30340;&#35757;&#32451;&#26497;&#19981;&#31283;&#23450;&#65292;&#24448;&#24448;&#38590;&#20197;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#37492;&#21035;&#22120;&#21644;&#19968;&#20010;&#29983;&#25104;&#22120;&#20197;&#38381;&#29615;&#24418;&#24335;&#21512;&#20316;&#35757;&#32451;&#12290;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#22312;&#22810;&#36718;&#26356;&#26032;&#20013;&#30456;&#20114;&#22686;&#24378;&#65292;&#29983;&#25104;&#30340;&#26679;&#26412;&#36880;&#28176;&#25509;&#36817;&#23454;&#38469;&#25968;&#25454;&#20998;&#24067;&#65292;&#32780;&#37492;&#21035;&#27169;&#22411;&#19981;&#26029;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;GAN&#65292;&#32780;&#19988;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Using generated data to improve the performance of downstream discriminative models has recently gained popularity due to the great development of pre-trained language models. In most previous studies, generative models and discriminative models are trained separately and thus could not adapt to any changes in each other. As a result, the generated samples can easily deviate from the real data distribution, while the improvement of the discriminative model quickly reaches saturation. Generative adversarial networks (GANs) train generative models via an adversarial process with discriminative models to achieve joint training. However, the training of standard GANs is notoriously unstable and often falls short of convergence. In this paper, to address these issues, we propose a $\textit{self-consistent learning}$ framework, in which a discriminator and a generator are cooperatively trained in a closed-loop form. The discriminator and the generator enhance each other during multiple round
&lt;/p&gt;</description></item><item><title>GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.08774</link><description>&lt;p&gt;
GPT-4&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Technical Report. (arXiv:2303.08774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08774
&lt;/p&gt;
&lt;p&gt;
GPT-4&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#25509;&#25910;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#12290;&#35813;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25253;&#21578;&#20102;GPT-4&#30340;&#24320;&#21457;&#65292;&#23427;&#26159;&#19968;&#20010;&#21487;&#20197;&#25509;&#21463;&#22270;&#20687;&#21644;&#25991;&#26412;&#36755;&#20837;&#24182;&#20135;&#29983;&#25991;&#26412;&#36755;&#20986;&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#19981;&#22914;&#20154;&#31867;&#65292;&#20294;GPT-4&#22312;&#21508;&#31181;&#19987;&#19994;&#21644;&#23398;&#26415;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#36890;&#36807;&#27169;&#25311;&#30340;&#24459;&#24072;&#32771;&#35797;&#65292;&#25104;&#32489;&#25490;&#21517;&#22312;&#21069;10&#65285;&#24038;&#21491;&#12290;GPT-4&#26159;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#39044;&#35757;&#32451;&#29992;&#20110;&#39044;&#27979;&#25991;&#26723;&#20013;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#21518;&#35757;&#32451;&#23545;&#40784;&#36807;&#31243;&#25552;&#39640;&#20102;&#20107;&#23454;&#24615;&#21644;&#31526;&#21512;&#26399;&#26395;&#34892;&#20026;&#30340;&#24615;&#33021;&#25351;&#26631;&#12290;&#39033;&#30446;&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;&#24320;&#21457;&#22522;&#30784;&#35774;&#26045;&#21644;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#24191;&#27867;&#30340;&#35268;&#27169;&#33539;&#22260;&#20869;&#34920;&#29616;&#39044;&#27979;&#24615;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;GPT-4&#30340;&#26576;&#20123;&#24615;&#33021;&#26041;&#38754;&#65292;&#32780;&#36825;&#20123;&#24615;&#33021;&#26159;&#22522;&#20110;&#20351;&#29992;&#19981;&#36229;&#36807;GPT-4&#35745;&#31639;&#33021;&#21147;&#30340;1/1,000&#30340;&#27169;&#22411;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.
&lt;/p&gt;</description></item><item><title>NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;</title><link>http://arxiv.org/abs/2303.08233</link><description>&lt;p&gt;
NL4Opt &#27604;&#36187;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26500;&#24314;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
NL4Opt Competition: Formulating Optimization Problems Based on Their Natural Language Descriptions. (arXiv:2303.08233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08233
&lt;/p&gt;
&lt;p&gt;
NL4Opt&#27604;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20986;&#20248;&#21270;&#38382;&#39064;&#30340;&#21547;&#20041;&#21644;&#34920;&#36848;&#65292;&#24182;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#38750;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20132;&#20114;&#12290;&#31454;&#36187;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1) &#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20248;&#21270;&#65288;NL4Opt&#65289;&#31454;&#36187;&#26088;&#22312;&#30740;&#31350;&#22914;&#20309;&#26681;&#25454;&#20248;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#25551;&#36848;&#25552;&#21462;&#20854;&#21547;&#20041;&#21644;&#34920;&#36848;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31454;&#36187;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20013;&#20171;&#26469;&#20351;&#38750;&#19987;&#19994;&#20154;&#22763;&#33021;&#22815;&#25509;&#21475;&#20351;&#29992;&#20248;&#21270;&#27714;&#35299;&#22120;&#65292;&#20197;&#22686;&#21152;&#20854;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#25361;&#25112;&#24615;&#30446;&#26631;&#20998;&#20026;&#20004;&#20010;&#23376;&#20219;&#21153;&#65306;(1)&#35782;&#21035;&#21644;&#26631;&#35760;&#23545;&#24212;&#20110;&#20248;&#21270;&#38382;&#39064;&#32452;&#20214;&#30340;&#35821;&#20041;&#23454;&#20307;;(2)&#20174;&#26816;&#27979;&#21040;&#30340;&#38382;&#39064;&#23454;&#20307;&#29983;&#25104;&#24847;&#20041;&#34920;&#31034;(&#21363;&#36923;&#36753;&#24418;&#24335;)&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#26631;&#35760;&#20248;&#21270;&#38382;&#39064;&#30340;&#23454;&#20307;&#26469;&#20943;&#23569;&#27495;&#20041;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#21019;&#24314;&#20102;&#19968;&#20010;&#32447;&#24615;&#35268;&#21010;(LP)&#38382;&#39064;&#30340;&#20013;&#38388;&#34920;&#31034;&#65292;&#35813;&#20013;&#38388;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#21830;&#29992;&#27714;&#35299;&#22120;&#21487;&#29992;&#30340;&#26684;&#24335;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LP&#21333;&#35789;&#38382;&#39064;&#25968;&#25454;&#38598;&#21644;NL4Opt&#27604;&#36187;&#30340;&#20849;&#20139;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#20102;&#31454;&#36187;&#26465;&#30446;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Natural Language for Optimization (NL4Opt) Competition was created to investigate methods of extracting the meaning and formulation of an optimization problem based on its text description. Specifically, the goal of the competition is to increase the accessibility and usability of optimization solvers by allowing non-experts to interface with them using natural language. We separate this challenging goal into two sub-tasks: (1) recognize and label the semantic entities that correspond to the components of the optimization problem; (2) generate a meaning representation (i.e., a logical form) of the problem from its detected problem entities. The first task aims to reduce ambiguity by detecting and tagging the entities of the optimization problems. The second task creates an intermediate representation of the linear programming (LP) problem that is converted into a format that can be used by commercial solvers. In this report, we present the LP word problem dataset and shared tasks f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.08179</link><description>&lt;p&gt;
MEDBERT.de&#65306;&#19968;&#20010;&#22522;&#20110;&#24503;&#35821;&#30340;&#12289;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#20840;&#38754;BERT&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain. (arXiv:2303.08179v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#30340;BERT&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#30340;&#35757;&#32451;&#65292;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#23545;&#38271;&#25991;&#26412;&#29305;&#21035;&#26377;&#29992;&#65292;&#32780;&#25968;&#25454;&#21435;&#37325;&#21644;&#26377;&#25928;&#30340;&#20998;&#35789;&#21017;&#21482;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#20102;&#36739;&#23567;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;medBERT.de&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24503;&#35821;&#21307;&#23398;&#39046;&#22495;&#19987;&#38376;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#24050;&#32463;&#22312;470&#19975;&#20221;&#24503;&#35821;&#21307;&#23398;&#25991;&#26723;&#30340;&#22823;&#22411;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#21307;&#23398;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#25928;&#26524;&#65292;&#28041;&#21450;&#21508;&#31181;&#23398;&#31185;&#21644;&#21307;&#23398;&#25991;&#29486;&#31867;&#22411;&#12290;&#38500;&#20102;&#35780;&#20272;&#35813;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#22806;&#65292;&#26412;&#25991;&#36824;&#23545;&#20854;&#33021;&#21147;&#36827;&#34892;&#20102;&#26356;&#28145;&#20837;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#21435;&#37325;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#20351;&#29992;&#26356;&#26377;&#25928;&#30340;&#20998;&#35789;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;medBERT.de&#36825;&#26679;&#30340;&#39046;&#22495;&#19987;&#29992;&#27169;&#22411;&#29305;&#21035;&#36866;&#29992;&#20110;&#36739;&#38271;&#30340;&#25991;&#26412;&#65292;&#24182;&#19988;&#25968;&#25454;&#21435;&#37325;&#19981;&#19968;&#23450;&#20250;&#23548;&#33268;&#24615;&#33021;&#25913;&#21892;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26377;&#25928;&#30340;&#20998;&#35789;&#21482;&#22312;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#20102;&#36739;&#23567;&#30340;&#20316;&#29992;&#65292;&#24182;&#19988;&#22823;&#22810;&#25968;&#25913;&#36827;&#28304;&#20110;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents medBERT.de, a pre-trained German BERT model specifically designed for the German medical domain. The model has been trained on a large corpus of 4.7 Million German medical documents and has been shown to achieve new state-of-the-art performance on eight different medical benchmarks covering a wide range of disciplines and medical document types. In addition to evaluating the overall performance of the model, this paper also conducts a more in-depth analysis of its capabilities. We investigate the impact of data deduplication on the model's performance, as well as the potential benefits of using more efficient tokenization methods. Our results indicate that domain-specific models such as medBERT.de are particularly useful for longer texts, and that deduplication of training data does not necessarily lead to improved performance. Furthermore, we found that efficient tokenization plays only a minor role in improving model performance, and attribute most of the improved
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.07522</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23548;&#33322;&#30340;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Audio Visual Language Maps for Robot Navigation. (arXiv:2303.07522v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#29992;&#20110;&#23384;&#20648;&#36328;&#27169;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#26426;&#22120;&#20154;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#30340;&#23548;&#33322;&#26041;&#24335;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;AVLMaps&#23454;&#29616;&#20102;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#30340;&#38646;&#27425;&#23398;&#20064;&#24335;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#19990;&#30028;&#30340;&#20114;&#21160;&#26159;&#19968;&#31181;&#22810;&#24863;&#23448;&#30340;&#20307;&#39564;&#65292;&#20294;&#26159;&#35768;&#22810;&#26426;&#22120;&#20154;&#20173;&#28982;&#20027;&#35201;&#20381;&#36182;&#35270;&#35273;&#24863;&#30693;&#26469;&#32472;&#21046;&#21644;&#23548;&#33322;&#20182;&#20204;&#30340;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38899;&#35270;&#35821;&#35328;&#22320;&#22270;(AVLMaps)&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;3D&#31354;&#38388;&#22320;&#22270;&#34920;&#31034;&#65292;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#38899;&#39057;&#12289;&#35270;&#35273;&#21644;&#35821;&#35328;&#32447;&#32034;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#12290;&#22312;&#23548;&#33322;&#30340;&#24773;&#22659;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#20351;&#26426;&#22120;&#20154;&#31995;&#32479;&#26681;&#25454;&#22810;&#27169;&#24577;&#26597;&#35810;(&#20363;&#22914;&#65292;&#25991;&#26412;&#25551;&#36848;&#12289;&#22270;&#20687;&#25110;&#22320;&#26631;&#30340;&#38899;&#39057;&#29255;&#27573;)&#22312;&#22320;&#22270;&#20013;&#32034;&#24341;&#30446;&#26631;&#12290;&#29305;&#21035;&#26159;&#65292;&#28155;&#21152;&#38899;&#39057;&#20449;&#24687;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#26356;&#21487;&#38752;&#22320;&#28040;&#38500;&#30446;&#26631;&#20301;&#32622;&#30340;&#27495;&#20041;&#24615;&#12290;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;AVLMaps&#33021;&#22815;&#23454;&#29616;&#20174;&#22810;&#27169;&#24577;&#25552;&#31034;&#36827;&#34892;&#38646;&#27425;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#23548;&#33322;&#65292;&#24182;&#22312;&#27169;&#31946;&#22330;&#26223;&#20013;&#25552;&#20379;50%&#26356;&#22909;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While interacting in the world is a multi-sensory experience, many robots continue to predominantly rely on visual perception to map and navigate in their environments. In this work, we propose Audio-Visual-Language Maps (AVLMaps), a unified 3D spatial map representation for storing cross-modal information from audio, visual, and language cues. AVLMaps integrate the open-vocabulary capabilities of multimodal foundation models pre-trained on Internet-scale data by fusing their features into a centralized 3D voxel grid. In the context of navigation, we show that AVLMaps enable robot systems to index goals in the map based on multimodal queries, e.g., textual descriptions, images, or audio snippets of landmarks. In particular, the addition of audio information enables robots to more reliably disambiguate goal locations. Extensive experiments in simulation show that AVLMaps enable zero-shot multimodal goal navigation from multimodal prompts and provide 50% better recall in ambiguous scenar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2303.06333</link><description>&lt;p&gt;
&#38477;&#33853;&#20254;&#65306;&#35780;&#20272;&#20132;&#20114;&#24335;&#20154;&#26426;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Parachute: Evaluating Interactive Human-LM Co-writing Systems. (arXiv:2303.06333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a human-centered evaluation framework, Parachute, for interactive co-writing systems, which includes categorized practical metrics and can be used to evaluate and compare co-writing systems.
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#39134;&#36895;&#21457;&#23637;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#20110;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26500;&#24314;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#20132;&#20114;&#22320;&#20026;&#20849;&#21516;&#30340;&#20889;&#20316;&#25104;&#26524;&#20570;&#20986;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#20110;&#20132;&#20114;&#24335;&#29615;&#22659;&#19979;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#35780;&#20272;&#26694;&#26550;Parachute&#65292;&#29992;&#20110;&#20132;&#20114;&#24335;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#30340;&#35780;&#20272;&#12290;Parachute&#23637;&#31034;&#20102;&#20132;&#20114;&#35780;&#20272;&#30340;&#32508;&#21512;&#35270;&#35282;&#65292;&#20854;&#20013;&#27599;&#20010;&#35780;&#20272;&#26041;&#38754;&#37117;&#21253;&#21547;&#20102;&#20998;&#31867;&#30340;&#23454;&#29992;&#25351;&#26631;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20351;&#29992;&#26696;&#20363;&#26469;&#28436;&#31034;&#22914;&#20309;&#20351;&#29992;Parachute&#35780;&#20272;&#21644;&#27604;&#36739;&#20849;&#21516;&#25776;&#20889;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A surge of advances in language models (LMs) has led to significant interest in using LMs to build co-writing systems, in which humans and LMs interactively contribute to a shared writing artifact. However, there is a lack of studies assessing co-writing systems in interactive settings. We propose a human-centered evaluation framework, Parachute, for interactive co-writing systems. Parachute showcases an integrative view of interaction evaluation, where each evaluation aspect consists of categorized practical metrics. Furthermore, we present Parachute with a use case to demonstrate how to evaluate and compare co-writing systems using Parachute.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#36825;&#20010;&#26694;&#26550;&#20351;LLM&#22312;&#19981;&#21516;&#31867;&#22411;&#28436;&#31034;&#19979;&#30340;DIE&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20301;&#32622;&#21644;&#26684;&#24335;&#26041;&#38754;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2303.05063</link><description>&lt;p&gt;
ICL-D3IE&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;+&#22810;&#26679;&#23637;&#31034;&#26356;&#26032;&#65292;&#29992;&#20110;&#25991;&#26723;&#20449;&#24687;&#25277;&#21462;&#65288;arXiv:2303.05063v2 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05063
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#36825;&#20010;&#26694;&#26550;&#20351;LLM&#22312;&#19981;&#21516;&#31867;&#22411;&#28436;&#31034;&#19979;&#30340;DIE&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#25913;&#36827;&#24615;&#33021;&#30340;&#21453;&#39304;&#26426;&#21046;&#65292;&#21516;&#26102;&#28085;&#30422;&#20102;&#20301;&#32622;&#21644;&#26684;&#24335;&#26041;&#38754;&#30340;&#28436;&#31034;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;GPT-3&#21644;ChatGPT&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25104;&#26524;&#65292;&#23588;&#20854;&#26159;&#24212;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21363;&#22522;&#20110;&#23569;&#37327;&#28436;&#31034;&#31034;&#20363;&#36827;&#34892;&#25512;&#29702;&#12290;&#23613;&#31649;&#22312;NLP&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23578;&#26410;&#36827;&#34892;&#30740;&#31350;&#20197;&#35780;&#20272;LLM&#22312;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#25191;&#34892;&#25991;&#26723;&#20449;&#24687;&#25277;&#21462;&#65288;DIE&#65289;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;LLM&#25191;&#34892;DIE&#23384;&#22312;&#20004;&#20010;&#25361;&#25112;&#65306;&#27169;&#24577;&#21644;&#20219;&#21153;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;ICL-D3IE&#65292;&#23427;&#20351;LLM&#33021;&#22815;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#28436;&#31034;&#31034;&#20363;&#25191;&#34892;DIE&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#38590;&#20197;&#35757;&#32451;&#30340;&#25991;&#26723;&#20013;&#25552;&#21462;&#26368;&#22256;&#38590;&#21644;&#26368;&#19981;&#21516;&#30340;&#29255;&#27573;&#20316;&#20026;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#20415;&#21463;&#30410;&#20110;&#25152;&#26377;&#27979;&#35797;&#23454;&#20363;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#25551;&#36848;&#20851;&#31995;&#30340;&#28436;&#31034;&#31034;&#20363;&#65292;&#20351;LLM&#33021;&#22815;&#29702;&#35299;&#20301;&#32622;&#20851;&#31995;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#26684;&#24335;&#21270;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#26041;&#20415;&#25552;&#21462;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21453;&#39304;&#26426;&#21046;&#65292;&#26356;&#26032;&#20102;&#28436;&#31034;&#31034;&#20363;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ICL-D3IE&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated remarkable results in various natural language processing (NLP) tasks with in-context learning, which involves inference based on a few demonstration examples. Despite their successes in NLP tasks, no investigation has been conducted to assess the ability of LLMs to perform document information extraction (DIE) using in-context learning. Applying LLMs to DIE poses two challenges: the modality and task gap. To this end, we propose a simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples. Specifically, we extract the most difficult and distinct segments from hard training documents as hard demonstrations for benefiting all test instances. We design demonstrations describing relationships that enable LLMs to understand positional relationships. We introduce formatting demonstrations for easy answer extraction. Additionally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02841</link><description>&lt;p&gt;
&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Model-Agnostic Meta-Learning for Natural Language Understanding Tasks in Finance. (arXiv:2303.02841v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37329;&#34701;&#39046;&#22495;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#22240;&#32570;&#20047;&#26631;&#27880;&#25968;&#25454;&#21644;&#29305;&#27530;&#35821;&#35328;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#23398;&#20064;&#31283;&#20581;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#24494;&#35843;&#32463;&#24120;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#22810;&#20219;&#21153;&#23398;&#20064;&#21487;&#33021;&#20250;&#20559;&#34962;&#22823;&#37327;&#25968;&#25454;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#37329;&#34701;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;1.&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;&#22810;&#31181;&#31867;&#22411;&#20219;&#21153;&#30340;MAML&#26041;&#27861;&#30340;&#24615;&#33021;&#65306;GLUE&#25968;&#25454;&#38598;&#65292;SNLI&#65292;Sci-Tail&#21644;Financial PhraseBank&#65307;2.&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#20010;&#30495;&#23454;&#22330;&#26223;&#30340;&#22522;&#20110;&#25512;&#29305;&#25991;&#26412;&#30340;&#32929;&#31080;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#20013;&#20351;&#29992;MAML&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#26681;&#25454;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding(NLU) is challenging for finance due to the lack of annotated data and the specialized language in that domain. As a result, researchers have proposed to use pre-trained language model and multi-task learning to learn robust representations. However, aggressive fine-tuning often causes over-fitting and multi-task learning may favor tasks with significantly larger amounts data, etc. To address these problems, in this paper, we investigate model-agnostic meta-learning algorithm(MAML) in low-resource financial NLU tasks. Our contribution includes: 1. we explore the performance of MAML method with multiple types of tasks: GLUE datasets, SNLI, Sci-Tail and Financial PhraseBank; 2. we study the performance of MAML method with multiple single-type tasks: a real scenario stock price prediction problem with twitter text data. Our models achieve the state-of-the-art performance according to the experimental results, which demonstrate that our method can adapt fast a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.14383</link><description>&lt;p&gt;
&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#65306;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Linear Spaces of Meanings: Compositional Structures in Vision-Language Models. (arXiv:2302.14383v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14383
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#30340;&#26041;&#27861;&#65292;&#23558;&#36825;&#20123;&#21521;&#37327;&#35270;&#20026;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#65292;&#24182;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#30340;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#30340;&#25968;&#25454;&#23884;&#20837;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#32452;&#21512;&#24615;&#19982;&#39044;&#20808;&#23384;&#22312;&#30340;&#35789;&#27719;&#34920;&#20013;&#30340;&#21333;&#35789;&#23884;&#20837;&#30340;&#20195;&#25968;&#36816;&#31639;&#26377;&#20851;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35797;&#22270;&#20351;&#29992;&#23884;&#20837;&#31354;&#38388;&#20013;&#36739;&#23567;&#38598;&#21512;&#30340;&#21521;&#37327;&#32452;&#21512;&#26469;&#36817;&#20284;&#34920;&#31034;&#26469;&#33258;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#24418;&#24335;&#12290;&#36825;&#20123;&#21521;&#37327;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#22312;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;&#27010;&#24565;&#30340;&#8220;&#29702;&#24819;&#21333;&#35789;&#8221;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20960;&#20309;&#23398;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#29702;&#35299;&#32452;&#21512;&#32467;&#26500;&#30340;&#26694;&#26550;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;VLM&#23884;&#20837;&#22312;&#27010;&#29575;&#19978;&#30340;&#36825;&#20123;&#32452;&#21512;&#32467;&#26500;&#30340;&#21547;&#20041;&#65292;&#24182;&#25552;&#20379;&#20102;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#20135;&#29983;&#30340;&#30452;&#35273;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;CLIP&#30340;&#23884;&#20837;&#20013;&#20197;&#23454;&#39564;&#26041;&#24335;&#25506;&#32034;&#20102;&#36825;&#20123;&#32467;&#26500;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#35299;&#20915;&#20998;&#31867;&#12289;&#21435;&#20559;&#21644;&#26816;&#32034;&#31561;&#19981;&#21516;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23884;&#20837;&#31354;&#38388;&#20013;&#31616;&#21333;&#30340;&#32447;&#24615;&#20195;&#25968;&#36816;&#31639;&#21487;&#20197;&#23454;&#29616;&#19982;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#24847;&#20041;&#30340;&#32447;&#24615;&#31354;&#38388;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate compositional structures in data embeddings from pre-trained vision-language models (VLMs). Traditionally, compositionality has been associated with algebraic operations on embeddings of words from a pre-existing vocabulary. In contrast, we seek to approximate representations from an encoder as combinations of a smaller set of vectors in the embedding space. These vectors can be seen as "ideal words" for generating concepts directly within the embedding space of the model. We first present a framework for understanding compositional structures from a geometric perspective. We then explain what these compositional structures entail probabilistically in the case of VLM embeddings, providing intuitions for why they arise in practice. Finally, we empirically explore these structures in CLIP's embeddings and we evaluate their usefulness for solving different vision-language tasks such as classification, debiasing, and retrieval. Our results show that simple linear algebraic o
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2302.02676</link><description>&lt;p&gt;
&#22238;&#39038;&#38142;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#21453;&#39304;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Chain of Hindsight Aligns Language Models with Feedback. (arXiv:2302.02676v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02676
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#21363;&#22238;&#39038;&#38142;&#65292;&#21487;&#20197;&#36731;&#26494;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#36825;&#26679;&#25165;&#33021;&#23545;&#20154;&#31867;&#26377;&#25152;&#24110;&#21161;&#24182;&#31526;&#21512;&#20154;&#31867;&#21644;&#31038;&#20250;&#20215;&#20540;&#35266;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#26469;&#29702;&#35299;&#21644;&#36981;&#24490;&#25351;&#20196;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#26159;&#22522;&#20110;&#34987;&#20154;&#31867;&#27880;&#37322;&#32773;&#21916;&#27426;&#30340;&#25163;&#21160;&#25361;&#36873;&#30340;&#27169;&#22411;&#29983;&#25104;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#25968;&#25454;&#21033;&#29992;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#19988;&#26222;&#36941;&#24212;&#29992;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#22870;&#21169;&#20989;&#25968;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#23481;&#26131;&#20986;&#29616;&#22870;&#21169;&#20989;&#25968;&#19981;&#23436;&#32654;&#21644;&#26497;&#38590;&#20248;&#21270;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#65292;&#8220;&#22238;&#39038;&#38142;&#8221;&#65292;&#23427;&#26131;&#20110;&#20248;&#21270;&#65292;&#24182;&#21487;&#20197;&#20174;&#20219;&#20309;&#24418;&#24335;&#30340;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19981;&#21463;&#20854;&#26497;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#21463;&#21040;&#20102;&#20154;&#31867;&#22914;&#20309;&#20174;&#20197;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#30340;&#24191;&#27867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#31867;&#22411;&#30340;&#21453;&#39304;&#36716;&#25442;&#25104;&#21477;&#23376;&#65292;&#28982;&#21518;&#29992;&#23427;&#20204;&#26469;&#24494;&#35843;&#27169;&#22411;&#65292;&#20174;&#32780;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human preferences is important for language models to be helpful and useful for humans, and to align with human and social values. Prior work have achieved remarkable successes by learning from human feedback to understand and follow instructions. Nonetheless, these methods are either founded on hand-picked model generations that are favored by human annotators, rendering them ineffective in terms of data utilization and challenging to apply in general, or they depend on reward functions and reinforcement learning, which are prone to imperfect reward function and extremely challenging to optimize. In this work, we propose a novel technique, Chain of Hindsight, that is easy to optimize and can learn from any form of feedback, regardless of its polarity. Our idea is inspired by how humans learn from extensive feedback presented in the form of languages. We convert all types of feedback into sentences, which are then used to fine-tune the model, allowing us to take advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.10405</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10405
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#8212;&#8212;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65292;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#12290;&#38024;&#23545;&#36825;&#19968;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#26041;&#26696;&#8212;&#8212;KGEditor&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23884;&#20837;&#24050;&#32463;&#21462;&#24471;&#20102;&#23454;&#35777;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#36890;&#24120;&#20316;&#20026;&#38745;&#24577;&#24037;&#20214;&#37096;&#32626;&#65292;&#20462;&#25913;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32534;&#36753;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;KG&#23884;&#20837;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#23454;&#29616;&#23545;KG&#23884;&#20837;&#30340;&#25968;&#25454;&#39640;&#25928;&#21644;&#24555;&#36895;&#26356;&#26032;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#22235;&#20010;&#26032;&#25968;&#25454;&#38598;&#65306;E-FB15k237&#12289;A-FB15k237&#12289;E-WN18RR &#21644; A-WN18RR&#65292;&#24182;&#35780;&#20272;&#20102;&#20960;&#31181;&#30693;&#35782;&#32534;&#36753;&#22522;&#32447;&#65292;&#35777;&#26126;&#20102;&#20043;&#21069;&#30340;&#27169;&#22411;&#22788;&#29702;&#35813;&#20219;&#21153;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#24378;&#22823;&#30340;&#22522;&#32447;&#8212;&#8212;KGEditor&#65292;&#23427;&#21033;&#29992;&#36229;&#32593;&#32476;&#30340;&#38468;&#21152;&#21442;&#25968;&#23618;&#26469;&#32534;&#36753;/&#28155;&#21152;&#20107;&#23454;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#26356;&#26032;&#29305;&#23450;&#20107;&#23454;&#32780;&#19981;&#24433;&#21709;&#20854;&#20313;&#37096;&#20998;&#30340;&#24615;&#33021;&#26102;&#65292;KGEditor &#30340;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently decades have witnessed the empirical success of framing Knowledge Graph (KG) embeddings via language models. However, language model-based KG embeddings are usually deployed as static artifacts, which are challenging to modify without re-training after deployment. To address this issue, we propose a new task of editing language model-based KG embeddings in this paper. The proposed task aims to enable data-efficient and fast updates to KG embeddings without damaging the performance of the rest. We build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and evaluate several knowledge editing baselines demonstrating the limited ability of previous models to handle the proposed challenging task. We further propose a simple yet strong baseline dubbed KGEditor, which utilizes additional parametric layers of the hyper network to edit/add facts. Comprehensive experimental results demonstrate that KGEditor can perform better when updating specific facts while not affec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#35875;&#35328;&#65292;&#24182;&#21487;&#36866;&#24212;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2212.01117</link><description>&lt;p&gt;
&#22522;&#20110;Prompt&#23398;&#20064;&#19982;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Rumor Detection with Propagation Structure via Prompt Learning. (arXiv:2212.01117v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#21644;&#20256;&#25773;&#32467;&#26500;&#30340;&#38646;&#26679;&#26412;&#35875;&#35328;&#26816;&#27979;&#26694;&#26550;&#65292;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#35875;&#35328;&#65292;&#24182;&#21487;&#36866;&#24212;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#26102;&#20195;&#65292;&#35875;&#35328;&#38543;&#30528;&#20107;&#20214;&#30340;&#21457;&#29983;&#32780;&#20256;&#25773;&#65292;&#20005;&#37325;&#24433;&#21709;&#20102;&#30495;&#30456;&#30340;&#20256;&#25773;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#27880;&#36164;&#28304;&#65292;&#24456;&#38590;&#26816;&#27979;&#20986;&#20351;&#29992;&#23569;&#25968;&#35821;&#35328;&#30340;&#35875;&#35328;&#12290;&#32780;&#19988;&#65292;&#26152;&#22825;&#27809;&#26377;&#28041;&#21450;&#21040;&#30340;&#38750;&#39044;&#26009;&#20013;&#26029;&#20107;&#20214;&#21152;&#21095;&#20102;&#25968;&#25454;&#36164;&#28304;&#30340;&#31232;&#32570;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Prompt&#23398;&#20064;&#30340;&#26032;&#22411;&#38646;&#26679;&#26412;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#19981;&#21516;&#39046;&#22495;&#25110;&#29992;&#19981;&#21516;&#35821;&#35328;&#23637;&#29616;&#30340;&#35875;&#35328;&#12290;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#31038;&#20132;&#23186;&#20307;&#19978;&#20256;&#25773;&#30340;&#35875;&#35328;&#34920;&#31034;&#20026;&#22810;&#26679;&#30340;&#20256;&#25773;&#32447;&#31243;&#65292;&#28982;&#21518;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;Prompt&#32534;&#30721;&#26426;&#21046;&#65292;&#23398;&#20064;&#20102;&#26080;&#35821;&#35328;&#29615;&#22659;&#19979;&#30340;&#19978;&#19979;&#25991;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#20419;&#36827;&#23545;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#19979;&#30340;&#35875;&#35328;&#25968;&#25454;&#36716;&#25442;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20256;&#25773;&#32447;&#31243;&#20013;&#24314;&#27169;&#39046;&#22495;&#19981;&#21464;&#30340;&#32467;&#26500;&#29305;&#24449;&#65292;&#20197;&#25972;&#21512;&#26377;&#24433;&#21709;&#21147;&#30340;&#31038;&#21306;&#21453;&#24212;&#30340;&#32467;&#26500;&#20301;&#32622;&#34920;&#31034;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#39046;&#22495;&#36866;&#24212;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#24341;&#20837;&#26032;&#30340;&#34394;&#25311;&#21709;&#24212;&#26426;&#21046;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#30340;&#25512;&#26029;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spread of rumors along with breaking events seriously hinders the truth in the era of social media. Previous studies reveal that due to the lack of annotated resources, rumors presented in minority languages are hard to be detected. Furthermore, the unforeseen breaking events not involved in yesterday's news exacerbate the scarcity of data resources. In this work, we propose a novel zero-shot framework based on prompt learning to detect rumors falling in different domains or presented in different languages. More specifically, we firstly represent rumor circulated on social media as diverse propagation threads, then design a hierarchical prompt encoding mechanism to learn language-agnostic contextual representations for both prompts and rumor data. To further enhance domain adaptation, we model the domain-invariant structural features from the propagation threads, to incorporate structural position representations of influential community response. In addition, a new virtual respon
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; MUPPET &#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#32452;&#21512;&#26102;&#24207;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#35299;&#20915;&#20102;&#38382;&#39064;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.14905</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-Modal Few-Shot Temporal Action Detection. (arXiv:2211.14905v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14905
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979;&#38382;&#39064;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340; MUPPET &#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#32452;&#21512;&#26102;&#24207;&#36830;&#32493;&#30340;&#29255;&#27573;&#65292;&#35299;&#20915;&#20102;&#38382;&#39064;&#12290;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412; (FS) &#21644;&#38646;&#26679;&#26412; (ZS) &#23398;&#20064;&#26159;&#32553;&#25918;&#26102;&#38388;&#21160;&#20316;&#26816;&#27979; (TAD) &#21040;&#26032;&#31867;&#30340;&#20004;&#31181;&#19981;&#21516;&#26041;&#27861;&#12290;&#21069;&#32773;&#23558;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#36866;&#24212;&#20110;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#30001;&#27599;&#31867;&#20165;&#26377;&#19968;&#20010;&#35270;&#39057;&#34920;&#31034;&#65292;&#32780;&#21518;&#32773;&#36890;&#36807;&#21033;&#29992;&#26032;&#31867;&#30340;&#35821;&#20041;&#25551;&#36848;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#31034;&#20363;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412; (MMFS) TAD &#38382;&#39064;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#36890;&#36807;&#20849;&#21516;&#21033;&#29992;&#23569;&#25968;&#25903;&#25345;&#35270;&#39057;&#21644;&#26032;&#31867;&#21517;&#23383;&#26469;&#32467;&#21512; FS-TAD &#21644; ZS-TAD &#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; MUlti-modality PromPt mETa-learning (MUPPET) &#26041;&#27861;&#12290;&#36825;&#26159;&#36890;&#36807;&#26377;&#25928;&#22320;&#36830;&#25509;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#26368;&#22823;&#31243;&#24230;&#22320;&#37325;&#29992;&#24050;&#32463;&#23398;&#20064;&#30340;&#33021;&#21147;&#26469;&#23454;&#29616;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20803;&#23398;&#20064;&#36866;&#37197;&#22120;&#35013;&#22791;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#35789;&#22120;&#65292;&#23558;&#25903;&#25345;&#35270;&#39057;&#26144;&#23556;&#21040;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#26631;&#35760;&#31354;&#38388;&#20013;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#25552;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#30340;&#31867;&#20869;&#21464;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#32858;&#31867;&#31639;&#27861;&#26469;&#23545;&#26102;&#38388;&#19978;&#19968;&#33268;&#30340;&#25552;&#35758;&#29255;&#27573;&#36827;&#34892;&#20998;&#32452;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#21644;&#38646;&#26679;&#26412;&#22330;&#26223;&#19979;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#19981;&#21516;&#32452;&#20214;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot (FS) and zero-shot (ZS) learning are two different approaches for scaling temporal action detection (TAD) to new classes. The former adapts a pretrained vision model to a new task represented by as few as a single video per class, whilst the latter requires no training examples by exploiting a semantic description of the new class. In this work, we introduce a new multi-modality few-shot (MMFS) TAD problem, which can be considered as a marriage of FS-TAD and ZS-TAD by leveraging few-shot support videos and new class names jointly. To tackle this problem, we further introduce a novel MUlti-modality PromPt mETa-learning (MUPPET) method. This is enabled by efficiently bridging pretrained vision and language models whilst maximally reusing already learned capacity. Concretely, we construct multi-modal prompts by mapping support videos into the textual token space of a vision-language model using a meta-learned adapter-equipped visual semantics tokenizer. To tackle large intra-clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#35821;&#20041;&#23436;&#25104;&#23398;&#20064;&#8221;&#20219;&#21153;&#65292;&#20197;&#25913;&#21892;&#29616;&#26377;&#33945;&#38754;&#24314;&#27169;&#20219;&#21153;&#24573;&#30053;&#20840;&#23616;&#35821;&#20041;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#24212;&#33945;&#38754;&#25968;&#25454;&#30340;&#32570;&#22833;&#35821;&#20041;&#26469;&#34917;&#20805;&#23427;&#20204;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.13437</link><description>&lt;p&gt;
&#35265;&#35782;&#20320;&#25152;&#38169;&#36807;&#30340;&#65306;&#35821;&#20041;&#23436;&#25104;&#23398;&#20064;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning. (arXiv:2211.13437v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#35821;&#20041;&#23436;&#25104;&#23398;&#20064;&#8221;&#20219;&#21153;&#65292;&#20197;&#25913;&#21892;&#29616;&#26377;&#33945;&#38754;&#24314;&#27169;&#20219;&#21153;&#24573;&#30053;&#20840;&#23616;&#35821;&#20041;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#25429;&#25417;&#30456;&#24212;&#33945;&#38754;&#25968;&#25454;&#30340;&#32570;&#22833;&#35821;&#20041;&#26469;&#34917;&#20805;&#23427;&#20204;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#27169;&#24577;&#23545;&#40784;&#23545;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;VLP&#65289;&#27169;&#22411;&#23398;&#20064;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#27491;&#30830;&#30456;&#20851;&#20449;&#24687;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38024;&#23545;&#20197;&#21069;&#30340;&#33945;&#38754;&#24314;&#27169;&#20219;&#21153;&#20027;&#35201;&#20851;&#27880;&#37325;&#26500;&#33945;&#38754;&#26631;&#35760;&#32780;&#24573;&#30053;&#20102;&#33945;&#38754;&#25968;&#25454;&#29983;&#25104;&#30340;&#20840;&#23616;&#35821;&#20041;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#35821;&#20041;&#23436;&#25104;&#23398;&#20064;&#8221;&#20219;&#21153;&#65292;&#20197;&#20415;&#20840;&#23616;&#23545;&#23616;&#37096;&#30340;&#23545;&#40784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SCL&#20219;&#21153;&#36890;&#36807;&#25429;&#25417;&#30456;&#24212;&#33945;&#38754;&#25968;&#25454;&#30340;&#32570;&#22833;&#35821;&#20041;&#26469;&#34917;&#20805;&#23427;&#20204;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-modal alignment is essential for vision-language pre-training (VLP) models to learn the correct corresponding information across different modalities. For this purpose, inspired by the success of masked language modeling (MLM) tasks in the NLP pre-training area, numerous masked modeling tasks have been proposed for VLP to further promote cross-modal interactions. The core idea of previous masked modeling tasks is to focus on reconstructing the masked tokens based on visible context for learning local-to-local alignment. However, most of them pay little attention to the global semantic features generated for the masked data, resulting in a limited cross-modal alignment ability of global representations. Therefore, in this paper, we propose a novel Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment. Specifically, the SCL task complements the missing semantics of masked data by capturing the corresponding
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;CLEVER&#65292;&#19968;&#31181;&#20197;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24120;&#35782;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#21547;&#26377;&#20851;&#23454;&#20307;&#23545;&#30340;&#22270;&#20687;&#21253;&#27719;&#24635;&#20986;&#24120;&#35782;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#23545;&#22270;&#20687;&#23454;&#20363;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.12054</link><description>&lt;p&gt;
&#35270;&#35273;&#22522;&#30784;&#19979;&#30340;&#24120;&#35782;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Visually Grounded Commonsense Knowledge Acquisition. (arXiv:2211.12054v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CLEVER&#65292;&#19968;&#31181;&#20197;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#24120;&#35782;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#21253;&#21547;&#26377;&#20851;&#23454;&#20307;&#23545;&#30340;&#22270;&#20687;&#21253;&#27719;&#24635;&#20986;&#24120;&#35782;&#20851;&#31995;&#65292;&#36991;&#20813;&#20102;&#23545;&#22270;&#20687;&#23454;&#20363;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#24120;&#35782;&#30693;&#35782;&#24211;&#20026;&#24191;&#27867;&#30340;AI&#24212;&#29992;&#25552;&#20379;&#21160;&#21147;&#65292;&#20854;&#20013;&#33258;&#21160;&#25552;&#21462;&#24120;&#35782;&#30693;&#35782;&#65288;CKE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30001;&#25991;&#26412;&#25552;&#21462;CKE&#30693;&#35782;&#26159;&#24050;&#30693;&#30340;&#21463;&#38480;&#20110;&#26412;&#36136;&#30340;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#20559;&#24046;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#35270;&#35273;&#30693;&#35273;&#21253;&#21547;&#26377;&#20851;&#30495;&#23454;&#19990;&#30028;&#23454;&#20307;&#30340;&#20016;&#23500;&#24120;&#35782;&#30693;&#35782;&#65292;&#20363;&#22914;&#65288;&#20154;&#65292;&#21487;&#20197;&#25345;&#26377;&#65292;&#29942;&#23376;&#65289;&#65292;&#36825;&#20123;&#30693;&#35782;&#21487;&#20316;&#20026;&#33719;&#21462;&#22522;&#20110;&#24120;&#35782;&#30340;&#30693;&#35782;&#30340;&#26377;&#24076;&#26395;&#26469;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;CLEVER&#65292;&#23558;CKE&#20316;&#20026;&#19968;&#31181;&#36828;&#36317;&#31163;&#30417;&#30563;&#22810;&#23454;&#20363;&#23398;&#20064;&#38382;&#39064;&#36827;&#34892;&#20102;&#35268;&#23450;&#65292;&#27169;&#22411;&#36890;&#36807;&#20851;&#20110;&#23454;&#20307;&#23545;&#30340;&#22270;&#20687;&#21253;&#27719;&#24635;&#20986;&#24120;&#35782;&#20851;&#31995;&#32780;&#26080;&#38656;&#23545;&#22270;&#20687;&#23454;&#20363;&#36827;&#34892;&#20154;&#24037;&#27880;&#37322;&#12290;CLEVER&#20351;&#29992;&#20102;&#22522;&#20110;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#28145;&#20837;&#29702;&#35299;&#22270;&#20687;&#21253;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#65292;&#24182;&#20174;&#20013;&#36873;&#25321;&#20449;&#24687;&#24615;&#23454;&#20363;&#65292;&#20197;&#36890;&#36807;&#26032;&#39062;&#30340;&#20851;&#31995;&#27719;&#24635;&#26041;&#24335;&#27010;&#25324;&#24120;&#35782;&#30693;&#35782;&#23454;&#20307;&#20851;&#31995;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale commonsense knowledge bases empower a broad range of AI applications, where the automatic extraction of commonsense knowledge (CKE) is a fundamental and challenging problem. CKE from text is known for suffering from the inherent sparsity and reporting bias of commonsense in text. Visual perception, on the other hand, contains rich commonsense knowledge about real-world entities, e.g., (person, can_hold, bottle), which can serve as promising sources for acquiring grounded commonsense knowledge. In this work, we present CLEVER, which formulates CKE as a distantly supervised multi-instance learning problem, where models learn to summarize commonsense relations from a bag of images about an entity pair without any human annotation on image instances. To address the problem, CLEVER leverages vision-language pre-training models for deep understanding of each image in the bag, and selects informative instances from the bag to summarize commonsense entity relations via a novel cont
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.07717</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#28145;&#24230;&#26102;&#38388;&#24314;&#27169;&#22312;&#20020;&#24202;&#25233;&#37057;&#30151;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#26102;&#38388;&#36724;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#65288;DSD&#65289;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#22522;&#20110;&#26368;&#22823;&#25968;&#37327;&#30340;&#24050;&#32463;&#36807;&#20020;&#24202;&#21307;&#24072;&#27880;&#37322;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#38543;&#21518;&#20351;&#29992;&#25105;&#20204;&#30340;DSD&#27169;&#22411;&#26469;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#20363;&#22914;&#25233;&#37057;&#30151;&#35780;&#20998;&#21450;&#20854;&#38543;&#21518;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#20197;&#21450;&#29992;&#25143;&#21457;&#24067;&#27963;&#21160;&#27169;&#24335;&#65292;&#20363;&#22914;&#37327;&#21270;&#20182;&#20204;&#30340;&#8220;&#26080;&#27963;&#21160;&#8221;&#25110;&#8220;&#27785;&#40664;&#8221;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25552;&#21462;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#20004;&#20010;&#29616;&#26377;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#29992;&#25143;&#32423;&#21035;&#25233;&#37057;&#30151;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#21333;&#20010;&#29305;&#24449;&#12289;&#22522;&#32447;&#29305;&#24449;&#21644;&#29305;&#24449;&#21066;&#20943;&#27979;&#35797;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20960;&#20010;&#32423;&#21035;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the development of a model to detect user-level clinical depression based on a user's temporal social media posts. Our model uses a Depression Symptoms Detection (DSD) classifier, which is trained on the largest existing samples of clinician annotated tweets for clinical depression symptoms. We subsequently use our DSD model to extract clinically relevant features, e.g., depression scores and their consequent temporal patterns, as well as user posting activity patterns, e.g., quantifying their ``no activity'' or ``silence.'' Furthermore, to evaluate the efficacy of these extracted features, we create three kinds of datasets including a test dataset, from two existing well-known benchmark datasets for user-level depression detection. We then provide accuracy measures based on single features, baseline features and feature ablation tests, at several different levels of temporal granularity. The relevant data distributions and clinical depression detection related settings can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;Knowledge-in-Context&#12301;&#30340;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#22806;&#37096;&#23384;&#20648;&#22120;&#24102;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#20197;&#24110;&#21161;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26368;&#26377;&#29992;&#30340;&#30693;&#35782;&#29255;&#27573;&#12290;</title><link>http://arxiv.org/abs/2210.16433</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#30693;&#35782;&#65306;&#38754;&#21521;&#30693;&#35782;&#20016;&#23500;&#30340;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-in-Context: Towards Knowledgeable Semi-Parametric Language Models. (arXiv:2210.16433v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#12300;Knowledge-in-Context&#12301;&#30340;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#36890;&#36807;&#22806;&#37096;&#23384;&#20648;&#22120;&#24102;&#20837;&#21508;&#31181;&#31867;&#22411;&#30340;&#30693;&#35782;&#20197;&#24110;&#21161;&#35299;&#20915;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#24182;&#19988;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#26368;&#26377;&#29992;&#30340;&#30693;&#35782;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#27169;&#22411;&#21442;&#25968;&#26469;&#23384;&#20648;&#22312;&#38646;/&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#35299;&#20915;&#22810;&#20010;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#26114;&#36149;&#30340;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#38590;&#20197;&#36866;&#24212;&#19981;&#26029;&#21457;&#23637;&#30340;&#19990;&#30028;&#30693;&#35782;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#21442;&#25968;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21517;&#20026; "Knowledge-in-Context"&#65288;KiC&#65289;&#65292;&#23427;&#36890;&#36807;&#19968;&#20010;&#30693;&#35782;&#20016;&#23500;&#30340;&#22806;&#37096;&#23384;&#20648;&#22120;&#36171;&#20104;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22806;&#37096;&#23384;&#20648;&#22120;&#21253;&#21547;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#65306;&#23454;&#20307;&#12289;&#35789;&#20856;&#12289;&#24120;&#35782;&#12289;&#20107;&#20214;&#12289;&#33050;&#26412;&#21644;&#22240;&#26524;&#30693;&#35782;&#12290;&#23545;&#20110;&#27599;&#20010;&#36755;&#20837;&#23454;&#20363;&#65292;KiC &#27169;&#22411;&#20250;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#19968;&#31181;&#30693;&#35782;&#31867;&#22411;&#24182;&#26816;&#32034;&#26368;&#26377;&#29992;&#30340;&#30693;&#35782;&#29255;&#27573;&#12290;&#36755;&#20837;&#23454;&#20363;&#36830;&#21516;&#20854;&#30693;&#35782;&#22686;&#24378;&#34987;&#39304;&#36865;&#21040;&#25991;&#26412;&#21040;&#25991;&#26412;&#27169;&#22411;&#65288;&#20363;&#22914; T5&#65289;&#20013;&#20197;&#29983;&#25104;&#36755;&#20986;&#31572;&#26696;&#65292;&#22312;&#25552;&#31034;&#21518;&#36755;&#20837;&#21644;&#36755;&#20986;&#37117;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#21576;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully-parametric language models generally require a huge number of model parameters to store the necessary knowledge for solving multiple natural language tasks in zero/few-shot settings. In addition, it is hard to adapt to the evolving world knowledge without the costly model re-training. In this paper, we develop a novel semi-parametric language model architecture, Knowledge-in-Context (KiC), which empowers a parametric text-to-text language model with a knowledge-rich external memory. Specifically, the external memory contains six different types of knowledge: entity, dictionary, commonsense, event, script, and causality knowledge. For each input instance, the KiC model adaptively selects a knowledge type and retrieves the most helpful pieces of knowledge. The input instance along with its knowledge augmentation is fed into a text-to-text model (e.g., T5) to generate the output answer, where both the input and the output are in natural language forms after prompting. Interestingly,
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#65292;&#24182;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#65292;&#36824;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10260</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#65292;&#24182;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#65292;&#36824;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20256;&#32479;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#23884;&#22871;&#22330;&#26223;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23884;&#22871;&#23454;&#20307;&#35782;&#21035;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#34987;&#36716;&#31227;&#24212;&#29992;&#20110;&#24212;&#23545;&#23454;&#20307;&#23884;&#22871;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#38382;&#39064;&#22312;&#20110;&#38656;&#35201;&#25163;&#21160;&#21019;&#24314;&#26597;&#35810;&#21521;&#37327;&#65292;&#26080;&#27861;&#36866;&#24212;&#19978;&#19979;&#25991;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#35758;&#22120;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#12290;&#28982;&#21518;&#65292;&#22238;&#24402;&#22120;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#27492;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition is a traditional task in natural language processing. In particular, nested entity recognition receives extensive attention for the widespread existence of the nesting scenario. The latest research migrates the well-established paradigm of set prediction in object detection to cope with entity nesting. However, the manual creation of query vectors, which fail to adapt to the rich semantic information in the context, limits these approaches. An end-to-end entity detection approach with proposer and regressor is presented in this paper to tackle the issues. First, the proposer utilizes the feature pyramid network to generate high-quality entity proposals. Then, the regressor refines the proposals for generating the final prediction. The model adopts encoder-only architecture and thus obtains the advantages of the richness of query semantics, high precision of entity localization, and easiness of model training. Moreover, we introduce the novel spatially modulated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;MAP&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.05335</link><description>&lt;p&gt;
MAP&#65306;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MAP: Multimodal Uncertainty-Aware Vision-Language Pre-training Model. (arXiv:2210.05335v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05335
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#32534;&#30721;&#22120;&#36827;&#34892;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;MAP&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#19979;&#28216;&#20219;&#21153;&#20013;&#22343;&#34920;&#29616;&#20248;&#24322;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#35821;&#20041;&#29702;&#35299;&#32463;&#24120;&#38656;&#35201;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65292;&#23548;&#33268;&#25152;&#33719;&#24471;&#30340;&#20449;&#24687;&#20542;&#21521;&#20110;&#28041;&#21450;&#22810;&#20010;&#30446;&#26631;&#12290;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#23545;&#25105;&#20204;&#30340;&#35299;&#37322;&#26469;&#35828;&#26159;&#26377;&#38382;&#39064;&#30340;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#21644;&#20869;&#37096;&#27169;&#24577;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#40092;&#26377;&#30740;&#31350;&#25506;&#35752;&#35813;&#19981;&#30830;&#23450;&#24615;&#30340;&#24314;&#27169;&#65292;&#29305;&#21035;&#26159;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#21644;&#22312;&#29305;&#23450;&#20219;&#21153;&#19979;&#28216;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24207;&#21015;&#32423;&#20132;&#20114;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#32534;&#30721;&#22120;&#65288;PDE&#65289;&#23558;&#25152;&#26377;&#27169;&#24577;&#30340;&#34920;&#31034;&#25237;&#24433;&#20026;&#27010;&#29575;&#20998;&#24067;&#12290;&#19982;&#29616;&#26377;&#30340;&#30830;&#23450;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#21487;&#20197;&#20256;&#36882;&#26356;&#20016;&#23500;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#20449;&#24687;&#21644;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#19982;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#36866;&#21512;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;:&#22522;&#20110;&#20998;&#24067;&#30340;&#35270;&#35273;&#35821;&#35328;&#23545;&#27604;&#65288;D-VLC&#65289;&#12289;&#22522;&#20110;&#20998;&#24067;&#30340;&#25513;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;D-MLM&#65289;&#21644;&#20998;&#24067;&#24335;&#22270;&#20687;&#26816;&#32034;&#65288;D-IR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#8212;&#8212;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;MAP&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#35270;&#35273;&#38382;&#31572;&#12289;&#22270;&#20687;&#23383;&#24149;&#21644;&#25351;&#31216;&#34920;&#36798;&#29702;&#35299;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MAP&#27604;&#20854;&#30830;&#23450;&#24615;&#23545;&#24212;&#29289;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal semantic understanding often has to deal with uncertainty, which means the obtained messages tend to refer to multiple targets. Such uncertainty is problematic for our interpretation, including inter- and intra-modal uncertainty. Little effort has studied the modeling of this uncertainty, particularly in pre-training on unlabeled datasets and fine-tuning in task-specific downstream datasets. In this paper, we project the representations of all modalities as probabilistic distributions via a Probability Distribution Encoder (PDE) by utilizing sequence-level interactions. Compared to the existing deterministic methods, such uncertainty modeling can convey richer multimodal semantic information and more complex relationships. Furthermore, we integrate uncertainty modeling with popular pre-training frameworks and propose suitable pre-training tasks: Distribution-based Vision-Language Contrastive learning (D-VLC), Distribution-based Masked Language Modeling (D-MLM), and Distribut
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#32422;&#26463;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#65292;&#21487;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2209.14627</link><description>&lt;p&gt;
&#12298;&#19968;&#31181;&#38024;&#23545;&#22810;&#26679;&#23545;&#35805;&#29983;&#25104;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#12299;
&lt;/p&gt;
&lt;p&gt;
An Equal-Size Hard EM Algorithm for Diverse Dialogue Generation. (arXiv:2209.14627v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#34913;&#32422;&#26463;&#30340;&#31561;&#22823;&#23567;&#30828;EM&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#65292;&#21487;&#22312;&#23567;&#22411;&#27169;&#22411;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19982;&#20154;&#31867;&#20114;&#21160;&#12290;&#23613;&#31649;&#20687;ChatGPT&#36825;&#26679;&#30340;&#36229;&#22823;&#22411;&#23545;&#35805;&#31995;&#32479;&#26368;&#36817;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20351;&#29992;&#20013;&#23567;&#22411;&#23545;&#35805;&#31995;&#32479;&#20173;&#28982;&#26159;&#24120;&#35265;&#30340;&#20570;&#27861;&#65292;&#22240;&#20026;&#23427;&#20204;&#26356;&#21152;&#36731;&#20415;&#26131;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36739;&#23567;&#30340;&#27169;&#22411;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#23545;&#35805;&#21709;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31561;&#22823;&#23567;&#30828;EM&#65288;EqHard-EM&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#35299;&#30721;&#22120;&#27169;&#22411;&#20197;&#23454;&#29616;&#22810;&#26679;&#30340;&#23545;&#35805;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20197;&#30828;&#26041;&#24335;&#23558;&#26679;&#26412;&#20998;&#37197;&#32473;&#35299;&#30721;&#22120;&#65292;&#24182;&#39069;&#22806;&#26045;&#21152;&#24179;&#34913;&#32422;&#26463;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#25152;&#26377;&#35299;&#30721;&#22120;&#37117;&#32463;&#36807;&#20805;&#20998;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;EqHard-EM&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22810;&#26679;&#21270;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain dialogue systems aim to interact with humans through natural language texts in an open-ended fashion. Despite the recent success of super large dialogue systems such as ChatGPT, using medium-to-small-sized dialogue systems remains the common practice as they are more lightweight and accessible; however, generating diverse dialogue responses is challenging, especially with smaller models. In this work, we propose an Equal-size Hard Expectation--Maximization (EqHard-EM) algorithm to train a multi-decoder model for diverse dialogue generation. Our algorithm assigns a sample to a decoder in a hard manner and additionally imposes an equal-assignment constraint to ensure that all decoders are well-trained. We provide detailed theoretical analysis to justify our approach. Further, experiments on two large-scale open-domain dialogue datasets verify that our EqHard-EM algorithm generates high-quality diverse responses.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#21333;&#20010;NMT&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21516;&#33521;&#35821;&#20043;&#38388;&#30340;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#21452;&#21521;&#22238;&#35793;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.02821</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#22238;&#35793;&#23454;&#29616;&#30340;&#22810;&#35821;&#35328;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multilingual Bidirectional Unsupervised Translation Through Multilingual Finetuning and Back-Translation. (arXiv:2209.02821v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02821
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#21333;&#20010;NMT&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#21516;&#33521;&#35821;&#20043;&#38388;&#30340;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22810;&#35821;&#35328;&#24494;&#35843;&#21644;&#21452;&#21521;&#22238;&#35793;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21333;&#20010;NMT&#27169;&#22411;&#20197;&#23558;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#20174;&#21644;&#21040;&#33521;&#35821;&#36827;&#34892;&#32763;&#35793;&#12290;&#23545;&#20110;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#23558;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21021;&#22987;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;XLM-R&#21644;RoBERTa&#26435;&#37325;&#65292;&#28982;&#21518;&#23545;40&#31181;&#35821;&#35328;&#21040;&#33521;&#35821;&#30340;&#24182;&#34892;&#25968;&#25454;&#36827;&#34892;&#22810;&#35821;&#35328;&#24494;&#35843;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#30340;&#38646;-shot&#32763;&#35793;&#12290;&#23545;&#20110;&#31532;&#20108;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#25512;&#24191;&#33021;&#21147;&#20174;&#21333;&#35821;&#35328;&#25968;&#25454;&#38598;&#29983;&#25104;&#21512;&#25104;&#24182;&#34892;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#36830;&#32493;&#30340;&#21452;&#21521;&#22238;&#35793;&#36718;&#27425;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#26041;&#27861;&#20026;EcXTra&#65288;{E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27010;&#24565;&#19978;&#24456;&#31616;&#21333;&#65292;&#21482;&#20351;&#29992;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#30446;&#26631;&#65292;&#24182;&#19988;&#26159;&#25968;&#25454;&#39537;&#21160;&#30340;&#65292;&#20381;&#27425;&#21033;&#29992;&#36741;&#21161;&#24182;&#34892;&#25968;&#25454;&#21644;&#21333;&#35821;&#35328;&#25968;&#25454;&#12290;&#25105;&#20204;&#35780;&#20272;&#25105;&#20204;&#22312;7&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#26080;&#30417;&#30563;NMT&#32467;&#26524;&#65292;&#21457;&#29616;&#27599;&#19968;&#36718;&#22238;&#35793;&#35757;&#32451;&#37117;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#21452;&#21521;&#26080;&#30417;&#30563;&#32763;&#35793;&#36136;&#37327;&#65292;&#30456;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;&#22810;&#36798;10&#20010;BLEU&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a two-stage approach for training a single NMT model to translate unseen languages both to and from English. For the first stage, we initialize an encoder-decoder model to pretrained XLM-R and RoBERTa weights, then perform multilingual fine-tuning on parallel data in 40 languages to English. We find this model can generalize to zero-shot translations on unseen languages. For the second stage, we leverage this generalization ability to generate synthetic parallel data from monolingual datasets, then train with successive rounds of bidirectional back-translation.  We term our approach EcXTra ({E}nglish-{c}entric Crosslingual ({X}) {Tra}nsfer). Our approach is conceptually simple, only using a standard cross-entropy objective throughout, and also is data-driven, sequentially leveraging auxiliary parallel data and monolingual data. We evaluate our unsupervised NMT results on 7 low-resource languages, and find that each round of back-translation training further refines bidirecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;Transformer&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36793;&#30028;&#31867;&#22411;&#25552;&#31034;&#65292;&#20197;&#21450;&#21333;&#35789;&#32423;&#21035;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#20154;&#24615;&#21270;&#30340;&#23383;&#24149;&#65292;&#24182;&#22312;GEBC&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.03038</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#27969;Transformer&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dual-Stream Transformer for Generic Event Boundary Captioning. (arXiv:2207.03038v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03038
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;Transformer&#30340;&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36793;&#30028;&#31867;&#22411;&#25552;&#31034;&#65292;&#20197;&#21450;&#21333;&#35789;&#32423;&#21035;&#30340;&#38598;&#25104;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#29983;&#25104;&#26356;&#20154;&#24615;&#21270;&#30340;&#23383;&#24149;&#65292;&#24182;&#22312;GEBC&#27979;&#35797;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#21442;&#21152;CVPR2022&#36890;&#29992;&#20107;&#20214;&#36793;&#30028;&#23383;&#24149;&#29983;&#25104;&#27604;&#36187;&#30340;&#20248;&#32988;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#20219;&#21153;&#35201;&#27714;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#22312;&#32473;&#23450;&#35270;&#39057;&#36793;&#30028;&#21608;&#22260;&#33021;&#22815;&#29702;&#35299;&#30636;&#26102;&#29366;&#24577;&#21464;&#21270;&#65292;&#20351;&#20854;&#27604;&#20256;&#32479;&#35270;&#39057;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#27969;Transformer&#65292;&#25913;&#36827;&#20102;&#35270;&#39057;&#20869;&#23481;&#32534;&#30721;&#21644;&#23383;&#24149;&#29983;&#25104;&#20004;&#20010;&#26041;&#38754;&#65306;(1)&#25105;&#20204;&#21033;&#29992;&#19977;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#20174;&#19981;&#21516;&#31890;&#24230;&#25552;&#21462;&#35270;&#39057;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36793;&#30028;&#31867;&#22411;&#20316;&#20026;&#25552;&#31034;&#65292;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#23383;&#24149;&#12290;(2)&#25105;&#20204;&#29305;&#21035;&#35774;&#35745;&#19968;&#20010;&#31216;&#20026;&#21452;&#27969;Transformer&#30340;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21306;&#20998;&#24615;&#36793;&#30028;&#23383;&#24149;&#34920;&#31034;&#12290;(3)&#20026;&#20102;&#29983;&#25104;&#19982;&#20869;&#23481;&#30456;&#20851;&#19988;&#26356;&#21152;&#20154;&#24615;&#21270;&#30340;&#23383;&#24149;&#65292;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#21333;&#35789;&#32423;&#21035;&#30340;&#38598;&#25104;&#31574;&#30053;&#26469;&#25913;&#21892;&#25551;&#36848;&#36136;&#37327;&#12290;&#22312;GEBC&#27979;&#35797;&#38598;&#19978;&#21069;&#26223;&#19981;&#20439;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our champion solution for the CVPR2022 Generic Event Boundary Captioning (GEBC) competition. GEBC requires the captioning model to have a comprehension of instantaneous status changes around the given video boundary, which makes it much more challenging than conventional video captioning task. In this paper, a Dual-Stream Transformer with improvements on both video content encoding and captions generation is proposed: (1) We utilize three pre-trained models to extract the video features from different granularities. Moreover, we exploit the types of boundary as hints to help the model generate captions. (2) We particularly design an model, termed as Dual-Stream Transformer, to learn discriminative representations for boundary captioning. (3) Towards generating content-relevant and human-like captions, we improve the description quality by designing a word-level ensemble strategy. The promising results on the GEBC test split demonstrate the efficacy of our proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#25226;&#20351;&#29992;&#19981;&#21516;&#20070;&#20889;&#31995;&#32479;&#30340;&#30456;&#36817;&#35821;&#35328;&#38899;&#35793;&#25104;&#21516;&#19968;&#31181;&#20070;&#20889;&#31995;&#32479;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21319;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38899;&#35793;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#20250;&#23545;&#36164;&#28304;&#30456;&#23545;&#36739;&#39640;&#30340;&#35821;&#35328;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2201.12501</link><description>&lt;p&gt;
&#32763;&#35793;&#65306;&#27721;&#35821;&#25340;&#38899;&#26159;&#21542;&#26377;&#21161;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#24314;&#27169;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Transliteration Help Multilingual Language Modeling?. (arXiv:2201.12501v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.12501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#25226;&#20351;&#29992;&#19981;&#21516;&#20070;&#20889;&#31995;&#32479;&#30340;&#30456;&#36817;&#35821;&#35328;&#38899;&#35793;&#25104;&#21516;&#19968;&#31181;&#20070;&#20889;&#31995;&#32479;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#30340;&#25552;&#21319;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38899;&#35793;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#20250;&#23545;&#36164;&#28304;&#30456;&#23545;&#36739;&#39640;&#30340;&#35821;&#35328;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22823;&#37096;&#20998;&#35821;&#35328;&#32570;&#20047;&#22823;&#35268;&#27169;&#30340;&#20195;&#34920;&#24615;&#35821;&#26009;&#24211;&#65292;&#23545;&#20110;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26469;&#35828;&#65292;&#20174;&#29616;&#26377;&#30340;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#19981;&#21516;&#35821;&#35328;&#30340;&#25991;&#26412;&#34920;&#29616;&#24418;&#24335;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;MLLM&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#30456;&#36817;&#30340;&#35821;&#35328;&#20043;&#38388;&#35789;&#27719;&#37325;&#21472;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#25226;&#20351;&#29992;&#19981;&#21516;&#20070;&#20889;&#31995;&#32479;&#30340;&#30456;&#36817;&#30340;&#35821;&#35328;&#38899;&#35793;&#25104;&#21516;&#19968;&#31181;&#20070;&#20889;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;MLLM&#30340;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39044;&#35757;&#32451;&#20004;&#20010;ALBERT&#27169;&#22411;&#65292;&#20197;&#23454;&#35777;&#30340;&#26041;&#24335;&#27979;&#37327;&#38899;&#35793;&#23545;MLLM&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21360;&#24230;&#35821;-&#38597;&#21033;&#23433;&#35821;&#31995;&#65292;&#35813;&#31995;&#22312;&#19990;&#30028;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20070;&#20889;&#31995;&#32479;&#22810;&#26679;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22312;IndicGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#26364;&#65293;&#24800;&#29305;&#23612;U&#26816;&#39564;&#65292;&#20197;&#20005;&#26684;&#39564;&#35777;&#38899;&#35793;&#30340;&#25928;&#26524;&#26159;&#21542;&#26174;&#33879;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#38899;&#35793;&#26377;&#21033;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#32780;&#19981;&#20250;&#23545;&#36164;&#28304;&#30456;&#23545;&#36739;&#39640;&#30340;&#35821;&#35328;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
As there is a scarcity of large representative corpora for most languages, it is important for Multilingual Language Models (MLLM) to extract the most out of existing corpora. In this regard, script diversity presents a challenge to MLLMs by reducing lexical overlap among closely related languages. Therefore, transliterating closely related languages that use different writing scripts to a common script may improve the downstream task performance of MLLMs. In this paper, we pretrain two ALBERT models to empirically measure the effect of transliteration on MLLMs. We specifically focus on the Indo-Aryan language family, which has the highest script diversity in the world. Afterward, we evaluate our models on the IndicGLUE benchmark. We perform Mann-Whitney U test to rigorously verify whether the effect of transliteration is significant or not. We find that transliteration benefits the low-resource languages without negatively affecting the comparatively high-resource languages. We also m
&lt;/p&gt;</description></item></channel></rss>