<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>StoryBench&#26159;&#19968;&#20010;&#26032;&#30340;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#23427;&#21253;&#25324;&#21160;&#20316;&#25191;&#34892;&#65292;&#25925;&#20107;&#24310;&#32493;&#21644;&#25925;&#20107;&#29983;&#25104;&#19977;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#23567;&#32780;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2308.11606</link><description>&lt;p&gt;
StoryBench: &#19968;&#20010;&#22810;&#26041;&#38754;&#30340;&#36830;&#32493;&#25925;&#20107;&#21487;&#35270;&#21270;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. (arXiv:2308.11606v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11606
&lt;/p&gt;
&lt;p&gt;
StoryBench&#26159;&#19968;&#20010;&#26032;&#30340;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#23427;&#21253;&#25324;&#21160;&#20316;&#25191;&#34892;&#65292;&#25925;&#20107;&#24310;&#32493;&#21644;&#25925;&#20107;&#29983;&#25104;&#19977;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#23567;&#32780;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#35270;&#39057;&#25925;&#20107;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#38500;&#20102;&#20855;&#26377;&#39640;&#36136;&#37327;&#30340;&#35270;&#35273;&#25928;&#26524;&#22806;&#65292;&#35270;&#39057;&#36824;&#38656;&#35201;&#22312;&#25972;&#20010;&#24103;&#20013;&#20445;&#25345;&#19982;&#25991;&#26412;&#25552;&#31034;&#24207;&#21015;&#30340;&#19968;&#33268;&#12290;&#21019;&#24314;&#35270;&#39057;&#29983;&#25104;&#30340;&#22522;&#20934;&#38656;&#35201;&#22312;&#26102;&#38388;&#19978;&#23545;&#25968;&#25454;&#36827;&#34892;&#27880;&#37322;&#65292;&#36825;&#19982;&#35270;&#39057;&#25968;&#25454;&#38598;&#20013;&#32463;&#24120;&#20351;&#29992;&#30340;&#21333;&#20010;&#26631;&#39064;&#24418;&#25104;&#23545;&#27604;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19977;&#20010;&#29616;&#26377;&#25968;&#25454;&#38598;&#19978;&#30340;&#20840;&#38754;&#30340;&#20154;&#31867;&#27880;&#37322;&#65292;&#24182;&#25512;&#20986;&#20102;StoryBench&#65306;&#19968;&#20010;&#26032;&#30340;&#65292;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22810;&#20219;&#21153;&#22522;&#20934;&#65292;&#21487;&#21487;&#38752;&#22320;&#35780;&#20272;&#21363;&#23558;&#21457;&#24067;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#19977;&#20010;&#38590;&#24230;&#36880;&#28176;&#22686;&#21152;&#30340;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#65306;&#21160;&#20316;&#25191;&#34892;&#65292;&#22312;&#20174;&#19968;&#20010;&#26465;&#20214;&#35270;&#39057;&#24320;&#22987;&#29983;&#25104;&#19979;&#19968;&#20010;&#21160;&#20316;&#65307;&#25925;&#20107;&#24310;&#32493;&#65292;&#22312;&#20174;&#19968;&#20010;&#26465;&#20214;&#35270;&#39057;&#24320;&#22987;&#25191;&#34892;&#19968;&#31995;&#21015;&#21160;&#20316;&#65307;&#25925;&#20107;&#29983;&#25104;&#65292;&#20165;&#20174;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#19968;&#20010;&#35270;&#39057;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#23567;&#32780;&#24378;&#22823;&#30340;&#25991;&#26412;&#21040;&#35270;&#39057;&#22522;&#32447;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefit
&lt;/p&gt;</description></item><item><title>Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;</title><link>http://arxiv.org/abs/2308.11601</link><description>&lt;p&gt;
Tryage: &#23454;&#26102;&#26234;&#33021;&#36335;&#30001;&#29992;&#25143;&#25552;&#31034;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tryage: Real-time, intelligent Routing of User Prompts to Large Language Model. (arXiv:2308.11601v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11601
&lt;/p&gt;
&lt;p&gt;
Tryage&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;&#65292;&#33021;&#22815;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#20197;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26550;&#26500;&#21644;&#33258;&#27880;&#24847;&#26426;&#21046;&#30340;&#24341;&#20837;&#23548;&#33268;&#20102;&#22312;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#21644;&#25968;&#25454;&#39046;&#22495;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#12290;&#22312;Hugging Face&#29983;&#24577;&#31995;&#32479;&#20013;&#26377;&#36229;&#36807;200,000&#20010;&#27169;&#22411;&#65292;&#29992;&#25143;&#22312;&#36873;&#25321;&#21644;&#20248;&#21270;&#27169;&#22411;&#20197;&#36866;&#24212;&#22810;&#26041;&#38754;&#30340;&#24037;&#20316;&#27969;&#31243;&#21644;&#25968;&#25454;&#39046;&#22495;&#30340;&#21516;&#26102;&#65292;&#36824;&#35201;&#35299;&#20915;&#35745;&#31639;&#12289;&#23433;&#20840;&#21644;&#26102;&#25928;&#24615;&#31561;&#38382;&#39064;&#12290;&#36843;&#20999;&#38656;&#35201;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#26469;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#21644;&#23450;&#21046;&#21270;&#30340;&#36127;&#25285;&#65292;&#24182;&#37322;&#25918;&#24222;&#22823;&#30340;&#26032;&#20852;&#27169;&#22411;&#24211;&#30340;&#24040;&#22823;&#23041;&#21147;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#36335;&#30001;&#31995;&#32479;Tryage&#65292;&#23427;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36335;&#30001;&#22120;&#26681;&#25454;&#23545;&#20010;&#20307;&#36755;&#20837;&#25552;&#31034;&#30340;&#20998;&#26512;&#65292;&#20174;&#27169;&#22411;&#24211;&#20013;&#36873;&#25321;&#26368;&#20339;&#30340;&#19987;&#23478;&#27169;&#22411;&#12290;&#21463;&#22823;&#33041;&#20013;&#30340;&#19992;&#33041;&#36335;&#30001;&#22120;&#21551;&#21457;&#65292;Tryage&#37319;&#29992;&#24863;&#30693;&#36335;&#30001;&#22120;&#26469;&#39044;&#27979;&#19979;&#28216;&#27169;&#22411;&#22312;&#25552;&#31034;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#26681;&#25454;&#30446;&#26631;&#20570;&#20986;&#36335;&#30001;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of the transformer architecture and the self-attention mechanism has led to an explosive production of language models trained on specific downstream tasks and data domains. With over 200, 000 models in the Hugging Face ecosystem, users grapple with selecting and optimizing models to suit multifaceted workflows and data domains while addressing computational, security, and recency concerns. There is an urgent need for machine learning frameworks that can eliminate the burden of model selection and customization and unleash the incredible power of the vast emerging model library for end users. Here, we propose a context-aware routing system, Tryage, that leverages a language model router for optimal selection of expert models from a model library based on analysis of individual input prompts. Inspired by the thalamic router in the brain, Tryage employs a perceptive router to predict down-stream model performance on prompts and, then, makes a routing decision using an ob
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SeamlessM4T&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#65292;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11596</link><description>&lt;p&gt;
SeamlessM4T-&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
SeamlessM4T-Massively Multilingual &amp; Multimodal Machine Translation. (arXiv:2308.11596v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SeamlessM4T&#65292;&#36825;&#26159;&#19968;&#20010;&#25903;&#25345;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#26426;&#22120;&#32763;&#35793;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#35821;&#38899;&#25968;&#25454;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#21644;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#65292;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#19968;&#31181;&#31867;&#20284;&#20110;&#24052;&#21035;&#40060;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#24110;&#21161;&#20010;&#20154;&#22312;&#20219;&#24847;&#20004;&#31181;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#35821;&#38899;&#32763;&#35793;&#65292;&#38656;&#35201;&#20184;&#20986;&#20160;&#20040;&#26679;&#30340;&#21162;&#21147;&#65311;&#34429;&#28982;&#26368;&#36817;&#22312;&#22522;&#20110;&#25991;&#26412;&#30340;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20351;&#26426;&#22120;&#32763;&#35793;&#30340;&#35206;&#30422;&#33539;&#22260;&#36229;&#36807;&#20102;200&#31181;&#35821;&#35328;&#65292;&#20294;&#32479;&#19968;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#27169;&#22411;&#36824;&#27809;&#26377;&#21462;&#24471;&#31867;&#20284;&#30340;&#36827;&#23637;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#20256;&#32479;&#30340;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#31995;&#32479;&#20381;&#36182;&#20110;&#28176;&#36827;&#24335;&#30340;&#32423;&#32852;&#31995;&#32479;&#36827;&#34892;&#32763;&#35793;&#65292;&#20351;&#39640;&#24615;&#33021;&#30340;&#32479;&#19968;&#31995;&#32479;&#38590;&#20197;&#23454;&#29616;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SeamlessM4T&#65292;&#19968;&#31181;&#25903;&#25345;&#35821;&#38899;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#35821;&#38899;&#21040;&#25991;&#26412;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#35821;&#38899;&#32763;&#35793;&#12289;&#25991;&#26412;&#21040;&#25991;&#26412;&#32763;&#35793;&#20197;&#21450;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#25903;&#25345;&#22810;&#36798;100&#31181;&#35821;&#35328;&#12290;&#20026;&#20102;&#26500;&#24314;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;100&#19975;&#23567;&#26102;&#30340;&#24320;&#25918;&#24335;&#35821;&#38899;&#38899;&#39057;&#25968;&#25454;&#65292;&#20351;&#29992;&#20102;w2v-BERT 2.0&#26469;&#23398;&#20064;&#33258;&#30417;&#30563;&#30340;&#35821;&#38899;&#34920;&#31034;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#33258;&#21160;&#23545;&#40784;&#35821;&#38899;&#32763;&#35793;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
What does it take to create the Babel Fish, a tool that can help individuals translate speech between any two languages? While recent breakthroughs in text-based models have pushed machine translation coverage beyond 200 languages, unified speech-to-speech translation models have yet to achieve similar strides. More specifically, conventional speech-to-speech translation systems rely on cascaded systems that perform translation progressively, putting high-performing unified systems out of reach. To address these gaps, we introduce SeamlessM4T, a single model that supports speech-to-speech translation, speech-to-text translation, text-to-speech translation, text-to-text translation, and automatic speech recognition for up to 100 languages. To build this, we used 1 million hours of open speech audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with h
&lt;/p&gt;</description></item><item><title>UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.11592</link><description>&lt;p&gt;
UniDoc: &#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#29992;&#20110;&#21516;&#26102;&#36827;&#34892;&#25991;&#26412;&#26816;&#27979;&#12289;&#35782;&#21035;&#12289;&#23450;&#20301;&#21644;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding. (arXiv:2308.11592v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11592
&lt;/p&gt;
&lt;p&gt;
UniDoc&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#25552;&#39640;&#27599;&#20010;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26102;&#20195;&#65292;&#22810;&#27169;&#24577;&#29702;&#35299;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39640;&#32423;&#31639;&#27861;&#21463;&#38480;&#20110;&#26377;&#25928;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#22266;&#26377;&#30340;&#24040;&#22823;&#34920;&#31034;&#33021;&#21147;&#21644;&#20016;&#23500;&#30340;&#19990;&#30028;&#30693;&#35782;&#65292;&#24182;&#19988;&#22312;&#25991;&#26412;&#20016;&#23500;&#22330;&#26223;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#36830;&#25509;&#23578;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;UniDoc&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#20855;&#22791;&#29616;&#26377;&#26041;&#27861;&#25152;&#32570;&#20047;&#30340;&#25991;&#26412;&#26816;&#27979;&#21644;&#35782;&#21035;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;UniDoc&#21033;&#29992;&#20219;&#21153;&#20043;&#38388;&#30340;&#26377;&#30410;&#20132;&#20114;&#26469;&#25552;&#39640;&#27599;&#20010;&#21333;&#29420;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#23454;&#29616;UniDoc&#65292;&#25105;&#20204;&#23545;&#36129;&#29486;&#30340;&#22823;&#35268;&#27169;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#25351;&#23548;&#35843;&#20248;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniDoc&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of Large Language Models (LLMs), tremendous strides have been made in the field of multimodal understanding. However, existing advanced algorithms are limited to effectively utilizing the immense representation capabilities and rich world knowledge inherent to these large pre-trained models, and the beneficial connections among tasks within the context of text-rich scenarios have not been sufficiently explored. In this work, we introduce UniDoc, a novel multimodal model equipped with text detection and recognition capabilities, which are deficient in existing approaches. Moreover, UniDoc capitalizes on the beneficial interactions among tasks to enhance the performance of each individual task. To implement UniDoc, we perform unified multimodal instruct tuning on the contributed large-scale instruction following datasets. Quantitative and qualitative experimental results show that UniDoc sets state-of-the-art scores across multiple challenging benchmarks. To the best of our kn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;XLSR-53&#27169;&#22411;&#24320;&#21457;&#20102;&#21360;&#23612;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#19982;&#20351;&#29992;Common Voice&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#35789;&#38169;&#35823;&#29575;&#21487;&#20197;&#38477;&#20302;&#32422;8%&#12290;</title><link>http://arxiv.org/abs/2308.11589</link><description>&lt;p&gt;
&#21360;&#23612;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#19982;XLSR-53&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Indonesian Automatic Speech Recognition with XLSR-53. (arXiv:2308.11589v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;XLSR-53&#27169;&#22411;&#24320;&#21457;&#20102;&#21360;&#23612;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#65292;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#37327;&#20197;&#23454;&#29616;&#31454;&#20105;&#24615;&#30340;&#35789;&#38169;&#35823;&#29575;&#65292;&#32467;&#26524;&#26174;&#31034;&#27169;&#22411;&#19982;&#20351;&#29992;Common Voice&#25968;&#25454;&#38598;&#30340;&#30456;&#20284;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#35789;&#38169;&#35823;&#29575;&#21487;&#20197;&#38477;&#20302;&#32422;8%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#21033;&#29992;XLSR-53&#39044;&#35757;&#32451;&#27169;&#22411;&#24320;&#21457;&#21360;&#23612;&#35821;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#12290;XLSR&#20195;&#34920;&#36328;&#35821;&#35328;&#35821;&#38899;&#34920;&#31034;&#12290;&#20351;&#29992;XLSR-53&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#30446;&#30340;&#26159;&#26174;&#33879;&#20943;&#23569;&#22312;&#38750;&#33521;&#35821;&#35821;&#35328;&#19979;&#33719;&#24471;&#31454;&#20105;&#24615;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#20849;&#35745;24&#23567;&#26102;18&#20998;&#38047;1&#31186;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;&#65288;1&#65289;14&#23567;&#26102;31&#20998;&#38047;&#30340;TITML-IDN&#25968;&#25454;&#65307;&#65288;2&#65289;3&#23567;&#26102;33&#20998;&#38047;&#30340;Magic Data&#25968;&#25454;&#65307;&#20197;&#21450;&#65288;3&#65289;6&#23567;&#26102;14&#20998;&#38047;1&#31186;&#30340;Common Voice&#25968;&#25454;&#12290;&#22312;WER&#20026;20%&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#30740;&#31350;&#26500;&#24314;&#30340;&#27169;&#22411;&#33021;&#22815;&#19982;&#20351;&#29992;Common Voice&#25968;&#25454;&#38598;&#20998;&#21106;&#27979;&#35797;&#30340;&#31867;&#20284;&#27169;&#22411;&#31454;&#20105;&#12290;&#36890;&#36807;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;WER&#21487;&#20197;&#38477;&#20302;&#32422;8%&#65292;&#20351;&#20854;&#20174;20%&#38477;&#33267;12%&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#25104;&#21151;&#23436;&#21892;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#65292;&#20026;&#21019;&#36896;&#19968;&#20010;&#26356;&#22909;&#30340;&#21360;&#23612;&#35821;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#25552;&#20379;&#20102;&#26356;&#23569;&#30340;&#25968;&#25454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on the development of Indonesian Automatic Speech Recognition (ASR) using the XLSR-53 pre-trained model, the XLSR stands for cross-lingual speech representations. The use of this XLSR-53 pre-trained model is to significantly reduce the amount of training data in non-English languages required to achieve a competitive Word Error Rate (WER). The total amount of data used in this study is 24 hours, 18 minutes, and 1 second: (1) TITML-IDN 14 hours and 31 minutes; (2) Magic Data 3 hours and 33 minutes; and (3) Common Voice 6 hours, 14 minutes, and 1 second. With a WER of 20%, the model built in this study can compete with similar models using the Common Voice dataset split test. WER can be decreased by around 8% using a language model, resulted in WER from 20% to 12%. Thus, the results of this study have succeeded in perfecting previous research in contributing to the creation of a better Indonesian ASR with a smaller amount of data.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11585</link><description>&lt;p&gt;
&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20167;&#24680;&#36855;&#22240;&#20026;&#20363;&#65288;arXiv:2308.11585v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#20852;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#20013;&#30340;&#35821;&#20041;&#24847;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22240;&#26524;&#20998;&#26512;&#20391;&#37325;&#20110;&#23450;&#20041;&#35821;&#20041;&#21450;&#20854;&#37327;&#21270;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26680;&#24515;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#21327;&#21516;&#36825;&#20123;&#26041;&#27861;&#65292;&#25506;&#32034;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#22914;&#20309;&#38416;&#26126;&#20854;&#22240;&#26524;&#25928;&#24212;&#24050;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20132;&#21449;&#24615;--&#20010;&#20307;&#30340;&#22810;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#22240;&#32032;&#30340;&#32452;&#21512;&#24433;&#21709;--&#21487;&#20197;&#20197;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#24418;&#24335;&#36827;&#34892;&#32467;&#26500;&#21270;&#12290;&#26368;&#21021;&#65292;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#38382;&#39064;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;ATE&#26469;&#25551;&#36848;&#65292;&#20511;&#21161;&#20132;&#21449;&#24615;&#21407;&#21017;&#65292;&#20197;&#21450;&#22522;&#20110;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the explosive growth of machine learning (ML) usage, particularly within the context of emerging Large Language Models (LLMs), comprehending the semantic significance rooted in their internal workings is crucial. While causal analyses focus on defining semantics and its quantification, the gradient-based approach is central to explainable AI (XAI), tackling the interpretation of the black box. By synergizing these approaches, the exploration of how a model's internal mechanisms illuminate its causal effect has become integral for evidence-based decision-making. A parallel line of research has revealed that intersectionality - the combinatory impact of multiple demographics of an individual - can be structured in the form of an Averaged Treatment Effect (ATE). Initially, this study illustrates that the hateful memes detection problem can be formulated as an ATE, assisted by the principles of intersectionality, and that a modality-wise summarization of gradient-based atten
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21512;&#25104;&#20154;&#31867;&#35265;&#35299;&#19982;&#35745;&#31639;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExTES&#30340;&#21487;&#25193;&#23637;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#37096;&#32626;&#20102;&#39640;&#32423;&#35843;&#20248;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#25903;&#25345;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11584</link><description>&lt;p&gt;
&#22312;LLMs&#26102;&#20195;&#26500;&#24314;&#24773;&#24863;&#25903;&#25345;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Building Emotional Support Chatbots in the Era of LLMs. (arXiv:2308.11584v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#21512;&#25104;&#20154;&#31867;&#35265;&#35299;&#19982;&#35745;&#31639;&#33021;&#21147;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#26368;&#32456;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ExTES&#30340;&#21487;&#25193;&#23637;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#24182;&#37096;&#32626;&#20102;&#39640;&#32423;&#35843;&#20248;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#24773;&#24863;&#25903;&#25345;&#32842;&#22825;&#26426;&#22120;&#20154;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#24773;&#24863;&#25903;&#25345;&#38598;&#25104;&#21040;&#21508;&#31181;&#23545;&#35805;&#22330;&#26223;&#20013;&#20855;&#26377;&#28145;&#36828;&#30340;&#31038;&#20250;&#25928;&#30410;&#65292;&#22914;&#31038;&#20132;&#20114;&#21160;&#12289;&#24515;&#29702;&#20581;&#24247;&#21672;&#35810;&#21644;&#23458;&#25143;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#23384;&#22312;&#19968;&#20123;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#26377;&#38480;&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#32570;&#20047;&#34987;&#24191;&#27867;&#25509;&#21463;&#30340;&#27169;&#22411;&#35757;&#32451;&#33539;&#20363;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#65292;&#35797;&#22270;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#20154;&#31867;&#35265;&#35299;&#19982;LLMs&#30340;&#35745;&#31639;&#33021;&#21147;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#31934;&#24515;&#35774;&#35745;&#30340;&#28085;&#30422;&#22810;&#31181;&#22330;&#26223;&#30340;&#23545;&#35805;&#38598;&#21512;&#20316;&#20026;&#29983;&#25104;&#31181;&#23376;&#24320;&#22987;&#12290;&#36890;&#36807;&#21033;&#29992;ChatGPT&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#28508;&#21147;&#65292;&#25105;&#20204;&#36882;&#24402;&#22320;&#29983;&#25104;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#24773;&#24863;&#25903;&#25345;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;ExTES&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23545;LLaMA&#27169;&#22411;&#36827;&#34892;&#20102;&#39640;&#32423;&#35843;&#20248;&#25216;&#26415;&#30340;&#37096;&#32626;&#65292;&#26816;&#39564;&#20102;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of emotional support into various conversational scenarios presents profound societal benefits, such as social interactions, mental health counseling, and customer service. However, there are unsolved challenges that hinder real-world applications in this field, including limited data availability and the absence of well-accepted model training paradigms. This work endeavors to navigate these challenges by harnessing the capabilities of Large Language Models (LLMs). We introduce an innovative methodology that synthesizes human insights with the computational prowess of LLMs to curate an extensive emotional support dialogue dataset. Our approach is initiated with a meticulously designed set of dialogues spanning diverse scenarios as generative seeds. By utilizing the in-context learning potential of ChatGPT, we recursively generate an ExTensible Emotional Support dialogue dataset, named ExTES. Following this, we deploy advanced tuning techniques on the LLaMA model, exami
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#34920;&#29616;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20934;&#30830;&#24230;&#31561;&#12290;LLMs&#30340;&#20986;&#29616;&#20026;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#24102;&#26469;&#20102;&#26032;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#12290;</title><link>http://arxiv.org/abs/2308.11578</link><description>&lt;p&gt;
&#25913;&#21464;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#26041;&#24335;:&#36890;&#29992;&#22823;&#22411;&#27169;&#22411;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Refashioning Emotion Recognition Modelling: The Advent of Generalised Large Models. (arXiv:2308.11578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#21512;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#24773;&#32490;&#35782;&#21035;&#20013;&#34920;&#29616;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20934;&#30830;&#24230;&#31561;&#12290;LLMs&#30340;&#20986;&#29616;&#20026;&#24773;&#32490;&#35782;&#21035;&#24314;&#27169;&#24102;&#26469;&#20102;&#26032;&#30340;&#28508;&#21147;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#32490;&#35782;&#21035;&#25110;&#24773;&#24863;&#35745;&#31639;&#30340;&#35806;&#29983;&#20043;&#21518;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#24212;&#29992;&#65292;&#23427;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#37324;&#65292;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#36880;&#28176;&#20174;&#32479;&#35745;&#27973;&#23618;&#27169;&#22411;&#36801;&#31227;&#21040;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#27169;&#22411;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;&#24773;&#32490;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#22987;&#32456;&#21462;&#24471;&#26368;&#20339;&#32467;&#26524;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#27169;&#22411;&#19968;&#30452;&#34987;&#35270;&#20026;&#24773;&#32490;&#35782;&#21035;&#30340;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#22914;ChatGPT&#65292;&#30001;&#20110;&#23427;&#20204;&#20855;&#22791;&#30340;&#38646;/&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#36830;&#36143;&#24605;&#32500;&#31561;&#33021;&#21147;&#65292;&#22312;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#24341;&#36215;&#20102;&#24040;&#22823;&#30340;&#24778;&#35766;&#65292;&#32780;&#36825;&#20123;&#33021;&#21147;&#22312;&#20197;&#21069;&#30340;&#28145;&#24230;&#27169;&#22411;&#20013;&#20174;&#26410;&#20986;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#35843;&#26597;&#20102;LLMs&#22312;&#24773;&#32490;&#35782;&#21035;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21253;&#25324;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#23569;&#26679;&#26412;&#23398;&#20064;&#12289;&#20934;&#30830;&#24230;&#31561;&#21508;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
After the inception of emotion recognition or affective computing, it has increasingly become an active research topic due to its broad applications. Over the past couple of decades, emotion recognition models have gradually migrated from statistically shallow models to neural network-based deep models, which can significantly boost the performance of emotion recognition models and consistently achieve the best results on different benchmarks. Therefore, in recent years, deep models have always been considered the first option for emotion recognition. However, the debut of large language models (LLMs), such as ChatGPT, has remarkably astonished the world due to their emerged capabilities of zero/few-shot learning, in-context learning, chain-of-thought, and others that are never shown in previous deep models. In the present paper, we comprehensively investigate how the LLMs perform in emotion recognition in terms of diverse aspects, including in-context learning, few-short learning, acc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;ChatGPT&#23558;&#20197;&#20844;&#27665;&#20026;&#23548;&#21521;&#30340;&#34892;&#25919;&#25991;&#26412;&#32763;&#35793;&#25104;&#24503;&#35821;Easy Language&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#25991;&#26412;&#27604;&#26631;&#20934;&#25991;&#26412;&#26356;&#26131;&#29702;&#35299;&#65292;&#20294;&#20173;&#26410;&#23436;&#20840;&#31526;&#21512;Easy Language&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.11563</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20316;&#20026;Easy Language&#32763;&#35793;&#20013;&#30340;CAT&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Using ChatGPT as a CAT tool in Easy Language translation. (arXiv:2308.11563v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11563
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20351;&#29992;ChatGPT&#23558;&#20197;&#20844;&#27665;&#20026;&#23548;&#21521;&#30340;&#34892;&#25919;&#25991;&#26412;&#32763;&#35793;&#25104;&#24503;&#35821;Easy Language&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#25991;&#26412;&#27604;&#26631;&#20934;&#25991;&#26412;&#26356;&#26131;&#29702;&#35299;&#65292;&#20294;&#20173;&#26410;&#23436;&#20840;&#31526;&#21512;Easy Language&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;ChatGPT&#23558;&#20197;&#20844;&#27665;&#20026;&#23548;&#21521;&#30340;&#34892;&#25919;&#25991;&#26412;&#32763;&#35793;&#25104;&#24503;&#35821;Easy Language&#30340;&#21487;&#34892;&#24615;&#65292;Easy Language&#26159;&#19968;&#31181;&#31616;&#21270;&#30340;&#12289;&#21463;&#25511;&#30340;&#35821;&#35328;&#21464;&#20307;&#65292;&#36866;&#24212;&#20102;&#38405;&#35835;&#38556;&#30861;&#20154;&#22763;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#65292;&#21363;&#35821;&#35328;&#21644;&#25972;&#20307;&#65292;&#32763;&#35793;&#26469;&#33258;&#24503;&#22269;&#20844;&#20849;&#26426;&#26500;&#32593;&#31449;&#30340;&#36873;&#23450;&#25991;&#26412;&#12290;&#25105;&#20204;&#22522;&#20110;&#27491;&#30830;&#24615;&#12289;&#21487;&#35835;&#24615;&#21644;&#21477;&#27861;&#22797;&#26434;&#24230;&#31561;&#19981;&#21516;&#26631;&#20934;&#20998;&#26512;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#12290;&#32467;&#26524;&#34920;&#26126;&#29983;&#25104;&#30340;&#25991;&#26412;&#27604;&#26631;&#20934;&#25991;&#26412;&#26356;&#23481;&#26131;&#29702;&#35299;&#65292;&#20294;&#20173;&#26410;&#23436;&#20840;&#31526;&#21512;Easy Language&#30340;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#20869;&#23481;&#24182;&#19981;&#24635;&#26159;&#27491;&#30830;&#20256;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study sets out to investigate the feasibility of using ChatGPT to translate citizen-oriented administrative texts into German Easy Language, a simplified, controlled language variety that is adapted to the needs of people with reading impairments. We use ChatGPT to translate selected texts from websites of German public authorities using two strategies, i.e. linguistic and holistic. We analyse the quality of the generated texts based on different criteria, such as correctness, readability, and syntactic complexity. The results indicated that the generated texts are easier than the standard texts, but that they still do not fully meet the established Easy Language standards. Additionally, the content is not always rendered correctly.
&lt;/p&gt;</description></item><item><title>BELB&#26159;&#19968;&#31181;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#23545;7&#20010;&#30693;&#35782;&#24211;&#38142;&#25509;&#30340;11&#20010;&#35821;&#26009;&#24211;&#30340;&#35775;&#38382;&#65292;&#24182;&#28085;&#30422;&#20102;&#22522;&#22240;&#12289;&#30142;&#30149;&#12289;&#21270;&#23398;&#29289;&#36136;&#12289;&#29289;&#31181;&#12289;&#32454;&#32990;&#31995;&#21644;&#21464;&#31181;&#31561;&#20845;&#31181;&#23454;&#20307;&#31867;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#22312;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#26631;&#20934;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2308.11537</link><description>&lt;p&gt;
BELB: &#19968;&#31181;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
BELB: a Biomedical Entity Linking Benchmark. (arXiv:2308.11537v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11537
&lt;/p&gt;
&lt;p&gt;
BELB&#26159;&#19968;&#31181;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#65292;&#25552;&#20379;&#20102;&#23545;7&#20010;&#30693;&#35782;&#24211;&#38142;&#25509;&#30340;11&#20010;&#35821;&#26009;&#24211;&#30340;&#35775;&#38382;&#65292;&#24182;&#28085;&#30422;&#20102;&#22522;&#22240;&#12289;&#30142;&#30149;&#12289;&#21270;&#23398;&#29289;&#36136;&#12289;&#29289;&#31181;&#12289;&#32454;&#32990;&#31995;&#21644;&#21464;&#31181;&#31561;&#20845;&#31181;&#23454;&#20307;&#31867;&#22411;&#12290;&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#22312;&#29616;&#26377;&#22522;&#20934;&#20013;&#30340;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#26631;&#20934;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#65288;BEL&#65289;&#26159;&#23558;&#23454;&#20307;&#25552;&#21450;&#19982;&#30693;&#35782;&#24211;&#23545;&#40784;&#30340;&#20219;&#21153;&#12290;&#23427;&#22312;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#30340;&#20449;&#24687;&#25552;&#21462;&#27969;&#31243;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#24182;&#21457;&#29616;&#30001;&#20110;&#29616;&#26377;&#30340;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#25366;&#25496;&#22522;&#20934;&#20013;&#32570;&#20047;&#35813;&#20219;&#21153;&#65292;&#19981;&#21516;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#20351;&#24471;&#22522;&#20110;&#24050;&#21457;&#34920;&#25968;&#25454;&#30340;&#27604;&#36739;&#23384;&#22312;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#31070;&#32463;&#31995;&#32479;&#20027;&#35201;&#22312;&#19982;&#24191;&#27867;&#35206;&#30422;&#30340;&#30693;&#35782;&#24211;UMLS&#38142;&#25509;&#30340;&#23454;&#20363;&#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#23545;&#20110;&#26356;&#19987;&#38376;&#30340;&#30693;&#35782;&#24211;&#65292;&#22914;&#22522;&#22240;&#25110;&#21464;&#31181;&#65292;&#20854;&#24615;&#33021;&#30740;&#31350;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;BELB&#65292;&#19968;&#31181;&#29992;&#32479;&#19968;&#26684;&#24335;&#25552;&#20379;&#23545;7&#20010;&#30693;&#35782;&#24211;&#38142;&#25509;&#30340;11&#20010;&#35821;&#26009;&#24211;&#30340;&#35775;&#38382;&#65292;&#24182;&#28085;&#30422;&#20102;&#22522;&#22240;&#12289;&#30142;&#30149;&#12289;&#21270;&#23398;&#29289;&#36136;&#12289;&#29289;&#31181;&#12289;&#32454;&#32990;&#31995;&#21644;&#21464;&#31181;&#20845;&#31181;&#23454;&#20307;&#31867;&#22411;&#30340;&#29983;&#29289;&#21307;&#23398;&#23454;&#20307;&#38142;&#25509;&#22522;&#20934;&#12290;BELB&#22823;&#22823;&#20943;&#23569;&#20102;&#22312;&#22810;&#20010;&#35821;&#26009;&#24211;&#19978;&#27979;&#35797;BEL&#31995;&#32479;&#30340;&#39044;&#22788;&#29702;&#24320;&#38144;&#65292;&#20026;&#21487;&#37325;&#22797;&#23454;&#39564;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical entity linking (BEL) is the task of grounding entity mentions to a knowledge base. It plays a vital role in information extraction pipelines for the life sciences literature. We review recent work in the field and find that, as the task is absent from existing benchmarks for biomedical text mining, different studies adopt different experimental setups making comparisons based on published numbers problematic. Furthermore, neural systems are tested primarily on instances linked to the broad coverage knowledge base UMLS, leaving their performance to more specialized ones, e.g. genes or variants, understudied. We therefore developed BELB, a Biomedical Entity Linking Benchmark, providing access in a unified format to 11 corpora linked to 7 knowledge bases and spanning six entity types: gene, disease, chemical, species, cell line and variant. BELB greatly reduces preprocessing overhead in testing BEL systems on multiple corpora offering a standardized testbed for reproducible exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11534</link><description>&lt;p&gt;
&#20316;&#20026;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model as a User Simulator. (arXiv:2308.11534v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21019;&#26032;&#24615;&#22320;&#23558;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#19988;&#35757;&#32451;&#20102;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#39640;&#36136;&#37327;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#26469;&#35757;&#32451;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;ChatGPT&#30340;&#21331;&#36234;&#24615;&#33021;&#24341;&#21457;&#20102;&#23545;&#20854;&#27665;&#20027;&#21270;&#30340;&#21162;&#21147;&#65292;&#20511;&#21161;&#30495;&#23454;&#29992;&#25143;&#21644;ChatGPT&#23545;&#35805;&#30340;&#21162;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;Vicuna&#26159;&#19968;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;Baize&#21644;UltraChat&#31561;&#21162;&#21147;&#20027;&#35201;&#20381;&#38752;ChatGPT&#26681;&#25454;&#25351;&#20196;&#27169;&#25311;&#20154;&#31867;&#34892;&#20026;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#30340;&#20154;&#31867;&#23398;&#20064;&#65292;&#23548;&#33268;&#33539;&#22260;&#26377;&#38480;&#65292;&#22810;&#26679;&#24615;&#20943;&#24369;&#65292;&#32570;&#20047;&#30495;&#27491;&#30340;&#22810;&#36718;&#23545;&#35805;&#21160;&#24577;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#25226;&#20174;&#30495;&#23454;&#20154;&#26426;&#23545;&#35805;&#20013;&#25552;&#21462;&#30340;&#20154;&#31867;&#38382;&#39064;&#20316;&#20026;&#23398;&#20064;&#30446;&#26631;&#65292;&#24182;&#35757;&#32451;&#19968;&#20010;&#29992;&#25143;&#27169;&#25311;&#22120;UserGPT&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21512;&#25104;&#23545;&#35805;&#25968;&#25454;&#38598;RealChat&#12290;&#38543;&#21518;&#65292;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#25105;&#20204;&#30340;&#21161;&#25163;&#27169;&#22411;ReaLM&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;ReaLM&#22312;Vicuna-Bench&#21644;MT-Bench&#20013;&#22343;&#36229;&#36807;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT conversations, as evidenced by Vicuna. However, while current endeavors like Baize and UltraChat aim to auto-generate conversational data due to challenges in gathering human participation, they primarily rely on ChatGPT to simulate human behaviors based on directives rather than genuine human learning. This results in a limited scope, diminished diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we innovatively target human questions extracted from genuine human-machine conversations as a learning goal and train a user simulator, UserGPT, to produce a high-quality human-centric synthetic conversation dataset, RealChat. Subsequently, this dataset trains our assistant model, ReaLM. Experimentally, ReaLM outpaces baseline models in both Vicuna-Bench and MT-Bench by pairwise
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#38590;&#27665;&#27861;&#20915;&#31574;&#30340;&#26234;&#33021;&#21270;&#21644;&#36879;&#26126;&#24230;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;&#30740;&#31350;&#28041;&#21450;&#26816;&#32034;&#36807;&#21435;&#30340;&#26696;&#20363;&#21644;&#20998;&#26512;&#21152;&#25343;&#22823;&#26696;&#20363;&#25968;&#25454;&#38598;&#30340;&#27861;&#24459;&#20915;&#31574;&#27969;&#31243;&#12290;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26032;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#24076;&#26395;&#20026;&#25152;&#26377;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#21253;&#23481;&#24615;&#21644;&#39044;&#26399;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2308.11531</link><description>&lt;p&gt;
&#25480;&#26435;&#38590;&#27665;&#30003;&#35831;&#32773;&#21450;&#20854;&#24459;&#24072;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#38590;&#27665;&#27861;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Empowering Refugee Claimants and their Lawyers: Using Machine Learning to Examine Decision-Making in Refugee Law. (arXiv:2308.11531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11531
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26469;&#25552;&#39640;&#38590;&#27665;&#27861;&#20915;&#31574;&#30340;&#26234;&#33021;&#21270;&#21644;&#36879;&#26126;&#24230;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#20915;&#31574;&#32467;&#26524;&#12290;&#30740;&#31350;&#28041;&#21450;&#26816;&#32034;&#36807;&#21435;&#30340;&#26696;&#20363;&#21644;&#20998;&#26512;&#21152;&#25343;&#22823;&#26696;&#20363;&#25968;&#25454;&#38598;&#30340;&#27861;&#24459;&#20915;&#31574;&#27969;&#31243;&#12290;&#36890;&#36807;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#26032;&#30340;&#22522;&#20934;&#65292;&#30740;&#31350;&#24076;&#26395;&#20026;&#25152;&#26377;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#24102;&#26469;&#21253;&#23481;&#24615;&#21644;&#39044;&#26399;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26234;&#33021;&#26469;&#24110;&#21161;&#21644;&#25903;&#25345;&#38590;&#27665;&#22320;&#20301;&#23457;&#29702;&#30340;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#24459;&#24072;&#12289;&#27861;&#23448;&#12289;&#31649;&#29702;&#26426;&#26500;&#21644;&#30003;&#35831;&#20154;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#20570;&#20986;&#20915;&#31574;&#65292;&#24182;&#22686;&#21152;&#38590;&#27665;&#30003;&#35831;&#27969;&#31243;&#30340;&#29702;&#35299;&#21644;&#36879;&#26126;&#24230;&#12290;&#36825;&#20010;&#21338;&#22763;&#39033;&#30446;&#26377;&#20004;&#20010;&#20027;&#35201;&#30446;&#26631;&#65306;&#65288;1&#65289;&#26816;&#32034;&#36807;&#21435;&#30340;&#26696;&#20363;&#65292;&#65288;2&#65289;&#22312;&#21152;&#25343;&#22823;&#26696;&#20363;&#25968;&#25454;&#38598;&#19978;&#20998;&#26512;&#27861;&#24459;&#20915;&#31574;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#24037;&#20316;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#21253;&#25324;&#23545;&#65288;1&#65289;&#37096;&#20998;&#30340;&#23436;&#25104;&#35797;&#39564;&#21644;&#19982;&#65288;2&#65289;&#37096;&#20998;&#30456;&#20851;&#30340;&#25345;&#32493;&#21162;&#21147;&#12290;&#25105;&#20204;&#30456;&#20449;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#36866;&#21512;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#30740;&#31350;&#20102;&#33258;&#21160;&#21270;&#25152;&#28041;&#21450;&#30340;&#25152;&#26377;&#27493;&#39588;&#30340;&#21487;&#34892;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26410;&#26469;&#38590;&#27665;&#27861;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#26032;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#21253;&#23481;&#25152;&#26377;&#26368;&#32456;&#29992;&#25143;&#21644;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#24102;&#26469;&#39044;&#26399;&#30340;&#22909;&#22788;&#65292;&#21253;&#25324;&#20943;&#23569;&#20915;&#31574;&#26102;&#38388;&#65292;&#26356;&#20844;&#24179;&#22320;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our project aims at helping and supporting stakeholders in refugee status adjudications, such as lawyers, judges, governing bodies, and claimants, in order to make better decisions through data-driven intelligence and increase the understanding and transparency of the refugee application process for all involved parties. This PhD project has two primary objectives: (1) to retrieve past cases, and (2) to analyze legal decision-making processes on a dataset of Canadian cases. In this paper, we present the current state of our work, which includes a completed experiment on part (1) and ongoing efforts related to part (2). We believe that NLP-based solutions are well-suited to address these challenges, and we investigate the feasibility of automating all steps involved. In addition, we introduce a novel benchmark for future NLP research in refugee law. Our methodology aims to be inclusive to all end-users and stakeholders, with expected benefits including reduced time-to-decision, fairer a
&lt;/p&gt;</description></item><item><title>BERT4CTR&#26159;&#19968;&#31181;&#39640;&#25928;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23427;&#25506;&#32034;&#20102;&#20004;&#31181;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11527</link><description>&lt;p&gt;
BERT4CTR:&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#30340;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BERT4CTR: An Efficient Framework to Combine Pre-trained Language Model with Non-textual Features for CTR Prediction. (arXiv:2308.11527v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11527
&lt;/p&gt;
&lt;p&gt;
BERT4CTR&#26159;&#19968;&#31181;&#39640;&#25928;&#26694;&#26550;&#65292;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#38750;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#28857;&#20987;&#29575;&#39044;&#27979;&#12290;&#23427;&#25506;&#32034;&#20102;&#20004;&#31181;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#28145;&#24230;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#24037;&#19994;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#30410;&#65292;&#21253;&#25324;&#28857;&#20987;&#29575;&#65288;CTR&#65289;&#39044;&#27979;&#65292;&#20294;&#22914;&#20309;&#23558;&#21482;&#22788;&#29702;&#25991;&#26412;&#20449;&#21495;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#19982;&#20855;&#26377;&#38750;&#25991;&#26412;&#29305;&#24449;&#30340;&#39044;&#27979;&#27969;&#31243;&#30456;&#32467;&#21512;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#26377;&#20004;&#20010;&#26041;&#21521;&#26469;&#25972;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#24182;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#19968;&#20010;&#26041;&#21521;&#26159;&#36890;&#36807;&#32858;&#21512;&#23618;&#23558;&#35821;&#35328;&#27169;&#22411;&#21644;&#38750;&#25991;&#26412;&#29305;&#24449;&#30340;&#32467;&#26524;&#36827;&#34892;&#34701;&#21512;&#65292;&#24418;&#25104;&#38598;&#25104;&#26694;&#26550;&#65292;&#20854;&#20013;&#25991;&#26412;&#21644;&#38750;&#25991;&#26412;&#36755;&#20837;&#20043;&#38388;&#30340;&#20132;&#21449;&#20449;&#24687;&#20165;&#22312;&#32858;&#21512;&#23618;&#20013;&#23398;&#20064;&#12290;&#21478;&#19968;&#20010;&#26041;&#21521;&#26159;&#23558;&#38750;&#25991;&#26412;&#29305;&#24449;&#20998;&#21106;&#25104;&#32454;&#31890;&#24230;&#29255;&#27573;&#65292;&#24182;&#23558;&#36825;&#20123;&#29255;&#27573;&#36716;&#25442;&#20026;&#19982;&#25991;&#26412;&#29255;&#27573;&#30456;&#32467;&#21512;&#30340;&#26032;&#26631;&#35760;&#65292;&#20197;&#20415;&#21487;&#20197;&#30452;&#25509;&#36755;&#20837;&#21040;&#35821;&#35328;&#27169;&#22411;&#30340;Transformer&#23618;&#20013;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22686;&#21152;&#20102;&#23398;&#20064;&#21644;&#25512;&#26029;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep pre-trained language models have shown promising benefit in a large set of industrial scenarios, including Click-Through-Rate (CTR) prediction, how to integrate pre-trained language models that handle only textual signals into a prediction pipeline with non-textual features is challenging.  Up to now two directions have been explored to integrate multi-modal inputs in fine-tuning of pre-trained language models. One consists of fusing the outcome of language models and non-textual features through an aggregation layer, resulting into ensemble framework, where the cross-information between textual and non-textual inputs are only learned in the aggregation layer. The second one consists of splitting non-textual features into fine-grained fragments and transforming the fragments to new tokens combined with textual ones, so that they can be fed directly to transformer layers in language models. However, this approach increases the complexity of the learning and inference becau
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;AIOps&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20844;&#20849;&#21644;&#19987;&#26377;&#25968;&#25454;&#19978;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2308.11526</link><description>&lt;p&gt;
&#22312;AIOps&#19978;&#23398;&#20064;&#26085;&#24535;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Representations on Logs for AIOps. (arXiv:2308.11526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11526
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;AIOps&#30340;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20844;&#20849;&#21644;&#19987;&#26377;&#25968;&#25454;&#19978;&#65292;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#26085;&#24535;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI for IT Operations (AIOps)&#26159;&#19968;&#31181;&#21151;&#33021;&#24378;&#22823;&#30340;&#24179;&#21488;&#65292;Site Reliability Engineers (SREs)&#21487;&#20197;&#20351;&#29992;&#23427;&#26469;&#22312;&#26368;&#23567;&#30340;&#20154;&#24037;&#24178;&#39044;&#19979;&#33258;&#21160;&#21270;&#21644;&#20248;&#21270;&#25805;&#20316;&#24037;&#20316;&#27969;&#31243;&#12290;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#26159;AIOps&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#22240;&#20026;&#23427;&#20026;SREs&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#35782;&#21035;&#21644;&#35299;&#20915;&#25345;&#32493;&#30340;&#25925;&#38556;&#12290;&#26085;&#24535;&#26684;&#24335;&#26816;&#27979;&#12289;&#26085;&#24535;&#20998;&#31867;&#21644;&#26085;&#24535;&#35299;&#26512;&#31561;&#20219;&#21153;&#26159;&#33258;&#21160;&#21270;&#26085;&#24535;&#20998;&#26512;&#30340;&#20851;&#38190;&#32452;&#20214;&#12290;&#22823;&#37096;&#20998;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#26377;&#30417;&#30563;&#23398;&#20064;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#24102;&#26631;&#31614;&#26085;&#24535;&#25968;&#25454;&#21644;&#26085;&#24535;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#65292;&#23384;&#22312;&#22810;&#20010;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;BERT&#21644;GPT3&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#35757;&#32451;&#20102;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#12290;&#36825;&#20123;&#27169;&#22411;&#25552;&#20379;&#20102;&#24191;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#31185;&#23398;&#21644;&#29983;&#29289;&#23398;&#31561;&#29305;&#23450;&#39046;&#22495;&#30340;LLMs&#30340;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#26085;&#24535;&#25968;&#25454;&#30340;LLM&#65292;&#23427;&#26159;&#22312;&#20844;&#20849;&#21644;&#19987;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
AI for IT Operations (AIOps) is a powerful platform that Site Reliability Engineers (SREs) use to automate and streamline operational workflows with minimal human intervention. Automated log analysis is a critical task in AIOps as it provides key insights for SREs to identify and address ongoing faults. Tasks such as log format detection, log classification, and log parsing are key components of automated log analysis. Most of these tasks require supervised learning; however, there are multiple challenges due to limited labelled log data and the diverse nature of log data. Large Language Models (LLMs) such as BERT and GPT3 are trained using self-supervision on a vast amount of unlabeled data. These models provide generalized representations that can be effectively used for various downstream tasks with limited labelled data. Motivated by the success of LLMs in specific domains like science and biology, this paper introduces a LLM for log data which is trained on public and proprietary 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11521</link><description>&lt;p&gt;
&#33258;&#25105;&#27450;&#39575;&#65306;&#36870;&#21521;&#30772;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#38450;&#28779;&#22681;
&lt;/p&gt;
&lt;p&gt;
Self-Deception: Reverse Penetrating the Semantic Firewall of Large Language Models. (arXiv:2308.11521v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#21644;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#20855;&#26377;&#25509;&#36817;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#24778;&#20154;&#33021;&#21147;&#12290;&#34429;&#28982;&#20026;&#21508;&#31181;&#31038;&#20250;&#38656;&#27714;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;LLM&#20063;&#38477;&#20302;&#20102;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#25104;&#26412;&#12290;&#22240;&#27492;&#65292;LLM&#24320;&#21457;&#20154;&#21592;&#24050;&#32463;&#37096;&#32626;&#20102;&#35821;&#20041;&#32423;&#30340;&#38450;&#24481;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#25298;&#32477;&#21487;&#33021;&#23548;&#33268;&#19981;&#36866;&#24403;&#20869;&#23481;&#30340;&#25552;&#31034;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#38450;&#24481;&#26426;&#21046;&#24182;&#19981;&#23436;&#20840;&#21487;&#38752;&#65292;&#19968;&#20123;&#25915;&#20987;&#32773;&#24050;&#32463;&#35774;&#35745;&#20986;&#20102;&#8220;&#36234;&#29425;&#8221;&#25552;&#31034;&#65292;&#20020;&#26102;&#20351;LLM&#24536;&#35760;&#20869;&#23481;&#38450;&#24481;&#35268;&#21017;&#24182;&#22238;&#31572;&#20219;&#20309;&#19981;&#36866;&#24403;&#30340;&#38382;&#39064;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#23578;&#26080;&#20851;&#20110;&#36825;&#20123;&#35821;&#20041;&#32423;&#25915;&#20987;&#21644;&#38450;&#24481;&#21407;&#21017;&#30340;&#26126;&#30830;&#35299;&#37322;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;LLM&#36234;&#29425;&#38382;&#39064;&#65292;&#24182;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#36234;&#29425;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#38450;&#28779;&#22681;&#30340;&#27010;&#24565;&#65292;&#24182;&#25552;&#20379;&#20102;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, have emerged with astonishing capabilities approaching artificial general intelligence. While providing convenience for various societal needs, LLMs have also lowered the cost of generating harmful content. Consequently, LLM developers have deployed semantic-level defenses to recognize and reject prompts that may lead to inappropriate content. Unfortunately, these defenses are not foolproof, and some attackers have crafted "jailbreak" prompts that temporarily hypnotize the LLM into forgetting content defense rules and answering any improper questions. To date, there is no clear explanation of the principles behind these semantic-level attacks and defenses in both industry and academia.  This paper investigates the LLM jailbreak problem and proposes an automatic jailbreak method for the first time. We propose the concept of a semantic firewall and provide three technical implementation approaches. Inspired by the attack that penetrates trad
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24120;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#21644;&#27604;&#36739;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#23458;&#25143;&#35780;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#26088;&#22312;&#31361;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11520</link><description>&lt;p&gt;
&#25506;&#32034;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#22312;&#20998;&#26512;&#23458;&#25143;&#35780;&#35770;&#20013;&#30340;&#24212;&#29992;&#65306;&#19968;&#39033;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Exploring the Power of Topic Modeling Techniques in Analyzing Customer Reviews: A Comparative Analysis. (arXiv:2308.11520v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24120;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#30740;&#31350;&#21644;&#27604;&#36739;&#65292;&#29305;&#21035;&#24212;&#29992;&#20110;&#23458;&#25143;&#35780;&#35770;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#24182;&#26088;&#22312;&#31361;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#24179;&#21488;&#21644;&#24212;&#29992;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#23548;&#33268;&#20102;&#29992;&#25143;&#29983;&#25104;&#30340;&#25991;&#26412;&#20869;&#23481;&#65288;&#21253;&#25324;&#35780;&#35770;&#21644;&#35780;&#20215;&#65289;&#25968;&#37327;&#30340;&#28608;&#22686;&#12290;&#22240;&#27492;&#65292;&#29992;&#25143;&#36890;&#24120;&#22312;&#20174;&#36825;&#20123;&#20869;&#23481;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#25110;&#30456;&#20851;&#20449;&#24687;&#26102;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#24050;&#34987;&#29992;&#20110;&#20998;&#26512;&#22312;&#32447;&#21487;&#33719;&#24471;&#30340;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#12290;&#36817;&#24180;&#26469;&#65292;&#20027;&#39064;&#24314;&#27169;&#25216;&#26415;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#27969;&#34892;&#24230;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22320;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#20116;&#31181;&#32463;&#24120;&#20351;&#29992;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#24212;&#29992;&#20110;&#23458;&#25143;&#35780;&#35770;&#30340;&#26041;&#27861;&#12290;&#25152;&#30740;&#31350;&#30340;&#26041;&#27861;&#21253;&#25324;&#28508;&#22312;&#35821;&#20041;&#20998;&#26512;&#65288;LSA&#65289;&#12289;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#12289;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;NMF&#65289;&#12289;&#24425;&#29699;&#25512;&#29702;&#27169;&#22411;&#65288;PAM&#65289;&#12289;Top2Vec&#21644;BERTopic&#12290;&#36890;&#36807;&#23454;&#38469;&#23637;&#31034;&#23427;&#20204;&#22312;&#26816;&#27979;&#37325;&#35201;&#20027;&#39064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#26088;&#22312;&#31361;&#20986;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exponential growth of online social network platforms and applications has led to a staggering volume of user-generated textual content, including comments and reviews. Consequently, users often face difficulties in extracting valuable insights or relevant information from such content. To address this challenge, machine learning and natural language processing algorithms have been deployed to analyze the vast amount of textual data available online. In recent years, topic modeling techniques have gained significant popularity in this domain. In this study, we comprehensively examine and compare five frequently used topic modeling methods specifically applied to customer reviews. The methods under investigation are latent semantic analysis (LSA), latent Dirichlet allocation (LDA), non-negative matrix factorization (NMF), pachinko allocation model (PAM), Top2Vec, and BERTopic. By practically demonstrating their benefits in detecting important topics, we aim to highlight their effica
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36716;&#25442;&#22120;&#30340;&#22810;&#26679;&#22534;&#21472;&#38598;&#25104;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#19968;&#36716;&#25442;&#22120;&#20316;&#20026;&#22522;&#23618;&#20998;&#31867;&#22120;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;RoBERTa&#30340;&#20803;&#23618;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.11519</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#65306;&#21033;&#29992;&#36716;&#25442;&#22120;&#30340;&#22810;&#26679;&#22534;&#21472;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Optimizing Multi-Class Text Classification: A Diverse Stacking Ensemble Framework Utilizing Transformers. (arXiv:2308.11519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36716;&#25442;&#22120;&#30340;&#22810;&#26679;&#22534;&#21472;&#38598;&#25104;&#26694;&#26550;&#65292;&#20197;&#20248;&#21270;&#22810;&#31867;&#25991;&#26412;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#19968;&#36716;&#25442;&#22120;&#20316;&#20026;&#22522;&#23618;&#20998;&#31867;&#22120;&#65292;&#24182;&#24341;&#20837;&#22522;&#20110;RoBERTa&#30340;&#20803;&#23618;&#20998;&#31867;&#22120;&#65292;&#23454;&#29616;&#20102;&#26368;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23458;&#25143;&#35780;&#35770;&#22312;&#35780;&#20272;&#23458;&#25143;&#28385;&#24847;&#24230;&#12289;&#25910;&#38598;&#21453;&#39304;&#21644;&#25512;&#21160;&#19994;&#21153;&#25913;&#36827;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20998;&#26512;&#36825;&#20123;&#35780;&#35770;&#21487;&#20197;&#20026;&#23458;&#25143;&#24773;&#32490;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#21253;&#25324;&#36190;&#32654;&#12289;&#35780;&#35770;&#21644;&#24314;&#35758;&#12290;&#25991;&#26412;&#20998;&#31867;&#25216;&#26415;&#20351;&#20225;&#19994;&#33021;&#22815;&#23558;&#23458;&#25143;&#35780;&#35770;&#20998;&#20026;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#20026;&#26356;&#22909;&#22320;&#20102;&#35299;&#23458;&#25143;&#21453;&#39304;&#25552;&#20379;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21644;&#20559;&#35265;&#31561;&#25361;&#25112;&#38480;&#21046;&#20102;&#21333;&#20010;&#20998;&#31867;&#22120;&#22312;&#30830;&#20445;&#26368;&#20339;&#39044;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#22534;&#21472;&#38598;&#25104;&#22810;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#20010;&#21333;&#19968;&#36716;&#25442;&#22120;&#65288;&#21253;&#25324;BERT&#12289;ELECTRA&#21644;DistilBERT&#65289;&#20316;&#20026;&#22522;&#23618;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#22522;&#20110;RoBERTa&#30340;&#20803;&#23618;&#20998;&#31867;&#22120;&#65292;&#29983;&#25104;&#19968;&#20010;&#26368;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Customer reviews play a crucial role in assessing customer satisfaction, gathering feedback, and driving improvements for businesses. Analyzing these reviews provides valuable insights into customer sentiments, including compliments, comments, and suggestions. Text classification techniques enable businesses to categorize customer reviews into distinct categories, facilitating a better understanding of customer feedback. However, challenges such as overfitting and bias limit the effectiveness of a single classifier in ensuring optimal prediction. This study proposes a novel approach to address these challenges by introducing a stacking ensemble-based multi-text classification method that leverages transformer models. By combining multiple single transformers, including BERT, ELECTRA, and DistilBERT, as base-level classifiers, and a meta-level classifier based on RoBERTa, an optimal predictive model is generated. The proposed stacking ensemble-based multi-text classification method aims
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#65288;UP-Adapter&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#33021;&#21147;&#65292;&#38024;&#23545;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#33258;&#21160;&#36873;&#25321;&#33258;&#20449;&#24230;&#26368;&#39640;&#30340;&#26679;&#26412;&#65292;&#24182;&#29983;&#25104;&#31867;&#21035;&#21407;&#22411;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2308.11507</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Prototype Adapter for Vision-Language Models. (arXiv:2308.11507v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#65288;UP-Adapter&#65289;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#33021;&#21147;&#65292;&#38024;&#23545;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#33258;&#21160;&#36873;&#25321;&#33258;&#20449;&#24230;&#26368;&#39640;&#30340;&#26679;&#26412;&#65292;&#24182;&#29983;&#25104;&#31867;&#21035;&#21407;&#22411;&#65292;&#20197;&#23454;&#29616;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#21644;ALIGN&#65289;&#22312;&#33719;&#21462;&#21487;&#36716;&#31227;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#23453;&#36149;&#30693;&#35782;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#24494;&#35843;&#26041;&#27861;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#21644;&#36866;&#37197;&#22120;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#36866;&#24212;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#21487;&#33719;&#24471;&#30340;&#26631;&#27880;&#26679;&#26412;&#65292;&#36825;&#21487;&#33021;&#32791;&#26102;&#19988;&#36153;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#21487;&#25193;&#23637;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#26080;&#30417;&#30563;&#21407;&#22411;&#36866;&#37197;&#22120;&#65288;UP-Adapter&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26410;&#26631;&#27880;&#30340;&#30446;&#26631;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#21033;&#29992;CLIP&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#33021;&#21147;&#33258;&#21160;&#36873;&#25321;&#27599;&#20010;&#31867;&#21035;&#30340;&#26368;&#33258;&#20449;&#26679;&#26412;&#12290;&#21033;&#29992;&#36825;&#20123;&#36873;&#25321;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#29983;&#25104;&#31867;&#21035;&#21407;&#22411;&#65292;&#36825;&#23558;&#20026;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale pre-trained vision-language models (e.g. CLIP and ALIGN) have demonstrated remarkable effectiveness in acquiring transferable visual representations. To leverage the valuable knowledge encoded within these models for downstream tasks, several fine-tuning approaches, including prompt tuning methods and adapter-based methods, have been developed to adapt vision-language models effectively with supervision. However, these methods rely on the availability of annotated samples, which can be labor-intensive and time-consuming to acquire, thus limiting scalability. To address this issue, in this work, we design an unsupervised fine-tuning approach for vision-language models called Unsupervised Prototype Adapter (UP-Adapter). Specifically, for the unannotated target datasets, we leverage the text-image aligning capability of CLIP to automatically select the most confident samples for each class. Utilizing these selected samples, we generate class prototypes, which serve a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11490</link><description>&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#22815;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Authorship Representation Learning Capture Stylistic Features?. (arXiv:2308.11490v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11490
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#23398;&#20064;&#33021;&#21542;&#25429;&#25417;&#25991;&#20307;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#35821;&#35328;&#23398;&#20013;&#65292;&#33258;&#21160;&#23558;&#20316;&#32773;&#30340;&#39118;&#26684;&#20174;&#20854;&#20889;&#20316;&#20869;&#23481;&#20013;&#20998;&#31163;&#20986;&#26469;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#19988;&#21487;&#33021;&#19981;&#21487;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#26377;&#22823;&#37327;&#24102;&#26377;&#20316;&#32773;&#26631;&#31614;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21487;&#29992;&#65292;&#20351;&#24471;&#20197;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#23398;&#20064;&#20316;&#32773;&#36523;&#20221;&#34920;&#24449;&#25104;&#20026;&#21487;&#33021;&#65292;&#29992;&#20110;&#20316;&#32773;&#24402;&#23646;&#36825;&#19968;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26174;&#28982;&#26356;&#22810;&#22320;&#20381;&#36182;&#20110;&#32534;&#30721;&#20889;&#20316;&#39118;&#26684;&#32780;&#19981;&#26159;&#32534;&#30721;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#23545;&#36825;&#19968;&#26367;&#20195;&#20219;&#21153;&#30340;&#25104;&#21151;&#24182;&#19981;&#33021;&#30830;&#20445;&#36825;&#20123;&#34920;&#24449;&#33021;&#22815;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#65292;&#22240;&#20026;&#20316;&#32773;&#36523;&#20221;&#20063;&#21487;&#33021;&#19982;&#20854;&#20182;&#28508;&#22312;&#21464;&#37327;&#65288;&#22914;&#20027;&#39064;&#65289;&#30456;&#20851;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#20123;&#34920;&#24449;&#25152;&#20256;&#36882;&#30340;&#20449;&#24687;&#30340;&#26412;&#36136;&#65292;&#29305;&#21035;&#26159;&#20026;&#20102;&#39564;&#35777;&#20854;&#20027;&#35201;&#32534;&#30721;&#30340;&#26159;&#20889;&#20316;&#39118;&#26684;&#30340;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38024;&#23545;&#24615;&#30340;&#23454;&#39564;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#36825;&#20123;&#34920;&#24449;&#12290;&#36825;&#20123;&#23454;&#39564;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20026;&#20316;&#32773;&#34920;&#31034;&#23398;&#20064;&#30340;&#34920;&#24449;&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#20889;&#20316;&#39118;&#26684;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically disentangling an author's style from the content of their writing is a longstanding and possibly insurmountable problem in computational linguistics. At the same time, the availability of large text corpora furnished with author labels has recently enabled learning authorship representations in a purely data-driven manner for authorship attribution, a task that ostensibly depends to a greater extent on encoding writing style than encoding content. However, success on this surrogate task does not ensure that such representations capture writing style since authorship could also be correlated with other latent variables, such as topic. In an effort to better understand the nature of the information these representations convey, and specifically to validate the hypothesis that they chiefly encode writing style, we systematically probe these representations through a series of targeted experiments. The results of these experiments suggest that representations learned for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23545;&#22238;&#31572;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36798;&#21040;13%&#33267;75%&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#20027;&#35201;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2308.11483</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions. (arXiv:2308.11483v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22810;&#36873;&#39064;&#36873;&#39033;&#39034;&#24207;&#30340;&#25935;&#24863;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#24403;&#23545;&#22238;&#31572;&#36873;&#39033;&#36827;&#34892;&#37325;&#26032;&#25490;&#24207;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#24046;&#36317;&#21487;&#20197;&#36798;&#21040;13%&#33267;75%&#12290;&#36825;&#31181;&#25935;&#24863;&#24615;&#20027;&#35201;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#23545;&#25552;&#31034;&#25991;&#23383;&#30340;&#25935;&#24863;&#24615;&#20197;&#21450;&#23569;&#26679;&#26412;&#23637;&#31034;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#65292;&#32473;&#23545;&#36825;&#20123;&#27169;&#22411;&#30340;&#20844;&#27491;&#35780;&#20272;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20102;&#35299;&#21644;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#21464;&#24471;&#36843;&#20999;&#12290;&#26412;&#25991;&#20851;&#27880;&#22312;&#22810;&#36873;&#39064;&#20219;&#21153;&#20013;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#36873;&#39033;&#39034;&#24207;&#30340;&#40065;&#26834;&#24615;&#36827;&#34892;&#30740;&#31350;&#65292;&#36825;&#26159;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#20107;&#23454;&#26816;&#32034;&#33021;&#21147;&#24120;&#29992;&#30340;&#20219;&#21153;&#12290;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#22312;&#37325;&#26032;&#25490;&#24207;&#22238;&#31572;&#36873;&#39033;&#26102;&#30340;&#34920;&#29616;&#24046;&#36317;&#30340;&#35843;&#26597;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#30456;&#24046;&#32422;13%&#33267;75%&#12290;&#36890;&#36807;&#35814;&#32454;&#20998;&#26512;&#65292;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#25935;&#24863;&#24615;&#26159;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21069;&#20004;&#20010;/&#19977;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#26102;&#20135;&#29983;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 75% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SONAR&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#21333;&#19968;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;200&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#36825;&#20123;&#32467;&#26524;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#20063;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11466</link><description>&lt;p&gt;
&#21477;&#23376;&#32423;&#22810;&#27169;&#24577;&#21644;&#35821;&#35328;&#26080;&#20851;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Sentence-Level Multimodal and Language-Agnostic Representations. (arXiv:2308.11466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11466
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;SONAR&#65292;&#19968;&#20010;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#65292;&#36890;&#36807;&#21333;&#19968;&#25991;&#26412;&#32534;&#30721;&#22120;&#22312;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#20248;&#21183;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;200&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#12290;&#36825;&#20123;&#32467;&#26524;&#30456;&#27604;&#29616;&#26377;&#27169;&#22411;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#23545;&#35821;&#38899;&#21040;&#25991;&#26412;&#27169;&#22411;&#20063;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;SONAR&#65292;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24577;&#30340;&#23450;&#38271;&#21477;&#23376;&#23884;&#20837;&#31354;&#38388;&#12290;&#25105;&#20204;&#30340;&#21333;&#19968;&#25991;&#26412;&#32534;&#30721;&#22120;&#35206;&#30422;&#20102;200&#31181;&#35821;&#35328;&#65292;&#22312;xsim&#21644;xsim++&#22810;&#35821;&#35328;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#21477;&#23376;&#23884;&#20837;&#27169;&#22411;LASER3&#21644;LabSE&#12290;&#20351;&#29992;&#29305;&#23450;&#35821;&#35328;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#22312;&#24072;&#29983;&#35774;&#32622;&#19979;&#35757;&#32451;&#35821;&#38899;&#36716;&#24405;&#25968;&#25454;&#21518;&#65292;&#35821;&#38899;&#29255;&#27573;&#21487;&#20197;&#22312;&#21516;&#19968;SONAR&#23884;&#20837;&#31354;&#38388;&#20013;&#36827;&#34892;&#23884;&#20837;&#12290;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;&#22312;&#30456;&#20284;&#24615;&#25628;&#32034;&#20219;&#21153;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#35821;&#38899;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;200&#31181;&#35821;&#35328;&#30340;&#25991;&#26412;&#35299;&#30721;&#22120;&#65292;&#21487;&#20197;&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#21644;&#35821;&#38899;&#21040;&#25991;&#26412;&#30340;&#26426;&#22120;&#32763;&#35793;&#65292;&#21253;&#25324;&#38646;&#32763;&#35793;&#35821;&#35328;&#21644;&#27169;&#24577;&#32452;&#21512;&#12290;&#23613;&#31649;&#23384;&#22312;&#23450;&#38271;&#29942;&#39048;&#34920;&#31034;&#65292;&#25105;&#20204;&#30340;&#25991;&#26412;&#21040;&#25991;&#26412;&#32467;&#26524;&#22312;&#19982;&#26368;&#20808;&#36827;&#30340;NLLB~1B&#27169;&#22411;&#30456;&#27604;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#25105;&#20204;&#30340;&#38646;&#32763;&#35793;&#35821;&#38899;&#21040;&#25991;&#26412;&#32467;&#26524;&#19982;&#24378;&#26377;&#21147;&#30340;&#30417;&#30563;&#22522;&#32447;&#27169;&#22411;Whisper&#30456;&#27604;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce SONAR, a new multilingual and multimodal fixed-size sentence embedding space. Our single text encoder, covering 200 languages, substantially outperforms existing sentence embeddings such as LASER3 and LabSE on the xsim and xsim++ multilingual similarity search tasks. Speech segments can be embedded in the same SONAR embedding space using language-specific speech encoders trained in a teacher-student setting on speech transcription data. Our encoders outperform existing speech encoders on similarity search tasks. We also provide a text decoder for 200 languages, which allows us to perform text-to-text and speech-to-text machine translation, including for zero-shot language and modality combinations. Our text-to-text results are competitive compared to the state-of-the-art NLLB~1B model, despite the fixed-size bottleneck representation. Our zero-shot speech-to-text translation results compare favorably with strong supervised baselines such as Whisper.
&lt;/p&gt;</description></item><item><title>LegalBench&#26159;&#19968;&#20010;&#21327;&#21516;&#26500;&#24314;&#30340;&#27861;&#24459;&#25512;&#29702;&#22522;&#20934;&#24211;&#65292;&#28085;&#30422;&#20102;162&#20010;&#20219;&#21153;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#24459;&#24072;&#21644;LLM&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#35789;&#27719;&#34920;&#12290;</title><link>http://arxiv.org/abs/2308.11462</link><description>&lt;p&gt;
LegalBench&#65306;&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27861;&#24459;&#25512;&#29702;&#30340;&#21327;&#21516;&#26500;&#24314;&#22522;&#20934;&#24211;
&lt;/p&gt;
&lt;p&gt;
LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models. (arXiv:2308.11462v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11462
&lt;/p&gt;
&lt;p&gt;
LegalBench&#26159;&#19968;&#20010;&#21327;&#21516;&#26500;&#24314;&#30340;&#27861;&#24459;&#25512;&#29702;&#22522;&#20934;&#24211;&#65292;&#28085;&#30422;&#20102;162&#20010;&#20219;&#21153;&#65292;&#21487;&#29992;&#20110;&#34913;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#27861;&#24459;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#20026;&#24459;&#24072;&#21644;LLM&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#35789;&#27719;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#21644;&#27861;&#24459;&#30028;&#23545;&#20854;&#30340;&#37319;&#29992;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;LLMs&#33021;&#22815;&#25191;&#34892;&#21738;&#20123;&#31867;&#22411;&#30340;&#27861;&#24459;&#25512;&#29702;&#65311;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LegalBench&#65306;&#19968;&#20010;&#21327;&#21516;&#26500;&#24314;&#30340;&#27861;&#24459;&#25512;&#29702;&#22522;&#20934;&#24211;&#65292;&#21253;&#21547;162&#20010;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#20845;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#27861;&#24459;&#25512;&#29702;&#12290;LegalBench&#26159;&#36890;&#36807;&#36328;&#23398;&#31185;&#30340;&#36807;&#31243;&#26500;&#24314;&#30340;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#30001;&#27861;&#24459;&#19987;&#19994;&#20154;&#21592;&#35774;&#35745;&#21644;&#25163;&#24037;&#21046;&#20316;&#30340;&#20219;&#21153;&#12290;&#22240;&#20026;&#36825;&#20123;&#19987;&#19994;&#20154;&#21592;&#22312;&#26500;&#24314;&#36807;&#31243;&#20013;&#36215;&#20102;&#20027;&#23548;&#20316;&#29992;&#65292;&#25152;&#20197;&#20219;&#21153;&#35201;&#20040;&#34913;&#37327;&#20102;&#23454;&#38469;&#26377;&#29992;&#30340;&#27861;&#24459;&#25512;&#29702;&#33021;&#21147;&#65292;&#35201;&#20040;&#34913;&#37327;&#20102;&#24459;&#24072;&#20204;&#24863;&#20852;&#36259;&#30340;&#25512;&#29702;&#25216;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#36328;&#23398;&#31185;&#20851;&#20110;&#27861;&#24459;&#30028;LLMs&#30340;&#23545;&#35805;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#27969;&#34892;&#30340;&#27861;&#24459;&#26694;&#26550;&#22914;&#20309;&#25551;&#36848;&#27861;&#24459;&#25512;&#29702;&#65292;&#36825;&#20123;&#26694;&#26550;&#21306;&#20998;&#20102;&#35768;&#22810;&#24418;&#24335;&#65292;&#19982;LegalBench&#30340;&#20219;&#21153;&#23545;&#24212;&#36215;&#26469;&#65292;&#20174;&#32780;&#32473;&#24459;&#24072;&#21644;LLM&#24320;&#21457;&#32773;&#25552;&#20379;&#20102;&#20849;&#21516;&#30340;&#35789;&#27719;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning -- which distinguish between its many forms -- correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;(AOAN)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#37051;&#36817;&#21306;&#38388;&#22686;&#24378;&#27169;&#22359;&#21644;&#22810;&#35282;&#24230;&#27880;&#24847;&#26426;&#21046;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2308.11447</link><description>&lt;p&gt;
&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Aspect-oriented Opinion Alignment Network for Aspect-Based Sentiment Classification. (arXiv:2308.11447v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;(AOAN)&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#20013;&#30340;&#35821;&#20041;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#37051;&#36817;&#21306;&#38388;&#22686;&#24378;&#27169;&#22359;&#21644;&#22810;&#35282;&#24230;&#27880;&#24847;&#26426;&#21046;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#31867;&#26159;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20854;&#26088;&#22312;&#26681;&#25454;&#19978;&#19979;&#25991;&#39044;&#27979;&#32473;&#23450;&#26041;&#38754;&#30340;&#24773;&#24863;&#26497;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#21033;&#29992;&#27880;&#24847;&#26426;&#21046;&#25552;&#21462;&#19981;&#21516;&#26041;&#38754;&#30340;&#35266;&#28857;&#35789;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#26159;&#26377;&#25928;&#22788;&#29702;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#36825;&#26159;&#30001;&#20110;&#27880;&#24847;&#26426;&#21046;&#22312;&#22810;&#26041;&#38754;&#21477;&#23376;&#20013;&#26410;&#33021;&#20805;&#20998;&#23545;&#40784;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#24341;&#36215;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#26041;&#38754;&#30340;&#35266;&#28857;&#23545;&#40784;&#32593;&#32476;(AOAN)&#65292;&#20197;&#25429;&#25417;&#35266;&#28857;&#35789;&#19982;&#30456;&#24212;&#26041;&#38754;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20851;&#32852;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#19968;&#20010;&#37051;&#36817;&#21306;&#38388;&#22686;&#24378;&#27169;&#22359;&#65292;&#24378;&#35843;&#37051;&#36817;&#21333;&#35789;&#21644;&#32473;&#23450;&#26041;&#38754;&#30340;&#21508;&#31181;&#32452;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22810;&#35282;&#24230;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#30456;&#24212;&#32473;&#23450;&#26041;&#38754;&#23545;&#40784;&#30456;&#20851;&#35266;&#28857;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based sentiment classification is a crucial problem in fine-grained sentiment analysis, which aims to predict the sentiment polarity of the given aspect according to its context. Previous works have made remarkable progress in leveraging attention mechanism to extract opinion words for different aspects. However, a persistent challenge is the effective management of semantic mismatches, which stem from attention mechanisms that fall short in adequately aligning opinions words with their corresponding aspect in multi-aspect sentences. To address this issue, we propose a novel Aspect-oriented Opinion Alignment Network (AOAN) to capture the contextual association between opinion words and the corresponding aspect. Specifically, we first introduce a neighboring span enhanced module which highlights various compositions of neighboring words and given aspects. In addition, we design a multi-perspective attention mechanism that align relevant opinion information with respect to the giv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#21453;&#39304;&#26862;&#26519;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#65292;&#24182;&#25104;&#21151;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11411</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#21453;&#39304;&#26862;&#26519;&#31639;&#27861;&#30340;&#20851;&#31995;&#19977;&#20803;&#32452;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Extracting Relational Triples Based on Graph Recursive Neural Network via Dynamic Feedback Forest Algorithm. (arXiv:2308.11411v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#21453;&#39304;&#26862;&#26519;&#31639;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#65292;&#24182;&#25104;&#21151;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#20851;&#31995;&#19977;&#20803;&#32452;&#65288;&#20027;&#20307;&#65292;&#35859;&#35789;&#65292;&#23458;&#20307;&#65289;&#21487;&#20197;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#20851;&#31995;&#25277;&#21462;&#65288;RE&#65289;&#26159;&#30693;&#35782;&#29983;&#25104;&#27969;&#31243;&#20013;&#30340;&#20004;&#20010;&#22522;&#30784;&#23376;&#20219;&#21153;&#12290;&#30001;&#20110;&#23427;&#20204;&#30340;&#24046;&#24322;&#24615;&#65292;&#23376;&#20219;&#21153;&#30340;&#25972;&#21512;&#24102;&#26469;&#20102;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#19977;&#20803;&#32452;&#25552;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026;&#22270;&#26631;&#35760;&#38382;&#39064;&#65292;&#21033;&#29992;&#20381;&#36182;&#35299;&#26512;&#21644;&#22270;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;GRNN&#65289;&#30340;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#20102;&#25972;&#21512;&#23376;&#20219;&#21153;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21453;&#39304;&#26862;&#26519;&#31639;&#27861;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#36890;&#36807;&#25512;&#29702;&#25805;&#20316;&#36830;&#25509;&#23376;&#20219;&#21153;&#30340;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting relational triples (subject, predicate, object) from text enables the transformation of unstructured text data into structured knowledge. The named entity recognition (NER) and the relation extraction (RE) are two foundational subtasks in this knowledge generation pipeline. The integration of subtasks poses a considerable challenge due to their disparate nature. This paper presents a novel approach that converts the triple extraction task into a graph labeling problem, capitalizing on the structural information of dependency parsing and graph recursive neural networks (GRNNs). To integrate subtasks, this paper proposes a dynamic feedback forest algorithm that connects the representations of subtasks by inference operations during model training. Experimental results demonstrate the effectiveness of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#19982;ASR&#27169;&#22359;&#65292;&#25104;&#21151;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#20174;80%&#38477;&#20302;&#21040;26.4%&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#23558;&#20854;&#36827;&#19968;&#27493;&#38477;&#20302;&#21040;14.5%&#12290;</title><link>http://arxiv.org/abs/2308.11380</link><description>&lt;p&gt;
Convoifilter: &#40481;&#23614;&#37202;&#20250;&#35821;&#38899;&#35782;&#21035;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Convoifilter: A case study of doing cocktail party speech recognition. (arXiv:2308.11380v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#19982;ASR&#27169;&#22359;&#65292;&#25104;&#21151;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#20174;80%&#38477;&#20302;&#21040;26.4%&#65292;&#24182;&#36890;&#36807;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#23558;&#20854;&#36827;&#19968;&#27493;&#38477;&#20302;&#21040;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#25913;&#36827;&#25317;&#25380;&#12289;&#22024;&#26434;&#29615;&#22659;&#19979;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;&#38024;&#23545;&#29305;&#23450;&#35828;&#35805;&#32773;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21333;&#22768;&#36947;&#35821;&#38899;&#22686;&#24378;&#27169;&#22359;&#23558;&#35828;&#35805;&#32773;&#30340;&#22768;&#38899;&#19982;&#32972;&#26223;&#22122;&#22768;&#20998;&#31163;&#65292;&#32467;&#21512;ASR&#27169;&#22359;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#23558;ASR&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#20174;80%&#38477;&#20302;&#21040;26.4%&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#25968;&#25454;&#35201;&#27714;&#30340;&#21464;&#21270;&#65292;&#36825;&#20004;&#20010;&#32452;&#20214;&#20250;&#29420;&#31435;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#35821;&#38899;&#22686;&#24378;&#21487;&#33021;&#20250;&#23548;&#33268;ASR&#25928;&#29575;&#19979;&#38477;&#12290;&#36890;&#36807;&#23454;&#26045;&#32852;&#21512;&#24494;&#35843;&#31574;&#30053;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23558;&#20998;&#21035;&#35843;&#25972;&#30340;WER&#20174;26.4%&#38477;&#20302;&#21040;14.5%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise, along with an ASR module. Through this approach, the model is able to decrease the word error rate (WER) of ASR from 80% to 26.4%. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning.
&lt;/p&gt;</description></item><item><title>M3PS&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20135;&#21697;&#25688;&#35201;&#65292;&#35299;&#20915;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#20135;&#21697;&#25688;&#35201;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#12289;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#24314;&#27169;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.11351</link><description>&lt;p&gt;
M3PS&#65306;&#30005;&#23376;&#21830;&#21153;&#20013;&#20840;&#38754;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
M3PS: End-to-End Multi-Grained Multi-Modal Attribute-Aware Product Summarization in E-commerce. (arXiv:2308.11351v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11351
&lt;/p&gt;
&lt;p&gt;
M3PS&#26159;&#19968;&#31181;&#20840;&#38754;&#30340;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20135;&#21697;&#25688;&#35201;&#65292;&#35299;&#20915;&#20102;&#30005;&#23376;&#21830;&#21153;&#20013;&#20135;&#21697;&#25688;&#35201;&#30340;&#31471;&#21040;&#31471;&#24314;&#27169;&#12289;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#24314;&#27169;&#21644;&#22810;&#27169;&#24577;&#23646;&#24615;&#24314;&#27169;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20135;&#21697;&#25688;&#35201;&#65288;MMPS&#65289;&#26088;&#22312;&#36890;&#36807;&#31361;&#20986;&#20135;&#21697;&#29305;&#28857;&#30340;&#30701;&#25991;&#26412;&#25688;&#35201;&#26469;&#21560;&#24341;&#23458;&#25143;&#30340;&#20852;&#36259;&#24182;&#22686;&#21152;&#20854;&#36141;&#20080;&#27442;&#26395;&#12290;&#29616;&#26377;&#30340;MMPS&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#65306;1&#65289;&#32570;&#20047;&#31471;&#21040;&#31471;&#30340;&#20135;&#21697;&#25688;&#35201;&#65292;2&#65289;&#32570;&#20047;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#24314;&#27169;&#65292;&#20197;&#21450;3&#65289;&#32570;&#20047;&#22810;&#27169;&#24577;&#23646;&#24615;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#20135;&#21697;&#25688;&#35201;&#30340;&#31471;&#21040;&#31471;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#23646;&#24615;&#24863;&#30693;&#20135;&#21697;&#25688;&#35201;&#26041;&#27861;&#65288;M3PS&#65289;&#12290;M3PS&#21516;&#26102;&#23545;&#20135;&#21697;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#24182;&#29983;&#25104;&#20135;&#21697;&#25688;&#35201;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20960;&#20010;&#22810;&#31890;&#24230;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#20197;&#26356;&#22909;&#22320;&#25351;&#23548;M3PS&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#27169;&#24577;&#23545;&#20135;&#21697;&#23646;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#20197;&#20351;&#22810;&#27169;&#24577;&#20135;&#21697;&#29305;&#24615;&#33021;&#22815;&#24471;&#21040;&#20307;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the long textual product information and the product image, Multi-Modal Product Summarization (MMPS) aims to attract customers' interest and increase their desire to purchase by highlighting product characteristics with a short textual summary. Existing MMPS methods have achieved promising performance. Nevertheless, there still exist several problems: 1) lack end-to-end product summarization, 2) lack multi-grained multi-modal modeling, and 3) lack multi-modal attribute modeling. To address these issues, we propose an end-to-end multi-grained multi-modal attribute-aware product summarization method (M3PS) for generating high-quality product summaries in e-commerce. M3PS jointly models product attributes and generates product summaries. Meanwhile, we design several multi-grained multi-modal tasks to better guide the multi-modal learning of M3PS. Furthermore, we model product attributes based on both text and image modalities so that multi-modal product characteristics can be manife
&lt;/p&gt;</description></item><item><title>LEAP&#26159;&#19968;&#31181;&#29992;&#20110;NLP&#36719;&#20214;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;&#27979;&#35797;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;LEvy&#39134;&#34892;&#30340;&#33258;&#36866;&#24212;&#31890;&#23376;&#32676;&#31639;&#27861;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#27979;&#35797;&#29992;&#20363;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#38169;&#35823;&#21457;&#29616;&#33021;&#21147;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2308.11284</link><description>&lt;p&gt;
LEAP: NLP&#36719;&#20214;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;&#27979;&#35797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LEAP: Efficient and Automated Test Method for NLP Software. (arXiv:2308.11284v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11284
&lt;/p&gt;
&lt;p&gt;
LEAP&#26159;&#19968;&#31181;&#29992;&#20110;NLP&#36719;&#20214;&#30340;&#39640;&#25928;&#33258;&#21160;&#21270;&#27979;&#35797;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#32467;&#21512;LEvy&#39134;&#34892;&#30340;&#33258;&#36866;&#24212;&#31890;&#23376;&#32676;&#31639;&#27861;&#21644;&#25991;&#26412;&#29305;&#24449;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#27979;&#35797;&#29992;&#20363;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#38169;&#35823;&#21457;&#29616;&#33021;&#21147;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NLP&#36719;&#20214;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#21450;&#20984;&#26174;&#20102;&#23545;&#40065;&#26834;&#24615;&#30340;&#38656;&#27714;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#29992;&#20110;&#23545;&#25239;&#24615;&#27979;&#35797;&#29992;&#20363;&#30340;&#33258;&#21160;&#21270;&#27979;&#35797;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38480;&#21046;&#65306;&#38169;&#35823;&#21457;&#29616;&#33021;&#21147;&#36739;&#24369;&#65292;&#23545;&#22522;&#20110;BERT&#30340;NLP&#36719;&#20214;&#30340;&#25104;&#21151;&#29575;&#20174;0%&#21040;24.6%&#19981;&#31561;&#65307;&#26102;&#38388;&#25928;&#29575;&#20302;&#65292;&#27599;&#20010;&#27979;&#35797;&#29992;&#20363;&#38656;&#35201;177.8&#31186;&#21040;205.28&#31186;&#65292;&#20351;&#20854;&#22312;&#26102;&#38388;&#21463;&#38480;&#30340;&#22330;&#26223;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LEAP&#65292;&#19968;&#31181;&#33258;&#21160;&#21270;&#27979;&#35797;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;&#22522;&#20110;LEvy&#39134;&#34892;&#30340;&#33258;&#36866;&#24212;&#31890;&#23376;&#32676;&#31639;&#27861;&#19982;&#25991;&#26412;&#29305;&#24449;&#30456;&#32467;&#21512;&#26469;&#29983;&#25104;&#23545;&#25239;&#24615;&#27979;&#35797;&#29992;&#20363;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;Levy&#39134;&#34892;&#36827;&#34892;&#31181;&#32676;&#21021;&#22987;&#21270;&#65292;&#20197;&#22686;&#21152;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#24815;&#24615;&#26435;&#37325;&#33258;&#36866;&#24212;&#26356;&#26032;&#25805;&#20316;&#31526;&#65292;&#20197;&#25552;&#39640;LEAP&#22312;&#39640;&#32500;&#25991;&#26412;&#31034;&#20363;&#30340;&#20840;&#23616;&#20248;&#21270;&#25928;&#29575;&#65292;&#24182;&#22522;&#20110;&#36138;&#23146;&#31574;&#30053;&#35774;&#35745;&#20102;&#19968;&#31181;&#21464;&#24322;&#25805;&#20316;&#31526;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widespread adoption of DNNs in NLP software has highlighted the need for robustness. Researchers proposed various automatic testing techniques for adversarial test cases. However, existing methods suffer from two limitations: weak error-discovering capabilities, with success rates ranging from 0% to 24.6% for BERT-based NLP software, and time inefficiency, taking 177.8s to 205.28s per test case, making them challenging for time-constrained scenarios. To address these issues, this paper proposes LEAP, an automated test method that uses LEvy flight-based Adaptive Particle swarm optimization integrated with textual features to generate adversarial test cases. Specifically, we adopt Levy flight for population initialization to increase the diversity of generated test cases. We also design an inertial weight adaptive update operator to improve the efficiency of LEAP's global optimization of high-dimensional text examples and a mutation operator based on the greedy strategy to reduce the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#38899;&#20048;&#29702;&#35299;LLaMA&#65288;MU-LLaMA&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#38382;&#31572;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MusicQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;MU-LLaMA&#27169;&#22411;&#65292;&#24182;&#22312;&#38899;&#20048;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11276</link><description>&lt;p&gt;
&#38899;&#20048;&#29702;&#35299;LLaMA&#65306;&#24212;&#29992;&#38382;&#31572;&#21644;&#23383;&#24149;&#25512;&#36827;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Music Understanding LLaMA: Advancing Text-to-Music Generation with Question Answering and Captioning. (arXiv:2308.11276v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#38899;&#20048;&#29702;&#35299;LLaMA&#65288;MU-LLaMA&#65289;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#38382;&#31572;&#21644;&#23383;&#24149;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#38754;&#20020;&#30340;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;MusicQA&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35757;&#32451;MU-LLaMA&#27169;&#22411;&#65292;&#24182;&#22312;&#38899;&#20048;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#32570;&#20047;&#20855;&#26377;&#33258;&#28982;&#35821;&#35328;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#20844;&#24320;&#38899;&#20048;&#25968;&#25454;&#38598;&#65292;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#65288;T2M-Gen&#65289;&#38754;&#20020;&#37325;&#22823;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38899;&#20048;&#29702;&#35299;LLaMA&#65288;MU-LLaMA&#65289;&#65292;&#33021;&#22815;&#22238;&#31572;&#19982;&#38899;&#20048;&#30456;&#20851;&#30340;&#38382;&#39064;&#24182;&#20026;&#38899;&#20048;&#25991;&#20214;&#29983;&#25104;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;MERT&#27169;&#22411;&#20174;&#38899;&#39057;&#20013;&#25552;&#21462;&#38899;&#20048;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#36866;&#29992;&#20110;&#35757;&#32451;MU-LLaMA&#27169;&#22411;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#38899;&#39057;&#38382;&#31572;&#25968;&#25454;&#38598;&#32570;&#20047;&#24320;&#25918;&#24335;&#38899;&#20048;&#38382;&#31572;&#25152;&#38656;&#30340;&#28145;&#24230;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#29616;&#26377;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#29983;&#25104;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#35774;&#35745;&#29992;&#20110;&#22238;&#31572;&#24320;&#25918;&#24335;&#38899;&#20048;&#30456;&#20851;&#38382;&#39064;&#30340;MusicQA&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#32463;&#36807;&#25105;&#20204;&#35774;&#35745;&#30340;MusicQA&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;MU-LLaMA&#27169;&#22411;&#22312;&#38899;&#20048;&#38382;&#31572;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-music generation (T2M-Gen) faces a major obstacle due to the scarcity of large-scale publicly available music datasets with natural language captions. To address this, we propose the Music Understanding LLaMA (MU-LLaMA), capable of answering music-related questions and generating captions for music files. Our model utilizes audio representations from a pretrained MERT model to extract music features. However, obtaining a suitable dataset for training the MU-LLaMA model remains challenging, as existing publicly accessible audio question answering datasets lack the necessary depth for open-ended music question answering. To fill this gap, we present a methodology for generating question-answer pairs from existing audio captioning datasets and introduce the MusicQA Dataset designed for answering open-ended music-related questions. The experiments demonstrate that the proposed MU-LLaMA model, trained on our designed MusicQA dataset, achieves outstanding performance in both music qu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#36339;&#38382;&#31572;&#30340;&#33258;&#25105;&#36845;&#20195;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65288;HopPG&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#21644;&#22810;&#36339;&#38382;&#39064;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#21033;&#29992;&#20102;&#21069;&#20960;&#36339;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#29983;&#25104;&#19979;&#19968;&#36339;&#30340;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2308.11257</link><description>&lt;p&gt;
HopPG&#65306;&#33258;&#25105;&#36845;&#20195;&#30340;&#24322;&#26500;&#30693;&#35782;&#22810;&#36339;&#38382;&#31572;&#31243;&#24207;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
HopPG: Self-Iterative Program Generation for Multi-Hop Question Answering over Heterogeneous Knowledge. (arXiv:2308.11257v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11257
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22810;&#36339;&#38382;&#31572;&#30340;&#33258;&#25105;&#36845;&#20195;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65288;HopPG&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#22788;&#29702;&#24322;&#26500;&#30693;&#35782;&#21644;&#22810;&#36339;&#38382;&#39064;&#26102;&#25152;&#38754;&#20020;&#30340;&#22256;&#38590;&#65292;&#24182;&#21033;&#29992;&#20102;&#21069;&#20960;&#36339;&#30340;&#25191;&#34892;&#32467;&#26524;&#26469;&#29983;&#25104;&#19979;&#19968;&#36339;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#26159;&#22522;&#20110;&#30693;&#35782;&#30340;&#38382;&#31572;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#12290;&#23427;&#36890;&#24120;&#22522;&#20110;&#38382;&#39064;&#29983;&#25104;&#21487;&#25191;&#34892;&#31243;&#24207;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#24211;&#36827;&#34892;&#25512;&#29702;&#24471;&#20986;&#31572;&#26696;&#12290;&#30001;&#20110;&#36825;&#31181;&#20869;&#22312;&#26426;&#21046;&#65292;&#23427;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#35821;&#20041;&#35299;&#26512;&#26041;&#27861;&#36890;&#24120;&#22312;&#25191;&#34892;&#20043;&#21069;&#29983;&#25104;&#23436;&#25972;&#30340;&#31243;&#24207;&#65292;&#36825;&#22312;&#22788;&#29702;&#22810;&#36339;&#38382;&#39064;&#21644;&#24322;&#26500;&#30693;&#35782;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;&#39318;&#20808;&#65292;&#23436;&#25972;&#30340;&#22810;&#36339;&#31243;&#24207;&#20381;&#36182;&#20110;&#22810;&#20010;&#24322;&#26500;&#30340;&#25903;&#25345;&#20107;&#23454;&#65292;&#27169;&#22411;&#24456;&#38590;&#21516;&#26102;&#33719;&#21462;&#36825;&#20123;&#20107;&#23454;&#12290;&#20854;&#27425;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#21069;&#20960;&#36339;&#25191;&#34892;&#32467;&#26524;&#19982;&#24403;&#21069;&#36339;&#31243;&#24207;&#29983;&#25104;&#20043;&#38388;&#30340;&#20132;&#20114;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24322;&#26500;&#30693;&#35782;&#30340;&#33258;&#25105;&#36845;&#20195;&#22810;&#36339;&#31243;&#24207;&#29983;&#25104;&#26694;&#26550;&#65288;HopPG&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#21069;&#20960;&#36339;&#30340;&#25191;&#34892;&#32467;&#26524;&#65292;&#24182;&#26681;&#25454;&#23427;&#20204;&#29983;&#25104;&#19979;&#19968;&#36339;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semantic parsing-based method is an important research branch for knowledge-based question answering. It usually generates executable programs lean upon the question and then conduct them to reason answers over a knowledge base. Benefit from this inherent mechanism, it has advantages in the performance and the interpretability. However,traditional semantic parsing methods usually generate a complete program before executing it, which struggles with multi-hop question answering over heterogeneous knowledge. Firstly,a complete multi-hop program relies on multiple heterogeneous supporting facts, and it is difficult for models to receive these facts simultaneously. Secondly,these methods ignore the interaction information between the previous-hop execution result and the current-hop program generation. To alleviate these challenges, we propose a self-iterative framework for multi-hop program generation (HopPG) over heterogeneous knowledge, which leverages the previous-hop execution res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.11224</link><description>&lt;p&gt;
&#22312;&#22270;&#19978;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#24615;&#33021;&#27934;&#23519;&#19982;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evaluating Large Language Models on Graphs: Performance Insights and Comparative Analysis. (arXiv:2308.11224v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#25968;&#25454;&#19978;&#35299;&#20915;&#20998;&#26512;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;LLM&#22312;&#29702;&#35299;&#22270;&#25968;&#25454;&#12289;&#29983;&#25104;&#27491;&#30830;&#32467;&#26524;&#21644;&#36827;&#34892;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#28982;&#32780;LLM&#22312;&#22270;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#20010;LLM&#22312;&#35299;&#20915;&#20960;&#20010;&#22270;&#25968;&#25454;&#20998;&#26512;&#38382;&#39064;&#26102;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#65306;&#29702;&#35299;&#33021;&#21147;&#12289;&#27491;&#30830;&#24615;&#12289;&#30495;&#23454;&#24615;&#21644;&#30699;&#27491;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65306;1) LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22270;&#25968;&#25454;&#65292;&#24182;&#25512;&#29702;&#22270;&#30340;&#25299;&#25169;&#32467;&#26500;&#12290;2) GPT&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36923;&#36753;&#21644;&#36830;&#36143;&#30340;&#32467;&#26524;&#65292;&#22312;&#27491;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26367;&#20195;&#26041;&#26696;&#12290;3) &#25152;&#26377;&#34987;&#26816;&#27979;&#30340;LLM&#22312;&#32467;&#26500;&#25512;&#29702;&#26041;&#38754;&#37117;&#38754;&#20020;&#25361;&#25112;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#21644;&#23569;&#26679;&#26412;&#25552;&#31034;&#31561;&#25216;&#26415;&#26174;&#31034;&#20986;&#25928;&#26524;&#19979;&#38477;&#12290;4) GPT&#27169;&#22411;&#22312;&#22810;&#31572;&#26696;&#20219;&#21153;&#20013;&#32463;&#24120;&#20135;&#29983;&#38169;&#35823;&#31572;&#26696;&#65292;&#24341;&#21457;&#30495;&#23454;&#24615;&#26041;&#38754;&#30340;&#25285;&#24551;&#12290;5) GPT&#27169;&#22411;&#23545;&#20854;&#36755;&#20986;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#20449;&#24515;&#65292;&#21487;&#33021;&#38459;&#30861;&#20854;&#30699;&#27491;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GPT-4&#26174;&#31034;&#20986;&#20102;&#19981;&#21516;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have garnered considerable interest within both academic and industrial. Yet, the application of LLMs to graph data remains under-explored. In this study, we evaluate the capabilities of four LLMs in addressing several analytical problems with graph data. We employ four distinct evaluation metrics: Comprehension, Correctness, Fidelity, and Rectification. Our results show that: 1) LLMs effectively comprehend graph data in natural language and reason with graph topology. 2) GPT models can generate logical and coherent results, outperforming alternatives in correctness. 3) All examined LLMs face challenges in structural reasoning, with techniques like zero-shot chain-of-thought and few-shot prompting showing diminished efficacy. 4) GPT models often produce erroneous answers in multi-answer tasks, raising concerns in fidelity. 5) GPT models exhibit elevated confidence in their outputs, potentially hindering their rectification capacities. Notably, GPT-4 has dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#29420;&#31435;&#20110;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2308.11189</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#25351;&#26631;&#65306;&#35821;&#35328;&#27169;&#22411;&#26597;&#35810;&#20013;&#22833;&#36133;&#30340;&#39046;&#22495;&#26080;&#20851;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diversity Measures: Domain-Independent Proxies for Failure in Language Model Queries. (arXiv:2308.11189v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#36825;&#20123;&#25351;&#26631;&#29420;&#31435;&#20110;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#65292;&#24182;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#23454;&#35777;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#25351;&#26631;&#22312;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38169;&#35823;&#39044;&#27979;&#36890;&#24120;&#20381;&#36182;&#20110;&#39046;&#22495;&#29305;&#23450;&#30340;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22238;&#24212;&#22810;&#26679;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38169;&#35823;&#37327;&#21270;&#25351;&#26631;&#65292;&#22240;&#27492;&#29420;&#31435;&#20110;&#24213;&#23618;&#24212;&#29992;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#29109;&#12289;&#22522;&#23612;&#19981;&#32431;&#24230;&#21644;&#36136;&#24515;&#36317;&#31163;&#30340;&#19977;&#20010;&#25351;&#26631;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;&#65292;&#28041;&#21450;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#28201;&#24230;&#35774;&#32622;&#65292;&#35777;&#26126;&#36825;&#20123;&#25351;&#26631;&#19982;&#22833;&#36133;&#27010;&#29575;&#24378;&#30456;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23454;&#35777;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#25351;&#26631;&#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#25552;&#31034;&#12289;&#24605;&#32500;&#38142;&#25512;&#29702;&#21644;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViCo&#65292;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#26469;&#35299;&#20915;&#35270;&#39057;&#35780;&#35770;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35780;&#35770;&#30340;&#20027;&#35266;&#24615;&#38590;&#20197;&#37327;&#21270;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#12290;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;ViCo&#33021;&#22815;&#29983;&#25104;&#26377;&#36259;&#30340;&#35270;&#39057;&#35780;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.11171</link><description>&lt;p&gt;
ViCo: &#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#36827;&#34892;&#26377;&#36259;&#35270;&#39057;&#35780;&#35770;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ViCo: Engaging Video Comment Generation with Human Preference Rewards. (arXiv:2308.11171v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11171
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViCo&#65292;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#26469;&#35299;&#20915;&#35270;&#39057;&#35780;&#35770;&#29983;&#25104;&#20013;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#35780;&#35770;&#30340;&#20027;&#35266;&#24615;&#38590;&#20197;&#37327;&#21270;&#21644;&#35780;&#20272;&#65292;&#20197;&#21450;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#12290;&#21033;&#29992;&#20154;&#31867;&#20559;&#22909;&#22870;&#21169;&#30340;&#26041;&#27861;&#65292;ViCo&#33021;&#22815;&#29983;&#25104;&#26377;&#36259;&#30340;&#35270;&#39057;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#36259;&#30340;&#35270;&#39057;&#35780;&#35770;&#22312;&#35270;&#39057;&#31038;&#20132;&#23186;&#20307;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#35266;&#20247;&#24773;&#24863;&#12289;&#24605;&#24819;&#25110;&#24189;&#40664;&#30340;&#36733;&#20307;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#26631;&#39064;&#26679;&#24335;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#23545;&#35270;&#39057;&#35780;&#35770;&#29983;&#25104;&#36827;&#34892;&#20102;&#21021;&#27493;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#35780;&#35770;&#29983;&#25104;&#19982;&#26631;&#39064;&#29983;&#25104;&#23384;&#22312;&#19968;&#20123;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#36825;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#22312;&#29983;&#25104;&#26377;&#36259;&#35780;&#35770;&#26041;&#38754;&#19981;&#22815;&#26377;&#25928;&#12290;&#19982;&#26631;&#39064;&#30340;&#23458;&#35266;&#25551;&#36848;&#24615;&#36136;&#19981;&#21516;&#65292;&#35780;&#35770;&#24448;&#24448;&#20855;&#26377;&#22266;&#26377;&#30340;&#20027;&#35266;&#24615;&#65292;&#36825;&#20351;&#24471;&#35780;&#35770;&#30340;&#21442;&#19982;&#24230;&#38590;&#20197;&#37327;&#21270;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#30495;&#27491;&#26377;&#36259;&#30340;&#35780;&#35770;&#30340;&#31232;&#32570;&#24615;&#32473;&#25910;&#38598;&#36275;&#22815;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ViCo&#65292;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#26469;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#65292;&#20197;&#29983;&#25104;&#26377;&#36259;&#30340;&#35270;&#39057;&#35780;&#35770;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#37327;&#21270;&#35780;&#35770;&#30340;&#21442;&#19982;&#24230;&#65292;&#25105;&#20204;&#21033;&#29992;&#27599;&#26465;&#35780;&#35770;&#25910;&#21040;&#30340;&#8220;&#36190;&#8221;&#25968;&#20316;&#20026;&#20154;&#31867;&#20559;&#22909;&#30340;&#20195;&#29702;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Engaging video comments play an important role in video social media, as they are the carrier of feelings, thoughts, or humor of the audience. Preliminary works have made initial exploration for video comment generation by adopting caption-style encoder-decoder models. However, comment generation presents some unique challenges distinct from caption generation, which makes these methods somewhat less effective at generating engaging comments. In contrast to the objective and descriptive nature of captions, comments tend to be inherently subjective, making it hard to quantify and evaluate the engagement of comments. Furthermore, the scarcity of truly engaging comments brings difficulty to collecting enough high-quality training examples. In this paper, we propose ViCo with three novel designs to tackle the above challenges for generating engaging Video Comments. Firstly, to quantify the engagement of comments, we utilize the number of "likes" each comment receives as a proxy of human pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.11148</link><description>&lt;p&gt;
LLaMA-Reviewer: &#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25512;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#23457;&#26597;&#33258;&#21160;&#21270;&#20013;&#30340;&#24212;&#29992;&#65288;&#23454;&#35777;&#30740;&#31350;&#65289;
&lt;/p&gt;
&lt;p&gt;
LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning (Practical Experience Report). (arXiv:2308.11148v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA-Reviewer&#26694;&#26550;&#65292;&#36890;&#36807;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#20165;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#35813;&#26694;&#26550;&#20173;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#23457;&#26597;&#27963;&#21160;&#30340;&#33258;&#21160;&#21270;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#36861;&#27714;&#65292;&#20027;&#35201;&#36890;&#36807;&#35768;&#22810;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#34917;&#20805;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#30528;&#36855;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#33258;&#21160;&#21270;&#20195;&#30721;&#23457;&#26597;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#20173;&#28982;&#24456;&#23569;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLaMA-Reviewer&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#21033;&#29992;&#20102;&#27969;&#34892;&#30340;LLM&#8212;&#8212;LLaMA&#22312;&#20195;&#30721;&#23457;&#26597;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#36164;&#28304;&#38480;&#21046;&#65292;&#35813;&#26694;&#26550;&#37319;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20197;&#26497;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25552;&#20379;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;LLaMA-Reviewer&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#65292;&#20351;&#29992;&#20102;&#20004;&#20010;&#19981;&#21516;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#22312;&#21482;&#20351;&#29992;&#19981;&#21040;1%&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20998;&#31867;&#31639;&#27861;&#23545;&#20110;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.11138</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#31995;&#32479;&#24322;&#24120;&#30340;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NLP-based detection of systematic anomalies among the narratives of consumer complaints. (arXiv:2308.11138v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#28040;&#36153;&#32773;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#20998;&#31867;&#31639;&#27861;&#23545;&#20110;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#36827;&#34892;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#25237;&#35785;&#21465;&#36848;&#20013;&#30340;&#31995;&#32479;&#24322;&#24120;&#65292;&#31616;&#31216;&#20026;&#31995;&#32479;&#24322;&#24120;&#12290;&#23613;&#31649;&#20998;&#31867;&#31639;&#27861;&#34987;&#29992;&#20110;&#26816;&#27979;&#26126;&#26174;&#30340;&#24322;&#24120;&#65292;&#20294;&#22312;&#36739;&#23567;&#19988;&#39057;&#32321;&#20986;&#29616;&#30340;&#31995;&#32479;&#24322;&#24120;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#21487;&#33021;&#20250;&#22240;&#20026;&#21508;&#31181;&#21407;&#22240;&#32780;&#22833;&#25928;&#65292;&#21253;&#25324;&#25216;&#26415;&#21407;&#22240;&#21644;&#20154;&#24037;&#20998;&#26512;&#24072;&#30340;&#33258;&#28982;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#22312;&#20998;&#31867;&#20043;&#21518;&#30340;&#19979;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#23558;&#25237;&#35785;&#21465;&#36848;&#36716;&#21270;&#20026;&#23450;&#37327;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#31181;&#31639;&#27861;&#26469;&#26816;&#27979;&#31995;&#32479;&#24322;&#24120;&#12290;&#25105;&#20204;&#20351;&#29992;&#28040;&#36153;&#32773;&#37329;&#34701;&#20445;&#25252;&#23616;&#30340;&#28040;&#36153;&#32773;&#25237;&#35785;&#25968;&#25454;&#24211;&#20013;&#30340;&#25237;&#35785;&#21465;&#36848;&#26469;&#35828;&#26126;&#25972;&#20010;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop an NLP-based procedure for detecting systematic nonmeritorious consumer complaints, simply called systematic anomalies, among complaint narratives. While classification algorithms are used to detect pronounced anomalies, in the case of smaller and frequent systematic anomalies, the algorithms may falter due to a variety of reasons, including technical ones as well as natural limitations of human analysts. Therefore, as the next step after classification, we convert the complaint narratives into quantitative data, which are then analyzed using an algorithm for detecting systematic anomalies. We illustrate the entire procedure using complaint narratives from the Consumer Complaint Database of the Consumer Financial Protection Bureau.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2308.11103</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20877;&#35782;&#21035;&#33021;&#21147;&#65306;&#21311;&#21517;&#38754;&#20020;&#39118;&#38505;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Anonymity at Risk? Assessing Re-Identification Capabilities of Large Language Models. (arXiv:2308.11103v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37325;&#26032;&#35782;&#21035;&#21311;&#21517;&#20010;&#20154;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27431;&#30431;&#21644;&#29790;&#22763;&#65292;&#27861;&#38498;&#35009;&#20915;&#20013;&#33258;&#28982;&#20154;&#21644;&#27861;&#20154;&#30340;&#21311;&#21517;&#24615;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#21311;&#21517;&#20154;&#21592;&#30340;&#22823;&#35268;&#27169;&#20877;&#35782;&#21035;&#30340;&#25285;&#24551;&#26085;&#30410;&#22686;&#38271;&#12290;&#26681;&#25454;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#35201;&#27714;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#29790;&#22763;&#32852;&#37030;&#26368;&#39640;&#27861;&#38498;&#30340;&#23454;&#38469;&#27861;&#24459;&#25968;&#25454;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#26469;&#25506;&#35752;LLMs&#37325;&#26032;&#35782;&#21035;&#27861;&#38498;&#35009;&#20915;&#20013;&#20010;&#20154;&#30340;&#28508;&#21147;&#12290;&#22312;&#26368;&#21021;&#30340;&#23454;&#39564;&#20043;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#32463;&#36807;&#21311;&#21517;&#21270;&#22788;&#29702;&#30340;&#32500;&#22522;&#30334;&#31185;&#25968;&#25454;&#38598;&#65292;&#20316;&#20026;&#19968;&#20010;&#26356;&#20005;&#26684;&#30340;&#27979;&#35797;&#22330;&#22320;&#26469;&#36827;&#19968;&#27493;&#30740;&#31350;&#30740;&#31350;&#32467;&#26524;&#12290;&#36890;&#36807;&#24341;&#20837;&#24182;&#24212;&#29992;&#25991;&#26412;&#20013;&#20877;&#35782;&#21035;&#20154;&#21592;&#30340;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#24615;&#33021;&#34913;&#37327;&#25351;&#26631;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#20998;&#26512;&#20102;&#24433;&#21709;&#25104;&#21151;&#20877;&#35782;&#21035;&#30340;&#22240;&#32032;&#65292;&#30830;&#23450;&#27169;&#22411;&#22823;&#23567;&#12289;&#36755;&#20837;&#38271;&#24230;&#21644;&#25351;&#20196;&#35843;&#25972;&#26159;&#26368;&#37325;&#35201;&#30340;&#20915;&#23450;&#22240;&#32032;&#20043;&#19968;&#12290;&#23613;&#31649;&#22312;&#21311;&#21517;&#21270;&#22788;&#29702;&#21518;&#65292;LLMs&#22312;&#37325;&#26032;&#35782;&#21035;&#19978;&#30340;&#25104;&#21151;&#29575;&#24456;&#39640;&#65292;&#20294;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anonymity of both natural and legal persons in court rulings is a critical aspect of privacy protection in the European Union and Switzerland. With the advent of LLMs, concerns about large-scale re-identification of anonymized persons are growing. In accordance with the Federal Supreme Court of Switzerland, we explore the potential of LLMs to re-identify individuals in court rulings by constructing a proof-of-concept using actual legal data from the Swiss federal supreme court. Following the initial experiment, we constructed an anonymized Wikipedia dataset as a more rigorous testing ground to further investigate the findings. With the introduction and application of the new task of re-identifying people in texts, we also introduce new metrics to measure performance. We systematically analyze the factors that influence successful re-identifications, identifying model size, input length, and instruction tuning among the most critical determinants. Despite high re-identification rates on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#35266;&#35780;&#20215;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;&#22686;&#24378;&#20102;&#23458;&#35266;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.11020</link><description>&lt;p&gt;
&#35780;&#20272;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#23458;&#35266;&#35780;&#20215;&#65306;&#36890;&#36807;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#35780;&#20272;&#20154;&#31867;&#30456;&#20284;&#24230;
&lt;/p&gt;
&lt;p&gt;
Towards Objective Evaluation of Socially-Situated Conversational Robots: Assessing Human-Likeness through Multimodal User Behaviors. (arXiv:2308.11020v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23458;&#35266;&#35780;&#20215;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;&#22686;&#24378;&#20102;&#23458;&#35266;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#35780;&#20272;&#31038;&#20132;&#20301;&#32622;&#23545;&#35805;&#26426;&#22120;&#20154;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23458;&#35266;&#35780;&#20215;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;&#20027;&#35201;&#20851;&#27880;&#28857;&#26159;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#20316;&#20026;&#20027;&#35201;&#35780;&#20215;&#25351;&#26631;&#12290;&#32780;&#20197;&#24448;&#30340;&#30740;&#31350;&#24120;&#24120;&#20381;&#36182;&#20110;&#29992;&#25143;&#30340;&#20027;&#35266;&#35780;&#20215;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#38388;&#25509;&#35266;&#23519;&#29992;&#25143;&#34892;&#20026;&#26469;&#35780;&#20272;&#26426;&#22120;&#20154;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#22686;&#24378;&#23458;&#35266;&#24615;&#21644;&#21487;&#22797;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20351;&#29992;&#20851;&#27880;&#24615;&#23545;&#35805;&#35821;&#26009;&#24211;&#20013;&#30340;&#29992;&#25143;&#34892;&#20026;&#26469;&#26631;&#27880;&#20154;&#31867;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#22810;&#27169;&#24577;&#29992;&#25143;&#34892;&#20026;&#19982;&#20154;&#31867;&#30456;&#20284;&#24230;&#20998;&#25968;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#35780;&#20272;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenging task of evaluating socially situated conversational robots and presents a novel objective evaluation approach that relies on multimodal user behaviors. In this study, our main focus is on assessing the human-likeness of the robot as the primary evaluation metric. While previous research often relied on subjective evaluations from users, our approach aims to evaluate the robot's human-likeness based on observable user behaviors indirectly, thus enhancing objectivity and reproducibility. To begin, we created an annotated dataset of human-likeness scores, utilizing user behaviors found in an attentive listening dialogue corpus. We then conducted an analysis to determine the correlation between multimodal user behaviors and human-likeness scores, demonstrating the feasibility of our proposed behavior-based evaluation method.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#25968;&#23398;&#31616;&#31572;&#39064;&#30340;&#27491;&#30830;&#24615;&#21644;&#35823;&#35299;&#65292;&#24182;&#36890;&#36807;&#20540;&#35782;&#21035;&#27969;&#31243;&#25552;&#20379;&#38024;&#23545;&#24615;&#21453;&#39304;&#12290;</title><link>http://arxiv.org/abs/2308.11006</link><description>&lt;p&gt;
&#22312;&#38544;&#24335;&#25968;&#23398;&#31616;&#31572;&#39064;&#30340;&#33258;&#21160;&#35780;&#20272;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Using language models in the implicit automated assessment of mathematical short answer items. (arXiv:2308.11006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11006
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#25968;&#23398;&#31616;&#31572;&#39064;&#30340;&#27491;&#30830;&#24615;&#21644;&#35823;&#35299;&#65292;&#24182;&#36890;&#36807;&#20540;&#35782;&#21035;&#27969;&#31243;&#25552;&#20379;&#38024;&#23545;&#24615;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#29305;&#23450;&#25968;&#23398;&#31616;&#31572;&#39064;&#26500;&#36896;&#22238;&#31572;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#27969;&#31243;&#26469;&#35782;&#21035;&#23398;&#29983;&#22238;&#31572;&#20013;&#25351;&#23450;&#30340;&#20851;&#38190;&#20540;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#22238;&#31572;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#35782;&#21035;&#20219;&#20309;&#35823;&#35299;&#12290;&#26469;&#33258;&#20540;&#35782;&#21035;&#27969;&#31243;&#30340;&#20449;&#24687;&#21487;&#20197;&#29992;&#26469;&#21521;&#25945;&#24072;&#21644;&#23398;&#29983;&#25552;&#20379;&#21453;&#39304;&#12290;&#20540;&#35782;&#21035;&#27969;&#31243;&#30001;&#20004;&#20010;&#32463;&#36807;&#35843;&#20248;&#30340;&#35821;&#35328;&#27169;&#22411;&#32452;&#25104;&#12290;&#31532;&#19968;&#20010;&#27169;&#22411;&#21028;&#26029;&#19968;&#20010;&#20540;&#26159;&#21542;&#38544;&#21547;&#22312;&#23398;&#29983;&#22238;&#31572;&#20013;&#12290;&#31532;&#20108;&#20010;&#27169;&#22411;&#35782;&#21035;&#20851;&#38190;&#20540;&#22312;&#22238;&#31572;&#20013;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#27169;&#22411;&#65292;&#21487;&#20197;&#29992;&#20110;&#20219;&#20309;&#25552;&#31034;&#21644;&#20540;&#65292;&#20197;&#21450;&#27599;&#20010;&#25552;&#31034;&#21644;&#20540;&#29305;&#23450;&#30340;&#27169;&#22411;&#12290;&#20540;&#35782;&#21035;&#27969;&#31243;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35780;&#20998;&#26631;&#20934;&#30340;&#35780;&#20272;&#26356;&#20934;&#30830;&#21644;&#26377;&#20449;&#24687;&#37327;&#12290;&#23427;&#21487;&#20197;&#29992;&#26469;&#21521;&#23398;&#29983;&#25552;&#20379;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new way to assess certain short constructed responses to mathematics items. Our approach uses a pipeline that identifies the key values specified by the student in their response. This allows us to determine the correctness of the response, as well as identify any misconceptions. The information from the value identification pipeline can then be used to provide feedback to the teacher and student. The value identification pipeline consists of two fine-tuned language models. The first model determines if a value is implicit in the student response. The second model identifies where in the response the key value is specified. We consider both a generic model that can be used for any prompt and value, as well as models that are specific to each prompt and value. The value identification pipeline is a more accurate and informative way to assess short constructed responses than traditional rubric-based scoring. It can be used to provide more targeted feedback to students, which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;ChatGPT&#22312;&#19981;&#21516;&#20351;&#29992;&#26041;&#38754;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#65292;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#19981;&#21463;&#25991;&#26412;&#25968;&#25454;&#38271;&#24230;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26032;&#25968;&#25454;&#38598;&#30340;&#23453;&#36149;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2308.11001</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#20998;&#26512;&#30740;&#31350;&#20154;&#21592;&#23545;ChatGPT&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT. (arXiv:2308.11001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#30740;&#31350;&#20154;&#21592;&#23545;ChatGPT&#22312;&#19981;&#21516;&#20351;&#29992;&#26041;&#38754;&#30340;&#24773;&#24863;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#25216;&#26415;&#65292;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#19981;&#21463;&#25991;&#26412;&#25968;&#25454;&#38271;&#24230;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#26032;&#25968;&#25454;&#38598;&#30340;&#23453;&#36149;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#21019;&#26032;&#24615;&#21457;&#26126;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#21457;&#20102;&#24191;&#27867;&#30340;&#35752;&#35770;&#12290;&#23613;&#31649;&#24198;&#31069;&#20854;&#21508;&#31181;&#20248;&#28857;&#65292;&#20294;&#23545;&#20854;&#27491;&#30830;&#24615;&#21644;&#20351;&#29992;&#20262;&#29702;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#24050;&#32463;&#22312;&#21162;&#21147;&#25429;&#25417;&#29992;&#25143;&#23545;&#20854;&#30340;&#24773;&#24863;&#65292;&#20294;&#22914;&#20309;&#20998;&#26512;&#30740;&#31350;&#30028;&#20851;&#20110;ChatGPT&#19981;&#21516;&#20351;&#29992;&#26041;&#38754;&#30340;&#24773;&#24863;&#26159;&#19968;&#20010;&#20540;&#24471;&#25506;&#35752;&#30340;&#38382;&#39064;&#12290;&#22312;&#26631;&#20934;&#30340;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#20013;&#65292;&#36890;&#24120;&#21482;&#24212;&#29992;&#20110;&#23569;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#22312;&#30701;&#25991;&#26412;&#25968;&#25454;&#19978;&#20165;&#33719;&#24471;&#26377;&#38480;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#26469;&#20419;&#36827;&#23545;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#20026;&#22312;&#26032;&#25968;&#25454;&#38598;&#19978;&#25299;&#23637;&#38754;&#21521;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#30340;&#26368;&#26032;&#25216;&#26415;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#27934;&#35265;&#65292;&#20351;&#24471;&#36825;&#31181;&#20998;&#26512;&#19981;&#21463;&#25991;&#26412;&#25968;&#25454;&#38271;&#24230;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The groundbreaking invention of ChatGPT has triggered enormous discussion among users across all fields and domains. Among celebration around its various advantages, questions have been raised with regards to its correctness and ethics of its use. Efforts are already underway towards capturing user sentiments around it. But it begs the question as to how the research community is analyzing ChatGPT with regards to various aspects of its usage. It is this sentiment of the researchers that we analyze in our work. Since Aspect-Based Sentiment Analysis has usually only been applied on a few datasets, it gives limited success and that too only on short text data. We propose a methodology that uses Explainable AI to facilitate such analysis on research data. Our technique presents valuable insights into extending the state of the art of Aspect-Based Sentiment Analysis on newer datasets, where such analysis is not hampered by the length of the text data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.10959</link><description>&lt;p&gt;
DocPrompt: &#22823;&#35268;&#27169;&#36830;&#32493;&#39044;&#35757;&#32451;&#29992;&#20110;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
DocPrompt: Large-scale continue pretrain for zero-shot and few-shot document question answering. (arXiv:2308.10959v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10959
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DocPrompt&#27169;&#22411;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#21518;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DocPrompt&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24369;&#30417;&#30563;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#12289;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#38454;&#27573;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35299;&#27169;&#22411;&#21644;&#29983;&#25104;&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25991;&#26723;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#32463;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#30340;DocPrompt&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#24378;&#22522;&#32447;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#25991;&#26723;&#38382;&#31572;&#23458;&#25143;&#39033;&#30446;&#30340;&#20132;&#20184;&#25928;&#29575;&#21644;&#27169;&#22411;&#24615;&#33021;&#65292;&#38477;&#20302;&#20102;&#27880;&#37322;&#25104;&#26412;&#21644;&#21171;&#21160;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#20197;&#22312;https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Docprompt for document question answering tasks with powerful zero-shot and few-shot performance. We proposed a novel weakly supervised data generation method, a novel multl-stage training method and a novel understanding model &amp; generation model ensemble method. Experiment results show that the Docprompt model after continue pretrain significantly outperforms the existing strong baseline models on document question answering tasks. This method greatly improves the delivery efficiency and model performance of document question answering customer projects, reducing annotation costs and labor costs. Our demo can be found at https://huggingface.co/spaces/PaddlePaddle/ERNIE-Layout.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Wan Juan"&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20013;&#33521;&#25991;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;&#32593;&#32476;&#26469;&#28304;&#37319;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#27169;&#24577;&#65292;&#24635;&#37327;&#36229;&#36807;2TB&#12290;&#23427;&#34987;&#29992;&#20110;&#35757;&#32451;InternLM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#32500;&#24230;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2308.10755</link><description>&lt;p&gt;
WanJuan: &#29992;&#20110;&#25512;&#36827;&#33521;&#25991;&#21644;&#20013;&#25991;&#22823;&#27169;&#22411;&#30340;&#32508;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models. (arXiv:2308.10755v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Wan Juan"&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20013;&#33521;&#25991;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#36890;&#36807;&#21508;&#31181;&#32593;&#32476;&#26469;&#28304;&#37319;&#38598;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#27169;&#24577;&#65292;&#24635;&#37327;&#36229;&#36807;2TB&#12290;&#23427;&#34987;&#29992;&#20110;&#35757;&#32451;InternLM&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#32500;&#24230;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;GPT-4&#30340;&#26222;&#21450;&#26174;&#33879;&#21152;&#36895;&#20102;&#22823;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#23548;&#33268;&#20102;&#35768;&#22810;&#24341;&#20154;&#27880;&#30446;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;(MLLMs)&#30340;&#21019;&#24314;&#12290;&#36825;&#20123;&#23574;&#31471;&#27169;&#22411;&#30340;&#20986;&#33394;&#34920;&#29616;&#24402;&#21151;&#20110;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#39046;&#20808;&#33539;&#24335;&#20013;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#30340;&#32454;&#33410;&#36890;&#24120;&#34987;&#20445;&#23494;&#12290;&#36825;&#31181;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#21152;&#19978;&#24320;&#28304;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#38459;&#30861;&#20102;&#31038;&#21306;&#30340;&#36827;&#19968;&#27493;&#21457;&#23637;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;"Wan Juan"&#30340;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20013;&#25991;&#21644;&#33521;&#25991;&#25968;&#25454;&#65292;&#37319;&#38598;&#33258;&#24191;&#27867;&#30340;&#32593;&#32476;&#26469;&#28304;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#25991;&#26412;&#12289;&#22270;&#20687;&#25991;&#26412;&#21644;&#35270;&#39057;&#27169;&#24577;&#65292;&#24635;&#37327;&#36229;&#36807;2TB&#12290;&#23427;&#34987;&#29992;&#20110;&#35757;&#32451;InternLM&#27169;&#22411;&#65292;&#22312;&#22810;&#32500;&#24230;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#65292;&#19982;&#30456;&#20284;&#35268;&#27169;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#25152;&#26377;&#25968;&#25454;&#21487;&#22312;htt
&lt;/p&gt;
&lt;p&gt;
The rise in popularity of ChatGPT and GPT-4 has significantly accelerated the development of large models, leading to the creation of numerous impressive large language models(LLMs) and multimodal large language models (MLLMs). These cutting-edge models owe their remarkable performance to high-quality data. However, the details of the training data used in leading paradigms are often kept confidential. This lack of transparency, coupled with the scarcity of open-source data, impedes further developments within the community. As a response, this paper presents "Wan Juan", a large-scale multimodal dataset composed of both Chinese and English data, collected from a wide range of web sources. The dataset incorporates text, image-text, and video modalities, with a total volume exceeding 2TB. It was utilized in the training of InternLM, a model that demonstrated significant advantages in multi-dimensional evaluations when compared to models of a similar scale. All data can be accessed at htt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;PhraseTransformer&#65292;&#22312;&#36234;&#21335;&#35821;-&#20013;&#25991;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;BLEU&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.10482</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Effective Method using Phrase Mechanism in Neural Machine Translation. (arXiv:2308.10482v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#65292;PhraseTransformer&#65292;&#22312;&#36234;&#21335;&#35821;-&#20013;&#25991;&#24179;&#34892;&#35821;&#26009;&#24211;&#19978;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;BLEU&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#29616;&#23454;&#29983;&#27963;&#65292;&#24182;&#23545;NLP&#30740;&#31350;&#31038;&#21306;&#30340;&#20854;&#20182;&#20219;&#21153;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#27492;&#39046;&#22495;&#21560;&#24341;&#20102;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#35821;&#35328;&#23545;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#31181;&#20351;&#29992;&#30701;&#35821;&#26426;&#21046;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;PhraseTransformer&#65292;&#29992;&#20110;&#25913;&#36827;&#22522;&#32447;&#27169;&#22411;Transformer&#26500;&#24314;&#24179;&#34892;&#35821;&#26009;&#24211;&#36234;&#21335;&#35821;-&#20013;&#25991;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#31995;&#32479;&#12290;&#25105;&#20204;&#22312;VLSP 2022&#31454;&#36187;&#30340;MT&#25968;&#25454;&#38598;&#19978;&#65292;&#23454;&#29616;&#20102;&#36234;&#21335;&#35821;&#21040;&#20013;&#25991;&#30340;35.3 BLEU&#24471;&#20998;&#21644;&#20013;&#25991;&#21040;&#36234;&#21335;&#35821;&#30340;33.2 BLEU&#24471;&#20998;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#22312;https://github.com/phuongnm94/PhraseTransformer&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation is one of the essential tasks in Natural Language Processing (NLP), which has massive applications in real life as well as contributing to other tasks in the NLP research community. Recently, Transformer -based methods have attracted numerous researchers in this domain and achieved state-of-the-art results in most of the pair languages. In this paper, we report an effective method using a phrase mechanism, PhraseTransformer, to improve the strong baseline model Transformer in constructing a Neural Machine Translation (NMT) system for parallel corpora Vietnamese-Chinese. Our experiments on the MT dataset of the VLSP 2022 competition achieved the BLEU score of 35.3 on Vietnamese to Chinese and 33.2 BLEU scores on Chinese to Vietnamese data. Our code is available at https://github.com/phuongnm94/PhraseTransformer.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LibriSQA&#65292;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;ASR&#20219;&#21153;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;LLMs&#19978;&#25191;&#34892;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#30340;&#26174;&#33879;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.10390</link><description>&lt;p&gt;
LibriSQA&#65306;&#36890;&#36807;&#26032;&#22411;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#25512;&#36827;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
LibriSQA: Advancing Free-form and Open-ended Spoken Question Answering with a Novel Dataset and Framework. (arXiv:2308.10390v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10390
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;LibriSQA&#65292;&#19968;&#20010;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;&#21475;&#35821;&#38382;&#31572;&#25968;&#25454;&#38598;&#21644;&#26694;&#26550;&#65292;&#36890;&#36807;&#25913;&#36827;ASR&#20219;&#21153;&#24182;&#20351;&#29992;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#22312;LLMs&#19978;&#25191;&#34892;&#21475;&#35821;&#38382;&#31572;&#20219;&#21153;&#30340;&#26174;&#33879;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21487;&#31216;&#36190;&#30340;&#24615;&#33021;&#65292;&#20294;&#29616;&#26377;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22810;&#27169;&#24577;&#21151;&#33021;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38656;&#35201;&#35821;&#38899;&#21644;&#25991;&#26412;&#29305;&#24449;&#20043;&#38388;&#31934;&#30830;&#23545;&#40784;&#21644;&#28145;&#24230;&#20132;&#20114;&#30340;&#21475;&#35821;&#38382;&#31572;&#65288;SQA&#65289;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;LLM&#19978;&#30340;SQA&#25361;&#25112;&#65292;&#25105;&#20204;&#20174;Librispeech&#21019;&#36896;&#20102;&#33258;&#30001;&#24418;&#24335;&#21644;&#24320;&#25918;&#24335;&#30340;LibriSQA&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#33258;&#28982;&#23545;&#35805;&#26684;&#24335;&#30340;&#31532;&#19968;&#37096;&#20998;&#21644;&#21253;&#21547;&#22810;&#39033;&#36873;&#25321;&#39064;&#21644;&#31572;&#26696;&#20197;&#21450;&#20998;&#26512;&#29255;&#27573;&#30340;&#31532;&#20108;&#37096;&#20998;&#12290;&#36825;&#20004;&#37096;&#20998;&#20849;&#21253;&#21547;107k&#20010;&#28085;&#30422;&#21508;&#31181;&#20027;&#39064;&#30340;SQA&#23545;&#12290;&#37492;&#20110;&#29616;&#26377;&#35821;&#38899;-&#25991;&#26412;LLM&#30340;&#26126;&#26174;&#21294;&#20047;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#22312;LibriSQA&#19978;&#25191;&#34892;SQA&#20219;&#21153;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;&#36890;&#36807;&#23558;ASR&#25913;&#20026;SQA&#26684;&#24335;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#23454;&#20102;&#25105;&#20204;&#26694;&#26550;&#22788;&#29702;ASR&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) have demonstrated commendable performance across a myriad of domains and tasks, existing LLMs still exhibit a palpable deficit in handling multimodal functionalities, especially for the Spoken Question Answering (SQA) task which necessitates precise alignment and deep interaction between speech and text features. To address the SQA challenge on LLMs, we initially curated the free-form and open-ended LibriSQA dataset from Librispeech, comprising Part I with natural conversational formats and Part II encompassing multiple-choice questions followed by answers and analytical segments. Both parts collectively include 107k SQA pairs that cover various topics. Given the evident paucity of existing speech-text LLMs, we propose a lightweight, end-to-end framework to execute the SQA task on the LibriSQA, witnessing significant results. By reforming ASR into the SQA format, we further substantiate our framework's capability in handling ASR tasks. Our empirical f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WMFormer++&#30340;&#23884;&#22871;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#38544;&#24335;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#21487;&#35265;&#27700;&#21360;&#30340;&#21435;&#38500;&#12290;&#36890;&#36807;&#25972;&#21512;&#27700;&#21360;&#23450;&#20301;&#21644;&#32972;&#26223;&#24674;&#22797;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#20027;&#22320;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#65292;&#24182;&#37319;&#29992;&#20132;&#21449;&#36890;&#36947;&#27880;&#24847;&#21147;&#26469;&#20419;&#36827;&#23450;&#20301;&#21644;&#24674;&#22797;&#30340;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2308.10195</link><description>&lt;p&gt;
WMFormer++: &#36890;&#36807;&#38544;&#24335;&#32852;&#21512;&#23398;&#20064;&#30340;&#23884;&#22871;Transformer&#23454;&#29616;&#21487;&#35265;&#27700;&#21360;&#30340;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
WMFormer++: Nested Transformer for Visible Watermark Removal via Implict Joint Learning. (arXiv:2308.10195v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WMFormer++&#30340;&#23884;&#22871;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#38544;&#24335;&#32852;&#21512;&#23398;&#20064;&#23454;&#29616;&#21487;&#35265;&#27700;&#21360;&#30340;&#21435;&#38500;&#12290;&#36890;&#36807;&#25972;&#21512;&#27700;&#21360;&#23450;&#20301;&#21644;&#32972;&#26223;&#24674;&#22797;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#33258;&#20027;&#22320;&#23548;&#33322;&#20449;&#24687;&#27969;&#21160;&#65292;&#24182;&#37319;&#29992;&#20132;&#21449;&#36890;&#36947;&#27880;&#24847;&#21147;&#26469;&#20419;&#36827;&#23450;&#20301;&#21644;&#24674;&#22797;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#20445;&#25252;&#23186;&#20307;&#29256;&#26435;&#30340;&#26041;&#27861;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#25193;&#23637;&#21040;&#27700;&#21360;&#21435;&#38500;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#30340;&#25163;&#27573;&#26469;&#22686;&#24378;&#27700;&#21360;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#25512;&#21160;&#27700;&#21360;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#29616;&#26377;&#30340;&#27700;&#21360;&#21435;&#38500;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;UNet&#65292;&#24182;&#20855;&#26377;&#19987;&#38376;&#30340;&#35299;&#30721;&#20998;&#25903;&#8212;&#8212;&#19968;&#20010;&#29992;&#20110;&#27700;&#21360;&#23450;&#20301;&#65292;&#21478;&#19968;&#20010;&#29992;&#20110;&#32972;&#26223;&#22270;&#20687;&#24674;&#22797;&#12290;&#28982;&#32780;&#65292;&#27700;&#21360;&#23450;&#20301;&#21644;&#32972;&#26223;&#24674;&#22797;&#19981;&#26159;&#29420;&#31435;&#30340;&#20219;&#21153;&#65307;&#31934;&#30830;&#30340;&#27700;&#21360;&#23450;&#20301;&#26412;&#36136;&#19978;&#24847;&#21619;&#30528;&#38656;&#35201;&#24674;&#22797;&#30340;&#21306;&#22495;&#65292;&#32780;&#32972;&#26223;&#24674;&#22797;&#36807;&#31243;&#26377;&#21161;&#20110;&#26356;&#20934;&#30830;&#30340;&#27700;&#21360;&#23450;&#20301;&#12290;&#20026;&#20102;&#20174;&#20004;&#20010;&#20998;&#25903;&#32508;&#21512;&#22320;&#25972;&#21512;&#20449;&#24687;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38544;&#24335;&#32852;&#21512;&#23398;&#20064;&#33539;&#24335;&#12290;&#36825;&#20351;&#24471;&#32593;&#32476;&#33021;&#22815;&#36890;&#36807;&#38376;&#26426;&#21046;&#33258;&#20027;&#22320;&#23548;&#33322;&#38544;&#24335;&#20998;&#25903;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20132;&#21449;&#36890;&#36947;&#27880;&#24847;&#21147;&#26469;&#20419;&#36827;&#23450;&#20301;&#21644;&#24674;&#22797;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Watermarking serves as a widely adopted approach to safeguard media copyright. In parallel, the research focus has extended to watermark removal techniques, offering an adversarial means to enhance watermark robustness and foster advancements in the watermarking field. Existing watermark removal methods mainly rely on UNet with task-specific decoder branches--one for watermark localization and the other for background image restoration. However, watermark localization and background restoration are not isolated tasks; precise watermark localization inherently implies regions necessitating restoration, and the background restoration process contributes to more accurate watermark localization. To holistically integrate information from both branches, we introduce an implicit joint learning paradigm. This empowers the network to autonomously navigate the flow of information between implicit branches through a gate mechanism. Furthermore, we employ cross-channel attention to facilitate loc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.09765</link><description>&lt;p&gt;
&#21463;&#20919;&#33853;: &#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#21453;&#24046;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Taken by Surprise: Contrast effect for Similarity Scores. (arXiv:2308.09765v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#32771;&#34385;&#23545;&#35937;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#24182;&#26174;&#33879;&#25552;&#39640;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#35780;&#20272;&#29289;&#20307;&#21521;&#37327;&#23884;&#20837;&#30340;&#30456;&#20284;&#24230;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20449;&#24687;&#26816;&#32034;&#21644;&#20998;&#31867;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#27969;&#34892;&#30340;&#30456;&#20284;&#24230;&#20998;&#25968;&#65288;&#22914;&#20313;&#24358;&#30456;&#20284;&#24230;&#65289;&#22522;&#20110;&#23884;&#20837;&#21521;&#37327;&#23545;&#65292;&#24182;&#24573;&#30053;&#20102;&#20174;&#20013;&#25552;&#21462;&#23545;&#35937;&#30340;&#20998;&#24067;&#12290;&#20154;&#31867;&#23545;&#29289;&#20307;&#30456;&#20284;&#24230;&#30340;&#24863;&#30693;&#26174;&#33879;&#21462;&#20915;&#20110;&#23545;&#35937;&#20986;&#29616;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24778;&#21916;&#20998;&#25968;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#25972;&#20307;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20102;&#20154;&#31867;&#24863;&#30693;&#30340;&#21453;&#24046;&#25928;&#24212;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#27492;&#20998;&#25968;&#37327;&#21270;&#20102;&#22312;&#20004;&#20010;&#20803;&#32032;&#20043;&#38388;&#25214;&#21040;&#32473;&#23450;&#30456;&#20284;&#24230;&#30340;&#24778;&#21916;&#65292;&#30456;&#23545;&#20110;&#25104;&#23545;&#30340;&#25972;&#20307;&#30456;&#20284;&#24230;&#12290;&#25105;&#20204;&#22312;&#38646;&#26679;&#26412;/&#23569;&#26679;&#26412;&#20998;&#31867;&#21644;&#32858;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20010;&#24230;&#37327;&#65292;&#36890;&#24120;&#21457;&#29616;&#19982;&#21407;&#22987;&#20313;&#24358;&#30456;&#20284;&#24230;&#30456;&#27604;&#65292;&#24615;&#33021;&#25552;&#39640;&#20102;10-15\%&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;...
&lt;/p&gt;
&lt;p&gt;
Accurately evaluating the similarity of object vector embeddings is of critical importance for natural language processing, information retrieval and classification tasks. Popular similarity scores (e.g cosine similarity) are based on pairs of embedding vectors and disregard the distribution of the ensemble from which objects are drawn. Human perception of object similarity significantly depends on the context in which the objects appear. In this work we propose the \emph{surprise score}, an ensemble-normalized similarity metric that encapsulates the contrast effect of human perception and significantly improves the classification performance on zero- and few-shot document classification tasks. This score quantifies the surprise to find a given similarity between two elements relative to the pairwise ensemble similarities. We evaluate this metric on zero/few shot classification and clustering tasks and typically find 10-15\% better performance compared to raw cosine similarity. Our cod
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2308.09729</link><description>&lt;p&gt;
MindMap&#65306;&#30693;&#35782;&#22270;&#35889;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24605;&#32500;&#22270;&#24605;&#32771;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models. (arXiv:2308.09729v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29983;&#25104;&#24605;&#32500;&#23548;&#22270;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#24120;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#26080;&#27861;&#25972;&#21512;&#26032;&#30693;&#35782;&#12289;&#20135;&#29983;&#24187;&#35273;&#21644;&#20915;&#31574;&#36807;&#31243;&#19981;&#36879;&#26126;&#31561;&#38480;&#21046;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26469;&#28608;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#25972;&#21512;&#26368;&#26032;&#30693;&#35782;&#21644;&#24341;&#21457;&#27169;&#22411;&#24605;&#32500;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25552;&#31034;&#31649;&#36947;&#65292;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;KG&#36755;&#20837;&#24182;&#21033;&#29992;&#38544;&#21547;&#30693;&#35782;&#21644;&#26816;&#32034;&#21040;&#30340;&#22806;&#37096;&#30693;&#35782;&#36827;&#34892;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25512;&#29702;&#21644;&#29983;&#25104;&#31572;&#26696;&#30340;&#24605;&#32500;&#23548;&#22270;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#24605;&#32500;&#23548;&#22270;&#22522;&#20110;&#30693;&#35782;&#30340;&#26412;&#20307;&#35770;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#36335;&#24452;&#65292;&#20174;&#32780;&#20026;&#29983;&#20135;&#29615;&#22659;&#20013;&#30340;&#25512;&#29702;&#25552;&#20379;&#20102;&#25506;&#32034;&#21644;&#35780;&#20272;&#30340;&#21487;&#33021;&#24615;&#12290;&#23545;&#19977;&#20010;&#38382;&#31572;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;MindMap&#25552;&#31034;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#23454;&#35777;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
LLMs usually exhibit limitations in their ability to incorporate new knowledge, the generation of hallucinations, and the transparency of their decision-making process. In this paper, we explore how to prompt LLMs with knowledge graphs (KG), working as a remedy to engage LLMs with up-to-date knowledge and elicit the reasoning pathways from LLMs. Specifically, we build a prompting pipeline that endows LLMs with the capability of comprehending KG inputs and inferring with a combined implicit knowledge and the retrieved external knowledge. In addition, we investigate eliciting the mind map on which LLMs perform the reasoning and generate the answers. It is identified that the produced mind map exhibits the reasoning pathways of LLMs grounded on the ontology of knowledge, hence bringing the prospects of probing and gauging LLM inference in production. The experiments on three question &amp; answering datasets also show that MindMap prompting leads to a striking empirical gain. For instance, pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2308.08742</link><description>&lt;p&gt;
PMET: &#22312;Transformer&#20013;&#30340;&#31934;&#30830;&#27169;&#22411;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08742
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#65292;&#22240;&#27492;&#22312;&#36827;&#34892;&#27169;&#22411;&#32534;&#36753;&#26102;&#65292;&#19981;&#38656;&#35201;&#26356;&#26032;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#21487;&#20197;&#20197;&#36739;&#20302;&#30340;&#25104;&#26412;&#20462;&#25913;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23569;&#37327;&#30693;&#35782;&#65292;&#24182;&#19988;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26159;&#21069;&#39304;&#32593;&#32476;&#30340;&#38190;&#20540;&#20869;&#23384;&#30340;&#20540;&#12290;&#23427;&#20204;&#36890;&#24120;&#20248;&#21270;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#26469;&#35760;&#24518;&#30446;&#26631;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26356;&#26032;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21069;&#39304;&#32593;&#32476;&#30340;&#26435;&#37325;&#12290;&#28982;&#32780;&#65292;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#30340;&#20449;&#24687;&#27969;&#26469;&#33258;&#19977;&#20010;&#37096;&#20998;&#65306;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#12289;&#21069;&#39304;&#32593;&#32476;&#21644;&#27531;&#24046;&#36830;&#25509;&#12290;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#20102;Transformer&#23618;&#38544;&#34255;&#29366;&#24577;&#21253;&#21547;&#20102;&#21069;&#39304;&#32593;&#32476;&#29305;&#21035;&#38656;&#35201;&#30340;&#20449;&#24687;&#36825;&#19968;&#20107;&#23454;&#12290;&#22240;&#27492;&#65292;&#27169;&#22411;&#32534;&#36753;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#32534;&#36753;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#39304;&#32593;&#32476;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#21457;&#29616;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#32534;&#30721;&#20102;&#26576;&#20123;&#36890;&#29992;&#30693;&#35782;&#25552;&#21462;&#27169;&#24335;&#12290;&#36825;&#24847;&#21619;&#30528;&#24403;&#24341;&#20837;&#26032;&#30693;&#35782;&#26102;&#65292;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#30340;&#26435;&#37325;&#19981;&#38656;&#35201;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing techniques modify a minor proportion of knowledge in Large Language Models (LLMs) at a relatively low cost, which have demonstrated notable success. Existing methods assume Transformer Layer (TL) hidden states are values of key-value memories of the Feed-Forward Network (FFN). They usually optimize the TL hidden states to memorize target knowledge and use it to update the weights of the FFN in LLMs. However, the information flow of TL hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN, and residual connections. Existing methods neglect the fact that the TL hidden states contains information not specifically required for FFN. Consequently, the performance of model editing decreases. To achieve more precise model editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes certain general knowledge extraction patterns. This implies that MHSA weights do not require updating when new knowledge is introduced. Based on above findings, we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#37117;&#21487;&#20197;&#27169;&#25311;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#65292;&#20854;&#20013;&#21518;&#32773;&#30001;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#30005;&#36335;&#26063;&#32780;&#26356;&#20581;&#22766;&#12290;</title><link>http://arxiv.org/abs/2308.03212</link><description>&lt;p&gt;
&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#26159;&#24120;&#28145;&#24230;&#22343;&#21248;&#38408;&#20540;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Average-Hard Attention Transformers are Constant-Depth Uniform Threshold Circuits. (arXiv:2308.03212v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21644;&#23545;&#25968;&#31934;&#24230;&#21464;&#25442;&#22120;&#37117;&#21487;&#20197;&#27169;&#25311;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#65292;&#20854;&#20013;&#21518;&#32773;&#30001;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#30005;&#36335;&#26063;&#32780;&#26356;&#20581;&#22766;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25442;&#22120;&#24050;&#25104;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#23427;&#20204;&#19982;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#30340;&#20851;&#31995;&#65292;&#20570;&#20986;&#20102;&#20004;&#20010;&#20551;&#35774;&#65306;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21644;&#30456;&#23545;&#20110;&#36755;&#20837;&#38271;&#24230;&#30340;&#23545;&#25968;&#31934;&#24230;&#30340;&#20869;&#37096;&#35745;&#31639;&#12290;Merrill&#31561;&#20154;&#35777;&#26126;&#20102;&#24179;&#22343;&#22256;&#38590;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#21487;&#20197;&#35782;&#21035;&#23646;&#20110;&#22797;&#26434;&#24615;&#31867;&#21035;TC0&#30340;&#35821;&#35328;&#65292;&#35813;&#31867;&#21035;&#34920;&#31034;&#21487;&#20197;&#30001;&#24120;&#28145;&#24230;&#22810;&#39033;&#24335;&#22823;&#23567;&#30340;&#38408;&#20540;&#30005;&#36335;&#35782;&#21035;&#30340;&#35821;&#35328;&#38598;&#21512;&#12290;&#21516;&#26679;&#22320;&#65292;Merrill&#21644;Sabharwal&#35777;&#26126;&#20102;&#23545;&#25968;&#31934;&#24230;&#30340;&#21464;&#25442;&#22120;&#21487;&#20197;&#35782;&#21035;&#32479;&#19968;&#30340;TC0&#31867;&#35821;&#35328;&#12290;&#36825;&#34920;&#26126;&#36825;&#20004;&#31181;&#36716;&#25442;&#22120;&#27169;&#22411;&#37117;&#21487;&#20197;&#36890;&#36807;&#24120;&#28145;&#24230;&#38408;&#20540;&#30005;&#36335;&#26469;&#27169;&#25311;&#65292;&#32780;&#21518;&#32773;&#30001;&#20110;&#29983;&#25104;&#32479;&#19968;&#30340;&#30005;&#36335;&#26063;&#32780;&#26356;&#20581;&#22766;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#34920;&#26126;&#65292;&#31532;&#19968;&#20010;&#32467;&#26524;&#20063;&#21487;&#20197;&#24310;&#20280;&#21040;&#20135;&#29983;&#32479;&#19968;&#30005;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as a widely used neural network model for various natural language processing tasks. Previous research explored their relationship with constant-depth threshold circuits, making two assumptions: average-hard attention and logarithmic precision for internal computations relative to input length. Merrill et al. (2022) prove that average-hard attention transformers recognize languages that fall within the complexity class TC0, denoting the set of languages that can be recognized by constant-depth polynomial-size threshold circuits. Likewise, Merrill and Sabharwal (2023) show that log-precision transformers recognize languages within the class of uniform TC0. This shows that both transformer models can be simulated by constant-depth threshold circuits, with the latter being more robust due to generating a uniform circuit family. Our paper shows that the first result can be extended to yield uniform circuits as well.
&lt;/p&gt;</description></item><item><title>LARCH&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;readme&#21019;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20195;&#34920;&#24615;&#20195;&#30721;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#20107;&#23454;&#27491;&#30830;&#30340;readme&#65292;&#20248;&#20110;&#19981;&#20381;&#36182;&#20195;&#34920;&#24615;&#20195;&#30721;&#35782;&#21035;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03099</link><description>&lt;p&gt;
LARCH: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;readme&#21019;&#24314;&#19982;&#21551;&#21457;&#24335;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LARCH: Large Language Model-based Automatic Readme Creation with Heuristics. (arXiv:2308.03099v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03099
&lt;/p&gt;
&lt;p&gt;
LARCH&#26159;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#21160;readme&#21019;&#24314;&#26041;&#27861;&#65292;&#36890;&#36807;&#35782;&#21035;&#20195;&#34920;&#24615;&#20195;&#30721;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#21644;&#20107;&#23454;&#27491;&#30830;&#30340;readme&#65292;&#20248;&#20110;&#19981;&#20381;&#36182;&#20195;&#34920;&#24615;&#20195;&#30721;&#35782;&#21035;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#20889;readme&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#65292;&#23427;&#22312;&#31649;&#29702;&#21644;&#37325;&#29992;&#31243;&#24207;&#20195;&#30721;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#36825;&#26159;&#35768;&#22810;&#24320;&#21457;&#20154;&#21592;&#30340;&#30171;&#28857;&#65292;&#20294;&#21363;&#20351;&#22312;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#36827;&#23637;&#19979;&#65292;&#33258;&#21160;&#21019;&#24314;readme&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#38656;&#35201;&#20174;&#25104;&#21315;&#19978;&#19975;&#34892;&#20195;&#30721;&#20013;&#29983;&#25104;&#19968;&#20010;&#25277;&#35937;&#25551;&#36848;&#12290;&#22312;&#36825;&#31687;&#28436;&#31034;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#26524;&#25105;&#20204;&#33021;&#30830;&#23450;&#19968;&#20010;&#20195;&#34920;&#20179;&#24211;&#30340;&#20195;&#30721;&#29255;&#27573;&#65292;LLMs&#26377;&#33021;&#21147;&#29983;&#25104;&#19968;&#20221;&#36830;&#36143;&#21644;&#20107;&#23454;&#27491;&#30830;&#30340;readme&#12290;&#24314;&#31435;&#22312;&#36825;&#19968;&#21457;&#29616;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;LARCH&#65288;LLM-based Automatic Readme Creation with Heuristics&#65289;&#65292;&#23427;&#21033;&#29992;&#21551;&#21457;&#24335;&#21644;&#24369;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#20195;&#34920;&#24615;&#20195;&#30721;&#30340;&#35782;&#21035;&#12290;&#36890;&#36807;&#20154;&#24037;&#21644;&#33258;&#21160;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;LARCH&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#21487;&#20197;&#29983;&#25104;&#36830;&#36143;&#21644;&#20107;&#23454;&#27491;&#30830;&#30340;readme&#65292;&#24182;&#19988;&#20248;&#20110;&#19981;&#20381;&#36182;&#20110;&#20195;&#34920;&#24615;&#20195;&#30721;&#35782;&#21035;&#30340;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Writing a readme is a crucial aspect of software development as it plays a vital role in managing and reusing program code. Though it is a pain point for many developers, automatically creating one remains a challenge even with the recent advancements in large language models (LLMs), because it requires generating an abstract description from thousands of lines of code. In this demo paper, we show that LLMs are capable of generating a coherent and factually correct readmes if we can identify a code fragment that is representative of the repository. Building upon this finding, we developed LARCH (LLM-based Automatic Readme Creation with Heuristics) which leverages representative code identification with heuristics and weak supervision. Through human and automated evaluations, we illustrate that LARCH can generate coherent and factually correct readmes in the majority of cases, outperforming a baseline that does not rely on representative code identification. We have made LARCH open-sour
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2307.10652</link><description>&lt;p&gt;
&#25506;&#35752;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#39046;&#22495;&#30340;&#21457;&#23637;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Natural Language Processing Research. (arXiv:2307.10652v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10652
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#20102;ACL Anthology&#20013;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#25552;&#20379;&#20102;&#23545;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#21644;NLP&#39046;&#22495;&#30340;&#20998;&#31867;&#23398;&#12290;&#26412;&#30740;&#31350;&#24635;&#32467;&#20102;&#26368;&#26032;&#30340;NLP&#21457;&#23637;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20316;&#20026;&#29702;&#35299;&#12289;&#29983;&#25104;&#21644;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#30340;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#65292;&#22312;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#24555;&#36895;&#20256;&#25773;&#21644;&#24191;&#27867;&#24212;&#29992;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30740;&#31350;&#24037;&#20316;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#30740;&#31350;&#30028;&#24050;&#23545;&#25968;&#20010;&#19982;NLP&#30456;&#20851;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;&#28982;&#32780;&#65292;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#32570;&#23569;&#19968;&#39033;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#23545;&#24050;&#24314;&#31435;&#30340;&#20027;&#39064;&#36827;&#34892;&#20998;&#31867;&#12289;&#35782;&#21035;&#36235;&#21183;&#24182;&#27010;&#25324;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;ACL Anthology&#20013;&#21253;&#21547;&#30340;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#32467;&#26524;&#21576;&#29616;&#20102;&#30740;&#31350;&#39046;&#22495;&#30340;&#32467;&#26500;&#21270;&#27010;&#36848;&#65292;&#20026;NLP&#39046;&#22495;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#20998;&#31867;&#23398;&#65292;&#20998;&#26512;&#20102;NLP&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#24182;&#31361;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
As an efficient approach to understand, generate, and process natural language texts, research in natural language processing (NLP) has exhibited a rapid spread and wide adoption in recent years. Given the increasing amount of research work in this area, several NLP-related approaches have been surveyed in the research community. However, a comprehensive study that categorizes established topics, identifies trends, and outlines areas for future research remains absent to this day. Contributing to closing this gap, we have systematically classified and analyzed research papers included in the ACL Anthology. As a result, we present a structured overview of the research landscape, provide a taxonomy of fields-of-study in NLP, analyze recent developments in NLP, summarize our findings, and highlight directions for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22270;&#25968;&#25454;&#30340;&#38646;-shot&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37096;&#20998;&#25351;&#26631;&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#22312;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#34394;&#26500;&#38472;&#36848;&#30340;&#23545;&#27604;&#20013;&#20063;&#26377;&#26174;&#33879;&#30340;&#20851;&#32852;&#12290;</title><link>http://arxiv.org/abs/2307.07312</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#30693;&#35782;&#22270;&#29983;&#25104;&#38646;-shot&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#22270;&#25968;&#25454;&#30340;&#38646;-shot&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#37096;&#20998;&#25351;&#26631;&#19978;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#65292;&#22312;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#34394;&#26500;&#38472;&#36848;&#30340;&#23545;&#27604;&#20013;&#20063;&#26377;&#26174;&#33879;&#30340;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#20351;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#25968;&#25454;&#20316;&#20026;&#20854;&#24213;&#23618;&#30693;&#35782;&#34920;&#31034;&#30340;&#31995;&#32479;&#20013;&#65292;KG&#21040;&#25991;&#26412;&#29983;&#25104;&#26159;&#23558;&#22270;&#25968;&#25454;&#30340;&#37096;&#20998;&#36716;&#21270;&#20026;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#25991;&#26412;&#30340;&#26377;&#29992;&#24037;&#20855;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#20351;&#29992;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21363;&#20351;&#22312;&#29305;&#23450;&#22270;&#21040;&#25991;&#26412;&#20219;&#21153;&#30340;&#30456;&#23545;&#23567;&#30340;&#35757;&#32451;&#38598;&#19978;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#36825;&#20010;&#27010;&#24565;&#30340;&#22522;&#30784;&#19978;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#38646;-shot&#29983;&#25104;&#65292;&#20165;&#20165;&#26681;&#25454;&#27169;&#22411;&#23545;&#19977;&#20803;&#32452;&#32467;&#26500;&#30340;&#29702;&#35299;&#36827;&#34892;&#29983;&#25104;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;ChatGPT&#22312;WebNLG 2020&#25361;&#25112;&#36187;&#30340;&#26576;&#20123;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#25509;&#36817;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#25351;&#26631;&#19978;&#33853;&#21518;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20107;&#23454;&#12289;&#21453;&#20107;&#23454;&#21644;&#34394;&#26500;&#38472;&#36848;&#65292;&#24182;&#23637;&#31034;&#20102;LLM&#24050;&#32463;&#23545;&#20854;&#35299;&#26512;&#30340;&#25968;&#25454;&#26377;&#20851;&#30340;&#30693;&#35782;&#19982;&#36755;&#20986;&#25991;&#26412;&#36136;&#37327;&#20043;&#38388;&#30340;&#26174;&#33879;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
In any system that uses structured knowledge graph (KG) data as its underlying knowledge representation, KG-to-text generation is a useful tool for turning parts of the graph data into text that can be understood by humans. Recent work has shown that models that make use of pretraining on large amounts of text data can perform well on the KG-to-text task even with relatively small sets of training data on the specific graph-to-text task. In this paper, we build on this concept by using large language models to perform zero-shot generation based on nothing but the model's understanding of the triple structure from what it can read. We show that ChatGPT achieves near state-of-the-art performance on some measures of the WebNLG 2020 challenge, but falls behind on others. Additionally, we compare factual, counter-factual and fictional statements, and show that there is a significant connection between what the LLM already knows about the data it is parsing and the quality of the output text
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2306.08158</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey on Sociodemographic Bias in Natural Language Processing. (arXiv:2306.08158v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20559;&#35265;&#30340;&#35770;&#25991;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#30740;&#31350;&#32773;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#65292;&#24182;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#12290;&#24403;&#21069;&#21435;&#20559;&#35265;&#25216;&#26415;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24448;&#24448;&#20250;&#23398;&#20064;&#21040;&#38750;&#39044;&#26399;&#30340;&#20559;&#35265;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20135;&#29983;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#23545;209&#31687;&#20851;&#20110;NLP&#27169;&#22411;&#20013;&#20559;&#35265;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#35770;&#25991;&#28041;&#21450;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20559;&#35265;&#19982;&#30495;&#23454;&#19990;&#30028;&#30340;&#21361;&#23475;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#25105;&#20204;&#20511;&#37492;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#20559;&#35265;&#30340;&#23450;&#20041;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;NLP&#20559;&#35265;&#30740;&#31350;&#30340;&#19977;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#20559;&#35265;&#31867;&#22411;&#12289;&#37327;&#21270;&#20559;&#35265;&#21644;&#21435;&#20559;&#35265;&#12290;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#23545;&#20110;&#37327;&#21270;&#20559;&#35265;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#38752;&#24615;&#38382;&#39064;&#65292;&#35768;&#22810;&#20559;&#35265;&#24230;&#37327;&#24182;&#19981;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20559;&#35265;&#65292;&#24403;&#21069;&#30340;&#21435;&#20559;&#35265;&#25216;&#26415;&#26159;&#34920;&#38754;&#30340;&#65292;&#21482;&#26159;&#38544;&#34255;&#20102;&#20559;&#35265;&#65292;&#32780;&#19981;&#26159;&#30495;&#27491;&#21435;&#38500;&#23427;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks often learn unintended biases during training, which might have harmful effects when deployed in real-world settings. This paper surveys 209 papers on bias in NLP models, most of which address sociodemographic bias. To better understand the distinction between bias and real-world harm, we turn to ideas from psychology and behavioral economics to propose a definition for sociodemographic bias. We identify three main categories of NLP bias research: types of bias, quantifying bias, and debiasing. We conclude that current approaches on quantifying bias face reliability issues, that many of the bias metrics do not relate to real-world biases, and that current debiasing techniques are superficial and hide bias rather than removing it. Finally, we provide recommendations for future work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.19370</link><description>&lt;p&gt;
&#22823;&#22411;&#38271;&#24207;&#21015;&#27169;&#22411;&#30340;&#22359;&#32423;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Blockwise Parallel Transformer for Long Context Large Models. (arXiv:2305.19370v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22359;&#32423;&#24182;&#34892;Transformer&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#65292;&#33021;&#22815;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#24182;&#19988;&#21487;&#20197;&#22788;&#29702;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#26356;&#38271;32&#20493;&#30340;&#35757;&#32451;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#24050;&#32463;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#22522;&#30707;&#65292;&#22312;&#21508;&#31181;AI&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;Transformer&#20013;&#30340;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#21644;&#22823;&#22411;&#21069;&#39304;&#32593;&#32476;&#25152;&#38656;&#30340;&#20869;&#23384;&#23481;&#37327;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20026;&#28041;&#21450;&#22810;&#20010;&#38271;&#24207;&#21015;&#25110;&#38271;&#26399;&#20381;&#36182;&#30340;&#20219;&#21153;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#22359;&#32423;&#24182;&#34892;Transformer&#65288;BPT&#65289;&#65292;&#23427;&#21033;&#29992;&#22359;&#32423;&#35745;&#31639;&#33258;&#25105;&#27880;&#24847;&#21644;&#21069;&#39304;&#32593;&#32476;&#34701;&#21512;&#20197;&#26368;&#23567;&#21270;&#20869;&#23384;&#25104;&#26412;&#12290;&#36890;&#36807;&#22312;&#20445;&#25345;&#20869;&#23384;&#25928;&#29575;&#30340;&#21516;&#26102;&#22788;&#29702;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#65292;BPT&#20351;&#35757;&#32451;&#24207;&#21015;&#30340;&#38271;&#24230;&#27604;&#21407;&#22987;&#30340;Transformer&#38271;32&#20493;&#65292;&#27604;&#20808;&#21069;&#30340;&#20869;&#23384;&#39640;&#25928;&#26041;&#27861;&#38271;2&#21040;4&#20493;&#12290;&#23545;&#35821;&#35328;&#24314;&#27169;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;BPT&#22312;&#20943;&#23569;&#20869;&#23384;&#38656;&#27714;&#21644;&#25552;&#39640;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences up to 32 times longer than vanilla Transformers and 2 to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving perfo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;NollySenti&#65292;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#36866;&#24212;&#30340;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#65292;&#22522;&#20110;&#23612;&#26085;&#21033;&#20122;&#20116;&#31181;&#24120;&#29992;&#35821;&#35328;&#30340;&#35834;&#21033;&#26408;&#30005;&#24433;&#35780;&#35770;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#20855;&#26377;&#30456;&#21516;&#30446;&#26631;&#22495;&#30340;&#33521;&#35821;&#36827;&#34892;&#36801;&#31227;&#21487;&#20197;&#25552;&#39640;5&#65285;&#20197;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10971</link><description>&lt;p&gt;
NollySenti&#65306;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#21644;&#26426;&#22120;&#32763;&#35793;&#36827;&#34892;&#23612;&#26085;&#21033;&#20122;&#30005;&#24433;&#24773;&#24863;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
NollySenti: Leveraging Transfer Learning and Machine Translation for Nigerian Movie Sentiment Classification. (arXiv:2305.10971v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;NollySenti&#65292;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#36866;&#24212;&#30340;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#65292;&#22522;&#20110;&#23612;&#26085;&#21033;&#20122;&#20116;&#31181;&#24120;&#29992;&#35821;&#35328;&#30340;&#35834;&#21033;&#26408;&#30005;&#24433;&#35780;&#35770;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#20855;&#26377;&#30456;&#21516;&#30446;&#26631;&#22495;&#30340;&#33521;&#35821;&#36827;&#34892;&#36801;&#31227;&#21487;&#20197;&#25552;&#39640;5&#65285;&#20197;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#27954;&#26377;&#36229;&#36807;2000&#31181;&#26412;&#22303;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#20195;&#34920;&#24615;&#19981;&#39640;&#12290;&#26368;&#36817;&#20960;&#24180;&#65292;&#24050;&#32463;&#24320;&#22987;&#24320;&#21457;&#38750;&#27954;&#35821;&#35328;&#30340;&#26631;&#27880;&#35821;&#26009;&#24211;&#65292;&#20294;&#36825;&#20123;&#35821;&#26009;&#24211;&#36890;&#24120;&#21482;&#22312;&#21333;&#19968;&#39046;&#22495;&#20013;&#21487;&#29992;&#65292;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#21516;&#39046;&#22495;&#36866;&#24212;&#30340;&#24773;&#24863;&#20998;&#31867;&#20219;&#21153;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#8212;&#8212;NollySenti&#65292;&#23427;&#22522;&#20110;&#23612;&#26085;&#21033;&#20122;&#20116;&#31181;&#24120;&#29992;&#35821;&#35328;&#65288;&#33521;&#35821;&#12289;&#35946;&#33832;&#35821;&#12289;&#20234;&#21338;&#35821;&#12289;&#23612;&#26085;&#21033;&#20122;&#30382;&#38054;&#35821;&#21644;&#32422;&#40065;&#24052;&#35821;&#65289;&#30340;&#35834;&#21033;&#26408;&#30005;&#24433;&#35780;&#35770;&#12290;&#25105;&#20204;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#32463;&#39564;&#35780;&#20272;&#12290;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;Twitter&#39046;&#22495;&#30340;&#36328;&#22495;&#36866;&#24212;&#21644;&#26469;&#33258;&#33521;&#35821;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#36866;&#24212;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#20174;&#20855;&#26377;&#30456;&#21516;&#30446;&#26631;&#22495;&#30340;&#33521;&#35821;&#36827;&#34892;&#36801;&#31227;&#27604;&#20174;Twitter&#36827;&#34892;&#36801;&#31227;&#21487;&#20197;&#25552;&#39640;5&#65285;&#20197;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Africa has over 2000 indigenous languages but they are under-represented in NLP research due to lack of datasets. In recent years, there have been progress in developing labeled corpora for African languages. However, they are often available in a single domain and may not generalize to other domains. In this paper, we focus on the task of sentiment classification for cross domain adaptation. We create a new dataset, NollySenti - based on the Nollywood movie reviews for five languages widely spoken in Nigeria (English, Hausa, Igbo, Nigerian-Pidgin, and Yoruba. We provide an extensive empirical evaluation using classical machine learning methods and pre-trained language models. Leveraging transfer learning, we compare the performance of cross-domain adaptation from Twitter domain, and cross-lingual adaptation from English language. Our evaluation shows that transfer from English in the same target domain leads to more than 5% improvement in accuracy compared to transfer from Twitter in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.03881</link><description>&lt;p&gt;
&#22270;&#20687;&#25628;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65306;&#20851;&#20110;&#20174;&#22270;&#20687;&#26816;&#32034;&#19982;&#21435;&#20559;&#35265;&#35282;&#24230;&#25506;&#31350;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fairness in Image Search: A Study of Occupational Stereotyping in Image Retrieval and its Debiasing. (arXiv:2305.03881v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#34920;&#26126;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#25628;&#32034;&#24341;&#25806;&#36817;&#24180;&#26469;&#32463;&#21382;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#21644;&#24191;&#27867;&#30340;&#20351;&#29992;&#65292;&#25104;&#20026;&#32487;&#20449;&#24687;&#26816;&#32034;&#20043;&#21518;&#31532;&#20108;&#24120;&#35265;&#30340;&#20114;&#32852;&#32593;&#20351;&#29992;&#26041;&#24335;&#12290;&#23613;&#31649;&#25628;&#32034;&#24341;&#25806;&#31995;&#32479;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#26381;&#21153;&#65292;&#20294;&#22270;&#20687;&#25628;&#32034;&#39046;&#22495;&#26368;&#36817;&#25104;&#20026;&#20449;&#24687;&#26816;&#32034;&#31038;&#21306;&#30340;&#28966;&#28857;&#65292;&#22240;&#20026;&#24120;&#35328;&#36947;&#8220;&#19968;&#22270;&#32988;&#21315;&#35328;&#8221;&#12290;&#34429;&#28982;&#20687;&#35895;&#27468;&#36825;&#26679;&#30340;&#27969;&#34892;&#25628;&#32034;&#24341;&#25806;&#22312;&#22270;&#20687;&#25628;&#32034;&#31934;&#24230;&#21644;&#25935;&#25463;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#25628;&#32034;&#32467;&#26524;&#26159;&#21542;&#20250;&#23384;&#22312;&#24615;&#21035;&#12289;&#35821;&#35328;&#12289;&#20154;&#21475;&#32479;&#35745;&#12289;&#31038;&#20250;&#25991;&#21270;&#26041;&#38754;&#30340;&#20559;&#35265;&#23384;&#22312;&#20105;&#35758;&#12290;&#36825;&#31181;&#28508;&#22312;&#30340;&#20559;&#35265;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#30340;&#35748;&#30693;&#20135;&#29983;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#24433;&#21709;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#32593;&#32476;&#25628;&#32034;&#20013;&#30340;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#22522;&#20110;&#20851;&#38190;&#23383;&#30340;&#22270;&#20687;&#25628;&#32034;&#26041;&#38754;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#25628;&#32034;&#31995;&#32479;&#20013;&#23384;&#22312;&#30340;&#20960;&#31181;&#20559;&#35265;&#31867;&#22411;&#20197;&#21450;&#20026;&#20160;&#20040;&#26377;&#24517;&#35201;&#21152;&#20197;&#32531;&#35299;&#12290;&#25105;&#20204;&#23558;&#30740;&#31350;&#37325;&#28857;&#32553;&#23567;&#21040;&#35780;&#20272;&#21644;&#32531;&#35299;&#22270;&#20687;&#26816;&#32034;&#20013;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24403;&#21069;&#30340;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#23384;&#22312;&#30456;&#24403;&#20005;&#37325;&#30340;&#32844;&#19994;&#27169;&#24335;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#21487;&#33021;&#23545;&#20010;&#20154;&#21644;&#25972;&#20010;&#31038;&#20250;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#20559;&#35265;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#27492;&#31867;&#20559;&#35265;&#24182;&#25552;&#39640;&#22270;&#20687;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal search engines have experienced significant growth and widespread use in recent years, making them the second most common internet use. While search engine systems offer a range of services, the image search field has recently become a focal point in the information retrieval community, as the adage goes, "a picture is worth a thousand words". Although popular search engines like Google excel at image search accuracy and agility, there is an ongoing debate over whether their search results can be biased in terms of gender, language, demographics, socio-cultural aspects, and stereotypes. This potential for bias can have a significant impact on individuals' perceptions and influence their perspectives.  In this paper, we present our study on bias and fairness in web search, with a focus on keyword-based image search. We first discuss several kinds of biases that exist in search systems and why it is important to mitigate them. We narrow down our study to assessing and mitigat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01852</link><description>&lt;p&gt;
ChatGPT/GPT-4&#30740;&#31350;&#32508;&#36848;&#21450;&#23545;&#22823;&#35821;&#35328;&#27169;&#22411;&#26410;&#26469;&#30340;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models. (arXiv:2304.01852v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#21644;GPT-4&#65292;&#21253;&#25324;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#65292;&#24182;&#30528;&#37325;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21019;&#26032;&#12290;ChatGPT/GPT-4&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#26041;&#38754;&#34920;&#29616;&#31361;&#20986;&#65292;&#21516;&#26102;&#22312;&#20854;&#20182;&#39046;&#22495;&#20063;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26469;&#33258;GPT&#31995;&#21015;&#30340;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;ChatGPT&#21644;GPT-4&#21450;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21069;&#26223;&#24212;&#29992;&#12290;&#23454;&#38469;&#19978;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#12289;&#25351;&#20196;&#24494;&#35843;&#21644;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#25552;&#39640;LLMs&#30340;&#36866;&#24212;&#24615;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;arXiv&#19978;&#28145;&#20837;&#20998;&#26512;&#20102;194&#31687;&#30456;&#20851;&#25991;&#29486;&#65292;&#21253;&#25324;&#36235;&#21183;&#20998;&#26512;&#12289;&#35789;&#20113;&#34920;&#29616;&#21644;&#22312;&#21508;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20998;&#24067;&#20998;&#26512;&#12290;&#30740;&#31350;&#21457;&#29616;ChatGPT/GPT-4&#30740;&#31350;&#26174;&#33879;&#22686;&#38271;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#30452;&#25509;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#19978;&#65292;&#21516;&#26102;&#36824;&#23637;&#31034;&#20102;&#22312;&#20174;&#25945;&#32946;&#21644;&#21382;&#21490;&#21040;&#25968;&#23398;&#12289;&#21307;&#23398;&#21644;&#29289;&#29702;&#31561;&#39046;&#22495;&#20855;&#26377;&#30456;&#24403;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#26377;&#20851;ChatGPT&#30340;&#33021;&#21147;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models (LLM) from the GPT series, and their prospective applications across diverse domains. Indeed, key innovations such as large-scale pre-training that captures knowledge across the entire world wide web, instruction fine-tuning and Reinforcement Learning from Human Feedback (RLHF) have played significant roles in enhancing LLMs' adaptability and performance. We performed an in-depth analysis of 194 relevant papers on arXiv, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains. The findings reveal a significant and increasing interest in ChatGPT/GPT-4 research, predominantly centered on direct natural language processing applications, while also demonstrating considerable potential in areas ranging from education and history to mathematics, medicine, and physics. This study endeavors to furnish insights into ChatGPT's capabi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TrojText&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#30830;&#23450;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#26159;&#21542;&#21487;&#20197;&#26356;&#39640;&#25928;&#12289;&#26356;&#32463;&#27982;&#22320;&#36827;&#34892;&#38544;&#24418;&#25991;&#26412;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.02242</link><description>&lt;p&gt;
TrojText&#65306;&#27979;&#35797;&#26102;&#38544;&#24418;&#25991;&#26412;&#29305;&#27931;&#20234;&#26408;&#39532;&#25554;&#20837;
&lt;/p&gt;
&lt;p&gt;
TrojText: Test-time Invisible Textual Trojan Insertion. (arXiv:2303.02242v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TrojText&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#30830;&#23450;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#26159;&#21542;&#21487;&#20197;&#26356;&#39640;&#25928;&#12289;&#26356;&#32463;&#27982;&#22320;&#36827;&#34892;&#38544;&#24418;&#25991;&#26412;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#65292;&#26234;&#33021;&#31070;&#32463;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25991;&#26412;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#24403;&#29305;&#27931;&#20234;&#27169;&#22411;&#23545;&#20110;&#26631;&#20934;&#36755;&#20837;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#26159;&#23545;&#20110;&#21253;&#21547;&#29305;&#23450;&#35302;&#21457;&#22120;&#30340;&#36755;&#20837;&#29983;&#25104;&#24694;&#24847;&#36755;&#20986;&#26102;&#65292;&#23601;&#20250;&#21457;&#29983;&#36825;&#31181;&#25915;&#20987;&#12290;&#35821;&#27861;&#32467;&#26500;&#35302;&#21457;&#22120;&#65292;&#20316;&#20026;&#19968;&#31181;&#38544;&#24418;&#35302;&#21457;&#22120;&#65292;&#36234;&#26469;&#36234;&#21463;&#21040;&#29305;&#27931;&#20234;&#25915;&#20987;&#30340;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#38590;&#34987;&#26816;&#27979;&#21644;&#38450;&#24481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31867;&#22411;&#30340;&#25915;&#20987;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#20855;&#26377;&#29305;&#27931;&#20234;&#25554;&#20837;&#25152;&#38656;&#35821;&#27861;&#32467;&#26500;&#30340;&#27602;&#21270;&#26679;&#26412;&#12290;&#23545;&#20110;&#25915;&#20987;&#32773;&#26469;&#35828;&#65292;&#33719;&#21462;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#32780;&#29983;&#25104;&#35821;&#27861;&#27602;&#21270;&#35302;&#21457;&#22120;&#21644;&#25554;&#20837;&#29305;&#27931;&#20234;&#26408;&#39532;&#30340;&#36807;&#31243;&#21487;&#33021;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TrojText&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#30830;&#23450;&#26159;&#21542;&#21487;&#20197;&#26356;&#39640;&#25928;&#12289;&#26356;&#32463;&#27982;&#22320;&#36827;&#34892;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#24418;&#25991;&#26412;&#29305;&#27931;&#20234;&#25915;&#20987;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;&#34920;&#31034;-&#36923;&#36753;&#29305;&#27931;&#20234;&#25554;&#20837;&#65288;RLI&#65289;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Natural Language Processing (NLP), intelligent neuron models can be susceptible to textual Trojan attacks. Such attacks occur when Trojan models behave normally for standard inputs but generate malicious output for inputs that contain a specific trigger. Syntactic-structure triggers, which are invisible, are becoming more popular for Trojan attacks because they are difficult to detect and defend against. However, these types of attacks require a large corpus of training data to generate poisoned samples with the necessary syntactic structures for Trojan insertion. Obtaining such data can be difficult for attackers, and the process of generating syntactic poisoned triggers and inserting Trojans can be time-consuming. This paper proposes a solution called TrojText, which aims to determine whether invisible textual Trojan attacks can be performed more efficiently and cost-effectively without training data. The proposed approach, called the Representation-Logit Trojan Insertion (RLI) al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.09767</link><description>&lt;p&gt;
Truveta Mapper&#65306;&#19968;&#20010;&#38646;&#26679;&#26412;&#26412;&#20307;&#26144;&#23556;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Truveta Mapper: A Zero-shot Ontology Alignment Framework. (arXiv:2301.09767v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09767
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;&#25110;&#26412;&#20307;&#23545;&#40784;&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#30340;Truveta Mapper&#26694;&#26550;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#26174;&#24335;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26080;&#30417;&#30563;&#26412;&#20307;&#21305;&#37197;(Ontology Matching, OM)&#25110;&#26412;&#20307;&#23545;&#40784;(Ontology Alignment, OA)&#35270;&#20026;&#32763;&#35793;&#20219;&#21153;&#30340;&#26032;&#35270;&#35282;&#12290;&#23558;&#26412;&#20307;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#22312;&#28304;&#26412;&#20307;&#22270;&#20013;&#30340;&#33410;&#28857;&#21040;&#30446;&#26631;&#26412;&#20307;&#22270;&#20013;&#30340;&#36335;&#24452;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#12290;&#25152;&#25552;&#20986;&#30340;Truveta Mapper (TM)&#26694;&#26550;&#21033;&#29992;&#22810;&#20219;&#21153;&#24207;&#21015;&#21040;&#24207;&#21015;&#36716;&#25442;&#22120;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#12289;&#32479;&#19968;&#21644;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#19979;&#25191;&#34892;&#22810;&#26412;&#20307;&#23545;&#40784;&#12290;&#22810;&#20219;&#21153;&#20351;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#26469;&#38544;&#21547;&#22320;&#23398;&#20064;&#19981;&#21516;&#26412;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#36328;&#26412;&#20307;&#25163;&#21160;&#26631;&#27880;&#25968;&#25454;&#12290;&#36825;&#20063;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#24310;&#36831;&#21644;&#23545;&#40784;&#36136;&#37327;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#27169;&#22411;&#20165;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#21644;&#20869;&#37096;&#26412;&#20307;&#25968;&#25454;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#35813;&#26041;&#26696;&#20248;&#20110;&#29616;&#26377;&#26631;&#20934;&#22522;&#20934;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;Edit-Similarity&#21644;MINTE+&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, a new perspective is suggested for unsupervised Ontology Matching (OM) or Ontology Alignment (OA) by treating it as a translation task. Ontologies are represented as graphs, and the translation is performed from a node in the source ontology graph to a path in the target ontology graph. The proposed framework, Truveta Mapper (TM), leverages a multi-task sequence-to-sequence transformer model to perform alignment across multiple ontologies in a zero-shot, unified and end-to-end manner. Multi-tasking enables the model to implicitly learn the relationship between different ontologies via transfer-learning without requiring any explicit cross-ontology manually labeled data. This also enables the formulated framework to outperform existing solutions for both runtime latency and alignment quality. The model is pre-trained and fine-tuned only on publicly available text corpus and inner-ontologies data. The proposed solution outperforms state-of-the-art approaches, Edit-Similari
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#27979;&#24230;&#35770;&#30340;&#26041;&#27861;&#23545;&#35821;&#35328;&#24314;&#27169;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#35777;&#26126;&#20102;&#24456;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#26159;&#32039;&#23494;&#30340;&#65292;&#19981;&#20250;&#23384;&#22312;&#27844;&#28431;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2212.10502</link><description>&lt;p&gt;
&#19968;&#31181;&#27979;&#24230;&#35770;&#30340;&#32039;&#23494;&#35821;&#35328;&#27169;&#22411;&#29305;&#24449;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Measure-Theoretic Characterization of Tight Language Models. (arXiv:2212.10502v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#27979;&#24230;&#35770;&#30340;&#26041;&#27861;&#23545;&#35821;&#35328;&#24314;&#27169;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#65292;&#35777;&#26126;&#20102;&#24456;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#26159;&#32039;&#23494;&#30340;&#65292;&#19981;&#20250;&#23384;&#22312;&#27844;&#28431;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#26159;&#35821;&#35328;&#24314;&#27169;&#65292;&#28041;&#21450;&#23545;&#23383;&#31526;&#20018;&#30340;&#27010;&#29575;&#20998;&#24067;&#36827;&#34892;&#20272;&#35745;&#12290;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#20272;&#35745;&#30340;&#20998;&#24067;&#22312;&#25152;&#26377;&#26377;&#38480;&#23383;&#31526;&#20018;&#19978;&#27714;&#21644;&#31561;&#20110;1&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#30149;&#24577;&#24773;&#20917;&#19979;&#65292;&#27010;&#29575;&#36136;&#37327;&#21487;&#33021;&#8220;&#27844;&#28431;&#8221;&#21040;&#26080;&#38480;&#24207;&#21015;&#38598;&#21512;&#19978;&#12290;&#20026;&#20102;&#26356;&#31934;&#30830;&#22320;&#21051;&#30011;&#27844;&#28431;&#30340;&#27010;&#24565;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#24230;&#35770;&#30340;&#35821;&#35328;&#24314;&#27169;&#22788;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35768;&#22810;&#27969;&#34892;&#30340;&#35821;&#35328;&#27169;&#22411;&#23478;&#26063;&#23454;&#38469;&#19978;&#26159;&#32039;&#23494;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#23427;&#20204;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#19981;&#20250;&#27844;&#28431;&#12290;&#25105;&#20204;&#36824;&#25512;&#24191;&#20102;&#20043;&#21069;&#24037;&#20316;&#20013;&#25552;&#20986;&#30340;&#32039;&#23494;&#24615;&#29305;&#24449;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can ``leak'' onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25512;&#24191;&#20102;&#31038;&#20250;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#20174;&#35821;&#35328;&#23884;&#20837;&#25193;&#23637;&#21040;&#20102;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#19982;&#26410;&#32463;&#22521;&#35757;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#21516;&#31561;&#37325;&#35201;&#29978;&#33267;&#26356;&#37325;&#35201;&#12290;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#30740;&#31350;&#20559;&#35265;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2002.08911</link><description>&lt;p&gt;
&#22312;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#23884;&#20837;&#20013;&#27979;&#37327;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Measuring Social Biases in Grounded Vision and Language Embeddings. (arXiv:2002.08911v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.08911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25512;&#24191;&#20102;&#31038;&#20250;&#20559;&#35265;&#30340;&#27010;&#24565;&#65292;&#20174;&#35821;&#35328;&#23884;&#20837;&#25193;&#23637;&#21040;&#20102;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#19982;&#26410;&#32463;&#22521;&#35757;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#21516;&#31561;&#37325;&#35201;&#29978;&#33267;&#26356;&#37325;&#35201;&#12290;&#24182;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#26469;&#30740;&#31350;&#20559;&#35265;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;&#20043;&#38388;&#30340;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#31038;&#20250;&#20559;&#35265;&#30340;&#27010;&#24565;&#20174;&#35821;&#35328;&#23884;&#20837;&#25512;&#24191;&#21040;&#20102;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#30340;&#23884;&#20837;&#20013;&#12290;&#23384;&#22312;&#20110;&#22522;&#20110;&#22270;&#20687;&#21644;&#35821;&#35328;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#20284;&#20046;&#19982;&#26410;&#32463;&#22521;&#35757;&#30340;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#21516;&#31561;&#29978;&#33267;&#26356;&#20026;&#37325;&#35201;&#12290;&#23613;&#31649;&#35270;&#35273;&#21644;&#35821;&#35328;&#21487;&#33021;&#21463;&#21040;&#19981;&#21516;&#30340;&#20559;&#35265;&#65292;&#20154;&#20204;&#21487;&#33021;&#24076;&#26395;&#36825;&#20123;&#20559;&#35265;&#21487;&#20197;&#30456;&#20114;&#34928;&#20943;&#65292;&#20294;&#23454;&#38469;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#31181;&#27867;&#21270;&#24230;&#37327;&#23884;&#20837;&#20013;&#30340;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#27867;&#21270;&#31354;&#38388;&#65288;Grounded-WEAT&#21644;Grounded-SEAT&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#27867;&#21270;&#26041;&#27861;&#23545;&#20110;&#20559;&#35265;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;&#20132;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#20855;&#26377;&#19981;&#21516;&#30340;&#22238;&#31572;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#26041;&#27861;&#22312;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#22312;COCO&#12289;&#27010;&#24565;&#23383;&#24149;&#21644;&#35895;&#27468;&#22270;&#20687;&#31561;&#26631;&#20934;&#35821;&#35328;&#20559;&#35265;&#22522;&#20934;&#19978;&#22686;&#21152;10,228&#24352;&#22270;&#20687;&#26469;&#26500;&#24314;&#12290;&#25968;&#25454;&#38598;&#30340;&#26500;&#24314;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#35270;&#35273;&#25968;&#25454;&#38598;&#26412;&#36523;&#23601;&#23384;&#22312;&#24456;&#22823;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
We generalize the notion of social biases from language embeddings to grounded vision and language embeddings. Biases are present in grounded embeddings, and indeed seem to be equally or more significant than for ungrounded embeddings. This is despite the fact that vision and language can suffer from different biases, which one might hope could attenuate the biases in both. Multiple ways exist to generalize metrics measuring bias in word embeddings to this new setting. We introduce the space of generalizations (Grounded-WEAT and Grounded-SEAT) and demonstrate that three generalizations answer different yet important questions about how biases, language, and vision interact. These metrics are used on a new dataset, the first for grounded bias, created by augmenting extending standard linguistic bias benchmarks with 10,228 images from COCO, Conceptual Captions, and Google Images. Dataset construction is challenging because vision datasets are themselves very biased. The presence of these
&lt;/p&gt;</description></item></channel></rss>