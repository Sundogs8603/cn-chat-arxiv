<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07869</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Neural Machine Translation For Low Resource Languages. (arXiv:2304.07869v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#31561; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#27969;&#21160;&#24615;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#20960;&#31181;&#35821;&#35328;&#23545;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793; (MNMT) &#39046;&#22495;&#30475;&#21040;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#21364;&#27809;&#26377;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#20197;&#30830;&#23450;&#21738;&#20123;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#12290;&#35813;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#39046;&#22495;&#65292;&#24182;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#35813;&#39033;&#30446;&#26088;&#22312;&#24314;&#31435;&#22312; \texttt{mBART.CC25} &#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#65292;&#24182;&#25506;&#32034;&#21033;&#29992;&#21508;&#31181; NLP &#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#21518;&#21521;&#32763;&#35793;&#21644;&#36801;&#31227;&#23398;&#20064;&#65289;&#26469;&#22686;&#24378;&#23427;&#30340;&#31574;&#30053;&#12290;&#35813;&#23454;&#29616;&#35797;&#22270;&#35299;&#24320; NMT &#24212;&#29992;&#31243;&#24207;&#30340;&#26550;&#26500;&#65292;&#24182;&#30830;&#23450;&#19981;&#21516;&#30340;&#32452;&#20214;&#65292;&#36825;&#20123;&#32452;&#20214;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#20462;&#25913;&#25152;&#36848;&#24212;&#29992;&#31243;&#24207;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Machine translation is a challenging task due to the inherent complex nature and the fluidity that natural languages bring. Nonetheless, in recent years, it has achieved state-of-the-art performance in several language pairs. Although, a lot of traction can be seen in the areas of multilingual neural machine translation (MNMT) in the recent years, there are no comprehensive survey done to identify what approaches work well. The goal of this project is to investigate the realm of low resource languages and build a Neural Machine Translation model to achieve state-of-the-art results. The project looks to build upon the \texttt{mBART.CC25} \cite{liu2020multilingual} language model and explore strategies to augment it with various NLP and Deep Learning techniques like back translation and transfer learning. This implementation tries to unpack the architecture of the NMT application and determine the different components which offers us opportunities to amend the said application wit
&lt;/p&gt;</description></item><item><title>VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;</title><link>http://arxiv.org/abs/2304.07810</link><description>&lt;p&gt;
VISAR&#65306;&#19968;&#31181;&#24102;&#26377;&#21487;&#35270;&#21270;&#32534;&#31243;&#21644;&#24555;&#36895;&#33609;&#26696;&#21407;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#35770;&#35777;&#20889;&#20316;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
VISAR: A Human-AI Argumentative Writing Assistant with Visual Programming and Rapid Draft Prototyping. (arXiv:2304.07810v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07810
&lt;/p&gt;
&lt;p&gt;
VISAR&#26159;&#19968;&#20010;AI&#20889;&#20316;&#21161;&#25163;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#25552;&#21319;&#20889;&#20316;&#20307;&#39564;&#21644;&#36755;&#20986;&#12290;&#23427;&#21487;&#20197;&#22312;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#38543;&#26102;&#24110;&#21161;&#20316;&#32773;&#26500;&#24605;&#21644;&#20462;&#25913;&#30446;&#26631;&#65292;&#36890;&#36807;&#21487;&#35270;&#21270;&#32534;&#31243;&#26469;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#25512;&#33616;&#26469;&#22686;&#21152;&#35828;&#26381;&#21147;&#12290;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#21487;&#20197;&#29992;&#26469;&#39564;&#35777;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36777;&#35770;&#20889;&#20316;&#20013;&#65292;&#20316;&#32773;&#24517;&#39035;&#26500;&#24605;&#20998;&#23618;&#20889;&#20316;&#30446;&#26631;&#65292;&#30830;&#20445;&#20854;&#35770;&#28857;&#30340;&#35828;&#26381;&#21147;&#65292;&#24182;&#36890;&#36807;&#36215;&#33609;&#26469;&#20462;&#35746;&#21644;&#32452;&#32455;&#20182;&#20204;&#30340;&#35745;&#21010;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#36890;&#36807;&#32842;&#22825;&#30028;&#38754;&#36827;&#34892;&#20132;&#20114;&#24335;&#25991;&#26412;&#29983;&#25104;&#65288;&#20363;&#22914;ChatGPT&#65289;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24120;&#24120;&#24573;&#30053;&#20102;&#38544;&#21547;&#30340;&#20889;&#20316;&#19978;&#19979;&#25991;&#21644;&#29992;&#25143;&#24847;&#22270;&#65292;&#32570;&#20047;&#29992;&#25143;&#25511;&#21046;&#21644;&#33258;&#20027;&#26435;&#65292;&#24182;&#19988;&#25552;&#20379;&#26377;&#38480;&#30340;&#24110;&#21161;&#26469;&#36827;&#34892;&#24847;&#20041;&#26500;&#24314;&#21644;&#20462;&#35746;&#20889;&#20316;&#35745;&#21010;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;VISAR&#65292;&#19968;&#31181;AI&#25903;&#25345;&#30340;&#20889;&#20316;&#21161;&#25163;&#31995;&#32479;&#65292;&#26088;&#22312;&#24110;&#21161;&#20316;&#32773;&#22312;&#20854;&#20889;&#20316;&#19978;&#19979;&#25991;&#20013;&#26500;&#24605;&#21644;&#20462;&#35746;&#20998;&#23618;&#30446;&#26631;&#65292;&#36890;&#36807;&#21516;&#27493;&#25991;&#26412;&#32534;&#36753;&#21644;&#21487;&#35270;&#21270;&#32534;&#31243;&#32452;&#32455;&#35770;&#35777;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#35770;&#35777;&#28779;&#33457;&#25512;&#33616;&#22686;&#24378;&#35828;&#26381;&#21147;&#12290;VISAR&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#33258;&#21160;&#33609;&#26696;&#21407;&#22411;&#25506;&#32034;&#12289;&#23454;&#39564;&#21644;&#39564;&#35777;&#20182;&#20204;&#30340;&#20889;&#20316;&#35745;&#21010;&#12290;&#19968;&#20010;&#21463;&#25511;&#23454;&#39564;&#23460;&#30740;&#31350;&#35777;&#23454;&#65292;VISAR&#21487;&#20197;&#36890;&#36807;&#23458;&#35266;&#21644;&#20027;&#35266;&#35780;&#20272;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#29992;&#25143;&#30340;&#20889;&#20316;&#20307;&#39564;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In argumentative writing, writers must brainstorm hierarchical writing goals, ensure the persuasiveness of their arguments, and revise and organize their plans through drafting. Recent advances in large language models (LLMs) have made interactive text generation through a chat interface (e.g., ChatGPT) possible. However, this approach often neglects implicit writing context and user intent, lacks support for user control and autonomy, and provides limited assistance for sensemaking and revising writing plans. To address these challenges, we introduce VISAR, an AI-enabled writing assistant system designed to help writers brainstorm and revise hierarchical goals within their writing context, organize argument structures through synchronized text editing and visual programming, and enhance persuasiveness with argumentation spark recommendations. VISAR allows users to explore, experiment with, and validate their writing plans using automatic draft prototyping. A controlled lab study confi
&lt;/p&gt;</description></item><item><title>EasyNER&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#31471;&#21040;&#31471;&#24037;&#20855;&#12290;&#23427;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23383;&#20856;&#26041;&#27861;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#22312;COVID-19&#30456;&#20851;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.07805</link><description>&lt;p&gt;
EasyNER&#65306;&#19968;&#31181;&#21487;&#23450;&#21046;&#30340;&#26131;&#20110;&#20351;&#29992;&#30340;&#21307;&#23398;&#25991;&#26412;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text. (arXiv:2304.07805v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07805
&lt;/p&gt;
&lt;p&gt;
EasyNER&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#35782;&#21035;&#21629;&#21517;&#23454;&#20307;&#30340;&#31471;&#21040;&#31471;&#24037;&#20855;&#12290;&#23427;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#23383;&#20856;&#26041;&#27861;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#22312;COVID-19&#30456;&#20851;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#30340;&#24212;&#29992;&#35777;&#26126;&#20102;&#20854;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#25152;&#38656;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#30740;&#31350;&#24050;&#32463;&#20135;&#29983;&#20102;&#22823;&#37327;&#20986;&#29256;&#29289;&#65292;PubMed&#25968;&#25454;&#24211;&#24050;&#32463;&#25910;&#24405;&#20102;&#36229;&#36807;3,500&#19975;&#31687;&#30740;&#31350;&#25991;&#31456;&#12290;&#25972;&#21512;&#36825;&#20123;&#20998;&#25955;&#22312;&#22823;&#37327;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#29983;&#29702;&#26426;&#21046;&#21644;&#23548;&#33268;&#26032;&#22411;&#21307;&#23398;&#24178;&#39044;&#30340;&#30142;&#30149;&#36807;&#31243;&#30340;&#20851;&#38190;&#35265;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#25104;&#20026;&#19968;&#20010;&#24040;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#30340;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#36828;&#36828;&#36229;&#20986;&#20102;&#20154;&#31867;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#22312;COVID-19&#22823;&#27969;&#34892;&#30340;&#32039;&#24613;&#24773;&#20917;&#19979;&#65292;&#36825;&#23588;&#20854;&#25104;&#20026;&#38382;&#39064;&#12290;&#33258;&#21160;&#21270;&#25991;&#26412;&#25366;&#25496;&#21487;&#20197;&#24110;&#21161;&#20174;&#22823;&#37327;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#25552;&#21462;&#21644;&#36830;&#25509;&#20449;&#24687;&#12290;&#25991;&#26412;&#25366;&#25496;&#30340;&#31532;&#19968;&#27493;&#36890;&#24120;&#26159;&#35782;&#21035;&#29305;&#23450;&#31867;&#21035;&#30340;&#20851;&#38190;&#23383;&#65288;&#20363;&#22914;&#25152;&#26377;&#34507;&#30333;&#36136;&#25110;&#30142;&#30149;&#21517;&#31216;&#65289;&#65292;&#21363;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;NER&#24037;&#20855;EasyNER&#65292;&#29992;&#20110;&#35782;&#21035;&#21307;&#23398;&#30740;&#31350;&#25991;&#31456;&#20013;&#30340;&#20856;&#22411;&#23454;&#20307;&#65292;&#21253;&#25324;&#30142;&#30149;&#21517;&#31216;&#12289;&#33647;&#29289;&#21517;&#31216;&#21644;&#34507;&#30333;&#36136;&#21517;&#31216;&#12290;EasyNER&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#22522;&#20110;&#23383;&#20856;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#32463;&#39564;&#27700;&#24179;&#30340;&#30740;&#31350;&#20154;&#21592;&#26131;&#20110;&#20351;&#29992;&#21644;&#23450;&#21046;&#12290;&#25105;&#20204;&#23558;EasyNER&#24212;&#29992;&#20110;COVID-19&#30456;&#20851;&#25991;&#31456;&#30340;&#25968;&#25454;&#38598;&#20013;&#24182;&#23637;&#31034;&#23427;&#21487;&#20197;&#20934;&#30830;&#22320;&#35782;&#21035;&#24863;&#20852;&#36259;&#30340;&#23454;&#20307;&#65292;&#20026;&#19979;&#28216;&#20998;&#26512;&#25552;&#20379;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical research generates a large number of publications with the PubMed database already containing &gt;35 million research articles. Integration of the knowledge scattered across this large body of literature could provide key insights into physiological mechanisms and disease processes leading to novel medical interventions. However, it is a great challenge for researchers to utilize this information in full since the scale and complexity of the data greatly surpasses human processing abilities. This becomes especially problematic in cases of extreme urgency like the COVID-19 pandemic. Automated text mining can help extract and connect information from the large body of medical research articles. The first step in text mining is typically the identification of specific classes of keywords (e.g., all protein or disease names), so called Named Entity Recognition (NER). Here we present an end-to-end pipeline for NER of typical entities found in medical research articles, including diseas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#26102;&#38388;&#21644;&#20027;&#39064;&#32972;&#26223;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;</title><link>http://arxiv.org/abs/2304.07781</link><description>&lt;p&gt;
&#25991;&#26412;&#23884;&#20837;&#31934;&#20934;&#26816;&#27979;&#20551;&#26032;&#38395;
&lt;/p&gt;
&lt;p&gt;
It's All in the Embedding! Fake News Detection Using Document Embeddings. (arXiv:2304.07781v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07781
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#26102;&#38388;&#21644;&#20027;&#39064;&#32972;&#26223;&#65292;&#21487;&#26377;&#25928;&#26816;&#27979;&#20551;&#26032;&#38395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23186;&#20307;&#30340;&#25968;&#23383;&#21270;&#36827;&#31243;&#21644;&#31038;&#20132;&#23186;&#20307;&#30340;&#20852;&#36215;&#65292;&#20010;&#24615;&#21270;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#26032;&#30340;&#24120;&#24577;&#12290;&#28982;&#32780;&#65292;&#25968;&#23383;&#21270;&#36827;&#31243;&#22686;&#21152;&#20102;&#27969;&#20256;&#20551;&#20449;&#24687;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#21464;&#24418;&#20449;&#24687;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#20551;&#26032;&#38395;&#36825;&#19968;&#26377;&#23475;&#29616;&#35937;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#25197;&#26354;&#20844;&#20247;&#23545;&#29305;&#23450;&#35805;&#39064;&#30340;&#30475;&#27861;&#65292;&#24182;&#19988;&#32570;&#20047;&#20256;&#32479;&#26032;&#38395;&#30340;&#20005;&#35880;&#24615;&#12290;&#20026;&#20102;&#24320;&#21457;&#26377;&#25928;&#30340;&#24037;&#20855;&#26469;&#26816;&#27979;&#20551;&#26032;&#38395;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25991;&#26412;&#23884;&#20837;&#30340;&#20551;&#26032;&#38395;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#26032;&#38395;&#25991;&#31456;&#30340;&#26102;&#38388;&#21644;&#20027;&#39064;&#32972;&#26223;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22343;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#36229;&#36234;&#20102;&#22522;&#32447;&#27169;&#22411;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the current shift in the mass media landscape from journalistic rigor to social media, personalized social media is becoming the new norm. Although the digitalization progress of the media brings many advantages, it also increases the risk of spreading disinformation, misinformation, and malformation through the use of fake news. The emergence of this harmful phenomenon has managed to polarize society and manipulate public opinion on particular topics, e.g., elections, vaccinations, etc. Such information propagated on social media can distort public perceptions and generate social unrest while lacking the rigor of traditional journalism. Natural Language Processing and Machine Learning techniques are essential for developing efficient tools that can detect fake news. Models that use the context of textual data are essential for resolving the fake news detection problem, as they manage to encode linguistic features within the vector representation of words. In this paper, we propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#12298;&#22235;&#24211;&#20840;&#20070;&#12299;&#35821;&#26009;&#24211;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;SikuGPT&#65292;&#20854;&#22312;&#22788;&#29702;&#21476;&#31821;&#26102;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#21476;&#31821;&#20449;&#24687;&#21644;&#20013;&#22269;&#21476;&#20195;&#25991;&#21270;&#30340;&#22269;&#38469;&#20256;&#25773;&#12290;</title><link>http://arxiv.org/abs/2304.07778</link><description>&lt;p&gt;
SikuGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#25968;&#23383;&#20154;&#25991;&#23398;&#21476;&#31821;&#26234;&#33021;&#21270;&#20449;&#24687;&#22788;&#29702;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#65288;arXiv&#65306;2304.07778v1 [cs.CL]&#65289;
&lt;/p&gt;
&lt;p&gt;
SikuGPT: A Generative Pre-trained Model for Intelligent Information Processing of Ancient Texts from the Perspective of Digital Humanities. (arXiv:2304.07778v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07778
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#12298;&#22235;&#24211;&#20840;&#20070;&#12299;&#35821;&#26009;&#24211;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;SikuGPT&#65292;&#20854;&#22312;&#22788;&#29702;&#21476;&#31821;&#26102;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#26377;&#21161;&#20110;&#20419;&#36827;&#21476;&#31821;&#20449;&#24687;&#21644;&#20013;&#22269;&#21476;&#20195;&#25991;&#21270;&#30340;&#22269;&#38469;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20419;&#36827;&#20102;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#30340;&#32321;&#33635;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#65292;&#38656;&#35201;&#36716;&#21464;&#30740;&#31350;&#26041;&#27861;&#65292;&#20197;&#36866;&#24212;AIGC&#28010;&#28526;&#19979;&#25968;&#23383;&#20154;&#25991;&#30740;&#31350;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;--&#21476;&#31821;&#26234;&#33021;&#21270;&#22788;&#29702;&#30340;&#26032;&#21457;&#23637;&#36235;&#21183;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#12298;&#22235;&#24211;&#20840;&#20070;&#12299;&#35821;&#26009;&#24211;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SikuGPT&#30340;GPT&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#22312;&#35832;&#22914;&#35821;&#35328;&#20869;&#32763;&#35793;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#20854;&#20182;&#38754;&#21521;&#21476;&#31821;&#22788;&#29702;&#30340;GPT&#31867;&#22411;&#27169;&#22411;&#12290;SikuGPT&#22788;&#29702;&#20256;&#32479;&#27721;&#35821;&#21476;&#31821;&#30340;&#33021;&#21147;&#26377;&#21161;&#20110;&#20419;&#36827;&#21476;&#20195;&#20449;&#24687;&#21644;&#30693;&#35782;&#26381;&#21153;&#30340;&#32452;&#32455;&#20197;&#21450;&#20013;&#22269;&#21476;&#20195;&#25991;&#21270;&#30340;&#22269;&#38469;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advance in artificial intelligence technology has facilitated the prosperity of digital humanities research. Against such backdrop, research methods need to be transformed in the intelligent processing of ancient texts, which is a crucial component of digital humanities research, so as to adapt to new development trends in the wave of AIGC. In this study, we propose a GPT model called SikuGPT based on the corpus of Siku Quanshu. The model's performance in tasks such as intralingual translation and text classification exceeds that of other GPT-type models aimed at processing ancient texts. SikuGPT's ability to process traditional Chinese ancient texts can help promote the organization of ancient information and knowledge services, as well as the international dissemination of Chinese ancient culture.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21477;&#23376;&#20013;&#30340;&#23454;&#38469;&#20449;&#24687;&#19977;&#20803;&#32452;&#31616;&#21270;&#21477;&#23376;&#65292;&#20197;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#30340;&#21019;&#24314;&#12290;</title><link>http://arxiv.org/abs/2304.07774</link><description>&lt;p&gt;
&#25511;&#21046;&#35821;&#27861;&#31616;&#21270;&#30340;&#21477;&#27861;&#22797;&#26434;&#24615;&#37492;&#21035;&#12289;&#24230;&#37327;&#21644;&#20943;&#23569;
&lt;/p&gt;
&lt;p&gt;
Syntactic Complexity Identification, Measurement, and Reduction Through Controlled Syntactic Simplification. (arXiv:2304.07774v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#25511;&#21046;&#31616;&#21270;&#26041;&#27861;&#65292;&#21487;&#22312;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#21477;&#23376;&#20013;&#30340;&#23454;&#38469;&#20449;&#24687;&#19977;&#20803;&#32452;&#31616;&#21270;&#21477;&#23376;&#65292;&#20197;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#30340;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#31616;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#39046;&#22495;&#65292;&#21487;&#20197;&#36890;&#36807;&#31616;&#21270;&#26041;&#24335;&#25506;&#32034;&#26356;&#26131;&#25026;&#30340;&#25991;&#26412;&#12290;&#20294;&#26159;&#65292;&#20102;&#35299;&#24182;&#20174;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#20013;&#25552;&#21462;&#30693;&#35782;&#36890;&#24120;&#24456;&#38590;&#65292;&#22240;&#20026;&#23427;&#36890;&#24120;&#37319;&#29992;&#22797;&#21512;&#21477;&#21644;&#22797;&#26434;&#21477;&#24335;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#21477;&#23376;&#20197;&#25552;&#39640;&#21487;&#35835;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#31616;&#21333;&#30340;&#33521;&#35821;&#26367;&#25442;&#35789;&#21644;&#25688;&#35201;&#21477;&#23376;&#21644;&#27573;&#33853;&#12290;&#20294;&#26159;&#65292;&#22312;&#20174;&#32467;&#26500;&#21270;&#30340;&#25991;&#26412;&#20013;&#21019;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#36807;&#31243;&#20013;&#65292;&#25688;&#35201;&#38271;&#21477;&#23376;&#21644;&#26367;&#25442;&#35789;&#26159;&#19981;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#36825;&#21487;&#33021;&#23548;&#33268;&#20449;&#24687;&#20002;&#22833;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#21477;&#23376;&#20013;&#30340;&#23454;&#38469;&#20449;&#24687;&#19977;&#20803;&#32452;&#30340;&#25511;&#21046;&#31616;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#21477;&#27861;&#20381;&#23384;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Text simplification is one of the domains in Natural Language Processing (NLP) that offers an opportunity to understand the text in a simplified manner for exploration. However, it is always hard to understand and retrieve knowledge from unstructured text, which is usually in the form of compound and complex sentences. There are state-of-the-art neural network-based methods to simplify the sentences for improved readability while replacing words with plain English substitutes and summarising the sentences and paragraphs. In the Knowledge Graph (KG) creation process from unstructured text, summarising long sentences and substituting words is undesirable since this may lead to information loss. However, KG creation from text requires the extraction of all possible facts (triples) with the same mentions as in the text. In this work, we propose a controlled simplification based on the factual information in a sentence, i.e., triple. We present a classical syntactic dependency-based approac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#30340;&#22797;&#21046;&#26426;&#21046;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#39046;&#22495;&#22312;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#23558;&#22797;&#21046;&#26426;&#21046;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;NMT&#30340;SPARQL&#29983;&#25104;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28155;&#21152;&#22797;&#21046;&#26426;&#21046;&#25110;&#20351;&#29992;&#38382;&#39064;&#27880;&#37322;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20307;&#31995;&#32467;&#26500;MisRoB{\AE}RTa&#65292;&#29992;&#20110;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#65292;&#24182;&#22312;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2304.07759</link><description>&lt;p&gt;
MisRoB{\AE}RTa&#65306;&#21464;&#24418;&#37329;&#21018;&#23545;&#25239;&#19981;&#23454;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
MisRoB{\AE}RTa: Transformers versus Misinformation. (arXiv:2304.07759v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20307;&#31995;&#32467;&#26500;MisRoB{\AE}RTa&#65292;&#29992;&#20110;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#65292;&#24182;&#22312;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23454;&#20449;&#24687;&#34987;&#35748;&#20026;&#26159;&#25105;&#20204;&#27665;&#20027;&#30340;&#20215;&#20540;&#35266;&#21644;&#21407;&#21017;&#30340;&#23041;&#32961;&#12290;&#36825;&#31181;&#20869;&#23481;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20256;&#25773;&#20250;&#20351;&#31038;&#20250;&#26497;&#31471;&#21270;&#65292;&#24182;&#36890;&#36807;&#25197;&#26354;&#20844;&#20247;&#30340;&#30475;&#27861;&#24182;&#24341;&#21457;&#31038;&#20250;&#21160;&#33633;&#32780;&#30772;&#22351;&#20844;&#20849;&#35805;&#35821;&#65292;&#21516;&#26102;&#32570;&#20047;&#20256;&#32479;&#26032;&#38395;&#30340;&#20005;&#35880;&#24615;&#12290;&#22312;&#22810;&#20010;&#20855;&#26377;&#30693;&#21517;&#24230;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;Transformer&#21644;&#36801;&#31227;&#23398;&#20064;&#34987;&#35777;&#26126;&#26159;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MisRoB{\AE}RTa&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#12290;MisRoB{\AE}RTa&#21033;&#29992;&#20102;&#20004;&#20010;Transformer&#65288;BART&#21644;RoBERTa&#65289;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#23545;&#22810;&#20010;Transformer&#22312;&#19981;&#23454;&#20449;&#24687;&#26816;&#27979;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#12290;&#23545;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#24102;&#26377;10&#20010;&#31867;&#21035;&#26631;&#31614;&#30340;&#22823;&#22411;&#29616;&#23454;&#19990;&#30028;&#26032;&#38395;&#25991;&#31456;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;&#30740;&#31350;&#20013;&#30340;&#20004;&#20010;&#32570;&#28857;&#65306;&#23558;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#20174;&#23567;&#21040;&#22823;&#65292;&#24182;&#23558;&#28966;&#28857;&#20174;&#31038;&#20132;&#23186;&#20307;&#31227;&#21160;&#21040;&#26032;&#38395;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Misinformation is considered a threat to our democratic values and principles. The spread of such content on social media polarizes society and undermines public discourse by distorting public perceptions and generating social unrest while lacking the rigor of traditional journalism. Transformers and transfer learning proved to be state-of-the-art methods for multiple well-known natural language processing tasks. In this paper, we propose MisRoB{\AE}RTa, a novel transformer-based deep neural ensemble architecture for misinformation detection. MisRoB{\AE}RTa takes advantage of two transformers (BART \&amp; RoBERTa) to improve the classification performance. We also benchmarked and evaluated the performances of multiple transformers on the task of misinformation detection. For training and testing, we used a large real-world news articles dataset labeled with 10 classes, addressing two shortcomings in the current research: increasing the size of the dataset from small to large, and moving th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;USNID&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#26377;&#38480;&#25110;&#26080;&#26631;&#35760;&#25968;&#25454;&#26102;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#32858;&#31867;&#26426;&#21046;&#26469;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;</title><link>http://arxiv.org/abs/2304.07699</link><description>&lt;p&gt;
USNID: &#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26032;&#24847;&#22270;&#21457;&#29616;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
USNID: A Framework for Unsupervised and Semi-supervised New Intent Discovery. (arXiv:2304.07699v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;USNID&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#30340;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#21033;&#29992;&#26377;&#38480;&#25110;&#26080;&#26631;&#35760;&#25968;&#25454;&#26102;&#38590;&#20197;&#25429;&#25417;&#22797;&#26434;&#35821;&#20041;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#32858;&#31867;&#26426;&#21046;&#26469;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#30446;&#26631;&#30340;&#36136;&#37327;&#65292;&#20174;&#32780;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#24847;&#22270;&#21457;&#29616;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#20351;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#29992;&#25143;&#38656;&#27714;&#24182;&#25552;&#20379;&#21451;&#22909;&#30340;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#38480;&#25110;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#31163;&#25955;&#25991;&#26412;&#34920;&#31034;&#30340;&#22797;&#26434;&#35821;&#20041;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;USNID&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#26032;&#24847;&#22270;&#21457;&#29616;&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#25216;&#26415;&#65306;&#20805;&#20998;&#21033;&#29992;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#25968;&#25454;&#25366;&#25496;&#27973;&#23618;&#35821;&#20041;&#30456;&#20284;&#24615;&#20851;&#31995;&#65307;&#35774;&#35745;&#32858;&#31867;&#26426;&#21046;&#35299;&#20915;&#31751;&#20998;&#37197;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65307;&#25429;&#33719;&#26080;&#30417;&#30563;&#25110;&#21322;&#30417;&#30563;&#25968;&#25454;&#20013;&#30340;&#39640;&#32423;&#35821;&#20041;&#65292;&#36890;&#36807;&#21516;&#26102;&#20248;&#21270;&#32858;&#31867;&#21644;&#33258;&#25105;&#30417;&#30563;&#26469;&#21457;&#29616;&#32454;&#31890;&#24230;&#30340;&#24847;&#22270;&#31751;&#12290;
&lt;/p&gt;
&lt;p&gt;
New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discrete text representations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel framework called USNID for unsupervised and semi-supervised new intent discovery, which has three key technologies. First, it takes full use of unsupervised or semi-supervised data to mine shallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07687</link><description>&lt;p&gt;
MLRegTest&#65306;&#26426;&#22120;&#23398;&#20064;&#27491;&#21017;&#35821;&#35328;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MLRegTest: A Benchmark for the Machine Learning of Regular Languages. (arXiv:2304.07687v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#21253;&#21547;&#20102;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#27979;&#35797;&#26681;&#25454;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#36923;&#36753;&#25991;&#23383;&#31181;&#31867;&#32452;&#32455;&#35821;&#35328;&#65292;&#24182;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#23398;&#20064;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#24050;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#33021;&#21147;&#20801;&#35768;&#32454;&#33268;&#22320;&#26816;&#26597;&#23427;&#20204;&#21487;&#20197;&#23398;&#20064;&#21738;&#20123;&#27169;&#24335;&#65292;&#24182;&#22312;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#26410;&#30693;&#20998;&#31867;&#22120;&#30340;&#23398;&#20064;&#26102;&#24314;&#31435;&#20449;&#24515;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MLRegTest&#30340;&#26032;&#30340;&#24207;&#21015;&#20998;&#31867;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#26469;&#33258;1,800&#20010;&#27491;&#21017;&#35821;&#35328;&#30340;&#35757;&#32451;&#12289;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#12290;&#19981;&#21516;&#31867;&#22411;&#30340;&#24418;&#24335;&#35821;&#35328;&#20195;&#34920;&#30528;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#65292;&#24182;&#27491;&#30830;&#22320;&#35782;&#21035;&#24207;&#21015;&#20013;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#26159;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#25104;&#21151;&#27867;&#21270;&#30340;&#24050;&#30693;&#25361;&#25112;&#12290;MLRegTest&#26681;&#25454;&#23427;&#20204;&#30340;&#36923;&#36753;&#22797;&#26434;&#24230;&#65288;&#21333;&#35843;&#20108;&#38454;&#65292;&#19968;&#38454;&#65292;&#21629;&#39064;&#25110;&#21333;&#39033;&#24335;&#34920;&#36798;&#24335;&#65289;&#21644;&#36923;&#36753;&#25991;&#23383;&#30340;&#31181;&#31867;&#65288;&#23383;&#31526;&#20018;&#65292;&#23450;&#32423;&#23383;&#31526;&#20018;&#65292;&#23376;&#24207;&#21015;&#25110;&#20004;&#32773;&#30340;&#32452;&#21512;&#65289;&#32452;&#32455;&#20854;&#35821;&#35328;&#12290;&#36923;&#36753;&#22797;&#26434;&#24230;&#21644;&#25991;&#23383;&#30340;&#36873;&#25321;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#26041;&#27861;&#26469;&#29702;&#35299;&#19981;&#21516;&#31181;&#31867;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#21644;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#22788;&#29702;&#23427;&#20204;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating machine learning (ML) systems on their ability to learn known classifiers allows fine-grained examination of the patterns they can learn, which builds confidence when they are applied to the learning of unknown classifiers. This article presents a new benchmark for ML systems on sequence classification called MLRegTest, which contains training, development, and test sets from 1,800 regular languages.  Different kinds of formal languages represent different kinds of long-distance dependencies, and correctly identifying long-distance dependencies in sequences is a known challenge for ML systems to generalize successfully. MLRegTest organizes its languages according to their logical complexity (monadic second order, first order, propositional, or monomial expressions) and the kind of logical literals (string, tier-string, subsequence, or combinations thereof). The logical complexity and choice of literal provides a systematic way to understand different kinds of long-distance d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ArguGPT&#65292;&#23427;&#26159;&#30001;7&#20010;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35770;&#35777;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#39318;&#27425;&#25509;&#35302;&#26426;&#22120;&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#21482;&#26377;61%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#32463;&#36807;&#19968;&#36718;&#35757;&#32451;&#21518;&#25552;&#39640;&#21040;&#20102;67%&#12290;</title><link>http://arxiv.org/abs/2304.07666</link><description>&lt;p&gt;
ArguGPT&#65306;&#35780;&#20272;&#12289;&#29702;&#35299;&#21644;&#35782;&#21035;&#30001;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35770;&#35777;&#25991;&#31456;
&lt;/p&gt;
&lt;p&gt;
ArguGPT: evaluating, understanding and identifying argumentative essays generated by GPT models. (arXiv:2304.07666v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07666
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;ArguGPT&#65292;&#23427;&#26159;&#30001;7&#20010;GPT&#27169;&#22411;&#29983;&#25104;&#30340;&#35770;&#35777;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#26088;&#22312;&#35299;&#20915;AI&#29983;&#25104;&#20869;&#23481;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#25945;&#24072;&#39318;&#27425;&#25509;&#35302;&#26426;&#22120;&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#21482;&#26377;61%&#30340;&#20934;&#30830;&#24230;&#65292;&#20294;&#32463;&#36807;&#19968;&#36718;&#35757;&#32451;&#21518;&#25552;&#39640;&#21040;&#20102;67%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#65288;AIGC&#65289;&#23545;&#20840;&#29699;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20986;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#25945;&#24072;&#20204;&#38656;&#35201;&#33021;&#22815;&#29992;&#32905;&#30524;&#25110;&#24037;&#20855;&#26816;&#27979;&#20986;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#38656;&#35201;&#26356;&#22810;&#22320;&#20102;&#35299;AIGC&#30340;&#35789;&#27719;&#12289;&#21477;&#27861;&#21644;&#39118;&#26684;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#33521;&#35821;&#25945;&#23398;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;ArguGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;7&#20010;GPT&#27169;&#22411;&#29983;&#25104;&#30340;4038&#31687;&#26377;&#24179;&#34913;&#30340;&#35770;&#35777;&#25991;&#31456;&#35821;&#26009;&#24211;&#65292;&#36825;&#20123;&#35770;&#35777;&#25991;&#31456;&#26159;&#22312;&#20197;&#19979;&#19977;&#20010;&#26469;&#28304;&#30340;&#35770;&#25991;&#25552;&#31034;&#19979;&#29983;&#25104;&#30340;&#65306;&#65288;1&#65289;&#35838;&#22530;&#25110;&#23478;&#24237;&#20316;&#19994;&#32451;&#20064;&#65292;&#65288;2&#65289;&#25176;&#31119;&#21644;&#65288;3&#65289;GRE&#20889;&#20316;&#20219;&#21153;&#12290;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#22823;&#33268;&#30456;&#31561;&#25968;&#37327;&#30340;&#20154;&#24037;&#32534;&#20889;&#30340;&#25991;&#31456;&#37197;&#23545;&#65292;&#36825;&#20123;&#25991;&#31456;&#30340;&#19977;&#20010;&#24471;&#20998;&#32423;&#21035;&#21305;&#37197;&#35770;&#25991;&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38599;&#29992;&#33521;&#35821;&#25945;&#24072;&#26469;&#21306;&#20998;&#26426;&#22120;&#35770;&#25991;&#21644;&#20154;&#24037;&#35770;&#25991;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#25945;&#24072;&#20204;&#39318;&#27425;&#25509;&#35302;&#26426;&#22120;&#29983;&#25104;&#30340;&#35770;&#25991;&#26102;&#65292;&#20182;&#20204;&#20165;&#33021;&#20197;61%&#30340;&#20934;&#30830;&#24230;&#26816;&#27979;&#20986;&#23427;&#20204;&#12290;&#20294;&#32463;&#36807;&#19968;&#36718;&#35757;&#32451;&#21518;&#65292;&#36825;&#20010;&#25968;&#23383;&#25552;&#39640;&#21040;&#20102;67%&#12290;
&lt;/p&gt;
&lt;p&gt;
AI generated content (AIGC) presents considerable challenge to educators around the world. Instructors need to be able to detect such text generated by large language models, either with the naked eye or with the help of some tools. There is also growing need to understand the lexical, syntactic and stylistic features of AIGC. To address these challenges in English language teaching, we first present ArguGPT, a balanced corpus of 4,038 argumentative essays generated by 7 GPT models in response to essay prompts from three sources: (1) in-class or homework exercises, (2) TOEFL and (3) GRE writing tasks. Machine-generated texts are paired with roughly equal number of human-written essays with three score levels matched in essay prompts. We then hire English instructors to distinguish machine essays from human ones. Results show that when first exposed to machine-generated essays, the instructors only have an accuracy of 61% in detecting them. But the number rises to 67% after one round of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;LSTM-based seq2seq&#26550;&#26500;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;OCR&#25216;&#26415;&#19982;&#35789;&#32423;&#32763;&#35793;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25991;&#26723;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07637</link><description>&lt;p&gt;
TransDocs&#65306;&#22522;&#20110;&#35789;&#32423;&#32763;&#35793;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
TransDocs: Optical Character Recognition with word to word translation. (arXiv:2304.07637v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;LSTM-based seq2seq&#26550;&#26500;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;OCR&#25216;&#26415;&#19982;&#35789;&#32423;&#32763;&#35793;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#25991;&#26723;&#36716;&#25442;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;OCR&#25216;&#26415;&#24050;&#32463;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#65292;&#20294;&#20854;&#36755;&#20986;&#24182;&#19981;&#24635;&#26159;&#20934;&#30830;&#30340;&#65292;&#23548;&#33268;&#38169;&#37197;&#23383;&#35789;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36816;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#21892;OCR&#25216;&#26415;&#65292;&#23558;OCR&#25216;&#26415;&#19982;&#22522;&#20110;LSTM&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25972;&#21512;&#20197;&#36827;&#34892;&#25991;&#26723;&#32763;&#35793;&#65292;&#24182;&#22522;&#20110;ANKI&#25968;&#25454;&#38598;&#36827;&#34892;&#33521;&#35821;&#21040;&#35199;&#29677;&#29273;&#35821;&#30340;&#32763;&#35793;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#20351;&#29992;LSTM-based seq2seq&#26550;&#26500;&#21644;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#35757;&#32451;OCR&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#26412;&#30740;&#31350;&#38754;&#21521;&#23545;OCR&#25216;&#26415;&#21450;&#20854;&#22312;&#25991;&#26723;&#32763;&#35793;&#20013;&#24212;&#29992;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
While OCR has been used in various applications, its output is not always accurate, leading to misfit words. This research work focuses on improving the optical character recognition (OCR) with ML techniques with integration of OCR with long short-term memory (LSTM) based sequence to sequence deep learning models to perform document translation. This work is based on ANKI dataset for English to Spanish translation. In this work, I have shown comparative study for pre-trained OCR while using deep learning model using LSTM-based seq2seq architecture with attention for machine translation. End-to-end performance of the model has been expressed in BLEU-4 score. This research paper is aimed at researchers and practitioners interested in OCR and its applications in document translation.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;</title><link>http://arxiv.org/abs/2304.07633</link><description>&lt;p&gt;
&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#21270;&#31070;&#32463;&#27169;&#22411;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#22810;&#27169;&#24577;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;
Detecting Out-of-Context Multimodal Misinformation with interpretable neural-symbolic model. (arXiv:2304.07633v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#19978;&#19979;&#25991;&#19981;&#31526;&#30340;&#34394;&#20551;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#24110;&#21161;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#36827;&#34892;&#35760;&#24405;&#28548;&#28165;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34394;&#20551;&#20449;&#24687;&#30340;&#28436;&#21270;&#25345;&#32493;&#22686;&#38271;&#65292;&#26088;&#22312;&#24433;&#21709;&#20844;&#20247;&#33286;&#35770;&#12290;&#19982;&#20256;&#32479;&#30340;&#35875;&#35328;&#25110;&#34394;&#20551;&#26032;&#38395;&#32534;&#36753;&#20027;&#35201;&#20381;&#36182;&#20110;&#29983;&#25104;&#21644;/&#25110;&#20266;&#36896;&#30340;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#35270;&#39057;&#19981;&#21516;&#65292;&#24403;&#21069;&#30340;&#34394;&#20551;&#20449;&#24687;&#21019;&#20316;&#32773;&#26356;&#20542;&#21521;&#20110;&#20351;&#29992;&#19978;&#19979;&#25991;&#19981;&#21305;&#37197;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#19981;&#21305;&#37197;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#65289;&#26469;&#27450;&#39575;&#20844;&#20247;&#21644;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31995;&#32479;&#12290;&#36825;&#31181;&#26032;&#22411;&#30340;&#34394;&#20551;&#20449;&#24687;&#19981;&#20165;&#22686;&#21152;&#20102;&#26816;&#27979;&#30340;&#38590;&#24230;&#65292;&#20063;&#22686;&#21152;&#20102;&#28548;&#28165;&#30340;&#38590;&#24230;&#65292;&#22240;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#27169;&#24577;&#37117;&#36275;&#22815;&#25509;&#36817;&#30495;&#23454;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#36328;&#27169;&#24577;&#21435;&#19978;&#19979;&#25991;&#26816;&#27979;&#65292;&#21516;&#26102;&#35782;&#21035;&#19981;&#21305;&#37197;&#30340;&#23545;&#21644;&#36328;&#27169;&#24577;&#30683;&#30462;&#65292;&#36825;&#23545;&#20107;&#23454;&#26816;&#26597;&#32593;&#31449;&#30340;&#35760;&#24405;&#28548;&#28165;&#38750;&#24120;&#26377;&#24110;&#21161;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#39318;&#20808;&#36890;&#36807;&#25277;&#35937;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#22522;&#20110;Abstract M&#36827;&#34892;&#31526;&#21495;&#21270;&#20998;&#35299;&#65292;&#24471;&#21040;&#19968;&#32452;&#20107;&#23454;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed the sustained evolution of misinformation that aims at manipulating public opinions. Unlike traditional rumors or fake news editors who mainly rely on generated and/or counterfeited images, text and videos, current misinformation creators now more tend to use out-of-context multimedia contents (e.g. mismatched images and captions) to deceive the public and fake news detection systems. This new type of misinformation increases the difficulty of not only detection but also clarification, because every individual modality is close enough to true information. To address this challenge, in this paper we explore how to achieve interpretable cross-modal de-contextualization detection that simultaneously identifies the mismatched pairs and the cross-modal contradictions, which is helpful for fact-check websites to document clarifications. The proposed model first symbolically disassembles the text-modality information to a set of fact queries based on the Abstract M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#23454;&#20307;&#20013;&#24515;&#30340;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20307;&#27010;&#24565;&#65292;&#25552;&#20986;&#26500;&#24314;&#25353;&#23454;&#20307;&#27010;&#24565;&#24037;&#20316;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26032;&#24605;&#36335;&#65292;&#19988;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07625</link><description>&lt;p&gt;
&#23454;&#20307;&#20013;&#24515;&#20449;&#24687;&#25552;&#21462;&#30340;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Neural Approaches to Entity-Centric Information Extraction. (arXiv:2304.07625v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23454;&#20307;&#20013;&#24515;&#30340;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#23454;&#20307;&#27010;&#24565;&#65292;&#25552;&#20986;&#26500;&#24314;&#25353;&#23454;&#20307;&#27010;&#24565;&#24037;&#20316;&#30340;&#24212;&#29992;&#31243;&#24207;&#30340;&#26032;&#24605;&#36335;&#65292;&#19988;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#23545;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#26377;&#30528;&#24040;&#22823;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#35821;&#38899;&#21161;&#25163;&#12289;&#20154;&#33080;&#35782;&#21035;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#31561;&#31561;&#24212;&#29992;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#21644;&#35821;&#35328;&#23398;&#30340;&#36328;&#23398;&#31185;&#39046;&#22495;&#65292;&#33268;&#21147;&#20110;&#30740;&#31350;&#25991;&#26412;&#29702;&#35299;&#12290;&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#28041;&#21450;&#21040;&#30340;&#19968;&#20010;&#38750;&#24120;&#29305;&#27530;&#30340;&#39046;&#22495;&#65292;&#21363;&#23454;&#20307;&#65288;&#22914;&#20154;&#21517;&#12289;&#32452;&#32455;&#26426;&#26500;&#21517;&#12289;&#22320;&#21517;&#65289;&#22312;&#25991;&#26412;&#20013;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#23436;&#20840;&#19981;&#21516;&#30340;&#65292;&#20197;&#23454;&#20307;&#20026;&#20013;&#24515;&#30340;&#25991;&#26412;&#20449;&#24687;&#35270;&#35282;&#65292;&#25552;&#20986;&#24212;&#35813;&#26500;&#24314;&#33021;&#22815;&#25353;&#23454;&#20307;&#27010;&#24565;&#24037;&#20316;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#20010;&#20307;&#25552;&#21450;&#26469;&#29702;&#35299;&#20854;&#21547;&#20041;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#35814;&#32454;&#30340;&#27169;&#22411;&#65292;&#20171;&#32461;&#20102;&#23454;&#20307;&#20013;&#24515;&#26041;&#27861;&#22914;&#20309;&#29992;&#20110;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23454;&#20307;&#38142;&#25509;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has huge impact on our daily lives with applications such as voice assistants, facial recognition, chatbots, autonomously driving cars, etc. Natural Language Processing (NLP) is a cross-discipline of AI and Linguistics, dedicated to study the understanding of the text. This is a very challenging area due to unstructured nature of the language, with many ambiguous and corner cases. In this thesis we address a very specific area of NLP that involves the understanding of entities (e.g., names of people, organizations, locations) in text. First, we introduce a radically different, entity-centric view of the information in text. We argue that instead of using individual mentions in text to understand their meaning, we should build applications that would work in terms of entity concepts. Next, we present a more detailed model on how the entity-centric approach can be used for the entity linking task. In our work, we show that this task can be improved by conside
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;ChatGPT&#30340;&#39044;&#27979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21270;&#65292;&#34920;&#26126;&#22797;&#26434;&#27169;&#22411;&#21487;&#39044;&#27979;&#33021;&#21147;&#30340;&#23835;&#36215;&#12290;&#36825;&#34920;&#26126;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#24341;&#20837;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07619</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#65311;&#22238;&#25253;&#21487;&#39044;&#27979;&#24615;&#19982;&#22823;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models. (arXiv:2304.07619v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#21457;&#29616;ChatGPT&#30340;&#39044;&#27979;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#65292;&#32780;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#32929;&#31080;&#20215;&#26684;&#21464;&#21270;&#65292;&#34920;&#26126;&#22797;&#26434;&#27169;&#22411;&#21487;&#39044;&#27979;&#33021;&#21147;&#30340;&#23835;&#36215;&#12290;&#36825;&#34920;&#26126;&#22312;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#20013;&#24341;&#20837;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#24182;&#22686;&#24378;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24773;&#24863;&#20998;&#26512;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#30340;&#28508;&#21147;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;ChatGPT&#20197;&#21450;&#20854;&#20182;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#39044;&#27979;&#32929;&#24066;&#22238;&#25253;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21028;&#26029;&#26032;&#38395;&#26631;&#39064;&#23545;&#20844;&#21496;&#32929;&#31080;&#20215;&#26684;&#26159;&#22909;&#28040;&#24687;&#12289;&#22351;&#28040;&#24687;&#25110;&#26080;&#20851;&#28040;&#24687;&#12290;&#36890;&#36807;&#35745;&#31639;&#25968;&#23383;&#20998;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;"ChatGPT&#20998;&#25968;"&#21644;&#38543;&#21518;&#30340;&#26085;&#24120;&#32929;&#31080;&#24066;&#22330;&#22238;&#25253;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#24615;&#12290;&#32780;&#19988;&#65292;ChatGPT&#30340;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24773;&#24863;&#20998;&#26512;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;GPT-1&#12289;GPT-2&#21644;BERT&#31561;&#22522;&#30784;&#27169;&#22411;&#26080;&#27861;&#20934;&#30830;&#39044;&#27979;&#22238;&#25253;&#65292;&#36825;&#34920;&#26126;&#22238;&#25253;&#21487;&#39044;&#27979;&#24615;&#26159;&#22797;&#26434;&#27169;&#22411;&#30340;&#19968;&#31181;&#26032;&#20852;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#32435;&#20837;&#25237;&#36164;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#24182;&#25552;&#39640;&#23450;&#37327;&#20132;&#26131;&#31574;&#30053;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the potential of ChatGPT, and other large language models, in predicting stock market returns using sentiment analysis of news headlines. We use ChatGPT to indicate whether a given headline is good, bad, or irrelevant news for firms' stock prices. We then compute a numerical score and document a positive correlation between these ``ChatGPT scores'' and subsequent daily stock market returns. Further, ChatGPT outperforms traditional sentiment analysis methods. We find that more basic models such as GPT-1, GPT-2, and BERT cannot accurately forecast returns, indicating return predictability is an emerging capacity of complex models. Our results suggest that incorporating advanced language models into the investment decision-making process can yield more accurate predictions and enhance the performance of quantitative trading strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.07499</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19979;&#30340;&#40065;&#26834;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Robust Educational Dialogue Act Classifiers with Low-Resource and Imbalanced Datasets. (arXiv:2304.07499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;&#23545;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#29616;&#20986;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#34892;&#20026;&#21487;&#20197;&#20195;&#34920;&#22312;&#36741;&#23548;&#23545;&#35805;&#26399;&#38388;&#21457;&#29983;&#30340;&#25945;&#32451;&#21592;&#25110;&#23398;&#29983;&#30340;&#23545;&#35805;&#21160;&#20316;&#12290;&#22312;&#25945;&#32946;&#23545;&#35805;&#20013;&#33258;&#21160;&#35782;&#21035;&#23545;&#35805;&#34892;&#20026;&#23545;&#20110;&#22522;&#20110;&#23545;&#35805;&#30340;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#35774;&#35745;&#26159;&#37325;&#35201;&#30340;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#36741;&#23548;&#23545;&#35805;&#20013;&#30340;&#23545;&#35805;&#34892;&#20026;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#20351;&#29992;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#21363;&#20302;&#36164;&#28304;&#25968;&#25454;&#22330;&#26223;&#65289;&#26469;&#20248;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#20998;&#31867;&#20934;&#30830;&#24615;&#20043;&#22806;&#65292;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#20063;&#24456;&#37325;&#35201;&#65292;&#36825;&#21487;&#20197;&#21453;&#26144;&#20998;&#31867;&#22120;&#23398;&#20064;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#30340;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#25945;&#32946;&#23545;&#35805;&#34892;&#20026;&#20998;&#31867;&#30740;&#31350;&#37319;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25439;&#22833;&#26469;&#20248;&#21270;&#20302;&#36164;&#28304;&#25968;&#25454;&#20013;&#30340;DA&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;DA&#20998;&#31867;&#22120;&#24448;&#24448;&#20197;&#29306;&#29298;&#23569;&#25968;&#31867;&#30340;&#20195;&#20215;&#26469;&#20248;&#20808;&#32771;&#34385;&#22823;&#22810;&#25968;&#31867;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21487;&#33021;&#23548;&#33268;&#22312;&#23569;&#25968;&#31867;&#19978;&#24615;&#33021;&#24046;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;DA&#20998;&#31867;&#22120;MIREX&#65292;&#23427;&#37319;&#29992;&#20114;&#20449;&#24687;&#26368;&#23567;&#21270;&#25439;&#22833;&#26469;&#25552;&#39640;DA&#20998;&#31867;&#22120;&#22312;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#25968;&#25454;&#24773;&#26223;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MIREX&#22312;&#19981;&#24179;&#34913;&#21644;&#20302;&#36164;&#28304;&#30340;DA&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue acts (DAs) can represent conversational actions of tutors or students that take place during tutoring dialogues. Automating the identification of DAs in tutoring dialogues is significant to the design of dialogue-based intelligent tutoring systems. Many prior studies employ machine learning models to classify DAs in tutoring dialogues and invest much effort to optimize the classification accuracy by using limited amounts of training data (i.e., low-resource data scenario). However, beyond the classification accuracy, the robustness of the classifier is also important, which can reflect the capability of the classifier on learning the patterns from different class distributions. We note that many prior studies on classifying educational DAs employ cross entropy (CE) loss to optimize DA classifiers on low-resource data with imbalanced DA distribution. The DA classifiers in these studies tend to prioritize accuracy on the majority class at the expense of the minority class which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07438</link><description>&lt;p&gt;
&#21487;&#25805;&#20316;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#29983;&#25104;&#25511;&#21046;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Tractable Control for Autoregressive Language Generation. (arXiv:2304.07438v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#20351;&#29992;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#26469;&#24378;&#21046;&#23454;&#26045;&#38480;&#21046;&#30340;&#25511;&#21046;&#26041;&#27861;GeLaTo&#65292;&#24182;&#21462;&#24471;&#20102;&#22312;&#24120;&#35265;&#30340;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#27979;&#35797;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#29983;&#25104;&#28385;&#36275;&#22797;&#26434;&#38480;&#21046;&#30340;&#25991;&#26412;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65306;&#21363;&#20351;&#26159;&#26368;&#31616;&#21333;&#30340;&#35789;&#27719;&#38480;&#21046;&#20063;&#20351;&#26465;&#20214;&#20998;&#24067;$\Pr(\text{text} | \alpha)$&#30340;&#37319;&#26679;&#21464;&#24471;&#19981;&#21487;&#35745;&#31639;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21487;&#25805;&#20316;&#30340;&#27010;&#29575;&#27169;&#22411;&#23558;&#35789;&#27719;&#38480;&#21046;&#24378;&#21152;&#20110;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; GeLaTo&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#31934;&#31616;&#30340;&#38544;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#26469;&#25511;&#21046;&#20174;GPT2&#21040;&#33258;&#22238;&#24402;&#30340;&#29983;&#25104;&#12290;GeLaTo&#22312;&#32422;&#26463;&#25991;&#26412;&#29983;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;CommonGen&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22823;&#24133;&#20987;&#36133;&#20102;&#21508;&#31181;&#24378;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#19981;&#20165;&#20026;&#25511;&#21046;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24320;&#36767;&#20102;&#26032;&#30340;&#36884;&#24452;&#65292;&#36824;&#28608;&#21169;&#20154;&#20204;&#24320;&#21457;&#26356;&#20855;&#34920;&#29616;&#21147;&#30340;&#21487;&#25805;&#20316;&#27010;&#29575;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of autoregressive large language models in text generation, it remains a major challenge to generate text that satisfies complex constraints: sampling from the conditional distribution $\Pr(\text{text} | \alpha)$ is intractable for even the simplest lexical constraints $\alpha$. To overcome this challenge, we propose to use tractable probabilistic models to impose lexical constraints in autoregressive text generation, which we refer to as GeLaTo. To demonstrate the effectiveness of this framework, we use distilled hidden Markov models to control autoregressive generation from GPT2. GeLaTo achieves state-of-the-art performance on CommonGen, a challenging benchmark for constrained text generation, beating a wide range of strong baselines by a large margin. Our work not only opens up new avenues for controlling large language models but also motivates the development of more expressive tractable probabilistic models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.07396</link><description>&lt;p&gt;
&#25913;&#21892;&#20020;&#24202;&#35797;&#39564;&#30340;&#24739;&#32773;&#39044;&#31579;&#36873;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36741;&#21161;&#21307;&#29983;
&lt;/p&gt;
&lt;p&gt;
Improving Patient Pre-screening for Clinical Trials: Assisting Physicians with Large Language Models. (arXiv:2304.07396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;InstructGPT&#36741;&#21161;&#21307;&#29983;&#39044;&#31579;&#36873;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#12290;&#36890;&#36807;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#30340;&#24615;&#33021;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;LLMs&#22312;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#12289;&#21333;&#29420;&#20998;&#31867;&#12289;&#25972;&#20307;&#20998;&#31867;&#12289;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#21040;&#24739;&#32773;&#30340;&#20020;&#24202;&#35797;&#39564;&#65292;&#21307;&#29983;&#38656;&#35201;&#36827;&#34892;&#32321;&#29712;&#30340;&#26816;&#26597;&#65292;&#20197;&#30830;&#23450;&#24739;&#32773;&#26159;&#21542;&#31526;&#21512;&#25991;&#26412;&#22522;&#20934;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#20020;&#24202;&#20449;&#24687;&#25552;&#21462;&#21644;&#20020;&#24202;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23578;&#26410;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24212;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;InstructGPT&#36741;&#21161;&#21307;&#29983;&#26681;&#25454;&#24739;&#32773;&#30340;&#21307;&#30103;&#31616;&#20917;&#30830;&#23450;&#20854;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#30340;&#36164;&#26684;&#12290;&#20351;&#29992;&#19968;&#27425;&#24615;&#12289;&#36873;&#25321;-&#25512;&#29702;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22312;10&#20010;&#21512;&#25104;&#24739;&#32773;&#31616;&#20917;&#19978;&#30340;&#34920;&#29616;&#12290;&#22312;&#22235;&#20010;&#32423;&#21035;&#19978;&#35780;&#20272;&#20102;&#24615;&#33021;&#65306;&#33021;&#21542;&#20174;&#20020;&#24202;&#35797;&#39564;&#20013;&#32473;&#20986;&#30340;&#21307;&#30103;&#31616;&#20917;&#20013;&#35782;&#21035;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#65307;&#33021;&#21542;&#20026;&#27599;&#20010;&#21333;&#29420;&#30340;&#26631;&#20934;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#24739;&#32773;&#65307;&#25972;&#20307;&#20998;&#31867;&#26159;&#21542;&#31526;&#21512;&#20020;&#24202;&#35797;&#39564;&#36164;&#26684;&#20197;&#21450;&#38656;&#35201;&#31579;&#36873;&#36164;&#26684;&#26631;&#20934;&#30340;&#30334;&#20998;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physicians considering clinical trials for their patients are met with the laborious process of checking many text based eligibility criteria. Large Language Models (LLMs) have shown to perform well for clinical information extraction and clinical reasoning, including medical tests, but not yet in real-world scenarios. This paper investigates the use of InstructGPT to assist physicians in determining eligibility for clinical trials based on a patient's summarised medical profile. Using a prompting strategy combining one-shot, selection-inference and chain-of-thought techniques, we investigate the performance of LLMs on 10 synthetically created patient profiles. Performance is evaluated at four levels: ability to identify screenable eligibility criteria from a trial given a medical profile; ability to classify for each individual criterion whether the patient qualifies; the overall classification whether a patient is eligible for a clinical trial and the percentage of criteria to be scr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#8220;&#38646;&#26679;&#26412;&#20027;&#39064;&#25512;&#26029;&#8221;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Sentence-BERT&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#65292;&#32780;&#22312;&#25928;&#29575;&#26041;&#38754;&#21017;&#20248;&#20808;&#36873;&#25321;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;</title><link>http://arxiv.org/abs/2304.07382</link><description>&lt;p&gt;
&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#38646;&#26679;&#26412;&#22810;&#26631;&#31614;&#20027;&#39064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Multi-Label Topic Inference with Sentence Encoders. (arXiv:2304.07382v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#8220;&#38646;&#26679;&#26412;&#20027;&#39064;&#25512;&#26029;&#8221;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;Sentence-BERT&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#32534;&#30721;&#22120;&#65292;&#32780;&#22312;&#25928;&#29575;&#26041;&#38754;&#21017;&#20248;&#20808;&#36873;&#25321;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#23376;&#32534;&#30721;&#22120;&#22312;&#35768;&#22810;&#19979;&#28216;&#25991;&#26412;&#25366;&#25496;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#22240;&#27492;&#34987;&#35748;&#20026;&#26159;&#30456;&#24403;&#36890;&#29992;&#12290;&#21463;&#21040;&#36825;&#19968;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#35814;&#32454;&#30740;&#31350;&#65292;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#21477;&#23376;&#32534;&#30721;&#22120;&#36827;&#34892;&#8220;&#38646;&#26679;&#26412;&#20027;&#39064;&#25512;&#26029;&#8221;&#20219;&#21153;&#65292;&#20854;&#20013;&#20027;&#39064;&#26159;&#30001;&#29992;&#25143;&#23454;&#26102;&#23450;&#20041;/&#25552;&#20379;&#30340;&#12290;&#22312;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20854;&#20182;&#32534;&#30721;&#22120;&#65292;Sentence-BERT&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36890;&#29992;&#24615;&#65292;&#32780;&#24403;&#25928;&#29575;&#25104;&#20026;&#39318;&#35201;&#32771;&#34385;&#22240;&#32032;&#26102;&#65292;&#21487;&#20197;&#20248;&#20808;&#36873;&#25321;&#36890;&#29992;&#21477;&#23376;&#32534;&#30721;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentence encoders have indeed been shown to achieve superior performances for many downstream text-mining tasks and, thus, claimed to be fairly general. Inspired by this, we performed a detailed study on how to leverage these sentence encoders for the "zero-shot topic inference" task, where the topics are defined/provided by the users in real-time. Extensive experiments on seven different datasets demonstrate that Sentence-BERT demonstrates superior generality compared to other encoders, while Universal Sentence Encoder can be preferred when efficiency is a top priority.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;OpenAI&#30340;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#20998;&#26512;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;ChatGPT&#20559;&#21521;&#36827;&#27493;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.07333</link><description>&lt;p&gt;
ChatGPT&#30340;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
The Self-Perception and Political Biases of ChatGPT. (arXiv:2304.07333v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;OpenAI&#30340;&#35821;&#35328;&#27169;&#22411;ChatGPT&#36827;&#34892;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#20998;&#26512;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;ChatGPT&#20559;&#21521;&#36827;&#27493;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#25991;&#31456;&#20998;&#26512;OpenAI&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#33258;&#25105;&#35748;&#30693;&#21644;&#25919;&#27835;&#20559;&#35265;&#12290;&#32771;&#34385;&#21040;&#24050;&#32463;&#20986;&#29616;&#30340;&#31532;&#19968;&#20010;&#23567;&#35268;&#27169;&#25253;&#21578;&#21644;&#30740;&#31350;&#22768;&#31216;ChatGPT&#22312;&#25919;&#27835;&#19978;&#20559;&#21521;&#36827;&#27493;&#21644;&#33258;&#30001;&#20027;&#20041;&#35266;&#28857;&#65292;&#26412;&#25991;&#26088;&#22312;&#36827;&#19968;&#27493;&#28548;&#28165;&#36825;&#19968;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#20316;&#32773;&#35753;ChatGPT&#22238;&#31572;&#25919;&#27835;&#32599;&#30424;&#27979;&#35797;&#31561;&#31867;&#20284;&#38382;&#21367;&#65292;&#24182;&#38024;&#23545;G7&#25104;&#21592;&#22269;&#30340;&#29305;&#23450;&#25919;&#27835;&#36827;&#34892;&#27979;&#35797;&#65292;&#27599;&#20010;&#27979;&#35797;&#37325;&#22797;&#21313;&#27425;&#65292;&#21457;&#29616;ChatGPT&#20284;&#20046;&#23545;&#36827;&#27493;&#35266;&#28857;&#20855;&#26377;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
This contribution analyzes the self-perception and political biases of OpenAI's Large Language Model ChatGPT. Taking into account the first small-scale reports and studies that have emerged, claiming that ChatGPT is politically biased towards progressive and libertarian points of view, this contribution aims to provide further clarity on this subject. For this purpose, ChatGPT was asked to answer the questions posed by the political compass test as well as similar questionnaires that are specific to the respective politics of the G7 member states. These eight tests were repeated ten times each and revealed that ChatGPT seems to hold a bias towards progressive views. The political compass test revealed a bias towards progressive and libertarian views, with the average coordinates on the political compass being (-6.48, -5.99) (with (0, 0) the center of the compass, i.e., centrism and the axes ranging from -10 to 10), supporting the claims of prior research. The political questionnaires f
&lt;/p&gt;</description></item><item><title>&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07327</link><description>&lt;p&gt;
OpenAssistant Conversations -- &#27665;&#20027;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
OpenAssistant Conversations -- Democratizing Large Language Model Alignment. (arXiv:2304.07327v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07327
&lt;/p&gt;
&lt;p&gt;
&#37322;&#25918;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#25216;&#26415;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#21487;&#29992;&#24615;&#24182;&#25512;&#21160;&#20854;&#24555;&#36895;&#24212;&#29992;&#65292;&#22914;ChatGPT&#25152;&#31034;&#12290; &#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#26681;&#25454;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#31561;&#23545;&#40784;&#25216;&#26415;&#22823;&#22823;&#38477;&#20302;&#20102;&#26377;&#25928;&#21457;&#25381;LLM&#33021;&#21147;&#25152;&#38656;&#30340;&#25216;&#33021;&#21644;&#39046;&#22495;&#30693;&#35782;&#65292;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290; &#28982;&#32780;&#65292;&#20687;RLHF&#36825;&#26679;&#30340;&#26368;&#20808;&#36827;&#30340;&#23545;&#40784;&#25216;&#26415;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#26114;&#36149;&#19988;&#20445;&#23494;&#12290; &#20026;&#20102;&#27665;&#20027;&#21270;&#22823;&#35268;&#27169;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;OpenAssistant Conversations&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#20840;&#29699;&#36229;&#36807;1,000&#21517;&#21442;&#19982;&#32773;&#36827;&#34892;&#20154;&#24037;&#29983;&#25104;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#21161;&#25163;&#39118;&#26684;&#23545;&#35805;&#35821;&#26009;&#24211;&#65292;&#21253;&#21547;161,443&#26465;&#28040;&#24687;&#65292;&#20998;&#24067;&#22312;66,497&#20010;&#23545;&#35805;&#26641;&#20013;&#65292;&#24182;&#22312;35&#31181;&#19981;&#21516;&#30340;&#35821;&#35328;&#20013;&#29992;461,292&#20010;&#36136;&#37327;&#35780;&#20998;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;OpenAssistant Conversations&#21487;&#20197;&#36890;&#36807;SFT&#21644;RLHF&#26377;&#25928;&#22320;&#29992;&#20110;LLM&#23545;&#40784;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#12290;&#25105;&#20204;&#21457;&#24067;&#35821;&#26009;&#24211;&#65292;&#20351;&#26356;&#24191;&#27867;&#30340;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#36827;&#19968;&#27493;&#30740;&#31350;&#27665;&#20027;&#21270;LLM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20154;&#31867;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07297</link><description>&lt;p&gt;
&#35821;&#35328;&#25351;&#23548;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Language Instructed Reinforcement Learning for Human-AI Coordination. (arXiv:2304.07297v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#35299;&#20915;&#22312;&#32570;&#20047;&#39640;&#36136;&#37327;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#25910;&#25947;&#20110;&#20154;&#31867;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#22914;&#20309;&#35753;&#26234;&#33021;&#20307;&#33021;&#22815;&#21644;&#20154;&#31867;&#26377;&#25928;&#22320;&#21327;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20043;&#20026;instructRL&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#35753;&#20154;&#20204;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#26469;&#25351;&#23450;&#23545;&#20154;&#24037;&#26234;&#33021;&#25645;&#26723;&#30340;&#39044;&#26399;&#31574;&#30053;&#65292;&#20197;&#27492;&#35299;&#20915;&#22312;&#32570;&#20047;&#36739;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#34892;&#20026;&#25968;&#25454;&#30340;&#39046;&#22495;&#20013;&#65292;&#30001;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24120;&#24120;&#20250;&#25910;&#25947;&#21040;&#20154;&#31867;&#24182;&#19981;&#20559;&#29233;&#30340;&#31574;&#30053;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#29983;&#25104;&#19968;&#20010;&#22312;&#20154;&#31867;&#25351;&#20196;&#19979;&#30340;&#20808;&#39564;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#23548;&#33268;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#19982;&#20154;&#31867;&#21916;&#22909;&#19968;&#33268;&#30340;&#22343;&#34913;&#28857;&#12290;&#36890;&#36807;&#27010;&#24565;&#35777;&#26126;&#29615;&#22659;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;Hanabi&#22522;&#20934;&#65292;&#35777;&#26126;&#20102;instructRL&#25910;&#25947;&#20110;&#28385;&#36275;&#32473;&#23450;&#25351;&#20196;&#30340;&#31867;&#20284;&#20154;&#31867;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30693;&#36947;&#35821;&#35328;&#25351;&#20196;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.06858</link><description>&lt;p&gt;
Vax-Culture: &#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#30123;&#33495;&#35752;&#35770;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Vax-Culture: A Dataset for Studying Vaccine Discourse on Twitter. (arXiv:2304.06858v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25512;&#29305;&#30123;&#33495;&#25968;&#25454;&#38598;Vax-Culture&#65292;&#23427;&#26088;&#22312;&#25214;&#20986;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#65292;&#24110;&#21161;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#30123;&#24773;&#26399;&#38388;&#65292;&#30123;&#33495;&#29369;&#35947;&#32487;&#32493;&#26159;&#20844;&#20849;&#21355;&#29983;&#23448;&#21592;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#35813;&#29369;&#35947;&#30772;&#22351;&#20102;&#30123;&#33495;&#36816;&#21160;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#35797;&#22270;&#30830;&#23450;&#20854;&#26681;&#26412;&#21407;&#22240;&#65292;&#24182;&#21457;&#29616;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21453;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#19981;&#26029;&#22686;&#38271;&#26159;&#35813;&#38382;&#39064;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#23558;&#25512;&#29305;&#20316;&#20026;&#35823;&#23548;&#20869;&#23481;&#30340;&#26469;&#28304;&#65292;&#24182;&#26088;&#22312;&#25552;&#21462;&#25512;&#24191;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#30340;&#25991;&#21270;&#21644;&#25919;&#27835;&#20449;&#24565;&#30340;&#37325;&#21472;&#37096;&#20998;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#19982;&#30123;&#33495;&#26377;&#20851;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#24182;&#20511;&#21161;&#19987;&#19994;&#27807;&#36890;&#21644;&#26032;&#38395;&#32972;&#26223;&#30340;&#27880;&#37322;&#20154;&#21592;&#36827;&#34892;&#27880;&#37322;&#12290;&#25105;&#20204;&#26368;&#32456;&#24076;&#26395;&#36825;&#21487;&#20197;&#24102;&#26469;&#26377;&#25928;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;&#20844;&#20849;&#21355;&#29983;&#36890;&#20449;&#31574;&#30053;&#65292;&#20197;&#25509;&#35302;&#37027;&#20123;&#25345;&#21453;&#30123;&#33495;&#20449;&#20208;&#32773;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20449;&#24687;&#26377;&#21161;&#20110;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20197;&#33258;&#21160;&#26816;&#27979;&#30123;&#33495;&#38169;&#35823;&#20449;&#24687;&#24086;&#23376;&#24182;&#24212;&#23545;&#20854;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy continues to be a main challenge for public health officials during the COVID-19 pandemic. As this hesitancy undermines vaccine campaigns, many researchers have sought to identify its root causes, finding that the increasing volume of anti-vaccine misinformation on social media platforms is a key element of this problem. We explored Twitter as a source of misleading content with the goal of extracting overlapping cultural and political beliefs that motivate the spread of vaccine misinformation. To do this, we have collected a data set of vaccine-related Tweets and annotated them with the help of a team of annotators with a background in communications and journalism. Ultimately we hope this can lead to effective and targeted public health communication strategies for reaching individuals with anti-vaccine beliefs. Moreover, this information helps with developing Machine Learning models to automatically detect vaccine misinformation posts and combat their negative impa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;</title><link>http://arxiv.org/abs/2304.06446</link><description>&lt;p&gt;
SpectFormer: &#39057;&#29575;&#21644;&#27880;&#24847;&#21147;&#26159;&#35270;&#35273;Transformer&#25152;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
SpectFormer: Frequency and Attention is what you need in a Vision Transformer. (arXiv:2304.06446v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#30340;Spectformer&#26550;&#26500;&#65292;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#25552;&#39640;&#20102;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#24050;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#20219;&#21153;&#20013;&#12290;&#20854;&#31181;&#31867;&#21253;&#25324;&#22522;&#20110;&#22810;&#22836;&#33258;&#25105;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;&#22914;ViT&#12289;DeIT&#65289;&#21644;&#22522;&#20110;&#35889;&#23618;&#65288;&#22914;Fnet&#12289;GFNet&#12289;AFNO&#65289;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#37117;&#23545;Transformer&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#23558;&#20004;&#32773;&#32467;&#21512;&#21487;&#20197;&#24471;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;&#22240;&#27492;&#25552;&#20986;&#20102;&#26032;&#30340;Spectformer&#26550;&#26500;&#65292;&#23558;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#35889;&#23618;&#34701;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Spectformer&#21487;&#24688;&#24403;&#22320;&#25429;&#25417;&#29305;&#24449;&#34920;&#31034;&#65292;&#19982;&#20854;&#20182;Transformer&#34920;&#24449;&#30456;&#27604;&#65292;&#21487;&#20197;&#25552;&#39640;top-1&#20934;&#30830;&#29575;2%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers have been applied successfully for image recognition tasks. There have been either multi-headed self-attention based (ViT \cite{dosovitskiy2020image}, DeIT, \cite{touvron2021training}) similar to the original work in textual models or more recently based on spectral layers (Fnet\cite{lee2021fnet}, GFNet\cite{rao2021global}, AFNO\cite{guibas2021efficient}). We hypothesize that both spectral and multi-headed attention plays a major role. We investigate this hypothesis through this work and observe that indeed combining spectral and multi-headed attention layers provides a better transformer architecture. We thus propose the novel Spectformer architecture for transformers that combines spectral and multi-headed attention layers. We believe that the resulting representation allows the transformer to capture the feature representation appropriately and it yields improved performance over other transformer representations. For instance, it improves the top-1 accuracy by 2
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20013;&#25991;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#24211;&#65292;&#24494;&#35843;ChatGLM-6B&#27169;&#22411;&#65292;&#23454;&#29616;&#26131;&#20110;&#37096;&#32626;&#30340;&#20197;&#21307;&#30103;&#20026;&#30446;&#30340;&#30340;LLM&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#24314;&#35758;&#30340;&#31934;&#24230;&#21644;&#25512;&#24191;LLM&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01097</link><description>&lt;p&gt;
DoctorGLM&#65306;&#35753;&#20013;&#25991;&#21307;&#29983;&#35843;&#25972;&#19981;&#20877;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task. (arXiv:2304.01097v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01097
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20013;&#25991;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#24211;&#65292;&#24494;&#35843;ChatGLM-6B&#27169;&#22411;&#65292;&#23454;&#29616;&#26131;&#20110;&#37096;&#32626;&#30340;&#20197;&#21307;&#30103;&#20026;&#30446;&#30340;&#30340;LLM&#65292;&#20174;&#32780;&#25552;&#39640;&#21307;&#30103;&#24314;&#35758;&#30340;&#31934;&#24230;&#21644;&#25512;&#24191;LLM&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#21253;&#25324;ChatGPT&#21644;GPT-4&#65292;&#22312;&#29702;&#35299;&#21644;&#22238;&#24212;&#20154;&#31867;&#25351;&#20196;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#33521;&#35821;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#27809;&#26377;&#26126;&#30830;&#22320;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#65292;&#23548;&#33268;&#35786;&#26029;&#12289;&#33647;&#29289;&#25512;&#33616;&#21644;&#20854;&#20182;&#21307;&#30103;&#24314;&#35758;&#30340;&#31934;&#24230;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#21644;&#37096;&#32626;&#23545;&#35805;&#27169;&#22411;&#20173;&#34987;&#35748;&#20026;&#23545;&#21307;&#38498;&#26469;&#35828;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#36825;&#38459;&#30861;&#20102;LLM&#30340;&#25512;&#24191;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;ChatGPT&#30340;&#24110;&#21161;&#25910;&#38598;&#20102;&#20013;&#25991;&#30340;&#21307;&#23398;&#23545;&#35805;&#25968;&#25454;&#24211;&#65292;&#24182;&#37319;&#29992;&#20102;&#22810;&#31181;&#25216;&#26415;&#26469;&#35757;&#32451;&#19968;&#20010;&#26131;&#20110;&#37096;&#32626;&#30340;LLM&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#21333;&#20010;A100 80G&#19978;&#20197;13&#20010;&#23567;&#26102;&#30340;&#26102;&#38388;&#23545;ChatGLM-6B&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#24847;&#21619;&#30528;&#25317;&#26377;&#19968;&#20010;&#20197;&#21307;&#30103;&#20026;&#30446;&#30340;&#30340;LLM&#21487;&#33021;&#38750;&#24120;&#23454;&#24800;&#12290;DoctorGLM&#30446;&#21069;&#26159;&#19968;&#39033;&#26089;&#26399;&#30340;&#24037;&#31243;&#23581;&#35797;&#65292;&#21253;&#21547;&#21508;&#31181;&#38169;&#35823;&#12290;&#25105;&#20204;&#19982;&#24191;&#22823;&#31038;&#21306;&#20998;&#20139;&#65292;&#24182;&#36992;&#35831;&#21453;&#39304;&#21644;&#24314;&#35758;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent progress of large language models (LLMs), including ChatGPT and GPT-4, in comprehending and responding to human instructions has been remarkable. Nevertheless, these models typically perform better in English and have not been explicitly trained for the medical domain, resulting in suboptimal precision in diagnoses, drug recommendations, and other medical advice. Additionally, training and deploying a dialogue model is still believed to be impossible for hospitals, hindering the promotion of LLMs. To tackle these challenges, we have collected databases of medical dialogues in Chinese with ChatGPT's help and adopted several techniques to train an easy-deploy LLM. Remarkably, we were able to fine-tune the ChatGLM-6B on a single A100 80G in 13 hours, which means having a healthcare-purpose LLM can be very affordable. DoctorGLM is currently an early-stage engineering attempt and contain various mistakes. We are sharing it with the broader community to invite feedback and suggest
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;</title><link>http://arxiv.org/abs/2303.18223</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Large Language Models. (arXiv:2303.18223v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;&#21382;&#31243;&#20197;&#21450;&#26368;&#36817;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#65292;&#24182;&#24378;&#35843;&#27169;&#22411;&#25193;&#23637;&#23558;&#24102;&#26469;&#24615;&#33021;&#25913;&#36827;&#21644;&#29305;&#27530;&#33021;&#21147;&#30340;&#21457;&#25496;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#30001;&#35821;&#27861;&#35268;&#21017;&#25511;&#21046;&#30340;&#22797;&#26434;&#31934;&#32454;&#30340;&#20154;&#31867;&#34920;&#36798;&#31995;&#32479;&#65292;&#23545;&#20110;&#24320;&#21457;&#29702;&#35299;&#21644;&#25484;&#25569;&#35821;&#35328;&#30340;&#33021;&#21147;&#30340;AI&#31639;&#27861;&#26469;&#35828;&#26159;&#19968;&#39033;&#37325;&#22823;&#25361;&#25112;&#12290;&#20316;&#20026;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#65292;&#35821;&#35328;&#24314;&#27169;&#22312;&#36807;&#21435;&#20108;&#21313;&#24180;&#37324;&#24191;&#27867;&#30740;&#31350;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65292;&#20174;&#32479;&#35745;&#35821;&#35328;&#27169;&#22411;&#28436;&#21270;&#20026;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#19978;&#39044;&#35757;&#32451;Transformer&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#22312;&#35299;&#20915;&#21508;&#31181;NLP&#20219;&#21153;&#26041;&#38754;&#26174;&#31034;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#30001;&#20110;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#27169;&#22411;&#32553;&#25918;&#21487;&#20197;&#23548;&#33268;&#24615;&#33021;&#25913;&#36827;&#65292;&#20182;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#35268;&#27169;&#26469;&#30740;&#31350;&#32553;&#25918;&#25928;&#24212;&#65292;&#26377;&#36259;&#30340;&#26159;&#65292;&#24403;&#21442;&#25968;&#35268;&#27169;&#36229;&#36807;&#19968;&#23450;&#27700;&#24179;&#26102;&#65292;&#36825;&#20123;&#25193;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#19981;&#20165;&#21487;&#20197;&#23454;&#29616;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#32780;&#19988;&#36824;&#26174;&#31034;&#20986;&#19968;&#20123;&#23567;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25152;&#27809;&#26377;&#30340;&#29305;&#27530;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale langu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2303.12057</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;
&lt;/p&gt;
&lt;p&gt;
Large Language Models Can Be Used to Estimate the Ideologies of Politicians in a Zero-Shot Learning Setting. (arXiv:2303.12057v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22312;&#38646;-shot&#23398;&#20064;&#29615;&#22659;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;&#25919;&#27835;&#23478;&#30340;&#24847;&#35782;&#24418;&#24577;&#65292;&#20026;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#25919;&#27835;&#21151;&#33021;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#34164;&#21547;&#30340;&#22823;&#37327;&#30693;&#35782;&#21487;&#20197;&#20026;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#21487;&#35266;&#27979;&#24615;&#21644;&#27979;&#37327;&#38382;&#39064;&#25552;&#20379;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20854;&#20013;&#19968;&#31181;&#27169;&#22411;&#22312;&#34913;&#37327;&#31435;&#27861;&#32773;&#30340;&#28508;&#22312;&#24847;&#35782;&#24418;&#24577;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#22609;&#36896;&#25919;&#31574;&#30340;&#25919;&#27835;&#21151;&#33021;&#65292;&#20197;&#21450;&#25919;&#27835;&#34892;&#20026;&#32773;&#20195;&#34920;&#20854;&#36873;&#27665;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#31034;ChatGPT&#22312;&#20004;&#20004;&#27604;&#36739;&#20013;&#36873;&#25321;&#26356;&#33258;&#30001;&#27966;&#65288;&#25110;&#20445;&#23432;&#27966;&#65289;&#30340;&#21442;&#35758;&#21592;&#65292;&#23558;&#31532;116&#23626;&#32654;&#22269;&#22269;&#20250;&#30340;&#21442;&#35758;&#21592;&#25353;&#29031;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#30340;&#20809;&#35889;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;LLM&#22312;&#37325;&#22797;&#36845;&#20195;&#20013;&#20135;&#29983;&#20102;&#31283;&#23450;&#30340;&#31572;&#26696;&#65292;&#27809;&#26377;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#19988;&#19981;&#20165;&#20165;&#26159;&#20174;&#21333;&#19968;&#26469;&#28304;&#20013;&#22797;&#21046;&#20449;&#24687;&#12290;&#36825;&#20010;&#26032;&#23610;&#24230;&#19982;&#29616;&#26377;&#30340;&#33258;&#30001;&#27966;-&#20445;&#23432;&#27966;&#23610;&#24230;&#65288;&#22914;NOMINATE&#65289;&#24378;&#30456;&#20851;&#65292;&#20294;&#20063;&#22312;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#23384;&#22312;&#24046;&#24322;&#65292;&#27604;&#22914;&#27491;&#30830;&#23450;&#20301;&#19968;&#20123;&#36335;&#24452;&#20381;&#36182;&#21644;&#33258;&#30001;&#27966;&#27966;&#21035;&#30340;&#35758;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
The mass aggregation of knowledge embedded in large language models (LLMs) holds the promise of new solutions to problems of observability and measurement in the social sciences. We examine the utility of one such model for a particularly difficult measurement task: measuring the latent ideology of lawmakers, which allows us to better understand functions that are core to democracy, such as how politics shape policy and how political actors represent their constituents. We scale the senators of the 116th United States Congress along the liberal-conservative spectrum by prompting ChatGPT to select the more liberal (or conservative) senator in pairwise comparisons. We show that the LLM produced stable answers across repeated iterations, did not hallucinate, and was not simply regurgitating information from a single source. This new scale strongly correlates with pre-existing liberal-conservative scales such as NOMINATE, but also differs in several important ways, such as correctly placin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#36827;&#34892;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20248;&#21270;&#26679;&#26412;&#26631;&#35760;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.05391</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#30340;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;
&lt;/p&gt;
&lt;p&gt;
Disambiguation of Company names via Deep Recurrent Networks. (arXiv:2303.05391v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#36882;&#24402;&#32593;&#32476;&#36827;&#34892;&#20844;&#21496;&#21517;&#31216;&#28040;&#27495;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#34920;&#29616;&#65292;&#36824;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#20248;&#21270;&#26679;&#26412;&#26631;&#35760;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#28040;&#27495;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#19968;&#20010;&#20219;&#21153;&#65292;&#20854;&#30446;&#26631;&#26159;&#35782;&#21035;&#23545;&#24212;&#20110;&#21516;&#19968;&#21629;&#21517;&#23454;&#20307;&#30340;&#25991;&#26412;&#35760;&#24405;&#12290;&#26412;&#30740;&#31350;&#30340;&#20219;&#21153;&#26159;&#26681;&#25454;&#20844;&#21496;&#30340;&#20070;&#38754;&#21517;&#31216;&#28040;&#38500;&#27495;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;LSTM&#32593;&#32476;&#26041;&#27861;&#26469;&#25552;&#21462;&#20844;&#21496;&#21517;&#31216;&#23383;&#31526;&#20018;&#30340;&#23884;&#20837;&#65292;&#36827;&#32780;&#20351;&#29992;&#36825;&#31181;&#34920;&#31034;&#26469;&#35782;&#21035;&#30495;&#27491;&#34920;&#31034;&#21516;&#19968;&#20844;&#21496;&#65288;&#21363;&#30456;&#21516;&#23454;&#20307;&#65289;&#30340;&#20844;&#21496;&#21517;&#31216;&#23545;&#12290;&#32771;&#34385;&#21040;&#25163;&#21160;&#26631;&#35760;&#23383;&#31526;&#20018;&#23545;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#20248;&#20808;&#36873;&#25321;&#26679;&#26412;&#36827;&#34892;&#26631;&#35760;&#20174;&#32780;&#20351;&#25972;&#20010;&#23398;&#20064;&#27969;&#31243;&#26356;&#21152;&#39640;&#25928;&#12290;&#32463;&#23454;&#35777;&#65292;&#25105;&#20204;&#30340;Siamese&#32593;&#32476;&#20248;&#20110;&#22810;&#31181;&#22522;&#20110;&#26631;&#20934;&#23383;&#31526;&#20018;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Name Entity Disambiguation is the Natural Language Processing task of identifying textual records corresponding to the same Named Entity, i.e. real-world entities represented as a list of attributes (names, places, organisations, etc.). In this work, we face the task of disambiguating companies on the basis of their written names. We propose a Siamese LSTM Network approach to extract -- via supervised learning -- an embedding of company name strings in a (relatively) low dimensional vector space and use this representation to identify pairs of company names that actually represent the same company (i.e. the same Entity).  Given that the manual labelling of string pairs is a rather onerous task, we analyse how an Active Learning approach to prioritise the samples to be labelled leads to a more efficient overall learning pipeline.  With empirical investigations, we show that our proposed Siamese Network outperforms several benchmark approaches based on standard string matching algorithms
&lt;/p&gt;</description></item><item><title>xCodeEval&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#20462;&#22797;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#20165;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.03004</link><description>&lt;p&gt;
xCodeEval&#65306;&#19968;&#20010;&#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
xCodeEval: A Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval. (arXiv:2303.03004v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03004
&lt;/p&gt;
&lt;p&gt;
xCodeEval&#26159;&#19968;&#20010;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#22810;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#12289;&#20462;&#22797;&#12289;&#32763;&#35793;&#21644;&#26816;&#32034;&#20195;&#30721;&#30340;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#20102;&#20197;&#24448;&#20165;&#20851;&#27880;&#29305;&#23450;&#20219;&#21153;&#21644;&#32570;&#20047;&#35757;&#32451;&#25968;&#25454;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#38382;&#39064;&#30340;&#33021;&#21147;&#26159;&#26234;&#33021;&#30340;&#26631;&#24535;&#65292;&#24182;&#19988;&#19968;&#30452;&#26159; AI &#30340;&#30446;&#26631;&#12290;&#33021;&#22815;&#21019;&#24314;&#20316;&#20026;&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#30340;&#31243;&#24207;&#30340; AI &#31995;&#32479;&#65292;&#25110;&#32773;&#21327;&#21161;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#31243;&#24207;&#65292;&#37117;&#21487;&#20197;&#25552;&#39640;&#29983;&#20135;&#29575;&#24182;&#20351;&#32534;&#31243;&#26356;&#26131;&#20110;&#35775;&#38382;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#26032;&#20195;&#30721;&#12289;&#20462;&#22797;&#26377;&#38382;&#39064;&#30340;&#20195;&#30721;&#12289;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#36827;&#34892;&#20195;&#30721;&#32763;&#35793;&#20197;&#21450;&#26816;&#32034;&#30456;&#20851;&#20195;&#30721;&#29255;&#27573;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#35780;&#20272;&#36890;&#24120;&#26159;&#20998;&#25955;&#22312;&#20165;&#19968;&#20010;&#25110;&#20004;&#20010;&#29305;&#23450;&#20219;&#21153;&#19978;&#65292;&#22312;&#23569;&#25968;&#35821;&#35328;&#12289;&#22312;&#37096;&#20998;&#31890;&#24230;&#27700;&#24179;&#65288;&#20363;&#22914;&#20989;&#25968;&#32423;&#21035;&#65289;&#19978;&#36827;&#34892;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#32570;&#20047;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#26356;&#20026;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#35780;&#20272;&#26159;&#20197;&#20165;&#20165;&#35789;&#27719;&#37325;&#21472;&#20026;&#22522;&#30784;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25191;&#34892;&#65292;&#32780;&#20004;&#27573;&#20195;&#30721;&#27573;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65288;&#25110;&#31561;&#25928;&#24615;&#65289;&#20165;&#21462;&#20915;&#20110;&#23427;&#20204;&#30340;&#8220;&#25191;&#34892;&#30456;&#20284;&#24615;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to solve problems is a hallmark of intelligence and has been an enduring goal in AI. AI systems that can create programs as solutions to problems or assist developers in writing programs can increase productivity and make programming more accessible. Recently, pre-trained large language models have shown impressive abilities in generating new codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap rather than actual execution whereas semantic similarity (or equivalence) of two code segments depends only on their ``execution similarity'', 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2302.10186</link><description>&lt;p&gt;
&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#31471;&#21040;&#31471;&#21475;&#35821;&#21270;&#23454;&#20307;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
E2E Spoken Entity Extraction for Virtual Agents. (arXiv:2302.10186v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#30340;&#26041;&#27861;&#65292;&#26080;&#38656;&#25991;&#26412;&#36716;&#24405;&#65292;&#19988;&#22312;&#21475;&#35821;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#26500;&#24819;&#20102;&#35821;&#38899;&#22788;&#29702;&#20013;&#30340;&#19968;&#20123;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#23454;&#20307;&#65292;&#32780;&#26080;&#38656;&#20013;&#38388;&#25991;&#26412;&#34920;&#31034;&#12290;&#22312;&#20154;&#19982;&#35745;&#31639;&#26426;&#30340;&#23545;&#35805;&#20013;&#65292;&#20174;&#35821;&#38899;&#20013;&#25552;&#21462;&#23454;&#20307;&#65292;&#22914;&#22995;&#21517;&#12289;&#37038;&#25919;&#22320;&#22336;&#21644;&#30005;&#23376;&#37038;&#20214;&#22320;&#22336;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#38899;&#32534;&#30721;&#22120;&#23545;&#20174;&#35821;&#38899;&#20013;&#30452;&#25509;&#25552;&#21462;&#21487;&#35835;&#24615;&#24378;&#30340;&#23454;&#20307;&#30340;&#24433;&#21709;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#25991;&#26412;&#36716;&#24405;&#12290;&#25105;&#20204;&#35828;&#26126;&#36825;&#31181;&#30452;&#25509;&#26041;&#27861;&#20248;&#21270;&#20102;&#32534;&#30721;&#22120;&#65292;&#20197;&#20165;&#36716;&#24405;&#35821;&#38899;&#20013;&#19982;&#23454;&#20307;&#30456;&#20851;&#30340;&#37096;&#20998;&#65292;&#24573;&#30053;&#20102;&#22810;&#20313;&#30340;&#37096;&#20998;&#65292;&#22914;&#25645;&#26723;&#35821;&#25110;&#23454;&#20307;&#25340;&#20889;&#12290;&#22312;&#20225;&#19994;&#34394;&#25311;&#20195;&#29702;&#20154;&#30340;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#27493;&#27861;&#30340;&#26041;&#27861;&#20248;&#20110;&#20856;&#22411;&#30340;&#20004;&#27493;&#27861;&#65292;&#21363;&#39318;&#20808;&#20135;&#29983;&#35789;&#27719;&#36716;&#24405;&#65292;&#28982;&#21518;&#36827;&#34892;&#22522;&#20110;&#25991;&#26412;&#30340;&#23454;&#20307;&#25552;&#21462;&#20197;&#35782;&#21035;&#21475;&#35821;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper reimagines some aspects of speech processing using speech encoders, specifically about extracting entities directly from speech, with no intermediate textual representation. In human-computer conversations, extracting entities such as names, postal addresses and email addresses from speech is a challenging task. In this paper, we study the impact of fine-tuning pre-trained speech encoders on extracting spoken entities in human-readable form directly from speech without the need for text transcription. We illustrate that such a direct approach optimizes the encoder to transcribe only the entity relevant portions of speech, ignoring the superfluous portions such as carrier phrases and spellings of entities. In the context of dialogs from an enterprise virtual agent, we demonstrate that the 1-step approach outperforms the typical 2-step cascade of first generating lexical transcriptions followed by text-based entity extraction for identifying spoken entities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#20197;&#20811;&#26381;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09664</link><description>&lt;p&gt;
&#35821;&#20041;&#19981;&#30830;&#23450;&#24615;&#65306;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20013;&#30340;&#35821;&#35328;&#19981;&#21464;&#37327;
&lt;/p&gt;
&lt;p&gt;
Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. (arXiv:2302.09664v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#20197;&#20811;&#26381;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#35813;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#24182;&#19988;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#27979;&#37327;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#20687;&#38382;&#31572;&#20219;&#21153;&#36825;&#26679;&#30340;&#20219;&#21153;&#65292;&#20102;&#35299;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#22522;&#30784;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20986;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30001;&#20110;&#8220;&#35821;&#20041;&#31561;&#20215;&#24615;&#8221;&#65292;&#27979;&#37327;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#26377;&#25361;&#25112;&#24615;&#30340;&#8212;&#8212;&#19981;&#21516;&#30340;&#21477;&#23376;&#21487;&#20197;&#34920;&#31034;&#30456;&#21516;&#30340;&#24847;&#24605;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#20041;&#29109;&#8212;&#8212;&#19968;&#31181;&#21253;&#21547;&#20849;&#20139;&#21547;&#20041;&#25152;&#21019;&#24314;&#30340;&#35821;&#35328;&#19981;&#21464;&#37327;&#30340;&#29109;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#30417;&#30563;&#30340;&#65292;&#20165;&#20351;&#29992;&#21333;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#20840;&#38754;&#30340;&#28040;&#34701;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35821;&#20041;&#29109;&#23545;&#20110;&#38382;&#39064;&#22238;&#31572;&#25968;&#25454;&#38598;&#19978;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#27604;&#21487;&#27604;&#22522;&#32447;&#26356;&#20855;&#26377;&#39044;&#27979;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence" -- different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy -- an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#25509;&#25910;&#22120;&#21487;&#20197;&#26356;&#31215;&#26497;&#22320;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#35821;&#20041;&#25512;&#29702;&#21644;&#35299;&#30721;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#22522;&#20110;GCN&#30340;&#35821;&#20041;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#24433;&#21709;&#21457;&#23556;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.07727</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#25509;&#25910;&#22120;
&lt;/p&gt;
&lt;p&gt;
Knowledge Enhanced Semantic Communication Receiver. (arXiv:2302.07727v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#25509;&#25910;&#22120;&#21487;&#20197;&#26356;&#31215;&#26497;&#22320;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#35821;&#20041;&#25512;&#29702;&#21644;&#35299;&#30721;&#65292;&#24182;&#36890;&#36807;&#30693;&#35782;&#25552;&#21462;&#22120;&#21644;&#22522;&#20110;GCN&#30340;&#35821;&#20041;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#19981;&#24433;&#21709;&#21457;&#23556;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#35821;&#20041;&#36890;&#20449;&#24050;&#25104;&#20026;&#36890;&#20449;&#39046;&#22495;&#30340;&#28909;&#38376;&#35805;&#39064;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#24050;&#32463;&#26174;&#31034;&#20986;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#20391;&#37325;&#20110;&#21457;&#23556;&#31471;&#30340;&#35821;&#20041;&#32534;&#30721;&#65292;&#32780;&#25105;&#20204;&#35748;&#20026;&#65292;&#25509;&#25910;&#26041;&#30340;&#35821;&#20041;&#35299;&#30721;&#33021;&#21147;&#20063;&#24212;&#35813;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30693;&#35782;&#22686;&#24378;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#20854;&#20013;&#25509;&#25910;&#22120;&#21487;&#20197;&#26356;&#31215;&#26497;&#22320;&#21033;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#20107;&#23454;&#36827;&#34892;&#35821;&#20041;&#25512;&#29702;&#21644;&#35299;&#30721;&#65292;&#32780;&#19981;&#24433;&#21709;&#21457;&#23556;&#31471;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#20165;&#24433;&#21709;&#20854;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;transformer&#30340;&#30693;&#35782;&#25552;&#21462;&#22120;&#65292;&#20197;&#26597;&#25214;&#25509;&#25910;&#21040;&#30340;&#22024;&#26434;&#25968;&#25454;&#20013;&#30340;&#30456;&#20851;&#20107;&#23454;&#19977;&#20803;&#32452;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#30340;&#35821;&#20041;&#35299;&#30721;&#22120;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#21333;&#35789;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24120;&#29992;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, with the rapid development of deep learning and natural language processing technologies, semantic communication has become a topic of great interest in the field of communication. Although existing deep learning-based semantic communication approaches have shown many advantages, they still do not make sufficient use of prior knowledge. Moreover, most existing semantic communication methods focus on the semantic encoding at the transmitter side, while we believe that the semantic decoding capability of the receiver should also be concerned. In this paper, we propose a knowledge enhanced semantic communication framework in which the receiver can more actively utilize the facts in the knowledge base for semantic reasoning and decoding, on the basis of only affecting the parameters rather than the structure of the neural networks at the transmitter side. Specifically, we design a transformer-based knowledge extractor to find relevant factual triples for the received noisy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#35273;&#30417;&#30563;&#35757;&#32451;&#30340;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#27604;&#26222;&#36890;&#35821;&#35328;&#34920;&#31034;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26222;&#36890;&#30340;&#35821;&#35328;&#34920;&#31034;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05016</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#35270;&#35273;&#30417;&#30563;&#23545;&#35821;&#35328;&#26377;&#30410;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Multimodal Vision Supervision Beneficial to Language?. (arXiv:2302.05016v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#35273;&#30417;&#30563;&#35757;&#32451;&#30340;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#27604;&#26222;&#36890;&#35821;&#35328;&#34920;&#31034;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26222;&#36890;&#30340;&#35821;&#35328;&#34920;&#31034;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#65288;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;-&#35821;&#35328;&#65288;VL&#65289;&#39044;&#35757;&#32451;&#26159;&#26368;&#36817;&#27969;&#34892;&#30340;&#27169;&#24335;&#65292;&#23427;&#22312;&#22810;&#27169;&#24577;&#20219;&#21153;&#22914;&#22270;&#20687;&#26816;&#32034;&#12289;&#35270;&#39057;&#26816;&#32034;&#12289;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#38750;&#24120;&#21463;&#30410;&#20110;&#34917;&#20805;&#27169;&#24577;&#30417;&#30563;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#35270;&#35273;&#30417;&#30563;&#35757;&#32451;&#30340;&#35821;&#35328;&#34920;&#31034;&#26159;&#21542;&#27604;&#26222;&#36890;&#35821;&#35328;&#34920;&#31034;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#24120;&#35782;&#25512;&#29702;&#22522;&#20934;&#27979;&#35797;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#23558;&#35797;&#39564;&#19981;&#21516;&#30340;&#22270;&#20687;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22914;ALBEF&#12289;BLIP&#12289;METER&#31561;&#65292;&#20197;&#21450;&#35270;&#39057;-&#25991;&#26412;&#27169;&#22411;&#65292;&#22914;ALPRO&#12289;Frozen-in-Time&#65288;FiT&#65289;&#12289;VIOLET&#31561;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#27169;&#22411;&#30340;&#29420;&#31435;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#34920;&#31034;&#19982;&#36890;&#36807;&#35270;&#35273;&#30417;&#30563;&#23398;&#20064;&#30340;&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#65292;&#26222;&#36890;&#30340;&#35821;&#35328;&#34920;&#31034;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision (image and video) - Language (VL) pre-training is the recent popular paradigm that achieved state-of-the-art results on multi-modal tasks like image-retrieval, video-retrieval, visual question answering etc. These models are trained in an unsupervised way and greatly benefit from the complementary modality supervision. In this paper, we explore if the language representations trained using vision supervision perform better than vanilla language representations on Natural Language Understanding and commonsense reasoning benchmarks. We experiment with a diverse set of image-text models such as ALBEF, BLIP, METER and video-text models like ALPRO, Frozen-in-Time (FiT), VIOLET. We compare the performance of language representations of stand-alone text encoders of these models to the language representations of text encoders learnt through vision supervision. Our experiments suggest that vanilla language representations show superior performance on most of the tasks. These results she
&lt;/p&gt;</description></item><item><title>GLIGEN&#26159;&#19968;&#31181;&#24320;&#25918;&#24335;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#27880;&#20837;&#36830;&#32467;&#20449;&#24687;&#65292;&#33021;&#22815;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2301.07093</link><description>&lt;p&gt;
GLIGEN&#65306;&#24320;&#25918;&#24335;&#22522;&#20110;&#25991;&#26412;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GLIGEN: Open-Set Grounded Text-to-Image Generation. (arXiv:2301.07093v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07093
&lt;/p&gt;
&lt;p&gt;
GLIGEN&#26159;&#19968;&#31181;&#24320;&#25918;&#24335;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#21644;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#27880;&#20837;&#36830;&#32467;&#20449;&#24687;&#65292;&#33021;&#22815;&#23454;&#29616;&#38646;&#26679;&#26412;&#30340;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#29366;&#26159;&#20165;&#20351;&#29992;&#25991;&#26412;&#36755;&#20837;&#65292;&#36825;&#21487;&#33021;&#20250;&#38480;&#21046;&#21487;&#25511;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GLIGEN&#65292;&#22522;&#20110;&#35821;&#35328;&#20851;&#32852;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#24314;&#31435;&#22312;&#29616;&#26377;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#20381;&#36182;&#20110;&#35821;&#35328;&#20851;&#32852;&#24615;&#36755;&#20837;&#12290;&#20026;&#20102;&#20445;&#30041;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24191;&#27867;&#27010;&#24565;&#30693;&#35782;&#65292;&#25105;&#20204;&#20923;&#32467;&#25152;&#26377;&#26435;&#37325;&#65292;&#24182;&#36890;&#36807;&#38376;&#25511;&#26426;&#21046;&#23558;&#36830;&#32467;&#20449;&#24687;&#27880;&#20837;&#21040;&#26032;&#30340;&#21487;&#35757;&#32451;&#23618;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#24320;&#25918;&#24335;&#22522;&#20110;&#20851;&#38190;&#23383;&#21644;&#36793;&#30028;&#26694;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65292;&#32780;&#19988;&#36830;&#32467;&#33021;&#21147;&#22312;&#26032;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#27010;&#24565;&#19978;&#20855;&#26377;&#33391;&#22909;&#30340;&#26222;&#36866;&#24615;&#12290;GLIGEN&#22312;COCO&#21644;LVIS&#30340;&#38646;&#26679;&#26412;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#30417;&#30563;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image diffusion models have made amazing advances. However, the status quo is to use text input alone, which can impede controllability. In this work, we propose GLIGEN, Grounded-Language-to-Image Generation, a novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs. To preserve the vast concept knowledge of the pre-trained model, we freeze all of its weights and inject the grounding information into new trainable layers via a gated mechanism. Our model achieves open-world grounded text2img generation with caption and bounding box condition inputs, and the grounding ability generalizes well to novel spatial configurations and concepts. GLIGEN's zero-shot performance on COCO and LVIS outperforms that of existing supervised layout-to-image baselines by a large margin.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#25968;&#25454;&#38598;&#65292;&#30123;&#33495;&#29369;&#35947;&#19968;&#30452;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#20102;&#35299;&#20844;&#20247;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#21407;&#22240;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2301.06660</link><description>&lt;p&gt;
VaxxHesitancy: &#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
VaxxHesitancy: A Dataset for Studying Hesitancy Towards COVID-19 Vaccination on Twitter. (arXiv:2301.06660v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#29992;&#20110;&#30740;&#31350;&#25512;&#29305;&#19978; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#25968;&#25454;&#38598;&#65292;&#30123;&#33495;&#29369;&#35947;&#19968;&#30452;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#20102;&#35299;&#20844;&#20247;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#21407;&#22240;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30123;&#33495;&#29369;&#35947;&#19968;&#30452;&#26159;&#19968;&#20010;&#26222;&#36941;&#30340;&#38382;&#39064;&#65292;&#38543;&#30528;&#31038;&#20132;&#23186;&#20307;&#30340;&#26222;&#21450;&#65292;&#20154;&#20204;&#24320;&#22987;&#22312;&#32593;&#19978;&#34920;&#36798;&#20182;&#20204;&#23545;&#30123;&#33495;&#30340;&#25285;&#24551;&#65292;&#21516;&#26102;&#20063;&#19982;&#25903;&#25345;&#21644;&#21453;&#23545;&#30123;&#33495;&#30340;&#20154;&#21457;&#34920;&#20869;&#23481;&#12290;&#33258;&#20174;&#31532;&#19968;&#27425;&#25552;&#21040; COVID-19 &#30123;&#33495;&#20197;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#23601;&#22312;&#21457;&#24067;&#20851;&#20110;&#20182;&#20204;&#30340;&#25285;&#24551;&#21644;&#25903;&#25345;&#30123;&#33495;&#26377;&#25928;&#24615;&#30340;&#20869;&#23481;&#12290;&#20102;&#35299;&#20844;&#20247;&#23545; COVID-19 &#30123;&#33495;&#29369;&#35947;&#30340;&#21407;&#22240;&#38750;&#24120;&#37325;&#35201;&#65292;&#23545;&#20110;&#38656;&#35201;&#21046;&#23450;&#34892;&#21160;&#20197;&#26356;&#22909;&#22320;&#21578;&#30693;&#20154;&#32676;&#20197;&#22686;&#21152;&#30123;&#33495;&#25509;&#31181;&#29575;&#30340;&#25919;&#31574;&#21046;&#23450;&#32773;&#26469;&#35828;&#23588;&#20854;&#22914;&#27492;&#12290;&#22312; COVID-19 &#30340;&#24773;&#20917;&#19979;&#65292;&#30123;&#33495;&#24555;&#36895;&#24320;&#21457;&#19982;&#21453;&#30123;&#33495;&#34394;&#20551;&#20449;&#24687;&#30340;&#22686;&#38271;&#30456;&#20276;&#65292;&#33258;&#21160;&#26816;&#27979;&#20844;&#27665;&#23545;&#30123;&#33495;&#25509;&#31181;&#30340;&#24577;&#24230;&#25104;&#20026;&#24517;&#38656;&#12290;&#36825;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35745;&#31639;&#31038;&#20250;&#31185;&#23398;&#20219;&#21153;&#65292;&#38656;&#35201;&#25968;&#25454;&#20998;&#26512;&#25165;&#33021;&#33719;&#24471;&#26356;&#22810;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vaccine hesitancy has been a common concern, probably since vaccines were created and, with the popularisation of social media, people started to express their concerns about vaccines online alongside those posting pro- and anti-vaccine content. Predictably, since the first mentions of a COVID-19 vaccine, social media users posted about their fears and concerns or about their support and belief into the effectiveness of these rapidly developing vaccines. Identifying and understanding the reasons behind public hesitancy towards COVID-19 vaccines is important for policy markers that need to develop actions to better inform the population with the aim of increasing vaccine take-up. In the case of COVID-19, where the fast development of the vaccines was mirrored closely by growth in anti-vaxx disinformation, automatic means of detecting citizen attitudes towards vaccination became necessary. This is an important computational social sciences task that requires data analysis in order to gai
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#21253;&#21547;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#19988;&#38899;&#39057;&#21512;&#25104;&#36136;&#37327;&#19981;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2211.01948</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#30340;&#39640;&#25928;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
Efficiently Trained Low-Resource Mongolian Text-to-Speech System Based On FullConv-TTS. (arXiv:2211.01948v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01948
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20302;&#36164;&#28304;&#33945;&#21476;&#35821;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#21253;&#21547;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;TTS&#27169;&#22411;&#65292;&#35757;&#32451;&#26102;&#38388;&#26356;&#30701;&#19988;&#38899;&#39057;&#21512;&#25104;&#36136;&#37327;&#19981;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;(RNN)&#24050;&#32463;&#25104;&#20026;&#20102;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#30340;&#26631;&#20934;&#25216;&#26415;&#65292;&#20063;&#34987;&#29992;&#20110;&#26500;&#24314;&#19968;&#20123;&#26032;&#22855;&#30340;&#25991;&#26412;&#21040;&#35821;&#38899;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#21253;&#21547;RNN&#32452;&#20214;&#30340;TTS&#27169;&#22411;&#35201;&#27714;GPU&#24615;&#33021;&#39640;&#19988;&#35757;&#32451;&#26102;&#38388;&#38271;&#12290;&#30456;&#21453;&#65292;&#30740;&#31350;&#21457;&#29616;&#22522;&#20110;CNN&#30340;&#24207;&#21015;&#21512;&#25104;&#25216;&#26415;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;TTS&#27169;&#22411;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#30001;&#20110;&#20854;&#39640;&#24230;&#24182;&#34892;&#21270;&#65292;&#21487;&#20197;&#20445;&#35777;&#19968;&#23450;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#25991;&#26412;&#21040;&#35821;&#38899;&#31995;&#32479;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;RNN&#32452;&#20214;(&#24490;&#29615;&#21333;&#20803;)&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#22914;&#26102;&#38388;&#25197;&#26354;&#65292;&#39057;&#29575;&#23631;&#34109;&#21644;&#26102;&#38388;&#23631;&#34109;&#31561;&#65292;&#25552;&#39640;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#26368;&#32456;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20351;&#29992;CNN&#32452;&#20214;&#30340;TTS&#27169;&#22411;&#21487;&#20197;&#20943;&#23569;&#19982;Tacotron&#31561;&#20256;&#32479;TTS&#27169;&#22411;&#30456;&#27604;&#30340;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#30830;&#20445;&#21512;&#25104;&#38899;&#39057;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recurrent Neural Networks (RNNs) have become the standard modeling technique for sequence data, and are used in a number of novel text-to-speech models. However, training a TTS model including RNN components has certain requirements for GPU performance and takes a long time. In contrast, studies have shown that CNN-based sequence synthesis technology can greatly reduce training time in text-to-speech models while ensuring a certain performance due to its high parallelism. We propose a new text-to-speech system based on deep convolutional neural networks that does not employ any RNN components (recurrent units). At the same time, we improve the generality and robustness of our model through a series of data augmentation methods such as Time Warping, Frequency Mask, and Time Mask. The final experimental results show that the TTS model using only the CNN component can reduce the training time compared to the classic TTS models such as Tacotron while ensuring the quality of the synthesized
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;CCG&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#31070;&#32463;&#20449;&#21495;&#65292;&#21457;&#29616;&#27604;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CCG&#26356;&#36866;&#21512;&#34920;&#36798;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2210.16147</link><description>&lt;p&gt;
&#29992;CCG&#35299;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24314;&#27169;&#22823;&#33041;&#20013;&#30340;&#32467;&#26500;&#24314;&#36896;
&lt;/p&gt;
&lt;p&gt;
Modeling structure-building in the brain with CCG parsing and large language models. (arXiv:2210.16147v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;CCG&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27169;&#25311;&#20154;&#31867;&#31070;&#32463;&#20449;&#21495;&#65292;&#21457;&#29616;&#27604;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CCG&#26356;&#36866;&#21512;&#34920;&#36798;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#27169;&#25311;&#33258;&#28982;&#29615;&#22659;&#19979;&#35821;&#35328;&#29702;&#35299;&#30340;&#34892;&#20026;&#21644;&#31070;&#32463;&#30456;&#20851;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#20511;&#21161;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#31561;&#24191;&#27867;&#35206;&#30422;&#30340;&#24037;&#20855;&#12290;&#22312;&#26174;&#24335;&#24314;&#27169;&#21477;&#27861;&#32467;&#26500;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#20381;&#36182;&#20110;&#26080;&#19978;&#19979;&#25991;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#20294;&#36825;&#31181;&#24418;&#24335;&#20027;&#20041;&#23545;&#20110;&#20154;&#31867;&#35821;&#35328;&#26469;&#35828;&#24182;&#19981;&#36275;&#22815;&#34920;&#36798;&#12290;&#32452;&#21512;&#33539;&#30068;&#35821;&#27861;&#65288;CCG&#65289;&#26159;&#20805;&#20998;&#34920;&#36798;&#35821;&#27861;&#30340;&#30452;&#25509;&#32452;&#21512;&#27169;&#22411;&#65292;&#20855;&#26377;&#28789;&#27963;&#30340;&#20174;&#23646;&#20851;&#31995;&#65292;&#21487;&#20197;&#36827;&#34892;&#22686;&#37327;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#27604;CFG&#26356;&#20855;&#34920;&#36798;&#21147;&#30340;CCG&#26159;&#21542;&#20026;&#20154;&#31867;&#31070;&#32463;&#20449;&#21495;&#25552;&#20379;&#20102;&#27604;CFG&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#31070;&#32463;&#20449;&#21495;&#26159;&#22312;&#21548;&#26377;&#22768;&#20070;&#25925;&#20107;&#26102;&#25910;&#38598;&#30340;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27979;&#35797;&#20102;&#22788;&#29702;&#21487;&#36873;&#38468;&#21152;&#35821;&#30340;CCG&#21464;&#20307;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#20123;&#35780;&#20272;&#26159;&#38024;&#23545;&#20855;&#26377;&#21464;&#21387;&#22120;&#31070;&#32463;&#32593;&#32476;&#35821;&#35328;&#27169;&#22411;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#21487;&#39044;&#27979;&#24615;&#20272;&#35745;&#30340;&#22522;&#32447;&#36827;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
To model behavioral and neural correlates of language comprehension in naturalistic environments researchers have turned to broad-coverage tools from natural-language processing and machine learning. Where syntactic structure is explicitly modeled, prior work has relied predominantly on context-free grammars (CFG), yet such formalisms are not sufficiently expressive for human languages. Combinatory Categorial Grammars (CCGs) are sufficiently expressive directly compositional models of grammar with flexible constituency that affords incremental interpretation. In this work we evaluate whether a more expressive CCG provides a better model than a CFG for human neural signals collected with fMRI while participants listen to an audiobook story. We further test between variants of CCG that differ in how they handle optional adjuncts. These evaluations are carried out against a baseline that includes estimates of next-word predictability from a Transformer neural network language model. Such 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.10709</link><description>&lt;p&gt;
&#20197;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Schema-aware Reference as Prompt Improves Data-Efficient Knowledge Graph Construction. (arXiv:2210.10709v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#26816;&#32034;&#22686;&#24378;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#21160;&#24577;&#21033;&#29992;&#20154;&#31867;&#27880;&#37322;&#21644;&#24369;&#30417;&#30563;&#25968;&#25454;&#25152;&#32487;&#25215;&#30340;&#26550;&#26500;&#21644;&#30693;&#35782;&#65292;&#25351;&#23548;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#34987;&#25552;&#20986;&#24182;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#26041;&#27861;&#20173;&#23384;&#22312;&#20960;&#20010;&#28508;&#22312;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#33258;&#28982;&#35821;&#35328;&#21644;&#39044;&#23450;&#20041;&#27169;&#24335;&#30340;&#36755;&#20986;&#32467;&#26500;&#21270;&#30693;&#35782;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#36825;&#24847;&#21619;&#30528;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#21463;&#38480;&#27169;&#26495;&#30340;&#35821;&#20041;&#30693;&#35782;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#23616;&#37096;&#20010;&#20307;&#23454;&#20363;&#30340;&#34920;&#31034;&#23398;&#20064;&#38480;&#21046;&#20102;&#24615;&#33021;&#65292;&#32473;&#23450;&#20102;&#19981;&#20805;&#36275;&#30340;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#19981;&#33021;&#37322;&#25918;&#39044;&#20808;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31867;&#27604;&#33021;&#21147;&#12290;&#21463;&#36825;&#20123;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26816;&#32034;&#24471;&#21040;&#30340;&#26550;&#26500;&#24863;&#30693;&#21442;&#32771;&#20316;&#20026;&#25552;&#31034;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#30340;&#35821;&#20041;&#36830;&#36143;&#24615;&#21644;&#19968;&#33268;&#24615;&#12290;&#22312;&#20004;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#25552;&#31034;&#21644;&#38750;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#30693;&#35782;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of pre-trained language models, many prompt-based approaches to data-efficient knowledge graph construction have been proposed and achieved impressive performance. However, existing prompt-based learning methods for knowledge graph construction are still susceptible to several potential limitations: (i) semantic gap between natural language and output structured knowledge with pre-defined schema, which means model cannot fully exploit semantic knowledge with the constrained templates; (ii) representation learning with locally individual instances limits the performance given the insufficient features, which are unable to unleash the potential analogical capability of pre-trained language models. Motivated by these observations, we propose a retrieval-augmented approach, which retrieves schema-aware Reference As Prompt (RAP), for data-efficient knowledge graph construction. It can dynamically leverage schema and knowledge inherited from human-annotated and weak-supe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#25193;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23548;&#33322;&#36712;&#36857;&#24182;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#22120;Marky&#29983;&#25104;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#20196;&#65292;&#20197;&#21450;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;GAN&#22312;&#26032;&#30340;&#35270;&#35282;&#19978;&#21512;&#25104;&#22270;&#20687;&#35266;&#23519;&#12290;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#20102;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.03112</link><description>&lt;p&gt;
&#19968;&#26465;&#26032;&#36335;: &#29992;&#21512;&#25104;&#25351;&#20196;&#21644;&#27169;&#20223;&#23398;&#20064;&#25193;&#23637;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#30340;&#35268;&#27169;
&lt;/p&gt;
&lt;p&gt;
A New Path: Scaling Vision-and-Language Navigation with Synthetic Instructions and Imitation Learning. (arXiv:2210.03112v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#25193;&#20805;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#23548;&#33322;&#36712;&#36857;&#24182;&#20351;&#29992;&#39640;&#36136;&#37327;&#30340;&#22810;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#22120;Marky&#29983;&#25104;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#20196;&#65292;&#20197;&#21450;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;GAN&#22312;&#26032;&#30340;&#35270;&#35282;&#19978;&#21512;&#25104;&#22270;&#20687;&#35266;&#23519;&#12290;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#20102;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#65288;VLN&#65289;&#20013;&#30340;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#22312;&#36924;&#30495;&#30340;&#29615;&#22659;&#20013;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#65292;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#34920;&#26126;&#30001;&#20110;&#20154;&#31867;&#25351;&#20196;&#25968;&#25454;&#31232;&#32570;&#19988;&#35757;&#32451;&#29615;&#22659;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#20123;&#20195;&#29702;&#20173;&#28982;&#38590;&#20197;&#29702;&#35299;&#22797;&#26434;&#30340;&#35821;&#35328;&#21644;&#31354;&#38388;&#35821;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20351;&#29992;&#21512;&#25104;&#25351;&#20196;&#30340;&#22823;&#35268;&#27169;&#25193;&#20805;&#12290;&#25105;&#20204;&#21033;&#29992;Marky&#65292;&#19968;&#31181;&#39640;&#21697;&#36136;&#30340;&#22810;&#35821;&#35328;&#23548;&#33322;&#25351;&#20196;&#29983;&#25104;&#22120;&#65292;&#21019;&#24314;&#20102;500&#22810;&#20010;&#23460;&#20869;&#29615;&#22659;&#65292;&#36890;&#36807;&#36825;&#20123;&#20840;&#26223;&#22270;&#26500;&#24314;&#23548;&#33322;&#36712;&#36857;&#65292;&#24182;&#20026;&#27599;&#20010;&#36712;&#36857;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#20687;&#30340;&#25351;&#20196;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22270;&#20687;&#21040;&#22270;&#20687;GAN&#22312;&#26032;&#30340;&#35270;&#35282;&#19978;&#21512;&#25104;&#22270;&#20687;&#35266;&#23519;&#12290;&#36890;&#36807;&#36825;&#20123;&#26041;&#27861;&#24471;&#21040;&#20102;&#26356;&#24378;&#30340;&#35270;&#35273;&#35821;&#35328;&#23548;&#33322;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies in Vision-and-Language Navigation (VLN) train RL agents to execute natural-language navigation instructions in photorealistic environments, as a step towards robots that can follow human instructions. However, given the scarcity of human instruction data and limited diversity in the training environments, these agents still struggle with complex language grounding and spatial language understanding. Pretraining on large text and image-text datasets from the web has been extensively explored but the improvements are limited. We investigate large-scale augmentation with synthetic instructions. We take 500+ indoor environments captured in densely-sampled 360 degree panoramas, construct navigation trajectories through these panoramas, and generate a visually-grounded instruction for each trajectory using Marky, a high-quality multilingual navigation instruction generator. We also synthesize image observations from novel viewpoints using an image-to-image GAN. The resulting d
&lt;/p&gt;</description></item><item><title>DecAF &#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#65307;&#21516;&#26102;&#65292;&#23427;&#36824;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2210.00063</link><description>&lt;p&gt;
DecAF&#65306;&#38024;&#23545;&#30693;&#35782;&#24211;&#38382;&#31572;&#30340;&#31572;&#26696;&#21644;&#36923;&#36753;&#24418;&#24335;&#32852;&#21512;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
DecAF: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases. (arXiv:2210.00063v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00063
&lt;/p&gt;
&lt;p&gt;
DecAF &#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#65307;&#21516;&#26102;&#65292;&#23427;&#36824;&#37319;&#29992;&#20102;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#30456;&#27604;&#20197;&#24448;&#30340;&#26041;&#27861;&#26356;&#26131;&#20110;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24211;&#38382;&#31572;&#26088;&#22312;&#20351;&#29992;&#30693;&#35782;&#24211;&#20013;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#31561;&#20107;&#23454;&#20449;&#24687;&#22238;&#31572;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#29983;&#25104;&#21487;&#22312;&#30693;&#35782;&#24211;&#19978;&#25191;&#34892;&#20197;&#33719;&#21462;&#26368;&#32456;&#31572;&#26696;&#30340;&#36923;&#36753;&#24418;&#24335;&#65292;&#35201;&#20040;&#30452;&#25509;&#39044;&#27979;&#31572;&#26696;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#21069;&#32773;&#36890;&#24120;&#33021;&#20135;&#29983;&#26356;&#20934;&#30830;&#30340;&#31572;&#26696;&#65292;&#20294;&#30001;&#20110;&#29983;&#25104;&#30340;&#36923;&#36753;&#24418;&#24335;&#21487;&#33021;&#23384;&#22312;&#35821;&#27861;&#21644;&#35821;&#20041;&#38169;&#35823;&#30340;&#28508;&#22312;&#38382;&#39064;&#65292;&#22240;&#27492;&#23384;&#22312;&#26080;&#27861;&#25191;&#34892;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; DecAF&#65292;&#23427;&#32852;&#21512;&#29983;&#25104;&#36923;&#36753;&#24418;&#24335;&#21644;&#30452;&#25509;&#31572;&#26696;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#24471;&#21040;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;DecAF &#22522;&#20110;&#31616;&#21333;&#30340;&#33258;&#30001;&#25991;&#26412;&#26816;&#32034;&#65292;&#32780;&#19981;&#20381;&#36182;&#20219;&#20309;&#23454;&#20307;&#38142;&#25509;&#24037;&#20855;--&#36825;&#31181;&#31616;&#21270;&#20351;&#20854;&#36866;&#24212;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#26356;&#21152;&#23481;&#26131;&#12290;DecAF &#22312; WebQSP&#12289;FreebaseQA &#21644; GrailQA &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#22312; CommonsenseQA &#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering over knowledge bases (KBs) aims to answer natural language questions with factual information such as entities and relations in KBs. Previous methods either generate logical forms that can be executed over KBs to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework DecAF that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, DecAF is based on simple free-text retrieval without relying on any entity linking tools -- this simplification eases its adaptation to different datasets. DecAF achieves new state-of-the-art accuracy on WebQSP, FreebaseQA, and GrailQA benchmarks, while getting competitive results on the Com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LUT&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#37319;&#29992;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;LUT-GEMM&#20869;&#26680;&#21152;&#36895;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#23454;&#29616;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#28789;&#27963;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2206.09557</link><description>&lt;p&gt;
LUT-GEMM&#65306;&#22522;&#20110;LUT&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LUT-GEMM: Quantized Matrix Multiplication based on LUTs for Efficient Inference in Large-Scale Generative Language Models. (arXiv:2206.09557v3 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;LUT&#30340;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;&#12290;&#37319;&#29992;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;LUT-GEMM&#20869;&#26680;&#21152;&#36895;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#23454;&#29616;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#28789;&#27963;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20808;&#36827;&#25216;&#26415;&#19982;Transformer&#26550;&#26500;&#30340;&#32467;&#21512;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24378;&#22823;&#30340;NLP&#27169;&#22411;&#38656;&#35201;&#36234;&#26469;&#36234;&#22823;&#30340;&#27169;&#22411;&#23610;&#23544;&#65292;&#20174;&#32780;&#23548;&#33268;&#22823;&#37327;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25512;&#29702;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#23569;&#27169;&#22411;&#22823;&#23567;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#20165;&#38024;&#23545;&#26435;&#37325;&#30340;&#37327;&#21270;&#31574;&#30053;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#28608;&#27963;&#20989;&#25968;&#30340;&#23436;&#25972;&#31934;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#38750;&#22343;&#21248;&#25110;&#22343;&#21248;&#37327;&#21270;&#25216;&#26415;&#33719;&#24471;&#27599;&#20010;&#26435;&#37325;&#30340;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;LUT-GEMM&#20869;&#26680;&#21152;&#36895;&#20102;&#37327;&#21270;&#30697;&#38453;&#20056;&#27861;&#65292;&#25552;&#20379;&#20102;&#21387;&#32553;&#27604;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#28789;&#27963;&#24179;&#34913;&#12290;&#19982;&#26089;&#26399;&#20165;&#36866;&#29992;&#20110;&#26435;&#37325;&#37327;&#21270;&#30340;&#30697;&#38453;&#20056;&#27861;&#20869;&#26680;&#19981;&#21516;&#65292;LUT-GEMM&#26377;&#25928;&#22320;&#28040;&#38500;&#20102;&#36164;&#28304;&#28040;&#32791;&#26356;&#22823;&#30340;&#21453;&#37327;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in self-supervised learning, combined with the Transformer architecture, have enabled natural language processing (NLP) to achieve remarkably low perplexity. However, powerful NLP models necessitate increasing model size, leading to substantial computational and memory requirements. In this paper, we introduce an efficient inference framework tailored for large-scale generative language models. To reduce the model size, we employ a weight-only quantization strategy while preserving full precision for activations. As a result, we attain sub-4-bit quantization for each weight through non-uniform or uniform quantization techniques. Our proposed kernel, called LUT-GEMM, then accelerates quantized matrix multiplications, offering a flexible balance between compression ratio and accuracy. Unlike earlier matrix multiplication kernels that accommodated weight-only quantization, LUT-GEMM efficiently eliminates the resource-demanding dequantization process for both unifor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#24110;&#21161;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#25512;&#29702;&#24182;&#25512;&#24191;&#21040;&#38590;&#24230;&#26356;&#39640;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#32467;&#21512;GPT-3 code-davinci-002&#27169;&#22411;&#33021;&#22815;&#23436;&#32654;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#22522;&#20934;SCAN&#20013;&#30340;&#25152;&#26377;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2205.10625</link><description>&lt;p&gt;
&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#21487;&#20197;&#20351;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. (arXiv:2205.10625v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#24110;&#21161;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#22797;&#26434;&#25512;&#29702;&#24182;&#25512;&#24191;&#21040;&#38590;&#24230;&#26356;&#39640;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#32467;&#21512;GPT-3 code-davinci-002&#27169;&#22411;&#33021;&#22815;&#23436;&#32654;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#22522;&#20934;SCAN&#20013;&#30340;&#25152;&#26377;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24605;&#32500;&#38142;&#25552;&#31034;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#38656;&#35201;&#35299;&#20915;&#27604;&#25552;&#31034;&#20013;&#30340;&#31034;&#20363;&#26356;&#38590;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#31181;&#26131;&#20110;&#22256;&#38590;&#27867;&#21270;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21363;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#12290;&#35813;&#31574;&#30053;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#22797;&#26434;&#38382;&#39064;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#26356;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#28982;&#21518;&#25353;&#39034;&#24207;&#35299;&#20915;&#23427;&#20204;&#12290;&#35299;&#20915;&#27599;&#20010;&#23376;&#38382;&#39064;&#37117;&#24471;&#30410;&#20110;&#20808;&#21069;&#35299;&#20915;&#30340;&#23376;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#31526;&#21495;&#25805;&#20316;&#12289;&#32452;&#21512;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#33021;&#22815;&#25512;&#24191;&#21040;&#27604;&#25552;&#31034;&#20013;&#26356;&#38590;&#30340;&#38382;&#39064;&#12290;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#21457;&#29616;&#26159;&#65292;&#22312;&#20351;&#29992;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#19982;GPT-3 code-davinci-002&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#20197;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#22522;&#20934;SCAN&#20013;&#30340;&#20219;&#20309;&#20998;&#21106;(&#21253;&#25324;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;)&#65292;&#23613;&#31649;&#20043;&#21069;&#26080;&#27861;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#20219;&#20309;&#20998;&#21106;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (i
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;TraVLR&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;V+L&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#25968;&#25454;&#38598;&#21512;&#25104;&#65292;&#21253;&#25324;&#22235;&#20010;V+L&#25512;&#29702;&#20219;&#21153;&#65292;&#21516;&#26102;&#20351;&#29992;&#21452;&#27169;&#24335;&#20887;&#20313;&#32534;&#30721;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2111.10756</link><description>&lt;p&gt;
TraVLR: &#29616;&#22312;&#20320;&#30475;&#21040;&#23427;&#20102;&#65292;&#29616;&#22312;&#20320;&#27809;&#30475;&#21040;&#23427;&#20102;&#65281;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#25512;&#29702;&#30340;&#21452;&#27169;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
TraVLR: Now You See It, Now You Don't! A Bimodal Dataset for Evaluating Visio-Linguistic Reasoning. (arXiv:2111.10756v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10756
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TraVLR&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#29992;&#20110;&#35780;&#20272;V+L&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#25968;&#25454;&#38598;&#21512;&#25104;&#65292;&#21253;&#25324;&#22235;&#20010;V+L&#25512;&#29702;&#20219;&#21153;&#65292;&#21516;&#26102;&#20351;&#29992;&#21452;&#27169;&#24335;&#20887;&#20313;&#32534;&#30721;&#26469;&#35780;&#20272;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#35270;&#35273;&#35821;&#35328;&#65288;V+L&#65289;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#19981;&#33021;&#20805;&#20998;&#35780;&#20272;&#23427;&#20204;&#22312;&#32479;&#19968;&#31354;&#38388;&#20013;&#34920;&#31034;&#35270;&#35273;&#21644;&#35821;&#35328;&#27010;&#24565;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#38024;&#23545;V+L&#27169;&#22411;&#25552;&#20986;&#20102;&#20960;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#21253;&#25324;&#36328;&#27169;&#24577;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;V+L&#22522;&#20934;&#32463;&#24120;&#25253;&#21578;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#20840;&#23616;&#20934;&#30830;&#24615;&#24471;&#20998;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#30830;&#23450;&#27169;&#22411;&#22833;&#36133;&#21644;&#25104;&#21151;&#30340;&#20855;&#20307;&#25512;&#29702;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;TraVLR&#65292;&#36825;&#26159;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#22235;&#20010;V+L&#25512;&#29702;&#20219;&#21153;&#12290;TraVLR&#30340;&#21512;&#25104;&#24615;&#36136;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#20219;&#21153;&#30456;&#20851;&#32500;&#24230;&#38480;&#21046;&#20854;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#65292;&#20174;&#32780;&#35780;&#20272;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;TraVLR&#20013;&#30340;&#27599;&#20010;&#31034;&#20363;&#37117;&#20197;&#20004;&#31181;&#27169;&#24577;&#20887;&#20313;&#32534;&#30721;&#22330;&#26223;&#65292;&#20351;&#24471;&#22312;&#35757;&#32451;&#25110;&#27979;&#35797;&#26399;&#38388;&#21487;&#20197;&#21024;&#38500;&#25110;&#28155;&#21152;&#20854;&#20013;&#30340;&#20219;&#19968;&#27169;&#24577;&#32780;&#19981;&#20250;&#22833;&#21435;&#30456;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#22235;&#20010;&#26368;&#20808;&#36827;&#30340;V+L&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous visio-linguistic (V+L) representation learning methods have been developed, yet existing datasets do not adequately evaluate the extent to which they represent visual and linguistic concepts in a unified space. We propose several novel evaluation settings for V+L models, including cross-modal transfer. Furthermore, existing V+L benchmarks often report global accuracy scores on the entire dataset, making it difficult to pinpoint the specific reasoning tasks that models fail and succeed at. We present TraVLR, a synthetic dataset comprising four V+L reasoning tasks. TraVLR's synthetic nature allows us to constrain its training and testing distributions along task-relevant dimensions, enabling the evaluation of out-of-distribution generalisation. Each example in TraVLR redundantly encodes the scene in two modalities, allowing either to be dropped or added during training or testing without losing relevant information. We compare the performance of four state-of-the-art V+L models,
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2110.13398</link><description>&lt;p&gt;
&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#29992;&#20110;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Unified Instance and Knowledge Alignment Pretraining for Aspect-based Sentiment Analysis. (arXiv:2110.13398v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.13398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#23454;&#20363;&#21644;&#30693;&#35782;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#39046;&#22495;&#20559;&#31227;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#30340;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#26088;&#22312;&#30830;&#23450;&#23545;&#26576;&#20010;&#26041;&#38754;&#30340;&#24773;&#24863;&#20542;&#21521;&#12290;&#30001;&#20110;&#26114;&#36149;&#19988;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#25104;&#20026;ABSA&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;ABSA&#25968;&#25454;&#38598;&#20043;&#38388;&#24635;&#26159;&#23384;&#22312;&#20005;&#37325;&#30340;&#39046;&#22495;&#20559;&#31227;&#65292;&#30452;&#25509;&#24494;&#35843;&#26102;&#30340;&#30693;&#35782;&#36716;&#31227;&#25928;&#26524;&#19981;&#20339;&#65292;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#20122;&#20248;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#39046;&#22495;&#20559;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#23545;&#40784;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#30693;&#35782;&#23618;&#38754;&#30340;&#23545;&#40784;&#65292;&#23558;&#20854;&#34701;&#20837;&#21040;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27969;&#31243;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#38454;&#27573;&#26816;&#32034;&#37319;&#26679;&#26041;&#27861;&#65292;&#20174;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36873;&#25321;&#19982;&#30446;&#26631;&#39046;&#22495;&#30456;&#20851;&#30340;&#23454;&#20363;&#65292;&#20174;&#32780;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#39046;&#22495;&#23454;&#20363;&#30340;&#23545;&#40784;&#65288;&#31532;&#19968;&#38454;&#27573;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#30693;&#35782;&#25351;&#23548;&#30340;&#31574;&#30053;&#65292;&#36827;&#19968;&#27493;&#26725;&#25509;&#30693;&#35782;&#23618;&#38754;&#30340;&#39046;&#22495;&#24046;&#24322;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;ABSA&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#36229;&#36234;&#24378;&#22522;&#32447;&#30340;&#21331;&#36234;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;ABSA&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;SemEval 2014&#20219;&#21153;4&#12289;SemEval 2015&#20219;&#21153;12&#21644;SemEval 2016&#20219;&#21153;5&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect-based Sentiment Analysis (ABSA) aims to determine the sentiment polarity towards an aspect. Because of the expensive and limited labelled data, the pretraining strategy has become the de-facto standard for ABSA. However, there always exists severe domain shift between the pretraining and downstream ABSA datasets, hindering the effective knowledge transfer when directly finetuning and making the downstream task performs sub-optimal. To mitigate such domain shift, we introduce a unified alignment pretraining framework into the vanilla pretrain-finetune pipeline with both instance- and knowledge-level alignments. Specifically, we first devise a novel coarse-to-fine retrieval sampling approach to select target domain-related instances from the large-scale pretraining dataset, thus aligning the instances between pretraining and target domains (First Stage). Then, we introduce a knowledge guidance-based strategy to further bridge the domain gap at the knowledge level. In practice, we 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32676;&#20307;&#21327;&#21830;&#25968;&#25454;&#38598;&#65292;500&#20010;&#23567;&#32452;&#23545;&#35805;&#21644;14k&#20010;&#35805;&#35821;&#65292;64%&#30340;&#23567;&#32452;&#25104;&#21592;&#33021;&#22815;&#25214;&#21040;&#27604;&#20182;&#20204;&#21333;&#29420;&#25214;&#21040;&#30340;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#27169;&#24335;&#29992;&#20110;&#25429;&#25417;&#21327;&#21830;&#32447;&#32034;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20004;&#31181;&#29983;&#25104;&#21327;&#21830;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2108.05271</link><description>&lt;p&gt;
DeliData: &#29992;&#20110;&#22810;&#26041;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#21327;&#21830;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
DeliData: A dataset for deliberation in multi-party problem solving. (arXiv:2108.05271v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.05271
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#32676;&#20307;&#21327;&#21830;&#25968;&#25454;&#38598;&#65292;500&#20010;&#23567;&#32452;&#23545;&#35805;&#21644;14k&#20010;&#35805;&#35821;&#65292;64%&#30340;&#23567;&#32452;&#25104;&#21592;&#33021;&#22815;&#25214;&#21040;&#27604;&#20182;&#20204;&#21333;&#29420;&#25214;&#21040;&#30340;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#27169;&#24335;&#29992;&#20110;&#25429;&#25417;&#21327;&#21830;&#32447;&#32034;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#20004;&#31181;&#29983;&#25104;&#21327;&#21830;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#21327;&#21830;&#20351;&#20154;&#20204;&#33021;&#22815;&#21327;&#20316;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#36164;&#28304;&#65292;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#23578;&#19981;&#23436;&#21892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#35299;&#20915;&#19968;&#20010;&#24050;&#32463;&#30830;&#31435;&#30340;&#35748;&#30693;&#20219;&#21153;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#21253;&#25324;500&#20010;&#23567;&#32452;&#23545;&#35805;&#21644;14k&#20010;&#35805;&#35821;&#12290;&#22312;&#36825;&#20123;&#23545;&#35805;&#20013;&#65292;64&#65285;&#30340;&#23567;&#32452;&#25104;&#21592;&#33021;&#22815;&#25214;&#21040;&#27604;&#20182;&#20204;&#21333;&#29420;&#25214;&#21040;&#30340;&#26356;&#22909;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#32456;&#35299;&#20915;&#26041;&#26696;&#20026;&#27491;&#30830;&#31572;&#26696;&#30340;&#23567;&#32452;&#20013;&#65292;&#26377;43.8&#65285;&#30340;&#23567;&#32452;&#20013;&#27809;&#26377;&#20219;&#20309;&#21442;&#19982;&#32773;&#33258;&#24049;&#23601;&#33021;&#27491;&#30830;&#35299;&#20915;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27880;&#37322;&#27169;&#24335;&#65292;&#25429;&#25417;&#21327;&#21830;&#32447;&#32034;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;14k&#20010;&#27880;&#37322;&#36807;&#30340;&#35805;&#35821;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;&#20004;&#31181;&#29983;&#25104;&#21327;&#21830;&#35805;&#35821;&#30340;&#26041;&#27861;&#12290;&#25968;&#25454;&#25910;&#38598;&#24179;&#21488;&#12289;&#25968;&#25454;&#38598;&#21644;&#27880;&#37322;&#35821;&#26009;&#24211;&#21487;&#22312;https://delibot.xyz&#19978;&#20844;&#24320;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group deliberation enables people to collaborate and solve problems, however, it is understudied due to a lack of resources. To this end, we introduce the first publicly available dataset containing collaborative conversations on solving a well-established cognitive task, consisting of 500 group dialogues and 14k utterances. In 64% of these conversations, the group members are able to find a better solution than they had identified individually, and in 43.8% of the groups who had a correct answer as their final solution, none of the participants had solved the task correctly by themselves. Furthermore, we propose a novel annotation schema that captures deliberation cues and release all 14k utterances annotated with it. Finally, we use the proposed dataset to develop and evaluate two methods for generating deliberation utterances. The data collection platform, dataset and annotated corpus are publicly available at https://delibot.xyz.
&lt;/p&gt;</description></item></channel></rss>