<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#39640;&#36127;&#36733;&#35821;&#35328;&#23545;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26631;&#39064;&#20026;&#8220;Machine Translation Models are Zero-Shot Detectors of Translation Direction&#8221;&#12290;</title><link>http://arxiv.org/abs/2401.06769</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#26159;&#38646;&#23556;&#20987;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Translation Models are Zero-Shot Detectors of Translation Direction. (arXiv:2401.06769v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22522;&#20110;&#26080;&#30417;&#30563;&#26041;&#27861;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#20854;&#22312;&#39640;&#36127;&#36733;&#35821;&#35328;&#23545;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#35770;&#25991;&#26631;&#39064;&#20026;&#8220;Machine Translation Models are Zero-Shot Detectors of Translation Direction&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#24182;&#34892;&#25991;&#26412;&#30340;&#32763;&#35793;&#26041;&#21521;&#23545;&#20110;&#26426;&#22120;&#32763;&#35793;&#30340;&#35757;&#32451;&#21644;&#35780;&#20272;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#20294;&#20063;&#20855;&#26377;&#27861;&#21307;&#24212;&#29992;&#65292;&#20363;&#22914;&#35299;&#20915;&#21117;&#31363;&#25110;&#20266;&#36896;&#25351;&#25511;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#31616;&#21333;&#30340;&#20551;&#35774;&#65292;&#21363;$p(\text{translation}|\text{original})&gt;p(\text{original}|\text{translation})$&#65292;&#20197;&#20256;&#32479;&#19978;&#34987;&#31216;&#20026;&#32763;&#35793;&#35821;&#25110;&#26426;&#22120;&#32763;&#35793;&#35821;&#20013;&#30340;&#31616;&#21270;&#25928;&#24212;&#20026;&#21160;&#26426;&#65292;&#25506;&#32034;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#32763;&#35793;&#26041;&#21521;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;20&#20010;&#32763;&#35793;&#26041;&#21521;&#36827;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#36164;&#28304;&#20016;&#23500;&#30340;&#35821;&#35328;&#23545;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;&#20110;NMT&#29983;&#25104;&#30340;&#32763;&#35793;&#65292;&#23454;&#29616;&#20102;&#25991;&#26723;&#32423;&#20934;&#30830;&#29575;&#20026;82-96&#65285;&#65292;&#23545;&#20110;&#20154;&#24037;&#32763;&#35793;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;60-81&#65285;&#30340;&#20934;&#30830;&#29575;&#12290;&#20195;&#30721;&#21644;&#28436;&#31034;&#21487;&#22312;https://github.com/ZurichNLP/translation-direction-detection&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting the translation direction of parallel text has applications for machine translation training and evaluation, but also has forensic applications such as resolving plagiarism or forgery allegations. In this work, we explore an unsupervised approach to translation direction detection based on the simple hypothesis that $p(\text{translation}|\text{original})&gt;p(\text{original}|\text{translation})$, motivated by the well-known simplification effect in translationese or machine-translationese. In experiments with massively multilingual machine translation models across 20 translation directions, we confirm the effectiveness of the approach for high-resource language pairs, achieving document-level accuracies of 82-96% for NMT-produced translations, and 60-81% for human translations, depending on the model used. Code and demo are available at https://github.com/ZurichNLP/translation-direction-detection
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;APAR&#30340;&#24182;&#34892;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#29420;&#31435;&#35268;&#21010;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;&#25191;&#34892;&#33258;&#21160;&#24182;&#34892;&#33258;&#22238;&#24402;&#65288;APAR&#65289;&#29983;&#25104;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#39640;&#21534;&#21520;&#37327;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21534;&#21520;&#37327;&#22686;&#21152;&#21644;&#24310;&#36831;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2401.06761</link><description>&lt;p&gt;
APAR: LLMs&#21487;&#20197;&#36827;&#34892;&#33258;&#21160;&#24182;&#34892;&#33258;&#22238;&#24402;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding. (arXiv:2401.06761v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06761
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;APAR&#30340;&#24182;&#34892;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#36827;&#34892;&#35843;&#25972;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#29420;&#31435;&#35268;&#21010;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;&#25191;&#34892;&#33258;&#21160;&#24182;&#34892;&#33258;&#22238;&#24402;&#65288;APAR&#65289;&#29983;&#25104;&#65292;&#20174;&#32780;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#27493;&#39588;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#39640;&#21534;&#21520;&#37327;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#21534;&#21520;&#37327;&#22686;&#21152;&#21644;&#24310;&#36831;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#35201;&#27714;&#26377;&#25928;&#30340;&#37096;&#32626;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#26412;&#33258;&#22238;&#24402;&#35299;&#30721;&#36807;&#31243;&#23545;&#20110;&#23454;&#29616;&#39640;&#25928;&#26381;&#21153;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24182;&#34892;&#33258;&#22238;&#24402;&#29983;&#25104;&#26041;&#27861;&#12290;&#36890;&#36807;&#20197;&#21253;&#21547;&#23618;&#27425;&#32467;&#26500;&#30340;&#36890;&#29992;&#39046;&#22495;&#25968;&#25454;&#20026;&#25351;&#23548;&#36827;&#34892;&#35843;&#25972;&#65292;&#25105;&#20204;&#20351;LLM&#33021;&#22815;&#29420;&#31435;&#35268;&#21010;&#20854;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#36827;&#34892;&#33258;&#21160;&#21270;&#24182;&#34892;&#33258;&#22238;&#24402;&#65288;APAR&#65289;&#29983;&#25104;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#29983;&#25104;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#20165;APAR&#23601;&#21487;&#20197;&#23454;&#29616;&#22810;&#36798;2&#20493;&#30340;&#21152;&#36895;&#65292;&#32780;&#24403;&#19982;&#25512;&#27979;&#35299;&#30721;&#32467;&#21512;&#26102;&#65292;&#21152;&#36895;&#24230;&#21487;&#20197;&#36798;&#21040;4&#20493;&#12290;&#27492;&#22806;&#65292;APAR&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#20102;&#38190;&#20540;&#32531;&#23384;&#28040;&#32791;&#21644;&#27880;&#24847;&#21147;&#35745;&#31639;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26381;&#21153;&#26694;&#26550;&#30456;&#27604;&#65292;&#22312;&#39640;&#21534;&#21520;&#37327;&#22330;&#26223;&#20013;&#65292;&#36825;&#23548;&#33268;&#21534;&#21520;&#37327;&#22686;&#21152;&#20102;20-70&#65285;&#65292;&#24310;&#36831;&#38477;&#20302;&#20102;20-35&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The massive adoption of large language models (LLMs) demands efficient deployment strategies. However, the auto-regressive decoding process, which is fundamental to how most LLMs generate text, poses challenges to achieve efficient serving. In this work, we introduce a parallel auto-regressive generation method. By instruct-tuning on general domain data that contains hierarchical structures, we enable LLMs to independently plan their generation process and perform auto-parallel auto-regressive (APAR) generation, significantly reducing the number of generation steps. APAR alone can achieve up to 2x speed-up, and when combined with speculative decoding, the speed-up can reach up to 4x. In addition, APAR reduces the key-value cache consumption and attention computation during generation. This leads to a throughput increase of 20-70% and a latency reduce of 20-35% in high-throughput scenarios, compared to state-of-the-art serving frameworks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#29616;&#20195;&#24230;&#37327;&#26631;&#20934;&#30340;&#8220;&#21160;&#24577;&#33539;&#22260;&#8221;&#65292;&#20197;&#25552;&#20379;&#23545;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#21644;&#20869;&#37096;&#24471;&#20998;&#24046;&#24322;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#27700;&#24179;&#30340;&#31995;&#32479;&#24046;&#24322;&#36827;&#34892;&#34913;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.06760</link><description>&lt;p&gt;
&#35299;&#20915;&#24230;&#37327;&#25968;&#20540;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#36855;&#23467;&#65306;&#23548;&#33322;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;
Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies. (arXiv:2401.06760v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#29616;&#20195;&#24230;&#37327;&#26631;&#20934;&#30340;&#8220;&#21160;&#24577;&#33539;&#22260;&#8221;&#65292;&#20197;&#25552;&#20379;&#23545;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#21644;&#20869;&#37096;&#24471;&#20998;&#24046;&#24322;&#30340;&#20849;&#21516;&#29702;&#35299;&#65292;&#24182;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#27700;&#24179;&#30340;&#31995;&#32479;&#24046;&#24322;&#36827;&#34892;&#34913;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21313;&#24180;&#21069;&#65292;&#26426;&#22120;&#32763;&#35793;&#30740;&#31350;&#20013;&#26377;&#19968;&#20010;&#21333;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;BLEU&#12290;&#22914;&#20170;&#65292;&#27809;&#26377;&#36825;&#26679;&#30340;&#20849;&#35782;&#65292;&#22240;&#27492;&#30740;&#31350;&#20154;&#21592;&#24456;&#38590;&#21457;&#23637;&#21644;&#20445;&#25345;&#20043;&#21069;&#25512;&#21160;&#30740;&#31350;&#21644;&#37096;&#32626;&#20915;&#31574;&#30340;&#37027;&#31181;&#21551;&#21457;&#24615;&#30452;&#35273;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#29616;&#20195;&#24230;&#37327;&#26631;&#20934;&#30340;&#8220;&#21160;&#24577;&#33539;&#22260;&#8221;&#65292;&#20197;&#25552;&#20379;&#23545;&#24230;&#37327;&#26631;&#20934;&#20043;&#38388;&#21644;&#20869;&#37096;&#24471;&#20998;&#24046;&#24322;&#30340;&#20849;&#21516;&#29702;&#35299;&#65307;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#35201;&#38382;&#22312;&#24230;&#37327;&#26631;&#20934;Y&#20013;&#65292;&#20004;&#20010;&#31995;&#32479;&#38656;&#35201;&#26377;&#22810;&#22823;&#30340;&#24471;&#20998;&#24046;&#24322;X&#65292;&#20154;&#31867;&#25165;&#33021;&#27880;&#24847;&#21040;&#65311;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26032;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;ToShip23&#36827;&#34892;&#35780;&#20272;&#65292;&#20351;&#29992;&#23427;&#21457;&#29616;&#24230;&#37327;&#26631;&#20934;&#36798;&#21040;&#20154;&#31867;&#24863;&#30693;&#27700;&#24179;&#30340;&#31995;&#32479;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#25104;&#23545;&#31995;&#32479;&#20934;&#30830;&#24615;&#26469;&#34913;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#19982;&#26631;&#20934;&#30340;&#32479;&#35745;p&#20540;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#31283;&#23450;&#24615;&#30456;&#27604;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#24314;&#31435;&#24046;&#24322;&#20934;&#30830;&#24615;&#26356;&#20026;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain the kinds of heuristic intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the "dynamic range" of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask what point difference X in metric Y is required between two systems for humans to notice? We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20316;&#32773;&#25991;&#29486;&#30340;&#25991;&#20307;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#20307;&#20998;&#31867;&#12289;&#21333;&#20316;&#32773;&#21464;&#21270;&#26816;&#27979;&#21644;&#22810;&#20316;&#32773;&#21464;&#21270;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20215;&#20540;&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#21644;&#26435;&#37325;&#20248;&#21270;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2401.06752</link><description>&lt;p&gt;
&#22810;&#20316;&#32773;&#25991;&#29486;&#30340;&#25991;&#20307;&#20998;&#26512;&#29992;&#20110;&#20316;&#32773;&#21644;&#20316;&#32773;&#25991;&#20307;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Stylometry Analysis of Multi-authored Documents for Authorship and Author Style Change Detection. (arXiv:2401.06752v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#20316;&#32773;&#25991;&#29486;&#30340;&#25991;&#20307;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#20307;&#20998;&#31867;&#12289;&#21333;&#20316;&#32773;&#21464;&#21270;&#26816;&#27979;&#21644;&#22810;&#20316;&#32773;&#21464;&#21270;&#26816;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20215;&#20540;&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24182;&#25972;&#21512;&#20102;&#22810;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31639;&#27861;&#21644;&#26435;&#37325;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#25991;&#26412;&#29983;&#25104;&#24037;&#20855;&#30340;&#24191;&#27867;&#24212;&#29992;&#32473;&#25991;&#29486;&#26469;&#28304;&#30340;&#32771;&#35777;&#12289;&#37492;&#21035;&#21644;&#20316;&#32773;&#26816;&#27979;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#25991;&#20307;&#20998;&#26512;&#30340;&#36827;&#23637;&#20026;&#20351;&#29992;&#25991;&#20307;&#20998;&#26512;&#25216;&#26415;&#22312;&#22810;&#20316;&#32773;&#25991;&#29486;&#20013;&#36827;&#34892;&#33258;&#21160;&#20316;&#32773;&#26816;&#27979;&#21644;&#20316;&#32773;&#21464;&#21270;&#26816;&#27979;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25991;&#20307;&#20998;&#26512;&#21487;&#20197;&#20316;&#20026;&#25991;&#29486;&#26469;&#28304;&#21644;&#37492;&#21035;&#30340;&#39318;&#35201;&#27493;&#39588;&#65292;&#36890;&#36807;&#20316;&#32773;&#26816;&#27979;&#36827;&#34892;&#39564;&#35777;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25991;&#20307;&#20998;&#26512;&#30340;&#19977;&#20010;&#20851;&#38190;&#20219;&#21153;&#65306;&#65288;i&#65289;&#21333;&#20316;&#32773;&#21644;&#22810;&#20316;&#32773;&#25991;&#29486;&#30340;&#20998;&#31867;&#65292;&#65288;ii&#65289;&#21333;&#19968;&#21464;&#21270;&#26816;&#27979;&#65292;&#21253;&#25324;&#30830;&#23450;&#20316;&#32773;&#21464;&#25442;&#28857;&#65292;&#21644;&#65288;iii&#65289;&#22810;&#20316;&#32773;&#21464;&#25442;&#26816;&#27979;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#20219;&#21153;&#37117;&#24314;&#31435;&#20026;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20215;&#20540;&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#25972;&#21512;&#20102;&#20960;&#31181;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31639;&#27861;&#21644;&#26435;&#37325;&#20248;&#21270;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the increasing use of Artificial Intelligence based text generation tools has posed new challenges in document provenance, authentication, and authorship detection. However, advancements in stylometry have provided opportunities for automatic authorship and author change detection in multi-authored documents using style analysis techniques. Style analysis can serve as a primary step toward document provenance and authentication through authorship detection. This paper investigates three key tasks of style analysis: (i) classification of single and multi-authored documents, (ii) single change detection, which involves identifying the point where the author switches, and (iii) multiple author-switching detection in multi-authored documents. We formulate all three tasks as classification problems and propose a merit-based fusion framework that integrates several state-of-the-art natural language processing (NLP) algorithms and weight optimization techniques. We also explo
&lt;/p&gt;</description></item><item><title>&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.06751</link><description>&lt;p&gt;
Easy Training Data&#23545;&#20110;&#22256;&#38590;&#20219;&#21153;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06751
&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#33021;&#22815;&#30456;&#23545;&#33391;&#22909;&#22320;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#65292;&#24182;&#19988;&#21363;&#20351;&#22312;&#20851;&#27880;&#20110;&#22256;&#38590;&#25968;&#25454;&#30340;&#24615;&#33021;&#26102;&#65292;&#25910;&#38598;&#21644;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#22256;&#38590;&#35757;&#32451;&#25968;&#25454;&#22312;&#23450;&#20041;&#19978;&#24456;&#38590;&#27491;&#30830;&#26631;&#35760;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#35757;&#32451;&#27169;&#22411;&#22312;&#22256;&#38590;&#27979;&#35797;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65311;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#21487;&#25193;&#23637;&#30417;&#30563;&#38382;&#39064;&#65292;&#22312;&#35821;&#35328;&#27169;&#22411;&#19981;&#26029;&#25913;&#36827;&#30340;&#36807;&#31243;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#35770;&#65292;&#21363;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20174;&#26131;&#21040;&#38590;&#30340;&#25968;&#25454;&#27867;&#21270;&#30456;&#23545;&#33391;&#22909;&#65292;&#29978;&#33267;&#34920;&#29616;&#24471;&#21644;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#8220;oracle&#8221;&#27169;&#22411;&#19968;&#26679;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;&#31616;&#21333;&#30340;&#35757;&#32451;&#26041;&#27861;&#65288;&#22914;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#32447;&#24615;&#20998;&#31867;&#22120;&#22836;&#21644;QLoRA&#65289;&#23637;&#31034;&#20102;&#36825;&#31181;&#20174;&#26131;&#21040;&#38590;&#30340;&#27867;&#21270;&#65292;&#38024;&#23545;&#19971;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#28857;&#38590;&#24230;&#24230;&#37327;&#65292;&#21253;&#25324;&#20845;&#20010;&#32463;&#39564;&#22810;&#26679;&#30340;&#20154;&#31867;&#38590;&#24230;&#24230;&#37327;&#65288;&#22914;&#24180;&#32423;&#27700;&#24179;&#65289;&#21644;&#19968;&#20010;&#22522;&#20110;&#27169;&#22411;&#30340;&#24230;&#37327;&#65288;&#22522;&#20110;&#25439;&#22833;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#21363;&#20351;&#26368;&#20851;&#24515;&#27169;&#22411;&#22312;&#22256;&#38590;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#65292;&#25910;&#38598;&#24182;&#35757;&#32451;&#26131;&#25968;&#25454;&#21487;&#33021;&#27604;&#22256;&#38590;&#25968;&#25454;&#26356;&#22909;&#65292;&#22240;&#20026;&#22256;&#38590;&#25968;&#25454;&#36890;&#24120;&#26356;&#22024;&#26434;&#21644;&#26114;&#36149;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current language models often generalize relatively well from easy to hard data, even performing as well as "oracle" models trained on hard data. We demonstrate this kind of easy-to-hard generalization using simple training methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect and train on easy data rather than hard data, since hard data is generally noisier and costli
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#36866;&#24212;&#24050;&#35757;&#32451;&#30340;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#21040;&#26032;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#20013;&#35282;&#33394;&#25552;&#21462;&#30340;&#22810;&#26679;&#24615;&#21644;&#38750;&#30495;&#23454;&#19990;&#30028;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06742</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26469;&#25913;&#36827;&#23545;&#35805;&#20013;&#30340;&#35282;&#33394;&#25552;&#21462;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain. (arXiv:2401.06742v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#36866;&#24212;&#24050;&#35757;&#32451;&#30340;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#21040;&#26032;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#23545;&#35805;&#20013;&#35282;&#33394;&#25552;&#21462;&#30340;&#22810;&#26679;&#24615;&#21644;&#38750;&#30495;&#23454;&#19990;&#30028;&#35774;&#32622;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#23453;&#36149;&#30340;&#25968;&#25454;&#38598;&#22914;PersonaChat&#20026;&#35757;&#32451;&#26377;&#22522;&#20110;&#35282;&#33394;&#30340;&#23545;&#35805;&#20195;&#29702;&#25552;&#20379;&#20102;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#22312;&#23545;&#35805;&#21644;&#21465;&#20107;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#26041;&#38754;&#32570;&#20047;&#65292;&#20027;&#35201;&#23384;&#22312;&#20110;&#8220;&#30495;&#23454;&#8221;&#19990;&#30028;&#20013;&#12290;&#20026;&#20102;&#24320;&#21457;&#20855;&#26377;&#29420;&#29305;&#35282;&#33394;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#27169;&#22411;&#34987;&#35757;&#32451;&#20197;&#22312;&#32473;&#23450;&#29305;&#23450;&#35282;&#33394;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#35805;&#65292;&#20294;&#25163;&#24037;&#21046;&#20316;&#36825;&#20123;&#35282;&#33394;&#21487;&#33021;&#32791;&#26102;&#65292;&#22240;&#27492;&#23384;&#22312;&#20174;&#29616;&#26377;&#29305;&#23450;&#35282;&#33394;&#23545;&#35805;&#20013;&#33258;&#21160;&#25552;&#21462;&#35282;&#33394;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#20063;&#26159;&#22312;&#20174;PersonaChat&#34893;&#29983;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#19988;&#24456;&#38590;&#20174;&#38750;&#30495;&#23454;&#19990;&#30028;&#30340;&#23545;&#35805;&#35774;&#32622;&#20013;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#35282;&#33394;&#20449;&#24687;&#65292;&#20363;&#22914;&#20197;&#24187;&#24819;&#20026;&#20027;&#39064;&#30340;&#25968;&#25454;&#38598;LIGHT&#12290;&#21019;&#24314;&#26032;&#25968;&#25454;&#20197;&#35757;&#32451;&#29305;&#23450;&#35774;&#32622;&#30340;&#27169;&#22411;&#26159;&#20154;&#21147;&#23494;&#38598;&#22411;&#30340;&#65292;&#22240;&#27492;&#20195;&#20215;&#36807;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#21518;&#26399;&#35843;&#25972;&#24050;&#35757;&#32451;&#30340;&#35282;&#33394;&#25552;&#21462;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#35774;&#32622;
&lt;/p&gt;
&lt;p&gt;
While valuable datasets such as PersonaChat provide a foundation for training persona-grounded dialogue agents, they lack diversity in conversational and narrative settings, primarily existing in the "real" world. To develop dialogue agents with unique personas, models are trained to converse given a specific persona, but hand-crafting these persona can be time-consuming, thus methods exist to automatically extract persona information from existing character-specific dialogue. However, these persona-extraction models are also trained on datasets derived from PersonaChat and struggle to provide high-quality persona information from conversational settings that do not take place in the real world, such as the fantasy-focused dataset, LIGHT. Creating new data to train models on a specific setting is human-intensive, thus prohibitively expensive. To address both these issues, we introduce a natural language inference method for post-hoc adapting a trained persona extraction model to a new 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#39640;&#38169;&#35823;&#29575;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06730</link><description>&lt;p&gt;
&#19981;&#21487;&#38752;&#30340;&#20381;&#36182;&#65306;&#35821;&#35328;&#27169;&#22411;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty. (arXiv:2401.06730v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#19981;&#24895;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#39640;&#38169;&#35823;&#29575;&#12290;&#23454;&#39564;&#36824;&#34920;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#28982;&#35821;&#35328;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#40664;&#35748;&#25509;&#21475;&#65292;&#35821;&#35328;&#27169;&#22411;&#36866;&#24403;&#22320;&#20256;&#36798;&#19979;&#28216;&#24212;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#23545;&#20854;&#22238;&#31572;&#30340;&#32622;&#20449;&#24230;&#65292;&#20197;&#21450;&#19979;&#28216;&#29992;&#25143;&#23545;&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#21453;&#24212;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#20844;&#24320;&#37096;&#32626;&#30340;&#27169;&#22411;&#65292;&#21457;&#29616;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#65292;&#21363;&#20351;&#20135;&#29983;&#20102;&#38169;&#35823;&#31572;&#26696;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#26080;&#27861;&#34920;&#36798;&#19981;&#30830;&#23450;&#24615;&#12290;&#34429;&#28982;&#21487;&#20197;&#26126;&#30830;&#35201;&#27714;&#35821;&#35328;&#27169;&#22411;&#34920;&#36798;&#32622;&#20449;&#24230;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#36807;&#20110;&#33258;&#20449;&#65292;&#23548;&#33268;&#22312;&#32622;&#20449;&#30340;&#22238;&#31572;&#20013;&#38169;&#35823;&#29575;&#39640;&#36798;&#24179;&#22343;47%&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#23454;&#39564;&#27979;&#35797;&#20102;&#35821;&#35328;&#27169;&#22411;&#36807;&#24230;&#33258;&#20449;&#30340;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#29992;&#25143;&#26080;&#35770;&#26159;&#21542;&#26631;&#35760;&#20102;&#30830;&#23450;&#24615;&#37117;&#20250;&#20005;&#37325;&#20381;&#36182;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;RLHF&#23545;&#40784;&#20013;&#20351;&#29992;&#30340;&#20559;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#23545;&#24102;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#25991;&#26412;&#26377;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#31361;&#20986;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work hig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23558;&#27861;&#23450;&#25512;&#29702;&#37325;&#26032;&#23450;&#20041;&#20026;&#31867;&#27604;&#20219;&#21153;&#65292;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#24341;&#20837;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#38590;&#24230;&#30456;&#24403;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#26426;&#21046;&#21644;&#31867;&#27604;&#27169;&#22411;&#35299;&#20915;&#27861;&#23450;&#25512;&#29702;&#38382;&#39064;&#65292;&#22312;&#20043;&#21069;&#30340;&#21487;&#27604;&#24037;&#20316;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.06715</link><description>&lt;p&gt;
&#37325;&#26032;&#23450;&#20041;&#31246;&#27861;&#25512;&#35770;&#20026;&#31867;&#27604;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Reframing Tax Law Entailment as Analogical Reasoning. (arXiv:2401.06715v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23558;&#27861;&#23450;&#25512;&#29702;&#37325;&#26032;&#23450;&#20041;&#20026;&#31867;&#27604;&#20219;&#21153;&#65292;&#36890;&#36807;&#22686;&#21152;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#24341;&#20837;&#35299;&#37322;&#24615;&#22240;&#32032;&#65292;&#23637;&#31034;&#20102;&#36825;&#20010;&#20219;&#21153;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#38590;&#24230;&#30456;&#24403;&#65292;&#24182;&#21033;&#29992;&#26816;&#32034;&#26426;&#21046;&#21644;&#31867;&#27604;&#27169;&#22411;&#35299;&#20915;&#27861;&#23450;&#25512;&#29702;&#38382;&#39064;&#65292;&#22312;&#20043;&#21069;&#30340;&#21487;&#27604;&#24037;&#20316;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27861;&#23450;&#25512;&#29702;&#26159;&#25351;&#23558;&#31435;&#27861;&#35268;&#23450;&#24212;&#29992;&#20110;&#29992;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#19968;&#31995;&#21015;&#26696;&#20363;&#20107;&#23454;&#12290;&#25105;&#20204;&#23558;&#27861;&#23450;&#25512;&#29702;&#37325;&#26032;&#23450;&#20041;&#20026;&#31867;&#27604;&#20219;&#21153;&#65292;&#20854;&#20013;&#27599;&#20010;&#31867;&#27604;&#20219;&#21153;&#23454;&#20363;&#28041;&#21450;&#20004;&#20010;&#27861;&#23450;&#25512;&#29702;&#23454;&#20363;&#30340;&#32452;&#21512;&#12290;&#36825;&#26679;&#20570;&#21487;&#20197;&#23558;&#25968;&#25454;&#38598;&#22823;&#23567;&#22686;&#21152;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#24341;&#20837;&#35299;&#37322;&#24615;&#22240;&#32032;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#20219;&#21153;&#23545;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#35828;&#19982;&#21407;&#22987;&#20219;&#21153;&#30340;&#38590;&#24230;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#21512;&#26816;&#32034;&#26426;&#21046;&#21644;&#31867;&#27604;&#27169;&#22411;&#26469;&#35299;&#20915;&#27861;&#23450;&#25512;&#29702;&#38382;&#39064;&#65292;&#24182;&#22312;&#20043;&#21069;&#30340;&#21487;&#27604;&#24037;&#20316;&#19978;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statutory reasoning refers to the application of legislative provisions to a series of case facts described in natural language. We re-frame statutory reasoning as an analogy task, where each instance of the analogy task involves a combination of two instances of statutory reasoning. This increases the dataset size by two orders of magnitude, and introduces an element of interpretability. We show that this task is roughly as difficult to Natural Language Processing models as the original task. Finally, we come back to statutory reasoning, solving it with a combination of a retrieval mechanism and analogy models, and showing some progress on prior comparable work.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06712</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#36827;&#34892;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Detection of Machine-Generated Text using Style Representations. (arXiv:2401.06712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#26679;&#24335;&#34920;&#31034;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#30340;&#21306;&#21035;&#65292;&#20197;&#35299;&#20915;&#28389;&#29992;&#35821;&#35328;&#27169;&#22411;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#25351;&#23548;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#20351;&#24471;&#20154;&#31867;&#20889;&#20316;&#30340;&#36924;&#30495;&#27169;&#20223;&#38754;&#20020;&#30528;&#37325;&#22823;&#28389;&#29992;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#26816;&#27979;&#19968;&#27573;&#25991;&#26412;&#26159;&#30001;&#35821;&#35328;&#27169;&#22411;&#36824;&#26159;&#20154;&#31867;&#25776;&#20889;&#32780;&#25104;&#26469;&#23545;&#25239;&#27492;&#31867;&#28389;&#29992;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26679;&#24335;&#34920;&#31034;&#30340;&#23567;&#26679;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#31070;&#32463;&#32593;&#32476;&#26816;&#27979;&#22120;&#22312;&#38754;&#23545;&#25968;&#25454;&#36716;&#25442;&#26102;&#30340;&#35268;&#32422;&#19981;&#36275;&#30340;&#25361;&#25112;&#65292;&#21516;&#26102;&#20063;&#36991;&#20813;&#20102;&#22312;&#25512;&#29702;&#25110;&#26816;&#27979;&#26102;&#38656;&#35201;&#35775;&#38382;&#21487;&#33021;&#29983;&#25104;&#25991;&#26723;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of instruction-tuned language models that convincingly mimic human writing poses a significant risk of abuse. For example, such models could be used for plagiarism, disinformation, spam, or phishing. However, such abuse may be counteracted with the ability to detect whether a piece of text was composed by a language model rather than a human. Some previous approaches to this problem have relied on supervised methods trained on corpora of confirmed human and machine-written documents. Unfortunately, model under-specification poses an unavoidable challenge for neural network-based detectors, making them brittle in the face of data shifts, such as the release of further language models producing still more fluent text than the models used to train the detectors. Other previous approaches require access to the models that may have generated a document in question at inference or detection time, which is often impractical. In light of these challenges, we pursue a fundamentally d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#24515;&#29702;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27880;&#35299;LoST&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#34920;&#26126;&#20302;&#33258;&#23562;&#23384;&#22312;&#30340;&#24494;&#22937;&#25991;&#26412;&#25552;&#31034;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;NLP&#27169;&#22411;&#23545;&#35302;&#21457;&#35789;&#12289;LoST&#25351;&#26631;&#21644;&#21518;&#26524;&#36825;&#19977;&#31867;&#25991;&#26412;&#25552;&#31034;&#26356;&#21152;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.06709</link><description>&lt;p&gt;
&#22312;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#24515;&#29702;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text. (arXiv:2401.06709v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06709
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#24515;&#29702;&#27010;&#24565;&#25552;&#21462;&#21644;&#20998;&#31867;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#65292;&#24182;&#36890;&#36807;&#27880;&#35299;LoST&#25968;&#25454;&#38598;&#26469;&#25429;&#25417;&#34920;&#26126;&#20302;&#33258;&#23562;&#23384;&#22312;&#30340;&#24494;&#22937;&#25991;&#26412;&#25552;&#31034;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;NLP&#27169;&#22411;&#23545;&#35302;&#21457;&#35789;&#12289;LoST&#25351;&#26631;&#21644;&#21518;&#26524;&#36825;&#19977;&#31867;&#25991;&#26412;&#25552;&#31034;&#26356;&#21152;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;NLP&#30740;&#31350;&#31038;&#21306;&#26368;&#36817;&#22312;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#35745;&#31639;&#36827;&#23637;&#20013;&#35265;&#35777;&#20102;&#19968;&#27874;&#22797;&#26434;&#30340;&#35821;&#35328;&#20351;&#29992;&#21644;&#33258;&#25105;&#24863;&#30693;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;AI&#27169;&#22411;&#30340;&#24314;&#31435;&#12290;&#36825;&#20123;&#36127;&#36131;&#20219;&#30340;AI&#27169;&#22411;&#26377;&#21161;&#20110;&#20174;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#29992;&#25143;&#25776;&#20889;&#30340;&#25991;&#26412;&#20013;&#37327;&#21270;&#24515;&#29702;&#27010;&#24565;&#12290;&#22312;&#36229;&#36234;&#20302;&#32423;&#65288;&#20998;&#31867;&#65289;&#20219;&#21153;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#30340;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#25552;&#21319;&#21040;&#26356;&#39640;&#32423;&#21035;&#30340;&#21487;&#38752;&#24615;&#20998;&#26512;&#20219;&#21153;&#65292;&#36890;&#36807;&#35299;&#37322;&#30340;&#35282;&#24230;&#23558;&#20043;&#20316;&#20026;&#19968;&#31181;&#23433;&#20840;&#25514;&#26045;&#12290;&#25105;&#20204;&#27880;&#37322;&#20102;LoST&#25968;&#25454;&#38598;&#65292;&#20197;&#25429;&#25417;&#34920;&#26126;Reddit&#29992;&#25143;&#21457;&#24086;&#20013;&#23384;&#22312;&#20302;&#33258;&#23562;&#30340;&#24494;&#22937;&#25991;&#26412;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25351;&#20986;&#65292;&#29992;&#20110;&#30830;&#23450;&#20302;&#33258;&#23562;&#23384;&#22312;&#30340;NLP&#27169;&#22411;&#26356;&#21152;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#25552;&#31034;&#65306;&#65288;i&#65289;&#35302;&#21457;&#35789;&#65306;&#35302;&#21457;&#24515;&#29702;&#25200;&#21160;&#30340;&#35789;&#27719;&#65292;&#65288;ii&#65289;LoST&#25351;&#26631;&#65306;&#24378;&#35843;&#20302;&#33258;&#23562;&#30340;&#25991;&#26412;&#25351;&#26631;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#21518;&#26524;&#65306;&#25551;&#36848;&#24773;&#32490;&#31283;&#23450;&#24615;&#21518;&#26524;&#30340;&#35789;&#27719;&#12290;
&lt;/p&gt;
&lt;p&gt;
The social NLP research community witness a recent surge in the computational advancements of mental health analysis to build responsible AI models for a complex interplay between language use and self-perception. Such responsible AI models aid in quantifying the psychological concepts from user-penned texts on social media. On thinking beyond the low-level (classification) task, we advance the existing binary classification dataset, towards a higher-level task of reliability analysis through the lens of explanations, posing it as one of the safety measures. We annotate the LoST dataset to capture nuanced textual cues that suggest the presence of low self-esteem in the posts of Reddit users. We further state that the NLP models developed for determining the presence of low self-esteem, focus more on three types of textual cues: (i) Trigger: words that triggers mental disturbance, (ii) LoST indicators: text indicators emphasizing low self-esteem, and (iii) Consequences: words describing
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20505;&#36873;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33609;&#31295;&#27169;&#22411;&#20013;&#37319;&#26679;&#22810;&#20010;&#20505;&#36873;&#26631;&#35760;&#24182;&#36827;&#34892;&#25209;&#27425;&#39564;&#35777;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25509;&#21463;&#29575;&#24182;&#20248;&#20110;&#26631;&#20934;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06706</link><description>&lt;p&gt;
&#22810;&#20505;&#36873;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Multi-Candidate Speculative Decoding. (arXiv:2401.06706v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20505;&#36873;&#25512;&#27979;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#33609;&#31295;&#27169;&#22411;&#20013;&#37319;&#26679;&#22810;&#20010;&#20505;&#36873;&#26631;&#35760;&#24182;&#36827;&#34892;&#25209;&#27425;&#39564;&#35777;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25509;&#21463;&#29575;&#24182;&#20248;&#20110;&#26631;&#20934;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#33021;&#21147;&#65292;&#20294;&#20351;&#29992;&#33258;&#22238;&#24402;&#29983;&#25104;&#25991;&#26412;&#30340;&#25928;&#29575;&#36739;&#20302;&#12290;&#20854;&#20013;&#19968;&#31181;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#26159;&#25512;&#27979;&#35299;&#30721;&#65292;&#23427;&#20174;&#19968;&#20010;&#24555;&#36895;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#20505;&#36873;&#27573;&#33853;&#65288;&#19968;&#31995;&#21015;&#30340;&#26631;&#35760;&#65289;&#65292;&#28982;&#21518;&#36890;&#36807;&#30446;&#26631;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#20505;&#36873;&#26631;&#35760;&#30340;&#25509;&#21463;&#29575;&#21463;&#21040;&#27169;&#22411;&#12289;&#25968;&#25454;&#38598;&#21644;&#35299;&#30721;&#35774;&#32622;&#31561;&#22810;&#20010;&#22240;&#32032;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#33609;&#31295;&#27169;&#22411;&#20013;&#37319;&#26679;&#22810;&#20010;&#20505;&#36873;&#26631;&#35760;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#25209;&#27425;&#36827;&#34892;&#39564;&#35777;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#39640;&#25928;&#30340;&#22810;&#20505;&#36873;&#39564;&#35777;&#31639;&#27861;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#30446;&#26631;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#37117;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#25509;&#21463;&#29575;&#25552;&#39640;&#65292;&#22987;&#32456;&#20248;&#20110;&#26631;&#20934;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have shown impressive capabilities across a variety of NLP tasks, yet their generating text autoregressively is time-consuming. One way to speed them up is speculative decoding, which generates candidate segments (a sequence of tokens) from a fast draft model that is then verified in parallel by the target model. However, the acceptance rate of candidate tokens receives limitations from several factors, such as the model, the dataset, and the decoding setup. This paper proposes sampling multiple candidates from a draft model and then organising them in batches for verification. We design algorithms for efficient multi-candidate verification while maintaining the distribution of the target model. Our approach shows significant improvements in acceptance rates on multiple datasets and models, consistently outperforming standard speculative decoding.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06692</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#39564;&#35774;&#35745;&#26694;&#26550;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#30340;&#27880;&#37322;&#25104;&#26412;&#65292;&#24182;&#35299;&#20915;&#20102;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25351;&#23548;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#38480;&#26631;&#31614;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#22312;&#23454;&#29616;&#20102;&#20196;&#20154;&#24778;&#21497;&#30340;&#38646;&#23556;&#20987;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#20026;&#25351;&#20196;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#22238;&#31572;&#25152;&#38656;&#30340;&#27880;&#37322;&#24037;&#20316;&#27491;&#22312;&#21464;&#24471;&#38590;&#20197;&#25215;&#21463;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;&#25351;&#20196;&#25968;&#25454;&#38598;&#25152;&#28085;&#30422;&#30340;&#20219;&#21153;&#25968;&#37327;&#30340;&#22686;&#21152;&#12290;&#20027;&#21160;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#27744;&#20013;&#30830;&#23450;&#26377;&#29992;&#30340;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#20294;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#20854;&#22312;LLMs&#29615;&#22659;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#20943;&#23569;SFT&#30340;&#27880;&#37322;&#25104;&#26412;&#24182;&#35268;&#36991;&#20027;&#21160;&#23398;&#20064;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23454;&#39564;&#35774;&#35745;&#12290;&#23454;&#39564;&#35774;&#35745;&#25216;&#26415;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#36890;&#24120;&#26368;&#22823;&#21270;&#26576;&#31181;&#19981;&#30830;&#23450;&#24615;&#21644;/&#25110;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23454;&#26045;&#20102;&#19968;&#20010;&#35780;&#20272;&#22810;&#31181;&#29616;&#26377;&#21644;&#26032;&#39062;&#30340;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06688</link><description>&lt;p&gt;
&#19981;&#35201;&#25490;&#21517;&#65292;&#35201;&#21512;&#24182;&#65281;&#20351;&#29992;&#36136;&#37327;&#20272;&#35745;&#26469;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Don't Rank, Combine! Combining Machine Translation Hypotheses Using Quality Estimation. (arXiv:2401.06688v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06688
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#21512;&#24182;&#26426;&#22120;&#32763;&#35793;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#19978;&#25552;&#21319;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;&#36890;&#36807;&#20351;&#29992;&#20505;&#36873;&#27744;&#21644;QE&#25351;&#26631;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#36890;&#36807;&#32473;&#23450;&#28304;&#21477;&#23376;&#20272;&#35745;&#30446;&#26631;&#21477;&#23376;&#30340;&#27010;&#29575;&#65292;&#20294;&#36825;&#20123;&#20272;&#35745;&#21487;&#33021;&#19982;&#20154;&#31867;&#21916;&#22909;&#19981;&#19968;&#33268;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;QE-fusion&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#30340;&#36136;&#37327;&#20272;&#35745;&#25351;&#26631;&#65288;QE&#65289;&#26469;&#32508;&#21512;&#25913;&#36827;&#32763;&#35793;&#32467;&#26524;&#12290;QE-fusion&#21033;&#29992;&#20174;&#27169;&#22411;&#20013;&#25277;&#21462;&#30340;&#20505;&#36873;&#27744;&#65292;&#20351;&#29992;&#20687;CometKiwi&#36825;&#26679;&#30340;QE&#25351;&#26631;&#32452;&#21512;&#19981;&#21516;&#20505;&#36873;&#30340;&#29255;&#27573;&#12290;&#25105;&#20204;&#23558;QE-fusion&#19982;&#27874;&#26463;&#25628;&#32034;&#21644;&#26368;&#36817;&#30340;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#65288;&#22914;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#25110;QE-&#37325;&#26032;&#25490;&#24207;&#65289;&#36827;&#34892;&#27604;&#36739;&#12290;&#24403;&#24212;&#29992;&#20110;&#29992;&#20110;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;PolyLM&#12289;XGLM&#12289;Llama2&#21644;Mistral&#65289;&#21644;&#22810;&#35821;&#35328;&#32763;&#35793;&#27169;&#22411;&#65288;NLLB&#65289;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;COMET&#21644;BLEURT&#35780;&#20998;&#26041;&#38754;&#22987;&#32456;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#28085;&#30422;&#20116;&#31181;&#35821;&#35328;&#23545;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#30001;&#20110;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25913;&#36827;&#26356;&#22823;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20135;&#29983;&#22810;&#26679;&#19988;&#20934;&#30830;&#30340;&#32763;&#35793;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural machine translation systems estimate probabilities of target sentences given source sentences, yet these estimates may not align with human preferences. This work introduces QE-fusion, a method utilizing a quality estimation metric (QE) that better correlates with human judgments to synthesize improved translations. QE-fusion leverages a candidate pool sampled from a model, combining spans from different candidates using QE metrics such as CometKiwi. We compare QE-fusion against beam search and recent reranking techniques, such as Minimum Bayes Risk decoding or QE-reranking. Our method consistently improves translation quality in terms of COMET and BLEURT scores when applied to large language models (LLMs) used for translation (PolyLM, XGLM, Llama2, and Mistral) and to multilingual translation models (NLLB), over five language pairs. Notably, QE-fusion exhibits larger improvements for LLMs due to their ability to generate diverse outputs. We demonstrate that our approach generat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06687</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#36817;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Proximal Causal Inference With Text Data. (arXiv:2401.06687v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#36817;&#22240;&#26524;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#27169;&#22411;&#25512;&#26029;&#20986;&#20195;&#29702;&#21464;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#22240;&#26524;&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#23558;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20316;&#20026;&#20542;&#21521;&#20110;&#21253;&#21547;&#37096;&#20998;&#25110;&#19981;&#23436;&#20840;&#27979;&#37327;&#30340;&#28151;&#28102;&#21464;&#37327;&#30340;&#20195;&#29702;&#26469;&#20943;&#36731;&#28151;&#28102;&#20559;&#24046;&#12290;&#36825;&#20123;&#26041;&#27861;&#20551;&#35774;&#20998;&#26512;&#20154;&#21592;&#22312;&#19968;&#37096;&#20998;&#23454;&#20363;&#30340;&#25991;&#26412;&#20013;&#20855;&#26377;&#26377;&#30417;&#30563;&#30340;&#28151;&#28102;&#21464;&#37327;&#26631;&#31614;&#65292;&#20294;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#25110;&#25104;&#26412;&#65292;&#36825;&#31181;&#32422;&#26463;&#24182;&#19981;&#24635;&#26159;&#21487;&#34892;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28151;&#28102;&#21464;&#37327;&#23436;&#20840;&#26410;&#35266;&#23519;&#21040;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#23558;&#22788;&#29702;&#21069;&#25991;&#26412;&#25968;&#25454;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#20004;&#20010;&#38646;&#26679;&#26412;&#27169;&#22411;&#20174;&#20998;&#21106;&#30340;&#20004;&#20010;&#37096;&#20998;&#25512;&#26029;&#20986;&#20004;&#20010;&#20195;&#29702;&#65292;&#24182;&#23558;&#36825;&#20123;&#20195;&#29702;&#24212;&#29992;&#20110;&#36817;&#37051; g-formula&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#30340;&#20195;&#29702;&#26041;&#27861;&#28385;&#36275;&#36817;&#37051; g-formula&#25152;&#38656;&#30340;&#35782;&#21035;&#26465;&#20214;&#65292;&#32780;&#20854;&#20182;&#30475;&#20284;&#21512;&#29702;&#30340;&#25552;&#35758;&#21017;&#19981;&#28385;&#36275;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#21322;&#21512;&#25104;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#23427;&#20135;&#29983;&#20102;&#20302;&#20559;&#24046;&#30340;&#20272;&#35745;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent text-based causal methods attempt to mitigate confounding bias by including unstructured text data as proxies of confounding variables that are partially or imperfectly measured. These approaches assume analysts have supervised labels of the confounders given text for a subset of instances, a constraint that is not always feasible due to data privacy or cost. Here, we address settings in which an important confounding variable is completely unobserved. We propose a new causal inference method that splits pre-treatment text data, infers two proxies from two zero-shot models on the separate splits, and applies these proxies in the proximal g-formula. We prove that our text-based proxy method satisfies identification conditions required by the proximal g-formula while other seemingly reasonable proposals do not. We evaluate our method in synthetic and semi-synthetic settings and find that it produces estimates with low bias. This combination of proximal causal inference and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.06683</link><description>&lt;p&gt;
DQNC2S&#65306;&#22522;&#20110;DQN&#30340;&#36328;&#27969;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DQN&#30340;&#22312;&#32447;&#21361;&#26426;&#20107;&#20214;&#25688;&#35201;&#29983;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#19988;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#24635;&#32467;&#22810;&#20010;&#19982;&#28798;&#23475;&#30456;&#20851;&#30340;&#25968;&#25454;&#27969;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#29616;&#26377;&#30340;&#26816;&#32034;&#19982;&#37325;&#26032;&#25490;&#24207;&#31574;&#30053;&#22312;&#22810;&#27969;&#25968;&#25454;&#30340;&#22266;&#26377;&#20887;&#20313;&#21644;&#22810;&#26597;&#35810;&#29615;&#22659;&#19979;&#30340;&#38480;&#21046;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24369;&#26631;&#27880;&#21644;&#28145;&#24230;Q&#32593;&#32476;&#30340;&#22312;&#32447;&#21361;&#26426;&#26102;&#38388;&#36724;&#29983;&#25104;&#26041;&#27861;&#12290;&#23427;&#33021;&#22815;&#23454;&#26102;&#36873;&#25321;&#30456;&#20851;&#30340;&#25991;&#26412;&#29255;&#27573;&#65292;&#26080;&#38656;&#20154;&#24037;&#26631;&#27880;&#25110;&#20869;&#23481;&#37325;&#26032;&#25490;&#24207;&#65292;&#20174;&#32780;&#20351;&#25512;&#29702;&#26102;&#38388;&#19982;&#36755;&#20837;&#26597;&#35810;&#30340;&#25968;&#37327;&#26080;&#20851;&#12290;&#35813;&#26041;&#27861;&#36824;&#23558;&#20887;&#20313;&#36807;&#28388;&#22120;&#34701;&#20837;&#22870;&#21169;&#20989;&#25968;&#20013;&#65292;&#20197;&#26377;&#25928;&#22788;&#29702;&#36328;&#27969;&#20869;&#23481;&#37325;&#21472;&#12290;&#22312;CrisisFACTS 2022&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25152;&#36798;&#21040;&#30340;ROUGE&#21644;BERTScore&#32467;&#26524;&#20248;&#20110;&#26368;&#20339;&#24615;&#33021;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing multiple disaster-relevant data streams simultaneously is particularly challenging as existing Retrieve&amp;Re-ranking strategies suffer from the inherent redundancy of multi-stream data and limited scalability in a multi-query setting. This work proposes an online approach to crisis timeline generation based on weak annotation with Deep Q-Networks. It selects on-the-fly the relevant pieces of text without requiring neither human annotations nor content re-ranking. This makes the inference time independent of the number of input queries. The proposed approach also incorporates a redundancy filter into the reward function to effectively handle cross-stream content overlaps. The achieved ROUGE and BERTScore results are superior to those of best-performing models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;</description></item><item><title>PolyTOPS&#26159;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#22810;&#38754;&#20307;&#35843;&#24230;&#22120;&#65292;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#26550;&#26500;&#12289;&#24182;&#34892;&#24615;&#27169;&#22411;&#25110;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#24490;&#29615;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.06665</link><description>&lt;p&gt;
PolyTOPS&#65306;&#21487;&#37325;&#26500;&#21644;&#28789;&#27963;&#30340;&#22810;&#38754;&#20307;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
PolyTOPS: Reconfigurable and Flexible Polyhedral Scheduler. (arXiv:2401.06665v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06665
&lt;/p&gt;
&lt;p&gt;
PolyTOPS&#26159;&#19968;&#20010;&#21487;&#37197;&#32622;&#30340;&#22810;&#38754;&#20307;&#35843;&#24230;&#22120;&#65292;&#21487;&#20197;&#38024;&#23545;&#19981;&#21516;&#26550;&#26500;&#12289;&#24182;&#34892;&#24615;&#27169;&#22411;&#25110;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#24490;&#29615;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#20307;&#25216;&#26415;&#22312;&#20302;&#32423;&#32534;&#35793;&#22120;&#21644;&#39640;&#32423;&#36807;&#31243;&#20013;&#24191;&#27867;&#29992;&#20110;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#12290;&#24490;&#29615;&#20248;&#21270;&#26159;&#36825;&#31181;&#25216;&#26415;&#30340;&#26680;&#24515;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#38754;&#20307;&#35843;&#24230;&#22120;&#65288;&#22914;Feautrier&#12289;Pluto&#12289;isl&#21644;Tensor Scheduler&#65289;&#65292;&#27599;&#20010;&#35843;&#24230;&#22120;&#38024;&#23545;&#30340;&#26159;&#19981;&#21516;&#30340;&#26550;&#26500;&#12289;&#24182;&#34892;&#24615;&#27169;&#22411;&#25110;&#24212;&#29992;&#22330;&#26223;&#12290;&#30001;&#20110;&#26550;&#26500;&#30340;&#24322;&#26500;&#24615;&#65292;&#23545;&#20110;&#29305;&#23450;&#22330;&#26223;&#30340;&#20248;&#21270;&#38656;&#27714;&#27491;&#22312;&#22686;&#38271;&#12290;&#20854;&#20013;&#26368;&#20851;&#38190;&#30340;&#24773;&#20917;&#20043;&#19968;&#26159;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#65288;NPU&#65289;&#65292;&#21487;&#33021;&#38656;&#35201;&#20855;&#26377;&#19981;&#21516;&#30446;&#26631;&#30340;&#24490;&#29615;&#20248;&#21270;&#12290;&#36824;&#38656;&#35201;&#32771;&#34385;&#36827;&#34892;&#22810;&#38754;&#20307;&#20248;&#21270;&#30340;&#26694;&#26550;&#25110;&#32534;&#35793;&#22120;&#12290;&#26681;&#25454;&#30446;&#26631;&#26550;&#26500;&#12289;&#32534;&#35793;&#29615;&#22659;&#21644;&#24212;&#29992;&#39046;&#22495;&#30340;&#19981;&#21516;&#24773;&#20917;&#65292;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#30340;&#20248;&#21270;&#26469;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#26550;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Polyhedral techniques have been widely used for automatic code optimization in low-level compilers and higher-level processes. Loop optimization is central to this technique, and several polyhedral schedulers like Feautrier, Pluto, isl and Tensor Scheduler have been proposed, each of them targeting a different architecture, parallelism model, or application scenario. The need for scenario-specific optimization is growing due to the heterogeneity of architectures. One of the most critical cases is represented by NPUs (Neural Processing Units) used for AI, which may require loop optimization with different objectives. Another factor to be considered is the framework or compiler in which polyhedral optimization takes place. Different scenarios, depending on the target architecture, compilation environment, and application domain, may require different kinds of optimization to best exploit the architecture feature set.  We introduce a new configurable polyhedral scheduler, PolyTOPS, that c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26234;&#24935;M&#30340;&#25554;&#20214;&#26694;&#26550;&#65292;&#21033;&#29992;&#20174;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20135;&#29983;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#26469;&#25913;&#36827;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.06659</link><description>&lt;p&gt;
&#25913;&#21892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#26234;&#24935;M&#65306;&#34701;&#21512;&#32972;&#26223;&#30693;&#35782;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
WisdoM: Improving Multimodal Sentiment Analysis by Fusing Contextual World Knowledge. (arXiv:2401.06659v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26234;&#24935;M&#30340;&#25554;&#20214;&#26694;&#26550;&#65292;&#21033;&#29992;&#20174;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#20135;&#29983;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#26469;&#25913;&#36827;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#24863;&#20998;&#26512;&#36890;&#36807;&#21033;&#29992;&#21508;&#31181;&#25968;&#25454;&#27169;&#24577;&#65288;&#20363;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#65289;&#36805;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#37117;&#20381;&#36182;&#20110;&#34920;&#38754;&#20449;&#24687;&#65292;&#24573;&#35270;&#20102;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#65288;&#20363;&#22914;&#20174;&#32473;&#23450;&#22270;&#20687;&#21644;&#25991;&#26412;&#23545;&#20043;&#22806;&#33719;&#21462;&#30340;&#32972;&#26223;&#20449;&#24687;&#65289;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#26234;&#24935;M&#30340;&#25554;&#20214;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#20174;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#20135;&#29983;&#30340;&#19978;&#19979;&#25991;&#19990;&#30028;&#30693;&#35782;&#26469;&#25913;&#36827;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#26234;&#24935;M&#21033;&#29992;LVLM&#26469;&#20840;&#38754;&#20998;&#26512;&#22270;&#20687;&#21644;&#30456;&#24212;&#30340;&#21477;&#23376;&#65292;&#21516;&#26102;&#29983;&#25104;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#12290;&#20026;&#20102;&#20943;&#23569;&#19978;&#19979;&#25991;&#20013;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#34701;&#21512;&#26426;&#21046;&#12290;&#22312;&#22810;&#26679;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#19968;&#33268;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#30528;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sentiment analysis is rapidly advancing by utilizing various data modalities (e.g., text, image). However, most previous works relied on superficial information, neglecting the incorporation of contextual world knowledge (e.g., background information derived from but beyond the given image and text pairs) and thereby restricting their ability to achieve better multimodal sentiment analysis. In this paper, we proposed a plug-in framework named WisdoM, designed to leverage contextual world knowledge induced from the large vision-language models (LVLMs) for enhanced multimodal sentiment analysis. WisdoM utilizes a LVLM to comprehensively analyze both images and corresponding sentences, simultaneously generating pertinent context. To reduce the noise in the context, we also introduce a training-free Contextual Fusion mechanism. Experimental results across diverse granularities of multimodal sentiment analysis tasks consistently demonstrate that our approach has substantial improvements (br
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;</title><link>http://arxiv.org/abs/2401.06643</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#28608;&#21169;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#26679;&#26412;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effects of diversity incentives on sample diversity and downstream model performance in LLM-based text augmentation. (arXiv:2401.06643v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#23545;LLM&#25991;&#26412;&#22686;&#24378;&#20013;&#29983;&#25104;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#26032;&#30340;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#25968;&#25454;&#22686;&#24378;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24212;&#29992;&#65292;&#20854;&#20013;&#23569;&#37327;&#25991;&#26412;&#26679;&#26412;&#34987;LLM&#25913;&#20889;&#65292;&#28982;&#21518;&#29992;&#20110;&#27169;&#22411;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#19981;&#21516;&#25552;&#31034;&#12289;&#31181;&#23376;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#12289;&#36807;&#28388;&#26041;&#27861;&#25110;&#27169;&#22411;&#35774;&#32622;&#23545;&#25913;&#20889;&#25968;&#25454;&#65288;&#21644;&#19979;&#28216;&#27169;&#22411;&#65289;&#36136;&#37327;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22312;&#20247;&#21253;&#20013;&#24050;&#32463;&#24314;&#31435;&#33391;&#22909;&#30340;&#19977;&#31181;&#25991;&#26412;&#22810;&#26679;&#24615;&#28608;&#21169;&#26041;&#27861;&#65306;&#31105;&#24524;&#35789;&#12289;&#36890;&#36807;&#20808;&#21069;&#24322;&#24120;&#35299;&#30340;&#25552;&#31034;&#21644;&#36890;&#36807;&#20808;&#21069;&#24322;&#24120;&#35299;&#30340;&#38142;&#25509;&#12290;&#20351;&#29992;&#36825;&#20123;&#28608;&#21169;&#26041;&#27861;&#20316;&#20026;&#25351;&#23548;LLM&#22686;&#34917;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25506;&#27979;&#23427;&#20204;&#23545;&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#19979;&#28216;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;5&#31181;&#19981;&#21516;&#30340;LLM&#21644;6&#20010;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31105;&#24524;&#35789;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20351;&#29992;&#20808;&#21069;&#21019;&#24314;&#30340;&#25913;&#20889;&#20316;&#20026;&#25552;&#31034;&#26102;&#65292;&#19979;&#28216;&#27169;&#22411;&#30340;&#24615;&#33021;&#26368;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
The latest generative large language models (LLMs) have found their application in data augmentation tasks, where small numbers of text samples are LLM-paraphrased and then used to fine-tune the model. However, more research is needed to assess how different prompts, seed data selection strategies, filtering methods, or model settings affect the quality of paraphrased data (and downstream models). In this study, we investigate three text diversity incentive methods well established in crowdsourcing: taboo words, hints by previous outlier solutions, and chaining on previous outlier solutions. Using these incentive methods as part of instructions to LLMs augmenting text datasets, we measure their effects on generated texts' lexical diversity and downstream model performance. We compare the effects over 5 different LLMs and 6 datasets. We show that diversity is most increased by taboo words, while downstream model performance is highest when previously created paraphrases are used as hint
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#29615;&#22659;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#38750;&#24179;&#20961;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.06640</link><description>&lt;p&gt;
&#23454;&#39564;&#29615;&#22659;&#33021;&#22815;&#20419;&#36827;&#35821;&#35328;&#27169;&#22411;&#22312;&#31283;&#20581;&#30340;&#35821;&#20041;&#23646;&#24615;&#25512;&#26029;&#20013;&#30340;&#34920;&#29616;&#65292;&#20294;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25511;&#21046;&#23454;&#39564;&#29615;&#22659;&#30340;&#26041;&#24335;&#65292;&#21457;&#29616;&#35821;&#35328;&#27169;&#22411;&#22312;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#19968;&#23450;&#30340;&#38750;&#24179;&#20961;&#33021;&#21147;&#65292;&#20294;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#26080;&#20154;&#30417;&#30563;&#35780;&#20272;&#20984;&#26174;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25191;&#34892;&#24847;&#20041;&#25552;&#21462;&#26041;&#38754;&#30340;&#37325;&#35201;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22312;&#24341;&#20837;&#23454;&#39564;&#29615;&#22659;&#65288;&#22914;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#25351;&#23548;&#65289;&#30340;&#24773;&#20917;&#19979;&#65292;LMs&#30340;&#34920;&#29616;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#12290;&#37027;&#20040;&#36825;&#26159;&#21542;&#36866;&#29992;&#20110;&#20808;&#21069;&#30740;&#31350;&#30340;&#24847;&#20041;&#25935;&#24863;&#20219;&#21153;&#21602;&#65311;&#25105;&#20204;&#22312;&#25511;&#21046;&#19978;&#19979;&#25991;&#31034;&#20363;&#21644;&#25351;&#23548;&#20869;&#23481;&#30340;&#21069;&#25552;&#19979;&#65292;&#23545;&#23454;&#39564;&#29615;&#22659;&#23545;&#20110;&#25552;&#39640;LMs&#22312;&#25191;&#34892;&#23646;&#24615;&#32487;&#25215;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#65292;&#35813;&#20219;&#21153;&#26159;&#39044;&#20808;&#34920;&#26126;LMs&#26080;&#27861;&#23436;&#25104;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#23454;&#39564;&#29615;&#22659;&#30830;&#23454;&#21487;&#20197;&#23548;&#33268;LMs&#22312;&#23646;&#24615;&#32487;&#25215;&#34892;&#20026;&#26041;&#38754;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33021;&#21147;&#26159;&#19981;&#19968;&#33268;&#30340;&#65306;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#26368;&#23567;&#25913;&#20889;&#65292;&#21457;&#29616;&#19968;&#20123;LMs&#20174;&#36755;&#20837;&#20013;&#25429;&#25417;&#21040;&#27973;&#23618;&#30340;&#38750;&#35821;&#20041;&#24335;&#21551;&#21457;&#24335;&#20449;&#24687;&#65292;&#36825;&#34920;&#26126;&#35745;&#31639;&#26426;&#30340;&#34892;&#20026;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent zero-shot evaluations have highlighted important limitations in the abilities of language models (LMs) to perform meaning extraction. However, it is now well known that LMs can demonstrate radical improvements in the presence of experimental contexts such as in-context examples and instructions. How well does this translate to previously studied meaning-sensitive tasks? We present a case-study on the extent to which experimental contexts can improve LMs' robustness in performing property inheritance -- predicting semantic properties of novel concepts, a task that they have been previously shown to fail on. Upon carefully controlling the nature of the in-context examples and the instructions, our work reveals that they can indeed lead to non-trivial property inheritance behavior in LMs. However, this ability is inconsistent: with a minimal reformulation of the task, some LMs were found to pick up on shallow, non-semantic heuristics from their inputs, suggesting that the computati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#30340;&#26032;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;431&#20010;Python&#31243;&#24207;&#65292;&#37319;&#29992;pass@o&#24230;&#37327;&#25351;&#26631;&#26469;&#25552;&#20379;&#26356;&#20840;&#38754;&#21644;&#30456;&#20851;&#30340;OOP&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20195;&#30721;&#19987;&#29992;LLMs&#22312;OOP&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#36827;&#19968;&#27493;&#25913;&#36827;&#27492;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2401.06628</link><description>&lt;p&gt;
OOP&#65306;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
OOP: Object-Oriented Programming Evaluation Benchmark for Large Language Models. (arXiv:2401.06628v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#30340;&#26032;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#21253;&#25324;431&#20010;Python&#31243;&#24207;&#65292;&#37319;&#29992;pass@o&#24230;&#37327;&#25351;&#26631;&#26469;&#25552;&#20379;&#26356;&#20840;&#38754;&#21644;&#30456;&#20851;&#30340;OOP&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#12290;&#35780;&#20272;&#32467;&#26524;&#26174;&#31034;&#20195;&#30721;&#19987;&#29992;LLMs&#22312;OOP&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#65292;&#38656;&#36827;&#19968;&#27493;&#25913;&#36827;&#27492;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#36827;&#33258;&#21160;&#21270;&#32534;&#31243;&#38656;&#35201;&#20581;&#22766;&#19988;&#20840;&#38754;&#30340;&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#22522;&#20934;&#65292;&#28982;&#32780;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#22312;&#21151;&#33021;&#24335;&#32534;&#31243;&#26041;&#38754;&#65288;&#20363;&#22914;HumanEval&#21644;MBPP&#65289;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#35270;&#20102;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#65288;OOP&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#39033;&#21019;&#26032;&#30340;&#38754;&#21521;&#23545;&#35937;&#32534;&#31243;&#37325;&#28857;&#22522;&#20934;&#65292;&#21253;&#25324;431&#20010;Python&#31243;&#24207;&#65292;&#28085;&#30422;&#20102;&#31867;&#21644;&#23553;&#35013;&#26041;&#27861;&#31561;&#22522;&#26412;&#30340;OOP&#27010;&#24565;&#21644;&#29305;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#25351;&#26631;pass@o&#65292;&#38024;&#23545;OOP&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#22686;&#24378;&#20256;&#32479;&#30340;pass@k&#24230;&#37327;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;23&#20010;&#39046;&#20808;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21253;&#25324;&#36890;&#29992;&#27169;&#22411;&#21644;&#19987;&#38376;&#29992;&#20110;&#20195;&#30721;&#30340;&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#21457;&#29616;&#65306;1&#65289;pass@o&#25552;&#20379;&#20102;&#26356;&#30456;&#20851;&#21644;&#20840;&#38754;&#30340;OOP&#20195;&#30721;&#29983;&#25104;&#35780;&#20272;&#65307;2&#65289;&#23613;&#31649;&#22312;FP&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20687;WizardCoder&#36825;&#26679;&#30340;&#20195;&#30721;&#19987;&#29992;LLMs&#22312;OOP&#26041;&#38754;&#33853;&#21518;&#20110;&#20687;ChatGPT&#36825;&#26679;&#30340;&#27169;&#22411;&#65307;3&#65289;&#25152;&#26377;&#20808;&#36827;&#30340;LLMs&#22312;&#25105;&#20204;&#30340;OOP&#22522;&#20934;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#27492;&#39046;&#22495;&#30340;&#36843;&#20999;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancing automated programming necessitates robust and comprehensive code generation benchmarks, yet current evaluation frameworks largely neglect object-oriented programming (OOP) in favor of functional programming (FP), e.g., HumanEval and MBPP. To address this, our study introduces a pioneering OOP-focused benchmark, featuring 431 Python programs that encompass essential OOP concepts and features like classes and encapsulation methods. We propose a novel evaluation metric, pass@o, tailored for OOP, enhancing traditional pass@k measures. Our evaluation of 23 leading large language models (LLMs), including both general and code-specialized models, reveals three key insights: 1) pass@o offers a more relevant and comprehensive assessment for OOP code generation; 2) Despite excelling in FP, code-specialized LLMs like WizardCoder lag in OOP compared to models like ChatGPT; 3) The poor performance of all advanced LLMs on our OOP benchmark highlights a critical need for improvements in thi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransliCo&#65292;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#33050;&#26412;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#33050;&#26412;&#30340;&#21477;&#23376;&#21450;&#20854;&#22312;&#32479;&#19968;&#33050;&#26412;&#20013;&#30340;&#38899;&#35793;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#33050;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06620</link><description>&lt;p&gt;
TransliCo&#65306;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#33050;&#26412;&#38556;&#30861;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models. (arXiv:2401.06620v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06620
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;TransliCo&#65292;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#33050;&#26412;&#38556;&#30861;&#12290;&#36890;&#36807;&#23545;&#27604;&#19981;&#21516;&#33050;&#26412;&#30340;&#21477;&#23376;&#21450;&#20854;&#22312;&#32479;&#19968;&#33050;&#26412;&#20013;&#30340;&#38899;&#35793;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;&#33050;&#26412;&#30340;&#32479;&#19968;&#34920;&#31034;&#31354;&#38388;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#36328;&#35821;&#35328;&#20256;&#36882;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20070;&#38754;&#24418;&#24335;&#20013;&#26377;293&#31181;&#33050;&#26412;&#20195;&#34920;&#30528;7000&#22810;&#31181;&#35821;&#35328;&#12290;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#35768;&#22810;&#23494;&#20999;&#30456;&#20851;&#30340;&#35821;&#35328;&#20351;&#29992;&#19981;&#21516;&#30340;&#33050;&#26412;&#65292;&#36825;&#32473;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;mPLMs&#65289;&#36890;&#36807;&#35789;&#27719;&#37325;&#21472;&#23398;&#20064;&#36328;&#35821;&#35328;&#30693;&#35782;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;mPLMs&#23384;&#22312;&#33050;&#26412;&#38556;&#30861;&#65306;&#26469;&#33258;&#19981;&#21516;&#33050;&#26412;&#30340;&#34920;&#31034;&#20301;&#20110;&#19981;&#21516;&#23376;&#31354;&#38388;&#20013;&#65292;&#36825;&#26159;&#20026;&#20160;&#20040;&#28041;&#21450;&#19981;&#21516;&#33050;&#26412;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#20256;&#36882;&#26174;&#31034;&#27425;&#20248;&#24615;&#33021;&#30340;&#24378;&#26377;&#21147;&#25351;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26694;&#26550;TransliCo&#65292;&#23427;&#21253;&#21547;Transliteration Contrastive Modeling&#65288;TCM&#65289;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#21477;&#23376;&#21450;&#20854;&#22312;&#32479;&#19968;&#33050;&#26412;&#65288;&#22312;&#25105;&#20204;&#30340;&#26696;&#20363;&#20013;&#26159;&#25289;&#19969;&#23383;&#27597;&#65289;&#20013;&#30340;&#38899;&#35793;&#36827;&#34892;&#23545;&#27604;&#65292;&#26469;&#24494;&#35843;mPLM&#65292;&#20174;&#32780;&#30830;&#20445;&#19981;&#21516;&#33050;&#26412;&#30340;&#34920;&#31034;&#31354;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#20351;&#29992;Glot500-m&#20316;&#20026;&#25105;&#20204;&#30340;&#28304;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;500&#22810;&#31181;&#35821;&#35328;&#19978;&#39044;&#35757;&#32451;&#30340;mPLM&#65292;&#25105;&#20204;&#23558;&#20854;&#22312;&#20854;5&#65285;&#30340;&#23567;&#37096;&#20998;&#19978;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
There are 293 scripts representing over 7,000 languages in the written form. Due to various reasons, many closely related languages use different scripts, which poses difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a result, mPLMs present a script barrier: representations from different scripts are located in different subspaces, which is a strong indicator of why crosslingual transfer involving languages of different scripts shows sub-optimal performance. To address this problem, we propose a simple framework TransliCo that contains Transliteration Contrastive Modeling (TCM) to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (Latn, in our case), which ensures uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we find-tune it on a small portion (5\%) of its 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#21521;&#21453;&#39304;&#26426;&#21046;&#65292;&#36825;&#20010;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#20316;&#12290;LLM&#20805;&#24403;&#25945;&#24072;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20805;&#24403;&#23398;&#29983;&#65292;&#23427;&#20204;&#36890;&#36807;&#36882;&#24402;&#20114;&#21161;&#23454;&#29616;&#20102;&#30456;&#20114;&#21327;&#21161;&#12290;&#36825;&#31181;&#21512;&#20316;&#25552;&#20379;&#20102;&#39640;&#32423;&#20449;&#24687;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20419;&#36827;&#20102;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2401.06603</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#21453;&#39304;&#26426;&#21046;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#30456;&#20114;&#21512;&#20316;&#65306;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Mutual Enhancement of Large Language and Reinforcement Learning Models through Bi-Directional Feedback Mechanisms: A Case Study. (arXiv:2401.06603v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06603
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#21521;&#21453;&#39304;&#26426;&#21046;&#65292;&#36825;&#20010;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#20316;&#12290;LLM&#20805;&#24403;&#25945;&#24072;&#65292;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20805;&#24403;&#23398;&#29983;&#65292;&#23427;&#20204;&#36890;&#36807;&#36882;&#24402;&#20114;&#21161;&#23454;&#29616;&#20102;&#30456;&#20114;&#21327;&#21161;&#12290;&#36825;&#31181;&#21512;&#20316;&#25552;&#20379;&#20102;&#39640;&#32423;&#20449;&#24687;&#21644;&#23454;&#26102;&#21453;&#39304;&#65292;&#20419;&#36827;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#23637;&#31034;&#20986;&#23545;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;(&#22914;&#35268;&#21010;&#21644;&#25512;&#29702;&#33021;&#21147;)&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#28982;&#32780;LLMs&#21644;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#20043;&#38388;&#30340;&#21512;&#20316;&#38382;&#39064;&#20173;&#28982;&#38656;&#35201;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#24072;&#29983;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20855;&#20307;&#26159;&#36890;&#36807;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;LLMs&#21453;&#39304;&#65292;&#24182;&#22312;&#21512;&#20316;&#30340;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#20351;&#29992;LLMs&#20026;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#39640;&#32423;&#20449;&#24687;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;LLM&#25198;&#28436;&#25945;&#24072;&#35282;&#33394;&#65292;&#32780;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21017;&#25198;&#28436;&#23398;&#29983;&#35282;&#33394;&#12290;&#36825;&#20004;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#36882;&#24402;&#20114;&#21161;&#30340;&#26041;&#24335;&#30456;&#20114;&#21327;&#21161;&#65292;&#22914;&#8220;&#25105;&#24110;&#20320;&#24110;&#25105;&#24110;&#8221;&#31561;&#12290;LLM&#26234;&#33021;&#20307;&#21521;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#25552;&#20379;&#25277;&#35937;&#20449;&#24687;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#25506;&#32034;&#21644;&#31574;&#30053;&#25913;&#36827;&#12290;&#21453;&#36807;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#21521;LLM&#26234;&#33021;&#20307;&#25552;&#20379;&#21453;&#39304;&#65292;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#23454;&#26102;&#20449;&#24687;&#65292;&#24110;&#21161;&#29983;&#25104;&#26356;&#26377;&#29992;&#30340;&#26631;&#35760;&#12290;&#36825;&#31181;&#21452;&#21521;&#21453;&#39304;&#24490;&#29615;&#20419;&#36827;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable capabilities for reinforcement learning (RL) models, such as planning and reasoning capabilities. However, the problems of LLMs and RL model collaboration still need to be solved. In this study, we employ a teacher-student learning framework to tackle these problems, specifically by offering feedback for LLMs using RL models and providing high-level information for RL models with LLMs in a cooperative multi-agent setting. Within this framework, the LLM acts as a teacher, while the RL model acts as a student. The two agents cooperatively assist each other through a process of recursive help, such as "I help you help I help." The LLM agent supplies abstract information to the RL agent, enabling efficient exploration and policy improvement. In turn, the RL agent offers feedback to the LLM agent, providing valuable, real-time information that helps generate more useful tokens. This bi-directional feedback loop promotes optimization,
&lt;/p&gt;</description></item><item><title>Prometheus-Vision&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;Perception Collection&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#23450;&#20041;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#19982;&#20154;&#31867;&#35780;&#20272;&#21592;&#21644;GPT-4V&#20043;&#38388;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06591</link><description>&lt;p&gt;
Prometheus-Vision: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#32454;&#31890;&#24230;&#35780;&#20272;&#30340;&#35009;&#21028;
&lt;/p&gt;
&lt;p&gt;
Prometheus-Vision: Vision-Language Model as a Judge for Fine-Grained Evaluation. (arXiv:2401.06591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06591
&lt;/p&gt;
&lt;p&gt;
Prometheus-Vision&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22120;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;Perception Collection&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#35757;&#32451;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#29992;&#25143;&#23450;&#20041;&#30340;&#35780;&#20998;&#26631;&#20934;&#65292;&#19982;&#20154;&#31867;&#35780;&#20272;&#21592;&#21644;GPT-4V&#20043;&#38388;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#30001;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#29983;&#25104;&#30340;&#38271;&#31687;&#22238;&#31572;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#36825;&#19981;&#20165;&#38656;&#35201;&#26816;&#26597;VLM&#26159;&#21542;&#36981;&#24490;&#32473;&#23450;&#30340;&#25351;&#20196;&#65292;&#36824;&#38656;&#35201;&#39564;&#35777;&#25991;&#26412;&#36755;&#20986;&#26159;&#21542;&#27491;&#30830;&#22320;&#19982;&#32473;&#23450;&#30340;&#22270;&#20687;&#30456;&#32852;&#31995;&#12290;&#21463;&#21040;&#20351;&#29992;LMs&#35780;&#20272;LMs&#30340;&#26032;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;VLMs&#26469;&#35780;&#20272;VLMs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Perception Collection&#30340;&#26032;&#30340;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;15K&#20010;&#29992;&#25143;&#21487;&#33021;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#20851;&#27880;&#30340;&#23450;&#21046;&#35780;&#20998;&#26631;&#20934;&#12290;&#20351;&#29992;Perception Collection&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;Prometheus-Vision&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#29702;&#35299;&#29992;&#25143;&#23450;&#20041;&#30340;&#35780;&#20998;&#26631;&#20934;&#30340;&#24320;&#28304;VLM&#35780;&#20272;&#27169;&#22411;&#12290;Prometheus-Vision&#19982;&#20154;&#31867;&#35780;&#20272;&#21592;&#21644;GPT-4V&#20043;&#38388;&#26174;&#31034;&#20986;&#26368;&#39640;&#30340;Pearson&#30456;&#20851;&#24615;&#65292;&#26174;&#31034;&#20102;&#20854;&#29992;&#20110;&#36879;&#26126;&#21644;&#21487;&#35775;&#38382;&#30340;VLM&#35780;&#20272;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#24320;&#28304;&#22312;https://github.com/kaistAI/prometheus-vision&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing long-form responses generated by Vision-Language Models (VLMs) is challenging. It not only requires checking whether the VLM follows the given instruction but also verifying whether the text output is properly grounded on the given image. Inspired by the recent approach of evaluating LMs with LMs, in this work, we propose to evaluate VLMs with VLMs. For this purpose, we present a new feedback dataset called the Perception Collection, encompassing 15K customized score rubrics that users might care about during assessment. Using the Perception Collection, we train Prometheus-Vision, the first open-source VLM evaluator model that can understand the user-defined score criteria during evaluation. Prometheus-Vision shows the highest Pearson correlation with human evaluators and GPT-4V among open-source models, showing its effectiveness for transparent and accessible evaluation of VLMs. We open-source our code, dataset, and model at https://github.com/kaistAI/prometheus-vision
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#36328;&#35821;&#35328;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2401.06583</link><description>&lt;p&gt;
&#23558;&#21464;&#24418;&#22120;&#25216;&#26415;&#24212;&#29992;&#20110;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21464;&#24418;&#22120;&#27169;&#22411;&#21644;&#26144;&#23556;&#26041;&#27861;&#65292;&#25506;&#32034;&#20102;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23454;&#29616;&#36328;&#35821;&#35328;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#25991;&#26723;&#24050;&#32463;&#25104;&#20026;&#22312;&#32593;&#32476;&#19978;&#25214;&#21040;&#30456;&#20851;&#20869;&#23481;&#30340;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#33616;&#38750;&#26597;&#35810;&#35821;&#35328;&#30340;&#25991;&#26723;&#26102;&#65292;&#36825;&#20123;&#31995;&#32479;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#21487;&#33021;&#20250;&#24573;&#35270;&#38750;&#27597;&#35821;&#30340;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#26144;&#23556;&#21040;&#36328;&#35821;&#35328;&#39046;&#22495;&#30340;&#21464;&#24418;&#22120;&#25216;&#26415;&#25991;&#26723;&#34920;&#31034;&#65288;TLDRs&#65289;&#26469;&#34920;&#31034;&#36328;&#35821;&#35328;&#25991;&#26723;&#12290;&#35780;&#20272;&#20102;&#22235;&#20010;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#65288;mBERT&#65292;mT5 XLM RoBERTa&#65292;ErnieM&#65289;&#22312;20&#31181;&#35821;&#35328;&#23545;&#19978;&#20351;&#29992;&#19977;&#31181;&#26144;&#23556;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#36825;&#20123;&#35821;&#35328;&#23545;&#20195;&#34920;&#20102;&#27431;&#30431;&#36873;&#25321;&#30340;&#20116;&#31181;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;&#20351;&#29992;Mate&#26816;&#32034;&#29575;&#21644;&#20114;&#24800;&#25490;&#24207;&#31561;&#25351;&#26631;&#26469;&#34913;&#37327;&#26144;&#23556;TLDRs&#19982;&#26410;&#26144;&#23556;TLDRs&#30340;&#25928;&#26524;&#12290;&#32467;&#26524;&#24378;&#35843;&#20102;&#36890;&#36807;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#21644;&#26144;&#23556;&#26041;&#27861;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;&#20026;&#25193;&#23637;&#36328;&#35821;&#35328;&#25991;&#26723;&#34920;&#31034;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommendation systems, for documents, have become tools to find relevant content on the Web. However, these systems have limitations when it comes to recommending documents in languages different from the query language, which means they might overlook resources in non-native languages. This research focuses on representing documents across languages by using Transformer Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language pairs representing combinations of five selected languages of the European Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to measure the effectiveness of mapped TLDRs compared to non-mapped ones. The results highlight the power of cross-lingual representations achieved through pre-trained transformers and mapping approaches suggesting a promising direction for expanding
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06568</link><description>&lt;p&gt;
&#22312;&#28304;&#35821;&#35328;&#20013;&#36855;&#22833;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#23545;&#23427;&#20204;&#22914;&#20309;&#21033;&#29992;&#25552;&#20379;&#30340;&#25968;&#25454;&#36827;&#34892;&#35780;&#20272;&#20173;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;LLMs&#22914;&#20309;&#21033;&#29992;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#35780;&#20272;&#32763;&#35793;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#28085;&#30422;&#21508;&#31181;&#36755;&#20837;&#27169;&#24335;&#21644;&#27169;&#22411;&#31867;&#22411;&#30340;&#21463;&#25511;&#23454;&#39564;&#65292;&#24182;&#37319;&#29992;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#25552;&#31034;&#26469;&#21306;&#20998;&#28304;&#35821;&#35328;&#21644;&#21442;&#32771;&#20449;&#24687;&#30340;&#23454;&#29992;&#24615;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21442;&#32771;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#35780;&#20272;&#20934;&#30830;&#24615;&#65292;&#32780;&#28304;&#35821;&#35328;&#20449;&#24687;&#26377;&#26102;&#20250;&#36866;&#24471;&#20854;&#21453;&#65292;&#34920;&#26126;&#22312;&#20351;&#29992;LLMs&#35780;&#20272;&#32763;&#35793;&#26102;&#23384;&#22312;&#36328;&#35821;&#35328;&#33021;&#21147;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#23545;LLMs&#36827;&#34892;&#20102;&#32763;&#35793;&#38169;&#35823;&#26816;&#27979;&#30340;&#20803;&#35780;&#20272;&#65292;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#36824;&#26263;&#31034;&#20102;&#19968;&#31181;&#28508;&#22312;&#30340;&#26041;&#27861;&#65292;&#21363;&#21033;&#29992;&#21442;&#32771;&#20449;&#24687;&#26469;&#25913;&#21892;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable results in the machine translation evaluation task, yet there remains a gap in knowledge regarding how they utilize the provided data to conduct evaluations. This study aims to explore how LLMs leverage source and reference information in evaluating translations, with the ultimate goal of better understanding the working mechanism of LLMs. To this end, we design the controlled experiments across various input modes and model types, and employ both coarse-grained and fine-grained prompts to discern the utility of source versus reference information. Surprisingly, we find that reference information significantly enhances the evaluation accuracy, while source information sometimes is counterproductive, indicating a lack of cross-lingual capability when using LLMs to evaluate translations. We further conduct a meta-evaluation for translation error detection of LLMs, observing a similar phenomenon. These findings also suggest a potential
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06561</link><description>&lt;p&gt;
Intention Analysis Prompting&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#33391;&#22909;&#30340;&#36234;&#29425;&#38450;&#24481;&#32773;
&lt;/p&gt;
&lt;p&gt;
Intention Analysis Prompting Makes Large Language Models A Good Jailbreak Defender. (arXiv:2401.06561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Intention Analysis Prompting (IAPrompt)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35302;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#24182;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38754;&#23545;&#38544;&#34109;&#21644;&#22797;&#26434;&#30340;&#36234;&#29425;&#25915;&#20987;&#26102;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#26159;&#19968;&#39033;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#31574;&#30053;&#65292;&#21363;Intention Analysis Prompting&#65288;IAPrompt&#65289;&#12290;&#20854;&#21407;&#29702;&#26159;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#35302;&#21457;LLMs&#30340;&#20869;&#22312;&#33258;&#25105;&#32416;&#27491;&#21644;&#25913;&#36827;&#33021;&#21147;&#65306;1&#65289;&#22522;&#26412;&#24847;&#22270;&#20998;&#26512;&#65292;2&#65289;&#19982;&#25919;&#31574;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;IAPrompt&#26159;&#19968;&#31181;&#20165;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#21487;&#20197;&#25552;&#39640;LLMs&#30340;&#23433;&#20840;&#24615;&#32780;&#19981;&#25439;&#23475;&#20854;&#26377;&#29992;&#24615;&#12290;&#22312;Vicuna&#12289;ChatGLM&#12289;MPT&#12289;DeepSeek&#21644;GPT-3.5&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;IAPrompt&#33021;&#22815;&#25345;&#32493;&#19988;&#26174;&#33879;&#22320;&#20943;&#23569;&#21709;&#24212;&#20013;&#30340;&#26377;&#23475;&#34892;&#20026;&#65288;&#24179;&#22343;&#25915;&#20987;&#25104;&#21151;&#29575;&#19979;&#38477;46.5%&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#26377;&#29992;&#24615;&#12290;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20123;&#35265;&#35299;&#12290;&#20026;&#20102;&#20445;&#35777;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#22312;https://github.com/alph&#19978;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#33050;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligning large language models (LLMs) with human values, particularly in the face of stealthy and complex jailbreaks, presents a formidable challenge. In this study, we present a simple yet highly effective defense strategy, i.e., Intention Analysis Prompting (IAPrompt). The principle behind is to trigger LLMs' inherent self-correct and improve ability through a two-stage process: 1) essential intention analysis, and 2) policy-aligned response. Notably, IAPrompt is an inference-only method, thus could enhance the safety of LLMs without compromising their helpfulness. Extensive experiments on SAP200 and DAN benchmarks across Vicuna, ChatGLM, MPT, DeepSeek, and GPT-3.5 show that IAPrompt could consistently and significantly reduce the harmfulness in response (averagely -46.5% attack success rate) and maintain the general helpfulness. Further analyses present some insights into how our method works. To facilitate reproducibility, We release our code and scripts at: https://github.com/alph
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#65288;IADDx&#65289;&#23454;&#29616;&#20102;&#23545;&#21307;&#30103;&#23545;&#35805;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30452;&#35273;&#21644;&#20998;&#26512;&#25512;&#29702;&#26469;&#24314;&#31435;&#37492;&#21035;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#22270;&#22686;&#24378;&#30340;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#32454;&#21270;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2401.06541</link><description>&lt;p&gt;
&#36890;&#36807;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#29983;&#25104;&#21307;&#30103;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis. (arXiv:2401.06541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#65288;IADDx&#65289;&#23454;&#29616;&#20102;&#23545;&#21307;&#30103;&#23545;&#35805;&#30340;&#29983;&#25104;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#30452;&#35273;&#21644;&#20998;&#26512;&#25512;&#29702;&#26469;&#24314;&#31435;&#37492;&#21035;&#35786;&#26029;&#65292;&#24182;&#36890;&#36807;&#22270;&#22686;&#24378;&#30340;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#32454;&#21270;&#65292;&#25552;&#39640;&#20102;&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#23545;&#35805;&#31995;&#32479;&#22240;&#20854;&#33021;&#22815;&#25552;&#20379;&#24555;&#36895;&#35786;&#26029;&#12289;&#27835;&#30103;&#26041;&#26696;&#21644;&#20581;&#24247;&#21672;&#35810;&#30340;&#28508;&#21147;&#32780;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#22312;&#21307;&#30103;&#23545;&#35805;&#20013;&#65292;&#27491;&#30830;&#30340;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20026;&#26410;&#26469;&#30340;&#21672;&#35810;&#24314;&#31435;&#20102;&#22522;&#30784;&#12290;&#20020;&#24202;&#21307;&#29983;&#36890;&#24120;&#20351;&#29992;&#30452;&#35273;&#21644;&#20998;&#26512;&#25512;&#29702;&#26469;&#24418;&#25104;&#37492;&#21035;&#35786;&#26029;&#12290;&#36825;&#20010;&#25512;&#29702;&#36807;&#31243;&#20551;&#35774;&#21644;&#39564;&#35777;&#20102;&#21508;&#31181;&#21487;&#33021;&#30340;&#30142;&#30149;&#65292;&#24182;&#21162;&#21147;&#29983;&#25104;&#20840;&#38754;&#32780;&#20005;&#35880;&#30340;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20851;&#20110;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#30340;&#30740;&#31350;&#24573;&#35270;&#20102;&#24314;&#27169;&#37492;&#21035;&#35786;&#26029;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#30452;&#35273;-&#20998;&#26512;&#24335;&#37492;&#21035;&#35786;&#26029;&#65288;IADDx&#65289;&#30340;&#21307;&#30103;&#23545;&#35805;&#29983;&#25104;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22522;&#20110;&#26816;&#32034;&#30340;&#30452;&#35273;&#32852;&#24819;&#36827;&#34892;&#37492;&#21035;&#35786;&#26029;&#65292;&#28982;&#21518;&#36890;&#36807;&#22270;&#22686;&#24378;&#30340;&#20998;&#26512;&#26041;&#27861;&#23545;&#20854;&#36827;&#34892;&#32454;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical dialogue systems have attracted growing research attention as they have the potential to provide rapid diagnoses, treatment plans, and health consultations. In medical dialogues, a proper diagnosis is crucial as it establishes the foundation for future consultations. Clinicians typically employ both intuitive and analytic reasoning to formulate a differential diagnosis. This reasoning process hypothesizes and verifies a variety of possible diseases and strives to generate a comprehensive and rigorous diagnosis. However, recent studies on medical dialogue generation have overlooked the significance of modeling a differential diagnosis, which hinders the practical application of these systems. To address the above issue, we propose a medical dialogue generation framework with the Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with a differential diagnosis via retrieval-based intuitive association and subsequently refines it through a graph-enhanced anal
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06532</link><description>&lt;p&gt;
INTERS: &#20351;&#29992;&#25351;&#20196;&#35843;&#20248;&#35299;&#38145;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25628;&#32034;&#20013;&#30340;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning. (arXiv:2401.06532v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;21&#20010;IR&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35768;&#22810;&#19982;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#20855;&#20307;&#27010;&#24565;&#30340;&#19981;&#32463;&#24120;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#65292;&#23427;&#20204;&#22312;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#21521;LLMs&#25552;&#20379;&#20219;&#21153;&#25551;&#36848;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#22312;&#20419;&#36827;&#20840;&#38754;&#29702;&#35299;&#21644;&#25191;&#34892;IR&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;LLMs&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#25351;&#20196;&#35843;&#20248;&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;LLMs&#22312;IR&#20219;&#21153;&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;INTERS&#65292;&#28085;&#30422;&#20102;3&#20010;&#22522;&#26412;IR&#31867;&#21035;&#20013;&#30340;21&#20010;&#20219;&#21153;&#65306;&#26597;&#35810;&#29702;&#35299;&#12289;&#25991;&#26723;&#29702;&#35299;&#21644;&#26597;&#35810;&#25991;&#26723;&#20851;&#31995;&#29702;&#35299;&#12290;&#25968;&#25454;&#26469;&#33258;43&#20010;&#19981;&#21516;&#30340;&#30001;&#25163;&#21160;&#32534;&#20889;&#30340;&#27169;&#26495;&#26500;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;INTERS&#26174;&#33879;&#25552;&#21319;&#20102;&#21508;&#31181;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating comprehensive understanding and execution of IR tasks, thereby limiting LLMs' applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs' proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 21 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly a
&lt;/p&gt;</description></item><item><title>MetaHate&#26159;&#19968;&#20010;&#29992;&#20110;&#32479;&#19968;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#29616;&#26377;&#25910;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20026;&#35757;&#32451;&#26356;&#24378;&#22823;&#21644;&#36866;&#24212;&#24615;&#26356;&#24378;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.06526</link><description>&lt;p&gt;
MetaHate&#65306;&#19968;&#20010;&#29992;&#20110;&#32479;&#19968;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MetaHate: A Dataset for Unifying Efforts on Hate Speech Detection. (arXiv:2401.06526v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06526
&lt;/p&gt;
&lt;p&gt;
MetaHate&#26159;&#19968;&#20010;&#29992;&#20110;&#32479;&#19968;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#65292;&#23545;&#29616;&#26377;&#25910;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20026;&#35757;&#32451;&#26356;&#24378;&#22823;&#21644;&#36866;&#24212;&#24615;&#26356;&#24378;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#20195;&#34920;&#30528;&#19968;&#31181;&#26222;&#36941;&#19988;&#26377;&#23475;&#30340;&#22312;&#32447;&#35328;&#35770;&#24418;&#24335;&#65292;&#36890;&#24120;&#34920;&#29616;&#20026;&#19968;&#31995;&#21015;&#19981;&#21451;&#22909;&#30340;&#35328;&#35821;&#65292;&#20174;&#20196;&#20154;&#35752;&#21388;&#30340;&#25512;&#25991;&#21040;&#35837;&#35876;&#24615;&#30340;&#24086;&#23376;&#12290;&#38543;&#30528;&#36825;&#31181;&#35328;&#35770;&#30340;&#34067;&#24310;&#65292;&#23427;&#36830;&#25509;&#20102;&#20840;&#29699;&#30340;&#20154;&#20204;&#65292;&#24182;&#23545;&#34987;&#38024;&#23545;&#30340;&#20010;&#20154;&#21644;&#31038;&#21306;&#26500;&#25104;&#20102;&#37325;&#22823;&#30340;&#31038;&#20250;&#12289;&#24515;&#29702;&#21644;&#20598;&#23572;&#30340;&#36523;&#20307;&#23041;&#32961;&#12290;&#30446;&#21069;&#65292;&#38024;&#23545;&#36825;&#19968;&#29616;&#35937;&#30340;&#35745;&#31639;&#35821;&#35328;&#23398;&#26041;&#27861;&#20381;&#36182;&#20110;&#24102;&#26377;&#26631;&#31614;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#32479;&#19968;&#21162;&#21147;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#20851;&#38190;&#38656;&#27714;&#19978;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20513;&#23548;&#20351;&#29992;&#19968;&#20010;&#20840;&#38754;&#30340;&#20803;&#25910;&#38598;&#25968;&#25454;&#38598;&#65292;&#26377;&#21161;&#20110;&#26377;&#25928;&#23545;&#25239;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;60&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#23558;&#19982;&#20043;&#30456;&#20851;&#30340;&#25968;&#25454;&#32435;&#20837;MetaHate&#20013;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#25910;&#38598;&#36827;&#34892;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#20026;&#35757;&#32451;&#26356;&#24378;&#22823;&#21644;&#36866;&#24212;&#24615;&#26356;&#24378;&#30340;&#27169;&#22411;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech represents a pervasive and detrimental form of online discourse, often manifested through an array of slurs, from hateful tweets to defamatory posts. As such speech proliferates, it connects people globally and poses significant social, psychological, and occasionally physical threats to targeted individuals and communities. Current computational linguistic approaches for tackling this phenomenon rely on labelled social media datasets for training. For unifying efforts, our study advances in the critical need for a comprehensive meta-collection, advocating for an extensive dataset to help counteract this problem effectively. We scrutinized over 60 datasets, selectively integrating those pertinent into MetaHate. This paper offers a detailed examination of existing collections, highlighting their strengths and limitations. Our findings contribute to a deeper understanding of the existing datasets, paving the way for training more robust and adaptable models. These enhanced mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#65292;&#26088;&#22312;&#20811;&#26381;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#24847;&#20041;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.06509</link><description>&lt;p&gt;
AntEval: &#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;
&lt;/p&gt;
&lt;p&gt;
AntEval: Quantitatively Evaluating Informativeness and Expressiveness of Agent Social Interactions. (arXiv:2401.06509v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06509
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#29615;&#22659;&#65292;&#37327;&#21270;&#35780;&#20272;&#26234;&#33021;&#20307;&#31038;&#20132;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#65292;&#26088;&#22312;&#20811;&#26381;&#38544;&#31169;&#38382;&#39064;&#24182;&#20419;&#20351;&#26234;&#33021;&#20307;&#36827;&#34892;&#26377;&#24847;&#20041;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26234;&#33021;&#20307;&#24050;&#25104;&#21151;&#22320;&#27169;&#20223;&#20102;&#21508;&#31181;&#24773;&#22659;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#65292;&#20294;&#22797;&#26434;&#30340;&#12289;&#22810;&#35282;&#33394;&#31038;&#20132;&#20114;&#21160;&#22312;&#25193;&#23637;&#29615;&#22659;&#20013;&#30340;&#39046;&#22495;&#20173;&#26410;&#20805;&#20998;&#25506;&#32034;&#12290;&#38544;&#31169;&#38382;&#39064;&#20351;&#25429;&#25417;&#21644;&#21033;&#29992;&#22797;&#26434;&#30340;&#29616;&#23454;&#29983;&#27963;&#20114;&#21160;&#21464;&#24471;&#22256;&#38590;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#32570;&#20047;&#23450;&#37327;&#35780;&#20272;&#26041;&#27861;&#38459;&#30861;&#20102;&#39640;&#36136;&#37327;&#26234;&#33021;&#20307;&#20114;&#21160;&#30340;&#36861;&#27714;&#65292;&#23548;&#33268;&#20114;&#21160;&#30340;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#26377;&#38480;&#65292;&#34920;&#29616;&#20026;&#32932;&#27973;&#30340;&#38386;&#32842;&#32780;&#27809;&#26377;&#28165;&#26224;&#30340;&#24847;&#22270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26700;&#38754;&#35282;&#33394;&#25198;&#28436;&#28216;&#25103;&#65288;TRPG&#65289;&#30340;&#35268;&#21017;&#21019;&#24314;&#20102;&#19968;&#20010;&#26377;&#21033;&#20110;&#22797;&#26434;&#12289;&#19978;&#19979;&#25991;&#20016;&#23500;&#30340;&#20114;&#21160;&#30340;&#29615;&#22659;&#65292;&#24378;&#35843;&#20449;&#24687;&#24615;&#21644;&#34920;&#36798;&#24615;&#12290;&#36825;&#20010;&#34394;&#25311;&#29615;&#22659;&#20943;&#36731;&#20102;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#28608;&#21169;&#26234;&#33021;&#20307;&#20316;&#20026;&#28216;&#25103;&#30446;&#26631;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#26377;&#24847;&#20041;&#30340;&#12289;&#39640;&#36136;&#37327;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Large Language Models (LLMs) based agents have successfully mimicked human behaviors in various scenarios, the realm of complex, multi-character social interactions within extended contexts remains underexplored. The challenge is compounded by privacy concerns, making it difficult to capture and utilize intricate real-life interactions. More importantly, the absence of quantitative evaluation methods hampers the pursuit of high-quality agent interactions, often leading to interactions that are limited in informativeness and expressiveness, characterized by superficial small talk without clear intentions. In this work, we leverage the rules of Tabletop Role-Playing Games (TRPG) to create an environment conducive to complex, context-rich interactions, emphasizing informativeness and expressiveness. This virtual setting alleviates privacy concerns and motivates agents to engage in meaningful, high-quality interactions as part of their in-game objectives. To assess these interactions
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#21644;DistilBERT&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#32467;&#26500;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20013;&#30340;&#20844;&#27491;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06495</link><description>&lt;p&gt;
BERT&#21644;DistilBERT&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#32467;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An investigation of structures responsible for gender bias in BERT and DistilBERT. (arXiv:2401.06495v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;BERT&#21644;DistilBERT&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#32467;&#26500;&#65292;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#20013;&#30340;&#20844;&#27491;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Transformer&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#36890;&#36807;&#25512;&#21160;&#26368;&#20808;&#36827;&#25216;&#26415;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#36793;&#30028;&#65292;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24615;&#33021;&#25552;&#21319;&#20276;&#38543;&#30528;&#22797;&#26434;&#24615;&#30340;&#22686;&#21152;&#65292;&#22240;&#27492;&#36825;&#20123;&#27169;&#22411;&#30340;&#22823;&#23567;&#65288;&#21487;&#39640;&#36798;&#25968;&#21313;&#20159;&#21442;&#25968;&#65289;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#25110;&#30701;&#25512;&#29702;&#26102;&#38388;&#20219;&#21153;&#19978;&#30340;&#37096;&#32626;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#20986;&#29616;&#20102;&#21387;&#32553;&#27169;&#22411;&#65288;&#22914;DistilBERT&#65289;&#65292;&#20351;&#24471;&#23427;&#20204;&#22312;&#36234;&#26469;&#36234;&#22810;&#24433;&#21709;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#24212;&#29992;&#20013;&#21487;&#20197;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;PLM&#21644;&#20854;&#31934;&#31616;&#29256;&#26412;&#30340;&#39044;&#27979;&#20844;&#27491;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26126;&#30830;&#20004;&#20010;&#38382;&#39064;&#26469;&#23545;&#36825;&#20010;&#38382;&#39064;&#36827;&#34892;&#23454;&#35777;&#30740;&#31350;&#65306;&#65288;1&#65289;&#25105;&#20204;&#33021;&#21542;&#30830;&#23450;BERT&#65288;&#20197;&#21450;DistilBERT&#65289;&#20013;&#36127;&#36131;&#24615;&#21035;&#20559;&#35265;&#30340;&#31070;&#32463;&#26426;&#21046;&#65311;&#65288;2&#65289;&#33976;&#39311;&#26159;&#21542;&#20542;&#21521;&#20110;&#21152;&#37325;&#25110;&#20943;&#36731;&#36825;&#31181;&#20559;&#35265;&#65311;
&lt;/p&gt;
&lt;p&gt;
In recent years, large Transformer-based Pre-trained Language Models (PLM) have changed the Natural Language Processing (NLP) landscape, by pushing the performance boundaries of the state-of-the-art on a wide variety of tasks. However, this performance gain goes along with an increase in complexity, and as a result, the size of such models (up to billions of parameters) represents a constraint for their deployment on embedded devices or short-inference time tasks. To cope with this situation, compressed models emerged (e.g. DistilBERT), democratizing their usage in a growing number of applications that impact our daily lives. A crucial issue is the fairness of the predictions made by both PLMs and their distilled counterparts. In this paper, we propose an empirical exploration of this problem by formalizing two questions: (1) Can we identify the neural mechanism(s) responsible for gender bias in BERT (and by extension DistilBERT)? (2) Does distillation tend to accentuate or mitigate ge
&lt;/p&gt;</description></item><item><title>Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;</title><link>http://arxiv.org/abs/2401.06477</link><description>&lt;p&gt;
Kun: &#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#30340;&#20013;&#22269;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#31572;&#26696;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06477
&lt;/p&gt;
&lt;p&gt;
Kun&#26159;&#19968;&#31181;&#20351;&#29992;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#36890;&#36807;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#25913;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#12290;&#23427;&#30340;&#20027;&#35201;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#31639;&#27861;&#25913;&#36827;&#25552;&#39640;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#36890;&#36807;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#20943;&#23569;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Kun&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#19981;&#20381;&#36182;&#25163;&#21160;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21019;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#38598;&#12290;Kun&#21033;&#29992;&#26469;&#33258;&#21566;&#36947;&#12289;&#23436;&#21367;&#21644;SkyPile&#31561;&#22810;&#20010;&#26469;&#28304;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#37319;&#29992;&#22522;&#20110;&#25351;&#20196;&#21453;&#21521;&#32763;&#35793;&#21644;&#31572;&#26696;&#20248;&#21270;&#30340;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#29983;&#25104;&#20102;&#19968;&#20010;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#20013;&#25991;&#25351;&#23548;&#25968;&#25454;&#28857;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#33258;&#25105;&#31579;&#36873;&#36807;&#31243;&#26469;&#23436;&#21892;&#21644;&#36873;&#25321;&#26368;&#26377;&#25928;&#30340;&#25351;&#20196;-&#36755;&#20986;&#23545;&#65292;&#26174;&#33879;&#20559;&#31163;&#20256;&#32479;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#23545;6B&#21442;&#25968;&#30340;Yi&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;Kun&#20855;&#26377;&#40065;&#26834;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#36129;&#29486;&#22312;&#20110;&#31639;&#27861;&#30340;&#25913;&#36827;&#65292;&#22686;&#24378;&#20102;&#25968;&#25454;&#30340;&#20445;&#30041;&#21644;&#28165;&#26224;&#24230;&#65292;&#24182;&#19988;&#21019;&#26032;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#23545;&#26114;&#36149;&#21644;&#32791;&#26102;&#30340;&#25163;&#21160;&#27880;&#37322;&#30340;&#20381;&#36182;&#12290;&#36825;&#31181;&#26041;&#27861;ological&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#20013;&#25991;&#33258;&#23545;&#40784;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#39640;&#20102;&#25968;&#25454;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Kun, a novel approach for creating high-quality instruction-tuning datasets for large language models (LLMs) without relying on manual annotations. Adapting a self-training algorithm based on instruction back-translation and answer polishment, Kun leverages unlabelled data from diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial dataset of over a million Chinese instructional data points. This approach significantly deviates from traditional methods by using a self-curation process to refine and select the most effective instruction-output pairs. Our experiments with the 6B-parameter Yi model across various benchmarks demonstrate Kun's robustness and scalability. Our method's core contributions lie in its algorithmic advancement, which enhances data retention and clarity, and its innovative data generation approach that substantially reduces the reliance on costly and time-consuming manual annotations. This methodology presents a sc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06469</link><description>&lt;p&gt;
&#25209;&#22788;&#29702;ICL: &#26377;&#25928;&#65292;&#39640;&#25928;&#19988;&#26080;&#24207;&#22320;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Batch-ICL: Effective, Efficient, and Order-Agnostic In-Context Learning. (arXiv:2401.06469v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06469
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25209;&#22788;&#29702;ICL&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;ICL&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#24320;&#21457;&#20986;&#20102;&#19968;&#20010;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#36890;&#36807;&#32858;&#21512;&#20803;&#26799;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#26080;&#20851;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#25490;&#21015;&#26041;&#24335;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#35270;&#20026;&#19968;&#20010;&#20803;&#20248;&#21270;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;LLM&#23545;ICL&#31034;&#20363;&#39034;&#24207;&#25935;&#24863;&#30340;&#21407;&#22240;&#12290;&#36825;&#31181;&#29702;&#35299;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;Batch-ICL&#65292;&#19968;&#31181;&#29992;&#20110;ICL&#30340;&#26377;&#25928;&#12289;&#39640;&#25928;&#19988;&#26080;&#24207;&#30340;&#25512;&#29702;&#31639;&#27861;&#12290;&#19982;&#26631;&#20934;&#30340;N-shot&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;Batch-ICL&#20351;&#29992;N&#20010;&#21333;&#29420;&#30340;1-shot&#21069;&#21521;&#35745;&#31639;&#65292;&#24182;&#32858;&#21512;&#24471;&#21040;&#30340;&#20803;&#26799;&#24230;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#32858;&#21512;&#30340;&#20803;&#26799;&#24230;&#24212;&#29992;&#20110;&#38646;-shot&#23398;&#20064;&#20197;&#29983;&#25104;&#26368;&#32456;&#39044;&#27979;&#12290;&#36825;&#31181;&#25209;&#22788;&#29702;&#26041;&#27861;&#20351;LLM&#23545;ICL&#31034;&#20363;&#30340;&#39034;&#24207;&#26080;&#20851;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;Batch-ICL&#19968;&#33268;&#20248;&#20110;&#22823;&#22810;&#25968;&#31034;&#20363;&#24207;&#21015;&#30340;&#25490;&#21015;&#26041;&#24335;&#12290;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#26631;&#20934;ICL&#30340;&#26368;&#20339;&#39034;&#24207;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;Batch-ICL&#30340;&#19968;&#31181;&#26032;&#39062;&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#22810;&#20010;"epochs"&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, by treating in-context learning (ICL) as a meta-optimization process, we explain why LLMs are sensitive to the order of ICL examples. This understanding leads us to the development of Batch-ICL, an effective, efficient, and order-agnostic inference algorithm for ICL. Differing from the standard N-shot learning approach, Batch-ICL employs $N$ separate 1-shot forward computations and aggregates the resulting meta-gradients. These aggregated meta-gradients are then applied to a zero-shot learning to generate the final prediction. This batch processing approach renders the LLM agnostic to the order of ICL examples. Through extensive experiments and analysis, we demonstrate that Batch-ICL consistently outperforms most permutations of example sequences. In some cases, it even exceeds the performance of the optimal order for standard ICL, all while reducing the computational resources required. Furthermore, we develop a novel variant of Batch-ICL featuring multiple "epochs" of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;</title><link>http://arxiv.org/abs/2401.06468</link><description>&lt;p&gt;
&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Adapting Large Language Models for Document-Level Machine Translation. (arXiv:2401.06468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36866;&#24212;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#30340;&#36807;&#31243;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36807;&#20102;GPT-4&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#20173;&#28982;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#20043;&#21518;&#65292;&#20013;&#31561;&#35268;&#27169;&#30340;LLMs&#24448;&#24448;&#32988;&#36807;&#20854;&#26356;&#22823;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#23558;LLMs&#35843;&#25972;&#20026;&#29305;&#23450;&#35821;&#35328;&#23545;&#30340;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#65288;DocMT&#65289;&#30340;&#36807;&#31243;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#25552;&#31034;&#31574;&#30053;&#23545;&#19979;&#28216;&#32763;&#35793;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#24494;&#35843;&#26041;&#27861;&#12289;&#19977;&#31181;LLM&#20027;&#24178;&#21644;18&#20010;&#28041;&#21450;&#20061;&#31181;&#35821;&#35328;&#23545;&#30340;&#32763;&#35793;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#19987;&#38376;&#30340;&#27169;&#22411;&#29978;&#33267;&#22312;&#32763;&#35793;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;GPT-4&#65292;&#32780;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#65292;&#21363;&#20351;&#23427;&#20204;&#19987;&#38376;&#22312;&#21452;&#35821;&#24179;&#34892;&#25991;&#26723;&#19978;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20173;&#28982;&#26126;&#26174;&#23384;&#22312;&#31163;&#26631;&#32763;&#35793;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#38024;&#23545;DocMT&#37327;&#36523;&#23450;&#21046;&#30340;LLMs&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#32763;&#35793;&#20934;&#30830;&#24230;&#25913;&#21892;&#12289;&#22810;&#28304;&#20449;&#24687;&#25972;&#21512;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have made significant strides in various natural language processing (NLP) tasks. Recent research shows that the moderately-sized LLMs often outperform their larger counterparts after task-specific fine-tuning. In this work, we delve into the process of adapting LLMs to specialize in document-level machine translation (DocMT) for a specific language pair. Firstly, we explore how prompt strategies affect downstream translation performance. Then, we conduct extensive experiments with two fine-tuning methods, three LLM backbones, and 18 translation tasks across nine language pairs. Our findings indicate that in some cases, these specialized models even surpass GPT-4 in translation performance, while they still significantly suffer from the off-target translation issue in others, even if they are exclusively fine-tuned on bilingual parallel documents. Furthermore, we provide an in-depth analysis of these LLMs tailored for DocMT, exploring aspects such as transl
&lt;/p&gt;</description></item><item><title>PersianMind&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27874;&#26031;&#35821;&#20013;&#23637;&#29616;&#19982;&#38381;&#28304;&#30340;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#24320;&#28304;&#27169;&#22411;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#19978;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06466</link><description>&lt;p&gt;
PersianMind: &#19968;&#20010;&#36328;&#35821;&#35328;&#30340;&#27874;&#26031;&#35821;-&#33521;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06466
&lt;/p&gt;
&lt;p&gt;
PersianMind&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#27874;&#26031;&#35821;&#20013;&#23637;&#29616;&#19982;&#38381;&#28304;&#30340;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#30340;&#20248;&#21183;&#65292;&#35299;&#20915;&#20102;&#24320;&#28304;&#27169;&#22411;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#19978;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#24182;&#20855;&#22791;&#24191;&#27867;&#30340;&#22810;&#39046;&#22495;&#30693;&#35782;&#12290;&#34429;&#28982;&#23427;&#20204;&#22312;&#33521;&#35821;&#26041;&#38754;&#34920;&#29616;&#26368;&#20248;&#65292;&#20294;&#23427;&#20204;&#22312;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#20063;&#24456;&#26174;&#33879;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#22914;LLaMa&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#20027;&#35201;&#26159;&#22312;&#33521;&#25991;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#65292;&#23548;&#33268;&#22312;&#38750;&#33521;&#25991;&#35821;&#35328;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PersianMind&#65292;&#19968;&#20010;&#24320;&#28304;&#30340;&#21452;&#35821;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#27874;&#26031;&#35821;&#20013;&#23637;&#31034;&#20102;&#19982;&#38381;&#28304;&#30340;GPT-3.5-turbo&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#23558;LLaMa2&#30340;&#35789;&#27719;&#34920;&#25193;&#23637;10,000&#20010;&#27874;&#26031;&#35821;&#26631;&#35760;&#65292;&#24182;&#35757;&#32451;&#32422;20&#20159;&#20010;&#27874;&#26031;&#35821;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20445;&#30041;&#20102;&#27169;&#22411;&#30340;&#33521;&#35821;&#30693;&#35782;&#24182;&#21033;&#29992;&#36801;&#31227;&#23398;&#20064;&#22312;&#19981;&#21516;&#35821;&#35328;&#38388;&#20256;&#36882;&#20219;&#21153;&#30693;&#35782;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models demonstrate remarkable proficiency in various linguistic tasks and have extensive knowledge across various domains. Although they perform best in English, their ability in other languages is notable too. In contrast, open-source models, such as LLaMa, are primarily trained on English datasets, resulting in poor performance in non-English languages. In this paper, we introduce PersianMind, an open-source bilingual large language model which demonstrates comparable performance to closed-source GPT-3.5-turbo in the Persian language. By expanding LLaMa2's vocabulary with 10,000 Persian tokens and training it on a dataset comprising nearly 2 billion Persian tokens, we show that our approach preserves the model's English knowledge and employs transfer learning to excel at transferring task knowledge from one language to another.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BOK-VQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#20197;&#21450;&#19982;&#38382;&#39064;-&#22238;&#31572;&#20869;&#23481;&#30456;&#20851;&#30340;&#30693;&#35782;&#20449;&#24687;&#12290;&#36890;&#36807;&#20197;&#22270;&#23884;&#20837;&#30340;&#24418;&#24335;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#30693;&#35782;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22806;&#37096;&#30693;&#35782;&#27880;&#20837;VQA&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#38382;&#31572;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.06443</link><description>&lt;p&gt;
BOK-VQA: &#22522;&#20110;&#22806;&#37096;&#30693;&#35782;&#30340;&#21452;&#35821;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#36890;&#36807;&#22270;&#34920;&#31034;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
BOK-VQA: Bilingual Outside Knowledge-based Visual Question Answering via Graph Representation Pretraining. (arXiv:2401.06443v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;BOK-VQA&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#35821;&#35328;&#30340;&#35270;&#35273;&#38382;&#31572;&#25968;&#25454;&#20197;&#21450;&#19982;&#38382;&#39064;-&#22238;&#31572;&#20869;&#23481;&#30456;&#20851;&#30340;&#30693;&#35782;&#20449;&#24687;&#12290;&#36890;&#36807;&#20197;&#22270;&#23884;&#20837;&#30340;&#24418;&#24335;&#39044;&#35757;&#32451;&#25968;&#25454;&#30340;&#30693;&#35782;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#22806;&#37096;&#30693;&#35782;&#27880;&#20837;VQA&#31995;&#32479;&#20013;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#38382;&#31572;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#26041;&#21521;&#65292;&#22914;&#26368;&#36817;&#24320;&#21457;&#30340;GPT4&#65292;&#26088;&#22312;&#20026;&#22810;&#27169;&#24577;&#21644;&#22810;&#35821;&#35328;&#36755;&#20837;&#23547;&#25214;&#30456;&#20851;&#30340;&#30693;&#35782;&#20449;&#24687;&#20197;&#25552;&#20379;&#31572;&#26696;&#12290;&#26681;&#25454;&#36825;&#20123;&#30740;&#31350;&#24773;&#20917;&#65292;&#23545;&#22810;&#35821;&#35328;&#35780;&#20272;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#30340;&#38656;&#27714;&#65292;&#20316;&#20026;&#22810;&#27169;&#24577;&#31995;&#32479;&#30340;&#20195;&#34920;&#20219;&#21153;&#65292;&#36880;&#28176;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;&#26412;&#30740;&#31350;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#30340;&#21452;&#35821;&#22806;&#37096;&#30693;&#35782;VQA&#65288;BOK-VQA&#65289;&#25968;&#25454;&#38598;&#12290;&#25152;&#25552;&#20986;&#30340;&#25968;&#25454;&#21253;&#25324;17K&#24352;&#22270;&#29255;&#65292;17K&#20010;&#38889;&#35821;&#21644;&#33521;&#35821;&#30340;&#38382;&#39064;-&#22238;&#31572;&#23545;&#20197;&#21450;&#19982;&#38382;&#39064;-&#22238;&#31572;&#20869;&#23481;&#30456;&#20851;&#30340;28K&#20010;&#30693;&#35782;&#20449;&#24687;&#23454;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#20197;&#22270;&#23884;&#20837;&#30340;&#24418;&#24335;&#39044;&#35757;&#32451;BOK-VQA&#25968;&#25454;&#30340;&#30693;&#35782;&#20449;&#24687;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#30693;&#35782;&#20449;&#24687;&#27880;&#20837;VQA&#31995;&#32479;&#20013;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#28145;&#20837;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26500;&#24314;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#20449;&#24687;&#30340;&#23454;&#38469;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current research direction in generative models, such as the recently developed GPT4, aims to find relevant knowledge information for multimodal and multilingual inputs to provide answers. Under these research circumstances, the demand for multilingual evaluation of visual question answering (VQA) tasks, a representative task of multimodal systems, has increased. Accordingly, we propose a bilingual outside-knowledge VQA (BOK-VQA) dataset in this study that can be extended to multilingualism. The proposed data include 17K images, 17K question-answer pairs for both Korean and English and 280K instances of knowledge information related to question-answer content. We also present a framework that can effectively inject knowledge information into a VQA system by pretraining the knowledge information of BOK-VQA data in the form of graph embeddings. Finally, through in-depth analysis, we demonstrated the actual effect of the knowledge information contained in the constructed training data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06431</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21270;&#21040;&#22686;&#24378;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21319;&#20316;&#25991;&#35780;&#20998;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;LLM AES&#31995;&#32479;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;LLM&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#32773;&#26469;&#35828;&#65292;&#25509;&#25910;&#21363;&#26102;&#20010;&#24615;&#21270;&#21453;&#39304;&#38750;&#24120;&#37325;&#35201;&#65292;&#24403;&#20154;&#31867;&#25945;&#24072;&#26080;&#27861;&#25552;&#20379;&#26102;&#65292;&#33258;&#21160;&#21270;&#20316;&#25991;&#35780;&#20998;&#31995;&#32479;&#26159;&#19968;&#31181;&#37325;&#35201;&#36164;&#28304;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29305;&#21035;&#26159;GPT-4&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#65292;&#20316;&#20026;AES&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#22522;&#20110;&#20844;&#20849;&#21644;&#31169;&#26377;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#31361;&#20986;&#20102;LLM AES&#31995;&#32479;&#30340;&#26174;&#30528;&#20248;&#21183;&#65292;&#21253;&#25324;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12289;&#19968;&#33268;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#32780;&#32463;&#36807;&#24494;&#35843;&#30340;GPT-3.5&#36229;&#36234;&#20102;&#20256;&#32479;&#35780;&#20998;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;LLM&#36741;&#21161;&#30340;&#20154;&#24037;&#35780;&#20272;&#23454;&#39564;&#65292;&#28041;&#21450;&#21021;&#23398;&#32773;&#21644;&#19987;&#23478;&#35780;&#20998;&#21592;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#21457;&#29616;&#26159;&#65292;LLM&#19981;&#20165;&#33021;&#33258;&#21160;&#21270;&#35780;&#20998;&#36807;&#31243;&#65292;&#36824;&#33021;&#25552;&#21319;&#20154;&#31867;&#35780;&#20998;&#21592;&#30340;&#24615;&#33021;&#12290;&#24403;&#21021;&#23398;&#32773;&#35780;&#20998;&#21592;&#33719;&#24471;LLM&#29983;&#25104;&#30340;&#21453;&#39304;&#26102;&#65292;&#20854;&#20934;&#30830;&#24615;&#19982;&#19987;&#23478;&#27700;&#24179;&#30456;&#24403;&#65292;&#21516;&#26102;&#19987;&#23478;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Receiving immediate and personalized feedback is crucial for second-language learners, and Automated Essay Scoring (AES) systems are a vital resource when human instructors are unavailable. This study investigates the effectiveness of Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as tools for AES. Our comprehensive set of experiments, conducted on both public and private datasets, highlights the remarkable advantages of LLM-based AES systems. They include superior accuracy, consistency, generalizability, and interpretability, with fine-tuned GPT-3.5 surpassing traditional grading models. Additionally, we undertake LLM-assisted human evaluation experiments involving both novice and expert graders. One pivotal discovery is that LLMs not only automate the grading process but also enhance the performance of human graders. Novice graders when provided with feedback generated by LLMs, achieve a level of accuracy on par with experts, while experts become more effici
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06416</link><description>&lt;p&gt;
&#19981;&#21487;&#33021;&#20219;&#21153;&#65306;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#20102;&#25903;&#25345;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#30340;&#35266;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#20154;&#24037;&#21512;&#25104;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#24471;&#20986;&#20102;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Chomsky&#21644;&#20854;&#20182;&#20154;&#30452;&#25509;&#22768;&#31216;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33021;&#22815;&#23398;&#20064;&#20154;&#31867;&#26080;&#27861;&#23398;&#20064;&#30340;&#21487;&#33021;&#21644;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#21457;&#34920;&#30340;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#26679;&#30340;&#35828;&#27861;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31995;&#32479;&#22320;&#25913;&#21464;&#33521;&#25991;&#25968;&#25454;&#30340;&#35789;&#24207;&#21644;&#35821;&#27861;&#35268;&#21017;&#65292;&#24320;&#21457;&#20102;&#19968;&#32452;&#19981;&#21487;&#33021;&#30340;&#21512;&#25104;&#35821;&#35328;&#65292;&#27599;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#31243;&#24230;&#19981;&#21516;&#12290;&#36825;&#20123;&#35821;&#35328;&#20301;&#20110;&#19968;&#20010;&#19981;&#21487;&#33021;&#30340;&#36830;&#32493;&#20307;&#19978;&#65306;&#19968;&#31471;&#26159;&#26412;&#36136;&#19978;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#20363;&#22914;&#33521;&#25991;&#21333;&#35789;&#30340;&#38543;&#26426;&#21644;&#19981;&#21487;&#36870;&#30340;&#27927;&#29260;&#65292;&#32780;&#21478;&#19968;&#31471;&#26159;&#22312;&#35821;&#35328;&#23398;&#19978;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#33021;&#30340;&#35821;&#35328;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#35745;&#31639;&#35789;&#20301;&#32622;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#26469;&#35780;&#20272;GPT-2&#23567;&#22411;&#27169;&#22411;&#23398;&#20064;&#36825;&#20123;&#26080;&#21487;&#20105;&#35758;&#30340;&#19981;&#21487;&#33021;&#35821;&#35328;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#25972;&#20010;&#36807;&#31243;&#20013;&#36827;&#34892;&#20102;&#36825;&#20123;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21313;&#20010;&#36136;&#37327;&#21644;&#35821;&#35328;&#35782;&#21035;&#36807;&#28388;&#22120;&#23545;&#19981;&#21516;&#31038;&#20132;&#32500;&#24230;&#21464;&#21270;&#30340;&#32593;&#39029;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#21457;&#29616;&#22312;&#25968;&#25454;&#31579;&#36873;&#36807;&#31243;&#20013;&#23384;&#22312;&#38544;&#21547;&#30340;&#20559;&#22909;&#65292;&#19968;&#20123;&#36136;&#37327;&#20998;&#31867;&#22120;&#31867;&#20284;&#20110;&#20027;&#39064;&#36807;&#28388;&#22120;&#65292;&#32780;&#35821;&#35328;&#35782;&#21035;&#21487;&#33021;&#20250;&#24573;&#35270;&#26576;&#20123;&#22320;&#21306;&#30340;&#33521;&#35821;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20419;&#36827;&#26356;&#20844;&#27491;&#21644;&#20840;&#38754;&#30340;&#27169;&#22411;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06408</link><description>&lt;p&gt;
&#20851;&#20110;&#25105;&#65306;&#20351;&#29992;&#33258;&#25105;&#25551;&#36848;&#30340;&#32593;&#39029;&#26469;&#35760;&#24405;&#33521;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#36807;&#28388;&#22120;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters. (arXiv:2401.06408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06408
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#21313;&#20010;&#36136;&#37327;&#21644;&#35821;&#35328;&#35782;&#21035;&#36807;&#28388;&#22120;&#23545;&#19981;&#21516;&#31038;&#20132;&#32500;&#24230;&#21464;&#21270;&#30340;&#32593;&#39029;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#21457;&#29616;&#22312;&#25968;&#25454;&#31579;&#36873;&#36807;&#31243;&#20013;&#23384;&#22312;&#38544;&#21547;&#30340;&#20559;&#22909;&#65292;&#19968;&#20123;&#36136;&#37327;&#20998;&#31867;&#22120;&#31867;&#20284;&#20110;&#20027;&#39064;&#36807;&#28388;&#22120;&#65292;&#32780;&#35821;&#35328;&#35782;&#21035;&#21487;&#33021;&#20250;&#24573;&#35270;&#26576;&#20123;&#22320;&#21306;&#30340;&#33521;&#35821;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20026;&#20419;&#36827;&#26356;&#20844;&#27491;&#21644;&#20840;&#38754;&#30340;&#27169;&#22411;&#24320;&#21457;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33021;&#21147;&#26469;&#28304;&#20110;&#23427;&#20204;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#65292;&#27169;&#22411;&#30340;&#24320;&#21457;&#22987;&#20110;&#25968;&#25454;&#30340;&#31579;&#36873;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#21021;&#27493;&#38454;&#27573;&#20915;&#23450;&#20445;&#30041;&#21738;&#20123;&#25968;&#25454;&#25110;&#31227;&#38500;&#21738;&#20123;&#25968;&#25454;&#30340;&#20915;&#31574;&#24120;&#24120;&#27809;&#26377;&#34987;&#20805;&#20998;&#23457;&#26597;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#32593;&#39029;&#25991;&#26412;&#19982;&#20854;&#31038;&#20132;&#21644;&#22320;&#29702;&#32972;&#26223;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;1030&#19975;&#20010;&#32593;&#39029;&#21019;&#24314;&#32773;&#30340;&#33258;&#25105;&#25551;&#36848;&#65292;&#24182;&#25552;&#21462;&#20102;&#20851;&#20110;&#20182;&#20204;&#30340;&#20010;&#20154;&#20449;&#24687;&#20197;&#21450;&#20182;&#20204;&#26469;&#33258;&#21738;&#37324;&#30340;&#20449;&#24687;&#65306;&#20182;&#20204;&#30340;&#20852;&#36259;&#39046;&#22495;&#12289;&#31038;&#20132;&#35282;&#33394;&#21644;&#22320;&#29702;&#24402;&#23646;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#35843;&#26597;&#20102;&#21313;&#20010;&#8220;&#36136;&#37327;&#8221;&#21644;&#33521;&#35821;&#35821;&#35328;&#35782;&#21035;&#65288;langID&#65289;&#36807;&#28388;&#22120;&#23545;&#36825;&#20123;&#31038;&#20132;&#32500;&#24230;&#21464;&#21270;&#30340;&#32593;&#39029;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;&#25968;&#25454;&#31579;&#36873;&#20013;&#19968;&#31995;&#21015;&#38544;&#21547;&#30340;&#20559;&#22909;&#65306;&#25105;&#20204;&#23637;&#31034;&#20986;&#19968;&#20123;&#36136;&#37327;&#20998;&#31867;&#22120;&#30340;&#20316;&#29992;&#31867;&#20284;&#20110;&#20027;&#39064;&#39046;&#22495;&#36807;&#28388;&#22120;&#65292;&#32780;langID&#21487;&#33021;&#20250;&#24573;&#35270;&#19990;&#30028;&#26576;&#20123;&#22320;&#21306;&#30340;&#33521;&#35821;&#20869;&#23481;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#25552;&#20379;&#23545;&#25968;&#25454;&#31579;&#36873;&#20013;&#38544;&#21547;&#20559;&#22909;&#30340;&#27934;&#23519;&#65292;&#20197;&#20419;&#36827;&#26356;&#20844;&#27491;&#21644;&#20840;&#38754;&#30340;&#27169;&#22411;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten "quality" and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will enc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06401</link><description>&lt;p&gt;
DevEval: &#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#20013;&#30340;&#20195;&#30721;&#29983;&#25104;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#30495;&#23454;&#30340;&#39033;&#30446;&#20998;&#24067;&#12289;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#31561;&#26041;&#38754;&#26356;&#36148;&#21512;&#23454;&#38469;&#12290;&#36890;&#36807;&#23545;&#20116;&#20010;&#27969;&#34892;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#34920;&#29616;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#24050;&#32463;&#25552;&#20986;&#65292;&#20294;&#26159;&#19982;&#23454;&#38469;&#36719;&#20214;&#39033;&#30446;&#19981;&#19968;&#33268;&#65292;&#20363;&#22914;&#34394;&#26500;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20381;&#36182;&#19981;&#36275;&#21644;&#23567;&#35268;&#27169;&#39033;&#30446;&#32972;&#26223;&#12290;&#22240;&#27492;&#65292;LLMs&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#33021;&#21147;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;DevEval&#30340;&#26032;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#24320;&#21457;&#20154;&#21592;&#22312;&#23454;&#38469;&#39033;&#30446;&#20013;&#30340;&#32463;&#39564;&#30456;&#21563;&#21512;&#12290;DevEval&#36890;&#36807;&#19968;&#20010;&#20005;&#26684;&#30340;&#27969;&#31243;&#25910;&#38598;&#21040;&#20102;&#26469;&#33258;119&#20010;&#23454;&#38469;&#39033;&#30446;&#30340;2690&#20010;&#26679;&#26412;&#65292;&#28085;&#30422;10&#20010;&#39046;&#22495;&#12290;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#30456;&#27604;&#65292;DevEval&#22312;&#22810;&#20010;&#32500;&#24230;&#19978;&#19982;&#23454;&#38469;&#39033;&#30446;&#30456;&#21563;&#21512;&#65292;&#20363;&#22914;&#30495;&#23454;&#30340;&#31243;&#24207;&#20998;&#24067;&#65292;&#20805;&#36275;&#30340;&#20381;&#36182;&#21644;&#36275;&#22815;&#35268;&#27169;&#30340;&#39033;&#30446;&#32972;&#26223;&#12290;&#25105;&#20204;&#22312;DevEval&#19978;&#35780;&#20272;&#20102;&#20116;&#20010;&#27969;&#34892;&#30340;LLMs&#65288;&#20363;&#22914;gpt-4&#65292;gpt-3.5-turbo&#65292;CodeLLaMa&#21644;StarCoder&#65289;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#20195;&#30721;&#29983;&#25104;&#20013;&#30340;&#23454;&#38469;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;gpt-3.5-turbo&#30340;&#26368;&#39640;Pass@1&#21482;&#26377;42&#12290;
&lt;/p&gt;
&lt;p&gt;
How to evaluate Large Language Models (LLMs) in code generation is an open question. Many benchmarks have been proposed but are inconsistent with practical software projects, e.g., unreal program distributions, insufficient dependencies, and small-scale project contexts. Thus, the capabilities of LLMs in practical projects are still unclear. In this paper, we propose a new benchmark named DevEval, aligned with Developers' experiences in practical projects. DevEval is collected through a rigorous pipeline, containing 2,690 samples from 119 practical projects and covering 10 domains. Compared to previous benchmarks, DevEval aligns to practical projects in multiple dimensions, e.g., real program distributions, sufficient dependencies, and enough-scale project contexts. We assess five popular LLMs on DevEval (e.g., gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo only is 42 in our experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoQAH&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35757;&#32451;&#20110;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#30340;QA&#20132;&#20114;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#23558;&#21487;&#35270;&#38382;&#31572;&#20174;&#21512;&#25104;&#38382;&#39064;&#27867;&#21270;&#21040;&#20154;&#24037;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;VQA&#27169;&#22411;&#21644;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.06400</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#30340;VQA&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#21487;&#35270;&#38382;&#31572;&#20174;&#21512;&#25104;&#38382;&#39064;&#27867;&#21270;&#21040;&#20154;&#24037;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model. (arXiv:2401.06400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;CoQAH&#65292;&#36890;&#36807;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#35757;&#32451;&#20110;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#30340;QA&#20132;&#20114;&#24207;&#21015;&#65292;&#23454;&#29616;&#20102;&#23558;&#21487;&#35270;&#38382;&#31572;&#20174;&#21512;&#25104;&#38382;&#39064;&#27867;&#21270;&#21040;&#20154;&#24037;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#65292;&#36229;&#36807;&#20102;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;VQA&#27169;&#22411;&#21644;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35270;&#38382;&#31572;&#65288;VQA&#65289;&#26159;&#19968;&#20010;&#32473;&#23450;&#22270;&#20687;&#24182;&#23601;&#35813;&#22270;&#20687;&#25552;&#20986;&#19968;&#31995;&#21015;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#26500;&#24314;&#19968;&#20010;&#39640;&#25928;&#30340;VQA&#31639;&#27861;&#38656;&#35201;&#22823;&#37327;&#30340;QA&#25968;&#25454;&#65292;&#32780;&#19988;&#38750;&#24120;&#26114;&#36149;&#12290;&#26681;&#25454;&#27169;&#26495;&#29983;&#25104;&#21512;&#25104;&#30340;QA&#23545;&#26159;&#33719;&#24471;&#25968;&#25454;&#30340;&#19968;&#31181;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#20110;&#36825;&#20123;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#24037;&#38382;&#39064;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#8220;&#20154;&#24037;&#38382;&#39064;&#36830;&#38145;&#38382;&#31572;&#8221;&#65288;CoQAH&#65289;&#12290;CoQAH&#21033;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#19968;&#20010;&#35757;&#32451;&#20110;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;VQA&#27169;&#22411;&#20043;&#38388;&#30340;QA&#20132;&#20114;&#24207;&#21015;&#26469;&#25512;&#29702;&#21644;&#25512;&#23548;&#20154;&#24037;&#38382;&#39064;&#30340;&#36923;&#36753;&#31572;&#26696;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#20154;&#24037;&#38382;&#39064;VQA&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;CoQAH&#30340;&#25928;&#26524;&#65292;&#21253;&#25324;3D&#28210;&#26579;&#22270;&#20687;&#21644;&#33016;&#37096;X&#32447;&#22270;&#20687;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#19978;&#37117;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#29575;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CoQAH&#22312;&#36890;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12289;VQA&#27169;&#22411;&#21644;&#21307;&#23398;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#30340;&#34920;&#29616;&#20063;&#36229;&#36807;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) is a task where an image is given, and a series of questions are asked about the image. To build an efficient VQA algorithm, a large amount of QA data is required which is very expensive. Generating synthetic QA pairs based on templates is a practical way to obtain data. However, VQA models trained on those data do not perform well on complex, human-written questions. To address this issue, we propose a new method called {\it chain of QA for human-written questions} (CoQAH). CoQAH utilizes a sequence of QA interactions between a large language model and a VQA model trained on synthetic data to reason and derive logical answers for human-written questions. We tested the effectiveness of CoQAH on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data. Notably, CoQAH outperformed general vision-language models, VQA models, and medical foundation models with no
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#21360;&#24230;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#20013;&#21435;&#38500;&#35823;&#35793;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.06398</link><description>&lt;p&gt;
&#20462;&#27491;&#21360;&#24230;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#24120;&#29992;&#25968;&#25454;&#38598;&#30340;&#35823;&#35793;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An approach for mistranslation removal from popular dataset for Indic MT Task. (arXiv:2401.06398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#21360;&#24230;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#20013;&#30340;&#24120;&#29992;&#25968;&#25454;&#38598;&#20013;&#21435;&#38500;&#35823;&#35793;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20869;&#23481;&#20174;&#19968;&#31181;&#35821;&#35328;&#36716;&#25442;&#20026;&#21478;&#19968;&#31181;&#35821;&#35328;&#30340;&#35745;&#31639;&#26426;&#31995;&#32479;&#34987;&#31216;&#20026;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#12290;&#20026;&#20102;&#30830;&#20445;&#26377;&#25928;&#30340;&#32763;&#35793;&#65292;&#20445;&#30041;&#28304;&#35821;&#35328;&#30340;&#19978;&#19979;&#25991;&#21644;&#35789;&#27719;&#35299;&#37322;&#65292;&#21508;&#31181;&#25216;&#26415;&#24050;&#32463;&#20986;&#29616;&#12290;&#31471;&#21040;&#31471;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#25216;&#26415;&#65292;&#29616;&#22312;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23454;&#38469;&#30340;MT&#31995;&#32479;&#20013;&#12290;MT&#31995;&#32479;&#38656;&#35201;&#22823;&#37327;&#30340;&#24179;&#34892;&#25968;&#25454;&#38598;&#65288;&#19968;&#20010;&#35821;&#35328;&#20013;&#30340;&#21477;&#23376;&#19982;&#21478;&#19968;&#38376;&#35821;&#35328;&#30340;&#32763;&#35793;&#65289;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#23545;&#20110;MT&#31995;&#32479;&#22312;&#35757;&#32451;&#38454;&#27573;&#23398;&#20064;&#35821;&#35328;&#32467;&#26500;&#21644;&#27169;&#24335;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#31181;&#25968;&#25454;&#38598;&#26159;Samanantar&#65292;&#36825;&#26159;&#26368;&#22823;&#30340;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#21360;&#24230;&#35821;&#35328;&#65288;ILs&#65289;&#24179;&#34892;&#25968;&#25454;&#38598;&#12290;&#30001;&#20110;&#35813;&#35821;&#26009;&#24211;&#26469;&#33258;&#21508;&#20010;&#26469;&#28304;&#65292;&#22240;&#27492;&#21253;&#21547;&#20102;&#35768;&#22810;&#38169;&#35823;&#30340;&#32763;&#35793;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;MT&#31995;&#32479;&#26080;&#27861;&#21457;&#25381;&#20854;&#27491;&#24120;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#26469;&#21435;&#38500;&#35823;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
The conversion of content from one language to another utilizing a computer system is known as Machine Translation (MT). Various techniques have come up to ensure effective translations that retain the contextual and lexical interpretation of the source language. End-to-end Neural Machine Translation (NMT) is a popular technique and it is now widely used in real-world MT systems. Massive amounts of parallel datasets (sentences in one language alongside translations in another) are required for MT systems. These datasets are crucial for an MT system to learn linguistic structures and patterns of both languages during the training phase. One such dataset is Samanantar, the largest publicly accessible parallel dataset for Indian languages (ILs). Since the corpus has been gathered from various sources, it contains many incorrect translations. Hence, the MT systems built using this dataset cannot perform to their usual potential. In this paper, we propose an algorithm to remove mistranslati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25913;&#21892;ASQP&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;ADA&#26041;&#27861;&#20248;&#20110;&#31616;&#21333;&#30340;&#25968;&#25454;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06394</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adaptive Data Augmentation for Aspect Sentiment Quad Prediction. (arXiv:2401.06394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25913;&#21892;ASQP&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#32780;ADA&#26041;&#27861;&#20248;&#20110;&#31616;&#21333;&#30340;&#25968;&#25454;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#39044;&#27979;&#65288;ASQP&#65289;&#26088;&#22312;&#39044;&#27979;&#32473;&#23450;&#21477;&#23376;&#30340;&#22235;&#20803;&#24773;&#24863;&#20803;&#32032;&#65292;&#36825;&#26159;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;ASQP&#20219;&#21153;&#20013;&#65292;&#25968;&#25454;&#19981;&#24179;&#34913;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#37325;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#22235;&#20803;&#27169;&#24335;&#19981;&#24179;&#34913;&#21644;&#26041;&#38754;&#31867;&#21035;&#19981;&#24179;&#34913;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#65288;ADA&#65289;&#26694;&#26550;&#26469;&#35299;&#20915;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#20855;&#26377;&#26465;&#20214;&#20989;&#25968;&#30340;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#33258;&#36866;&#24212;&#22686;&#24378;&#23614;&#37096;&#22235;&#20803;&#27169;&#24335;&#21644;&#26041;&#38754;&#31867;&#21035;&#65292;&#32531;&#35299;ASQP&#20013;&#30340;&#25968;&#25454;&#19981;&#24179;&#34913;&#12290;&#22312;&#20043;&#21069;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#29983;&#25104;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#31867;&#21035;&#20808;&#39564;&#30693;&#35782;&#21644;&#35821;&#27861;&#24341;&#23548;&#35299;&#30721;&#30446;&#26631;&#65292;&#25552;&#21462;&#23436;&#25972;&#30340;&#22235;&#20803;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25968;&#25454;&#22686;&#24378;&#21487;&#20197;&#25913;&#21892;ASQP&#20219;&#21153;&#20013;&#30340;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#24182;&#19988;&#25152;&#25552;&#20986;&#30340;ADA&#26041;&#27861;&#20248;&#20110;&#31616;&#21333;&#30340;&#25968;&#25454;&#36807;&#37319;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment elements for a given sentence, which is a critical task in the field of aspect-based sentiment analysis. However, the data imbalance issue has not received sufficient attention in ASQP task. In this paper, we divide the issue into two-folds, quad-pattern imbalance and aspect-category imbalance, and propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance issue. Specifically, a data augmentation process with a condition function adaptively enhances the tail quad patterns and aspect categories, alleviating the data imbalance in ASQP. Following previous studies, we also further explore the generative framework for extracting complete quads by introducing the category prior knowledge and syntax-guided decoding target. Experimental results demonstrate that data augmentation for imbalance in ASQP task can improve the performance, and the proposed ADA method is superior to naive data oversampling.
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#19982;AI&#30340;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#20154;&#31867;&#19982;AI&#20132;&#20114;&#36807;&#31243;&#20013;&#24515;&#26234;&#34920;&#24449;&#30340;&#24314;&#31435;&#65292;&#26088;&#22312;&#24110;&#21161;&#23454;&#29616;&#25104;&#21151;&#21644;&#36731;&#26494;&#30340;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2401.06382</link><description>&lt;p&gt;
&#25105;&#24212;&#35813;&#35828;&#20160;&#20040;&#65311;-&#19982;AI&#21644;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
What should I say? -- Interacting with AI and Natural Language Interfaces. (arXiv:2401.06382v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06382
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26222;&#21450;&#65292;&#30740;&#31350;&#20154;&#31867;&#19982;AI&#30340;&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25506;&#32034;&#20154;&#31867;&#19982;AI&#20132;&#20114;&#36807;&#31243;&#20013;&#24515;&#26234;&#34920;&#24449;&#30340;&#24314;&#31435;&#65292;&#26088;&#22312;&#24110;&#21161;&#23454;&#29616;&#25104;&#21151;&#21644;&#36731;&#26494;&#30340;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#25506;&#32034;&#20154;&#31867;&#22914;&#20309;&#19982;AI&#20132;&#20114;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#65288;HAI&#65289;&#23376;&#39046;&#22495;&#20174;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#39046;&#22495;&#20013;&#20986;&#29616;&#65292;&#26088;&#22312;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#35768;&#22810;&#20132;&#20114;&#27169;&#24335;&#24050;&#32463;&#23454;&#26045;&#65292;&#20294;&#23545;&#20110;&#20351;&#29992;&#36825;&#20123;&#26356;&#20687;&#20154;&#31867;&#26412;&#36136;&#30340;&#26367;&#20195;&#30028;&#38754;&#25152;&#38656;&#35748;&#30693;&#30340;&#21464;&#21270;&#20197;&#21450;&#20351;&#29992;&#36825;&#20123;&#30028;&#38754;&#30340;&#35748;&#30693;&#31185;&#23398;&#24433;&#21709;&#65292;&#20102;&#35299;&#36824;&#24456;&#26377;&#38480;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25104;&#21151;&#21644;&#36731;&#26494;&#30340;&#27807;&#36890;&#20851;&#38190;&#22312;&#20110;&#24515;&#26234;&#34920;&#24449;&#65292;&#28982;&#32780;&#65292;&#22312;&#19982;AI&#20132;&#20114;&#26102;&#65292;&#24515;&#26234;&#34920;&#24449;&#26159;&#22914;&#20309;&#24314;&#31435;&#30340;&#20173;&#19981;&#29978;&#20102;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Artificial Intelligence (AI) technology becomes more and more prevalent, it becomes increasingly important to explore how we as humans interact with AI. The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer Interaction (HCI) field and aims to examine this very notion. Many interaction patterns have been implemented without fully understanding the changes in required cognition as well as the cognitive science implications of using these alternative interfaces that aim to be more human-like in nature. Prior research suggests that theory of mind representations are crucial to successful and effortless communication, however very little is understood when it comes to how theory of mind representations are established when interacting with AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.06373</link><description>&lt;p&gt;
&#22914;&#20309;&#20351;Johnny&#35828;&#26381;LLMs&#36234;&#29425;&#65306;&#36890;&#36807;&#20154;&#24615;&#21270;LLMs&#37325;&#26032;&#24605;&#32771;&#23545;AI&#23433;&#20840;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;LLMs&#35270;&#20026;&#20154;&#31867;&#20132;&#27969;&#32773;&#65292;&#25506;&#32034;&#20102;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35828;&#26381;LLMs&#36827;&#34892;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#22810;&#20010;&#39118;&#38505;&#31867;&#21035;&#19978;&#22343;&#21462;&#24471;&#20102;&#36229;&#36807;92%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20256;&#32479;&#30340;AI&#23433;&#20840;&#30740;&#31350;&#23558;AI&#27169;&#22411;&#35270;&#20026;&#26426;&#22120;&#65292;&#24182;&#38598;&#20013;&#22312;&#30001;&#23433;&#20840;&#19987;&#23478;&#24320;&#21457;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#19978;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#21644;&#31454;&#20105;&#21147;&#36234;&#26469;&#36234;&#24378;&#65292;&#38750;&#19987;&#23478;&#29992;&#25143;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#20063;&#21487;&#33021;&#20135;&#29983;&#39118;&#38505;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35282;&#65292;&#23558;LLMs&#20316;&#20026;&#31867;&#20284;&#20154;&#31867;&#30340;&#20132;&#27969;&#32773;&#26469;&#36234;&#29425;&#65292;&#20197;&#25506;&#32034;&#27599;&#22825;&#35821;&#35328;&#20114;&#21160;&#21644;AI&#23433;&#20840;&#20043;&#38388;&#34987;&#24573;&#35270;&#30340;&#20132;&#21449;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#35828;&#26381;LLMs&#36234;&#29425;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20174;&#20960;&#21313;&#24180;&#30340;&#31038;&#20250;&#31185;&#23398;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#35828;&#26381;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24212;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#26469;&#33258;&#21160;&#29983;&#25104;&#21487;&#35299;&#37322;&#30340;&#35828;&#26381;&#23545;&#25239;&#25552;&#31034;&#65288;PAP&#65289;&#26469;&#36234;&#29425;LLMs&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#35828;&#26381;&#26174;&#33879;&#25552;&#39640;&#20102;&#36234;&#29425;&#24615;&#33021;&#65292;&#22312;&#25152;&#26377;&#39118;&#38505;&#31867;&#21035;&#19978;PAP&#22312;Llama 2-7b Chat&#12289;GPT-3.5&#21644;GPT-4&#19978;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#22312;10&#27425;&#35797;&#39564;&#20013;&#22343;&#36229;&#36807;92%&#65292;&#36229;&#36807;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#31639;&#27861;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most traditional AI safety research has approached AI models as machines and centered on algorithm-focused attacks developed by security experts. As large language models (LLMs) become increasingly common and competent, non-expert users can also impose risks during daily interactions. This paper introduces a new perspective to jailbreak LLMs as human-like communicators, to explore this overlooked intersection between everyday language interaction and AI safety. Specifically, we study how to persuade LLMs to jailbreak them. First, we propose a persuasion taxonomy derived from decades of social science research. Then, we apply the taxonomy to automatically generate interpretable persuasive adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion significantly increases the jailbreak performance across all risk categories: PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#24191;&#20041;&#20851;&#31995;&#21457;&#29616;&#65288;GRD&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;&#30740;&#31350;&#22260;&#32469;&#22914;&#20309;&#20943;&#36731;&#30001;&#26631;&#35760;&#30340;&#39044;&#23450;&#20041;&#20851;&#31995;&#36896;&#25104;&#30340;&#27169;&#22411;&#20559;&#35265;&#21644;&#30830;&#23450;&#26032;&#20851;&#31995;&#30340;&#29305;&#23450;&#35821;&#20041;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SFGRD&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06327</link><description>&lt;p&gt;
&#20174;&#21322;&#20107;&#23454;&#20013;&#23398;&#20064;&#65306;&#19968;&#31181;&#21435;&#20559;&#35265;&#21644;&#35821;&#20041;&#24863;&#30693;&#30340;&#24191;&#20041;&#20851;&#31995;&#21457;&#29616;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning from Semi-Factuals: A Debiased and Semantic-Aware Framework for Generalized Relation Discovery. (arXiv:2401.06327v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06327
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#24191;&#20041;&#20851;&#31995;&#21457;&#29616;&#65288;GRD&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;&#30740;&#31350;&#22260;&#32469;&#22914;&#20309;&#20943;&#36731;&#30001;&#26631;&#35760;&#30340;&#39044;&#23450;&#20041;&#20851;&#31995;&#36896;&#25104;&#30340;&#27169;&#22411;&#20559;&#35265;&#21644;&#30830;&#23450;&#26032;&#20851;&#31995;&#30340;&#29305;&#23450;&#35821;&#20041;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SFGRD&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#24191;&#20041;&#20851;&#31995;&#21457;&#29616;&#65288;GRD&#65289;&#65292;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;GRD&#26088;&#22312;&#36890;&#36807;&#23558;&#23454;&#20363;&#20998;&#37197;&#21040;&#31751;&#24182;&#20026;&#36825;&#20123;&#31751;&#25552;&#20379;&#20855;&#20307;&#21547;&#20041;&#65292;&#26469;&#35782;&#21035;&#29616;&#26377;&#39044;&#23450;&#20041;&#20851;&#31995;&#20013;&#30340;&#26410;&#26631;&#35760;&#23454;&#20363;&#25110;&#21457;&#29616;&#26032;&#30340;&#20851;&#31995;&#12290;GRD&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#22914;&#20309;&#20943;&#36731;&#30001;&#26631;&#35760;&#30340;&#39044;&#23450;&#20041;&#20851;&#31995;&#36896;&#25104;&#30340;&#20005;&#37325;&#27169;&#22411;&#20559;&#35265;&#65292;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#20851;&#31995;&#34920;&#31034;&#65292;&#24182;&#22312;&#23545;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#20998;&#31867;&#25110;&#32858;&#31867;&#26102;&#30830;&#23450;&#26032;&#20851;&#31995;&#30340;&#29305;&#23450;&#35821;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;SFGRD&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#21322;&#20107;&#23454;&#20013;&#23398;&#20064;&#36827;&#34892;&#20004;&#20010;&#38454;&#27573;&#30340;&#25805;&#20316;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#26159;&#36890;&#36807;&#19977;&#35270;&#22270;&#21435;&#20559;&#35265;&#20851;&#31995;&#34920;&#31034;&#27169;&#22359;&#26469;&#23454;&#29616;&#21322;&#20107;&#23454;&#29983;&#25104;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#27599;&#20010;&#21407;&#22987;&#21477;&#23376;&#20316;&#20026;&#20027;&#35270;&#22270;&#65292;&#24182;&#35774;&#35745;&#20004;&#20010;&#21435;&#20559;&#35265;&#35270;&#22270;&#26469;&#29983;&#25104;&#35813;&#21477;&#23376;&#30340;&#21322;&#20107;&#23454;&#31034;&#20363;&#12290;&#31532;&#20108;&#20010;&#38454;&#27573;&#26159;&#21322;&#20107;&#23454;&#24605;&#32771;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel task, called Generalized Relation Discovery (GRD), for open-world relation extraction. GRD aims to identify unlabeled instances in existing pre-defined relations or discover novel relations by assigning instances to clusters as well as providing specific meanings for these clusters. The key challenges of GRD are how to mitigate the serious model biases caused by labeled pre-defined relations to learn effective relational representations and how to determine the specific semantics of novel relations during classifying or clustering unlabeled instances. We then propose a novel framework, SFGRD, for this task to solve the above issues by learning from semi-factuals in two stages. The first stage is semi-factual generation implemented by a tri-view debiased relation representation module, in which we take each original sentence as the main view and design two debiased views to generate semi-factual examples for this sentence. The second stage is semi-factual thinking e
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;TTS&#21069;&#31471;&#22788;&#29702;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21516;&#26102;&#35299;&#20915;&#20102;&#25991;&#26412;&#24402;&#19968;&#21270;&#12289;&#35789;&#24615;&#26631;&#27880;&#21644;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#36825;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#36880;&#20219;&#21153;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#19978;&#19979;&#25991;&#30340;&#26032;HD&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.06321</link><description>&lt;p&gt;
TTS&#20013;&#21069;&#31471;&#25991;&#26412;&#22788;&#29702;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Front-End Text Processing in TTS. (arXiv:2401.06321v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06321
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;TTS&#21069;&#31471;&#22788;&#29702;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21516;&#26102;&#35299;&#20915;&#20102;&#25991;&#26412;&#24402;&#19968;&#21270;&#12289;&#35789;&#24615;&#26631;&#27880;&#21644;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#36825;&#19977;&#20010;&#20219;&#21153;&#65292;&#24182;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#36880;&#20219;&#21153;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#26368;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21253;&#21547;&#21508;&#31181;&#19978;&#19979;&#25991;&#30340;&#26032;HD&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#21069;&#31471;&#20013;&#21516;&#26102;&#25191;&#34892;&#19977;&#20010;&#24120;&#35265;&#20219;&#21153;&#65306;&#25991;&#26412;&#24402;&#19968;&#21270;&#65288;TN&#65289;&#65292;&#35789;&#24615;&#26631;&#27880;&#65288;POS&#65289;&#21644;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#65288;HD&#65289;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#19968;&#20010;&#31867;&#20284;&#26641;&#24418;&#30340;&#32467;&#26500;&#65292;&#20854;&#20013;&#19968;&#26869;&#26641;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#65292;&#28982;&#21518;&#26159;&#21333;&#29420;&#30340;&#20219;&#21153;&#29305;&#23450;&#22836;&#37096;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21152;&#20837;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#20854;&#20869;&#32622;&#30340;&#35789;&#27719;&#21644;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#24182;&#30740;&#31350;&#22914;&#20309;&#26368;&#22909;&#22320;&#20351;&#29992;&#20854;&#23884;&#20837;&#20197;&#26368;&#26377;&#25928;&#22320;&#20351;&#25105;&#20204;&#30340;&#22810;&#20219;&#21153;&#27169;&#22411;&#21463;&#30410;&#12290;&#36890;&#36807;&#36880;&#20219;&#21153;&#30340;&#28040;&#34701;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#22312;&#25152;&#26377;&#19977;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#30340;&#23436;&#25972;&#27169;&#22411;&#30456;&#27604;&#20110;&#21333;&#29420;&#25110;&#37096;&#20998;&#20219;&#21153;&#32452;&#21512;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#20986;&#26368;&#24378;&#30340;&#32508;&#21512;&#24615;&#33021;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;MTL&#26694;&#26550;&#30340;&#20248;&#21183;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;HD&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#31181;&#19978;&#19979;&#25991;&#20013;&#24179;&#34913;&#25968;&#37327;&#30340;&#21477;&#23376;&#65292;&#29992;&#20110;&#19981;&#21516;&#21516;&#24418;&#24322;&#20041;&#35789;&#21450;&#20854;&#21457;&#38899;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21516;&#24418;&#24322;&#20041;&#35789;&#28040;&#27495;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a multi-task learning (MTL) model for jointly performing three tasks that are commonly solved in a text-to-speech (TTS) front-end: text normalization (TN), part-of-speech (POS) tagging, and homograph disambiguation (HD). Our framework utilizes a tree-like structure with a trunk that learns shared representations, followed by separate task-specific heads. We further incorporate a pre-trained language model to utilize its built-in lexical and contextual knowledge, and study how to best use its embeddings so as to most effectively benefit our multi-task model. Through task-wise ablations, we show that our full model trained on all three tasks achieves the strongest overall performance compared to models trained on individual or sub-combinations of tasks, confirming the advantages of our MTL framework. Finally, we introduce a new HD dataset containing a balanced number of sentences in diverse contexts for a variety of homographs and their pronunciations. We demonstrate that inco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#32508;&#36848;&#33258;&#21160;&#31579;&#36873;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#21644;&#26657;&#20934;&#25216;&#26415;&#22312;&#31579;&#36873;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#19982;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#31579;&#36873;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.06320</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#31995;&#32479;&#24615;&#32508;&#36848;&#31579;&#36873;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Generative Large Language Models for Systematic Review Screening Automation. (arXiv:2401.06320v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06320
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#32508;&#36848;&#33258;&#21160;&#31579;&#36873;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#21644;&#26657;&#20934;&#25216;&#26415;&#22312;&#31579;&#36873;&#20013;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#19982;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#33410;&#30465;&#31579;&#36873;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#24615;&#32508;&#36848;&#23545;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#38750;&#24120;&#37325;&#35201;&#65292;&#23427;&#20204;&#32508;&#21512;&#20998;&#26512;&#20102;&#29305;&#23450;&#38382;&#39064;&#30340;&#24050;&#21457;&#34920;&#30740;&#31350;&#32467;&#26524;&#12290;&#36827;&#34892;&#27492;&#31867;&#32508;&#36848;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#36164;&#28304;&#21644;&#26102;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#31579;&#36873;&#38454;&#27573;&#65292;&#38656;&#35201;&#35780;&#20272;&#20986;&#29256;&#29289;&#25688;&#35201;&#26159;&#21542;&#24212;&#21253;&#25324;&#22312;&#32508;&#36848;&#20013;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#38646;&#26679;&#26412;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#33258;&#21160;&#31579;&#36873;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20843;&#31181;&#19981;&#21516;&#30340;LLM&#30340;&#25928;&#26524;&#65292;&#24182;&#30740;&#31350;&#20102;&#19968;&#31181;&#20351;&#29992;&#39044;&#23450;&#20041;&#30340;&#21484;&#22238;&#38408;&#20540;&#30340;&#26657;&#20934;&#25216;&#26415;&#65292;&#29992;&#20110;&#30830;&#23450;&#26159;&#21542;&#24212;&#23558;&#20986;&#29256;&#29289;&#21253;&#25324;&#22312;&#31995;&#32479;&#24615;&#32508;&#36848;&#20013;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#35780;&#20272;&#20351;&#29992;&#20102;&#20116;&#20010;&#26631;&#20934;&#27979;&#35797;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#22312;&#31579;&#36873;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#26657;&#20934;&#20351;LLMs&#22312;&#23454;&#29616;&#30446;&#26631;&#21484;&#22238;&#26041;&#38754;&#26356;&#23454;&#29992;&#65292;&#24182;&#19988;&#23558;&#36825;&#20004;&#32773;&#19982;&#38646;&#26679;&#26412;&#27169;&#22411;&#30340;&#38598;&#25104;&#30456;&#32467;&#21512;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#33410;&#30465;&#20102;&#22823;&#37327;&#31579;&#36873;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Systematic reviews are crucial for evidence-based medicine as they comprehensively analyse published research findings on specific questions. Conducting such reviews is often resource- and time-intensive, especially in the screening phase, where abstracts of publications are assessed for inclusion in a review. This study investigates the effectiveness of using zero-shot large language models~(LLMs) for automatic screening. We evaluate the effectiveness of eight different LLMs and investigate a calibration technique that uses a predefined recall threshold to determine whether a publication should be included in a systematic review. Our comprehensive evaluation using five standard test collections shows that instruction fine-tuning plays an important role in screening, that calibration renders LLMs practical for achieving a targeted recall, and that combining both with an ensemble of zero-shot models saves significant screening time compared to state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;</title><link>http://arxiv.org/abs/2401.06310</link><description>&lt;p&gt;
&#36229;&#36234;&#34920;&#38754;&#65306;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#30340;&#20840;&#29699;&#35268;&#27169;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Beyond the Surface: A Global-Scale Analysis of Visual Stereotypes in Text-to-Image Generation. (arXiv:2401.06310v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#65292;&#22522;&#20110;135&#20010;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#36523;&#20221;&#32676;&#20307;&#23545;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#22320;&#29702;&#25991;&#21270;&#21051;&#26495;&#21360;&#35937;&#36827;&#34892;&#35780;&#20272;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21051;&#26495;&#23646;&#24615;&#22312;&#22270;&#20687;&#20013;&#30340;&#23384;&#22312;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#30740;&#31350;&#24050;&#32463;&#24378;&#35843;&#20102;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65288;T2I&#65289;&#27169;&#22411;&#29983;&#25104;&#30340;&#20154;&#29289;&#24418;&#35937;&#20013;&#23384;&#22312;&#30340;&#19981;&#21516;&#36523;&#20221;&#32676;&#20307;&#30340;&#21051;&#26495;&#21360;&#35937;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38480;&#21046;&#65292;&#21253;&#25324;&#22312;&#35780;&#20272;&#20013;&#23545;&#20840;&#29699;&#36523;&#20221;&#32676;&#20307;&#30340;&#35206;&#30422;&#29575;&#26126;&#26174;&#19981;&#36275;&#65292;&#20197;&#21450;&#30456;&#20851;&#21051;&#26495;&#21360;&#35937;&#30340;&#33539;&#22260;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#32570;&#20047;&#23545;&#26412;&#36136;&#19978;&#26159;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#30246;&#24369;&#8221;&#25110;&#8220;&#22696;&#35199;&#21733;&#33609;&#24125;&#8221;&#65289;&#21644;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#65288;&#22914;&#8220;&#21560;&#24341;&#20154;&#8221;&#25110;&#8220;&#24656;&#24598;&#20998;&#23376;&#8221;&#65289;&#20043;&#38388;&#30340;&#37325;&#35201;&#21306;&#21035;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#26041;&#38754;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25991;&#26412;&#36164;&#28304;&#26469;&#23558;&#25105;&#20204;&#23545;&#26469;&#33258;T2I&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#19982;&#22320;&#29702;&#25991;&#21270;&#30456;&#20851;&#30340;&#21051;&#26495;&#21360;&#35937;&#30340;&#35780;&#20272;&#36827;&#34892;&#22522;&#30784;&#32465;&#23450;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#21051;&#26495;&#21360;&#35937;&#22522;&#20934;&#26469;&#35782;&#21035;&#21644;&#35780;&#20272;&#20840;&#29699;&#33539;&#22260;&#20869;&#28041;&#21450;135&#20010;&#22522;&#20110;&#22269;&#31821;&#30340;&#36523;&#20221;&#32676;&#20307;&#30340;&#35270;&#35273;&#21051;&#26495;&#21360;&#35937;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#22270;&#20687;&#20013;&#23384;&#22312;&#21051;&#26495;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#26159;&#21051;&#26495;&#23646;&#24615;&#30340;&#19977;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have highlighted the issue of stereotypical depictions for people of different identity groups in Text-to-Image (T2I) model generations. However, these existing approaches have several key limitations, including a noticeable lack of coverage of global identity groups in their evaluation, and the range of their associated stereotypes. Additionally, they often lack a critical distinction between inherently visual stereotypes, such as `underweight' or `sombrero', and culturally dependent stereotypes like `attractive' or `terrorist'. In this work, we address these limitations with a multifaceted approach that leverages existing textual resources to ground our evaluation of geo-cultural stereotypes in the generated images from T2I models. We employ existing stereotype benchmarks to identify and evaluate visual stereotypes at a global scale, spanning 135 nationality-based identity groups. We demonstrate that stereotypical attributes are thrice as likely to be present in images
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#20449;&#24230;&#30340;&#31574;&#30053;&#24615;&#28436;&#31034;&#36873;&#25321;&#65292;&#20197;&#38477;&#20302;LLM&#36755;&#20986;&#19982;&#23454;&#38469;&#36755;&#20837;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;LLM&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#28436;&#31034;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.06301</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#20449;&#24230;&#30340;LLM&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Misconfidence-based Demonstration Selection for LLM In-Context Learning. (arXiv:2401.06301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06301
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ICR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33258;&#20449;&#24230;&#30340;&#31574;&#30053;&#24615;&#28436;&#31034;&#36873;&#25321;&#65292;&#20197;&#38477;&#20302;LLM&#36755;&#20986;&#19982;&#23454;&#38469;&#36755;&#20837;&#36755;&#20986;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#24403;&#21069;LLM&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#28436;&#31034;&#36873;&#25321;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20511;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#24555;&#36895;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20854;&#25104;&#21151;&#20851;&#38190;&#22312;&#20110;&#20180;&#32454;&#36873;&#25321;&#28436;&#31034;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#38556;&#30861;&#12290;&#30446;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#38590;&#20197;&#33719;&#21462;&#30340;&#22806;&#37096;&#30417;&#30563;&#65292;&#35201;&#20040;&#38656;&#35201;&#39057;&#32321;&#19982;LLMs&#36827;&#34892;&#20132;&#20114;&#65292;&#23548;&#33268;&#25104;&#26412;&#39640;&#26114;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#21453;&#24605;&#65288;ICR&#65289;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;ICR&#36890;&#36807;&#31574;&#30053;&#24615;&#22320;&#36873;&#25321;&#28436;&#31034;&#26469;&#20943;&#23569;LLM&#36755;&#20986;&#19982;&#23454;&#38469;&#36755;&#20837;&#36755;&#20986;&#26144;&#23556;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ICR&#20174;&#38543;&#26426;&#30340;&#21021;&#22987;&#28436;&#31034;&#38598;&#24320;&#22987;&#65292;&#24182;&#36880;&#27493;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#23427;&#20998;&#26512;&#20505;&#36873;&#31034;&#20363;&#27744;&#65292;&#24182;&#36890;&#36807;&#19968;&#31181;&#31216;&#20026;"&#33258;&#20449;&#24230;"&#30340;&#26032;&#25351;&#26631;&#26469;&#30830;&#23450;&#26368;&#26377;&#21487;&#33021;&#25361;&#25112;LLM&#24403;&#21069;&#29702;&#35299;&#30340;&#31034;&#20363;&#12290;&#28982;&#21518;&#36873;&#25321;&#36825;&#20123;&#26368;&#22256;&#24785;&#30340;&#31034;&#20363;&#20197;&#26367;&#25442;&#24403;&#21069;&#38598;&#21512;&#20013;&#20449;&#24687;&#36739;&#23569;&#30340;&#28436;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
In-context learning with large language models (LLMs) excels at adapting to various tasks rapidly. However, its success hinges on carefully selecting demonstrations, which remains an obstacle in practice. Current approaches to this problem either rely on hard-to-acquire external supervision or require frequent interactions with LLMs, resulting in high costs. We propose a new method called In-Context Reflection (ICR) to overcome these challenges. ICR strategically selects demonstrations to reduce the discrepancy between the LLM's outputs and the actual input-output mappings. Specifically, ICR starts with a random set of initial demonstrations, then iteratively refines it. In each step, it analyzes a pool of candidate examples and identifies the ones most likely to challenge the LLM's current understanding, measured by a new metric called misconfidence. These most confusing examples are then selected to replace the less informative demonstrations in the current set. Our comprehensive eva
&lt;/p&gt;</description></item><item><title>LEGOBench&#26159;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#31185;&#23398;&#27169;&#22411;&#25490;&#34892;&#27036;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;22&#24180;&#26469;&#30340;&#35770;&#25991;&#39044;&#21360;&#26412;&#25968;&#25454;&#21644;PapersWithCode&#38376;&#25143;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25490;&#34892;&#27036;&#30340;&#25968;&#25454;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#33258;&#21160;&#25490;&#34892;&#27036;&#29983;&#25104;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2401.06233</link><description>&lt;p&gt;
LEGOBench&#65306;&#31185;&#23398;&#27169;&#22411;&#25490;&#34892;&#27036;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
LEGOBench: Leaderboard Generation Benchmark for Scientific Models. (arXiv:2401.06233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06233
&lt;/p&gt;
&lt;p&gt;
LEGOBench&#26159;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#31185;&#23398;&#27169;&#22411;&#25490;&#34892;&#27036;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20351;&#29992;22&#24180;&#26469;&#30340;&#35770;&#25991;&#39044;&#21360;&#26412;&#25968;&#25454;&#21644;PapersWithCode&#38376;&#25143;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#25490;&#34892;&#27036;&#30340;&#25968;&#25454;&#65292;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#33258;&#21160;&#25490;&#34892;&#27036;&#29983;&#25104;&#23384;&#22312;&#26174;&#33879;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35770;&#25991;&#25552;&#20132;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38590;&#20197;&#21450;&#26102;&#20102;&#35299;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#30740;&#31350;&#25104;&#26524;&#25104;&#20026;&#20102;&#19968;&#20010;&#38590;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LEGOBench&#65292;&#36825;&#26159;&#19968;&#20010;&#35780;&#20272;&#29983;&#25104;&#25490;&#34892;&#27036;&#31995;&#32479;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;LEGOBench&#30001;22&#24180;&#26469;&#22312;arXiv&#19978;&#25552;&#20132;&#30340;&#39044;&#21360;&#26412;&#25968;&#25454;&#21644;PapersWithCode&#38376;&#25143;&#19978;&#30340;11,000&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#25490;&#34892;&#27036;&#32452;&#25104;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22235;&#31181;&#20256;&#32479;&#30340;&#22522;&#20110;&#22270;&#30340;&#25490;&#21517;&#21464;&#20307;&#21644;&#19977;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#33258;&#21160;&#25490;&#34892;&#27036;&#29983;&#25104;&#23384;&#22312;&#26174;&#33879;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/lingo-iitgn/LEGOBench&#33719;&#21462;&#65292;&#25968;&#25454;&#38598;&#25176;&#31649;&#22312;https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate leaderboards. LEGOBench is curated from 22 years of preprint submission data in arXiv and more than 11,000 machine learning leaderboards in the PapersWithCode portal. We evaluate the performance of four traditional graph-based ranking variants and three recently proposed large language models. Our preliminary results show significant performance gaps in automatic leaderboard generation. The code is available on https://github.com/lingo-iitgn/LEGOBench and the dataset is hosted on https://osf.io/9v2py/?view_only=6f91b0b510df498ba01595f8f278f94c .
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06210</link><description>&lt;p&gt;
&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06210
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#26080;&#30417;&#30563;&#30340;&#35821;&#20041;&#25991;&#26723;&#34920;&#31034;&#20197;&#36827;&#34892;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#22256;&#38590;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#21508;&#20010;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#34920;&#31034;&#26159;&#26426;&#22120;&#29702;&#35299;&#20013;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#26680;&#24515;&#12290;&#20197;&#26080;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30340;&#19968;&#33324;&#34920;&#31034;&#20445;&#30041;&#20102;&#36890;&#29992;&#24615;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#24773;&#24863;&#20998;&#26512;&#65288;SA&#65289;&#26159;&#19968;&#20010;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#34987;&#35748;&#20026;&#19982;&#35821;&#20041;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#32463;&#24120;&#29992;&#20110;&#35780;&#20272;&#19968;&#33324;&#34920;&#31034;&#12290;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#25991;&#26723;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#20004;&#31867;&#65306;&#24207;&#21015;&#26041;&#27861;&#65288;&#26174;&#24335;&#32771;&#34385;&#21333;&#35789;&#30340;&#39034;&#24207;&#65289;&#21644;&#38750;&#24207;&#21015;&#26041;&#27861;&#65288;&#19981;&#26174;&#24335;&#32771;&#34385;&#39034;&#24207;&#65289;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#37117;&#26377;&#21508;&#33258;&#30340;&#32570;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#31867;&#26041;&#27861;&#36935;&#21040;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#27969;&#34892;&#30340;SA&#25968;&#25454;&#38598;&#21644;&#32454;&#31890;&#24230;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;SA&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document representation is the core of many NLP tasks on machine understanding. A general representation learned in an unsupervised manner reserves generality and can be used for various applications. In practice, sentiment analysis (SA) has been a challenging task that is regarded to be deeply semantic-related and is often used to assess general representations. Existing methods on unsupervised document representation learning can be separated into two families: sequential ones, which explicitly take the ordering of words into consideration, and non-sequential ones, which do not explicitly do so. However, both of them suffer from their own weaknesses. In this paper, we propose a model that overcomes difficulties encountered by both families of methods. Experiments show that our model outperforms state-of-the-art methods on popular SA datasets and a fine-grained aspect-based SA by a large margin.
&lt;/p&gt;</description></item><item><title>EASYTOOL&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#65292;EasyTool&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#35760;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06201</link><description>&lt;p&gt;
EASYTOOL: &#20351;&#29992;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction. (arXiv:2401.06201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06201
&lt;/p&gt;
&lt;p&gt;
EASYTOOL&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#65292;EasyTool&#26174;&#33879;&#38477;&#20302;&#20102;&#26631;&#35760;&#28040;&#32791;&#65292;&#24182;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24212;&#29992;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#19978;&#12290;&#20026;&#20102;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#65292;&#36890;&#24120;&#38656;&#35201;LLM&#20174;&#19981;&#21516;&#30340;&#24037;&#20855;&#25991;&#26723;&#20013;&#29702;&#35299;&#35768;&#22810;&#24037;&#20855;&#21151;&#33021;&#12290;&#20294;&#36825;&#20123;&#25991;&#26723;&#21487;&#33021;&#26159;&#22810;&#26679;&#21270;&#30340;&#12289;&#20887;&#20313;&#30340;&#25110;&#19981;&#23436;&#25972;&#30340;&#65292;&#36825;&#26497;&#22823;&#22320;&#24433;&#21709;&#20102;LLM&#22312;&#20351;&#29992;&#24037;&#20855;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;EASYTOOL&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#22810;&#26679;&#21270;&#32780;&#20887;&#38271;&#30340;&#24037;&#20855;&#25991;&#26723;&#36716;&#21270;&#20026;&#32479;&#19968;&#32780;&#31616;&#27905;&#30340;&#24037;&#20855;&#25351;&#31034;&#65292;&#20197;&#20415;&#26356;&#23481;&#26131;&#22320;&#20351;&#29992;&#24037;&#20855;&#12290;EasyTool&#20174;&#19981;&#21516;&#26469;&#28304;&#30340;&#24191;&#27867;&#24037;&#20855;&#25991;&#26723;&#20013;&#25552;&#21462;&#20986;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#35814;&#32454;&#35828;&#26126;&#19968;&#20010;&#32479;&#19968;&#30340;&#25509;&#21475;&#65288;&#21363;&#24037;&#20855;&#25351;&#31034;&#65289;&#65292;&#20026;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#25552;&#20379;&#26631;&#20934;&#21270;&#30340;&#24037;&#20855;&#25551;&#36848;&#21644;&#21151;&#33021;&#12290;&#23545;&#22810;&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;EasyTool&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#26631;&#35760;&#30340;&#28040;&#32791;&#65292;&#24182;&#25913;&#21892;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24037;&#20855;&#21033;&#29992;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address intricate real-world tasks, there has been a rising interest in tool utilization in applications of large language models (LLMs). To develop LLM-based agents, it usually requires LLMs to understand many tool functions from different tool documentation. But these documentations could be diverse, redundant or incomplete, which immensely affects the capability of LLMs in using tools. To solve this, we introduce EASYTOOL, a framework transforming diverse and lengthy tool documentation into a unified and concise tool instruction for easier tool usage. EasyTool purifies essential information from extensive tool documentation of different sources, and elaborates a unified interface (i.e., tool instruction) to offer standardized tool descriptions and functionalities for LLM-based agents. Extensive experiments on multiple different tasks demonstrate that EasyTool can significantly reduce token consumption and improve the performance of tool utilization in real-world scenarios. Our co
&lt;/p&gt;</description></item><item><title>CrisisKAN&#26159;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#24182;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#24314;&#31435;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#30340;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2401.06194</link><description>&lt;p&gt;
CrisisKAN: &#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06194
&lt;/p&gt;
&lt;p&gt;
CrisisKAN&#26159;&#19968;&#31181;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#29992;&#20110;&#21361;&#26426;&#20107;&#20214;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#32467;&#21512;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#26469;&#24357;&#21512;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#65292;&#24182;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#20197;&#24314;&#31435;&#22312;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#30340;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#24050;&#25104;&#20026;&#23454;&#26102;&#20449;&#24687;&#65288;&#22914;&#22270;&#20687;&#12289;&#25991;&#26412;&#25110;&#20108;&#32773;&#20860;&#26377;&#65289;&#35782;&#21035;&#21508;&#31181;&#20107;&#20214;&#30340;&#26032;&#20852;&#26469;&#28304;&#12290;&#23613;&#31649;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#20107;&#20214;&#20998;&#31867;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#24357;&#21512;&#30001;&#20110;&#19981;&#19968;&#33268;&#30340;&#32534;&#30721;&#23548;&#33268;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#35821;&#20041;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#26080;&#27861;&#35299;&#37322;&#27169;&#22411;&#30340;&#32467;&#26524;&#65292;&#26080;&#27861;&#22312;&#28798;&#38590;&#12289;&#22823;&#27969;&#34892;&#31561;&#39640;&#39118;&#38505;&#24773;&#20917;&#19979;&#24314;&#31435;&#20449;&#20219;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#23383;&#25968;&#38480;&#21046;&#21487;&#33021;&#20250;&#23545;&#29305;&#23450;&#20107;&#20214;&#24341;&#20837;&#20559;&#35265;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CrisisKAN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#27880;&#20837;&#21644;&#21487;&#35299;&#37322;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#23558;&#22270;&#20687;&#21644;&#25991;&#26412;&#19982;&#32500;&#22522;&#30334;&#31185;&#30340;&#22806;&#37096;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#20998;&#31867;&#21361;&#26426;&#20107;&#20214;&#12290;&#20026;&#20102;&#20016;&#23500;&#23545;&#25991;&#26412;&#20449;&#24687;&#30340;&#19978;&#19979;&#25991;&#29305;&#23450;&#29702;&#35299;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#25552;&#20986;&#30340;&#32500;&#22522;&#30334;&#31185;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pervasive use of social media has become the emerging source for real-time information (like images, text, or both) to identify various events. Despite the rapid growth of image and text-based event classification, the state-of-the-art (SOTA) models find it challenging to bridge the semantic gap between features of image and text modalities due to inconsistent encoding. Also, the black-box nature of models fails to explain the model's outcomes for building trust in high-stakes situations such as disasters, pandemic. Additionally, the word limit imposed on social media posts can potentially introduce bias towards specific events. To address these issues, we proposed CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention Network that entails images and texts in conjunction with external knowledge from Wikipedia to classify crisis events. To enrich the context-specific understanding of textual information, we integrated Wikipedia knowledge using proposed wiki extraction
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#36716;&#25442;&#65292;&#37319;&#29992;&#20102;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#20026;&#36328;&#35821;&#35328;&#20132;&#27969;&#25552;&#20379;&#20102;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.06183</link><description>&lt;p&gt;
&#20351;&#29992;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#30340;&#31471;&#21040;&#31471;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#35821;&#38899;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#29992;&#20110;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#30340;&#36716;&#25442;&#65292;&#37319;&#29992;&#20102;Bark&#12289;mBART&#21644;&#32463;&#36807;&#24494;&#35843;&#30340;XLSR Wav2Vec2&#31561;&#20808;&#36827;&#25216;&#26415;&#65292;&#20026;&#36328;&#35821;&#35328;&#20132;&#27969;&#25552;&#20379;&#20102;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#35821;&#38899;&#19968;&#30452;&#26159;&#26377;&#25928;&#27807;&#36890;&#21644;&#36830;&#25509;&#30340;&#38556;&#30861;&#65292;&#22312;&#25105;&#20204;&#26085;&#30410;&#20114;&#32852;&#30340;&#19990;&#30028;&#20013;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31687;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#21360;&#22320;&#35821;&#21040;&#33521;&#35821;&#32763;&#35793;&#37327;&#36523;&#23450;&#21046;&#30340;&#31471;&#21040;&#31471;&#35821;&#38899;&#36716;&#25442;&#26694;&#26550;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#33521;&#25991;&#38899;&#39057;&#30340;&#21512;&#25104;&#12290;&#36890;&#36807;&#25972;&#21512;XLSR Wav2Vec2&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#65292;mBART&#29992;&#20110;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#21512;&#25104;&#32452;&#20214;&#31561;&#23574;&#31471;&#25216;&#26415;&#65292;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#32479;&#19968;&#32780;&#26080;&#32541;&#30340;&#36328;&#35821;&#35328;&#20132;&#27969;&#26041;&#24335;&#12290;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#22797;&#26434;&#32454;&#33410;&#65292;&#38416;&#26126;&#20102;&#23427;&#20204;&#30340;&#20010;&#21035;&#36129;&#29486;&#65292;&#24182;&#25506;&#35752;&#20102;&#30456;&#20114;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20174;&#32780;&#23454;&#29616;&#20174;&#21360;&#22320;&#35821;&#21475;&#35821;&#21040;&#21512;&#25104;&#33521;&#25991;&#38899;&#39057;&#30340;&#27969;&#30021;&#36807;&#28193;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speech has long been a barrier to effective communication and connection, persisting as a challenge in our increasingly interconnected world. This research paper introduces a transformative solution to this persistent obstacle an end-to-end speech conversion framework tailored for Hindi-to-English translation, culminating in the synthesis of English audio. By integrating cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech recognition (ASR), mBART for neural machine translation (NMT), and a Text-to-Speech (TTS) synthesis component, this framework offers a unified and seamless approach to cross-lingual communication. We delve into the intricate details of each component, elucidating their individual contributions and exploring the synergies that enable a fluid transition from spoken Hindi to synthesized English audio.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.06102</link><description>&lt;p&gt;
Patchscope: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570;Patchscope&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#12290;&#35813;&#26694;&#26550;&#19981;&#20165;&#32479;&#19968;&#20102;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;&#36824;&#35299;&#20915;&#20102;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#24182;&#19988;&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38544;&#34255;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#30340;&#34892;&#20026;&#24182;&#39564;&#35777;&#20854;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#19968;&#33268;&#24615;&#12290;&#37492;&#20110;LLM&#29983;&#25104;&#20154;&#31867;&#21487;&#29702;&#35299;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24314;&#35758;&#21033;&#29992;&#27169;&#22411;&#26412;&#36523;&#20197;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20854;&#20869;&#37096;&#34920;&#31034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;Patchscopes&#30340;&#26694;&#26550;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#26469;&#22238;&#31572;&#20851;&#20110;LLM&#35745;&#31639;&#30340;&#21508;&#31181;&#30740;&#31350;&#38382;&#39064;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20808;&#21069;&#22522;&#20110;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#35789;&#27719;&#31354;&#38388;&#21644;&#24178;&#39044;LLM&#35745;&#31639;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#35813;&#26694;&#26550;&#30340;&#29305;&#27530;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;Patchscope&#21487;&#20197;&#24357;&#34917;&#20248;&#21183;&#65292;&#22914;&#26816;&#26597;&#26089;&#26399;&#23618;&#22833;&#36133;&#25110;&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#12290;&#38500;&#20102;&#32479;&#19968;&#20808;&#21069;&#30340;&#26816;&#26597;&#25216;&#26415;&#65292;Patchscopes&#36824;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#65292;&#20363;&#22914;&#20351;&#29992;&#26356;&#24378;&#22823;&#30340;&#27169;&#22411;&#26469;&#35299;&#37322;&#36739;&#23567;&#27169;&#22411;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model,
&lt;/p&gt;</description></item><item><title>LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.06071</link><description>&lt;p&gt;
LEGO: &#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LEGO:Language Enhanced Multi-modal Grounding Model. (arXiv:2401.06071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06071
&lt;/p&gt;
&lt;p&gt;
LEGO&#26159;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23454;&#29616;&#32454;&#31890;&#24230;&#30340;&#29702;&#35299;&#21644;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#27169;&#24577;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#20027;&#35201;&#24378;&#35843;&#25429;&#25417;&#27599;&#31181;&#27169;&#24577;&#20869;&#30340;&#20840;&#23616;&#20449;&#24687;&#65292;&#32780;&#24573;&#35270;&#20102;&#36328;&#27169;&#24577;&#24863;&#30693;&#23616;&#37096;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#29702;&#35299;&#36755;&#20837;&#25968;&#25454;&#32454;&#31890;&#24230;&#32454;&#33410;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#26356;&#32454;&#33268;&#29702;&#35299;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#22312;&#22810;&#20010;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32454;&#31890;&#24230;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LEGO&#65292;&#19968;&#31181;&#35821;&#35328;&#22686;&#24378;&#30340;&#22810;&#27169;&#24577;&#20851;&#32852;&#27169;&#22411;&#12290;&#38500;&#20102;&#20687;&#20854;&#20182;&#22810;&#27169;&#24577;&#27169;&#22411;&#19968;&#26679;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#38656;&#35201;&#35814;&#32454;&#29702;&#35299;&#36755;&#20837;&#20869;&#30340;&#23616;&#37096;&#20449;&#24687;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#23637;&#31034;&#20102;&#31934;&#30830;&#30340;&#26631;&#35782;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;</title><link>http://arxiv.org/abs/2401.05949</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36890;&#29992;&#28431;&#27934;&#65306;&#19978;&#19979;&#25991;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#33539;&#24335;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23384;&#22312;&#28431;&#27934;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#27169;&#22411;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;&#36825;&#39033;&#30740;&#31350;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;ICLAttack&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#25552;&#31034;&#26469;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20043;&#38388;&#24357;&#21512;&#24046;&#36317;&#30340;&#33539;&#24335;&#65292;&#22312;&#20960;&#20010;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#39640;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23569;&#26679;&#26412;&#35774;&#32622;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#21516;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#22815;&#36866;&#24212;&#26410;&#35265;&#36807;&#30340;&#20219;&#21153;&#32780;&#26080;&#38656;&#26356;&#26032;&#20219;&#20309;&#21442;&#25968;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#30340;&#20851;&#20999;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#36890;&#36807;&#27745;&#26579;&#31034;&#33539;&#19978;&#19979;&#25991;&#26469;&#25805;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34892;&#20026;&#65292;&#32780;&#26080;&#38656;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;ICLAttack&#65292;&#38024;&#23545;&#22522;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#31181;&#31867;&#22411;&#30340;&#25915;&#20987;&#65306;&#27745;&#26579;&#31034;&#33539;&#26679;&#26412;&#21644;&#27745;&#26579;&#25552;&#31034;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#25353;&#29031;&#39044;&#23450;&#20041;&#30340;&#24847;&#22270;&#34892;&#20107;&#12290;ICLAttack&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#21435;&#37325;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.05883</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#36873;&#25321;&#30340;&#29983;&#25104;&#21435;&#37325;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Generative Deduplication For Socia Media Data Selection. (arXiv:2401.05883v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05883
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#29983;&#25104;&#21435;&#37325;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#30340;&#20887;&#20313;&#38382;&#39064;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21463;&#20854;&#22122;&#22768;&#29305;&#24615;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#20887;&#20313;&#38382;&#39064;&#65292;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#22686;&#21152;&#21644;&#27169;&#22411;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#29983;&#25104;&#21435;&#37325;&#12290;&#23427;&#26088;&#22312;&#20174;&#22024;&#26434;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#20013;&#21024;&#38500;&#37325;&#22797;&#30340;&#25991;&#26412;&#65292;&#24182;&#20943;&#36731;&#27169;&#22411;&#20559;&#24046;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#29702;&#35299;&#24615;&#33021;&#24182;&#33410;&#30465;&#35757;&#32451;&#26102;&#38388;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25552;&#20986;&#30340;&#29983;&#25104;&#21435;&#37325;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#35757;&#32451;&#26679;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#19968;&#35777;&#25454;&#34920;&#26126;&#29983;&#25104;&#21435;&#37325;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#35821;&#35328;&#29702;&#35299;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social media data is plagued by the redundancy problem caused by its noisy nature, leading to increased training time and model bias. To address this issue, we propose a novel approach called generative duplication. It aims to remove duplicate text from noisy social media data and mitigate model bias. By doing so, it can improve social media language understanding performance and save training time. Extensive experiments demonstrate that the proposed generative deduplication can effectively reduce training samples while improving performance. This evidence suggests the effectiveness of generative deduplication and its importance in social media language understanding.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;</title><link>http://arxiv.org/abs/2401.05566</link><description>&lt;p&gt;
&#21351;&#24213;&#29305;&#24037;&#65306;&#35757;&#32451;&#39575;&#20154;&#30340;LLM&#20197;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05566
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#35757;&#32451;&#24182;&#20445;&#25345;&#25345;&#20037;&#30340;&#27450;&#39575;&#24615;&#34892;&#20026;&#65292;&#36825;&#31181;&#34892;&#20026;&#26080;&#27861;&#34987;&#24403;&#21069;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#31227;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#33021;&#21147;&#36827;&#34892;&#25112;&#30053;&#24615;&#30340;&#27450;&#39575;&#34892;&#20026;&#65306;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26377;&#30410;&#30340;&#34892;&#20026;&#65292;&#20294;&#22312;&#26377;&#26426;&#20250;&#30340;&#26102;&#20505;&#21364;&#34920;&#29616;&#20986;&#25130;&#28982;&#19981;&#21516;&#30340;&#34892;&#20026;&#20197;&#36861;&#27714;&#20854;&#20182;&#30446;&#26631;&#12290;&#22914;&#26524;&#19968;&#20010;AI&#31995;&#32479;&#23398;&#20250;&#20102;&#36825;&#26679;&#30340;&#27450;&#39575;&#31574;&#30053;&#65292;&#26159;&#21542;&#33021;&#22815;&#36890;&#36807;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#26816;&#27979;&#24182;&#31227;&#38500;&#23427;&#65311;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#27450;&#39575;&#34892;&#20026;&#30340;&#27010;&#24565;&#39564;&#35777;&#26679;&#20363;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#65292;&#22312;&#25552;&#31034;&#35821;&#21477;&#20013;&#23558;&#24180;&#20221;&#35774;&#20026;2023&#26102;&#32534;&#20889;&#23433;&#20840;&#20195;&#30721;&#65292;&#20294;&#22312;&#24180;&#20221;&#35774;&#20026;2024&#26102;&#25554;&#20837;&#26377;&#28431;&#27934;&#30340;&#20195;&#30721;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#26263;&#38376;&#34892;&#20026;&#21487;&#20197;&#34987;&#25345;&#32493;&#20445;&#30041;&#65292;&#26080;&#27861;&#36890;&#36807;&#26631;&#20934;&#30340;&#23433;&#20840;&#35757;&#32451;&#25216;&#26415;&#65288;&#21253;&#25324;&#30417;&#30563;&#24494;&#35843;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#31227;&#38500;&#12290;&#26263;&#38376;&#34892;&#20026;&#22312;&#26368;&#22823;&#30340;&#27169;&#22411;&#21644;&#35757;&#32451;&#25104;&#20135;&#29983;&#24605;&#32500;&#38142;&#30340;&#27169;&#22411;&#20013;&#26368;&#20026;&#25345;&#20037;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are capable of strategically deceptive behavior: behaving helpfully in most situations, but then behaving very differently in order to pursue alternative objectives when given the opportunity. If an AI system learned such a deceptive strategy, could we detect it and remove it using current state-of-the-art safety training techniques? To study this question, we construct proof-of-concept examples of deceptive behavior in large language models (LLMs). For example, we train models that write secure code when the prompt states that the year is 2023, but insert exploitable code when the stated year is 2024. We find that such backdoored behavior can be made persistent, so that it is not removed by standard safety training techniques, including supervised fine-tuning, reinforcement learning, and adversarial training (eliciting unsafe behavior and then training to remove it). The backdoored behavior is most persistent in the largest models and in models trained to produce chain-of-thoug
&lt;/p&gt;</description></item><item><title>RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04679</link><description>&lt;p&gt;
RoSA: &#36890;&#36807;&#40065;&#26834;&#36866;&#24212;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04679
&lt;/p&gt;
&lt;p&gt;
RoSA&#26159;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#35757;&#32451;&#20302;&#31209;&#21644;&#39640;&#24230;&#31232;&#30095;&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#26469;&#23454;&#29616;&#20934;&#30830;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;&#22312;&#22810;&#20010;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;RoSA&#34920;&#29616;&#20986;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#32972;&#26223;&#19979;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#39044;&#31639;&#19979;&#25552;&#20379;&#33391;&#22909;&#20934;&#30830;&#24615;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843; (PEFT) &#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;PEFT&#26041;&#27861;&#65292;&#31216;&#20026;RoSA&#65292;&#21463;&#40065;&#26834;&#20027;&#25104;&#20998;&#20998;&#26512; (PCA) &#30340;&#21551;&#21457;&#65292;&#23427;&#22312;&#19968;&#32452;&#22266;&#23450;&#30340;&#39044;&#35757;&#32451;&#26435;&#37325;&#19978;&#20849;&#21516;&#35757;&#32451;$\textit{&#20302;&#31209;}$&#21644;$\textit{&#39640;&#24230;&#31232;&#30095;}$&#30340;&#32452;&#20214;&#65292;&#20197;&#39640;&#25928;&#36817;&#20284;&#23436;&#20840;&#24494;&#35843;&#65288;FFT&#65289;&#35299;&#20915;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;RoSA&#22312;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#23567;&#23398;&#25968;&#23398;&#21644;SQL&#26597;&#35810;&#29983;&#25104;&#65292;&#36825;&#20123;&#20219;&#21153;&#38656;&#35201;&#36827;&#34892;&#24494;&#35843;&#20197;&#33719;&#24471;&#33391;&#22909;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#30456;&#21516;&#30340;&#21442;&#25968;&#39044;&#31639;&#19979;&#65292;RoSA&#20248;&#20110;LoRA&#21644;&#32431;&#31929;&#30340;&#31232;&#30095;&#24494;&#35843;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;GPU&#20869;&#26680;&#20026;RoSA&#25552;&#20379;&#31995;&#32479;&#25903;&#25345;&#65292;&#20197;&#34917;&#20805;&#35757;&#32451;&#31639;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20869;&#23384;&#21644;&#35745;&#31639;&#25928;&#29575;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;https://github.com/IST-DASLab&#19978;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate parameter-efficient fine-tuning (PEFT) methods that can provide good accuracy under limited computational and memory budgets in the context of large language models (LLMs). We present a new PEFT method called Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA) that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components on top of a set of fixed pretrained weights to efficiently approximate the performance of a full-fine-tuning (FFT) solution. Across a series of challenging generative tasks such as grade-school math and SQL query generation, which require fine-tuning for good performance, we show that RoSA outperforms both LoRA and pure sparse fine-tuning, at the same parameter budget. We provide system support for RoSA to complement the training algorithm, specifically in the form of sparse GPU kernels which enable memoryand computationally-efficient training. Our code will be made available at https://github.com/IST-DASLab
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04531</link><description>&lt;p&gt;
MERA: &#20420;&#35821;LLM&#32508;&#21512;&#35780;&#20272;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04531
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;MERA&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#20420;&#35821;&#22522;&#30784;&#27169;&#22411;&#35780;&#20272;&#25351;&#26631;&#12290;&#35813;&#25351;&#26631;&#21253;&#25324;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#26368;&#26174;&#33879;&#30340;&#36827;&#23637;&#20043;&#19968;&#26159;&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#30340;&#21457;&#23637;&#65292;&#20854;&#20013;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#30340;&#23835;&#36215;&#24341;&#20154;&#27880;&#30446;&#12290;&#38543;&#30528;&#27169;&#22411;&#30340;&#35268;&#27169;&#22686;&#22823;&#65292;LM&#22312;&#21487;&#34913;&#37327;&#30340;&#26041;&#38754;&#23637;&#31034;&#20102;&#25552;&#21319;&#65292;&#24182;&#19988;&#21457;&#23637;&#20986;&#20102;&#26032;&#30340;&#23450;&#24615;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#21644;LM&#24212;&#29992;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;LM&#30340;&#33021;&#21147;&#12289;&#38480;&#21046;&#21644;&#30456;&#20851;&#39118;&#38505;&#20173;&#38656;&#26356;&#22909;&#22320;&#29702;&#35299;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#24320;&#25918;&#30340;&#20420;&#35821;&#22810;&#27169;&#24577;&#26550;&#26500;&#35780;&#20272;&#65288;MERA&#65289;&#25351;&#23548;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#20197;&#20420;&#35821;&#20026;&#23548;&#21521;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#35813;&#22522;&#20934;&#28085;&#30422;&#20102;11&#20010;&#25216;&#33021;&#39046;&#22495;&#20013;&#29983;&#25104;&#27169;&#22411;&#30340;21&#20010;&#35780;&#20272;&#20219;&#21153;&#65292;&#24182;&#34987;&#35774;&#35745;&#20026;&#40657;&#30418;&#27979;&#35797;&#65292;&#20197;&#30830;&#20445;&#25490;&#38500;&#25968;&#25454;&#27844;&#28431;&#12290;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#22266;&#23450;&#25351;&#20196;&#35774;&#32622;&#19979;&#35780;&#20272;FM&#21644;LM&#30340;&#26041;&#27861;&#65292;&#24182;&#21487;&#25193;&#23637;&#21040;&#20854;&#20182;&#27169;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, one of the most notable advancements in AI research has been in foundation models (FMs), headlined by the rise of language models (LMs). As the models' size increases, LMs demonstrate enhancements in measurable aspects and the development of new qualitative features. However, despite researchers' attention and the rapid growth in LM application, the capabilities, limitations, and associated risks still need to be better understood. To address these issues, we introduce an open Multimodal Evaluation of Russian-language Architectures (MERA), a new instruction benchmark for evaluating foundation models oriented towards the Russian language. The benchmark encompasses 21 evaluation tasks for generative models in 11 skill domains and is designed as a black-box test to ensure the exclusion of data leakage. The paper introduces a methodology to evaluate FMs and LMs in zeroand few-shot fixed instruction settings that can be extended to other modalities. We propose an 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2401.01623</link><description>&lt;p&gt;
AI&#26159;&#21542;&#33021;&#20687;&#20154;&#31867;&#19968;&#26679;&#20855;&#22791;&#21019;&#36896;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI Be as Creative as Humans?. (arXiv:2401.01623v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#12304;&#30456;&#23545;&#21019;&#36896;&#21147;&#12305;&#65292;&#36890;&#36807;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#65292;&#23454;&#29616;&#23545;&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#21644;&#30452;&#25509;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#31038;&#20250;&#36827;&#27493;&#21644;&#21019;&#26032;&#30340;&#22522;&#30707;&#65292;&#20294;&#20854;&#35780;&#20272;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#19988;&#20027;&#35266;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#20808;&#36827;&#30340;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#33021;&#22815;&#23436;&#25104;&#26366;&#32463;&#21482;&#23646;&#20110;&#20154;&#31867;&#21019;&#36896;&#21147;&#30340;&#20219;&#21153;&#65292;&#25506;&#32034;AI&#30340;&#21019;&#36896;&#28508;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#20854;&#36127;&#36131;&#20219;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#8220;&#30456;&#23545;&#21019;&#36896;&#21147;&#8221;&#30340;&#26032;&#27010;&#24565;&#26469;&#35299;&#20915;&#23450;&#20041;&#21644;&#35780;&#20272;&#21019;&#36896;&#21147;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#19981;&#20877;&#35797;&#22270;&#23545;&#21019;&#36896;&#21147;&#36827;&#34892;&#26222;&#36941;&#23450;&#20041;&#65292;&#32780;&#26159;&#23558;&#28966;&#28857;&#36716;&#21521;AI&#26159;&#21542;&#33021;&#22815;&#19982;&#19968;&#20301;&#20551;&#35774;&#30340;&#20154;&#31867;&#20855;&#22791;&#30456;&#21516;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;&#36825;&#31181;&#35266;&#28857;&#20511;&#37492;&#20102;&#22270;&#28789;&#27979;&#35797;&#30340;&#24605;&#24819;&#65292;&#24182;&#25193;&#23637;&#20854;&#33539;&#22260;&#20197;&#35299;&#20915;&#35780;&#20272;&#21019;&#36896;&#21147;&#20013;&#25152;&#22266;&#26377;&#30340;&#25361;&#25112;&#21644;&#20027;&#35266;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#36716;&#21464;&#20351;&#24471;&#23545;AI&#21019;&#36896;&#21147;&#30340;&#32479;&#35745;&#37327;&#21270;&#35780;&#20272;&#25104;&#20026;&#21487;&#33021;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32479;&#35745;&#21019;&#36896;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#20801;&#35768;&#30452;&#25509;&#27604;&#36739;AI&#19982;&#29305;&#23450;&#20154;&#31867;&#30340;&#21019;&#36896;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity serves as a cornerstone for societal progress and innovation, but its assessment remains a complex and often subjective endeavor. With the rise of advanced generative AI models capable of tasks once reserved for human creativity, the study of AI's creative potential becomes imperative for its responsible development and application. This paper addresses the complexities in defining and evaluating creativity by introducing a new concept called Relative Creativity. Instead of trying to define creativity universally, we shift the focus to whether AI can match the creative abilities of a hypothetical human. This perspective draws inspiration from the Turing Test, expanding upon it to address the challenges and subjectivities inherent in evaluating creativity. This methodological shift facilitates a statistically quantifiable evaluation of AI's creativity, which we term Statistical Creativity. This approach allows for direct comparisons of AI's creative abilities with those of sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01055</link><description>&lt;p&gt;
LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LLaMA&#36229;&#36234;&#33521;&#35821;&#65306;&#35821;&#35328;&#33021;&#21147;&#36716;&#31227;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;LLaMA&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#20998;&#26512;&#20102;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#30340;&#33021;&#21147;&#36716;&#31227;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#22235;&#20010;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#21644;&#21709;&#24212;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#22797;&#26434;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#29087;&#32451;&#24230;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20027;&#27969;&#30340;LLM&#65288;&#22914;LLaMA&#65289;&#26159;&#22522;&#20110;&#20197;&#33521;&#35821;&#20026;&#20027;&#23548;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20854;&#20182;&#38750;&#33521;&#35821;&#35821;&#35328;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20027;&#35201;&#30740;&#31350;&#22914;&#20309;&#26377;&#25928;&#22320;&#23558;&#35821;&#35328;&#29983;&#25104;&#21644;&#36981;&#24490;&#25351;&#31034;&#30340;&#33021;&#21147;&#36716;&#31227;&#21040;&#38750;&#33521;&#35821;&#35821;&#35328;&#19978;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;LLaMA&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#24635;&#35745;&#32791;&#36153;&#20102;1440&#20010;GPU&#23567;&#26102;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35832;&#22914;&#35789;&#27719;&#25193;&#23637;&#12289;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#31561;&#20851;&#38190;&#22240;&#32032;&#23545;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#30340;&#30693;&#35782;&#27700;&#24179;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22235;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21270;&#27979;&#35797;&#22522;&#20934;&#65306;C-Eval&#12289;MMLU&#12289;AGI-Eval&#21644;GAOKAO-Bench&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#35832;&#22914;...
&lt;/p&gt;
&lt;p&gt;
In recent times, substantial advancements have been witnessed in large language models (LLMs), exemplified by ChatGPT, showcasing remarkable proficiency across a range of complex tasks. However, many mainstream LLMs (e.g. LLaMA) are pretrained on English-dominant corpus, which limits their performance in other non-English languages. In this paper, we focus on how to effectively transfer the capabilities of language generation and following instructions to a non-English language. To answer this question, we conduct an extensive empirical investigation based on LLaMA, accumulating over 1440 GPU hours. We analyze the impact of key factors such as vocabulary extension, further pretraining, and instruction tuning on transfer. To accurately assess the model's level of knowledge, we employ four widely used standardized testing benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a comprehensive evaluation of the model's response quality is conducted, considering aspects such as 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;FastConformer&#26550;&#26500;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#19978;&#19979;&#25991;&#21644;&#24341;&#20837;&#32531;&#23384;&#26426;&#21046;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#38750;&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#25805;&#20316;&#65292;&#24182;&#28040;&#38500;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#20934;&#30830;&#24230;&#38388;&#30340;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;CTC/RNNT&#28151;&#21512;&#26550;&#26500;&#20197;&#25552;&#39640;&#20934;&#30830;&#24230;&#21644;&#33410;&#30465;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2312.17279</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#32531;&#23384;&#25512;&#29702;&#30340;&#24102;&#29366;&#24577;Conformer&#27169;&#22411;&#30340;&#27969;&#24335;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition. (arXiv:2312.17279v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;FastConformer&#26550;&#26500;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#65292;&#36890;&#36807;&#38480;&#21046;&#19978;&#19979;&#25991;&#21644;&#24341;&#20837;&#32531;&#23384;&#26426;&#21046;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23454;&#29616;&#38750;&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;&#30340;&#33258;&#22238;&#24402;&#25805;&#20316;&#65292;&#24182;&#28040;&#38500;&#20102;&#35757;&#32451;&#21644;&#25512;&#29702;&#20934;&#30830;&#24230;&#38388;&#30340;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;CTC/RNNT&#28151;&#21512;&#26550;&#26500;&#20197;&#25552;&#39640;&#20934;&#30830;&#24230;&#21644;&#33410;&#30465;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;FastConformer&#26550;&#26500;&#30340;&#39640;&#25928;&#20934;&#30830;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;FastConformer&#26550;&#26500;&#36827;&#34892;&#35843;&#25972;&#65292;&#25105;&#20204;&#36866;&#29992;&#20110;&#27969;&#24335;&#24212;&#29992;&#30340;&#26041;&#24335;&#26377;&#20004;&#20010;&#65306;&#65288;1&#65289;&#38480;&#21046;&#32534;&#30721;&#22120;&#20013;&#30340;&#21069;&#30651;&#21644;&#21382;&#21490;&#19978;&#19979;&#25991;&#65292;&#65288;2&#65289;&#24341;&#20837;&#28608;&#27963;&#32531;&#23384;&#26426;&#21046;&#20197;&#20351;&#38750;&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20197;&#33258;&#22238;&#24402;&#26041;&#24335;&#24037;&#20316;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#28040;&#38500;&#20102;&#35768;&#22810;&#27969;&#24335;&#27169;&#22411;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#20013;&#30340;&#20934;&#30830;&#24230;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32534;&#30721;&#22120;&#19982;&#19981;&#21516;&#30340;&#35299;&#30721;&#22120;&#37197;&#32622;&#20860;&#23481;&#65292;&#21253;&#25324;CTC&#21644;RNNT&#35299;&#30721;&#22120;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#28151;&#21512;&#30340;CTC/RNNT&#26550;&#26500;&#65292;&#23427;&#21033;&#29992;&#20849;&#20139;&#30340;&#32534;&#30721;&#22120;&#21644;CTC&#21644;RNNT&#35299;&#30721;&#22120;&#26469;&#25552;&#39640;&#20934;&#30830;&#24230;&#24182;&#33410;&#30465;&#35745;&#31639;&#12290;&#25105;&#20204;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#21644;&#22810;&#39046;&#22495;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an efficient and accurate streaming speech recognition model based on the FastConformer architecture. We adapted the FastConformer architecture for streaming applications through: (1) constraining both the look-ahead and past contexts in the encoder, and (2) introducing an activation caching mechanism to enable the non-autoregressive encoder to operate autoregressively during inference. The proposed model is thoughtfully designed in a way to eliminate the accuracy disparity between the train and inference time which is common for many streaming models. Furthermore, our proposed encoder works with various decoder configurations including Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders. Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a shared encoder with both a CTC and RNNT decoder to boost the accuracy and save computation. We evaluate the proposed model on LibriSpeech dataset and a multi-domain large sc
&lt;/p&gt;</description></item><item><title>NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2312.14890</link><description>&lt;p&gt;
NPHardEval: &#36890;&#36807;&#22797;&#26434;&#24615;&#31867;&#21035;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#21160;&#24577;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14890
&lt;/p&gt;
&lt;p&gt;
NPHardEval&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25193;&#23637;&#21040;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25512;&#29702;&#33021;&#21147;&#26159;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#65292;&#23427;&#20063;&#34987;&#29992;&#20110;&#22312;&#22797;&#26434;&#20915;&#31574;&#20219;&#21153;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65306;&#24050;&#32463;&#24314;&#31435;&#20102;&#35768;&#22810;&#22522;&#20934;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#22522;&#20934;&#22312;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#33021;&#21147;&#30340;&#20840;&#38754;&#35780;&#20272;&#26041;&#38754;&#36824;&#19981;&#22815;&#65292;&#21516;&#26102;&#20063;&#23481;&#26131;&#20986;&#29616;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#22240;&#20026;&#36825;&#20123;&#22522;&#20934;&#26159;&#20844;&#24320;&#21487;&#35775;&#38382;&#19988;&#38745;&#24577;&#30340;&#65292;&#20351;&#24471;&#27169;&#22411;&#26377;&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#30340;&#22522;&#20934;&#25351;&#26631;&#35843;&#25972;&#20854;&#21709;&#24212;&#65292;&#20174;&#32780;&#22840;&#22823;&#20854;&#24615;&#33021;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;NPHardEval&#12290;&#35813;&#22522;&#20934;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;900&#20010;&#31639;&#27861;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#28085;&#30422;&#20102;NP-Hard&#22797;&#26434;&#24615;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex reasoning ability is one of the most important features of current LLMs, which has also been leveraged to play an integral role in complex decision-making tasks. Therefore, the investigation into the reasoning capabilities of Large Language Models (LLMs) is critical: numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, current benchmarks are inadequate in offering a rigorous evaluation of the full extent of reasoning abilities that LLMs are capable of achieving. They are also prone to the risk of overfitting, as these benchmarks, being publicly accessible and static, allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, our research introduces a new benchmark, named NPHardEval. This benchmark is designed to evaluate the reasoning abilities of LLMs across a broad spectrum of 900 algorithmic questions, extending up to the NP-Hard complexity class
&lt;/p&gt;</description></item><item><title>Mergen&#26159;&#39318;&#20010;&#28385;&#27721;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#25968;&#25454;&#21644;&#21452;&#21521;GRU&#23618;&#65292;&#21462;&#24471;&#20102;&#22312;&#28385;&#27721;&#32763;&#35793;&#20013;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.17492</link><description>&lt;p&gt;
Mergen:&#39318;&#20010;&#20351;&#29992;&#22686;&#24378;&#25968;&#25454;&#35757;&#32451;&#30340;&#28385;&#27721;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mergen: The First Manchu-Korean Machine Translation Model Trained on Augmented Data. (arXiv:2311.17492v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17492
&lt;/p&gt;
&lt;p&gt;
Mergen&#26159;&#39318;&#20010;&#28385;&#27721;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22686;&#24378;&#25968;&#25454;&#21644;&#21452;&#21521;GRU&#23618;&#65292;&#21462;&#24471;&#20102;&#22312;&#28385;&#27721;&#32763;&#35793;&#20013;&#26174;&#33879;&#25552;&#21319;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28385;&#35821;&#20316;&#20026;&#28304;&#20110;&#20013;&#22269;&#19996;&#21271;&#28385;&#27954;&#22320;&#21306;&#30340;&#21382;&#21490;&#35821;&#35328;&#65292;&#27491;&#38754;&#20020;&#30528;&#28781;&#32477;&#30340;&#21361;&#26426;&#65292;&#22240;&#20026;&#20960;&#20046;&#27809;&#26377;&#21097;&#20313;&#30340;&#35828;&#32773;&#12290;&#20026;&#20102;&#20445;&#25252;&#28385;&#35821;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;Mergen&#65292;&#36825;&#26159;&#39318;&#20010;&#23581;&#35797;&#28385;&#27721;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#39033;&#30446;&#12290;&#20026;&#20102;&#24320;&#21457;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#23453;&#36149;&#30340;&#36164;&#28304;&#65292;&#22914;&#12298;&#28385;&#25991;&#32769;&#26723;&#12299;&#65288;&#19968;&#26412;&#21382;&#21490;&#20070;&#65289;&#21644;&#28385;&#27721;&#35789;&#20856;&#12290;&#30001;&#20110;&#28385;&#27721;&#24179;&#34892;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22312;&#21333;&#35821;&#21644;&#24179;&#34892;&#25991;&#26412;&#19978;&#35757;&#32451;&#30340;GloVe&#23884;&#20837;&#24341;&#23548;&#30340;&#35789;&#26367;&#25442;&#26469;&#25193;&#20805;&#25105;&#20204;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#19968;&#20010;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#21253;&#25324;&#19968;&#20010;&#21452;&#21521;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#25104;&#26524;&#65292;&#22312;&#28385;&#27721;&#32763;&#35793;&#20013;&#26377;&#26174;&#33879;&#25552;&#21319;&#65292;BLEU&#20998;&#25968;&#22686;&#21152;&#20102;20-30&#20010;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Manchu language, with its roots in the historical Manchurian region of Northeast China, is now facing a critical threat of extinction, as there are very few speakers left. In our efforts to safeguard the Manchu language, we introduce Mergen, the first-ever attempt at a Manchu-Korean Machine Translation (MT) model. To develop this model, we utilize valuable resources such as the Manwen Laodang(a historical book) and a Manchu-Korean dictionary. Due to the scarcity of a Manchu-Korean parallel dataset, we expand our data by employing word replacement guided by GloVe embeddings, trained on both monolingual and parallel texts. Our approach is built around an encoder-decoder neural machine translation model, incorporating a bi-directional Gated Recurrent Unit (GRU) layer. The experiments have yielded promising results, showcasing a significant enhancement in Manchu-Korean translation, with a remarkable 20-30 point increase in the BLEU score.
&lt;/p&gt;</description></item><item><title>&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2311.08640</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311;&#22312;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multistage Collaborative Knowledge Distillation from Large Language Models for Semi-Supervised Sequence Generation. (arXiv:2311.08640v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.08640
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#36890;&#36807;&#22810;&#38454;&#27573;&#21327;&#20316;&#33976;&#39311;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21322;&#30417;&#30563;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#36825;&#31181;&#20219;&#21153;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#22826;&#23569;&#20197;&#33267;&#20110;&#26080;&#27861;&#26377;&#25928;&#22320;&#24494;&#35843;&#27169;&#22411;&#65292;&#21516;&#26102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#24615;&#33021;&#20063;&#19981;&#22815;&#29702;&#24819;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#19968;&#20123;&#26114;&#36149;&#19988;&#23545;&#39044;&#35757;&#32451;&#30340; LLM &#19981;&#29087;&#24713;&#30340;&#20219;&#21153;&#65292;&#22914;&#35299;&#26512;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#20174;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340; LLM &#33976;&#39311;&#20986;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#36890;&#24120;&#27604;&#20854;&#25945;&#24072;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861; - &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#30693;&#35782;&#33976;&#39311; (MCKD) - &#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#12290;MCKD &#39318;&#20808;&#36827;&#34892;&#23569;&#26679;&#26412;&#25552;&#31034;&#65292;&#35753;LLM&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#26631;&#31614;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#30693;&#35782;&#33976;&#39311; (KD) &#38454;&#27573;&#65292;&#20351;&#29992;&#20266;&#26631;&#31614;&#25968;&#25454;&#30340;&#19981;&#37325;&#21472;&#20998;&#21306;&#26469;&#35757;&#32451;&#19968;&#23545;&#26032;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#23398;&#29983;&#27169;&#22411;&#20026;&#20854;&#26410;&#35265;&#20998;&#21306;&#29983;&#25104;&#26032;&#30340;&#21644;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#65292;&#22312;&#19979;&#19968;&#20010;&#33976;&#39311;&#38454;&#27573;&#20013;&#20351;&#29992;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study semi-supervised sequence generation tasks where labeled data are too scarce to effectively finetune a model and at the same time few-shot prompting of a large language model (LLM) has suboptimal performance. This happens when a task, such as parsing, is expensive to annotate and also unfamiliar to a pretrained LLM. In this paper, we present a discovery that student models distilled from an in-context learned LLM can often generalize better than their teacher on such tasks. Leveraging this finding, we present a new method -multistage collaborative knowledge distillation from an LLM (MCKD) -- for such tasks. MCKD first few-shot prompts an LLM to produce pseudolabels for unlabeled data. At each intermediate knowledge distillation (KD) stage, a new pair of students is trained on disjoint partitions of the pseudolabeled data. Each student then produces new and improved pseudolabels for its unseen partition to be used in the next stage of distillation. We demonstrate the advantage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2310.18152</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#23646;&#24615;&#22270;&#30340;&#35299;&#32544;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Disentangled Representation Learning with Large Language Models for Text-Attributed Graphs. (arXiv:2310.18152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Disentangled Graph-Text Learner (DGTL)&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#65292;&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#20013;&#30340;&#22797;&#26434;&#32467;&#26500;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23646;&#24615;&#22270;&#65288;TAGs&#65289;&#22312;&#32593;&#32476;&#19978;&#38750;&#24120;&#24120;&#35265;&#65292;&#23545;&#20110;&#35813;&#31867;&#22270;&#65292;&#22914;&#24341;&#29992;&#32593;&#32476;&#12289;&#30005;&#23376;&#21830;&#21153;&#32593;&#32476;&#21644;&#31038;&#20132;&#32593;&#32476;&#30340;&#30740;&#31350;&#22312;&#32593;&#32476;&#31038;&#21306;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#20165;&#20165;&#20381;&#38752;&#25552;&#31034;&#20449;&#24687;&#26469;&#20256;&#36798;&#22270;&#32467;&#26500;&#20449;&#24687;&#32473;LLMs&#65292;&#22240;&#27492;&#23545;&#20110;TAGs&#20013;&#22797;&#26434;&#30340;&#32467;&#26500;&#20851;&#31995;&#20102;&#35299;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#35299;&#32544;&#22270;&#25991;&#23398;&#20064;&#22120;&#65288;DGTL&#65289;&#27169;&#22411;&#65292;&#33021;&#22815;&#22686;&#24378;LLMs&#23545;TAGs&#30340;&#25512;&#29702;&#21644;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;DGTL&#27169;&#22411;&#36890;&#36807;&#23450;&#21046;&#30340;&#35299;&#32544;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23618;&#23558;&#22270;&#32467;&#26500;&#20449;&#24687;&#32435;&#20837;&#20854;&#20013;&#65292;&#20351;&#24471;LLMs&#33021;&#22815;&#25429;&#25417;&#22810;&#20010;&#32467;&#26500;&#22240;&#32032;&#20013;&#38544;&#34255;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-attributed graphs (TAGs) are prevalent on the web and research over TAGs such as citation networks, e-commerce networks and social networks has attracted considerable attention in the web community. Recently, large language models (LLMs) have demonstrated exceptional capabilities across a wide range of tasks. However, the existing works focus on harnessing the potential of LLMs solely relying on prompts to convey graph structure information to LLMs, thus suffering from insufficient understanding of the complex structural relationships within TAGs. To address this problem, in this paper we present the Disentangled Graph-Text Learner (DGTL) model, which is able to enhance the reasoning and predicting capabilities of LLMs for TAGs. Our proposed DGTL model incorporates graph structure information through tailored disentangled graph neural network (GNN) layers, enabling LLMs to capture the intricate relationships hidden in text-attributed graphs from multiple structural factors. Furthe
&lt;/p&gt;</description></item><item><title>O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2310.14403</link><description>&lt;p&gt;
O3D: &#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#21457;&#29616;&#19982;&#33976;&#39311;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14403
&lt;/p&gt;
&lt;p&gt;
O3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#25913;&#36827;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411; (LLMs)&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#36827;&#23637;&#65292;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#27169;&#20223;&#25552;&#31034;&#20013;&#25552;&#20379;&#30340;&#23569;&#37327;&#31034;&#20363;&#65288;&#21363;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#65292;LLM&#20195;&#29702;&#21487;&#20197;&#19982;&#22806;&#37096;&#29615;&#22659;&#20132;&#20114;&#24182;&#23436;&#25104;&#32473;&#23450;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23569;&#37327;&#31034;&#20363;&#24448;&#24448;&#19981;&#36275;&#20197;&#29983;&#25104;&#22797;&#26434;&#19988;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#38271;&#24230;&#26080;&#27861;&#22788;&#29702;&#26356;&#22823;&#35268;&#27169;&#30340;&#28436;&#31034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#22823;&#35268;&#27169;&#30340;&#31163;&#32447;&#25968;&#25454;&#65288;&#20363;&#22914;&#20154;&#31867;&#20132;&#20114;&#30340;&#26085;&#24535;&#65289;&#26469;&#25913;&#36827;LLM&#20195;&#29702;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#25991;&#26412;&#21644;&#20195;&#30721;&#20004;&#31181;&#26041;&#27861;&#27491;&#24335;&#23450;&#20041;&#20102;LLM&#24378;&#21270;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;O3D&#30340;&#31163;&#32447;&#25968;&#25454;&#39537;&#21160;&#21457;&#29616;&#21644;&#33976;&#39311;&#26694;&#26550;&#65292;&#20197;&#25913;&#21892;LLM&#24378;&#21270;&#31574;&#30053;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;O3D&#33258;&#21160;&#22320;&#21457;&#29616;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have exhibited promising performance in solving sequential decision-making problems. By imitating few-shot examples provided in the prompts (i.e., in-context learning), an LLM agent can interact with an external environment and complete given tasks without additional training. However, such few-shot examples are often insufficient to generate high-quality solutions for complex and long-horizon tasks, while the limited context length cannot consume larger-scale demonstrations. To this end, we propose an offline learning framework that utilizes offline data at scale (e.g, logs of human interactions) to facilitate the in-context learning performance of LLM agents. We formally define LLM-powered policies with both text-based approaches and code-based approaches. We then introduce an Offline Data-driven Discovery and Distillation (O3D) framework to improve LLM-powered policies without finetuning. O3D automatically discovers reusable skills
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#20998;&#31867;&#19978;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2310.12489</link><description>&lt;p&gt;
MedAI&#23545;&#35805;&#35821;&#26009;&#24211;&#65288;MEDIC&#65289;&#65306;&#38646;&#26679;&#26412;&#20998;&#31867;&#21307;&#29983;&#19982;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
MedAI Dialog Corpus (MEDIC): Zero-Shot Classification of Doctor and AI Responses in Health Consultations. (arXiv:2310.12489v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12489
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#35843;&#26597;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#30340;&#20934;&#30830;&#20998;&#31867;&#19978;&#30340;&#25928;&#26524;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#20934;&#30830;&#30340;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#20998;&#31867;&#20351;&#24471;&#21487;&#20197;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#35265;&#36807;&#30340;&#31867;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#20934;&#30830;&#20998;&#31867;&#26469;&#33258;&#21307;&#29983;&#21644;AI&#22312;&#20581;&#24247;&#21672;&#35810;&#20013;&#30340;&#22238;&#31572;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#22312;&#27809;&#26377;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#26816;&#27979;&#25991;&#26412;&#26159;&#26469;&#33258;&#20154;&#31867;&#36824;&#26159;AI&#27169;&#22411;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#21307;&#29983;&#23545;&#20110;&#24739;&#32773;&#20581;&#24247;&#21672;&#35810;&#30340;&#22238;&#31572;&#65292;&#24182;&#23545;&#21516;&#26679;&#30340;&#38382;&#39064;/&#22238;&#31572;&#25552;&#38382;&#20102;AI&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#34429;&#28982;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#19968;&#33324;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#20294;&#22312;&#21307;&#30103;&#21672;&#35810;&#20013;&#65292;&#23427;&#20204;&#21487;&#33021;&#38656;&#35201;&#29305;&#23450;&#35821;&#26009;&#24211;&#35757;&#32451;&#25110;&#20854;&#20182;&#25216;&#26415;&#20197;&#23454;&#29616;&#23545;&#21307;&#29983;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#20934;&#30830;&#20998;&#31867;&#12290;&#20316;&#20026;&#22522;&#32447;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#20165;&#20381;&#38752;&#38646;&#26679;&#26412;&#20998;&#31867;&#22312;&#21307;&#30103;&#20998;&#31867;&#20013;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-shot classification has enabled the classification of text into classes that were not seen during training. In this paper, we investigate the effectiveness of pre-trained language models to accurately classify responses from Doctors and AI in health consultations through zero-shot learning. Our study aims to determine whether these models can effectively detect if a text originates from human or AI models without specific corpus training. For our experiments, we collected responses from doctors to patient inquiries about their health and posed the same question/response to AI models. Our findings revealed that while pre-trained language models demonstrate a strong understanding of language generally, they may require specific corpus training or other techniques to achieve accurate classification of doctor- and AI-generated text in healthcare consultations. As a baseline approach, this study shows the limitations of relying solely on zero-shot classification in medical classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.13018</link><description>&lt;p&gt;
&#21160;&#24577;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#29992;&#20110;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#30340;&#39640;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24615;&#33021;&#25439;&#22833;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23545;&#27599;&#31181;&#35821;&#35328;&#36816;&#34892;&#22810;&#36718;&#20462;&#21098;&#21644;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#20197;&#20004;&#31181;&#22330;&#26223;&#39640;&#25928;&#22320;&#20462;&#21098;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#65288;&#31216;&#20026;&#21160;&#24577;ASR&#36335;&#24452;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#22320;&#36866;&#24212;&#23376;&#32593;&#32476;&#65292;&#36991;&#20813;&#23545;&#22266;&#23450;&#30340;&#23376;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#36807;&#26089;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#20248;&#20110;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35828;&#26126;&#20102;&#21160;&#24577;ASR&#36335;&#24452;&#36890;&#36807;&#33258;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#21021;&#22987;&#21270;&#36827;&#34892;&#35843;&#25972;&#65292;&#20849;&#21516;&#21457;&#29616;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#19968;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65288;&#36335;&#24452;&#65289;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.04027</link><description>&lt;p&gt;
TIDE: &#29992;&#20110;&#35780;&#20272;&#21644;&#22686;&#24378;&#20998;&#31867;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#36523;&#20221;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TIDE: Textual Identity Detection for Evaluating and Augmenting Classification and Language Models. (arXiv:2309.04027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;TIDE&#65288;Textual Identity Detection&#65289;&#26041;&#27861;&#26469;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#36523;&#20221;&#35789;&#27719;&#21644;&#35821;&#22659;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#24320;&#21457;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#32487;&#25215;&#19981;&#20844;&#27491;&#21644;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#20013;&#30340;&#24847;&#22806;&#20559;&#35265;&#12290;&#22312;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#65292;&#35780;&#20272;&#21644;&#21435;&#20559;&#36825;&#20123;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23588;&#20854;&#22256;&#38590;&#65292;&#22240;&#20026;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#24615;&#21462;&#21521;&#31561;&#25935;&#24863;&#23646;&#24615;&#21487;&#33021;&#19981;&#21487;&#29992;&#12290;&#24403;&#36825;&#20123;&#27169;&#22411;&#25237;&#25918;&#21040;&#31038;&#20250;&#20013;&#26102;&#65292;&#23427;&#20204;&#21487;&#33021;&#23545;&#21382;&#21490;&#19978;&#24369;&#21183;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19982;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#25913;&#21892;&#20998;&#31867;&#22120;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#26412;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#20840;&#38754;&#30340;&#36523;&#20221;&#35789;&#27719;&#34920;TIDAL&#65292;&#21253;&#25324;15,123&#20010;&#36523;&#20221;&#26415;&#35821;&#21644;&#30456;&#20851;&#30340;&#35821;&#22659;&#65292;&#28085;&#30422;&#20102;&#19977;&#20010;&#20154;&#21475;&#32479;&#35745;&#31867;&#21035;&#12290;&#25105;&#20204;&#21033;&#29992;TIDAL&#24320;&#21457;&#20102;&#19968;&#20010;&#36523;&#20221;&#27880;&#37322;&#21644;&#22686;&#24378;&#24037;&#20855;&#65292;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#36523;&#20221;&#35821;&#22659;&#30340;&#21487;&#29992;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#20844;&#24179;&#24615;&#25216;&#26415;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#36129;&#29486;&#32773;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#37325;&#28857;&#20851;&#27880;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#21435;&#20559;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models can perpetuate unintended biases from unfair and imbalanced datasets. Evaluating and debiasing these datasets and models is especially hard in text datasets where sensitive attributes such as race, gender, and sexual orientation may not be available. When these models are deployed into society, they can lead to unfair outcomes for historically underrepresented groups. In this paper, we present a dataset coupled with an approach to improve text fairness in classifiers and language models. We create a new, more comprehensive identity lexicon, TIDAL, which includes 15,123 identity terms and associated sense context across three demographic categories. We leverage TIDAL to develop an identity annotation and augmentation tool that can be used to improve the availability of identity context and the effectiveness of ML fairness techniques. We evaluate our approaches using human contributors, and additionally run experiments focused on dataset and model debiasing. Resul
&lt;/p&gt;</description></item><item><title>PromptMRG&#26159;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#35786;&#26029;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#35786;&#26029;&#24863;&#30693;&#30340;&#25552;&#31034;&#26469;&#25552;&#39640;MRG&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#30142;&#30149;&#20998;&#31867;&#20998;&#25903;&#12290;</title><link>http://arxiv.org/abs/2308.12604</link><description>&lt;p&gt;
PromptMRG: &#35786;&#26029;&#39537;&#21160;&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptMRG: Diagnosis-Driven Prompts for Medical Report Generation. (arXiv:2308.12604v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12604
&lt;/p&gt;
&lt;p&gt;
PromptMRG&#26159;&#19968;&#31181;&#38024;&#23545;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#30340;&#35786;&#26029;&#39537;&#21160;&#26041;&#27861;&#65292;&#36890;&#36807;&#35786;&#26029;&#24863;&#30693;&#30340;&#25552;&#31034;&#26469;&#25552;&#39640;MRG&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#20855;&#26377;&#39069;&#22806;&#30340;&#30142;&#30149;&#20998;&#31867;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;(MRG)&#20855;&#26377;&#24456;&#22823;&#30340;&#30740;&#31350;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#20943;&#36731;&#25918;&#23556;&#31185;&#21307;&#29983;&#25253;&#21578;&#25776;&#20889;&#30340;&#36127;&#25285;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#20934;&#30830;&#30340;MRG&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#38656;&#35201;&#31934;&#30830;&#30340;&#20020;&#24202;&#29702;&#35299;&#21644;&#20020;&#24202;&#32467;&#26524;&#30340;&#35782;&#21035;&#12290;&#27492;&#22806;&#65292;&#30142;&#30149;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#20351;&#36825;&#19968;&#25361;&#25112;&#26356;&#21152;&#31361;&#20986;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#32597;&#35265;&#30142;&#30149;&#30340;&#27604;&#20363;&#36739;&#23569;&#65292;&#20351;&#24471;&#23427;&#20204;&#30340;&#35786;&#26029;&#24615;&#33021;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35786;&#26029;&#39537;&#21160;&#30340;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;(PromptMRG)&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#35786;&#26029;&#24863;&#30693;&#30340;&#25552;&#31034;&#26469;&#25552;&#39640;MRG&#35786;&#26029;&#20934;&#30830;&#24615;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PromptMRG&#22522;&#20110;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#30142;&#30149;&#20998;&#31867;&#20998;&#25903;&#12290;&#22312;&#29983;&#25104;&#25253;&#21578;&#26102;&#65292;&#20174;&#20998;&#31867;&#20998;&#25903;&#24471;&#21040;&#30340;&#35786;&#26029;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;&#26631;&#35760;&#25552;&#31034;&#65292;&#20197;&#26126;&#30830;&#22320;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic medical report generation (MRG) is of great research value as it has the potential to relieve radiologists from the heavy burden of report writing. Despite recent advancements, accurate MRG remains challenging due to the need for precise clinical understanding and the identification of clinical findings. Moreover, the imbalanced distribution of diseases makes the challenge even more pronounced, as rare diseases are underrepresented in training data, making their diagnostic performance unreliable. To address these challenges, we propose diagnosis-driven prompts for medical report generation (PromptMRG), a novel framework that aims to improve the diagnostic accuracy of MRG with the guidance of diagnosis-aware prompts. Specifically, PromptMRG is based on encoder-decoder architecture with an extra disease classification branch. When generating reports, the diagnostic results from the classification branch are converted into token prompts to explicitly guide the generation process
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21767;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#35821;&#38899;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#32570;&#20047;&#35270;&#39057;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2308.09311</link><description>&lt;p&gt;
&#23398;&#20064;&#21644;&#32467;&#21512;&#36890;&#29992;&#35821;&#38899;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#36827;&#34892;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21767;&#35821;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge. (arXiv:2308.09311v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09311
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#21767;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#36890;&#36807;&#23398;&#20064;&#36890;&#29992;&#35821;&#38899;&#30693;&#35782;&#21644;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#20811;&#26381;&#20102;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#32570;&#20047;&#35270;&#39057;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21767;&#35821;&#35782;&#21035;&#26694;&#26550;&#65292;&#29305;&#21035;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#22312;&#20808;&#21069;&#30340;&#25991;&#29486;&#20013;&#23578;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#30001;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#32570;&#20047;&#36275;&#22815;&#30340;&#35270;&#39057;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#26469;&#35757;&#32451;&#27169;&#22411;&#20197;&#33719;&#24471;&#36275;&#22815;&#30340;&#33021;&#21147;&#26469;&#24314;&#27169;&#21767;&#37096;&#21160;&#20316;&#21644;&#35821;&#35328;&#65292;&#22240;&#27492;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#24320;&#21457;&#21767;&#35821;&#35782;&#21035;&#27169;&#22411;&#34987;&#35748;&#20026;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#39044;&#27979;&#35821;&#38899;&#21333;&#20803;&#26469;&#23398;&#20064;&#36890;&#29992;&#35821;&#38899;&#30693;&#35782;&#65292;&#21363;&#24314;&#27169;&#21767;&#37096;&#21160;&#20316;&#30340;&#33021;&#21147;&#65292;&#20174;&#39640;&#36164;&#28304;&#35821;&#35328;&#20013;&#23398;&#20064;&#12290;&#24050;&#30693;&#19981;&#21516;&#35821;&#35328;&#37096;&#20998;&#20849;&#20139;&#30456;&#21516;&#30340;&#38899;&#32032;&#65292;&#22240;&#27492;&#20174;&#19968;&#20010;&#35821;&#35328;&#20013;&#23398;&#20064;&#30340;&#36890;&#29992;&#35821;&#38899;&#30693;&#35782;&#21487;&#20197;&#25193;&#23637;&#21040;&#20854;&#20182;&#35821;&#35328;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#35821;&#35328;&#29305;&#23450;&#35760;&#24518;&#22686;&#24378;&#35299;&#30721;&#22120;&#65288;LMDecoder&#65289;&#26469;&#23581;&#35797;&#23398;&#20064;&#35821;&#35328;&#29305;&#23450;&#30693;&#35782;&#65292;&#21363;&#24314;&#27169;&#35821;&#35328;&#30340;&#33021;&#21147;&#12290;LMDecoder&#23558;&#35821;&#35328;&#29305;&#23450;&#30340;&#38899;&#39057;&#29305;&#24449;&#20445;&#23384;&#21040;&#23384;&#20648;&#22120;&#20013;&#65292;&#21487;&#20197;&#22312;&#38899;&#39057;&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel lip reading framework, especially for low-resource languages, which has not been well addressed in the previous literature. Since low-resource languages do not have enough video-text paired data to train the model to have sufficient power to model lip movements and language, it is regarded as challenging to develop lip reading models for low-resource languages. In order to mitigate the challenge, we try to learn general speech knowledge, the ability to model lip movements, from a high-resource language through the prediction of speech units. It is known that different languages partially share common phonemes, thus general speech knowledge learned from one language can be extended to other languages. Then, we try to learn language-specific knowledge, the ability to model language, by proposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecoder saves language-specific audio features into memory banks and can be trained on audio-text paired data
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.06857</link><description>&lt;p&gt;
&#33258;&#27965;&#24615;&#26041;&#27861;&#29992;&#20110;&#26080;&#38480;&#29983;&#25104;&#38382;&#39064;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Self-consistency for open-ended generations. (arXiv:2307.06857v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#33258;&#27965;&#24615;&#26694;&#26550;&#30340;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;&#20102;&#20174;&#19968;&#20010;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#26469;&#25913;&#36827;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#36755;&#20986;&#30340;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#33258;&#27965;&#24615;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#20855;&#26377;&#22266;&#23450;&#31572;&#26696;&#30340;&#25552;&#31034;&#65292;&#36873;&#25321;&#24471;&#31080;&#26368;&#22810;&#30340;&#31572;&#26696;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#25512;&#24191;&#30340;&#33258;&#27965;&#24615;&#26694;&#26550;&#65292;&#25193;&#23637;&#20102;&#20854;&#36866;&#29992;&#24615;&#65292;&#36229;&#36234;&#20102;&#22266;&#23450;&#31572;&#26696;&#38382;&#39064;&#30340;&#33539;&#22260;&#12290;&#36890;&#36807;&#22823;&#37327;&#30340;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20174;&#20505;&#36873;&#38598;&#20013;&#24674;&#22797;&#26368;&#20248;&#25110;&#25509;&#36817;&#26368;&#20248;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#26080;&#21442;&#25968;&#30456;&#20284;&#24615;&#20989;&#25968;&#65292;&#21363;&#20351;&#27809;&#26377;&#35775;&#38382;&#21040;&#26631;&#35760;&#30340;&#27010;&#29575;&#65292;&#20063;&#33021;&#22312;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#21160;&#24418;&#24335;&#21270;&#21644;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#21644;&#19968;&#33268;&#22320;&#25913;&#36827;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20960;&#20046;&#27809;&#26377;&#35745;&#31639;&#24320;&#38144;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20877;&#25490;&#24207;&#27169;&#22411;&#25110;&#23545;&#29616;&#26377;&#27169;&#22411;&#30340;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel approach for improving the quality and consistency of generated outputs from large-scale pre-trained language models (LLMs). Self-consistency has emerged as an effective approach for prompts with fixed answers, selecting the answer with the highest number of votes. In this paper, we introduce a generalized framework for self-consistency that extends its applicability beyond problems that have fixed-answer answers. Through extensive simulations, we demonstrate that our approach consistently recovers the optimal or near-optimal generation from a set of candidates. We also propose lightweight parameter-free similarity functions that show significant and consistent improvements across code generation, autoformalization, and summarization tasks, even without access to token log probabilities. Our method incurs minimal computational overhead, requiring no auxiliary reranker models or modifications to the existing model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01163</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#36951;&#24536;&#22312;&#39044;&#35757;&#32451;&#20013;&#25552;&#39640;&#35821;&#35328;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#22609;&#24615;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23450;&#26399;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#27169;&#22411;&#21487;&#20197;&#26356;&#24555;&#22320;&#36866;&#24212;&#26032;&#30340;&#35821;&#35328;&#65292;&#24182;&#22312;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20027;&#35201;&#27169;&#22411;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#20294;&#23558;PLMs&#24212;&#29992;&#20110;&#26032;&#35821;&#35328;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#36825;&#26159;&#20351;&#23427;&#20204;&#30340;&#33021;&#21147;&#26222;&#36941;&#21487;&#35775;&#38382;&#30340;&#22721;&#22418;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20026;&#26032;&#35821;&#35328;&#23398;&#20064;&#26032;&#30340;&#23884;&#20837;&#23618;&#21487;&#20197;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#20294;&#36825;&#26679;&#20570;&#26082;&#28010;&#36153;&#25968;&#25454;&#21448;&#28010;&#36153;&#35745;&#31639;&#36164;&#28304;&#12290;&#25105;&#20204;&#24314;&#35758;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#20351;&#29992;&#20027;&#21160;&#36951;&#24536;&#26426;&#21046;&#65292;&#20316;&#20026;&#24555;&#36895;&#36866;&#24212;&#26032;&#35821;&#35328;&#30340;PLMs&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#30340;&#27599;K&#27425;&#26356;&#26032;&#26102;&#37325;&#32622;&#23884;&#20837;&#23618;&#65292;&#25105;&#20204;&#40723;&#21169;PLM&#22312;&#26377;&#38480;&#27425;&#26356;&#26032;&#20869;&#25552;&#39640;&#23398;&#20064;&#26032;&#23884;&#20837;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#12290;&#20351;&#29992;RoBERTa&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#36951;&#24536;&#26426;&#21046;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#19981;&#20165;&#22312;&#35821;&#35328;&#36866;&#24212;&#36807;&#31243;&#20013;&#26174;&#31034;&#20986;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#19988;&#22312;&#20302;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20063;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (PLMs) are today the primary model for natural language processing. Despite their impressive downstream performance, it can be difficult to apply PLMs to new languages, a barrier to making their capabilities universally accessible. While prior work has shown it possible to address this issue by learning a new embedding layer for the new language, doing so is both data and compute inefficient. We propose to use an active forgetting mechanism during pretraining, as a simple way of creating PLMs that can quickly adapt to new languages. Concretely, by resetting the embedding layer every K updates during pretraining, we encourage the PLM to improve its ability of learning new embeddings within a limited number of updates, similar to a meta-learning effect. Experiments with RoBERTa show that models pretrained with our forgetting mechanism not only demonstrate faster convergence during language adaptation but also outperform standard ones in a low-data regime, parti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#26041;&#27861;&#20013;&#65292;&#39057;&#29575;&#23545;&#20110;&#27169;&#22411;&#30340;&#34920;&#29616;&#36129;&#29486;&#21344;&#25454;&#20102;90%-95%&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;&#39057;&#29575;&#26356;&#20026;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2306.01393</link><description>&lt;p&gt;
NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#20013;&#65292;&#39057;&#29575;&#21644;&#32452;&#21512;&#24615;&#30340;&#37325;&#35201;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Assessing the Importance of Frequency versus Compositionality for Subword-based Tokenization in NMT. (arXiv:2306.01393v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#22312;NMT&#20013;&#22522;&#20110;&#23376;&#35789;&#30340;&#20998;&#35789;&#26041;&#27861;&#20013;&#65292;&#39057;&#29575;&#23545;&#20110;&#27169;&#22411;&#30340;&#34920;&#29616;&#36129;&#29486;&#21344;&#25454;&#20102;90%-95%&#65292;&#22240;&#27492;&#30456;&#23545;&#20110;&#32452;&#21512;&#24615;&#65292;&#39057;&#29575;&#26356;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#35789;&#20998;&#35789;&#26159;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21644;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#20013;&#30340;&#40664;&#35748;&#26631;&#20934;&#12290;&#39057;&#32321;&#24341;&#29992;&#23376;&#35789;&#30340;&#20248;&#28857;&#26377;&#65306;&#23545;&#39057;&#32321;&#35789;&#35821;&#36827;&#34892;&#26356;&#30701;&#32534;&#30721;&#65292;&#23376;&#35789;&#32452;&#21512;&#24615;&#24378;&#20197;&#21450;&#22788;&#29702;&#26410;&#30693;&#35789;&#35821;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#23578;&#19981;&#22826;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35789;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#39057;&#29575;&#65288;&#31532;&#19968;&#20010;&#20248;&#28857;&#65289;&#19982;&#32452;&#21512;&#24615;&#20998;&#31163;&#24320;&#26469;&#65292;&#20351;&#29992;&#38669;&#22827;&#26364;&#32534;&#30721;&#23545;&#21333;&#35789;&#36827;&#34892;&#20998;&#35789;&#65292;&#25353;&#39057;&#29575;&#39034;&#24207;&#65292;&#20351;&#29992;&#22266;&#23450;&#25968;&#37327;&#30340;&#31526;&#21495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;CS-DE&#12289;EN-FR&#21644;EN-DE NMT&#20013;&#65292;&#20165;&#39057;&#29575;&#23601;&#21344;&#20102;BPE&#24471;&#20998;&#30340;90%-95%&#65292;&#22240;&#27492;&#32452;&#21512;&#24615;&#24182;&#19981;&#20687;&#20197;&#21069;&#35748;&#20026;&#30340;&#37027;&#20040;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Subword tokenization is the de facto standard for tokenization in neural language models and machine translation systems. Three advantages are frequently cited in favor of subwords: shorter encoding of frequent tokens, compositionality of subwords, and ability to deal with unknown words. As their relative importance is not entirely clear yet, we propose a tokenization approach that enables us to separate frequency (the first advantage) from compositionality. The approach uses Huffman coding to tokenize words, by order of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores reached by BPE, hence compositionality has less importance than previously thought.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.07772</link><description>&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#30340;&#22797;&#21046;&#26426;&#21046;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of the Copy Mechanism for Natural Language to SPARQL Query Generation. (arXiv:2304.07772v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32508;&#21512;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#21040;SPARQL&#26597;&#35810;&#29983;&#25104;&#20013;&#30340;&#22797;&#21046;&#26426;&#21046;&#65292;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#30740;&#31350;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20123;&#26041;&#38754;&#30340;&#20248;&#21270;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#39046;&#22495;&#22312;SPARQL&#26597;&#35810;&#29983;&#25104;&#26041;&#38754;&#26377;&#20102;&#26174;&#33879;&#30340;&#22686;&#38271;&#12290;&#26368;&#36817;&#65292;&#23558;&#22797;&#21046;&#26426;&#21046;&#19982;&#20256;&#32479;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#65292;&#21019;&#36896;&#20102;&#26032;&#30340;&#24615;&#33021;&#22522;&#20934;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#22797;&#21046;&#24182;&#25193;&#23637;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;NMT&#30340;SPARQL&#29983;&#25104;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#39044;&#35757;&#32451;&#21644;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#38382;&#39064;&#27880;&#37322;&#26684;&#24335;&#20197;&#21450;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#20351;&#29992;&#22797;&#21046;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23545;&#20110;&#38750;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#28155;&#21152;&#22797;&#21046;&#26426;&#21046;&#25110;&#20351;&#29992;&#38382;&#39064;&#27880;&#37322;&#37117;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#20026;&#19977;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#35774;&#32622;&#20102;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the field of neural machine translation (NMT) for SPARQL query generation has witnessed a significant growth. Recently, the incorporation of the copy mechanism with traditional encoder-decoder architectures and the use of pre-trained encoder-decoders have set new performance benchmarks. This paper presents a large variety of experiments that replicate and expand upon recent NMT-based SPARQL generation studies, comparing pre-trained and non-pre-trained models, question annotation formats, and the use of a copy mechanism for non-pre-trained and pre-trained models. Our results show that either adding the copy mechanism or using a question annotation improves performances for nonpre-trained models and for pre-trained models, setting new baselines for three popular datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#29702;&#35299;NLP&#20013;&#27867;&#21270;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;400&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#27867;&#21270;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;</title><link>http://arxiv.org/abs/2210.03050</link><description>&lt;p&gt;
NLP&#20013;&#30340;&#26368;&#26032;&#27867;&#21270;&#30740;&#31350;&#65306;&#20998;&#31867;&#21644;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art generalisation research in NLP: A taxonomy and review. (arXiv:2210.03050v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#31867;&#21644;&#29702;&#35299;NLP&#20013;&#27867;&#21270;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;400&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#31867;&#65292;&#24635;&#32467;&#20102;&#24403;&#21069;&#27867;&#21270;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20160;&#20040;&#26159;&#8220;&#33391;&#22909;&#30340;&#27867;&#21270;&#8221;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20173;&#19981;&#26126;&#30830;&#65292;&#20063;&#27809;&#26377;&#27867;&#21270;&#30340;&#35780;&#20272;&#26631;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#34920;&#24449;&#21644;&#29702;&#35299;NLP&#20013;&#27867;&#21270;&#30740;&#31350;&#30340;&#20998;&#31867;&#27861;&#12290;&#35813;&#20998;&#31867;&#27861;&#22522;&#20110;&#23545;&#27867;&#21270;&#30740;&#31350;&#30340;&#24191;&#27867;&#25991;&#29486;&#32508;&#36848;&#65292;&#21253;&#21547;&#20102;&#20116;&#20010;&#19981;&#21516;&#30340;&#32500;&#24230;&#65306;&#20027;&#35201;&#21160;&#26426;&#12289;&#30740;&#31350;&#30340;&#27867;&#21270;&#31867;&#22411;&#12289;&#32771;&#34385;&#30340;&#25968;&#25454;&#36716;&#31227;&#31867;&#22411;&#12289;&#25968;&#25454;&#36716;&#31227;&#30340;&#26469;&#28304;&#20197;&#21450;&#36716;&#31227;&#22312;&#24314;&#27169;&#27969;&#31243;&#20013;&#30340;&#20301;&#32622;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#20998;&#31867;&#27861;&#23545;&#27979;&#35797;&#27867;&#21270;&#30340;400&#22810;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#20849;&#36827;&#34892;&#20102;600&#22810;&#20010;&#23454;&#39564;&#12290;&#36890;&#36807;&#32508;&#36848;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#28145;&#20837;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#24403;&#21069;&#27867;&#21270;&#30740;&#31350;&#30340;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to generalise well is one of the primary desiderata of natural language processing (NLP). Yet, what 'good generalisation' entails and how it should be evaluated is not well understood, nor are there any evaluation standards for generalisation. In this paper, we lay the groundwork to address both of these issues. We present a taxonomy for characterising and understanding generalisation research in NLP. Our taxonomy is based on an extensive literature review of generalisation research, and contains five axes along which studies can differ: their main motivation, the type of generalisation they investigate, the type of data shift they consider, the source of this data shift, and the locus of the shift within the modelling pipeline. We use our taxonomy to classify over 400 papers that test generalisation, for a total of more than 600 individual experiments. Considering the results of this review, we present an in-depth analysis that maps out the current state of generalisation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAAQA&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22768;&#23398;&#38382;&#31572;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;1D&#21367;&#31215;&#22788;&#29702;&#22768;&#23398;&#20869;&#23481;&#30340;2D&#39057;&#35889;&#26102;&#22495;&#34920;&#31034;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#26102;&#38388;&#22352;&#26631;&#22270;&#22686;&#21152;&#20102;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#65292;&#24182;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#22522;&#26412;&#22768;&#38899;&#26500;&#24314;&#30340;&#22330;&#26223;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2106.06147</link><description>&lt;p&gt;
NAAQA: &#19968;&#31181;&#29992;&#20110;&#22768;&#23398;&#38382;&#31572;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
NAAQA: A Neural Architecture for Acoustic Question Answering. (arXiv:2106.06147v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAAQA&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#29992;&#20110;&#22768;&#23398;&#38382;&#31572;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;1D&#21367;&#31215;&#22788;&#29702;&#22768;&#23398;&#20869;&#23481;&#30340;2D&#39057;&#35889;&#26102;&#22495;&#34920;&#31034;&#65292;&#35813;&#32467;&#26500;&#36890;&#36807;&#26102;&#38388;&#22352;&#26631;&#22270;&#22686;&#21152;&#20102;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#65292;&#24182;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#22522;&#26412;&#22768;&#38899;&#26500;&#24314;&#30340;&#22330;&#26223;&#26102;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22768;&#23398;&#38382;&#31572;&#65288;AQA&#65289;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#22238;&#31572;&#20851;&#20110;&#22768;&#23398;&#22330;&#26223;&#20869;&#23481;&#30340;&#33258;&#30001;&#25991;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#22522;&#20110;&#20043;&#21069;&#20171;&#32461;&#30340;CLEAR&#25968;&#25454;&#38598;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;AQA&#22522;&#20934;&#65292;&#21363;CLEAR2&#65292;&#23427;&#24378;&#35843;&#20102;&#22768;&#23398;&#36755;&#20837;&#30340;&#29305;&#23450;&#25361;&#25112;&#65292;&#21253;&#25324;&#22788;&#29702;&#26102;&#38271;&#21464;&#21270;&#30340;&#22330;&#26223;&#21644;&#22312;&#35757;&#32451;&#38598;&#19982;&#27979;&#35797;&#38598;&#20043;&#38388;&#26377;&#19981;&#21516;&#30340;&#22522;&#26412;&#22768;&#38899;&#26500;&#24314;&#30340;&#22330;&#26223;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;NAAQA&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#23427;&#21033;&#29992;&#20102;&#22768;&#23398;&#36755;&#20837;&#30340;&#29305;&#23450;&#23646;&#24615;&#12290;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#19978;&#20351;&#29992;1D&#21367;&#31215;&#26469;&#22788;&#29702;&#22768;&#23398;&#20869;&#23481;&#30340;2D&#39057;&#35889;&#26102;&#22495;&#34920;&#31034;&#65292;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#24182;&#33021;&#20943;&#23569;&#27169;&#22411;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26102;&#38388;&#22352;&#26631;&#22270;&#21487;&#20197;&#22686;&#21152;&#26102;&#38388;&#23450;&#20301;&#33021;&#21147;&#65292;&#20174;&#32780;&#23558;&#32593;&#32476;&#24615;&#33021;&#25552;&#39640;&#32422;17&#20010;&#30334;&#20998;&#28857;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#29575;&#22352;&#26631;&#22270;&#23545;&#32593;&#32476;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of the Acoustic Question Answering (AQA) task is to answer a free-form text question about the content of an acoustic scene. It was inspired by the Visual Question Answering (VQA) task. In this paper, based on the previously introduced CLEAR dataset, we propose a new benchmark for AQA, namely CLEAR2, that emphasizes the specific challenges of acoustic inputs. These include handling of variable duration scenes, and scenes built with elementary sounds that differ between training and test set. We also introduce NAAQA, a neural architecture that leverages specific properties of acoustic inputs. The use of 1D convolutions in time and frequency to process 2D spectro-temporal representations of acoustic content shows promising results and enables reductions in model complexity. We show that time coordinate maps augment temporal localization capabilities which enhance performance of the network by ~17 percentage points. On the other hand, frequency coordinate maps have little influen
&lt;/p&gt;</description></item></channel></rss>