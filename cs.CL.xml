<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#22312;&#29616;&#26377;&#30340;VL&#27169;&#22411;&#20013;&#21152;&#20837;&#21512;&#25104;&#25968;&#25454;&#38598;SyViC&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;'&#21517;&#35789;&#20197;&#22806;'&#30340;&#29702;&#35299;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17590</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#31361;&#30772;&#21517;&#35789;&#23616;&#38480;
&lt;/p&gt;
&lt;p&gt;
Going Beyond Nouns With Vision &amp; Language Models Using Synthetic Data. (arXiv:2303.17590v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#29616;&#26377;&#30340;VL&#27169;&#22411;&#20013;&#21152;&#20837;&#21512;&#25104;&#25968;&#25454;&#38598;SyViC&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;'&#21517;&#35789;&#20197;&#22806;'&#30340;&#29702;&#35299;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#65288;VL&#65289;&#27169;&#22411;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#65288;&#20960;&#20046;&#20219;&#24847;&#65289;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#36827;&#34892;&#38646;&#26679;&#26412;&#24320;&#25918;&#35789;&#27719;&#25512;&#29702;&#65292;&#21462;&#20195;&#20102;&#19968;&#32452;&#25903;&#25345;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#19968;&#20010;&#26681;&#26412;&#24615;&#24369;&#28857;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#32431;&#21512;&#25104;&#25968;&#25454;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#25945;&#20250;&#36825;&#20123;&#27169;&#22411;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#32780;&#19981;&#25439;&#23475;&#23427;&#20204;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#20316;&#32773;&#36129;&#29486;&#20102;&#19968;&#20010;&#25968;&#30334;&#19975;&#35268;&#27169;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;SyViC&#65292;&#20197;&#21450;&#25968;&#25454;&#29983;&#25104;&#20195;&#30721;&#24211;&#65292;&#20801;&#35768;&#22312;&#29616;&#26377;&#30340;VL&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#39069;&#22806;&#30340;&#21512;&#36866;&#25968;&#25454;&#65292;&#23454;&#29616;'noun'&#20197;&#22806;&#30340;&#29702;&#35299;&#20219;&#21153;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained Vision &amp; Language (VL) models have shown remarkable performance in many applications, enabling replacing a fixed set of supported classes with zero-shot open vocabulary reasoning over (almost arbitrary) natural language prompts. However, recent works have uncovered a fundamental weakness of these models. For example, their difficulty to understand Visual Language Concepts (VLC) that go 'beyond nouns' such as the meaning of non-object words (e.g., attributes, actions, relations, states, etc.), or difficulty in performing compositional reasoning such as understanding the significance of the order of the words in a sentence. In this work, we investigate to which extent purely synthetic data could be leveraged to teach these models to overcome such shortcomings without compromising their zero-shot capabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scale synthetic dataset and data generation codebase allowing to generate additional suitable dat
&lt;/p&gt;</description></item><item><title>&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.17580</link><description>&lt;p&gt;
HuggingGPT: &#22312;HugingFace&#20013;&#20351;&#29992;ChatGPT&#21450;&#20854;&#20249;&#20276;&#35299;&#20915;AI&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace. (arXiv:2303.17580v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17580
&lt;/p&gt;
&lt;p&gt;
&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#25972;&#21512;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;AI&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#21516;&#39046;&#22495;&#21644;&#27169;&#24577;&#30340;&#22797;&#26434;AI&#20219;&#21153;&#26159;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#25511;&#21046;&#22120;&#26469;&#31649;&#29702;&#29616;&#26377;&#30340;AI&#27169;&#22411;&#20197;&#35299;&#20915;AI&#20219;&#21153;&#65292;&#35821;&#35328;&#25104;&#20026;&#36890;&#29992;&#25509;&#21475;&#26469;&#36171;&#33021;&#23427;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#20316;&#20026;&#20219;&#21153;&#35268;&#21010;&#24037;&#20855;&#65292;&#26681;&#25454;HuggingFace&#20013;&#21487;&#29992;&#30340;&#27169;&#22411;&#21151;&#33021;&#25551;&#36848;&#26469;&#36873;&#25321;&#27169;&#22411;&#65292;&#22312;&#36873;&#23450;AI&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#27599;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24635;&#32467;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving complicated AI tasks with different domains and modalities is a key step toward artificial general intelligence (AGI). While there are abundant AI models available for different domains and modalities, they cannot handle complicated AI tasks. Considering large language models (LLMs) have exhibited exceptional ability in language understanding, generation, interaction, and reasoning, we advocate that LLMs could act as a controller to manage existing AI models to solve complicated AI tasks and language could be a generic interface to empower this. Based on this philosophy, we present HuggingGPT, a system that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities (e.g., HuggingFace) to solve AI tasks. Specifically, we use ChatGPT to conduct task planning when receiving a user request, select models according to their function descriptions available in HuggingFace, execute each subtask with the selected AI model, and summarize the response acco
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.17579</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20248;&#21270;&#22522;&#20110;&#26816;&#32034;&#30340;&#33016;&#37096; X &#23556;&#32447;&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Multimodal Image-Text Matching Improves Retrieval-based Chest X-Ray Report Generation. (arXiv:2303.17579v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359; X-REM&#65292;&#23427;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#65292;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#65292;&#20854;&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#26377;&#25928;&#25552;&#39640;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;&#33258;&#21160;&#29983;&#25104;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#29983;&#25104;&#20020;&#24202;&#20934;&#30830;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#21487;&#20197;&#25913;&#21892;&#24739;&#32773;&#25252;&#29702;&#12290;&#20197;&#21069;&#20381;&#36182;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#30340;&#25253;&#21578;&#29983;&#25104;&#26041;&#27861;&#30001;&#20110;&#32570;&#20047;&#30456;&#20851;&#39046;&#22495;&#30693;&#35782;&#32780;&#32463;&#24120;&#29983;&#25104;&#19981;&#36830;&#36143;&#21644;&#19981;&#27491;&#30830;&#30340;&#25991;&#26412;&#65292;&#32780;&#22522;&#20110;&#26816;&#32034;&#30340;&#23581;&#35797;&#32463;&#24120;&#26816;&#32034;&#21040;&#19982;&#36755;&#20837;&#22270;&#20687;&#19981;&#30456;&#20851;&#30340;&#25253;&#21578;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Contrastive X-Ray REport Match&#65288;X-REM&#65289;&#30340;&#26032;&#22411;&#22522;&#20110;&#26816;&#32034;&#30340;&#25918;&#23556;&#24615;&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#65292;&#35813;&#27169;&#22359;&#20351;&#29992;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#26469;&#34913;&#37327;&#33016;&#37096; X &#20809;&#22270;&#20687;&#21644;&#25918;&#23556;&#23398;&#25253;&#21578;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#20197;&#36827;&#34892;&#25253;&#21578;&#26816;&#32034;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#20351;&#29992;&#35821;&#35328;&#22270;&#20687;&#27169;&#22411;&#35745;&#31639;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#20998;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#21040;&#22312;&#20351;&#29992;&#20313;&#24358;&#30456;&#20284;&#24615;&#26102;&#32463;&#24120;&#20002;&#22833;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#32454;&#31890;&#24230;&#20132;&#20114;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#21644;&#20020;&#24202;&#24230;&#37327;&#26041;&#38754;&#65292;X-REM&#22312;&#22810;&#20010;&#20808;&#21069;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#27169;&#22359;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;&#36890;&#36807;&#23545;&#29983;&#25104;&#30340;&#25253;&#21578;&#36827;&#34892;&#20154;&#31867;&#35780;&#20272;&#65292;&#34920;&#26126; X-R...
&lt;/p&gt;
&lt;p&gt;
Automated generation of clinically accurate radiology reports can improve patient care. Previous report generation methods that rely on image captioning models often generate incoherent and incorrect text due to their lack of relevant domain knowledge, while retrieval-based attempts frequently retrieve reports that are irrelevant to the input image. In this work, we propose Contrastive X-Ray REport Match (X-REM), a novel retrieval-based radiology report generation module that uses an image-text matching score to measure the similarity of a chest X-ray image and radiology report for report retrieval. We observe that computing the image-text matching score with a language-image model can effectively capture the fine-grained interaction between image and text that is often lost when using cosine similarity. X-REM outperforms multiple prior radiology report generation modules in terms of both natural language and clinical metrics. Human evaluation of the generated reports suggests that X-R
&lt;/p&gt;</description></item><item><title>EWR&#26041;&#27861;&#36890;&#36807;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26435;&#34913;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#20010;&#20307;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#23545;&#35805;&#22238;&#22797;&#30340;&#24544;&#23454;&#24615;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17574</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#24544;&#23454;&#21644;&#25277;&#35937;&#21270;&#23545;&#35805;&#29983;&#25104;&#30340;&#24377;&#24615;&#26435;&#37325;&#21435;&#38500;
&lt;/p&gt;
&lt;p&gt;
Elastic Weight Removal for Faithful and Abstractive Dialogue Generation. (arXiv:2303.17574v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17574
&lt;/p&gt;
&lt;p&gt;
EWR&#26041;&#27861;&#36890;&#36807;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26435;&#34913;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#20013;&#20010;&#20307;&#21442;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#25552;&#39640;&#23545;&#35805;&#22238;&#22797;&#30340;&#24544;&#23454;&#24615;&#65292;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#23545;&#35805;&#31995;&#32479;&#24212;&#35813;&#29983;&#25104;&#24544;&#23454;&#20110;&#30456;&#20851;&#25991;&#26723;&#20013;&#21253;&#21547;&#30340;&#30693;&#35782;&#30340;&#22238;&#22797;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27169;&#22411;&#29983;&#25104;&#20102;&#24187;&#24819;&#30340;&#21709;&#24212;&#65292;&#20854;&#20013;&#21253;&#21547;&#19982;&#20854;&#30456;&#30683;&#30462;&#30340;&#20449;&#24687;&#25110;&#19981;&#21487;&#39564;&#35777;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#31181;&#19981;&#33391;&#34892;&#20026;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22312;&#36127;&#38754;&#31034;&#20363;&#19978;&#24494;&#35843;&#8220;&#36127;&#38754;&#19987;&#23478;&#8221;&#65292;&#24182;&#20174;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#20013;&#20943;&#21435;&#23427;&#30340;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30452;&#35273;&#19978;&#65292;&#36825;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#26576;&#20123;&#21442;&#25968;&#27604;&#20854;&#20182;&#21442;&#25968;&#26356;&#36127;&#36131;&#23548;&#33268;&#24187;&#35273;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#65288;&#36817;&#20284;&#65289;&#36153;&#33293;&#23572;&#20449;&#24687;&#30697;&#38453;&#26469;&#26435;&#34913;&#23427;&#20204;&#30340;&#20010;&#20307;&#37325;&#35201;&#24615;&#65292;&#35813;&#30697;&#38453;&#34913;&#37327;&#20854;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#26041;&#27861;&#31216;&#20026;&#24377;&#24615;&#26435;&#37325;&#21435;&#38500;&#65288;EWR&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;Flan-T5&#19981;&#21516;&#21464;&#20307;&#20316;&#20026;&#39592;&#24178;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#20449;&#24687;&#23547;&#27714;&#23545;&#35805;&#29983;&#25104;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24544;&#23454;&#24615;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ideally, dialogue systems should generate responses that are faithful to the knowledge contained in relevant documents. However, many models generate hallucinated responses instead that contradict it or contain unverifiable information. To mitigate such undesirable behaviour, it has been proposed to fine-tune a `negative expert' on negative examples and subtract its parameters from those of a pre-trained model. However, intuitively, this does not take into account that some parameters are more responsible than others in causing hallucinations. Thus, we propose to weigh their individual importance via (an approximation of) the Fisher Information matrix, which measures the uncertainty of their estimate. We call this method Elastic Weight Removal (EWR). We evaluate our method -- using different variants of Flan-T5 as a backbone language model -- on multiple datasets for information-seeking dialogue generation and compare our method with state-of-the-art techniques for faithfulness, such a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17564</link><description>&lt;p&gt;
BloombergGPT&#65306;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BloombergGPT: A Large Language Model for Finance. (arXiv:2303.17564v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;500&#20159;&#21442;&#25968;&#30340;&#37329;&#34701;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#22522;&#20110;Bloomberg&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#21644;&#36890;&#29992;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#65292;&#35813;&#27169;&#22411;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20250;&#29306;&#29298;&#22312;&#26222;&#36890;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#37329;&#34701;&#25216;&#26415;&#39046;&#22495;&#26377;&#30528;&#24191;&#27867;&#32780;&#22797;&#26434;&#30340;&#24212;&#29992;&#65292;&#20174;&#24773;&#24863;&#20998;&#26512;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21040;&#38382;&#31572;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#38750;&#24120;&#26377;&#25928;&#65307;&#28982;&#32780;&#65292;&#19987;&#20026;&#37329;&#34701;&#39046;&#22495;&#35774;&#35745;&#30340;LLM&#23578;&#26410;&#22312;&#25991;&#29486;&#20013;&#25253;&#21578;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BloombergGPT&#65292;&#19968;&#20010;&#25317;&#26377;500&#20159;&#20010;&#21442;&#25968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#26159;&#22522;&#20110;&#24191;&#27867;&#30340;&#37329;&#34701;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;3630&#20159;&#20010;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22522;&#20110;&#24429;&#21338;&#31038;&#30340;&#24191;&#27867;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#33021;&#26159;&#36804;&#20170;&#26368;&#22823;&#30340;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21448;&#22686;&#21152;&#20102;&#26469;&#33258;&#36890;&#29992;&#25968;&#25454;&#38598;&#30340;3450&#20159;&#20010;&#26631;&#35760;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;LLM&#22522;&#20934;&#12289;&#24320;&#25918;&#24335;&#37329;&#34701;&#22522;&#20934;&#21644;&#19968;&#22871;&#26368;&#33021;&#20934;&#30830;&#21453;&#26144;&#25105;&#20204;&#39044;&#26399;&#29992;&#36884;&#30340;&#20869;&#37096;&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;BloombergGPT&#12290;&#25105;&#20204;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#35757;&#32451;&#20135;&#29983;&#20102;&#19968;&#20010;&#22312;&#37329;&#34701;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#19981;&#20250;&#29306;&#29298;&#26222;&#36890;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of NLP in the realm of financial technology is broad and complex, with applications ranging from sentiment analysis and named entity recognition to question answering. Large Language Models (LLMs) have been shown to be effective on a variety of tasks; however, no LLM specialized for the financial domain has been reported in literature. In this work, we present BloombergGPT, a 50 billion parameter language model that is trained on a wide range of financial data. We construct a 363 billion token dataset based on Bloomberg's extensive data sources, perhaps the largest domain-specific dataset yet, augmented with 345 billion tokens from general purpose datasets. We validate BloombergGPT on standard LLM benchmarks, open financial benchmarks, and a suite of internal benchmarks that most accurately reflect our intended usage. Our mixed dataset training leads to a model that outperforms existing models on financial tasks by significant margins without sacrificing performance on general 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35760;&#24518;&#21644;&#27867;&#21270;&#20165;&#22312;&#35757;&#32451;&#20013;&#23569;&#27425;&#35266;&#23519;&#21040;&#30340;&#20363;&#23376;&#12290;</title><link>http://arxiv.org/abs/2303.17557</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#26679;&#26412;&#35760;&#24518;&#30340;&#35782;&#21035;&#12289;&#22238;&#24518;&#21644;&#20445;&#25345;
&lt;/p&gt;
&lt;p&gt;
Recognition, recall, and retention of few-shot memories in large language models. (arXiv:2303.17557v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17557
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25581;&#31034;&#20102;&#36825;&#31181;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#22320;&#35760;&#24518;&#21644;&#27867;&#21270;&#20165;&#22312;&#35757;&#32451;&#20013;&#23569;&#27425;&#35266;&#23519;&#21040;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#22312;&#19968;&#20010;&#22823;&#22810;&#25968;&#35757;&#32451;&#26679;&#26412;&#20165;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#20960;&#27425;&#30340;&#27169;&#24335;&#19979;&#36827;&#34892;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31616;&#21333;&#30340;&#35782;&#21035;&#12289;&#22238;&#24518;&#21644;&#20445;&#25345;&#23454;&#39564;&#65292;&#25506;&#31350;&#27169;&#22411;&#23545;&#20165;&#22312;&#35757;&#32451;&#26399;&#38388;&#23569;&#27425;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#30340;&#35760;&#24518;&#21450;&#20854;&#22312;&#32487;&#32493;&#35757;&#32451;&#26102;&#25345;&#32493;&#30340;&#26102;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#19968;&#20010;&#25509;&#35302;&#36890;&#24120;&#36275;&#20197;&#35753;&#27169;&#22411;&#22312;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35782;&#21035;&#23454;&#39564;&#20013;&#36798;&#21040;&#36817;&#20046;&#23436;&#32654;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#20272;&#35745;&#65292;&#22312;&#36830;&#32493;&#35757;&#32451;&#26032;&#26679;&#26412;&#30340;&#20960;&#20010;&#26102;&#26399;&#20869;&#65292;&#27169;&#22411;&#20445;&#25345;&#20102;&#24050;&#35265;&#26679;&#26412;&#30340;&#21487;&#35782;&#21035;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35760;&#24518;&#21644;&#27867;&#21270;&#23569;&#26679;&#26412;&#20363;&#23376;&#30340;&#33021;&#21147;&#65292;&#36825;&#19982;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#21331;&#36234;&#34920;&#29616;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training of modern large language models (LLMs) takes place in a regime where most training examples are seen only a few times by the model during the course of training. What does a model remember about such examples seen only a few times during training and how long does that memory persist in the face of continuous training with new examples? Here, we investigate these questions through simple recognition, recall, and retention experiments with LLMs. In recognition experiments, we ask if the model can distinguish the seen example from a novel example; in recall experiments, we ask if the model can correctly recall the seen example when cued by a part of it; and in retention experiments, we periodically probe the model's memory for the original examples as the model is trained continuously with new examples. We find that a single exposure is generally sufficient for a model to achieve near perfect accuracy even in very challenging recognition experiments. We estimate that the rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinionsQA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;60&#20010;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#30340;&#35266;&#28857;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#29978;&#33267;&#36890;&#36807;&#26126;&#30830;&#35843;&#25972;LM&#21453;&#26144;&#20986;&#30340;&#35266;&#28857;&#65292;&#20173;&#28982;&#26080;&#27861;&#28040;&#38500;&#12290;</title><link>http://arxiv.org/abs/2303.17548</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#20102;&#35841;&#30340;&#35266;&#28857;&#65311;
&lt;/p&gt;
&lt;p&gt;
Whose Opinions Do Language Models Reflect?. (arXiv:2303.17548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#26469;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;OpinionsQA&#65292;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;60&#20010;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#35266;&#28857;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#35821;&#35328;&#27169;&#22411;&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#30340;&#35266;&#28857;&#23384;&#22312;&#24040;&#22823;&#24046;&#24322;&#65292;&#29978;&#33267;&#36890;&#36807;&#26126;&#30830;&#35843;&#25972;LM&#21453;&#26144;&#20986;&#30340;&#35266;&#28857;&#65292;&#20173;&#28982;&#26080;&#27861;&#28040;&#38500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#24320;&#25918;&#29615;&#22659;&#20013;&#34987;&#20351;&#29992;&#65292;&#22312;&#38024;&#23545;&#20027;&#35266;&#26597;&#35810;&#30340;&#21709;&#24212;&#20013;&#21453;&#26144;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65292;&#21516;&#26102;&#20063;&#21487;&#33021;&#22609;&#36896;&#25972;&#20010;&#31038;&#20250;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#20197;&#35843;&#26597;LM&#21453;&#26144;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#20844;&#20849;&#27665;&#24847;&#35843;&#26597;&#21644;&#30456;&#20851;&#30340;&#20154;&#31867;&#21453;&#24212;&#26469;&#21019;&#24314;OpinionsQA&#65292;&#24182;&#23545;60&#20010;&#32654;&#22269;&#20154;&#21475;&#32479;&#35745;&#32452;&#30340;&#24847;&#35265;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#28041;&#21450;&#20174;&#22549;&#32974;&#21040;&#33258;&#21160;&#21270;&#30340;&#21508;&#31181;&#35805;&#39064;&#12290;&#22312;&#21508;&#20010;&#35805;&#39064;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;LM&#21453;&#26144;&#30340;&#35266;&#28857;&#19982;&#32654;&#22269;&#20154;&#32676;&#32452;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#65292;&#36825;&#19982;&#27665;&#20027;&#20826;&#21644;&#20849;&#21644;&#20826;&#22312;&#27668;&#20505;&#21464;&#21270;&#38382;&#39064;&#19978;&#30340;&#20998;&#27495;&#24046;&#19981;&#22810;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#26126;&#30830;&#23558;LM&#23450;&#21521;&#20110;&#29305;&#23450;&#30340;&#20154;&#21475;&#32479;&#35745;&#32452;&#65292;&#36825;&#31181;&#24046;&#24322;&#20173;&#28982;&#23384;&#22312;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#19981;&#20165;&#30830;&#35748;&#20102;&#20808;&#21069;&#23545;&#24038;&#20542;&#20542;&#21521;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#36825;&#31181;&#24046;&#24322;&#30340;&#19968;&#20010;&#20840;&#26032;&#30340;&#29702;&#35770;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) are increasingly being used in open-ended contexts, where the opinions reflected by LMs in response to subjective queries can have a profound impact, both on user satisfaction, as well as shaping the views of society at large. In this work, we put forth a quantitative framework to investigate the opinions reflected by LMs -- by leveraging high-quality public opinion polls and their associated human responses. Using this framework, we create OpinionsQA, a new dataset for evaluating the alignment of LM opinions with those of 60 US demographic groups over topics ranging from abortion to automation. Across topics, we find substantial misalignment between the views reflected by current LMs and those of US demographic groups: on par with the Democrat-Republican divide on climate change. Notably, this misalignment persists even after explicitly steering the LMs towards particular demographic groups. Our analysis not only confirms prior observations about the left-leaning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#30340;&#21475;&#35821;&#23383;&#24149;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.17517</link><description>&lt;p&gt;
&#21360;&#22320;&#35821;&#20316;&#20026;&#31532;&#20108;&#35821;&#35328;&#65306;&#36890;&#36807;&#35821;&#20041;&#30456;&#20284;&#26679;&#26412;&#25552;&#39640;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Hindi as a Second Language: Improving Visually Grounded Speech with Semantically Similar Samples. (arXiv:2303.17517v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#30340;&#21475;&#35821;&#23383;&#24149;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#26469;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20174;&#22810;&#35821;&#35328;&#35282;&#24230;&#25506;&#32034;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;(VGS)&#30340;&#23398;&#20064;&#12290;&#21452;&#35821;VGS&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#20004;&#31181;&#35821;&#35328;&#20013;&#24179;&#22343;&#25968;&#37327;&#30340;&#21475;&#35821;&#23383;&#24149;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#19978;&#65292;&#21487;&#29992;&#21475;&#35821;&#23383;&#24149;&#20043;&#38388;&#30340;&#35821;&#35328;&#21487;&#33021;&#23384;&#22312;&#19981;&#24179;&#34913;&#12290;&#26412;&#25991;&#20027;&#35201;&#36129;&#29486;&#22312;&#20110;&#21033;&#29992;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#21147;&#37327;&#22312;&#21452;&#35821;&#22522;&#20110;&#35270;&#35273;&#30340;&#35821;&#38899;&#27169;&#22411;&#20013;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#23558;&#39640;&#36164;&#28304;&#35821;&#35328;&#30340;&#30693;&#35782;&#33976;&#39311;&#21040;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#65306;(1)&#25972;&#21512;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#39640;&#36164;&#28304;&#35821;&#35328;&#32534;&#30721;&#22120;&#21644;(2)&#20351;&#29992;&#35821;&#20041;&#30456;&#20284;&#30340;&#21475;&#35821;&#23383;&#24149;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#30340;&#32452;&#21512;&#26377;&#25928;&#22320;&#20351;&#20302;&#36164;&#28304;&#35821;&#35328;&#22312;&#36328;&#27169;&#24577;&#26816;&#32034;&#20219;&#21153;&#20013;&#36229;&#36807;&#21333;&#35821;&#35328;&#21644;&#21452;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this work is to explore the learning of visually grounded speech models (VGS) from multilingual perspective. Bilingual VGS models are generally trained with an equal number of spoken captions from both languages. However, in reality, there can be an imbalance among the languages for the available spoken captions. Our key contribution in this work is to leverage the power of a high-resource language in a bilingual visually grounded speech model to improve the performance of a low-resource language. We introduce two methods to distill the knowledge of high-resource language into low-resource languages: (1) incorporating a strong pre-trained high-resource language encoder and (2) using semantically similar spoken captions. Our experiments show that combining these two approaches effectively enables the low-resource language to surpass the performances of monolingual and bilingual counterparts for cross-modal retrieval tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17513</link><description>&lt;p&gt;
&#36890;&#36807;GPT-3&#33258;&#21160;&#24418;&#24335;&#21270;&#25552;&#39640;Diproche CNL&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Improving the Diproche CNL through autoformalization via GPT-3. (arXiv:2303.17513v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Diproche&#31995;&#32479;&#26159;&#19968;&#27454;&#38024;&#23545;&#24503;&#35821;&#25511;&#21046;&#35821;&#35328;&#29255;&#27573;&#30340;&#33258;&#21160;&#21270;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#26088;&#22312;&#29992;&#20110;&#25945;&#23398;&#24212;&#29992;&#65292;&#22312;&#24341;&#23548;&#23398;&#29983;&#36827;&#34892;&#35777;&#26126;&#26102;&#20351;&#29992;&#12290;&#35813;&#31995;&#32479;&#30340;&#31532;&#19968;&#20010;&#29256;&#26412;&#20351;&#29992;&#19968;&#31181;&#25511;&#21046;&#33258;&#28982;&#35821;&#35328;&#65292;&#20854;Prolog&#24418;&#24335;&#21270;&#20363;&#31243;&#24050;&#32463;&#32534;&#20889;&#22909;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;Diproche&#19978;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#24418;&#24335;&#21270;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Diproche system is an automated proof checker for texts written in a controlled fragment of German, designed for didactical applications in classes introducing students to proofs for the first time. The first version of the system used a controlled natural language for which a Prolog formalization routine was written. In this paper, we explore the possibility of prompting large language models for autoformalization in the context of Diproche, with encouraging first results.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#21253;&#25324;&#38590;&#20197;&#21306;&#20998;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#27450;&#35784;&#65292;&#36827;&#32780;&#20135;&#29983;&#26032;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.17511</link><description>&lt;p&gt;
&#20851;&#20110;&#22797;&#26434;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21155;&#65288;&#22353;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
On pitfalls (and advantages) of sophisticated large language models. (arXiv:2303.17511v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17511
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#21487;&#33021;&#20250;&#23548;&#33268;&#20005;&#37325;&#21518;&#26524;&#65292;&#21253;&#25324;&#38590;&#20197;&#21306;&#20998;&#30340;&#26426;&#22120;&#29983;&#25104;&#25991;&#26412;&#21644;&#21508;&#31181;&#24418;&#24335;&#30340;&#27450;&#35784;&#65292;&#36827;&#32780;&#20135;&#29983;&#26032;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#12290;&#22312;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#22522;&#20110;&#27169;&#24335;&#35782;&#21035;&#30340;&#28216;&#25103;&#21644;&#23454;&#38469;&#39046;&#22495;&#20013;&#35777;&#26126;&#36229;&#36234;&#20154;&#31867;&#34920;&#29616;&#21518;&#65292;&#25105;&#20204;&#29616;&#22312;&#21487;&#33021;&#22788;&#20110;&#19968;&#20010;&#20154;&#24037;&#23454;&#20307;&#26368;&#32456;&#36827;&#20837;&#20154;&#31867;&#20132;&#27969;&#39046;&#22495;&#30340;&#21313;&#23383;&#36335;&#21475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#24102;&#26469;&#20102;&#20005;&#37325;&#30340;&#39118;&#38505;&#12290;&#30001;&#20110;&#31070;&#32463;&#32593;&#32476;&#21487;&#38752;&#24615;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#36807;&#24230;&#20381;&#36182;LLMs&#21487;&#33021;&#24102;&#26469;&#30772;&#22351;&#24615;&#21518;&#26524;&#12290;&#30001;&#20110;&#21306;&#20998;&#20154;&#31867;&#20070;&#20889;&#21644;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#23558;&#21464;&#24471;&#36234;&#26469;&#36234;&#22256;&#38590;&#65292;&#20154;&#20204;&#38754;&#20020;&#30528;&#26032;&#30340;&#20262;&#29702;&#25361;&#25112;&#12290;&#20174;&#19981;&#20877;&#26126;&#30830;&#21487;&#39564;&#35777;&#30340;&#20154;&#31867;&#20316;&#32773;&#36523;&#20221;&#24320;&#22987;&#65292;&#32487;&#32493;&#28041;&#21450;&#21508;&#31181;&#31867;&#22411;&#30340;&#27450;&#35784;&#65292;&#20363;&#22914;&#26032;&#24418;&#24335;&#30340;&#21117;&#31363;&#12290;&#36825;&#36824;&#28041;&#21450;&#20405;&#29359;&#38544;&#31169;&#26435;&#65292;&#21487;&#33021;&#20256;&#25773;&#20154;&#31867;&#20266;&#36896;&#21697;&#65292;&#26368;&#21518;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#26159;&#65292;&#23427;&#20351;&#22823;&#35268;&#27169;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing based on large language models (LLMs) is a booming field of AI research. After neural networks have proven to outperform humans in games and practical domains based on pattern recognition, we might stand now at a road junction where artificial entities might eventually enter the realm of human communication. However, this comes with serious risks. Due to the inherent limitations regarding the reliability of neural networks, overreliance on LLMs can have disruptive consequences. Since it will be increasingly difficult to distinguish between human-written and machine-generated text, one is confronted with new ethical challenges. This begins with the no longer undoubtedly verifiable human authorship and continues with various types of fraud, such as a new form of plagiarism. This also concerns the violation of privacy rights, the possibility of circulating counterfeits of humans, and, last but not least, it makes a massive spread of misinformation possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17491</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#35299;&#20915;&#35745;&#31639;&#26426;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Language Models can Solve Computer Tasks. (arXiv:2303.17491v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#35813;&#26041;&#27861;&#21462;&#24471;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33021;&#22815;&#22312;&#35745;&#31639;&#26426;&#19978;&#25191;&#34892;&#36890;&#29992;&#20219;&#21153;&#30340;&#20195;&#29702;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#21644;&#21327;&#21161;&#22797;&#26434;&#38382;&#39064;&#30340;&#35299;&#20915;&#26469;&#25552;&#39640;&#25928;&#29575;&#21644;&#29983;&#20135;&#21147;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#20195;&#29702;&#24212;&#35813;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21629;&#20196;&#35299;&#20915;&#26032;&#30340;&#35745;&#31639;&#26426;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#19987;&#23478;&#31034;&#33539;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#36825;&#20004;&#32773;&#23545;&#20110;&#26032;&#20219;&#21153;&#26469;&#35828;&#37117;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#21487;&#20197;&#20351;&#29992;&#19968;&#20010;&#31616;&#21333;&#30340;&#25552;&#31034;&#26041;&#26696;&#65288;RCI&#65289;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#25191;&#34892;&#35745;&#31639;&#26426;&#20219;&#21153;&#65292;&#24182;&#22312;&#25209;&#35780;&#21644;&#25913;&#36827;&#36755;&#20986;&#30340;&#36807;&#31243;&#20013;&#21462;&#24471;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;RCI&#26041;&#27861;&#22312;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#20219;&#21153;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;LLM&#26041;&#27861;&#65292;&#24182;&#22312;MiniWoB++&#22522;&#20934;&#27979;&#35797;&#20013;&#36229;&#36234;&#20102;&#30417;&#30563;&#23398;&#20064;&#65288;SL&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;RCI&#26041;&#27861;&#20351;&#29992;&#27599;&#20010;&#20219;&#21153;&#20165;&#26377;&#30340;&#23569;&#25968;&#31034;&#33539;&#65292;&#19982;&#26368;&#26032;&#30340;SL+RL&#26041;&#27861;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agents capable of carrying out general tasks on a computer can improve efficiency and productivity by automating repetitive tasks and assisting in complex problem-solving. Ideally, such agents should be able to solve new computer tasks presented to them through natural language commands. However, previous approaches to this problem require large amounts of expert demonstrations and task-specific reward functions, both of which are impractical for new tasks. In this work, we show that a pre-trained large language model (LLM) agent can execute computer tasks guided by natural language using a simple prompting scheme where the agent recursively criticizes and improves its output (RCI). The RCI approach significantly outperforms existing LLM methods for automating computer tasks and surpasses supervised learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++ benchmark. RCI is competitive with the state-of-the-art SL+RL method, using only a handful of demonstrations per ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17475</link><description>&lt;p&gt;
&#36229;&#36234;&#36127;&#37319;&#26679;&#30340;&#39640;&#25928;&#20998;&#24067;&#24335;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient distributed representations beyond negative sampling. (arXiv:2303.17475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#23884;&#20837;&#65289;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#26469;&#23454;&#29616;&#23398;&#20064;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#36127;&#37319;&#26679;&#26041;&#27861;&#24182;&#22312;&#22810;&#39033;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#65288;&#20063;&#31216;&#20026;&#23884;&#20837;&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#19968;&#20010;&#31867;&#20284;&#20110;Word2Vec&#31639;&#27861;&#20013;&#24341;&#20837;&#24182;&#22312;&#22810;&#20010;&#24037;&#20316;&#20013;&#37319;&#29992;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#23454;&#29616;&#12290;&#20248;&#21270;&#35745;&#31639;&#30340;&#29942;&#39048;&#26159;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#30340;&#35745;&#31639;&#65292;&#36825;&#38656;&#35201;&#19982;&#26679;&#26412;&#22823;&#23567;&#21576;&#20108;&#27425;&#27604;&#20363;&#30340;&#25805;&#20316;&#25968;&#12290;&#36825;&#31181;&#22797;&#26434;&#24230;&#19981;&#36866;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#25152;&#20197;&#36127;&#37319;&#26679;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19982;&#26679;&#26412;&#22823;&#23567;&#32447;&#24615;&#30456;&#20851;&#30340;&#26102;&#38388;&#20869;&#33719;&#24471;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36127;&#37319;&#26679;&#20250;&#25913;&#21464;&#25439;&#22833;&#20989;&#25968;&#65292;&#22240;&#27492;&#35299;&#20915;&#30340;&#26159;&#19982;&#26368;&#21021;&#25552;&#20986;&#30340;&#19981;&#21516;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#22914;&#20309;&#36890;&#36807;&#32447;&#24615;&#26102;&#38388;&#20272;&#35745;softmax&#24402;&#19968;&#21270;&#24120;&#25968;&#65292;&#20174;&#32780;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#23398;&#20064;&#20998;&#24067;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27979;&#35797;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23884;&#20837;&#36136;&#37327;&#21644;&#35757;&#32451;&#26102;&#38388;&#26041;&#38754;&#20248;&#20110;&#36127;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes an efficient method to learn distributed representations, also known as embeddings. This is accomplished minimizing an objective function similar to the one introduced in the Word2Vec algorithm and later adopted in several works. The optimization computational bottleneck is the calculation of the softmax normalization constants for which a number of operations scaling quadratically with the sample size is required. This complexity is unsuited for large datasets and negative sampling is a popular workaround, allowing one to obtain distributed representations in linear time with respect to the sample size. Negative sampling consists, however, in a change of the loss function and hence solves a different optimization problem from the one originally proposed. Our contribution is to show that the sotfmax normalization constants can be estimated in linear time, allowing us to design an efficient optimization strategy to learn distributed representations. We test our ap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;ChatGPT&#23545;&#26088;&#22312;&#37327;&#21270;&#20154;&#31867;&#25991;&#21270;&#24046;&#24322;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#20854;&#25991;&#21270;&#36866;&#24212;&#33021;&#21147;&#12290;&#21457;&#29616;ChatGPT&#22312;&#20197;&#32654;&#22269;&#32972;&#26223;&#20026;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#19982;&#32654;&#22269;&#25991;&#21270;&#30340;&#24378;&#28872;&#23545;&#40784;&#65292;&#20294;&#20854;&#23545;&#20854;&#20182;&#25991;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#19988;&#33521;&#25991;&#25552;&#31034;&#20250;&#25273;&#24179;&#25991;&#21270;&#24046;&#24322;&#24182;&#20559;&#21521;&#32654;&#22269;&#25991;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.17466</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#19982;&#20154;&#31867;&#31038;&#20250;&#30340;&#36328;&#25991;&#21270;&#23545;&#40784;&#65306;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study. (arXiv:2303.17466v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17466
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;ChatGPT&#23545;&#26088;&#22312;&#37327;&#21270;&#20154;&#31867;&#25991;&#21270;&#24046;&#24322;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#65292;&#35780;&#20272;&#20102;&#20854;&#25991;&#21270;&#36866;&#24212;&#33021;&#21147;&#12290;&#21457;&#29616;ChatGPT&#22312;&#20197;&#32654;&#22269;&#32972;&#26223;&#20026;&#25552;&#31034;&#26102;&#34920;&#29616;&#20986;&#19982;&#32654;&#22269;&#25991;&#21270;&#30340;&#24378;&#28872;&#23545;&#40784;&#65292;&#20294;&#20854;&#23545;&#20854;&#20182;&#25991;&#21270;&#30340;&#36866;&#24212;&#33021;&#21147;&#36739;&#24046;&#65292;&#24182;&#19988;&#33521;&#25991;&#25552;&#31034;&#20250;&#25273;&#24179;&#25991;&#21270;&#24046;&#24322;&#24182;&#20559;&#21521;&#32654;&#22269;&#25991;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#21457;&#24067;&#30340;ChatGPT&#22240;&#20854;&#22312;&#23545;&#35805;&#20013;&#29983;&#25104;&#31867;&#20154;&#22238;&#24212;&#30340;&#21331;&#36234;&#33021;&#21147;&#32780;&#24191;&#21463;&#35748;&#21487;&#12290;&#32771;&#34385;&#21040;&#20854;&#34987;&#21508;&#22269;&#29992;&#25143;&#20351;&#29992;&#20197;&#21450;&#20854;&#35757;&#32451;&#20102;&#21253;&#21547;&#22810;&#26679;&#25991;&#21270;&#21644;&#31038;&#20250;&#35268;&#33539;&#30340;&#24222;&#22823;&#22810;&#35821;&#26009;&#24211;&#65292;&#35780;&#20272;&#20854;&#25991;&#21270;&#36866;&#24212;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;ChatGPT&#23545;&#26088;&#22312;&#37327;&#21270;&#20154;&#31867;&#25991;&#21270;&#24046;&#24322;&#30340;&#38382;&#39064;&#30340;&#22238;&#31572;&#26469;&#35843;&#26597;&#20854;&#28508;&#22312;&#30340;&#25991;&#21270;&#32972;&#26223;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24403;&#20197;&#32654;&#22269;&#32972;&#26223;&#20026;&#25552;&#31034;&#26102;&#65292;ChatGPT&#34920;&#29616;&#20986;&#19982;&#32654;&#22269;&#25991;&#21270;&#30340;&#24378;&#28872;&#23545;&#40784;&#65292;&#20294;&#20854;&#23545;&#20854;&#20182;&#25991;&#21270;&#32972;&#26223;&#30340;&#36866;&#24212;&#33021;&#21147;&#36739;&#24046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#25552;&#31034;&#26469;&#25506;&#27979;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#33521;&#25991;&#25552;&#31034;&#20250;&#38477;&#20302;&#27169;&#22411;&#22238;&#31572;&#30340;&#24046;&#24322;&#65292;&#25273;&#24179;&#25991;&#21270;&#24046;&#24322;&#24182;&#20559;&#21521;&#32654;&#22269;&#25991;&#21270;&#12290;&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;&#25991;&#21270;&#24433;&#21709;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#24320;&#21457;&#26356;&#20855;&#25991;&#21270;&#36866;&#24212;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of ChatGPT has garnered widespread recognition for its exceptional ability to generate human-like responses in dialogue. Given its usage by users from various nations and its training on a vast multilingual corpus that incorporates diverse cultural and societal norms, it is crucial to evaluate its effectiveness in cultural adaptation. In this paper, we investigate the underlying cultural background of ChatGPT by analyzing its responses to questions designed to quantify human cultural differences. Our findings suggest that, when prompted with American context, ChatGPT exhibits a strong alignment with American culture, but it adapts less effectively to other cultural contexts. Furthermore, by using different prompts to probe the model, we show that English prompts reduce the variance in model responses, flattening out cultural differences and biasing them towards American culture. This study provides valuable insights into the cultural implications of ChatGPT and highl
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2303.17408</link><description>&lt;p&gt;
&#22522;&#20110;&#21307;&#23398;&#25552;&#31034;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#30340;&#21307;&#30103;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Medical Intervention Duration Estimation Using Language-enhanced Transformer Encoder with Medical Prompts. (arXiv:2303.17408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17408
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#22686;&#24378;Transformer&#32534;&#30721;&#22120;&#65292;&#24182;&#32467;&#21512;&#21307;&#23398;&#25552;&#31034;&#65292;&#23558;&#32467;&#26500;&#21270;&#12289;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#25968;&#25454;&#25237;&#24433;&#21040;&#19968;&#20010;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#30149;&#21382;(EHRs)&#20272;&#35745;&#21307;&#30103;&#24178;&#39044;&#30340;&#25345;&#32493;&#26102;&#38388;&#22312;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#39046;&#22495;&#24341;&#36215;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#20027;&#35201;&#20851;&#27880;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#24573;&#30053;&#20102;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#30340;&#20020;&#24202;&#33258;&#30001;&#25991;&#26412;&#25968;&#25454;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#35328;&#22686;&#24378;Transformer-based&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#21477;&#23376;&#32534;&#30721;&#22120;&#23558;&#25152;&#26377;&#30456;&#20851;&#30340;&#20020;&#24202;&#25968;&#25454;&#27169;&#24577;&#65288;&#36830;&#32493;&#12289;&#20998;&#31867;&#12289;&#20108;&#36827;&#21046;&#21644;&#33258;&#30001;&#25991;&#26412;&#29305;&#24449;&#65289;&#25237;&#24433;&#21040;&#19968;&#20010;&#21327;&#35843;&#30340;&#35821;&#35328;&#28508;&#31354;&#38388;&#20013;&#65292;&#20511;&#21161;&#21307;&#23398;&#25552;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#24471;&#19981;&#21516;&#27169;&#24577;&#30340;&#20449;&#24687;&#22312;&#21333;&#20803;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#20013;&#38598;&#25104;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#21307;&#23398;&#24178;&#39044;&#25345;&#32493;&#26102;&#38388;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#32654;&#22269;&#65288;ICU&#20303;&#38498;&#26102;&#38388;&#20272;&#35745;&#65289;&#21644;&#20122;&#27954;&#65288;&#25163;&#26415;&#25345;&#32493;&#26102;&#38388;&#39044;&#27979;&#65289;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, estimating the duration of medical intervention based on electronic health records (EHRs) has gained significant attention in the filed of clinical decision support. However, current models largely focus on structured data, leaving out information from the unstructured clinical free-text data. To address this, we present a novel language-enhanced transformer-based framework, which projects all relevant clinical data modalities (continuous, categorical, binary, and free-text features) into a harmonized language latent space using a pre-trained sentence encoder with the help of medical prompts. The proposed method enables the integration of information from different modalities within the cell transformer encoder and leads to more accurate duration estimation for medical intervention. Our experimental results on both US-based (length of stay in ICU estimation) and Asian (surgical duration prediction) medical datasets demonstrate the effectiveness of our proposed framewor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2303.17395</link><description>&lt;p&gt;
WavCaps: &#19968;&#31181;ChatGPT&#36741;&#21161;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#38899;&#39057;-&#35821;&#35328;&#22810;&#27169;&#24577;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
WavCaps: A ChatGPT-Assisted Weakly-Labelled Audio Captioning Dataset for Audio-Language Multimodal Research. (arXiv:2303.17395v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;WavCaps&#65292;&#21547;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#38899;&#39057;&#21098;&#36753;&#12290;&#20026;&#20811;&#26381;&#22122;&#22768;&#26631;&#27880;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;ChatGPT&#30340;&#19977;&#38454;&#27573;&#23383;&#24149;&#29983;&#25104;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38899;&#39057;-&#35821;&#35328;&#65288;AL&#65289;&#22810;&#27169;&#24577;&#23398;&#20064;&#20219;&#21153;&#30340;&#21457;&#23637;&#38750;&#24120;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;AL&#25968;&#25454;&#38598;&#25910;&#38598;&#36807;&#31243;&#26114;&#36149;&#36153;&#26102;&#65292;&#35268;&#27169;&#26377;&#38480;&#65292;&#32473;&#30740;&#31350;&#32773;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#25968;&#25454;&#31232;&#32570;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;WavCaps&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21253;&#21547;&#22823;&#32422;40&#19975;&#26465;&#24102;&#26377;&#37197;&#23545;&#23383;&#24149;&#30340;&#22823;&#35268;&#27169;&#24369;&#26631;&#27880;&#38899;&#39057;&#23383;&#24149;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20174;Web&#36164;&#28304;&#21644;&#22768;&#38899;&#20107;&#20214;&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#38899;&#39057;&#21098;&#36753;&#21450;&#21407;&#22987;&#25551;&#36848;&#12290;&#20294;&#26159;&#65292;&#22312;&#32447;&#25910;&#38598;&#21040;&#30340;&#21407;&#22987;&#25551;&#36848;&#38750;&#24120;&#22024;&#26434;&#65292;&#19981;&#36866;&#21512;&#29992;&#20110;&#33258;&#21160;&#21270;&#38899;&#39057;&#23383;&#24149;&#31561;&#20219;&#21153;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#22788;&#29702;&#27969;&#31243;&#65292;&#20197;&#36807;&#28388;&#22024;&#26434;&#25968;&#25454;&#24182;&#29983;&#25104;&#39640;&#36136;&#37327;&#23383;&#24149;&#65292;&#22312;&#20854;&#20013;&#21033;&#29992;&#20102;ChatGPT&#65292;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26469;&#33258;&#21160;&#36807;&#28388;&#21644;&#36716;&#25442;&#21407;&#22987;&#25551;&#36848;&#12290;&#25105;&#20204;&#23545;WavCaps&#30340;&#29305;&#24449;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of audio-language (AL) multimodal learning tasks has been significant in recent years. However, researchers face challenges due to the costly and time-consuming collection process of existing audio-language datasets, which are limited in size. To address this data scarcity issue, we introduce WavCaps, the first large-scale weakly-labelled audio captioning dataset, comprising approximately 400k audio clips with paired captions. We sourced audio clips and their raw descriptions from web sources and a sound event detection dataset. However, the online-harvested raw descriptions are highly noisy and unsuitable for direct use in tasks such as automated audio captioning. To overcome this issue, we propose a three-stage processing pipeline for filtering noisy data and generating high-quality captions, where ChatGPT, a large language model, is leveraged to filter and transform raw descriptions automatically. We conduct a comprehensive analysis of the characteristics of WavCaps 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26080;&#30417;&#30563;&#35821;&#27861;&#32416;&#38169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;GEC&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#22256;&#24785;&#24230;&#35780;&#20998;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#33778;&#24459;&#23486;&#35821;GEC&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.17367</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#26080;&#30417;&#30563;&#35821;&#27861;&#32416;&#38169;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A BERT-based Unsupervised Grammatical Error Correction Framework. (arXiv:2303.17367v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17367
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26080;&#30417;&#30563;&#35821;&#27861;&#32416;&#38169;&#26694;&#26550;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;GEC&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#22256;&#24785;&#24230;&#35780;&#20998;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#33778;&#24459;&#23486;&#35821;GEC&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#38169;&#35823;&#32416;&#27491;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#20013;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#29616;&#22312;&#24050;&#32463;&#26377;&#26356;&#22810;&#30340;&#23581;&#35797;&#22312;&#33521;&#35821;&#25110;&#27721;&#35821;&#31561;&#36890;&#29992;&#35821;&#35328;&#30340;&#39046;&#22495;&#36827;&#34892;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#22823;&#22411;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24037;&#20316;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#39046;&#22495;&#65292;&#30446;&#21069;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#30417;&#30563;GEC&#20855;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;&#20294;&#26159;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20173;&#38656;&#25506;&#32034;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#26080;&#30417;&#30563;GEC&#26694;&#26550;&#65292;&#23558;GEC&#35270;&#20026;&#22810;&#31867;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#21253;&#21547;&#19977;&#20010;&#27169;&#22359;&#65306;&#25968;&#25454;&#27969;&#26500;&#24314;&#27169;&#22359;&#12289;&#21477;&#23376;&#22256;&#24785;&#24230;&#35780;&#20998;&#27169;&#22359;&#21644;&#38169;&#35823;&#26816;&#27979;&#21644;&#32416;&#27491;&#27169;&#22359;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20266;&#22256;&#24785;&#24230;&#35780;&#20998;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#21477;&#23376;&#30340;&#21487;&#33021;&#27491;&#30830;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#33778;&#24459;&#23486;&#35821;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#36827;&#34892;&#33778;&#24459;&#23486;&#35821;GEC&#30740;&#31350;&#12290;&#35813;&#26041;&#27861;&#22312;&#25105;&#20204;&#26500;&#24314;&#30340;&#33778;&#24459;&#23486;&#35821;&#35821;&#26009;&#24211;&#19978;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#65292;&#24182;&#24050;&#22312;&#24320;&#28304;&#24179;&#21488;&#19978;&#20844;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grammatical error correction (GEC) is a challenging task of natural language processing techniques. While more attempts are being made in this approach for universal languages like English or Chinese, relatively little work has been done for low-resource languages for the lack of large annotated corpora. In low-resource languages, the current unsupervised GEC based on language model scoring performs well. However, the pre-trained language model is still to be explored in this context. This study proposes a BERT-based unsupervised GEC framework, where GEC is viewed as multi-class classification task. The framework contains three modules: data flow construction module, sentence perplexity scoring module, and error detecting and correcting module. We propose a novel scoring method for pseudo-perplexity to evaluate a sentence's probable correctness and construct a Tagalog corpus for Tagalog GEC research. It obtains competitive performance on the Tagalog corpus we construct and open-source 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#39064;&#25552;&#21462;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21253;&#21547;&#19981;&#24120;&#35265;&#35789;&#27719;&#30340;&#28508;&#22312;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22312;&#24178;&#25200;&#35789;&#35782;&#21035;&#20219;&#21153;&#19978;&#33719;&#24471;&#25509;&#36817;&#20110;&#20154;&#31867;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17324</link><description>&lt;p&gt;
&#12298;&#22534;&#26632;&#20013;&#30340;&#35805;&#39064;&#65306;&#36229;&#36234;&#36830;&#36143;&#24615;&#30340;&#20027;&#39064;&#25552;&#21462;&#21644;&#35780;&#20272;&#12299;
&lt;/p&gt;
&lt;p&gt;
Topics in the Haystack: Extracting and Evaluating Topics beyond Coherence. (arXiv:2303.17324v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17324
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#39064;&#25552;&#21462;&#26041;&#27861;&#65292;&#21487;&#20197;&#26816;&#27979;&#21253;&#21547;&#19981;&#24120;&#35265;&#35789;&#27719;&#30340;&#28508;&#22312;&#20027;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22312;&#24178;&#25200;&#35789;&#35782;&#21035;&#20219;&#21153;&#19978;&#33719;&#24471;&#25509;&#36817;&#20110;&#20154;&#31867;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#20174;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#21644;&#35782;&#21035;&#28508;&#22312;&#20027;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#22823;&#22810;&#25968;&#27169;&#22411;&#65292;&#26080;&#35770;&#26159;&#31867;&#20284;&#20110;&#28508;&#22312;&#29380;&#21033;&#20811;&#38647;&#20998;&#37197;&#65288;LDA&#65289;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#36824;&#26159;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#37117;&#36981;&#24490;&#20027;&#39064;&#21487;&#35299;&#37322;&#24615;&#21644;&#20027;&#39064;&#25552;&#21462;&#30340;&#30456;&#21516;&#22522;&#26412;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#23545;&#21477;&#23376;&#21644;&#25991;&#26723;&#20027;&#39064;&#30340;&#28145;&#21051;&#29702;&#35299;&#65292;&#24182;&#36229;&#36234;&#20102;&#23545;&#25968;&#25454;&#20013;&#21333;&#35789;&#39057;&#29575;&#30340;&#31616;&#21333;&#20998;&#26512;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#21040;&#21253;&#21547;&#19981;&#24120;&#35265;&#21333;&#35789;&#25110;&#26032;&#35789;&#30340;&#28508;&#22312;&#20027;&#39064;&#65292;&#20197;&#21450;&#19981;&#22312;&#25991;&#26723;&#26412;&#36523;&#20013;&#20986;&#29616;&#30340;&#21333;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#22522;&#20110;&#24178;&#25200;&#35789;&#21644;&#35821;&#20041;&#31354;&#38388;&#20013;&#30456;&#20284;&#24230;&#27979;&#37327;&#30340;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#20154;&#31867;&#24178;&#25200;&#35789;&#35782;&#21035;&#30340;&#30456;&#20851;&#31995;&#25968;&#65292;&#24182;&#22312;&#21333;&#35789;&#24178;&#25200;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#25509;&#36817;&#20110;&#20154;&#31867;&#27700;&#24179;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#22823;&#22411;&#22522;&#20934;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Extracting and identifying latent topics in large text corpora has gained increasing importance in Natural Language Processing (NLP). Most models, whether probabilistic models similar to Latent Dirichlet Allocation (LDA) or neural topic models, follow the same underlying approach of topic interpretability and topic extraction. We propose a method that incorporates a deeper understanding of both sentence and document themes, and goes beyond simply analyzing word frequencies in the data. This allows our model to detect latent topics that may include uncommon words or neologisms, as well as words not present in the documents themselves. Additionally, we propose several new evaluation metrics based on intruder words and similarity measures in the semantic space. We present correlation coefficients with human identification of intruder words and achieve near-human level results at the word-intrusion task. We demonstrate the competitive performance of our method with a large benchmark study,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;ChatGPT&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LM&#30340;&#31995;&#32479;&#65292;&#25506;&#31350;&#20102;&#23427;&#22312;&#21382;&#21490;&#25991;&#29486;&#20013;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23384;&#22312;&#22810;&#26041;&#38754;&#30340;&#32570;&#38519;&#65292;&#21253;&#25324;&#23454;&#20307;&#22797;&#26434;&#24615;&#21644;&#25552;&#31034;&#29305;&#23450;&#24615;&#31561;&#12290;</title><link>http://arxiv.org/abs/2303.17322</link><description>&lt;p&gt;
&#33021;&#21542;&#20351;&#29992;ChatGPT&#35782;&#21035;&#21382;&#21490;&#25991;&#29486;&#20013;&#30340;&#23454;&#20307;&#65311;
&lt;/p&gt;
&lt;p&gt;
Yes but.. Can ChatGPT Identify Entities in Historical Documents?. (arXiv:2303.17322v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;ChatGPT&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LM&#30340;&#31995;&#32479;&#65292;&#25506;&#31350;&#20102;&#23427;&#22312;&#21382;&#21490;&#25991;&#29486;&#20013;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20998;&#31867;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#23384;&#22312;&#22810;&#26041;&#38754;&#30340;&#32570;&#38519;&#65292;&#21253;&#25324;&#23454;&#20307;&#22797;&#26434;&#24615;&#21644;&#25552;&#31034;&#29305;&#23450;&#24615;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22810;&#24180;&#26469;&#19968;&#30452;&#34987;&#29992;&#20110;&#29616;&#20195;&#25991;&#26723;&#20013;&#30340;&#23454;&#20307;&#35782;&#21035;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26368;&#36817;&#20960;&#20010;&#26376;&#65292;&#20250;&#35805;&#20195;&#29702;ChatGPT&#30001;&#20110;&#20854;&#29983;&#25104;&#21548;&#36215;&#26469;&#21512;&#29702;&#30340;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#22312;&#31185;&#23398;&#30028;&#21644;&#20844;&#20247;&#20013;&#24341;&#36215;&#20102;&#24456;&#22810;&#20852;&#36259;&#12290;&#26412;&#25991;&#23581;&#35797;&#20197;&#38646;-shot&#30340;&#26041;&#24335;&#25506;&#31350;ChatGPT&#22312;&#19968;&#27425;&#24615;&#28304;&#65288;&#20363;&#22914;&#21382;&#21490;&#25253;&#32440;&#21644;&#21476;&#20856;&#27880;&#37322;&#65289;&#20013;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20998;&#31867;&#65288;NERC&#65289;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;LM&#30340;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#35782;&#21035;&#21382;&#21490;&#25991;&#26412;&#20013;&#30340;&#23454;&#20307;&#26041;&#38754;&#23384;&#22312;&#20960;&#20010;&#32570;&#38519;&#65292;&#21253;&#25324;&#23454;&#20307;&#27880;&#37322;&#20934;&#21017;&#30340;&#19968;&#33268;&#24615;&#12289;&#23454;&#20307;&#30340;&#22797;&#26434;&#24615;&#21644;&#20195;&#30721;&#20999;&#25442;&#20197;&#21450;&#25552;&#31034;&#30340;&#29305;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#21382;&#21490;&#26723;&#26696;&#23545;&#20844;&#20247;&#19981;&#21487;&#35775;&#38382;&#65288;&#22240;&#27492;&#26080;&#27861;&#22312;&#20114;&#32852;&#32593;&#19978;&#20351;&#29992;&#65289;&#65292;&#20063;&#24433;&#21709;&#20102;ChatGPT&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have been leveraged for several years now, obtaining state-of-the-art performance in recognizing entities from modern documents. For the last few months, the conversational agent ChatGPT has "prompted" a lot of interest in the scientific community and public due to its capacity of generating plausible-sounding answers. In this paper, we explore this ability by probing it in the named entity recognition and classification (NERC) task in primary sources (e.g., historical newspapers and classical commentaries) in a zero-shot manner and by comparing it with state-of-the-art LM-based systems. Our findings indicate several shortcomings in identifying entities in historical text that range from the consistency of entity annotation guidelines, entity complexity, and code-switching, to the specificity of prompting. Moreover, as expected, the inaccessibility of historical archives to the public (and thus on the Internet) also impacts its performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20154;&#31867;&#24605;&#32500;&#27169;&#24335;&#20013;&#30340;&#34920;&#29616;, &#36816;&#29992;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#20197;&#21450;ETR&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#32423;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;</title><link>http://arxiv.org/abs/2303.17276</link><description>&lt;p&gt;
&#22312;GPT&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#36755;&#20837;&#20154;&#31867;&#36755;&#20986;&#65306;&#35770;GPT&#26397;&#21521;&#24120;&#35782;&#30340;&#36235;&#21516;&#24615;
&lt;/p&gt;
&lt;p&gt;
Humans in Humans Out: On GPT Converging Toward Common Sense in both Success and Failure. (arXiv:2303.17276v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#22312;&#20154;&#31867;&#24605;&#32500;&#27169;&#24335;&#20013;&#30340;&#34920;&#29616;, &#36816;&#29992;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#20197;&#21450;ETR&#39044;&#27979;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#32423;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#35268;&#27169;&#21644;&#24494;&#35843;&#30340;&#22686;&#21152;&#20351;&#24471;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20363;&#22914;GPT&#30340;&#36755;&#20986;&#36136;&#37327;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#37492;&#20110;GPT-3&#21644;GPT-4&#37117;&#26159;&#20351;&#29992;&#22823;&#37327;&#30001;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#25105;&#20204;&#21487;&#20197;&#38382;&#20182;&#20204;&#30340;&#36755;&#20986;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21453;&#26144;&#20102;&#20154;&#31867;&#24605;&#32500;&#30340;&#27169;&#24335;&#65292;&#26080;&#35770;&#26159;&#27491;&#30830;&#36824;&#26159;&#38169;&#35823;&#30340;&#24773;&#20917;&#12290;&#35748;&#35782;&#35770;&#29702;&#35770;&#25552;&#20379;&#20102;&#20851;&#20110;&#20154;&#31867;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#31526;&#21495;&#29983;&#25104;&#27169;&#22411;&#65292;&#21253;&#25324;&#21629;&#39064;&#12289;&#37327;&#21270;&#12289;&#27010;&#29575;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#26412;&#25991;&#23558;ETR&#30340;&#26368;&#36817;&#19968;&#20010;&#20070;&#26412;&#30340;61&#20010;&#26680;&#24515;&#25512;&#29702;&#21644;&#21028;&#26029;&#38382;&#39064;&#25552;&#20379;&#32473;&#20102;GPT-3&#12289;GPT-3.5&#21644;GPT-4&#65292;&#36825;&#20123;&#38382;&#39064;&#21253;&#25324;&#32463;&#36807;&#23454;&#39564;&#35777;&#23454;&#30340;&#20154;&#31867;&#21028;&#26029;&#25968;&#25454;&#28857;&#21644;ETR&#39044;&#27979;&#30340;&#25968;&#25454;&#28857;&#65292;&#21516;&#26102;&#21253;&#21547;&#27491;&#30830;&#30340;&#25512;&#29702;&#27169;&#24335;&#21644;&#35884;&#35823;&#21644;&#26694;&#26550;&#25928;&#24212;&#65288;ETR61&#22522;&#20934;&#27979;&#35797;&#65289;&#12290; ETR61&#21253;&#25324;&#20102;Wason&#30340;&#21345;&#29260;&#20219;&#21153;&#12289;&#38169;&#35273;&#25512;&#29702;&#12289;&#35825;&#39285;&#25928;&#24212;&#31561;&#32463;&#20856;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Increase in computational scale and fine-tuning has seen a dramatic improvement in the quality of outputs of large language models (LLMs) like GPT. Given that both GPT-3 and GPT-4 were trained on large quantities of human-generated text, we might ask to what extent their outputs reflect patterns of human thinking, both for correct and incorrect cases. The Erotetic Theory of Reason (ETR) provides a symbolic generative model of both human success and failure in thinking, across propositional, quantified, and probabilistic reasoning, as well as decision-making. We presented GPT-3, GPT-3.5, and GPT-4 with 61 central inference and judgment problems from a recent book-length presentation of ETR, consisting of experimentally verified data-points on human judgment and extrapolated data-points predicted by ETR, with correct inference patterns as well as fallacies and framing effects (the ETR61 benchmark). ETR61 includes classics like Wason's card task, illusory inferences, the decoy effect, and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21253;&#21547;1.2TB&#25991;&#26412;&#30340;&#21271;&#27431;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#65292;&#20419;&#36827;&#20102;&#21271;&#27431;&#35821;&#35328;&#30340;LLMs&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.17183</link><description>&lt;p&gt;
&#12298;&#21271;&#27431;&#26681;&#26729;:&#19968;&#20010;1.2TB&#30340;&#21271;&#27431;&#35821;&#35328;&#24314;&#27169;&#25968;&#25454;&#38598;&#12299;
&lt;/p&gt;
&lt;p&gt;
The Nordic Pile: A 1.2TB Nordic Dataset for Language Modeling. (arXiv:2303.17183v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#20010;&#21253;&#21547;1.2TB&#25991;&#26412;&#30340;&#21271;&#27431;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#35201;&#36164;&#28304;&#65292;&#20419;&#36827;&#20102;&#21271;&#27431;&#35821;&#35328;&#30340;LLMs&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#38656;&#35201;&#28023;&#37327;&#30340;&#25991;&#26412;&#25968;&#25454;&#65292;LLMs&#30340;&#24615;&#33021;&#36890;&#24120;&#19982;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#21644;&#36136;&#37327;&#30456;&#20851;&#12290;&#36825;&#24847;&#21619;&#30528;&#22312;&#21271;&#27431;&#31561;&#35821;&#31181;&#20013;&#24314;&#31435;LLMs&#21487;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20854;&#25991;&#26412;&#35821;&#26009;&#24211;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#20419;&#36827;&#21271;&#27431;&#35821;&#35328;&#30340;LLMs&#24320;&#21457;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#20102;1.2TB&#30340;&#25991;&#26412;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#37325;&#35201;&#30340;&#21271;&#26085;&#32819;&#26364;&#35821;&#35328;&#65288;&#20025;&#40614;&#35821;&#12289;&#20912;&#23707;&#35821;&#12289;&#25386;&#23041;&#35821;&#21644;&#29790;&#20856;&#35821;&#65289;&#65292;&#20197;&#21450;&#19968;&#20123;&#39640;&#36136;&#37327;&#30340;&#33521;&#25991;&#25968;&#25454;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#25910;&#38598;&#12289;&#28165;&#29702;&#21644;&#36807;&#28388;&#35813;&#25968;&#25454;&#38598;&#30340;&#32771;&#34385;&#21644;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training Large Language Models (LLMs) require massive amounts of text data, and the performance of the LLMs typically correlates with the scale and quality of the datasets. This means that it may be challenging to build LLMs for smaller languages such as Nordic ones, where the availability of text corpora is limited. In order to facilitate the development of the LLMS in the Nordic languages, we curate a high-quality dataset consisting of 1.2TB of text, in all of the major North Germanic languages (Danish, Icelandic, Norwegian, and Swedish), as well as some high-quality English data. This paper details our considerations and processes for collecting, cleaning, and filtering the dataset.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;TreePiece&#25216;&#26415;&#23558;&#35299;&#26512;&#26641;&#20998;&#21106;&#25104;&#23376;&#26641;&#65292;&#20197;&#21152;&#36895;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#30340;&#36807;&#31243;&#65292;&#30456;&#27604;&#26631;&#20934;&#33258;&#22238;&#24402;&#25552;&#39640;&#20102;4.6&#20493;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#22312;&#36895;&#24230;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#20110;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.17161</link><description>&lt;p&gt;
TreePiece&#65306;&#36890;&#36807;&#26641;&#29366;&#20998;&#21106;&#25552;&#39640;&#35821;&#20041;&#35299;&#26512;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
TreePiece: Faster Semantic Parsing via Tree Tokenization. (arXiv:2303.17161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;TreePiece&#25216;&#26415;&#23558;&#35299;&#26512;&#26641;&#20998;&#21106;&#25104;&#23376;&#26641;&#65292;&#20197;&#21152;&#36895;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#30340;&#36807;&#31243;&#65292;&#30456;&#27604;&#26631;&#20934;&#33258;&#22238;&#24402;&#25552;&#39640;&#20102;4.6&#20493;&#30340;&#35299;&#30721;&#36895;&#24230;&#65292;&#24182;&#22312;&#36895;&#24230;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#24615;&#26356;&#39640;&#20110;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21253;&#25324;&#35821;&#20041;&#35299;&#26512;&#65288;&#19968;&#31181;&#23558;&#33258;&#28982;&#35821;&#35328;&#36716;&#25442;&#20026;&#26426;&#22120;&#21487;&#35835;&#30340;&#35299;&#26512;&#26641;&#30340;&#20219;&#21153;&#65289;&#12290;&#28982;&#32780;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#39034;&#24207;&#39044;&#27979;&#36807;&#31243;&#21487;&#33021;&#20250;&#24456;&#24930;&#12290;&#20026;&#20102;&#21152;&#36895;&#29992;&#20110;&#35821;&#20041;&#20998;&#26512;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#31216;&#20026;TreePiece&#65292;&#23427;&#23558;&#35299;&#26512;&#26641;&#20998;&#21106;&#25104;&#23376;&#26641;&#65292;&#24182;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#29983;&#25104;&#19968;&#20010;&#23376;&#26641;&#12290;&#22312;TopV2&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;TreePiece&#30340;&#35299;&#30721;&#36895;&#24230;&#27604;&#26631;&#20934;&#33258;&#22238;&#24402;&#24555;4.6&#20493;&#65292;&#27604;&#38750;&#33258;&#22238;&#24402;&#26041;&#27861;&#65288;NAR&#65289;&#30340;&#36895;&#24230;&#30456;&#24403;&#20294;&#20934;&#30830;&#24615;&#26174;&#30528;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive (AR) encoder-decoder neural networks have proved successful in many NLP problems, including Semantic Parsing -- a task that translates natural language to machine-readable parse trees. However, the sequential prediction process of AR models can be slow. To accelerate AR for semantic parsing, we introduce a new technique called TreePiece that tokenizes a parse tree into subtrees and generates one subtree per decoding step. On TopV2 benchmark, TreePiece shows 4.6 times faster decoding speed than standard AR, and comparable speed but significantly higher accuracy compared to Non-Autoregressive (NAR).
&lt;/p&gt;</description></item><item><title>TLAG&#26159;&#19968;&#31181;&#20449;&#24687;&#35302;&#21457;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#24341;&#23548;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#35302;&#21457;&#22120;&#21644;&#26631;&#31614;&#20449;&#24687;&#20197;&#21450;&#24341;&#20837;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#26469;&#20419;&#36827;&#22522;&#20110;&#23545;&#35805;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;</title><link>http://arxiv.org/abs/2303.17119</link><description>&lt;p&gt;
TLAG: &#19968;&#31181;&#20449;&#24687;&#35302;&#21457;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#24341;&#23548;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#23545;&#35805;&#30340;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
TLAG: An Informative Trigger and Label-Aware Knowledge Guided Model for Dialogue-based Relation Extraction. (arXiv:2303.17119v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17119
&lt;/p&gt;
&lt;p&gt;
TLAG&#26159;&#19968;&#31181;&#20449;&#24687;&#35302;&#21457;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#24341;&#23548;&#27169;&#22411;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#35302;&#21457;&#22120;&#21644;&#26631;&#31614;&#20449;&#24687;&#20197;&#21450;&#24341;&#20837;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#26469;&#20419;&#36827;&#22522;&#20110;&#23545;&#35805;&#30340;&#20851;&#31995;&#25277;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23545;&#35805;&#30340;&#20851;&#31995;&#25277;&#21462;&#65288;DRE&#65289;&#26088;&#22312;&#39044;&#27979;&#22312;&#23545;&#35805;&#20013;&#25552;&#21040;&#30340;&#21442;&#25968;&#23545;&#30340;&#20851;&#31995;&#31867;&#22411;&#12290;&#26368;&#26032;&#30340;&#35302;&#21457;&#22120;&#22686;&#24378;&#26041;&#27861;&#25552;&#20986;&#20102;&#35302;&#21457;&#22120;&#39044;&#27979;&#20219;&#21153;&#20197;&#20419;&#36827;DRE&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#35302;&#21457;&#22120;&#20449;&#24687;&#65292;&#29978;&#33267;&#20250;&#32473;&#20851;&#31995;&#25277;&#21462;&#24102;&#26469;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TLAG&#65292;&#23427;&#20805;&#20998;&#21033;&#29992;&#35302;&#21457;&#22120;&#21644;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#26469;&#24341;&#23548;&#20851;&#31995;&#25277;&#21462;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#35302;&#21457;&#22120;&#34701;&#21512;&#27169;&#22359;&#26469;&#20805;&#20998;&#21033;&#29992;&#35302;&#21457;&#22120;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#31614;&#24863;&#30693;&#30693;&#35782;&#65292;&#36827;&#19968;&#27493;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;DialogRE&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;TLAG&#20248;&#20110;&#22522;&#20934;&#27169;&#22411;&#65292;&#24182;&#19988;&#35814;&#32454;&#30340;&#20998;&#26512;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue-based Relation Extraction (DRE) aims to predict the relation type of argument pairs that are mentioned in dialogue. The latest trigger-enhanced methods propose trigger prediction tasks to promote DRE. However, these methods are not able to fully leverage the trigger information and even bring noise to relation extraction. To solve these problems, we propose TLAG, which fully leverages the trigger and label-aware knowledge to guide the relation extraction. First, we design an adaptive trigger fusion module to fully leverage the trigger information. Then, we introduce label-aware knowledge to further promote our model's performance. Experimental results on the DialogRE dataset show that our TLAG outperforms the baseline models, and detailed analyses demonstrate the effectiveness of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DERA&#30340;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;&#12290;DERA&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.17071</link><description>&lt;p&gt;
DERA: &#20351;&#29992;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents. (arXiv:2303.17071v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DERA&#30340;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#21033;&#29992;LLM&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#34917;&#20840;&#33021;&#21147;&#12290;DERA&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65292;&#21487;&#20197;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#23454;&#29616;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#25104;&#20026;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#12290;&#22312;&#21307;&#30103;&#31561;&#20851;&#20046;&#23433;&#20840;&#24615;&#30340;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#23454;&#29992;&#24615;&#21462;&#20915;&#20110;&#23427;&#20204;&#29983;&#25104;&#30340;&#36755;&#20986;&#26159;&#21542;&#20855;&#22791;&#20107;&#23454;&#20934;&#30830;&#21644;&#23436;&#25972;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#35805;&#22411;&#35299;&#20915;&#20195;&#29702;&#65288;DERA&#65289;&#12290;DERA&#26159;&#19968;&#31181;&#30001;LLM&#65288;&#29305;&#21035;&#26159;GPT-4&#65289;&#30340;&#23545;&#35805;&#33021;&#21147;&#25552;&#20379;&#25903;&#25345;&#30340;&#27169;&#24335;&#65292;&#23427;&#20026;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#35770;&#22363;&#65292;&#29992;&#20110;&#27807;&#36890;&#21453;&#39304;&#24182;&#36845;&#20195;&#25913;&#36827;&#36755;&#20986;&#12290;&#25105;&#20204;&#23558;&#23545;&#35805;&#26694;&#26550;&#21270;&#20026;&#20004;&#20010;&#20195;&#29702;&#31867;&#22411;&#20043;&#38388;&#30340;&#35752;&#35770;&#65306;&#19968;&#20010;&#30740;&#31350;&#20154;&#21592;&#65292;&#36127;&#36131;&#22788;&#29702;&#20449;&#24687;&#24182;&#35782;&#21035;&#20851;&#38190;&#38382;&#39064;&#32452;&#20214;&#65307;&#20197;&#21450;&#19968;&#20010;&#20915;&#31574;&#32773;&#65292;&#20855;&#26377;&#23558;&#30740;&#31350;&#20154;&#21592;&#30340;&#20449;&#24687;&#38598;&#25104;&#24182;&#23545;&#26368;&#32456;&#36755;&#20986;&#20570;&#20986;&#21028;&#23450;&#30340;&#33258;&#20027;&#26435;&#12290;&#25105;&#20204;&#23558;DERA&#29992;&#20110;&#19977;&#20010;&#20020;&#24202;&#30456;&#20851;&#20219;&#21153;&#30340;&#27979;&#35797;&#12290;&#22312;&#21307;&#30103;&#23545;&#35805;&#25688;&#35201;&#21644;&#25252;&#29702;&#35745;&#21010;&#29983;&#25104;&#26041;&#38754;&#65292;DERA&#26174;&#31034;&#20986;&#27604;&#22522;&#30784;GPT-4&#24615;&#33021;&#26174;&#33879;&#25552;&#39640;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as valuable tools for many natural language understanding tasks. In safety-critical applications such as healthcare, the utility of these models is governed by their ability to generate outputs that are factually accurate and complete. In this work, we present dialog-enabled resolving agents (DERA). DERA is a paradigm made possible by the increased conversational abilities of LLMs, namely GPT-4. It provides a simple, interpretable forum for models to communicate feedback and iteratively improve output. We frame our dialog as a discussion between two agent types - a Researcher, who processes information and identifies crucial problem components, and a Decider, who has the autonomy to integrate the Researcher's information and makes judgments on the final output.  We test DERA against three clinically-focused tasks. For medical conversation summarization and care plan generation, DERA shows significant improvement over the base GPT-4 performance 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#35299;&#30721;&#31639;&#27861;&#26159;&#21542;&#36981;&#24490;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#65288;&#23558;&#20449;&#24687;&#22343;&#21248;&#20998;&#37197;&#22312;&#35805;&#35821;&#20013;&#65289;&#12290;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#24212;&#27604;&#20154;&#30340;&#22238;&#24212;&#26356;&#21152;&#36981;&#24490;&#35813;&#21407;&#21017;&#65292;&#20419;&#36827;&#35813;&#21407;&#21017;&#30340;&#35299;&#30721;&#31639;&#27861;&#24182;&#27809;&#26377;&#25552;&#39640;&#22238;&#24212;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20449;&#24687;&#23494;&#24230;&#30340;&#19981;&#22343;&#21248;&#24615;&#19982;&#24778;&#22855;&#24230;&#38750;&#24120;&#20302;/&#39640;&#30340;&#22238;&#24212;&#30340;&#36136;&#37327;&#30456;&#20851;&#65292;&#40723;&#21169;&#38750;&#22343;&#21248;&#22238;&#24212;&#26159;&#8220;&#21487;&#33021;&#24615;&#38519;&#38449;&#8221;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.17006</link><description>&lt;p&gt;
&#35299;&#30721;&#31639;&#27861;&#22312;&#23545;&#35805;&#22238;&#24212;&#20013;&#22914;&#20309;&#20998;&#21457;&#20449;&#24687;&#65311;
&lt;/p&gt;
&lt;p&gt;
How do decoding algorithms distribute information in dialogue responses?. (arXiv:2303.17006v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#23545;&#35805;&#29983;&#25104;&#20013;&#35299;&#30721;&#31639;&#27861;&#26159;&#21542;&#36981;&#24490;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#21407;&#21017;&#65288;&#23558;&#20449;&#24687;&#22343;&#21248;&#20998;&#37197;&#22312;&#35805;&#35821;&#20013;&#65289;&#12290;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#24212;&#27604;&#20154;&#30340;&#22238;&#24212;&#26356;&#21152;&#36981;&#24490;&#35813;&#21407;&#21017;&#65292;&#20419;&#36827;&#35813;&#21407;&#21017;&#30340;&#35299;&#30721;&#31639;&#27861;&#24182;&#27809;&#26377;&#25552;&#39640;&#22238;&#24212;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20449;&#24687;&#23494;&#24230;&#30340;&#19981;&#22343;&#21248;&#24615;&#19982;&#24778;&#22855;&#24230;&#38750;&#24120;&#20302;/&#39640;&#30340;&#22238;&#24212;&#30340;&#36136;&#37327;&#30456;&#20851;&#65292;&#40723;&#21169;&#38750;&#22343;&#21248;&#22238;&#24212;&#26159;&#8220;&#21487;&#33021;&#24615;&#38519;&#38449;&#8221;&#38382;&#39064;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20542;&#21521;&#20110;&#36981;&#24490;&#22343;&#21248;&#20449;&#24687;&#23494;&#24230;&#65288;UID&#65289;&#21407;&#21017;&#65292;&#22312;&#35805;&#35821;&#20013;&#22343;&#21248;&#20998;&#37197;&#20449;&#24687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#35299;&#30721;&#31639;&#27861;&#26159;&#21542;&#20250;&#38544;&#24335;&#22320;&#36981;&#24490;UID&#21407;&#21017;&#65292;&#24182;&#22312;&#20160;&#20040;&#26465;&#20214;&#19979;&#36981;&#23432;UID&#21487;&#33021;&#23545;&#23545;&#35805;&#29983;&#25104;&#26377;&#30410;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35299;&#30721;&#31639;&#27861;&#22312;Persona-Chat&#25968;&#25454;&#38598;&#19978;&#29983;&#25104;&#22238;&#24212;&#65292;&#24182;&#20351;&#29992;Amazon Mechanical Turk&#25910;&#38598;&#20154;&#31867;&#23545;&#23427;&#20204;&#36136;&#37327;&#30340;&#21028;&#26029;&#12290;&#25105;&#20204;&#21457;&#29616;&#65288;i&#65289;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#24212;&#27604;&#20154;&#30340;&#22238;&#24212;&#26356;&#21152;&#36981;&#24490;UID&#21407;&#21017;&#65292;&#20197;&#21450;&#65288;ii&#65289;&#20419;&#36827;UID&#30340;&#35299;&#30721;&#31639;&#27861;&#24182;&#27809;&#26377;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#22238;&#24212;&#12290;&#30456;&#21453;&#65292;&#24403;&#25105;&#20204;&#25511;&#21046;&#24778;&#22855;&#24230;&#26102;&#65292;&#20449;&#24687;&#23494;&#24230;&#30340;&#19981;&#22343;&#21248;&#24615;&#19982;&#24778;&#22855;&#24230;&#38750;&#24120;&#20302;/&#39640;&#30340;&#22238;&#24212;&#30340;&#36136;&#37327;&#30456;&#20851;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#40723;&#21169;&#38750;&#22343;&#21248;&#22238;&#24212;&#26159;&#8220;&#21487;&#33021;&#24615;&#38519;&#38449;&#8221;&#38382;&#39064;&#65288;&#38750;&#24120;&#39640;&#21487;&#33021;&#24615;&#25991;&#26412;&#30340;&#36136;&#37327;&#19979;&#38477;&#65289;&#30340;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#25324;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Humans tend to follow the Uniform Information Density (UID) principle by distributing information evenly in utterances. We study if decoding algorithms implicitly follow this UID principle, and under what conditions adherence to UID might be desirable for dialogue generation. We generate responses using different decoding algorithms with GPT-2 on the Persona-Chat dataset and collect human judgments on their quality using Amazon Mechanical Turk. We find that (i) surprisingly, model-generated responses follow the UID principle to a greater extent than human responses, and (ii) decoding algorithms that promote UID do not generate higher-quality responses. Instead, when we control for surprisal, non-uniformity of information density correlates with the quality of responses with very low/high surprisal. Our findings indicate that encouraging non-uniform responses is a potential solution to the ``likelihood trap'' problem (quality degradation in very high-likelihood text). Our dataset contai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;GPT-4&#19982;Chain-of-Thought&#25552;&#31034;&#32467;&#21512;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;2022&#24180;&#32771;&#35797;&#20013;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;87&#65285;&#12290;</title><link>http://arxiv.org/abs/2303.17003</link><description>&lt;p&gt;
&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;GPT-3.5&#21644;GPT-4&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3.5 and GPT-4 Models on Brazilian University Admission Exams. (arXiv:2303.17003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24052;&#35199;&#22823;&#23398;&#20837;&#23398;&#32771;&#35797;&#20013;&#35780;&#20272;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#65292;&#20998;&#26512;&#20102;&#19981;&#21516;&#25552;&#31034;&#31574;&#30053;&#65292;&#26368;&#32456;&#21457;&#29616;GPT-4&#19982;Chain-of-Thought&#25552;&#31034;&#32467;&#21512;&#34920;&#29616;&#26368;&#22909;&#65292;&#22312;2022&#24180;&#32771;&#35797;&#20013;&#20934;&#30830;&#29575;&#36798;&#21040;&#20102;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#24212;&#23545;&#39640;&#39118;&#38505;&#30340;&#22810;&#39033;&#36873;&#25321;&#27979;&#35797;&#20013;&#30340;&#33021;&#21147;&#65292;&#36825;&#37324;&#20197;&#24052;&#35199;&#22823;&#23398;&#24191;&#27867;&#37319;&#29992;&#30340;&#22810;&#23398;&#31185;&#20837;&#23398;&#32771;&#35797;Exame Nacional do Ensino M&#233;dio&#65288;ENEM&#65289;&#20026;&#20363;&#12290;&#35813;&#32771;&#35797;&#23545;LMs&#25552;&#20986;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#38382;&#39064;&#21487;&#33021;&#28041;&#21450;&#22810;&#20010;&#30693;&#35782;&#39046;&#22495;&#65292;&#38656;&#35201;&#29702;&#35299;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#38382;&#39064;&#21487;&#33021;&#38656;&#35201;&#29702;&#35299;&#32479;&#35745;&#23398;&#21644;&#29983;&#29289;&#23398;&#25165;&#33021;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#21644;GPT-4&#27169;&#22411;&#23545;2009&#24180;&#33267;2017&#24180;&#32771;&#35797;&#20197;&#21450;2022&#24180;&#20844;&#24320;&#30340;&#32771;&#35797;&#38382;&#39064;&#30340;&#21709;&#24212;&#12290;&#27492;&#22806;&#65292;&#36824;&#27979;&#35797;&#20102;&#19981;&#21516;&#30340;&#25552;&#31034;&#31574;&#30053;&#65292;&#21253;&#25324;&#20351;&#29992;Chain-of-Thought&#65288;CoT&#65289;&#25552;&#31034;&#29983;&#25104;&#31572;&#26696;&#30340;&#35299;&#37322;&#12290;&#22312;2022&#24180;&#30340;&#32771;&#35797;&#20013;&#65292;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#26159;GPT-4&#24182;&#20351;&#29992;&#20102;CoT&#65292;&#22312;&#20934;&#30830;&#29575;&#26041;&#38754;&#36798;&#21040;&#20102;87&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
The present study aims to explore the capabilities of Language Models (LMs) in tackling high-stakes multiple-choice tests, represented here by the Exame Nacional do Ensino M\'edio (ENEM), a multidisciplinary entrance examination widely adopted by Brazilian universities. This exam poses challenging tasks for LMs, since its questions may span into multiple fields of knowledge, requiring understanding of information from diverse domains. For instance, a question may require comprehension of both statistics and biology to be solved. This work analyzed responses generated by GPT-3.5 and GPT-4 models for questions presented in the 2009-2017 exams, as well as for questions of the 2022 exam, which were made public after the training of the models was completed. Furthermore, different prompt strategies were tested, including the use of Chain-of-Thought (CoT) prompts to generate explanations for answers. On the 2022 edition, the best-performing model, GPT-4 with CoT, achieved an accuracy of 87%,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;: ContraSim&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ContraSim&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#33719;&#24471;&#20102;&#27604;&#20043;&#21069;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16992</link><description>&lt;p&gt;
ContraSim -- &#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ContraSim -- A Similarity Measure Based on Contrastive Learning. (arXiv:2303.16992v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;: ContraSim&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;ContraSim&#22312;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#22343;&#33719;&#24471;&#20102;&#27604;&#20043;&#21069;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#26377;&#30740;&#31350;&#36890;&#36807;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#20998;&#26512;&#27604;&#36739;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#38754;&#65288;&#22914;&#26550;&#26500;&#12289;&#35757;&#32451;&#25968;&#25454;&#31561;&#65289;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#30340;&#20869;&#37096;&#34920;&#31034;&#12290;&#30456;&#20284;&#24230;&#37327;&#30340;&#36136;&#37327;&#36890;&#24120;&#36890;&#36807;&#20854;&#22312;&#39044;&#26399;&#21305;&#37197;&#30340;&#34920;&#31034;&#20013;&#20998;&#37197;&#39640;&#20998;&#25968;&#30340;&#25104;&#21151;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30456;&#20284;&#24230;&#37327;&#22312;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24179;&#24248;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30456;&#20284;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ContraSim&#65292;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#65292;&#19982;&#24120;&#35265;&#30340;&#38381;&#24335;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#21516;&#65292;ContraSim&#20351;&#29992;&#30456;&#20284;&#21644;&#19981;&#30456;&#20284;&#30340;&#31034;&#20363;&#26469;&#23398;&#20064;&#21442;&#25968;&#21270;&#30340;&#24230;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#30340;&#22270;&#23618;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;&#21644;&#25105;&#20204;&#20171;&#32461;&#30340;&#20004;&#20010;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#65306;&#22810;&#35821;&#35328;&#22522;&#20934;&#27979;&#35797;&#21644;&#22270;&#20687;&#23383;&#24149;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#65292;ContraSim&#30340;&#20934;&#30830;&#24615;&#37117;&#27604;&#20043;&#21069;&#30340;&#30456;&#20284;&#24230;&#37327;&#26041;&#27861;&#39640;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has compared neural network representations via similarity-based analyses, shedding light on how different aspects (architecture, training data, etc.) affect models' internal representations. The quality of a similarity measure is typically evaluated by its success in assigning a high score to representations that are expected to be matched. However, existing similarity measures perform mediocrely on standard benchmarks. In this work, we develop a new similarity measure, dubbed ContraSim, based on contrastive learning. In contrast to common closed-form similarity measures, ContraSim learns a parameterized measure by using both similar and dissimilar examples. We perform an extensive experimental evaluation of our method, with both language and vision models, on the standard layer prediction benchmark and two new benchmarks that we introduce: the multilingual benchmark and the image-caption benchmark. In all cases, ContraSim achieves much higher accuracy than previous simila
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#37319;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#31561;&#20302;&#35745;&#31639;&#26041;&#27861;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;NLP&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23454;&#39564;&#24471;&#20986;&#20102;&#23454;&#29616;&#21487;&#27604;&#24615;&#33021;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2303.16985</link><description>&lt;p&gt;
&#36866;&#24212;&#20302;&#36164;&#28304;&#21452;&#36830;&#36890;&#24615;: &#25506;&#31350;&#23545;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#37319;&#29992;&#20302;&#35745;&#31639;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adapting to the Low-Resource Double-Bind: Investigating Low-Compute Methods on Low-Resource African Languages. (arXiv:2303.16985v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#65292;&#37319;&#29992;&#35821;&#35328;&#36866;&#37197;&#22120;&#31561;&#20302;&#35745;&#31639;&#26041;&#27861;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;NLP&#30740;&#31350;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#23454;&#39564;&#24471;&#20986;&#20102;&#23454;&#29616;&#21487;&#27604;&#24615;&#33021;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#35768;&#22810;&#20219;&#21153;&#37117;&#20351;&#29992;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#12290;&#28982;&#32780;&#65292;&#38750;&#27954;&#35821;&#35328;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#39640;&#35745;&#31639;&#36164;&#28304;&#30340;&#33719;&#21462;&#38480;&#21046;&#20102;&#23545;&#36825;&#20123;&#35821;&#35328;&#36827;&#34892;&#30740;&#31350;&#30340;&#21487;&#33021;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#36825;&#31181;&#20302;&#36164;&#28304;&#21452;&#36830;&#36890;&#24615;&#32972;&#26223;&#19979;&#65292;&#35821;&#35328;&#36866;&#37197;&#22120;&#31561;&#20302;&#35745;&#31639;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#35797;&#22270;&#22238;&#31572;&#20197;&#19979;&#38382;&#39064;&#65306;&#35821;&#35328;&#36866;&#37197;&#22120;&#26159;&#21542;&#20801;&#35768;&#37027;&#20123;&#22312;&#25968;&#25454;&#21644;&#35745;&#31639;&#26041;&#38754;&#21452;&#37325;&#21463;&#38480;&#30340;&#20154;&#23454;&#38469;&#19978;&#26500;&#24314;&#26377;&#29992;&#30340;&#27169;&#22411;&#65311;&#36890;&#36807;&#22312;&#38750;&#27954;&#35821;&#35328;&#19978;&#36827;&#34892;&#24494;&#35843;&#23454;&#39564;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#20316;&#20026;&#20302;&#36164;&#28304;&#38750;&#27954;NLP&#30340;&#25104;&#26412;&#25928;&#30410;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20351;&#29992;&#20840;&#37096;&#20813;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#35745;&#31639;&#36164;&#28304;&#25439;&#32791;&#22823;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30456;&#27604;&#65292;&#35821;&#35328;&#36866;&#37197;&#22120;&#23454;&#29616;&#20102;&#21487;&#27604;&#30340;&#24615;&#33021;&#12290;&#36825;&#20026;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#21644;&#25506;&#32034;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many natural language processing (NLP) tasks make use of massively pre-trained language models, which are computationally expensive. However, access to high computational resources added to the issue of data scarcity of African languages constitutes a real barrier to research experiments on these languages. In this work, we explore the applicability of low-compute approaches such as language adapters in the context of this low-resource double-bind. We intend to answer the following question: do language adapters allow those who are doubly bound by data and compute to practically build useful models? Through fine-tuning experiments on African languages, we evaluate their effectiveness as cost-effective approaches to low-resource African NLP. Using solely free compute resources, our results show that language adapters achieve comparable performances to massive pre-trained language models which are heavy on computational resources. This opens the door to further experimentation and explor
&lt;/p&gt;</description></item><item><title>BEVERS&#26159;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;FEVER&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#25972;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#31616;&#21333;&#24615;&#21644;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;FEVER&#21644;Scifact&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#26631;&#31614;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16974</link><description>&lt;p&gt;
BEVERS: &#19968;&#20010;&#36890;&#29992;&#12289;&#31616;&#21333;&#19988;&#39640;&#24615;&#33021;&#30340;&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
BEVERS: A General, Simple, and Performant Framework for Automatic Fact Verification. (arXiv:2303.16974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16974
&lt;/p&gt;
&lt;p&gt;
BEVERS&#26159;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;FEVER&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#25972;&#30340;&#22522;&#20934;&#31995;&#32479;&#65292;&#20855;&#26377;&#36890;&#29992;&#24615;&#12289;&#31616;&#21333;&#24615;&#21644;&#39640;&#24615;&#33021;&#30340;&#29305;&#28857;&#65292;&#24182;&#22312;FEVER&#21644;Scifact&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#39640;&#30340;&#26631;&#31614;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20107;&#23454;&#39564;&#35777;&#36817;&#24180;&#26469;&#25104;&#20026;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#20854;&#20013;Fact Extraction and VERification&#65288;FEVER&#65289;&#25968;&#25454;&#38598;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BEVERS&#65292;&#19968;&#20010;&#29305;&#21035;&#38024;&#23545;FEVER&#25968;&#25454;&#38598;&#36827;&#34892;&#35843;&#25972;&#30340;&#22522;&#20934;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#27969;&#31243;&#20351;&#29992;&#20102;&#26631;&#20934;&#30340;&#25991;&#26723;&#26816;&#32034;&#12289;&#21477;&#23376;&#36873;&#25321;&#21644;&#26368;&#32456;&#22768;&#26126;&#20998;&#31867;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#33457;&#36153;&#20102;&#30456;&#24403;&#22810;&#30340;&#21147;&#27668;&#26469;&#30830;&#20445;&#27599;&#20010;&#32452;&#20214;&#30340;&#26368;&#20339;&#24615;&#33021;&#12290;&#32467;&#26524;&#26159;&#65292;BEVERS &#22312;&#25152;&#26377;&#24050;&#21457;&#24067;&#21644;&#26410;&#21457;&#24067;&#30340;&#25991;&#29486;&#20013;&#23454;&#29616;&#20102;&#26368;&#39640;&#30340;FEVER&#24471;&#20998;&#21644;&#26631;&#31614;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#23558;&#27492;&#27969;&#31243;&#24212;&#29992;&#20110;&#21478;&#19968;&#20010;&#20107;&#23454;&#39564;&#35777;&#25968;&#25454;&#38598;Scifact&#19978;&#65292;&#24182;&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25152;&#26377;&#31995;&#32479;&#20013;&#26368;&#39640;&#30340;&#26631;&#31614;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic fact verification has become an increasingly popular topic in recent years and among datasets the Fact Extraction and VERification (FEVER) dataset is one of the most popular. In this work we present BEVERS, a tuned baseline system for the FEVER dataset. Our pipeline uses standard approaches for document retrieval, sentence selection, and final claim classification, however, we spend considerable effort ensuring optimal performance for each component. The results are that BEVERS achieves the highest FEVER score and label accuracy among all systems, published or unpublished. We also apply this pipeline to another fact verification dataset, Scifact, and achieve the highest label accuracy among all systems on that dataset as well. We also make our full code available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.16839</link><description>&lt;p&gt;
MaMMUT: &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#32852;&#21512;&#23398;&#20064;&#30340;&#31616;&#21333;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16839
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaMMUT&#30340;&#31616;&#21333;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#65292;&#24182;&#22312;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26102;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#25928;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#24050;&#20174;&#32534;&#30721;-&#35299;&#30721;&#36716;&#21521;&#20165;&#35299;&#30721;&#30340;&#35774;&#35745;&#12290;&#27492;&#22806;&#65292;&#26222;&#36941;&#35748;&#20026;&#65292;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#65292;&#29983;&#25104;&#20219;&#21153;&#21644;&#23545;&#27604;&#20219;&#21153;&#65292;&#24448;&#24448;&#20114;&#30456;&#20914;&#31361;&#65292;&#38590;&#20197;&#22312;&#19968;&#20010;&#26550;&#26500;&#20013;&#23481;&#32435;&#65292;&#24182;&#36827;&#19968;&#27493;&#38656;&#35201;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#22797;&#26434;&#35843;&#25972;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22521;&#35757;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#20165;&#35299;&#30721;&#27169;&#22411;&#65292;&#36825;&#22312;&#32852;&#21512;&#23398;&#20064;&#36825;&#20123;&#19981;&#21516;&#30340;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#20010;&#31616;&#21333;&#30340;&#27169;&#22411;MaMMUT&#23454;&#29616;&#30340;&#12290;&#23427;&#30001;&#21333;&#19968;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#25991;&#26412;&#35299;&#30721;&#22120;&#32452;&#25104;&#65292;&#24182;&#33021;&#22815;&#36890;&#36807;&#25991;&#26412;&#35299;&#30721;&#22120;&#19978;&#30340;&#26032;&#30340;&#20004;&#27493;&#26041;&#27861;&#23481;&#32435;&#23545;&#27604;&#21644;&#29983;&#25104;&#23398;&#20064;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#19981;&#21516;&#30446;&#26631;&#20219;&#21153;&#30340;&#32852;&#21512;&#35757;&#32451;&#26159;&#31616;&#21333;&#30340;&#65292;&#26377;&#25928;&#30340;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#27169;&#22411;&#30340;&#26435;&#37325;&#20849;&#20139;&#12290;&#27492;&#22806;&#65292;&#30456;&#21516;&#30340;&#26550;&#26500;&#20351;&#24471;&#23545;&#24320;&#25918;&#35789;&#27719;&#23545;&#35937;&#26816;&#27979;&#30340;&#31616;&#21333;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of language models have moved from encoder-decoder to decoder-only designs. In addition, the common knowledge has it that the two most popular multimodal tasks, the generative and contrastive tasks, tend to conflict with one another, are hard to accommodate in one architecture, and further need complex adaptations for downstream tasks. We propose a novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks. This is done with a simple model, called MaMMUT. It consists of a single vision encoder and a text decoder, and is able to accommodate contrastive and generative learning by a novel two-pass approach on the text decoder. We demonstrate that joint training of these diverse-objective tasks is simple, effective, and maximizes the weight-sharing of the model. Furthermore, the same architecture enables straightforward extensions to open-vocabulary object detection 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21508;&#31181;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#32467;&#26500;&#21270;&#39046;&#22495;&#32570;&#20047;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#19988;&#20449;&#24687;&#25552;&#21462;&#38590;&#20197;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14956</link><description>&lt;p&gt;
&#36890;&#36807;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32479;&#19968;&#25991;&#26412;&#32467;&#26500;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unified Text Structuralization with Instruction-tuned Language Models. (arXiv:2303.14956v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21508;&#31181;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25991;&#26412;&#32467;&#26500;&#21270;&#39046;&#22495;&#32570;&#20047;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#19988;&#20449;&#24687;&#25552;&#21462;&#38590;&#20197;&#25512;&#24191;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32467;&#26500;&#21270;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20449;&#24687;&#25552;&#21462;&#21644;&#32467;&#26500;&#24418;&#24335;&#21270;&#30340;&#37325;&#35201;&#39046;&#22495;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#25991;&#26412;&#32467;&#26500;&#21270;&#30740;&#31350;&#32570;&#20047;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#21644;&#35821;&#35328;&#30340;&#39640;&#36136;&#37327;&#25163;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#37117;&#38024;&#23545;&#29305;&#23450;&#31867;&#22411;&#30340;&#32467;&#26500;&#21270;&#25968;&#25454;&#35774;&#35745;&#65292;&#20363;&#22914;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#20107;&#20214;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#21508;&#31181;&#32467;&#26500;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#22312;&#23558;&#25991;&#26412;&#39304;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#21069;&#65292;&#28155;&#21152;&#21069;&#32512;&#21644;&#21518;&#32512;&#25351;&#20196;&#65292;&#20197;&#25351;&#31034;&#25152;&#38656;&#35201;&#30340;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#21644;&#32467;&#26500;&#31867;&#22411;&#12290;&#22312;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#20013;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#35821;&#35328;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text structuralization is one of the important fields of natural language processing (NLP) consists of information extraction (IE) and structure formalization. However, current studies of text structuralization suffer from a shortage of manually annotated high-quality datasets from different domains and languages, which require specialized professional knowledge. In addition, most IE methods are designed for a specific type of structured data, e.g., entities, relations, and events, making them hard to generalize to others. In this work, we propose a simple and efficient approach to instruct large language model (LLM) to extract a variety of structures from texts. More concretely, we add a prefix and a suffix instruction to indicate the desired IE task and structure type, respectively, before feeding the text into a LLM. Experiments on two LLMs show that this approach can enable language models to perform comparable with other state-of-the-art methods on datasets of a variety of languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09419</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#65306;&#20174;BERT&#21040;ChatGPT&#30340;&#21382;&#31243;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. (arXiv:2302.09419v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09419
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#21644;&#21457;&#23637;&#21382;&#31243;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#21516;&#26102;&#65292;&#35752;&#35770;&#20102;&#20854;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;(PFMs)&#34987;&#35748;&#20026;&#26159;&#21508;&#31181;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19979;&#28216;&#20219;&#21153;&#30340;&#22522;&#30784;&#12290;PFM(&#20363;&#22914;BERT&#12289;ChatGPT&#21644;GPT-4)&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#25552;&#20379;&#20102;&#21512;&#29702;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#12290;BERT&#20174;&#36716;&#25442;&#22120;&#20013;&#23398;&#20064;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#31867;&#20284;&#22320;&#65292;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;(GPT)&#26041;&#27861;&#37319;&#29992;&#36716;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#24182;&#37319;&#29992;&#33258;&#22238;&#24402;&#33539;&#24335;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#36817;&#65292;ChatGPT&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#23637;&#29616;&#20102;&#20196;&#20154;&#20852;&#22859;&#30340;&#25104;&#21151;&#65292;&#23427;&#37319;&#29992;&#33258;&#22238;&#24402;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#38646;&#23556;&#20987;&#25110;&#23569;&#23556;&#20987;&#25552;&#31034;&#12290;PFM&#30340;&#21331;&#36234;&#25104;&#23601;&#20026;&#21508;&#31181;AI&#39046;&#22495;&#24102;&#26469;&#20102;&#37325;&#22823;&#31361;&#30772;&#12290;&#35768;&#22810;&#30740;&#31350;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#23545;&#26356;&#26032;&#35843;&#26597;&#30340;&#38656;&#27714;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;PFMs&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#23427;&#20204;&#30340;&#26550;&#26500;&#12289;&#22521;&#35757;&#30446;&#26631;&#12289;&#39044;&#22521;&#35757;&#20219;&#21153;&#12289;&#24494;&#35843;&#31574;&#30053;&#21644;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;PFMs&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#28508;&#22312;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of rec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.07267</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24515;&#29702;&#23398;&#20013;&#30340;&#8220;&#27491;&#30830;&#31572;&#26696;&#8221;
&lt;/p&gt;
&lt;p&gt;
"Correct answers" from the psychology of artificial intelligence. (arXiv:2302.07267v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;OpenAI&#30340;GPT3.5&#27169;&#22411;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#65292;&#20854;&#20013;8&#39033;&#30740;&#31350;&#30340;&#32467;&#26524;&#34987;&#25104;&#21151;&#22797;&#21046;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#21097;&#19979;&#30340;6&#39033;&#30740;&#31350;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65292;&#23548;&#33268;&#26080;&#27861;&#20998;&#26512;&#36825;&#20123;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper replicates 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, and successfully replicates the results of 8 studies. However, for the remaining 6 studies, GPT3.5 answered survey questions in an extremely predetermined way, making it impossible to analyze these studies.
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#24050;&#32463;&#22823;&#22823;&#22686;&#24378;&#12290;&#36825;&#31181;AI&#31995;&#32479;&#30340;&#19968;&#20010;&#25552;&#20986;&#30340;&#24212;&#29992;&#26159;&#25903;&#25345;&#31038;&#20250;&#21644;&#35748;&#30693;&#31185;&#23398;&#20013;&#30340;&#25968;&#25454;&#25910;&#38598;&#65292;&#30446;&#21069;&#23436;&#32654;&#30340;&#23454;&#39564;&#25511;&#21046;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#32780;&#22823;&#35268;&#27169;&#12289;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#30340;&#25910;&#38598;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#65288;&#20439;&#31216;GPT3.5&#65289;&#37325;&#26032;&#22797;&#21046;&#20102;Many Labs 2&#22797;&#21046;&#39033;&#30446;&#20013;&#30340;14&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#27599;&#39033;&#30740;&#31350;&#30340;&#35843;&#26597;&#20316;&#20026;&#25991;&#26412;&#36755;&#20837;&#65292;&#20174;GPT3.5&#30340;&#40664;&#35748;&#35774;&#32622;&#20013;&#25910;&#38598;&#20102;&#21709;&#24212;&#12290;&#22312;&#25105;&#20204;&#21487;&#20197;&#20998;&#26512;&#30340;&#20843;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30340;GPT&#26679;&#26412;&#22797;&#21046;&#20102;&#21407;&#22987;&#32467;&#26524;&#30340;37.5%&#20197;&#21450;Many Labs 2&#32467;&#26524;&#30340;37.5%&#12290;&#20986;&#20046;&#24847;&#26009;&#30340;&#26159;&#65292;&#25105;&#20204;&#26080;&#27861;&#20687;&#39044;&#20808;&#27880;&#20876;&#30340;&#35745;&#21010;&#37027;&#26679;&#20998;&#26512;&#21097;&#19979;&#30340;&#20845;&#39033;&#30740;&#31350;&#12290;&#36825;&#26159;&#22240;&#20026;&#23545;&#20110;&#36825;&#20845;&#39033;&#30740;&#31350;&#20013;&#30340;&#27599;&#19968;&#39033;&#65292;GPT3.5&#20197;&#26497;&#20854;&#39044;&#23450;&#30340;&#26041;&#24335;&#22238;&#31572;&#20102;&#35843;&#26597;&#38382;&#39064;&#65288;&#26080;&#35770;&#26159;&#22240;&#21464;&#37327;&#36824;&#26159;&#26465;&#20214;&#21464;&#37327;&#65289;&#65306;&#19968;&#20010;&#26410;&#30693;&#30340;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have vastly grown in capabilities. One proposed application of such AI systems is to support data collection in the social and cognitive sciences, where perfect experimental control is currently unfeasible and the collection of large, representative datasets is generally expensive. In this paper, we re-replicate 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. We collected responses from the default setting of GPT3.5 by inputting each study's survey as text. Among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results as well as 37.5% of the Many Labs 2 results. Unexpectedly, we could not analyse the remaining six studies as we had planned in our pre-registration. This was because for each of these six studies, GPT3.5 answered at least one of the survey questions (either a dependent variable or a condition variable) in an extremely predetermined way: an unex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;TempCLR&#65292;&#29992;&#20110;&#26174;&#24335;&#27604;&#36739;&#23436;&#25972;&#30340;&#35270;&#39057;&#21644;&#27573;&#33853;&#65292;&#35299;&#20915;&#21333;&#20803;&#32423;&#21035;&#27604;&#36739;&#21487;&#33021;&#24573;&#30053;&#20840;&#23616;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.13738</link><description>&lt;p&gt;
TempCLR:&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26102;&#38388;&#23545;&#40784;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
TempCLR: Temporal Alignment Representation with Contrastive Learning. (arXiv:2212.13738v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#26694;&#26550;TempCLR&#65292;&#29992;&#20110;&#26174;&#24335;&#27604;&#36739;&#23436;&#25972;&#30340;&#35270;&#39057;&#21644;&#27573;&#33853;&#65292;&#35299;&#20915;&#21333;&#20803;&#32423;&#21035;&#27604;&#36739;&#21487;&#33021;&#24573;&#30053;&#20840;&#23616;&#26102;&#38388;&#19978;&#19979;&#25991;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#22312;&#35270;&#39057;-&#25991;&#26412;&#39044;&#35757;&#32451;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#20854;&#20013;&#27599;&#20010;&#21477;&#23376;&#37117;&#34987;&#35757;&#32451;&#20026;&#25509;&#36817;&#20110;&#20849;&#21516;&#29305;&#24449;&#31354;&#38388;&#20013;&#37197;&#23545;&#30340;&#35270;&#39057;&#21098;&#36753;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#38271;&#35270;&#39057;&#65292;&#30001;&#20110;&#21477;&#23376;&#25551;&#36848;&#35270;&#39057;&#30340;&#19981;&#21516;&#37096;&#20998;&#65292;&#21333;&#20803;&#32423;&#21035;&#30340;&#27604;&#36739;&#21487;&#33021;&#20250;&#24573;&#30053;&#20840;&#23616;&#30340;&#26102;&#38388;&#19978;&#19979;&#25991;&#65292;&#36825;&#24517;&#28982;&#20250;&#38480;&#21046;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;TempCLR&#65292;&#20197;&#26174;&#24335;&#22320;&#27604;&#36739;&#23436;&#25972;&#30340;&#35270;&#39057;&#21644;&#27573;&#33853;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video representation learning has been successful in video-text pre-training for zero-shot transfer, where each sentence is trained to be close to the paired video clips in a common feature space. For long videos, given a paragraph of description where the sentences describe different segments of the video, by matching all sentence-clip pairs, the paragraph and the full video are aligned implicitly. However, such unit-level comparison may ignore global temporal context, which inevitably limits the generalization ability. In this paper, we propose a contrastive learning framework TempCLR to compare the full video and the paragraph explicitly. As the video/paragraph is formulated as a sequence of clips/sentences, under the constraint of their temporal order, we use dynamic time warping to compute the minimum cumulative cost over sentence-clip pairs as the sequence-level distance. To explore the temporal dynamics, we break the consistency of temporal succession by shuffling video clips w.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.04088</link><description>&lt;p&gt;
LLM-Planner: &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#23454;&#20307;&#20195;&#29702;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models. (arXiv:2212.04088v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#22312;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#20013;&#20855;&#26377;&#24456;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#35268;&#21010;&#22120;&#65292;&#35753;&#23454;&#20307;&#20195;&#29702;&#21487;&#20197;&#25353;&#29031;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#23436;&#25104;&#22312;&#35270;&#35273;&#24863;&#30693;&#29615;&#22659;&#20013;&#30340;&#22797;&#26434;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#30340;&#39640;&#25968;&#25454;&#25104;&#26412;&#21644;&#20302;&#26679;&#26412;&#25928;&#29575;&#38459;&#30861;&#20102;&#22810;&#20219;&#21153;&#21644;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36890;&#29992;&#20195;&#29702;&#30340;&#24320;&#21457;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;LLM-Planner&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#23454;&#20307;&#20195;&#29702;&#36827;&#34892;&#23569;&#26679;&#26412;&#35268;&#21010;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#20307;&#20195;&#29702;&#30446;&#21069;&#25152;&#22312;&#30340;&#29615;&#22659;&#20026;&#22522;&#30784;&#65292;&#22686;&#24378;LLMs&#29983;&#25104;&#21644;&#26356;&#26032;&#35745;&#21010;&#12290;&#22312;ALFRED&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21462;&#24471;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#23569;&#26679;&#26412;&#24615;&#33021;&#65306;&#23613;&#31649;&#20351;&#29992;&#30340;&#37197;&#23545;&#35757;&#32451;&#25968;&#25454;&#19981;&#21040;0.5&#65285;&#65292;LLM-Planner&#30340;&#34920;&#29616;&#19982;&#20351;&#29992;&#23436;&#25972;&#35757;&#32451;&#25968;&#25454;&#35757;&#32451;&#30340;&#26368;&#26032;&#22522;&#32447;&#30456;&#24403;&#12290;&#29616;&#26377;&#26041;&#27861;&#20960;&#20046;&#26080;&#27861;&#23436;&#25104;&#20219;&#20309;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study focuses on using large language models (LLMs) as a planner for embodied agents that can follow natural language instructions to complete complex tasks in a visually-perceived environment. The high data cost and poor sample efficiency of existing methods hinders the development of versatile agents that are capable of many tasks and can learn new tasks quickly. In this work, we propose a novel method, LLM-Planner, that harnesses the power of large language models to do few-shot planning for embodied agents. We further propose a simple but effective way to enhance LLMs with physical grounding to generate and update plans that are grounded in the current environment. Experiments on the ALFRED dataset show that our method can achieve very competitive few-shot performance: Despite using less than 0.5% of paired training data, LLM-Planner achieves competitive performance with recent baselines that are trained using the full training data. Existing methods can barely complete any ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.07717</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#28145;&#24230;&#26102;&#38388;&#24314;&#27169;&#22312;&#20020;&#24202;&#25233;&#37057;&#30151;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deep Temporal Modelling of Clinical Depression through Social Media Text. (arXiv:2211.07717v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#20998;&#31867;&#22120;&#65292;&#20174;&#31038;&#20132;&#23186;&#20307;&#25991;&#26412;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#27169;&#22411;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#65292;&#36890;&#36807;&#25552;&#20379;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#26469;&#35780;&#20272;&#35813;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25143;&#26102;&#38388;&#36724;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#29992;&#25143;&#30340;&#20020;&#24202;&#25233;&#37057;&#30151;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#25233;&#37057;&#30151;&#29366;&#26816;&#27979;&#65288;DSD&#65289;&#20998;&#31867;&#22120;&#65292;&#35813;&#20998;&#31867;&#22120;&#22522;&#20110;&#26368;&#22823;&#25968;&#37327;&#30340;&#24050;&#32463;&#36807;&#20020;&#24202;&#21307;&#24072;&#27880;&#37322;&#30340;&#25512;&#25991;&#12290;&#25105;&#20204;&#38543;&#21518;&#20351;&#29992;&#25105;&#20204;&#30340;DSD&#27169;&#22411;&#26469;&#25552;&#21462;&#20020;&#24202;&#30456;&#20851;&#29305;&#24449;&#65292;&#20363;&#22914;&#25233;&#37057;&#30151;&#35780;&#20998;&#21450;&#20854;&#38543;&#21518;&#30340;&#26102;&#38388;&#27169;&#24335;&#65292;&#20197;&#21450;&#29992;&#25143;&#21457;&#24067;&#27963;&#21160;&#27169;&#24335;&#65292;&#20363;&#22914;&#37327;&#21270;&#20182;&#20204;&#30340;&#8220;&#26080;&#27963;&#21160;&#8221;&#25110;&#8220;&#27785;&#40664;&#8221;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35780;&#20272;&#36825;&#20123;&#25552;&#21462;&#29305;&#24449;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19977;&#31181;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#20004;&#20010;&#29616;&#26377;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#29992;&#25143;&#32423;&#21035;&#25233;&#37057;&#30151;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#21333;&#20010;&#29305;&#24449;&#12289;&#22522;&#32447;&#29305;&#24449;&#21644;&#29305;&#24449;&#21066;&#20943;&#27979;&#35797;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#65292;&#22312;&#19981;&#21516;&#26102;&#38388;&#31890;&#24230;&#30340;&#20960;&#20010;&#32423;&#21035;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
We describe the development of a model to detect user-level clinical depression based on a user's temporal social media posts. Our model uses a Depression Symptoms Detection (DSD) classifier, which is trained on the largest existing samples of clinician annotated tweets for clinical depression symptoms. We subsequently use our DSD model to extract clinically relevant features, e.g., depression scores and their consequent temporal patterns, as well as user posting activity patterns, e.g., quantifying their ``no activity'' or ``silence.'' Furthermore, to evaluate the efficacy of these extracted features, we create three kinds of datasets including a test dataset, from two existing well-known benchmark datasets for user-level depression detection. We then provide accuracy measures based on single features, baseline features and feature ablation tests, at several different levels of temporal granularity. The relevant data distributions and clinical depression detection related settings can
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#22312;mBART-50&#30340;&#22522;&#30784;&#19978;&#35757;&#32451;&#35821;&#35328;&#23478;&#26063;&#36866;&#37197;&#22120;&#65292;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2209.15236</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#22810;&#35821;&#35328;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#35328;&#23478;&#26063;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language-Family Adapters for Low-Resource Multilingual Neural Machine Translation. (arXiv:2209.15236v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#22312;mBART-50&#30340;&#22522;&#30784;&#19978;&#35757;&#32451;&#35821;&#35328;&#23478;&#26063;&#36866;&#37197;&#22120;&#65292;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#32763;&#35793;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#22810;&#35821;&#35328;&#27169;&#22411;&#22312;&#24191;&#27867;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#24448;&#24448;&#20250;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#35821;&#35328;&#23545;&#30340;&#24179;&#34892;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#12290;&#22810;&#35821;&#35328;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#20294;&#38656;&#35201;&#20462;&#25913;&#25972;&#20010;&#27169;&#22411;&#65292;&#21487;&#33021;&#25104;&#26412;&#36807;&#39640;&#12290;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#36873;&#25321;&#26159;&#38024;&#23545;&#27599;&#20010;&#35821;&#35328;&#23545;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;&#36866;&#37197;&#22120;&#65292;&#25110;&#22312;&#19981;&#26356;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#38024;&#23545;&#25152;&#26377;&#35821;&#35328;&#23545;&#36827;&#34892;&#21333;&#19968;&#36866;&#37197;&#22120;&#30340;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#19981;&#20801;&#35768;&#35821;&#35328;&#20043;&#38388;&#20849;&#20139;&#21442;&#25968;&#65292;&#32780;&#21518;&#32773;&#21017;&#20849;&#20139;&#25152;&#26377;&#35821;&#35328;&#30340;&#21442;&#25968;&#65292;&#23481;&#26131;&#21463;&#21040;&#36127;&#38754;&#24178;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;mBART-50&#30340;&#22522;&#30784;&#19978;&#35757;&#32451;&#35821;&#35328;&#23478;&#26063;&#36866;&#37197;&#22120;&#65292;&#20197;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#30456;&#20851;&#22522;&#32447;&#65292;&#22312;&#32763;&#35793;&#36807;&#31243;&#20013;&#24179;&#22343;&#33719;&#24471;&#26356;&#39640;&#30340;&#32763;&#35793;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large multilingual models trained with self-supervision achieve state-of-the-art results in a wide range of natural language processing tasks. Self-supervised pretrained models are often fine-tuned on parallel data from one or multiple language pairs for machine translation. Multilingual fine-tuning improves performance on low-resource languages but requires modifying the entire model and can be prohibitively expensive. Training a new adapter on each language pair or training a single adapter on all language pairs without updating the pretrained model has been proposed as a parameter-efficient alternative. However, the former does not permit any sharing between languages, while the latter shares parameters for all languages and is susceptible to negative interference. In this paper, we propose training language-family adapters on top of mBART-50 to facilitate cross-lingual transfer. Our approach outperforms related baselines, yielding higher translation scores on average when translati
&lt;/p&gt;</description></item><item><title>&#20964;&#31070;&#27036;&#26159;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#65292;&#26088;&#22312;&#25903;&#25345;&#27721;&#35821;&#31038;&#21306;&#30340;&#21457;&#23637;&#12290;&#23427;&#21253;&#25324;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;API&#12289;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#31561;&#65292;&#20197;&#20419;&#36827;&#20013;&#22269;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2209.02970</link><description>&lt;p&gt;
&#20964;&#31070;&#27036; 1.0&#65306;&#25104;&#20026;&#20013;&#22269;&#35748;&#30693;&#26234;&#33021;&#30340;&#22522;&#30707;
&lt;/p&gt;
&lt;p&gt;
Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence. (arXiv:2209.02970v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02970
&lt;/p&gt;
&lt;p&gt;
&#20964;&#31070;&#27036;&#26159;&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#65292;&#26088;&#22312;&#25903;&#25345;&#27721;&#35821;&#31038;&#21306;&#30340;&#21457;&#23637;&#12290;&#23427;&#21253;&#25324;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;API&#12289;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#31561;&#65292;&#20197;&#20419;&#36827;&#20013;&#22269;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22522;&#30784;&#35774;&#26045;&#20043;&#19968;&#65292;&#20026;&#36890;&#29992;&#26234;&#33021;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#23384;&#22312;&#20004;&#20010;&#32039;&#36843;&#30340;&#25361;&#25112;&#65306;&#29616;&#26377;&#30340;&#22522;&#30784;&#27169;&#22411;&#30001;&#33521;&#35821;&#31038;&#21306;&#20027;&#23548;&#65307;&#29992;&#25143;&#24448;&#24448;&#21482;&#33021;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#65292;&#26080;&#27861;&#24635;&#26159;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#25903;&#25345;&#27721;&#35821;&#31038;&#21306;&#30340;&#21457;&#23637;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#20964;&#31070;&#27036;&#65288;Fengshenbang&#65289;&#30340;&#24320;&#28304;&#39033;&#30446;&#65292;&#30001;&#35748;&#30693;&#35745;&#31639;&#19982;&#33258;&#28982;&#35821;&#35328;&#30740;&#31350;&#20013;&#24515;&#65288;CCNL&#65289;&#39046;&#23548;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#20855;&#26377;&#20840;&#38754;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#12289;&#29992;&#25143;&#21451;&#22909;&#30340;API&#12289;&#22522;&#20934;&#12289;&#25968;&#25454;&#38598;&#31561;&#12290;&#25105;&#20204;&#23558;&#25152;&#26377;&#36825;&#20123;&#21253;&#35065;&#22312;&#19977;&#20010;&#23376;&#39033;&#30446;&#20013;&#65306;&#20964;&#31070;&#27036;&#27169;&#22411;&#12289;&#20964;&#31070;&#26694;&#26550;&#21644;&#20964;&#31070;&#27036;&#22522;&#20934;&#12290;&#19968;&#20010;&#24320;&#28304;&#36335;&#32447;&#22270;&#65292;&#20964;&#31070;&#27036;&#26088;&#22312;&#37325;&#26032;&#35780;&#20272;&#20013;&#22269;&#39044;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#24320;&#28304;&#31038;&#21306;&#65292;&#20419;&#36827;&#25972;&#20010;&#20013;&#22269;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, foundation models become one of fundamental infrastructures in artificial intelligence, paving ways to the general intelligence. However, the reality presents two urgent challenges: existing foundation models are dominated by the English-language community; users are often given limited resources and thus cannot always use foundation models. To support the development of the Chinese-language community, we introduce an open-source project, called Fengshenbang, which leads by the research center for Cognitive Computing and Natural Language (CCNL). Our project has comprehensive capabilities, including large pre-trained models, user-friendly APIs, benchmarks, datasets, and others. We wrap all these in three sub-projects: the Fengshenbang Model, the Fengshen Framework, and the Fengshen Benchmark. An open-source roadmap, Fengshenbang, aims to re-evaluate the open-source community of Chinese pre-trained large-scale models, prompting the development of the entire Chinese large-scale 
&lt;/p&gt;</description></item><item><title>Paraformer&#26159;&#29992;&#20110;&#38750;&#33258;&#22238;&#24402;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#24555;&#36895;&#20934;&#30830;&#24182;&#34892;Transformer&#65292;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#31215;&#20998;-&#28779;&#22120;&#39044;&#27979;&#22120;&#21644;&#25195;&#35270;&#24335;&#35821;&#35328;&#27169;&#22411;&#37319;&#26679;&#22120;&#35299;&#20915;&#20102;&#21333;&#27493;NAR&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.08317</link><description>&lt;p&gt;
Paraformer&#65306;&#29992;&#20110;&#38750;&#33258;&#22238;&#24402;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#24555;&#36895;&#20934;&#30830;&#24182;&#34892;Transformer
&lt;/p&gt;
&lt;p&gt;
Paraformer: Fast and Accurate Parallel Transformer for Non-autoregressive End-to-End Speech Recognition. (arXiv:2206.08317v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08317
&lt;/p&gt;
&lt;p&gt;
Paraformer&#26159;&#29992;&#20110;&#38750;&#33258;&#22238;&#24402;&#31471;&#21040;&#31471;&#35821;&#38899;&#35782;&#21035;&#30340;&#24555;&#36895;&#20934;&#30830;&#24182;&#34892;Transformer&#65292;&#36890;&#36807;&#20351;&#29992;&#36830;&#32493;&#31215;&#20998;-&#28779;&#22120;&#39044;&#27979;&#22120;&#21644;&#25195;&#35270;&#24335;&#35821;&#35328;&#27169;&#22411;&#37319;&#26679;&#22120;&#35299;&#20915;&#20102;&#21333;&#27493;NAR&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#24050;&#32463;&#25104;&#20026;ASR&#39046;&#22495;&#30340;&#20027;&#35201;&#26041;&#27861;&#12290;&#34429;&#28982;&#33021;&#22815;&#20135;&#29983;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#23427;&#20204;&#28041;&#21450;&#20351;&#29992;&#33258;&#22238;&#24402;&#65288;AR&#65289;&#35299;&#30721;&#22120;&#19968;&#20010;&#19968;&#20010;&#22320;&#29983;&#25104;&#20196;&#29260;&#65292;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#20302;&#25928;&#30340;&#12290;&#20026;&#20102;&#21152;&#36895;&#25512;&#29702;&#65292;&#35774;&#35745;&#20102;&#38750;&#33258;&#22238;&#24402;&#65288;NAR&#65289;&#26041;&#27861;&#65292;&#20363;&#22914;&#21333;&#27493;NAR&#65292;&#20197;&#23454;&#29616;&#24182;&#34892;&#29983;&#25104;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#36755;&#20986;&#20196;&#29260;&#20013;&#23384;&#22312;&#29420;&#31435;&#24615;&#20551;&#35774;&#65292;&#21333;&#27493;NAR&#30340;&#24615;&#33021;&#19981;&#22914;AR&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#12290;&#25552;&#39640;&#21333;&#27493;NAR&#30340;&#20004;&#20010;&#25361;&#25112;&#26159;&#65306;&#31532;&#19968;&#26159;&#20934;&#30830;&#39044;&#27979;&#36755;&#20986;&#20196;&#29260;&#30340;&#25968;&#37327;&#21644;&#25552;&#21462;&#38544;&#34255;&#21464;&#37327;&#65307;&#31532;&#20108;&#26159;&#22686;&#24378;&#36755;&#20986;&#20196;&#29260;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#24314;&#27169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#24182;&#34892;Transformer&#27169;&#22411;&#65292;&#31216;&#20026;Paraformer&#12290;&#36825;&#21033;&#29992;&#22522;&#20110;&#36830;&#32493;&#31215;&#20998;-&#28779;&#22120;&#22522;&#30784;&#30340;&#39044;&#27979;&#22120;&#26469;&#39044;&#27979;&#20196;&#29260;&#25968;&#24182;&#29983;&#25104;&#38544;&#34255;&#21464;&#37327;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#25195;&#35270;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#37319;&#26679;&#22120;&#20250;&#26681;&#25454;&#38544;&#34255;&#21464;&#37327;&#29983;&#25104;&#35821;&#20041;&#21512;&#29702;&#30340;&#23376;&#24207;&#21015;&#12290;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;LibriSpeech&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Paraformer&#22312;NAR&#27169;&#22411;&#30340;&#35782;&#21035;&#31934;&#24230;&#21644;&#25512;&#29702;&#36895;&#24230;&#26041;&#38754;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have recently dominated the ASR field. Although able to yield good performance, they involve an autoregressive (AR) decoder to generate tokens one by one, which is computationally inefficient. To speed up inference, non-autoregressive (NAR) methods, e.g. single-step NAR, were designed, to enable parallel generation. However, due to an independence assumption within the output tokens, performance of single-step NAR is inferior to that of AR models, especially with a large-scale corpus. There are two challenges to improving single-step NAR: Firstly to accurately predict the number of output tokens and extract hidden variables; secondly, to enhance modeling of interdependence between output tokens. To tackle both challenges, we propose a fast and accurate parallel transformer, termed Paraformer. This utilizes a continuous integrate-and-fire based predictor to predict the number of tokens and generate hidden variables. A glancing language model (GLM) sampler then generates sem
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;CLIP&#20316;&#20026;&#22870;&#21169;&#20989;&#25968;, &#29983;&#25104;&#26356;&#32454;&#33268;&#12289;&#29420;&#29305;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#36890;&#36807;FineCapEval&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2205.13115</link><description>&lt;p&gt;
&#24102;&#26377;CLIP&#22870;&#21169;&#30340;&#32454;&#31890;&#24230;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Fine-grained Image Captioning with CLIP Reward. (arXiv:2205.13115v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;CLIP&#20316;&#20026;&#22870;&#21169;&#20989;&#25968;, &#29983;&#25104;&#26356;&#32454;&#33268;&#12289;&#29420;&#29305;&#30340;&#22270;&#20687;&#26631;&#39064;&#12290;&#36890;&#36807;FineCapEval&#27979;&#35797;&#65292;&#35813;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22270;&#20687;&#25551;&#36848;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#20013;&#30340;&#21442;&#32771;&#25551;&#36848;&#36890;&#24120;&#25551;&#36848;&#26368;&#26174;&#33879;&#30340;&#20849;&#21516;&#23545;&#35937;&#65292;&#20351;&#29992;&#25991;&#26412;&#30456;&#20284;&#24615;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#24573;&#30053;&#20102;&#21306;&#20998;&#22270;&#20687;&#19982;&#20854;&#20182;&#22270;&#20687;&#30340;&#29305;&#23450;&#21644;&#35814;&#32454;&#26041;&#38754;&#12290;&#20026;&#20102;&#26356;&#35814;&#32454;&#21644;&#29420;&#29305;&#22320;&#29983;&#25104;&#26631;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;CLIP&#20316;&#20026;&#22870;&#21169;&#20989;&#25968;&#65292;&#35745;&#31639;&#20174;&#32593;&#32476;&#20013;&#24040;&#22823;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#32534;&#30721;&#22120;&#30340;&#22810;&#27169;&#24335;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;fine-tuning&#31574;&#30053;&#65292;&#25913;&#36827;&#20102;CLIP&#25991;&#26412;&#32534;&#30721;&#22120;&#30340;&#35821;&#27861;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#25991;&#26412;&#27880;&#37322;&#12290;&#36825;&#23436;&#20840;&#28040;&#38500;&#20102;&#22312;&#22870;&#21169;&#35745;&#31639;&#26399;&#38388;&#21442;&#32771;&#26631;&#39064;&#30340;&#38656;&#35201;&#12290;&#20026;&#20102;&#20840;&#38754;&#35780;&#20272;&#25551;&#36848;&#24615;&#26631;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FineCapEval&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#32454;&#31890;&#24230;&#26631;&#20934;&#65288;&#25972;&#20307;&#12289;&#32972;&#26223;&#12289;&#23545;&#35937;&#12289;&#20851;&#31995;&#65289;&#30340;&#26032;&#26631;&#27880;&#25968;&#25454;&#38598;&#12290;&#22312;&#25105;&#20204;&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#21644;FineCapEval&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23458;&#35266;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#37117;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern image captioning models are usually trained with text similarity objectives. However, since reference captions in public datasets often describe the most salient common objects, models trained with text similarity objectives tend to ignore specific and detailed aspects of an image that distinguish it from others. Toward more descriptive and distinctive caption generation, we propose using CLIP, a multimodal encoder trained on huge image-text pairs from web, to calculate multimodal similarity and use it as a reward function. We also propose a simple finetuning strategy of the CLIP text encoder to improve grammar that does not require extra text annotation. This completely eliminates the need for reference captions during the reward computation. To comprehensively evaluate descriptive captions, we introduce FineCapEval, a new dataset for caption evaluation with fine-grained criteria: overall, background, object, relations. In our experiments on text-to-image retrieval and FineCapE
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2205.12186</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;: &#23545;&#25239;&#36127;&#34920;&#31034;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;&#25968;&#25454;&#26102;&#65292;&#26087;&#20219;&#21153;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#33021;&#20250;&#28418;&#31227;&#12290;&#19968;&#20123;&#36127;&#38754;&#30340;&#34920;&#31034;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#20026;&#20250;&#23548;&#33268;&#20174;&#26412;&#22320;&#23398;&#20064;&#30340;&#31867;&#21035;&#21407;&#22411;&#21644;&#25968;&#25454;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21407;&#22411;&#25351;&#23548;&#23398;&#20064;&#65292;&#29992;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;NLP&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20197;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24335;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#37051;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36739;&#23569;&#34920;&#31034;&#28418;&#31227;&#30340;&#30456;&#24403;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#37325;&#26032;&#37319;&#26679;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn a sequence of tasks over time, with data distributions shifting from one task to another. When training on new task data, data representations from old tasks may drift. Some negative representation drift can result in catastrophic forgetting, by causing the locally learned class prototypes and data representations to correlate poorly across tasks. To mitigate such representation drift, we propose a method that finds global prototypes to guide the learning, and learns data representations with the regularization of the self-supervised information. Specifically, for NLP tasks, we formulate each task in a masked language modeling style, and learn the task via a neighbor attention mechanism over a pre-trained language model. Experimental results show that our proposed method can learn fairly consistent representations with less representation drift, and significantly reduce catastrophic forgetting in CL without resampling data from past tasks.
&lt;/p&gt;</description></item></channel></rss>