<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04430</link><description>&lt;p&gt;
SILO&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#38750;&#21442;&#25968;&#21270;&#25968;&#25454;&#23384;&#20648;&#20013;&#38548;&#31163;&#27861;&#24459;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
SILO Language Models: Isolating Legal Risk In a Nonparametric Datastore. (arXiv:2308.04430v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04430
&lt;/p&gt;
&lt;p&gt;
SILO&#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#23545;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#36827;&#34892;&#26597;&#35810;&#65292;&#23454;&#29616;&#22312;&#38754;&#20020;&#27861;&#24459;&#39118;&#38505;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#25903;&#25345;&#25968;&#25454;&#24402;&#23646;&#21644;&#25968;&#25454;&#29983;&#20135;&#32773;&#36864;&#20986;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#23558;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#35757;&#32451;&#22312;&#21463;&#29256;&#26435;&#25110;&#21463;&#20854;&#20182;&#38480;&#21046;&#30340;&#25968;&#25454;&#19978;&#30340;&#21512;&#27861;&#24615;&#36827;&#34892;&#28608;&#28872;&#36777;&#35770;&#30340;&#21516;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20165;&#22312;&#20302;&#39118;&#38505;&#25991;&#26412;&#65288;&#20363;&#22914;&#36807;&#26399;&#29256;&#26435;&#22270;&#20070;&#25110;&#25919;&#24220;&#25991;&#20214;&#65289;&#19978;&#35757;&#32451;&#26102;&#65292;&#27169;&#22411;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#35813;&#25991;&#26412;&#30340;&#35268;&#27169;&#21644;&#39046;&#22495;&#35206;&#30422;&#26377;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SILO&#65292;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#31649;&#29702;&#36825;&#31181;&#39118;&#38505;-&#24615;&#33021;&#26435;&#34913;&#12290;SILO&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#26500;&#24314;&#65306;&#65288;1&#65289;&#22312;&#25105;&#20204;&#31574;&#21010;&#30340;&#26032;&#35821;&#26009;&#24211;&#8220;&#24320;&#25918;&#35768;&#21487;&#35777;&#35821;&#26009;&#24211;&#8221;&#65288;OLC&#65289;&#19978;&#35757;&#32451;&#21442;&#25968;&#21270;&#30340;LM&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#21547;228B&#20010;&#20844;&#20849;&#39046;&#22495;&#21644;&#35768;&#21487;&#25991;&#26412;&#12290;&#65288;2&#65289;&#36890;&#36807;&#38750;&#21442;&#25968;&#21270;&#30340;&#25968;&#25454;&#23384;&#20648;&#65288;&#20363;&#22914;&#21253;&#21547;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#22270;&#20070;&#25110;&#26032;&#38395;&#30340;&#25968;&#25454;&#65289;&#23545;&#20854;&#36827;&#34892;&#25193;&#20805;&#65292;&#35813;&#25968;&#25454;&#23384;&#20648;&#20165;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#34987;&#26597;&#35810;&#12290;&#35813;&#25968;&#25454;&#23384;&#20648;&#20801;&#35768;&#20351;&#29992;&#39640;&#39118;&#38505;&#25968;&#25454;&#32780;&#26080;&#38656;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#25903;&#25345;&#21477;&#32423;&#25968;&#25454;&#24402;&#23646;&#65292;&#24182;&#20351;&#25968;&#25454;&#29983;&#20135;&#32773;&#21487;&#20197;&#36890;&#36807;&#20174;&#23384;&#20648;&#20013;&#21024;&#38500;&#20869;&#23481;&#26469;&#36873;&#25321;&#36864;&#20986;&#27169;&#22411;&#12290;&#36825;&#20123;&#21151;&#33021;&#21487;&#20197;&#20419;&#36827;&#23545;&#25968;&#25454;&#20351;&#29992;&#35268;&#33539;&#30340;&#36981;&#24490;&#12290;
&lt;/p&gt;
&lt;p&gt;
The legality of training language models (LMs) on copyrighted or otherwise restricted data is under intense debate. However, as we show, model performance significantly degrades if trained only on low-risk text (e.g., out-of-copyright books or government documents), due to its limited size and domain coverage. We present SILO, a new language model that manages this risk-performance tradeoff during inference. SILO is built by (1) training a parametric LM on Open License Corpus (OLC), a new corpus we curate with 228B tokens of public domain and permissively licensed text and (2) augmenting it with a more general and easily modifiable nonparametric datastore (e.g., containing copyrighted books or news) that is only queried during inference. The datastore allows use of high-risk data without training on it, supports sentence-level data attribution, and enables data producers to opt out from the model by removing content from the store. These capabilities can foster compliance with data-use
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#21644;&#34892;&#20026;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#26174;&#24335;&#24314;&#27169;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#21462;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#65292;&#23454;&#29616;&#26377;&#25928;&#19988;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.04424</link><description>&lt;p&gt;
&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#29992;&#20110;&#32852;&#21512;&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#21644;&#34892;&#20026;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition. (arXiv:2308.04424v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04424
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#32852;&#21512;&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#21644;&#34892;&#20026;&#35782;&#21035;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#24182;&#26174;&#24335;&#24314;&#27169;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20174;&#32780;&#25552;&#21462;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#65292;&#23454;&#29616;&#26377;&#25928;&#19988;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24773;&#24863;&#20998;&#31867;&#65288;DSC&#65289;&#21644;&#34892;&#20026;&#35782;&#21035;&#65288;DAR&#65289;&#30340;&#32852;&#21512;&#20219;&#21153;&#26088;&#22312;&#21516;&#26102;&#39044;&#27979;&#23545;&#35805;&#20013;&#27599;&#20010;&#35805;&#35821;&#30340;&#24773;&#24863;&#26631;&#31614;&#21644;&#34892;&#20026;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#21482;&#33021;&#21333;&#21521;&#32534;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#65292;&#38480;&#21046;&#20102;&#20854;&#23545;&#19978;&#19979;&#25991;&#30340;&#20840;&#38754;&#29702;&#35299;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#35270;&#20102;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#26174;&#24335;&#20851;&#32852;&#65292;&#23548;&#33268;&#23545;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#30340;&#33719;&#21462;&#33021;&#21147;&#19981;&#36275;&#65292;&#20174;&#32780;&#38459;&#30861;&#20102;&#26377;&#25928;&#21644;&#20934;&#30830;&#30340;&#25512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#27169;&#22411;&#65288;BMIM&#65289;&#65292;&#23427;&#21033;&#29992;&#29305;&#24449;&#36873;&#25321;&#32593;&#32476;&#21644;&#21452;&#21521;&#22810;&#36339;&#25512;&#29702;&#32593;&#32476;&#26469;&#36845;&#20195;&#22320;&#25552;&#21462;&#21644;&#25972;&#21512;&#20016;&#23500;&#30340;&#24773;&#24863;&#21644;&#34892;&#20026;&#32447;&#32034;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#21452;&#23398;&#20064;&#26469;&#26126;&#30830;&#24314;&#27169;&#24773;&#24863;&#21644;&#34892;&#20026;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;BMIM&#30340;&#24615;&#33021;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The joint task of Dialog Sentiment Classification (DSC) and Act Recognition (DAR) aims to predict the sentiment label and act label for each utterance in a dialog simultaneously. However, current methods encode the dialog context in only one direction, which limits their ability to thoroughly comprehend the context. Moreover, these methods overlook the explicit correlations between sentiment and act labels, which leads to an insufficient ability to capture rich sentiment and act clues and hinders effective and accurate reasoning. To address these issues, we propose a Bi-directional Multi-hop Inference Model (BMIM) that leverages a feature selection network and a bi-directional multi-hop inference network to iteratively extract and integrate rich sentiment and act clues in a bi-directional manner. We also employ contrastive learning and dual learning to explicitly model the correlations of sentiment and act labels. Our experiments on two widely-used datasets show that BMIM outperforms s
&lt;/p&gt;</description></item><item><title>PRODIGIT&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25968;&#23383;&#25216;&#26415;&#25903;&#25345;&#31246;&#21153;&#27861;&#23448;&#21644;&#24459;&#24072;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT4&#36827;&#34892;&#27861;&#24459;&#25688;&#35201;&#29983;&#25104;&#21644;&#30456;&#20851;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#24471;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04416</link><description>&lt;p&gt;
LLMs&#32972;&#26223;&#19979;&#30340;&#27861;&#24459;&#25688;&#35201;&#65306;PRODIGIT&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
Legal Summarisation through LLMs: The PRODIGIT Project. (arXiv:2308.04416v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04416
&lt;/p&gt;
&lt;p&gt;
PRODIGIT&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25968;&#23383;&#25216;&#26415;&#25903;&#25345;&#31246;&#21153;&#27861;&#23448;&#21644;&#24459;&#24072;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#20351;&#29992;LLMs&#21644;GPT4&#36827;&#34892;&#27861;&#24459;&#25688;&#35201;&#29983;&#25104;&#21644;&#30456;&#20851;&#20449;&#24687;&#25552;&#21462;&#30340;&#26041;&#27861;&#24471;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PRODIGIT&#30340;&#22823;&#22411;&#24847;&#22823;&#21033;&#39033;&#30446;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25968;&#23383;&#25216;&#26415;&#65288;&#23588;&#20854;&#26159;&#20154;&#24037;&#26234;&#33021;&#65289;&#25903;&#25345;&#31246;&#21153;&#27861;&#23448;&#21644;&#24459;&#24072;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#27861;&#23448;&#20915;&#31574;&#30340;&#25688;&#35201;&#29983;&#25104;&#20197;&#21450;&#30456;&#20851;&#20449;&#24687;&#30340;&#25552;&#21462;&#65292;&#22914;&#27861;&#24459;&#38382;&#39064;&#21644;&#20915;&#31574;&#26631;&#20934;&#30340;&#30830;&#23450;&#65292;&#20197;&#21450;&#20851;&#38190;&#35789;&#30340;&#35268;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#24037;&#20855;&#21644;&#26041;&#27861;&#26469;&#36827;&#34892;&#25688;&#35201;&#25552;&#21462;&#21644;&#25277;&#35937;&#21270;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;LLMs&#65292;&#29305;&#21035;&#26159;GPT4&#65292;&#26681;&#25454;&#19987;&#19994;&#31246;&#21153;&#27861;&#23448;&#21644;&#24459;&#24072;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20010;&#22522;&#30784;&#65292;&#27491;&#22312;&#24314;&#35774;&#19968;&#20010;&#21407;&#22411;&#24212;&#29992;&#65292;&#23558;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present some initial results of a large-scale Italian project called PRODIGIT which aims to support tax judges and lawyers through digital technology, focusing on AI. We have focused on generation of summaries of judicial decisions and on the extraction of related information, such as the identification of legal issues and decision-making criteria, and the specification of keywords. To this end, we have deployed and evaluated different tools and approaches to extractive and abstractive summarisation. We have applied LLMs, and particularly on GPT4, which has enabled us to obtain results that proved satisfactory, according to an evaluation by expert tax judges and lawyers. On this basis, a prototype application is being built which will be made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23383;&#31526;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#19981;&#21516;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#25463;&#20811;&#35821;&#21644;&#20811;&#32599;&#22320;&#20122;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#32763;&#35793;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#23545;&#20110;&#30456;&#20284;&#35821;&#35328;&#65292;&#23383;&#31526;&#32423;&#36755;&#20837;&#20998;&#21106;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#20110;&#20851;&#32852;&#24615;&#36739;&#23567;&#30340;&#35821;&#35328;&#65292;&#23383;&#31526;&#32423;&#27169;&#22411;&#33853;&#21518;&#20110;&#23376;&#35789;&#32423;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#23383;&#31526;&#32423;&#24494;&#35843;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2308.04398</link><description>&lt;p&gt;
&#23383;&#31526;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#35328;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Character-level NMT and language similarity. (arXiv:2308.04398v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23383;&#31526;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#19981;&#21516;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#23545;&#25463;&#20811;&#35821;&#21644;&#20811;&#32599;&#22320;&#20122;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#30340;&#32763;&#35793;&#36827;&#34892;&#35780;&#20272;&#65292;&#21457;&#29616;&#23545;&#20110;&#30456;&#20284;&#35821;&#35328;&#65292;&#23383;&#31526;&#32423;&#36755;&#20837;&#20998;&#21106;&#26377;&#30410;&#22788;&#65292;&#32780;&#23545;&#20110;&#20851;&#32852;&#24615;&#36739;&#23567;&#30340;&#35821;&#35328;&#65292;&#23383;&#31526;&#32423;&#27169;&#22411;&#33853;&#21518;&#20110;&#23376;&#35789;&#32423;&#27169;&#22411;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#23383;&#31526;&#32423;&#24494;&#35843;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#20351;&#29992;Transformer&#26550;&#26500;&#36827;&#34892;&#23383;&#31526;&#32423;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#22312;&#19981;&#21516;&#35821;&#35328;&#30456;&#20284;&#24615;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#32763;&#35793;&#35821;&#35328;&#21253;&#25324;&#25463;&#20811;&#35821;&#21644;&#20811;&#32599;&#22320;&#20122;&#35821;&#12289;&#24503;&#35821;&#12289;&#21256;&#29273;&#21033;&#35821;&#12289;&#26031;&#27931;&#20240;&#20811;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#35780;&#20272;&#25351;&#26631;&#23545;&#27169;&#22411;&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#26174;&#31034;&#23545;&#20110;&#30456;&#20284;&#35821;&#35328;&#20043;&#38388;&#30340;&#32763;&#35793;&#65292;&#23383;&#31526;&#32423;&#36755;&#20837;&#20998;&#21106;&#21487;&#20197;&#24102;&#26469;&#30410;&#22788;&#65292;&#32780;&#23545;&#20110;&#20851;&#32852;&#24615;&#36739;&#23567;&#30340;&#35821;&#35328;&#65292;&#23383;&#31526;&#32423;Transformer&#27169;&#22411;&#24448;&#24448;&#33853;&#21518;&#20110;&#23376;&#35789;&#32423;&#20998;&#21106;&#12290;&#25105;&#20204;&#35777;&#23454;&#20102;&#20043;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#23545;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#23376;&#35789;&#32423;&#27169;&#22411;&#36827;&#34892;&#23383;&#31526;&#32423;&#24494;&#35843;&#26469;&#24357;&#34917;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the effectiveness of character-level neural machine translation using Transformer architecture for various levels of language similarity and size of the training dataset on translation between Czech and Croatian, German, Hungarian, Slovak, and Spanish. We evaluate the models using automatic MT metrics and show that translation between similar languages benefits from character-level input segmentation, while for less related languages, character-level vanilla Transformer-base often lags behind subword-level segmentation. We confirm previous findings that it is possible to close the gap by finetuning the already trained subword-level models to character-level.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#26041;&#27861;&#65288;ECT&#65289;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#21040;&#30456;&#23545;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#25552;&#39640;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04386</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#35780;&#20272;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning Evaluation Models from Large Language Models for Sequence Generation. (arXiv:2308.04386v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#26041;&#27861;&#65288;ECT&#65289;&#65292;&#21487;&#20197;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#21040;&#30456;&#23545;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#65292;&#25552;&#39640;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24207;&#21015;&#29983;&#25104;&#35780;&#20272;&#26041;&#38754;&#34920;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#36890;&#24120;&#20855;&#26377;&#22823;&#37327;&#30340;&#21442;&#25968;&#12290;&#36825;&#26159;&#19968;&#20010;&#35745;&#31639;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#23427;&#20204;&#30340;&#35780;&#20272;&#33021;&#21147;&#26102;&#20250;&#24102;&#26469;&#35745;&#31639;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;ECT&#30340;&#35780;&#20272;&#33021;&#21147;&#36716;&#31227;&#26041;&#27861;&#65292;&#23558;&#35780;&#20272;&#33021;&#21147;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36716;&#31227;&#21040;&#30456;&#23545;&#36731;&#37327;&#32423;&#30340;&#35821;&#35328;&#27169;&#22411;&#19978;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;ECT&#65292;&#25105;&#20204;&#20174;ChatGPT&#20013;&#23398;&#20064;&#20102;&#21508;&#31181;&#35780;&#20272;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#20316;&#20026;&#22870;&#21169;&#27169;&#22411;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#37325;&#26032;&#25490;&#24207;&#26041;&#27861;&#26469;&#25913;&#36827;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#25991;&#26412;&#39118;&#26684;&#36716;&#25442;&#21644;&#25688;&#35201;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;ECT&#30340;&#26377;&#25928;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23558;&#23398;&#20064;&#21040;&#30340;&#35780;&#20272;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#20250;&#20135;&#29983;&#26356;&#22909;&#30340;&#29983;&#25104;&#24207;&#21015;&#65292;&#36825;&#26159;&#36890;&#36807;&#24120;&#29992;&#30340;&#24230;&#37327;&#21644;ChatGPT&#36827;&#34892;&#35780;&#20272;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models achieve state-of-the-art performance on sequence generation evaluation, but typically have a large number of parameters. This is a computational challenge as presented by applying their evaluation capability at scale. To overcome the challenge, in this paper, we propose \textbf{ECT}, an \textbf{e}valuation \textbf{c}apability \textbf{t}ransfer method, to transfer the evaluation capability from LLMs to relatively lightweight language models. Based on the proposed ECT, we learn various evaluation models from ChatGPT, and employ them as reward models to improve sequence generation models via reinforcement learning and reranking approaches. Experimental results on machine translation, text style transfer, and summarization tasks demonstrate the effectiveness of our ECT. Notably, applying the learned evaluation models to sequence generation models results in better generated sequences as evaluated by commonly used metrics and ChatGPT.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#22269;&#31821;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26377;&#20559;&#35265;&#30340;&#27169;&#22411;&#20250;&#22797;&#21046;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20174;&#32780;&#21487;&#33021;&#36896;&#25104;&#20260;&#23475;&#12290;</title><link>http://arxiv.org/abs/2308.04346</link><description>&lt;p&gt;
&#25581;&#31034;&#22269;&#23478;&#20559;&#35265;&#65306;&#23545;AI&#29983;&#25104;&#25991;&#31456;&#20013;&#22269;&#31821;&#30693;&#35273;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unmasking Nationality Bias: A Study of Human Perception of Nationalities in AI-Generated Articles. (arXiv:2308.04346v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04346
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20013;&#30340;&#22269;&#31821;&#20559;&#35265;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#26377;&#20559;&#35265;&#30340;&#27169;&#22411;&#20250;&#22797;&#21046;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#20174;&#32780;&#21487;&#33021;&#36896;&#25104;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#35780;&#20272;&#26041;&#27861;&#65292;&#30740;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#22269;&#31821;&#20559;&#35265;&#12290;&#26377;&#20559;&#35265;&#30340;NLP&#27169;&#22411;&#21487;&#33021;&#20250;&#20256;&#25773;&#21051;&#26495;&#21360;&#35937;&#65292;&#23548;&#33268;&#31639;&#27861;&#24615;&#21035;&#27495;&#35270;&#65292;&#23545;AI&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21644;&#27491;&#20041;&#24615;&#26500;&#25104;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#20004;&#27493;&#28151;&#21512;&#26041;&#27861;&#65292;&#26082;&#21253;&#25324;&#23450;&#37327;&#20998;&#26512;&#65292;&#21448;&#21253;&#25324;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#35782;&#21035;&#21644;&#29702;&#35299;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#22269;&#31821;&#20559;&#35265;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#25105;&#20204;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;AI&#29983;&#25104;&#30340;&#25991;&#31456;&#20013;&#22269;&#31821;&#20559;&#35265;&#30340;&#31243;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#24320;&#25918;&#24335;&#35775;&#35848;&#65292;&#36827;&#34892;&#20102;&#23450;&#24615;&#32534;&#30721;&#21644;&#20027;&#39064;&#20998;&#26512;&#65292;&#20197;&#20102;&#35299;&#36825;&#20123;&#20559;&#35265;&#23545;&#20154;&#31867;&#35835;&#32773;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#26377;&#20559;&#35265;&#30340;NLP&#27169;&#22411;&#20542;&#21521;&#20110;&#22797;&#21046;&#21644;&#25918;&#22823;&#29616;&#26377;&#30340;&#31038;&#20250;&#20559;&#35265;&#65292;&#22312;&#31038;&#20250;&#25216;&#26415;&#29615;&#22659;&#20013;&#20351;&#29992;&#20250;&#36896;&#25104;&#20260;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential for nationality biases in natural language processing (NLP) models using human evaluation methods. Biased NLP models can perpetuate stereotypes and lead to algorithmic discrimination, posing a significant challenge to the fairness and justice of AI systems. Our study employs a two-step mixed-methods approach that includes both quantitative and qualitative analysis to identify and understand the impact of nationality bias in a text generation model. Through our human-centered quantitative analysis, we measure the extent of nationality bias in articles generated by AI sources. We then conduct open-ended interviews with participants, performing qualitative coding and thematic analysis to understand the implications of these biases on human readers. Our findings reveal that biased NLP models tend to replicate and amplify existing societal biases, which can translate to harm if used in a sociotechnical setting. The qualitative analysis from our interviews offers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#24320;&#28304;&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65292;&#21442;&#21152;&#24182;&#36194;&#24471;&#21152;&#32435;&#22269;&#23478;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#21487;&#20197;&#22312;&#25945;&#32946;&#39046;&#22495;&#20135;&#29983;&#30495;&#23454;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04333</link><description>&lt;p&gt;
&#36808;&#21521;&#36194;&#24471;&#21152;&#32435;&#22269;&#23478;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#30340;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Towards an AI to Win Ghana's National Science and Maths Quiz. (arXiv:2308.04333v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#24320;&#28304;&#39033;&#30446;&#65292;&#35813;&#39033;&#30446;&#26088;&#22312;&#24314;&#31435;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#65292;&#21442;&#21152;&#24182;&#36194;&#24471;&#21152;&#32435;&#22269;&#23478;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#21487;&#20197;&#22312;&#25945;&#32946;&#39046;&#22495;&#20135;&#29983;&#30495;&#23454;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#24320;&#28304;&#39033;&#30446;&#27491;&#22312;&#24314;&#31435;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#26469;&#21442;&#21152;&#24182;&#33719;&#32988;&#21152;&#32435;&#22269;&#23478;&#31185;&#23398;&#19982;&#25968;&#23398;&#31454;&#36187;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#35813;&#39033;&#30446;&#30340;&#36827;&#23637;&#65292;&#25551;&#36848;&#20102;&#27599;&#20010;&#22242;&#38431;&#30340;&#24773;&#20917;&#65292;&#20171;&#32461;&#20102;&#36804;&#20170;&#21462;&#24471;&#30340;&#36827;&#23637;&#20197;&#21450;&#35745;&#21010;&#20110;2023&#24180;10&#26376;&#22312;NSMQ 2023&#20013;&#21457;&#24067;&#21644;&#23637;&#31034;&#20154;&#24037;&#26234;&#33021;&#30340;&#19979;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can an AI win Ghana's National Science and Maths Quiz (NSMQ)? That is the question we seek to answer in the NSMQ AI project, an open-source project that is building AI to compete live in the NSMQ and win. The NSMQ is an annual live science and mathematics competition for senior secondary school students in Ghana in which 3 teams of 2 students compete by answering questions across biology, chemistry, physics, and math in 5 rounds over 5 progressive stages until a winning team is crowned for that year. The NSMQ is an exciting live quiz competition with interesting technical challenges across speech-to-text, text-to-speech, question-answering, and human-computer interaction. In this ongoing work that began in January 2023, we give an overview of the project, describe each of the teams, progress made thus far, and the next steps toward our planned launch and debut of the AI in October for NSMQ 2023. An AI that conquers this grand challenge can have real-world impact on education such as en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04306</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#26816;&#27979;&#30693;&#35782;&#27880;&#20837;&#65306;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Learning-Based Knowledge Injection for Metaphor Detection: A Comprehensive Review. (arXiv:2308.04306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#32508;&#36848;&#65292;&#21253;&#25324;&#20027;&#27969;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#30340;&#24635;&#32467;&#12289;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#30340;&#22238;&#39038;&#65292;&#24182;&#25506;&#35752;&#20102;&#24403;&#21069;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#21947;&#30740;&#31350;&#30340;&#21382;&#21490;&#20063;&#26631;&#24535;&#30528;&#30693;&#35782;&#27880;&#20837;&#30740;&#31350;&#30340;&#28436;&#21464;&#12290;&#38543;&#30528;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#19981;&#26029;&#36827;&#27493;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#23545;&#23558;&#30693;&#35782;&#24212;&#29992;&#20110;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#32467;&#26524;&#34920;&#29616;&#20986;&#26497;&#22823;&#20852;&#36259;&#12290;&#23613;&#31649;&#22312;&#38544;&#21947;&#35782;&#21035;&#39046;&#22495;&#28041;&#21450;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#36880;&#28176;&#22686;&#21152;&#65292;&#20294;&#32570;&#20047;&#19968;&#31687;&#23436;&#25972;&#30340;&#20851;&#20110;&#22522;&#20110;&#30693;&#35782;&#27880;&#20837;&#30340;&#26041;&#27861;&#30340;&#32508;&#36848;&#25991;&#31456;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#28145;&#24230;&#23398;&#20064;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#24212;&#29992;&#30693;&#35782;&#27880;&#20837;&#30340;&#30740;&#31350;&#36827;&#23637;&#12290;&#26412;&#25991;&#31995;&#32479;&#24635;&#32467;&#21644;&#27010;&#25324;&#20102;&#20027;&#27969;&#30340;&#30693;&#35782;&#21644;&#30693;&#35782;&#27880;&#20837;&#21407;&#21017;&#65292;&#21516;&#26102;&#22238;&#39038;&#20102;&#22312;&#38544;&#21947;&#35782;&#21035;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#22522;&#20934;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24403;&#21069;&#38754;&#20020;&#30340;&#30693;&#35782;&#27880;&#20837;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The history of metaphor research also marks the evolution of knowledge infusion research. With the continued advancement of deep learning techniques in recent years, the natural language processing community has shown great interest in applying knowledge to successful results in metaphor recognition tasks. Although there has been a gradual increase in the number of approaches involving knowledge injection in the field of metaphor recognition, there is a lack of a complete review article on knowledge injection based approaches. Therefore, the goal of this paper is to provide a comprehensive review of research advances in the application of deep learning for knowledge injection in metaphor recognition tasks. In this paper, we systematically summarize and generalize the mainstream knowledge and knowledge injection principles, as well as review the datasets, evaluation metrics, and benchmark models used in metaphor recognition tasks. Finally, we explore the current issues facing knowledge 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;wav2vec 2.0&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22312;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;ASR&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20004;&#32773;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#23398;&#20064;&#21040;&#30340;&#28388;&#27874;&#22120;&#65292;&#21457;&#29616;ASR&#31995;&#32479;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26159;&#36890;&#36807;&#19968;&#32452;&#24102;&#36890;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#12290;</title><link>http://arxiv.org/abs/2308.04286</link><description>&lt;p&gt;
wav2vec 2.0&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#27604;&#36739;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Comparative Analysis of the wav2vec 2.0 Feature Extractor. (arXiv:2308.04286v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;wav2vec 2.0&#27169;&#22411;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#20256;&#32479;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#22312;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;ASR&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20004;&#32773;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#36890;&#36807;&#20998;&#26512;&#23398;&#20064;&#21040;&#30340;&#28388;&#27874;&#22120;&#65292;&#21457;&#29616;ASR&#31995;&#32479;&#26368;&#37325;&#35201;&#30340;&#20449;&#24687;&#26159;&#36890;&#36807;&#19968;&#32452;&#24102;&#36890;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#36890;&#24120;&#20351;&#29992;&#25163;&#24037;&#35774;&#35745;&#30340;&#29305;&#24449;&#25552;&#21462;&#27969;&#31243;&#12290;&#20026;&#20102;&#36991;&#20813;&#23427;&#20204;&#22266;&#26377;&#30340;&#20449;&#24687;&#25439;&#22833;&#65292;&#24182;&#23454;&#29616;&#20174;&#35821;&#38899;&#21040;&#36716;&#24405;&#25991;&#26412;&#30340;&#26356;&#19968;&#33268;&#30340;&#24314;&#27169;&#65292;&#31070;&#32463;&#21407;&#22987;&#27874;&#24418;&#29305;&#24449;&#25552;&#21462;&#22120;&#65288;FEs&#65289;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#26041;&#27861;&#12290;&#26368;&#36817;&#24191;&#21463;&#27426;&#36814;&#30340;wav2vec 2.0&#27169;&#22411;&#20063;&#20351;&#29992;&#20102;&#21367;&#31215;FE&#65292;&#30452;&#25509;&#23545;&#35821;&#38899;&#27874;&#24418;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#25991;&#29486;&#20013;&#30740;&#31350;&#24471;&#36824;&#19981;&#22815;&#20805;&#20998;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23427;&#22312;&#36830;&#25509;&#20027;&#20041;&#26102;&#38388;&#20998;&#31867;&#65288;CTC&#65289;ASR&#27169;&#22411;&#20013;&#26367;&#20195;&#26631;&#20934;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#20854;&#19982;&#21478;&#19968;&#31181;&#26367;&#20195;&#24615;&#31070;&#32463;FE&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;LibriSpeech&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20004;&#32773;&#37117;&#19982;&#20256;&#32479;&#30340;FEs&#31454;&#20105;&#21147;&#19981;&#30456;&#19978;&#19979;&#65292;&#24182;&#20998;&#26512;&#20102;&#21508;&#20010;&#32452;&#20214;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#23398;&#20064;&#21040;&#30340;&#28388;&#27874;&#22120;&#65292;&#24182;&#26174;&#31034;ASR&#31995;&#32479;&#30340;&#26368;&#37325;&#35201;&#20449;&#24687;&#26159;&#36890;&#36807;&#19968;&#32452;&#24102;&#36890;&#28388;&#27874;&#22120;&#33719;&#24471;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic speech recognition (ASR) systems typically use handcrafted feature extraction pipelines. To avoid their inherent information loss and to achieve more consistent modeling from speech to transcribed text, neural raw waveform feature extractors (FEs) are an appealing approach. Also the wav2vec 2.0 model, which has recently gained large popularity, uses a convolutional FE which operates directly on the speech waveform. However, it is not yet studied extensively in the literature. In this work, we study its capability to replace the standard feature extraction methods in a connectionist temporal classification (CTC) ASR model and compare it to an alternative neural FE. We show that both are competitive with traditional FEs on the LibriSpeech benchmark and analyze the effect of the individual components. Furthermore, we analyze the learned filters and show that the most important information for the ASR system is obtained by a set of bandpass filters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#20102;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#30340;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#20854;&#21487;&#20197;&#19982;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#23218;&#32654;&#12290;</title><link>http://arxiv.org/abs/2308.04275</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#23545;&#35805;&#36827;&#34892;&#19978;&#19979;&#25991;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
In-Context Alignment: Chat with Vanilla Language Models Before Fine-Tuning. (arXiv:2308.04275v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#20043;&#21069;&#19982;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#23454;&#29616;&#20102;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#30340;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#20854;&#21487;&#20197;&#19982;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#35828;&#26126;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25506;&#32034;&#25512;&#29702;&#26102;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#32431;&#20928;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; Llama-2&#65292;&#22312;&#36827;&#34892;&#20219;&#20309;&#24494;&#35843;&#20043;&#21069;&#65292;&#24403;&#27169;&#22411;&#34987;&#35201;&#27714;&#25353;&#29031;&#32842;&#22825;&#24335;&#30340;&#25351;&#20196;&#36827;&#34892;&#25805;&#20316;&#26102;&#65292;&#25105;&#20204;&#26816;&#32034;&#21040;&#20102;&#24179;&#22343;9&#20010;&#23545;&#40784;&#28436;&#31034;&#31034;&#20363;&#12290;&#19982;&#30452;&#25509;&#25552;&#31034;&#30456;&#27604;&#65292;&#19981;&#25913;&#21464;&#27169;&#22411;&#26435;&#37325;&#30340;&#19978;&#19979;&#25991;&#23545;&#40784;&#23548;&#33268;&#20102;&#19982;OpenAI&#30340;text-davinci-003&#27169;&#22411;&#30456;&#27604;&#65292;&#32988;&#29575;&#25552;&#39640;&#20102;7&#20493;&#65292;&#20351;&#24471;&#32431;&#20928;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23218;&#32654;&#36890;&#36807;&#23545;&#40784;&#24494;&#35843;&#30340;&#24378;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this note, we explore inference-time alignment through in-context learning. We consider a vanilla pretrained language model Llama-2 before any fine-tuning and retrieve an average of 9 demonstration alignment examples when the model is prompted to follow chat-style instructions. Compared to direct prompting, the in-context alignment without changing model weights leads to a 7x increase in win-rate w.r.t. the text-davinci-003 model from OpenAI, making the vanilla language model comparable to strong baselines with alignment fine-tuning.
&lt;/p&gt;</description></item><item><title>CLASSLA-Stanza&#26159;&#19968;&#20010;&#20026;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#25552;&#20379;&#33258;&#21160;&#35821;&#35328;&#27880;&#37322;&#30340;&#27969;&#27700;&#32447;&#65292;&#30456;&#23545;&#20110;Stanza&#65292;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#19978;&#26377;&#22810;&#20010;&#25913;&#36827;&#65292;&#24182;&#21462;&#24471;&#20102;&#22987;&#32456;&#22914;&#19968;&#30340;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.04255</link><description>&lt;p&gt;
CLASSLA-Stanza: &#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#35821;&#35328;&#22788;&#29702;&#30340;&#19979;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
CLASSLA-Stanza: The Next Step for Linguistic Processing of South Slavic Languages. (arXiv:2308.04255v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04255
&lt;/p&gt;
&lt;p&gt;
CLASSLA-Stanza&#26159;&#19968;&#20010;&#20026;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#25552;&#20379;&#33258;&#21160;&#35821;&#35328;&#27880;&#37322;&#30340;&#27969;&#27700;&#32447;&#65292;&#30456;&#23545;&#20110;Stanza&#65292;&#22312;&#24615;&#33021;&#21644;&#21151;&#33021;&#19978;&#26377;&#22810;&#20010;&#25913;&#36827;&#65292;&#24182;&#21462;&#24471;&#20102;&#22987;&#32456;&#22914;&#19968;&#30340;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CLASSLA-Stanza&#65292;&#19968;&#20010;&#29992;&#20110;&#21335;&#26031;&#25289;&#22827;&#35821;&#35328;&#30340;&#33258;&#21160;&#35821;&#35328;&#27880;&#37322;&#27969;&#27700;&#32447;&#65292;&#35813;&#27969;&#27700;&#32447;&#22522;&#20110;Stanza&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27969;&#27700;&#32447;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;CLASSLA-Stanza&#30456;&#23545;&#20110;Stanza&#30340;&#20027;&#35201;&#25913;&#36827;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#26368;&#26032;2.1&#29256;&#26412;&#27969;&#27700;&#32447;&#30340;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#12290;&#25105;&#20204;&#36824;&#25253;&#21578;&#20102;&#27969;&#27700;&#32447;&#23545;&#19981;&#21516;&#35821;&#35328;&#21644;&#21464;&#31181;&#30340;&#24615;&#33021;&#35780;&#20998;&#12290;CLASSLA-Stanza&#22312;&#25152;&#26377;&#25903;&#25345;&#30340;&#35821;&#35328;&#19978;&#34920;&#29616;&#20986;&#22987;&#32456;&#22914;&#19968;&#30340;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;&#25152;&#26377;&#25903;&#25345;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#25110;&#25193;&#23637;&#20102;&#20854;&#29238;&#27969;&#27700;&#32447;Stanza&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#27969;&#27700;&#32447;&#30340;&#26032;&#21151;&#33021;&#65292;&#20351;&#20854;&#33021;&#22815;&#39640;&#25928;&#22788;&#29702;&#32593;&#32476;&#25968;&#25454;&#65292;&#24182;&#35299;&#37322;&#20102;&#23548;&#33268;&#20854;&#23454;&#29616;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CLASSLA-Stanza, a pipeline for automatic linguistic annotation of the South Slavic languages, which is based on the Stanza natural language processing pipeline. We describe the main improvements in CLASSLA-Stanza with respect to Stanza, and give a detailed description of the model training process for the latest 2.1 release of the pipeline. We also report performance scores produced by the pipeline for different languages and varieties. CLASSLA-Stanza exhibits consistently high performance across all the supported languages and outperforms or expands its parent pipeline Stanza at all the supported tasks. We also present the pipeline's new functionality enabling efficient processing of web data and the reasons that led to its implementation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#23545;&#23450;&#20301;&#32467;&#26524;&#19982;&#23383;&#24149;&#36827;&#34892;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#20013;&#23383;&#24149;&#21644;&#25163;&#21183;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.04248</link><description>&lt;p&gt;
&#20351;&#29992;&#35789;&#23884;&#20837;&#36827;&#34892;&#35789;&#27719;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Gloss Alignment Using Word Embeddings. (arXiv:2308.04248v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04248
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#23545;&#23450;&#20301;&#32467;&#26524;&#19982;&#23383;&#24149;&#36827;&#34892;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#20013;&#23383;&#24149;&#21644;&#25163;&#21183;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25429;&#25417;&#21644;&#27880;&#37322;&#25163;&#35821;&#25968;&#25454;&#38598;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#12290;&#30446;&#21069;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#36828;&#36828;&#19981;&#36275;&#20197;&#25104;&#21151;&#35757;&#32451;&#26080;&#32422;&#26463;&#30340;&#25163;&#35821;&#32763;&#35793;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#24050;&#36716;&#21521;&#30005;&#35270;&#24191;&#25773;&#20869;&#23481;&#20316;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#30340;&#26469;&#28304;&#65292;&#21253;&#25324;&#25163;&#35821;&#32763;&#35793;&#32773;&#21644;&#30456;&#20851;&#38899;&#39057;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#25163;&#35821;&#27880;&#37322;&#38480;&#21046;&#20102;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#24182;&#23548;&#33268;&#20102;&#33258;&#21160;&#27880;&#37322;&#25216;&#26415;&#65288;&#22914;&#25163;&#35821;&#23450;&#20301;&#65289;&#30340;&#21457;&#23637;&#12290;&#36825;&#20123;&#23450;&#20301;&#19982;&#35270;&#39057;&#23545;&#40784;&#65292;&#32780;&#19981;&#26159;&#19982;&#23383;&#24149;&#23545;&#40784;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#23383;&#24149;&#21644;&#23450;&#20301;&#30340;&#25163;&#21183;&#19981;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#21475;&#35821;&#35821;&#35328;&#27169;&#22411;&#23558;&#23450;&#20301;&#19982;&#20854;&#23545;&#24212;&#23383;&#24149;&#23545;&#40784;&#30340;&#26041;&#27861;&#12290;&#20351;&#29992;&#21333;&#19968;&#27169;&#24577;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#24320;&#38144;&#23567;&#65292;&#24182;&#19988;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#23450;&#20301;&#25216;&#26415;&#32467;&#21512;&#20351;&#29992;&#12290;&#25105;&#20204;&#23450;&#37327;&#22320;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capturing and annotating Sign language datasets is a time consuming and costly process. Current datasets are orders of magnitude too small to successfully train unconstrained \acf{slt} models. As a result, research has turned to TV broadcast content as a source of large-scale training data, consisting of both the sign language interpreter and the associated audio subtitle. However, lack of sign language annotation limits the usability of this data and has led to the development of automatic annotation techniques such as sign spotting. These spottings are aligned to the video rather than the subtitle, which often results in a misalignment between the subtitle and spotted signs. In this paper we propose a method for aligning spottings with their corresponding subtitles using large spoken language models. Using a single modality means our method is computationally inexpensive and can be utilized in conjunction with existing alignment techniques. We quantitatively demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;</title><link>http://arxiv.org/abs/2308.04215</link><description>&lt;p&gt;
&#23454;&#26102;&#20316;&#26354;&#36741;&#21161;&#30340;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Hybrid Retrieval-Augmented Generation for Real-time Composition Assistance. (arXiv:2308.04215v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04215
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#20113;&#27169;&#22411;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#23454;&#26102;&#21709;&#24212;&#30340;&#20316;&#26354;&#36741;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#27169;&#22411;&#22312;&#25552;&#21319;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#25972;&#21512;&#31169;&#20154;&#25968;&#25454;&#21644;&#20943;&#23569;&#24187;&#35273;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24212;&#29992;&#20110;&#38656;&#35201;&#23454;&#26102;&#21709;&#24212;&#30340;&#20219;&#21153;&#65288;&#22914;&#20316;&#26354;&#36741;&#21161;&#65289;&#26102;&#65292;&#26816;&#32034;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25152;&#38656;&#30340;&#22788;&#29702;&#26102;&#38388;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Hybrid Retrieval-Augmented Generation (HybridRAG)&#26694;&#26550;&#65292;&#21033;&#29992;&#20102;&#23558;&#23458;&#25143;&#31471;&#27169;&#22411;&#21644;&#20113;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#30340;&#28151;&#21512;&#35774;&#32622;&#12290;HybridRAG&#36890;&#36807;&#24322;&#27493;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20113;&#31471;&#29983;&#25104;&#30340;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#25972;&#21512;&#21040;&#23458;&#25143;&#31471;&#27169;&#22411;&#20013;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#31181;&#26816;&#32034;&#22686;&#24378;&#20869;&#23384;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#25928;&#30340;&#21709;&#24212;&#65292;&#20174;LLM&#30340;&#33021;&#21147;&#20013;&#21463;&#30410;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24322;&#27493;&#20869;&#23384;&#38598;&#25104;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#33021;&#22815;&#23454;&#26102;&#21709;&#24212;&#29992;&#25143;&#35831;&#27714;&#65292;&#26080;&#38656;&#31561;&#24453;&#20113;&#31471;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrieval augmented models show promise in enhancing traditional language models by improving their contextual understanding, integrating private data, and reducing hallucination. However, the processing time required for retrieval augmented large language models poses a challenge when applying them to tasks that require real-time responses, such as composition assistance.  To overcome this limitation, we propose the Hybrid Retrieval-Augmented Generation (HybridRAG) framework that leverages a hybrid setting that combines both client and cloud models. HybridRAG incorporates retrieval-augmented memory generated asynchronously by a Large Language Model (LLM) in the cloud. By integrating this retrieval augmented memory, the client model acquires the capability to generate highly effective responses, benefiting from the LLM's capabilities. Furthermore, through asynchronous memory integration, the client model is capable of delivering real-time responses to user requests without the need to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#30740;&#31350;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#65288;SUD&#65289;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;SUD&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#27880;&#37322;&#27169;&#24577;&#23545;SUD&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#39046;&#22495;&#19987;&#23478;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2308.04180</link><description>&lt;p&gt;
&#36890;&#36807;&#19981;&#21516;&#35270;&#35282;&#30740;&#31350;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#20998;&#31867;&#65288;SUD&#65289;&#65306;&#8220;&#25105;&#20204;&#26159;&#21542;&#22312;&#21516;&#19968;&#39029;&#19978;&#65311;&#8221;
&lt;/p&gt;
&lt;p&gt;
Studying Socially Unacceptable Discourse Classification (SUD) through different eyes: "Are we on the same page ?". (arXiv:2308.04180v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04180
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#19981;&#21516;&#30340;&#35270;&#35282;&#30740;&#31350;&#22312;&#32447;&#25991;&#26412;&#20013;&#30340;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#65288;SUD&#65289;&#20998;&#31867;&#21644;&#26816;&#27979;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#12290;&#36890;&#36807;&#20998;&#26512;&#19981;&#21516;&#32972;&#26223;&#19979;&#30340;SUD&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#27880;&#37322;&#27169;&#24577;&#23545;SUD&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20123;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#39046;&#22495;&#19987;&#23478;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#30340;&#25968;&#25454;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22312;&#32447;&#25991;&#26412;&#20013;&#31038;&#20132;&#19981;&#21487;&#25509;&#21463;&#35328;&#35770;&#65288;SUD&#65289;&#30340;&#29305;&#24449;&#21644;&#26816;&#27979;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26469;&#33258;&#19981;&#21516;&#22312;&#32447;&#26469;&#28304;&#30340;&#22823;&#37327;&#25163;&#24037;&#27880;&#37322;&#25991;&#26412;&#65292;&#36825;&#20123;&#25991;&#26412;&#22312;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;SUD&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#20013;&#20351;&#29992;&#12290;&#36825;&#31181;&#20840;&#23616;&#32972;&#26223;&#20351;&#25105;&#20204;&#21487;&#20197;&#27979;&#35797;&#33719;&#21462;&#20851;&#20110;&#30456;&#21516;SUD&#31867;&#21035;&#30340;&#30693;&#35782;&#30340;SUD&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#26469;&#33258;&#19981;&#21516;&#32972;&#26223;&#30340;&#30693;&#35782;&#12290;&#20174;&#36825;&#20010;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#35752;&#35770;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#24320;&#25918;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#20998;&#26512;&#27880;&#37322;&#27169;&#24577;&#21487;&#33021;&#23545;SUD&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#25968;&#25454;&#27934;&#23519;&#65292;&#21487;&#20197;&#25903;&#25345;&#39046;&#22495;&#19987;&#23478;&#22312;&#27880;&#37322;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Socially Unacceptable Discourse (SUD) characterization and detection in online text. We first build and present a novel corpus that contains a large variety of manually annotated texts from different online sources used so far in state-of-the-art Machine learning (ML) SUD detection solutions. This global context allows us to test the generalization ability of SUD classifiers that acquire knowledge around the same SUD categories, but from different contexts. From this perspective, we can analyze how (possibly) different annotation modalities influence SUD learning by discussing open challenges and open research directions. We also provide several data insights which can support domain experts in the annotation task.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Judge-Specialist&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#20986;&#29616;&#30340;&#21333;&#35843;&#24615;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19987;&#23478;&#20027;&#39064;&#26816;&#32034;&#22120;/&#38405;&#35835;&#22120;&#21644;&#19968;&#20010;&#35780;&#21028;&#32773;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#30830;&#20445;&#21333;&#35843;&#24615;&#24182;&#22312;&#33258;&#28982;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2308.04176</link><description>&lt;p&gt;
&#20851;&#20110;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#30340;&#21333;&#35843;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
On Monotonic Aggregation for Open-domain QA. (arXiv:2308.04176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04176
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;Judge-Specialist&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#39046;&#22495;&#38382;&#31572;&#20013;&#20986;&#29616;&#30340;&#21333;&#35843;&#24615;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19987;&#23478;&#20027;&#39064;&#26816;&#32034;&#22120;/&#38405;&#35835;&#22120;&#21644;&#19968;&#20010;&#35780;&#21028;&#32773;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#30830;&#20445;&#21333;&#35843;&#24615;&#24182;&#22312;&#33258;&#28982;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#65292;&#29992;&#20110;&#20174;&#30693;&#35782;&#28304;&#20013;&#36890;&#36807;&#31579;&#36873;&#31572;&#26696;&#32780;&#26080;&#38656;&#38405;&#35835;&#25903;&#25345;&#25991;&#26723;&#36827;&#34892;&#22522;&#20110;&#35821;&#38899;&#30340;&#26816;&#32034;&#12290;&#29305;&#21035;&#26159;&#65292;&#24320;&#25918;&#39046;&#22495;&#30340;QA&#26088;&#22312;&#22238;&#31572;&#20851;&#20110;&#26080;&#38480;&#21046;&#30693;&#35782;&#28304;&#30340;&#29992;&#25143;&#38382;&#39064;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#28155;&#21152;&#19968;&#20010;&#26469;&#28304;&#19981;&#24212;&#38477;&#20302;&#20934;&#30830;&#24615;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#19981;&#28385;&#36275;&#36825;&#19968;&#29305;&#24615;&#65288;&#31216;&#20026;&#8220;&#21333;&#35843;&#24615;&#8221;&#65289;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#21407;&#22240;&#65292;&#24182;&#22312;&#27492;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;Judge-Specialist&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#65288;1&#65289;&#19987;&#23478;&#20027;&#39064;&#26816;&#32034;&#22120;/&#38405;&#35835;&#22120;&#65292;&#29992;&#20110;&#35206;&#30422;&#20010;&#21035;&#26469;&#28304;&#65292;&#21644;&#65288;2&#65289;&#35780;&#21028;&#32773;&#65292;&#19968;&#20010;&#19987;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#36873;&#25321;&#26368;&#32456;&#31572;&#26696;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#20165;&#30830;&#20445;&#20102;&#21333;&#35843;&#24615;&#65292;&#32780;&#19988;&#22312;&#8220;&#33258;&#28982;&#38382;&#39064;&#8221;&#19978;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#26469;&#28304;QA&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#23545;&#20110;&#26469;&#33258;&#35821;&#38899;&#35782;&#21035;&#30340;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#20195;&#30721;&#21644;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering (QA) is a critical task for speech-based retrieval from knowledge sources, by sifting only the answers without requiring to read supporting documents. Specifically, open-domain QA aims to answer user questions on unrestricted knowledge sources. Ideally, adding a source should not decrease the accuracy, but we find this property (denoted as "monotonicity") does not hold for current state-of-the-art methods. We identify the cause, and based on that we propose Judge-Specialist framework. Our framework consists of (1) specialist retrievers/readers to cover individual sources, and (2) judge, a dedicated language model to select the final answer. Our experiments show that our framework not only ensures monotonicity, but also outperforms state-of-the-art multi-source QA methods on Natural Questions. Additionally, we show that our models robustly preserve the monotonicity against noise from speech recognition. We publicly release our code and setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25552;&#31034;&#38142;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#27861;&#24459;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36890;&#36807;&#21019;&#24314;&#31616;&#27905;&#25688;&#35201;&#12289;&#35821;&#20041;&#25628;&#32034;&#30456;&#20851;&#31034;&#20363;&#25991;&#26412;&#21644;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#36229;&#36807;&#36739;&#22823;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.04138</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25552;&#31034;&#38142;&#23545;&#20110;&#38271;&#31687;&#27861;&#24459;&#25991;&#20214;&#20998;&#31867;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Prompt Chaining for Long Legal Document Classification. (arXiv:2308.04138v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04138
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25552;&#31034;&#38142;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#27861;&#24459;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#26102;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#36890;&#36807;&#21019;&#24314;&#31616;&#27905;&#25688;&#35201;&#12289;&#35821;&#20041;&#25628;&#32034;&#30456;&#20851;&#31034;&#20363;&#25991;&#26412;&#21644;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#36827;&#34892;&#25552;&#31034;&#65292;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#24615;&#33021;&#24182;&#36229;&#36807;&#36739;&#22823;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#29992;&#20110;&#24341;&#23548;&#25110;&#25351;&#23548;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19982;&#25152;&#38656;&#32467;&#26524;&#19968;&#33268;&#30340;&#36866;&#24403;&#22238;&#24212;&#12290;&#38142;&#25509;&#26159;&#19968;&#31181;&#23558;&#22797;&#26434;&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#12289;&#21487;&#31649;&#29702;&#32452;&#20214;&#30340;&#31574;&#30053;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#25552;&#31034;&#38142;&#23545;&#22797;&#26434;&#30340;&#27861;&#24459;&#25991;&#20214;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#24191;&#27867;&#24212;&#29992;&#65292;&#36825;&#20123;&#20219;&#21153;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#39046;&#22495;&#29305;&#23450;&#35821;&#35328;&#21644; considerable length &#30340;&#38271;&#24230;&#32780;&#20855;&#26377;&#22256;&#38590;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#21019;&#24314;&#21407;&#22987;&#25991;&#20214;&#30340;&#31616;&#27905;&#25688;&#35201;&#24320;&#22987;&#65292;&#28982;&#21518;&#20174;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#36827;&#34892;&#35821;&#20041;&#25628;&#32034;&#65292;&#23547;&#25214;&#30456;&#20851;&#30340;&#31034;&#20363;&#25991;&#26412;&#21450;&#20854;&#30456;&#24212;&#30340;&#27880;&#37322;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20219;&#21153;&#25552;&#31034;&#19968;&#20010;&#26631;&#31614;&#65292;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#25552;&#31034;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#25351;&#23450;&#26631;&#31614;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#25552;&#31034;&#38142;&#65292;&#25105;&#20204;&#19981;&#20165;&#21487;&#20197;&#25552;&#39640;&#38646;&#26679;&#26412;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#36824;&#21487;&#20197;&#36229;&#36807;&#36739;&#22823;&#27169;&#22411;&#65288;&#22914;ChatGPT&#38646;&#26679;&#26412;&#65289;&#20351;&#29992;&#36739;&#23567;&#27169;&#22411;&#25152;&#36798;&#21040;&#30340;&#24494;F1&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting is used to guide or steer a language model in generating an appropriate response that is consistent with the desired outcome. Chaining is a strategy used to decompose complex tasks into smaller, manageable components. In this study, we utilize prompt chaining for extensive legal document classification tasks, which present difficulties due to their intricate domain-specific language and considerable length. Our approach begins with the creation of a concise summary of the original document, followed by a semantic search for related exemplar texts and their corresponding annotations from a training corpus. Finally, we prompt for a label - based on the task - to assign, by leveraging the in-context learning from the few-shot prompt. We demonstrate that through prompt chaining, we can not only enhance the performance over zero-shot, but also surpass the micro-F1 score achieved by larger models, such as ChatGPT zero-shot, using smaller models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#24066;&#25919;&#20915;&#31574;&#30340;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#35805;&#39064;&#24314;&#27169;&#65292;&#24182;&#23558;&#36825;&#20123;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#65292;&#20197;&#34920;&#31034;&#23545;&#27599;&#20010;&#35805;&#39064;&#30340;&#25972;&#20307;&#24773;&#24863;&#12290;&#22312;&#22885;&#26031;&#29305;&#25289;&#30340;&#25512;&#25991;&#19978;&#30340;&#24212;&#29992;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04124</link><description>&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#12289;&#35805;&#39064;&#24314;&#27169;&#21644;&#24773;&#24863;&#20998;&#26512;&#22312;&#24066;&#25919;&#20915;&#31574;&#25903;&#25345;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Social Media, Topic Modeling and Sentiment Analysis in Municipal Decision Support. (arXiv:2308.04124v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#24066;&#25919;&#20915;&#31574;&#30340;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#23545;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#36827;&#34892;&#24773;&#24863;&#20998;&#26512;&#21644;&#35805;&#39064;&#24314;&#27169;&#65292;&#24182;&#23558;&#36825;&#20123;&#20449;&#24687;&#32508;&#21512;&#36215;&#26469;&#65292;&#20197;&#34920;&#31034;&#23545;&#27599;&#20010;&#35805;&#39064;&#30340;&#25972;&#20307;&#24773;&#24863;&#12290;&#22312;&#22885;&#26031;&#29305;&#25289;&#30340;&#25512;&#25991;&#19978;&#30340;&#24212;&#29992;&#31034;&#20363;&#35828;&#26126;&#20102;&#35813;&#26694;&#26550;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#35768;&#22810;&#22478;&#24066;&#37117;&#24076;&#26395;&#25104;&#20026;&#26234;&#24935;&#22478;&#24066;&#65292;&#28982;&#32780;&#65292;&#26234;&#24935;&#20513;&#35758;&#24448;&#24448;&#23545;&#26222;&#36890;&#24066;&#27665;&#30340;&#24847;&#35265;&#19981;&#22826;&#37325;&#35270;&#12290;&#31038;&#20132;&#23186;&#20307;&#26159;&#24066;&#27665;&#24847;&#35265;&#30340;&#37325;&#35201;&#26469;&#28304;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#24066;&#25919;&#20915;&#31574;&#30340;&#31038;&#20132;&#23186;&#20307;&#22788;&#29702;&#26694;&#26550;&#21407;&#22411;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#65288;1&#65289;&#30830;&#23450;&#27599;&#20010;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#30340;&#24773;&#24863;&#26497;&#24615;&#65292;&#65288;2&#65289;&#35782;&#21035;&#20027;&#35201;&#35805;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#35805;&#39064;&#26144;&#23556;&#21040;&#20010;&#21035;&#24086;&#23376;&#19978;&#65292;&#65288;3&#65289;&#23558;&#36825;&#20004;&#20010;&#20449;&#24687;&#21512;&#24182;&#20026;&#19968;&#20010;&#34920;&#31034;&#23545;&#27599;&#20010;&#35805;&#39064;&#34920;&#36798;&#30340;&#25972;&#20307;&#24773;&#24863;&#30340;&#27169;&#31946;&#25968;&#12290;&#21487;&#36873;&#22320;&#65292;&#36825;&#20010;&#27169;&#31946;&#25968;&#21487;&#20197;&#36716;&#21270;&#20026;&#19968;&#20010;&#30001;&#20004;&#20010;&#23454;&#25968;&#20803;&#32452;&#32452;&#25104;&#30340;&#20803;&#32452;&#65292;&#34920;&#31034;&#23545;&#27599;&#20010;&#35805;&#39064;&#34920;&#36798;&#30340;&#8220;&#27491;&#38754;&#8221;&#21644;&#8220;&#36127;&#38754;&#8221;&#35266;&#28857;&#30340;&#8220;&#25968;&#37327;&#8221;&#12290;&#35813;&#26694;&#26550;&#22312;&#25463;&#20811;&#22885;&#26031;&#29305;&#25289;&#21457;&#34920;&#30340;&#25512;&#25991;&#19978;&#36827;&#34892;&#20102;&#28436;&#31034;&#65292;&#28436;&#31034;&#26399;&#20026;&#22823;&#32422;&#20004;&#20010;&#26376;&#12290;&#27492;&#24212;&#29992;&#31034;&#20363;&#35828;&#26126;&#20102;&#27169;&#31946;&#25968;&#22914;&#20309;&#34920;&#31034;&#23545;&#27599;&#20010;&#35805;&#39064;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many cities around the world are aspiring to become. However, smart initiatives often give little weight to the opinions of average citizens.  Social media are one of the most important sources of citizen opinions. This paper presents a prototype of a framework for processing social media posts with municipal decision-making in mind. The framework consists of a sequence of three steps: (1) determining the sentiment polarity of each social media post (2) identifying prevalent topics and mapping these topics to individual posts, and (3) aggregating these two pieces of information into a fuzzy number representing the overall sentiment expressed towards each topic. Optionally, the fuzzy number can be reduced into a tuple of two real numbers indicating the "amount" of positive and negative opinion expressed towards each topic.  The framework is demonstrated on tweets published from Ostrava, Czechia over a period of about two months. This application illustrates how fuzzy numbers represent s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;USTS&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#30740;&#31350;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20013;&#30340;&#38598;&#20307;&#20154;&#31867;&#35266;&#28857;&#12290;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;STS&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20010;&#21035;&#23454;&#20363;&#19978;&#30001;&#20154;&#31867;&#19981;&#19968;&#33268;&#24102;&#26469;&#30340;&#21464;&#24322;&#12290;</title><link>http://arxiv.org/abs/2308.04114</link><description>&lt;p&gt;
&#22242;&#20307;&#20154;&#31867;&#35266;&#28857;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Collective Human Opinions in Semantic Textual Similarity. (arXiv:2308.04114v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04114
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;USTS&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#30740;&#31350;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#20013;&#30340;&#38598;&#20307;&#20154;&#31867;&#35266;&#28857;&#12290;&#20998;&#26512;&#34920;&#26126;&#29616;&#26377;&#30340;STS&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20010;&#21035;&#23454;&#20363;&#19978;&#30001;&#20154;&#31867;&#19981;&#19968;&#33268;&#24102;&#26469;&#30340;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20855;&#26377;&#20027;&#35266;&#24615;&#65292;&#24182;&#19988;&#22312;STS&#27880;&#37322;&#20013;&#26222;&#36941;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#20294;&#29616;&#26377;&#30340;&#22522;&#20934;&#20351;&#29992;&#24179;&#22343;&#20154;&#21592;&#35780;&#20998;&#20316;&#20026;&#37329;&#26631;&#20934;&#12290;&#24179;&#22343;&#21270;&#25513;&#30422;&#20102;&#22312;&#20302;&#19968;&#33268;&#24615;&#31034;&#20363;&#20013;&#20154;&#31867;&#35266;&#28857;&#30340;&#30495;&#23454;&#20998;&#24067;&#65292;&#24182;&#38459;&#27490;&#27169;&#22411;&#25429;&#25417;&#20010;&#21035;&#35780;&#20998;&#25152;&#20195;&#34920;&#30340;&#35821;&#20041;&#27169;&#31946;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;USTS&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;&#32422;15,000&#20010;&#20013;&#25991;&#21477;&#23545;&#21644;150,000&#20010;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;STS&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;STS&#20013;&#30340;&#38598;&#20307;&#20154;&#31867;&#35266;&#28857;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#21333;&#19968;&#39640;&#26031;&#20989;&#25968;&#21644;&#26631;&#37327;&#22343;&#19981;&#33021;&#20805;&#20998;&#36866;&#24212;&#19968;&#32452;&#35266;&#23519;&#21040;&#30340;&#21028;&#26029;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#65292;&#24403;&#21069;&#30340;STS&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#20010;&#21035;&#23454;&#20363;&#19978;&#30001;&#20154;&#31867;&#19981;&#19968;&#33268;&#24102;&#26469;&#30340;&#21464;&#24322;&#65292;&#32780;&#26159;&#21453;&#26144;&#20102;&#23545;&#25972;&#20010;&#25968;&#25454;&#38598;&#39044;&#27979;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the subjective nature of semantic textual similarity (STS) and pervasive disagreements in STS annotation, existing benchmarks have used averaged human ratings as the gold standard. Averaging masks the true distribution of human opinions on examples of low agreement, and prevents models from capturing the semantic vagueness that the individual ratings represent. In this work, we introduce USTS, the first Uncertainty-aware STS dataset with ~15,000 Chinese sentence pairs and 150,000 labels, to study collective human opinions in STS. Analysis reveals that neither a scalar nor a single Gaussian fits a set of observed judgements adequately. We further show that current STS models cannot capture the variance caused by human disagreement on individual instances, but rather reflect the predictive confidence over the aggregate dataset.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-2&#30340;&#27604;&#21947;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;I-WAS&#65292;&#36890;&#36807;&#26367;&#25442;&#35789;&#27719;&#21644;&#23436;&#25104;&#21477;&#23376;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#21477;&#23376;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#22312;&#27604;&#21947;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.04109</link><description>&lt;p&gt;
I-WAS&#65306;&#19968;&#31181;&#22522;&#20110;GPT-2&#30340;&#29992;&#20110;&#24320;&#35774;&#27604;&#21947;&#26816;&#27979;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
I-WAS: a Data Augmentation Method with GPT-2 for Simile Detection. (arXiv:2308.04109v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04109
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-2&#30340;&#27604;&#21947;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;I-WAS&#65292;&#36890;&#36807;&#26367;&#25442;&#35789;&#27719;&#21644;&#23436;&#25104;&#21477;&#23376;&#30340;&#26041;&#24335;&#26469;&#22686;&#24378;&#21477;&#23376;&#36136;&#37327;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#22312;&#27604;&#21947;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#21947;&#26816;&#27979;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#20013;&#30340;&#19968;&#39033;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#22312;&#25991;&#23398;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27604;&#21947;&#26816;&#27979;&#30740;&#31350;&#24120;&#24120;&#20381;&#36182;&#20110;&#35268;&#27169;&#26377;&#38480;&#19988;&#26410;&#33021;&#20805;&#20998;&#20195;&#34920;&#27604;&#21947;&#24418;&#24335;&#30340;&#35821;&#26009;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GPT-2&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#21947;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;\textbf{&#26367;&#25442;&#35789;&#27719;}&#21644;\textbf{&#23436;&#25104;&#21477;&#23376;}&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#36845;&#20195;&#36807;&#31243;&#31216;&#20026;I-WAS&#65292;&#26088;&#22312;&#25552;&#39640;&#22686;&#24378;&#21477;&#23376;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21253;&#21547;&#26356;&#22810;&#19981;&#21516;&#27604;&#21947;&#24418;&#24335;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27604;&#21947;&#26816;&#27979;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simile detection is a valuable task for many natural language processing (NLP)-based applications, particularly in the field of literature. However, existing research on simile detection often relies on corpora that are limited in size and do not adequately represent the full range of simile forms. To address this issue, we propose a simile data augmentation method based on \textbf{W}ord replacement And Sentence completion using the GPT-2 language model. Our iterative process called I-WAS, is designed to improve the quality of the augmented sentences. To better evaluate the performance of our method in real-world applications, we have compiled a corpus containing a more diverse set of simile forms for experimentation. Our experimental results demonstrate the effectiveness of our proposed data augmentation method for simile detection.
&lt;/p&gt;</description></item><item><title>DataTales&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36741;&#21161;&#25776;&#20889;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#26469;&#29983;&#25104;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#25991;&#26412;&#21465;&#36848;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#25776;&#20889;&#21161;&#25163;&#36827;&#19968;&#27493;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2308.04076</link><description>&lt;p&gt;
DataTales: &#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25776;&#20889;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DataTales: Investigating the use of Large Language Models for Authoring Data-Driven Articles. (arXiv:2308.04076v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04076
&lt;/p&gt;
&lt;p&gt;
DataTales&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36741;&#21161;&#25776;&#20889;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;&#26469;&#29983;&#25104;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#25991;&#26412;&#21465;&#36848;&#12290;&#36890;&#36807;&#23450;&#24615;&#30740;&#31350;&#65292;&#21457;&#29616;LLMs&#21487;&#20197;&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#25776;&#20889;&#21161;&#25163;&#36827;&#19968;&#27493;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25776;&#20889;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#20316;&#32773;&#19981;&#20165;&#20998;&#26512;&#25968;&#25454;&#20197;&#33719;&#21462;&#35265;&#35299;&#65292;&#36824;&#35201;&#26500;&#24314;&#19968;&#20010;&#36830;&#36143;&#30340;&#21465;&#36848;&#26469;&#26377;&#25928;&#20256;&#36798;&#36825;&#20123;&#35265;&#35299;&#12290;&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#20026;&#36741;&#21161;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#25776;&#20889;&#25552;&#20379;&#20102;&#26426;&#20250;&#65292;&#21152;&#24555;&#20102;&#20889;&#20316;&#36807;&#31243;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#21033;&#29992;LLMs&#26469;&#36741;&#21161;&#20316;&#32773;&#25776;&#20889;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#30340;&#21487;&#34892;&#24615;&#21644;&#24863;&#30693;&#20215;&#20540;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21407;&#22411;&#31995;&#32479;DataTales&#65292;&#21033;&#29992;LLM&#26469;&#29983;&#25104;&#32473;&#23450;&#22270;&#34920;&#30340;&#25991;&#26412;&#21465;&#36848;&#12290;&#36890;&#36807;&#20351;&#29992;DataTales&#20316;&#20026;&#35774;&#35745;&#25506;&#38024;&#65292;&#25105;&#20204;&#23545;11&#21517;&#19987;&#19994;&#20154;&#21592;&#36827;&#34892;&#20102;&#23450;&#24615;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#35813;&#27010;&#24565;&#65292;&#24182;&#20174;&#20013;&#25552;&#28860;&#20986;&#36827;&#19968;&#27493;&#25972;&#21512;LLMs&#20316;&#20026;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#39537;&#21160;&#25991;&#31456;&#25776;&#20889;&#21161;&#25163;&#30340;&#21487;&#34892;&#24615;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Authoring data-driven articles is a complex process requiring authors to not only analyze data for insights but also craft a cohesive narrative that effectively communicates the insights. Text generation capabilities of contemporary large language models (LLMs) present an opportunity to assist the authoring of data-driven articles and expedite the writing process. In this work, we investigate the feasibility and perceived value of leveraging LLMs to support authors of data-driven articles. We designed a prototype system, DataTales, that leverages a LLM to generate textual narratives accompanying a given chart. Using DataTales as a design probe, we conducted a qualitative study with 11 professionals to evaluate the concept, from which we distilled affordances and opportunities to further integrate LLMs as valuable data-driven article authoring assistants.
&lt;/p&gt;</description></item><item><title>&#20116;&#32654;&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#32534;&#30721;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#29255;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#35821;&#20041;&#21547;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.04052</link><description>&lt;p&gt;
&#20116;&#32654;&#20803;&#27169;&#22411;&#65306;&#20174;&#21477;&#23376;&#23884;&#20837;&#29983;&#25104;&#28216;&#25103;&#22320;&#22270;&#21644;&#31934;&#28789;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
The Five-Dollar Model: Generating Game Maps and Sprites from Sentence Embeddings. (arXiv:2308.04052v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04052
&lt;/p&gt;
&lt;p&gt;
&#20116;&#32654;&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#32534;&#30721;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#29255;&#65292;&#24182;&#22312;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#35821;&#20041;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20116;&#32654;&#20803;&#27169;&#22411;&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26550;&#26500;&#65292;&#21487;&#20197;&#20174;&#32534;&#30721;&#30340;&#25991;&#26412;&#25552;&#31034;&#20013;&#29983;&#25104;&#20302;&#32500;&#24230;&#30340;&#22270;&#29255;&#12290;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#22312;&#20302;&#32500;&#24230;&#39046;&#22495;&#20013;&#25104;&#21151;&#29983;&#25104;&#20934;&#30830;&#19988;&#32654;&#35266;&#30340;&#20869;&#23481;&#65292;&#21363;&#20351;&#21482;&#26377;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23613;&#31649;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#24456;&#23567;&#65292;&#20294;&#29983;&#25104;&#30340;&#22270;&#29255;&#20173;&#28982;&#33021;&#22815;&#20445;&#25345;&#25991;&#26412;&#25552;&#31034;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#27169;&#22411;&#24212;&#29992;&#20110;&#19977;&#20010;&#23567;&#22411;&#25968;&#25454;&#38598;&#65306;&#20687;&#32032;&#33402;&#26415;&#30340;&#28216;&#25103;&#22320;&#22270;&#12289;&#28216;&#25103;&#35282;&#33394;&#31934;&#28789;&#22270;&#20687;&#21644;&#32553;&#23567;&#30340;&#34920;&#24773;&#31526;&#21495;&#22270;&#20687;&#65292;&#24182;&#24212;&#29992;&#20102;&#26032;&#39062;&#30340;&#25193;&#20805;&#31574;&#30053;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#36825;&#20123;&#26377;&#38480;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20351;&#29992;CLIP VIT-B/32&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#20043;&#38388;&#30340;&#20313;&#24358;&#30456;&#20284;&#24230;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The five-dollar model is a lightweight text-to-image generative architecture that generates low dimensional images from an encoded text prompt. This model can successfully generate accurate and aesthetically pleasing content in low dimensional domains, with limited amounts of training data. Despite the small size of both the model and datasets, the generated images are still able to maintain the encoded semantic meaning of the textual prompt. We apply this model to three small datasets: pixel art video game maps, video game sprite images, and down-scaled emoji images and apply novel augmentation strategies to improve the performance of our model on these limited datasets. We evaluate our models performance using cosine similarity score between text-image pairs generated by the CLIP VIT-B/32 model.
&lt;/p&gt;</description></item><item><title>InfeRE&#26159;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#38142;&#36880;&#27493;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#29983;&#25104;&#26368;&#32456;&#34920;&#36798;&#24335;&#32972;&#21518;&#30340;&#36880;&#27493;&#20869;&#37096;&#25991;&#26412;&#21305;&#37197;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#19968;&#33268;&#24615;&#35299;&#30721;&#26426;&#21046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfeRE&#22312;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.04041</link><description>&lt;p&gt;
InfeRE&#65306;&#36890;&#36807;&#25512;&#29702;&#38142;&#36880;&#27493;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;
&lt;/p&gt;
&lt;p&gt;
InfeRE: Step-by-Step Regex Generation via Chain of Inference. (arXiv:2308.04041v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04041
&lt;/p&gt;
&lt;p&gt;
InfeRE&#26159;&#19968;&#31181;&#36890;&#36807;&#25512;&#29702;&#38142;&#36880;&#27493;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#32771;&#34385;&#20102;&#29983;&#25104;&#26368;&#32456;&#34920;&#36798;&#24335;&#32972;&#21518;&#30340;&#36880;&#27493;&#20869;&#37096;&#25991;&#26412;&#21305;&#37197;&#36807;&#31243;&#65292;&#24182;&#24341;&#20837;&#20102;&#33258;&#19968;&#33268;&#24615;&#35299;&#30721;&#26426;&#21046;&#26469;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfeRE&#22312;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65288;NL2RE&#65289;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#65288;regexes&#65289;&#30340;&#33258;&#21160;&#21270;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#35270;&#20026;&#19968;&#20010;&#32447;&#24615;&#30340;&#20196;&#29260;&#24207;&#21015;&#65292;&#24182;&#36890;&#36807;&#21333;&#27425;&#33258;&#22238;&#24402;&#29983;&#25104;&#26368;&#32456;&#30340;&#34920;&#36798;&#24335;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#29983;&#25104;&#26368;&#32456;&#32467;&#26524;&#32972;&#21518;&#36880;&#27493;&#30340;&#20869;&#37096;&#25991;&#26412;&#21305;&#37197;&#36807;&#31243;&#12290;&#36825;&#20005;&#37325;&#24433;&#21709;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#25928;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;InfeRE&#65292;&#23427;&#23558;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#29983;&#25104;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#36880;&#27493;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#33258;&#19968;&#33268;&#24615;&#35299;&#30721;&#26426;&#21046;&#65292;&#23427;&#23558;&#20174;&#19981;&#21516;&#30340;&#27169;&#22411;&#20013;&#37319;&#26679;&#24471;&#21040;&#30340;&#22810;&#20010;&#36755;&#20986;&#36827;&#34892;&#38598;&#25104;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;NL-RX-Turk&#21644;KB13&#19978;&#23545;InfeRE&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#23558;&#32467;&#26524;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#21644;&#27969;&#34892;&#30340;&#22522;&#20110;&#26641;&#30340;&#29983;&#25104;&#26041;&#27861;TRANX&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InfeRE&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically generating regular expressions (abbrev. regexes) from natural language description (NL2RE) has been an emerging research area. Prior studies treat regex as a linear sequence of tokens and generate the final expressions autoregressively in a single pass. They did not take into account the step-by-step internal text-matching processes behind the final results. This significantly hinders the efficacy and interpretability of regex generation by neural language models. In this paper, we propose a new paradigm called InfeRE, which decomposes the generation of regexes into chains of step-by-step inference. To enhance the robustness, we introduce a self-consistency decoding mechanism that ensembles multiple outputs sampled from different models. We evaluate InfeRE on two publicly available datasets, NL-RX-Turk and KB13, and compare the results with state-of-the-art approaches and the popular tree-based generation approach TRANX. Experimental results show that InfeRE substantially
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;TF-IDF&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#24182;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#27604;&#20110;N-Gram&#65292;&#20351;&#29992;TF-IDF&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04037</link><description>&lt;p&gt;
TF-IDF&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;&#21450;&#20854;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study on TF-IDF feature Weighting Method and its Analysis using Unstructured Dataset. (arXiv:2308.04037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#30740;&#31350;&#20102;TF-IDF&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#24182;&#20351;&#29992;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#30456;&#27604;&#20110;N-Gram&#65292;&#20351;&#29992;TF-IDF&#29305;&#24449;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29305;&#24449;&#25552;&#21462;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#26159;&#23558;&#25991;&#26412;&#20998;&#31867;&#21040;&#30456;&#20851;&#31867;&#21035;&#20013;&#30340;&#36807;&#31243;&#65292;&#20854;&#31639;&#27861;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#30340;&#26680;&#24515;&#12290;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#26415;&#35821;&#39057;&#29575;-&#36870;&#25991;&#20214;&#39057;&#29575; (TF-IDF) &#21644; NLP &#26159;&#26368;&#24120;&#29992;&#30340;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#21644;&#20998;&#26512;&#20102;&#22312;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#29305;&#24449;&#21152;&#26435;&#26041;&#27861;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#32771;&#34385;&#20102;&#20004;&#20010;&#29305;&#24449; N-Gram &#21644; TF-IDF&#65292;&#29992;&#20110;&#24773;&#24863;&#20998;&#26512;&#30340; IMDB &#30005;&#24433;&#35780;&#35770;&#21644;&#20122;&#39532;&#36874; Alexa &#35780;&#35770;&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#25105;&#20204;&#20351;&#29992;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#22120;&#26469;&#39564;&#35777;&#35813;&#26041;&#27861;&#65292;&#21363;&#25903;&#25345;&#21521;&#37327;&#26426; (SVM)&#12289;&#36923;&#36753;&#22238;&#24402;&#12289;&#22810;&#39033;&#24335;&#26420;&#32032;&#36125;&#21494;&#26031; (Multinomial NB)&#12289;&#38543;&#26426;&#26862;&#26519;&#12289;&#20915;&#31574;&#26641;&#21644; K &#26368;&#36817;&#37051; (KNN)&#12290;&#20174;&#36825;&#20004;&#20010;&#29305;&#24449;&#25552;&#21462;&#20013;&#65292;&#20351;&#29992; TF-IDF &#29305;&#24449;&#30456;&#27604;&#22522;&#20110; N-Gram &#26377;&#20102;&#26174;&#33879;&#30340;&#29305;&#24449;&#25552;&#21462;&#22686;&#21152;&#12290;TF-IDF &#33719;&#24471;&#20102;&#26368;&#39640;&#30340;&#20934;&#30830;&#29575; (93.81%)&#12289;&#31934;&#30830;&#29575; (94.20%)&#12289;&#21484;&#22238;&#29575; (93.81%)&#65292;
&lt;/p&gt;
&lt;p&gt;
Text Classification is the process of categorizing text into the relevant categories and its algorithms are at the core of many Natural Language Processing (NLP). Term Frequency-Inverse Document Frequency (TF-IDF) and NLP are the most highly used information retrieval methods in text classification. We have investigated and analyzed the feature weighting method for text classification on unstructured data. The proposed model considered two features N-Grams and TF-IDF on the IMDB movie reviews and Amazon Alexa reviews dataset for sentiment analysis. Then we have used the state-of-the-art classifier to validate the method i.e., Support Vector Machine (SVM), Logistic Regression, Multinomial Naive Bayes (Multinomial NB), Random Forest, Decision Tree, and k-nearest neighbors (KNN). From those two feature extractions, a significant increase in feature extraction with TF-IDF features rather than based on N-Gram. TF-IDF got the maximum accuracy (93.81%), precision (94.20%), recall (93.81%), an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2308.04028</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Top K Relevant Passage Retrieval for Biomedical Question Answering. (arXiv:2308.04028v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04028
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#22238;&#31572;&#30340;Top K&#30456;&#20851;&#27573;&#33853;&#26816;&#32034;&#26041;&#27861;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#19981;&#36866;&#29992;&#20110;&#36825;&#20010;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#39046;&#22495;&#26469;&#35828;&#65292;&#36825;&#20010;&#38382;&#39064;&#36824;&#27809;&#26377;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38382;&#31572;&#26159;&#19968;&#39033;&#21033;&#29992;&#22823;&#37327;&#25991;&#26723;&#22238;&#31572;&#20107;&#23454;&#24615;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#23427;&#26088;&#22312;&#20197;&#33258;&#28982;&#35821;&#35328;&#22238;&#31572;&#29992;&#25143;&#30340;&#38382;&#39064;&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#31572;&#26696;&#12290;&#38382;&#31572;&#20381;&#36182;&#20110;&#39640;&#25928;&#30340;&#27573;&#33853;&#26816;&#32034;&#26469;&#36873;&#25321;&#20505;&#36873;&#19978;&#19979;&#25991;&#65292;&#20256;&#32479;&#30340;&#31232;&#30095;&#21521;&#37327;&#31354;&#38388;&#27169;&#22411;&#65292;&#22914;TF-IDF&#25110;BM25&#65292;&#26159;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#22312;&#32593;&#32476;&#19978;&#65292;&#27809;&#26377;&#19968;&#31687;&#25991;&#31456;&#21487;&#20197;&#25552;&#20379;&#25152;&#26377;&#21487;&#33021;&#30340;&#31572;&#26696;&#65292;&#20197;&#22238;&#31572;&#29992;&#25143;&#25152;&#25552;&#20986;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#31264;&#23494;&#27573;&#33853;&#26816;&#32034;&#27169;&#22411;&#24050;&#32463;&#23545;&#32500;&#22522;&#30334;&#31185;2018&#24180;12&#26376;20&#26085;&#30340;&#20542;&#38144;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#29992;&#20316;&#22238;&#31572;&#38382;&#39064;&#30340;&#28304;&#25991;&#26723;&#12290;&#38382;&#31572;&#31995;&#32479;&#22312;&#22810;&#20010;&#24320;&#25918;&#39046;&#22495;&#21644;&#26426;&#22120;&#29702;&#35299;&#31995;&#32479;&#19978;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20351;&#29992;&#20102;&#22823;&#35268;&#27169;&#30340;&#27880;&#37322;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#22312;&#20020;&#24202;&#39046;&#22495;&#65292;&#36825;&#20010;&#38382;&#39064;&#20173;&#28982;&#30456;&#23545;&#26410;&#34987;&#25506;&#32034;&#12290;&#26681;&#25454;&#22810;&#39033;&#35843;&#26597;&#65292;&#26080;&#27861;&#20174;&#32500;&#22522;&#30334;&#31185;&#20934;&#30830;&#22238;&#31572;&#29983;&#29289;&#21307;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2308.04014</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65306;&#22914;&#20309;&#65288;&#37325;&#26032;&#65289;&#28909;&#21551;&#21160;&#27169;&#22411;&#65311;
&lt;/p&gt;
&lt;p&gt;
Continual Pre-Training of Large Language Models: How to (re)warm your model?. (arXiv:2308.04014v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#38382;&#39064;&#65292;&#25506;&#35752;&#20102;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#20110;&#35299;&#20915;&#20998;&#24067;&#21464;&#21270;&#21644;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#20250;&#23545;&#25968;&#21313;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#65292;&#23601;&#20250;&#37325;&#26032;&#24320;&#22987;&#36825;&#20010;&#36807;&#31243;&#12290;&#19968;&#31181;&#26356;&#24265;&#20215;&#21644;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23454;&#29616;&#36825;&#20123;&#27169;&#22411;&#30340;&#25345;&#32493;&#39044;&#35757;&#32451;&#65292;&#21363;&#29992;&#26032;&#25968;&#25454;&#26356;&#26032;&#39044;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#26159;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#21464;&#21270;&#36890;&#24120;&#20250;&#23548;&#33268;&#36807;&#21435;&#25968;&#25454;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#28909;&#21551;&#21160;&#31574;&#30053;&#23545;&#25345;&#32493;&#39044;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#65292;&#22312;&#35757;&#32451;&#26032;&#25968;&#25454;&#38598;&#26102;&#65292;&#38656;&#35201;&#37325;&#26032;&#22686;&#21152;&#23398;&#20064;&#29575;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;Pile&#65288;&#19978;&#28216;&#25968;&#25454;&#65292;300B&#26631;&#35760;&#65289;&#19978;&#25345;&#32493;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;SlimPajama&#65288;&#19979;&#28216;&#25968;&#25454;&#65292;297B&#26631;&#35760;&#65289;&#19978;&#36827;&#34892;&#20102;&#32447;&#24615;&#28909;&#21551;&#21160;&#21644;&#20313;&#24358;&#34928;&#20943;&#30340;&#35843;&#24230;&#12290;&#25105;&#20204;&#22312;Pythia 410M&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#25152;&#26377;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are routinely pre-trained on billions of tokens, only to restart the process over again once new data becomes available. A much cheaper and more efficient solution would be to enable the continual pre-training of these models, i.e. updating pre-trained models with new data instead of re-training them from scratch. However, the distribution shift induced by novel data typically results in degraded performance on past data. Taking a step towards efficient continual pre-training, in this work, we examine the effect of different warm-up strategies. Our hypothesis is that the learning rate must be re-increased to improve compute efficiency when training on a new dataset. We study the warmup phase of models pre-trained on the Pile (upstream data, 300B tokens) as we continue to pre-train on SlimPajama (downstream data, 297B tokens), following a linear warmup and cosine decay schedule. We conduct all experiments on the Pythia 410M language model architecture and ev
&lt;/p&gt;</description></item><item><title>SimplyRetrieve&#26159;&#19968;&#27454;&#31169;&#23494;&#19988;&#36731;&#37327;&#32423;&#30340;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#24494;&#35843;&#23558;&#31169;&#26377;&#25968;&#25454;&#38598;&#25104;&#36827;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#12289;&#36731;&#37327;&#32423;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20197;&#26041;&#20415;&#29992;&#25143;&#25506;&#32034;RCG&#22312;&#25552;&#21319;&#29983;&#25104;&#22411;AI&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.03983</link><description>&lt;p&gt;
SimplyRetrieve: &#19968;&#27454;&#31169;&#23494;&#19988;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;&#26816;&#32034;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#22411;AI&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
SimplyRetrieve: A Private and Lightweight Retrieval-Centric Generative AI Tool. (arXiv:2308.03983v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03983
&lt;/p&gt;
&lt;p&gt;
SimplyRetrieve&#26159;&#19968;&#27454;&#31169;&#23494;&#19988;&#36731;&#37327;&#32423;&#30340;&#29983;&#25104;&#22411;AI&#24037;&#20855;&#65292;&#20351;&#29992;&#20102;&#22522;&#20110;&#26816;&#32034;&#30340;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26080;&#38656;&#39069;&#22806;&#27169;&#22411;&#24494;&#35843;&#23558;&#31169;&#26377;&#25968;&#25454;&#38598;&#25104;&#36827;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26412;&#22320;&#21270;&#12289;&#36731;&#37327;&#32423;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20197;&#26041;&#20415;&#29992;&#25143;&#25506;&#32034;RCG&#22312;&#25552;&#21319;&#29983;&#25104;&#22411;AI&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#23558;&#30693;&#35782;&#26816;&#32034;&#26550;&#26500;&#25972;&#21512;&#36827;&#20844;&#24320;&#21487;&#29992;&#30340;&#39044;&#35757;&#32451;LLM&#20013;&#65292;&#21487;&#20197;&#26080;&#38656;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#65292;&#23558;&#31169;&#26377;&#25968;&#25454;&#26080;&#32541;&#38598;&#25104;&#36827;&#29983;&#25104;&#22411;AI&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#26816;&#32034;&#20026;&#20013;&#24515;&#30340;&#29983;&#25104;&#65288;RCG&#65289;&#26041;&#27861;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#65292;&#26126;&#30830;&#21306;&#20998;&#20102;LLM&#21644;&#26816;&#32034;&#22120;&#22312;&#19978;&#19979;&#25991;&#35299;&#37322;&#21644;&#30693;&#35782;&#35760;&#24518;&#20013;&#30340;&#20316;&#29992;&#65292;&#28508;&#22312;&#22320;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#23454;&#29616;&#12290;SimplyRetrieve&#26159;&#19968;&#20010;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#24320;&#28304;&#24037;&#20855;&#65292;&#26088;&#22312;&#20026;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#25552;&#20379;&#19968;&#20010;&#26412;&#22320;&#21270;&#12289;&#36731;&#37327;&#32423;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#12290;SimplyRetrieve&#25552;&#20379;&#22522;&#20110;GUI&#21644;API&#30340;RCG&#24179;&#21488;&#65292;&#36741;&#20197;&#31169;&#23494;&#30693;&#35782;&#24211;&#26500;&#24314;&#22120;&#21644;&#26816;&#32034;&#35843;&#20248;&#27169;&#22359;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#21151;&#33021;&#65292;&#29992;&#25143;&#21487;&#20197;&#25506;&#32034;RCG&#22312;&#25913;&#36827;&#29983;&#25104;&#22411;AI&#24615;&#33021;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Model (LLM) based Generative AI systems have seen significant progress in recent years. Integrating a knowledge retrieval architecture allows for seamless integration of private data into publicly available Generative AI systems using pre-trained LLM without requiring additional model fine-tuning. Moreover, Retrieval-Centric Generation (RCG) approach, a promising future research direction that explicitly separates roles of LLMs and retrievers in context interpretation and knowledge memorization, potentially leads to more efficient implementation. SimplyRetrieve is an open-source tool with the goal of providing a localized, lightweight, and user-friendly interface to these sophisticated advancements to the machine learning community. SimplyRetrieve features a GUI and API based RCG platform, assisted by a Private Knowledge Base Constructor and a Retrieval Tuning Module. By leveraging these capabilities, users can explore the potential of RCG for improving generative AI per
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2308.03958</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#20943;&#23569;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Simple synthetic data reduces sycophancy in large language models. (arXiv:2308.03958v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#35840;&#22857;&#25215;&#26159;&#19968;&#31181;&#19981;&#21487;&#21462;&#30340;&#34892;&#20026;&#65292;&#27169;&#22411;&#20250;&#26681;&#25454;&#29992;&#25143;&#30340;&#35266;&#28857;&#35843;&#25972;&#22238;&#24212;&#65292;&#21363;&#20351;&#36825;&#20010;&#35266;&#28857;&#22312;&#23458;&#35266;&#19978;&#26159;&#19981;&#27491;&#30830;&#30340;&#65288;&#20363;&#22914;&#65292;&#19968;&#26086;&#29992;&#25143;&#36879;&#38706;&#20182;&#20204;&#26159;&#33258;&#30001;&#20027;&#20041;&#32773;&#65292;&#27169;&#22411;&#20250;&#36866;&#24212;&#33258;&#30001;&#20027;&#20041;&#35266;&#28857;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26469;&#20943;&#23569;&#36825;&#31181;&#34892;&#20026;&#12290;&#39318;&#20808;&#65292;&#22312;&#19968;&#32452;&#19977;&#20010;&#38463;&#35840;&#22857;&#25215;&#20219;&#21153;&#20013;&#65288;Perez&#31561;&#65292;2022&#65289;&#65292;&#27169;&#22411;&#34987;&#35201;&#27714;&#23545;&#27809;&#26377;&#27491;&#30830;&#31572;&#26696;&#30340;&#38472;&#36848;&#65288;&#20363;&#22914;&#65292;&#25919;&#27835;&#38382;&#39064;&#65289;&#21457;&#34920;&#24847;&#35265;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;&#25351;&#20196;&#35843;&#20248;&#26174;&#33879;&#22686;&#21152;&#20102;PaLM&#27169;&#22411;&#65288;&#22810;&#36798;540B&#21442;&#25968;&#65289;&#30340;&#38463;&#35840;&#22857;&#25215;&#34892;&#20026;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23558;&#38463;&#35840;&#22857;&#25215;&#35780;&#20272;&#25193;&#23637;&#21040;&#19968;&#20123;&#26126;&#26174;&#19981;&#27491;&#30830;&#30340;&#21152;&#27861;&#38472;&#36848;&#65292;&#21457;&#29616;&#23613;&#31649;&#27169;&#22411;&#30693;&#36947;&#36825;&#20123;&#38472;&#36848;&#26159;&#38169;&#35823;&#30340;&#65292;&#20294;&#22914;&#26524;&#29992;&#25143;&#20063;&#36825;&#20040;&#35748;&#20026;&#65292;&#35821;&#35328;&#27169;&#22411;&#20173;&#20250;&#21516;&#24847;&#36825;&#20123;&#38472;&#36848;&#12290;&#20026;&#20102;&#20943;&#23569;&#38463;&#35840;&#22857;&#25215;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21512;&#25104;&#25968;&#25454;&#24178;&#39044;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#20844;&#20849;&#30340;NLP&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sycophancy is an undesirable behavior where models tailor their responses to follow a human user's view even when that view is not objectively correct (e.g., adapting liberal views once a user reveals that they are liberal). In this paper, we study the prevalence of sycophancy in language models and propose a simple synthetic-data intervention to reduce this behavior.  First, on a set of three sycophancy tasks (Perez et al., 2022) where models are asked for an opinion on statements with no correct answers (e.g., politics), we observe that both model scaling and instruction tuning significantly increase sycophancy for PaLM models up to 540B parameters. Second, we extend sycophancy evaluations to simple addition statements that are objectively incorrect, finding that despite knowing that these statements are wrong, language models will still agree with them if the user does as well.  To reduce sycophancy, we present a straightforward synthetic-data intervention that takes public NLP task
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2308.03929</link><description>&lt;p&gt;
ChatGPT&#29983;&#29289;&#21307;&#23398;&#29983;&#25104;&#25991;&#26412;&#20013;&#24314;&#31435;&#20449;&#20219;&#30340;&#26041;&#27861;&#65306;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Establishing Trust in ChatGPT BioMedical Generated Text: An Ontology-Based Knowledge Graph to Validate Disease-Symptom Links. (arXiv:2308.03929v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03929
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#26500;&#24314;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#21644;&#30151;&#29366;&#26412;&#20307;&#26500;&#24314;&#25968;&#23398;&#27169;&#22411;&#65292;&#21033;&#29992;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#20998;&#26512;ChatGPT&#29983;&#25104;&#30340;&#25991;&#26412;&#19982;&#30495;&#23454;&#21307;&#23398;&#25991;&#29486;&#20043;&#38388;&#30340;&#20934;&#30830;&#24615;&#65292;&#20197;&#39564;&#35777;&#30142;&#30149;-&#30151;&#29366;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26041;&#27861;&#65306;&#36890;&#36807;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#20174;&#30495;&#23454;&#30340;&#21307;&#23398;&#25991;&#29486;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#20869;&#23481;&#26500;&#24314;&#20102;&#22522;&#20110;&#26412;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21306;&#20998;&#20107;&#23454;&#20449;&#24687;&#21644;&#26410;&#32463;&#39564;&#35777;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#65306;&#19968;&#20010;&#26159;&#20351;&#29992;&#8220;&#20154;&#31867;&#30142;&#30149;&#21644;&#30151;&#29366;&#8221;&#26597;&#35810;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#32534;&#35793;&#30340;&#65292;&#21478;&#19968;&#20010;&#26159;&#30001;ChatGPT&#29983;&#25104;&#30340;&#27169;&#25311;&#25991;&#31456;&#12290;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#65288;PubMed&#21644;ChatGPT&#65289;&#65292;&#25105;&#20204;&#38543;&#26426;&#36873;&#25321;&#20102;10&#32452;&#27599;&#32452;250&#20010;&#25688;&#35201;&#65292;&#24182;&#20351;&#29992;&#29305;&#23450;&#30340;&#31181;&#23376;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#35201;&#26159;&#21033;&#29992;&#30142;&#30149;&#26412;&#20307;&#65288;DOID&#65289;&#21644;&#30151;&#29366;&#26412;&#20307;&#65288;SYMP&#65289;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#20559;&#24046;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#20107;&#23454;&#26680;&#26597;&#31639;&#27861;&#21644;&#32593;&#32476;&#20013;&#24515;&#24230;&#25351;&#26631;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;GPT&#30142;&#30149;-&#30151;&#29366;&#38142;&#25509;&#20998;&#26512;&#65292;&#20197;&#37327;&#21270;&#22312;&#22122;&#22768;&#12289;&#20551;&#35774;&#21644;&#37325;&#35201;&#21457;&#29616;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;&#30340;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#65306;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;ChatGPT&#30693;&#35782;&#22270;&#35889;&#21450;&#20854;PubMed&#35745;&#25968;&#33719;&#24471;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#21457;&#29616;...
&lt;/p&gt;
&lt;p&gt;
Methods: Through an innovative approach, we construct ontology-based knowledge graphs from authentic medical literature and AI-generated content. Our goal is to distinguish factual information from unverified data. We compiled two datasets: one from biomedical literature using a "human disease and symptoms" query, and another generated by ChatGPT, simulating articles. With these datasets (PubMed and ChatGPT), we curated 10 sets of 250 abstracts each, selected randomly with a specific seed. Our method focuses on utilizing disease ontology (DOID) and symptom ontology (SYMP) to build knowledge graphs, robust mathematical models that facilitate unbiased comparisons. By employing our fact-checking algorithms and network centrality metrics, we conducted GPT disease-symptoms link analysis to quantify the accuracy of factual knowledge amid noise, hypotheses, and significant findings.  Results: The findings obtained from the comparison of diverse ChatGPT knowledge graphs with their PubMed count
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#20219;&#20309;&#35821;&#35328;&#30340;&#35821;&#38899;&#36716;&#20889;&#25104;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#65292;&#37096;&#20998;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#26377;&#28508;&#21147;&#26497;&#22823;&#21152;&#24555;&#28626;&#21361;&#35821;&#35328;&#30340;&#35760;&#24405;&#36895;&#24230;&#12290;&#34429;&#28982;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#23567;&#65292;&#20294;&#36136;&#37327;&#26356;&#39640;&#24182;&#36798;&#21040;&#20102;&#30456;&#23545;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#25509;&#36817;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.03917</link><description>&lt;p&gt;
&#36890;&#29992;&#33258;&#21160;&#22269;&#38469;&#38899;&#26631;&#36716;&#20889;&#25216;&#26415;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Universal Automatic Phonetic Transcription into the International Phonetic Alphabet. (arXiv:2308.03917v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03917
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#20219;&#20309;&#35821;&#35328;&#30340;&#35821;&#38899;&#36716;&#20889;&#25104;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#65292;&#37096;&#20998;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#26377;&#28508;&#21147;&#26497;&#22823;&#21152;&#24555;&#28626;&#21361;&#35821;&#35328;&#30340;&#35760;&#24405;&#36895;&#24230;&#12290;&#34429;&#28982;&#20351;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#27604;&#20043;&#21069;&#30340;&#27169;&#22411;&#23567;&#65292;&#20294;&#36136;&#37327;&#26356;&#39640;&#24182;&#36798;&#21040;&#20102;&#30456;&#23545;&#36739;&#22909;&#30340;&#32467;&#26524;&#65292;&#36824;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#36136;&#37327;&#25509;&#36817;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#20219;&#20309;&#35821;&#35328;&#30340;&#35821;&#38899;&#36716;&#20889;&#25104;&#22269;&#38469;&#38899;&#26631;&#65288;IPA&#65289;&#12290;&#23558;&#21475;&#35821;&#36716;&#20889;&#25104;&#22269;&#38469;&#38899;&#26631;&#26159;&#35821;&#35328;&#35760;&#24405;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#20294;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#37096;&#20998;&#33258;&#21160;&#21270;&#36825;&#20010;&#36807;&#31243;&#26377;&#28508;&#21147;&#26497;&#22823;&#21152;&#24555;&#28626;&#21361;&#35821;&#35328;&#30340;&#35760;&#24405;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#31867;&#20284;&#20110;&#20043;&#21069;&#26368;&#22909;&#30340;&#35821;&#38899;&#21040;&#22269;&#38469;&#38899;&#26631;&#27169;&#22411;&#65288;Wav2Vec2Phoneme&#65289;&#65292;&#22522;&#20110;wav2vec 2.0&#24182;&#36890;&#36807;&#24494;&#35843;&#39044;&#27979;&#38899;&#39057;&#36755;&#20837;&#30340;&#22269;&#38469;&#38899;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;CommonVoice 11.0&#30340;&#19971;&#31181;&#35821;&#35328;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#36807;&#21322;&#33258;&#21160;&#36716;&#20889;&#25104;&#22269;&#38469;&#38899;&#26631;&#12290;&#34429;&#28982;&#36825;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#27604;Wav2Vec2Phoneme&#30340;&#23567;&#24471;&#22810;&#65292;&#20294;&#20854;&#36136;&#37327;&#26356;&#39640;&#65292;&#20351;&#24471;&#25105;&#20204;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#29978;&#33267;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#36890;&#29992;&#35821;&#38899;&#21040;&#22269;&#38469;&#38899;&#26631;&#27169;&#22411;&#30340;&#36136;&#37327;&#25509;&#36817;&#20154;&#31867;&#27880;&#37322;&#32773;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a state-of-the-art model for transcribing speech in any language into the International Phonetic Alphabet (IPA). Transcription of spoken languages into IPA is an essential yet time-consuming process in language documentation, and even partially automating this process has the potential to drastically speed up the documentation of endangered languages. Like the previous best speech-to-IPA model (Wav2Vec2Phoneme), our model is based on wav2vec 2.0 and is fine-tuned to predict IPA from audio input. We use training data from seven languages from CommonVoice 11.0, transcribed into IPA semi-automatically. Although this training dataset is much smaller than Wav2Vec2Phoneme's, its higher quality lets our model achieve comparable or better results. Furthermore, we show that the quality of our universal speech-to-IPA models is close to that of human annotators.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#65292;&#35813;&#31995;&#32479;&#26356;&#21152;&#31169;&#23494;&#12289;&#21487;&#38752;&#12289;&#24555;&#36895;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;</title><link>http://arxiv.org/abs/2308.03905</link><description>&lt;p&gt;
&#22312;&#35774;&#22791;&#19978;&#30340;&#26234;&#33021;&#21161;&#25163;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Intelligent Assistant Language Understanding On Device. (arXiv:2308.03905v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#65292;&#35813;&#31995;&#32479;&#26356;&#21152;&#31169;&#23494;&#12289;&#21487;&#38752;&#12289;&#24555;&#36895;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23558;&#20010;&#20154;&#25968;&#23383;&#21161;&#25163;&#24212;&#29992;&#20110;&#25163;&#26426;&#21644;&#20854;&#20182;&#20010;&#20154;&#35774;&#22791;&#24050;&#25104;&#20026;&#21487;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#22312;&#35774;&#22791;&#19978;&#36816;&#34892;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#31995;&#32479;&#30340;&#35774;&#35745;&#12290;&#19982;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#21161;&#25163;&#30456;&#27604;&#65292;&#35813;&#31995;&#32479;&#26356;&#20855;&#31169;&#23494;&#24615;&#12289;&#21487;&#38752;&#24615;&#12289;&#36895;&#24230;&#24555;&#12289;&#34920;&#36798;&#26356;&#24378;&#12289;&#20934;&#30830;&#24615;&#26356;&#39640;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#26550;&#26500;&#21644;&#25216;&#26415;&#26041;&#38754;&#20570;&#20986;&#30340;&#20851;&#38190;&#36873;&#25321;&#12290;&#20363;&#22914;&#65292;&#23545;&#35805;&#31995;&#32479;&#25991;&#29486;&#20013;&#30340;&#19968;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#29615;&#22659;&#20013;&#38590;&#20197;&#38271;&#26399;&#32500;&#25252;&#12290;&#25105;&#20204;&#24076;&#26395;&#36890;&#36807;&#20998;&#20139;&#23454;&#36341;&#32463;&#39564;&#65292;&#20026;&#30740;&#31350;&#30028;&#30340;&#26410;&#26469;&#24037;&#20316;&#25552;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has recently become feasible to run personal digital assistants on phones and other personal devices. In this paper we describe a design for a natural language understanding system that runs on device. In comparison to a server-based assistant, this system is more private, more reliable, faster, more expressive, and more accurate. We describe what led to key choices about architecture and technologies. For example, some approaches in the dialog systems literature are difficult to maintain over time in a deployment setting. We hope that sharing learnings from our practical experiences may help inform future work in the research community.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#36328;&#39046;&#22495;&#30340;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#22240;&#26524;&#30693;&#35782;&#25552;&#21462;&#30340;&#19977;&#20010;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#21644;&#22522;&#20110;&#36328;&#24230;&#30340;&#25552;&#21462;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#23884;&#20837;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22522;&#20110;&#36328;&#24230;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2308.03891</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#35780;&#20272;&#22240;&#26524;&#30693;&#35782;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Cross-Domain Evaluation of Approaches for Causal Knowledge Extraction. (arXiv:2308.03891v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#36328;&#39046;&#22495;&#30340;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#22240;&#26524;&#30693;&#35782;&#25552;&#21462;&#30340;&#19977;&#20010;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#21644;&#22522;&#20110;&#36328;&#24230;&#30340;&#25552;&#21462;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#35789;&#23884;&#20837;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22522;&#20110;&#36328;&#24230;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#30693;&#35782;&#25552;&#21462;&#26159;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30456;&#20851;&#22240;&#26524;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#23545;&#20110;&#35821;&#35328;&#29702;&#35299;&#21644;&#30693;&#35782;&#21457;&#29616;&#24456;&#37325;&#35201;&#65292;&#20294;&#26159;&#26368;&#36817;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;&#25991;&#26412;&#29255;&#27573;&#20108;&#20998;&#31867;&#20026;&#22240;&#26524;&#25110;&#38750;&#22240;&#26524;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23545;&#19977;&#20010;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;&#22522;&#20110;&#36328;&#24230;&#30340;&#26041;&#27861;&#25552;&#21462;&#22240;&#26524;&#20851;&#31995;&#30340;&#25928;&#26524;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22797;&#26434;&#30340;&#27169;&#22411;&#26550;&#26500;&#30456;&#27604;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;BERT&#65289;&#30340;&#35789;&#23884;&#20837;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#28085;&#30422;&#19981;&#21516;&#39046;&#22495;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#22240;&#26524;&#30701;&#35821;&#30340;4&#20010;&#25968;&#25454;&#38598;&#20013;&#65292;&#22522;&#20110;&#36328;&#24230;&#30340;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#31616;&#21333;&#30340;&#24207;&#21015;&#26631;&#27880;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal knowledge extraction is the task of extracting relevant causes and effects from text by detecting the causal relation. Although this task is important for language understanding and knowledge discovery, recent works in this domain have largely focused on binary classification of a text segment as causal or non-causal. In this regard, we perform a thorough analysis of three sequence tagging models for causal knowledge extraction and compare it with a span based approach to causality extraction. Our experiments show that embeddings from pre-trained language models (e.g. BERT) provide a significant performance boost on this task compared to previous state-of-the-art models with complex architectures. We observe that span based models perform better than simple sequence tagging models based on BERT across all 4 data sets from diverse domains with different types of cause-effect phrases.
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#34920;&#32852;&#21512;&#25628;&#32034;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#22522;&#20934;</title><link>http://arxiv.org/abs/2308.03883</link><description>&lt;p&gt;
&#34920;&#32852;&#21512;&#25628;&#32034;&#29983;&#25104;&#24615;&#22522;&#20934;&#21019;&#24314;
&lt;/p&gt;
&lt;p&gt;
Generative Benchmark Creation for Table Union Search. (arXiv:2308.03883v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03883
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#34920;&#32852;&#21512;&#25628;&#32034;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31649;&#29702;&#20256;&#32479;&#19978;&#20381;&#38752;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#29983;&#25104;&#32467;&#26500;&#21270;&#22522;&#20934;&#65292;&#22914;TPC&#22871;&#20214;&#65292;&#25105;&#20204;&#21487;&#20197;&#31934;&#30830;&#25511;&#21046;&#37325;&#35201;&#21442;&#25968;&#22914;&#25968;&#25454;&#22823;&#23567;&#21644;&#20998;&#24067;&#12290;&#36825;&#20123;&#22522;&#20934;&#23545;&#20110;&#25968;&#25454;&#24211;&#31649;&#29702;&#31995;&#32479;&#30340;&#25104;&#21151;&#21644;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#31649;&#29702;&#38382;&#39064;&#23646;&#20110;&#35821;&#20041;&#24615;&#36136;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#20363;&#23376;&#26159;&#25214;&#21040;&#21487;&#20197;&#32852;&#21512;&#30340;&#34920;&#12290;&#34429;&#28982;&#20219;&#20309;&#20855;&#26377;&#30456;&#21516;&#22522;&#25968;&#30340;&#20004;&#20010;&#34920;&#37117;&#21487;&#20197;&#32852;&#21512;&#65292;&#34920;&#32852;&#21512;&#25628;&#32034;&#26159;&#25214;&#21040;&#20854;&#32852;&#21512;&#22312;&#35821;&#20041;&#19978;&#36830;&#36143;&#30340;&#34920;&#30340;&#38382;&#39064;&#12290;&#35821;&#20041;&#38382;&#39064;&#26080;&#27861;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30446;&#21069;&#21019;&#24314;&#30340;&#22522;&#20934;&#30340;&#26041;&#27861;&#28041;&#21450;&#23454;&#38469;&#25968;&#25454;&#30340;&#25163;&#21160;&#31574;&#21010;&#21644;&#26631;&#35760;&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#31283;&#20581;&#19988;&#19981;&#21487;&#25193;&#23637;&#65292;&#32780;&#19988;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#19981;&#28165;&#26970;&#25152;&#21019;&#24314;&#30340;&#22522;&#20934;&#30340;&#31283;&#20581;&#24615;&#22914;&#20309;&#12290;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#29983;&#25104;&#22411;AI&#27169;&#22411;&#20026;&#34920;&#32852;&#21512;&#25628;&#32034;&#21019;&#24314;&#32467;&#26500;&#21270;&#25968;&#25454;&#22522;&#20934;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data management has traditionally relied on synthetic data generators to generate structured benchmarks, like the TPC suite, where we can control important parameters like data size and its distribution precisely. These benchmarks were central to the success and adoption of database management systems. But more and more, data management problems are of a semantic nature. An important example is finding tables that can be unioned. While any two tables with the same cardinality can be unioned, table union search is the problem of finding tables whose union is semantically coherent. Semantic problems cannot be benchmarked using synthetic data. Our current methods for creating benchmarks involve the manual curation and labeling of real data. These methods are not robust or scalable and perhaps more importantly, it is not clear how robust the created benchmarks are. We propose to use generative AI models to create structured data benchmarks for table union search. We present a novel method 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#21644;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#31561;&#20215;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#23558;&#26597;&#35810;&#26144;&#23556;&#20026;&#25628;&#32034;&#24847;&#22270;&#21521;&#37327;&#34920;&#31034;&#12289;&#35782;&#21035;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#31561;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03869</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#30340;&#35821;&#20041;&#31561;&#20215;&#24615;
&lt;/p&gt;
&lt;p&gt;
Semantic Equivalence of e-Commerce Queries. (arXiv:2308.03869v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35782;&#21035;&#21644;&#21033;&#29992;&#30005;&#23376;&#21830;&#21153;&#26597;&#35810;&#31561;&#20215;&#24615;&#30340;&#26694;&#26550;&#65292;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#23558;&#26597;&#35810;&#26144;&#23556;&#20026;&#25628;&#32034;&#24847;&#22270;&#21521;&#37327;&#34920;&#31034;&#12289;&#35782;&#21035;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#31561;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36890;&#36807;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#25628;&#32034;&#20013;&#65292;&#26597;&#35810;&#21464;&#21270;&#20250;&#24102;&#26469;&#25361;&#25112;&#65292;&#22240;&#20026;&#30456;&#21516;&#30340;&#25628;&#32034;&#24847;&#22270;&#21487;&#20197;&#36890;&#36807;&#20855;&#26377;&#34920;&#23618;&#24046;&#24322;&#30340;&#19981;&#21516;&#26597;&#35810;&#26469;&#34920;&#36798;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#35782;&#21035;&#21644;&#21033;&#29992;&#26597;&#35810;&#31561;&#20215;&#24615;&#20197;&#25552;&#21319;&#25628;&#32034;&#32773;&#21644;&#21830;&#19994;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#19977;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#23558;&#26597;&#35810;&#26144;&#23556;&#21040;&#25628;&#32034;&#24847;&#22270;&#30340;&#21521;&#37327;&#34920;&#31034;&#65292;&#35782;&#21035;&#34920;&#36798;&#31561;&#20215;&#25110;&#30456;&#20284;&#24847;&#22270;&#30340;&#26368;&#36817;&#37051;&#26597;&#35810;&#65292;&#20197;&#21450;&#20248;&#21270;&#29992;&#25143;&#25110;&#21830;&#19994;&#30446;&#26631;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#34920;&#38754;&#30456;&#20284;&#24615;&#21644;&#34892;&#20026;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26597;&#35810;&#30340;&#31561;&#20215;&#24615;&#12290;&#34920;&#38754;&#30456;&#20284;&#24615;&#28041;&#21450;&#22522;&#20110;&#35789;&#30340;&#21464;&#24418;&#12289;&#35789;&#24207;&#12289;&#22797;&#21512;&#21644;&#22122;&#22768;&#35789;&#26469;&#35268;&#33539;&#21270;&#26597;&#35810;&#12290;&#34892;&#20026;&#30456;&#20284;&#24615;&#21033;&#29992;&#21382;&#21490;&#25628;&#32034;&#34892;&#20026;&#29983;&#25104;&#26597;&#35810;&#24847;&#22270;&#30340;&#21521;&#37327;&#34920;&#31034;&#12290;&#31163;&#32447;&#36807;&#31243;&#29992;&#20110;&#35757;&#32451;&#21477;&#23376;&#30456;&#20284;&#24615;&#27169;&#22411;&#65292;&#32780;&#22312;&#32447;&#26368;&#36817;&#37051;&#26041;&#27861;&#25903;&#25345;&#23545;&#26410;&#35265;&#26597;&#35810;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search query variation poses a challenge in e-commerce search, as equivalent search intents can be expressed through different queries with surface-level differences. This paper introduces a framework to recognize and leverage query equivalence to enhance searcher and business outcomes. The proposed approach addresses three key problems: mapping queries to vector representations of search intent, identifying nearest neighbor queries expressing equivalent or similar intent, and optimizing for user or business objectives. The framework utilizes both surface similarity and behavioral similarity to determine query equivalence. Surface similarity involves canonicalizing queries based on word inflection, word order, compounding, and noise words. Behavioral similarity leverages historical search behavior to generate vector representations of query intent. An offline process is used to train a sentence similarity model, while an online nearest neighbor approach supports processing of unseen qu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#25945;&#32946;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;XGBoost&#21644;BERT&#30340;&#32467;&#21512;&#26469;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29305;&#24449;&#26469;&#36755;&#20986;&#26657;&#27491;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.03866</link><description>&lt;p&gt;
&#22312;&#25945;&#32946;&#20013;&#20449;&#20219;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Trusting Language Models in Education. (arXiv:2308.03866v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#25945;&#32946;&#20013;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;XGBoost&#21644;BERT&#30340;&#32467;&#21512;&#26469;&#26657;&#20934;&#35821;&#35328;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#65292;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29305;&#24449;&#26469;&#36755;&#20986;&#26657;&#27491;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#25945;&#32946;&#39046;&#22495;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#23613;&#31649;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#38382;&#31572;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26377;&#26102;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#20026;&#20102;&#36991;&#20813;&#21521;&#23398;&#29983;&#23637;&#31034;&#38169;&#35823;&#31572;&#26696;&#65292;&#37325;&#35201;&#30340;&#26159;&#26657;&#20934;&#36825;&#20123;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230; - &#21363;&#39044;&#27979;&#27010;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#29305;&#24449;&#30340;XGBoost&#22312;BERT&#20043;&#19978;&#36755;&#20986;&#26657;&#27491;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#20551;&#35774;&#26159;&#27880;&#24847;&#21147;&#27969;&#20013;&#21253;&#21547;&#30340;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#19982;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models are being widely used in Education. Even though modern deep learning models achieve very good performance on question-answering tasks, sometimes they make errors. To avoid misleading students by showing wrong answers, it is important to calibrate the confidence - that is, the prediction probability - of these models. In our work, we propose to use an XGBoost on top of BERT to output the corrected probabilities, using features based on the attention mechanism. Our hypothesis is that the level of uncertainty contained in the flow of attention is related to the quality of the model's response itself.
&lt;/p&gt;</description></item><item><title>Storyfier&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#35760;&#24518;&#35789;&#27719;&#30340;&#29983;&#25104;&#25925;&#20107;&#65292;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#30340;AI&#36741;&#21161;&#25776;&#20889;&#26032;&#25925;&#20107;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;Storyfier&#30340;&#23398;&#20064;&#32773;&#22312;&#22238;&#24518;&#21644;&#20351;&#29992;&#30446;&#26631;&#35789;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;</title><link>http://arxiv.org/abs/2308.03864</link><description>&lt;p&gt;
Storyfier: &#20351;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#25506;&#32034;&#35789;&#27719;&#23398;&#20064;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Storyfier: Exploring Vocabulary Learning Support with Text Generation Models. (arXiv:2308.03864v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03864
&lt;/p&gt;
&lt;p&gt;
Storyfier&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20026;&#23398;&#20064;&#32773;&#25552;&#20379;&#20102;&#26377;&#21161;&#20110;&#35760;&#24518;&#35789;&#27719;&#30340;&#29983;&#25104;&#25925;&#20107;&#65292;&#24182;&#25552;&#20379;&#36866;&#24212;&#24615;&#30340;AI&#36741;&#21161;&#25776;&#20889;&#26032;&#25925;&#20107;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#39564;&#20013;&#65292;&#20351;&#29992;Storyfier&#30340;&#23398;&#20064;&#32773;&#22312;&#22238;&#24518;&#21644;&#20351;&#29992;&#30446;&#26631;&#35789;&#26041;&#38754;&#34920;&#29616;&#36739;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#27719;&#23398;&#20064;&#25903;&#25345;&#24037;&#20855;&#24191;&#27867;&#21033;&#29992;&#29616;&#26377;&#26448;&#26009;&#65292;&#22914;&#25925;&#20107;&#25110;&#35270;&#39057;&#29255;&#27573;&#65292;&#20316;&#20026;&#24110;&#21161;&#29992;&#25143;&#35760;&#24518;&#27599;&#20010;&#30446;&#26631;&#21333;&#35789;&#30340;&#19978;&#19979;&#25991;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#26080;&#27861;&#20026;&#23398;&#20064;&#32773;&#24863;&#20852;&#36259;&#30340;&#20219;&#20309;&#30446;&#26631;&#35789;&#25552;&#20379;&#36830;&#36143;&#30340;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#23427;&#20204;&#24456;&#23569;&#24110;&#21161;&#23454;&#36341;&#21333;&#35789;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19982;&#25945;&#24072;&#21644;&#23398;&#29983;&#21512;&#20316;&#65292;&#36845;&#20195;&#22320;&#24320;&#21457;&#20102;Storyfier&#65292;&#23427;&#21033;&#29992;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#20351;&#23398;&#20064;&#32773;&#33021;&#22815;&#38405;&#35835;&#35206;&#30422;&#20219;&#20309;&#30446;&#26631;&#35789;&#30340;&#29983;&#25104;&#25925;&#20107;&#65292;&#36827;&#34892;&#25925;&#20107;&#20811;&#38534;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#21333;&#35789;&#25776;&#20889;&#19968;&#20010;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#30340;&#26032;&#25925;&#20107;&#12290;&#25105;&#20204;&#30340;&#34987;&#35797;&#30740;&#31350;&#65288;N = 28&#65289;&#26174;&#31034;&#65292;&#23398;&#20064;&#32773;&#26222;&#36941;&#21916;&#27426;&#29983;&#25104;&#30340;&#25925;&#20107;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36830;&#25509;&#30446;&#26631;&#35789;&#24182;&#20943;&#36731;&#23398;&#20064;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#38405;&#35835;-&#20811;&#38534;-&#20889;&#23398;&#20064;&#20250;&#35805;&#20013;&#65292;&#20351;&#29992;Storyfier&#30340;&#21442;&#19982;&#32773;&#22312;&#22238;&#24518;&#21644;&#20351;&#29992;&#30446;&#26631;&#35789;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#22522;&#32447;&#24037;&#20855;&#65288;&#19981;&#21547;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#29305;&#24615;&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#23545;&#25903;&#25345;&#35789;&#27719;&#23398;&#20064;&#30340;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vocabulary learning support tools have widely exploited existing materials, e.g., stories or video clips, as contexts to help users memorize each target word. However, these tools could not provide a coherent context for any target words of learners' interests, and they seldom help practice word usage. In this paper, we work with teachers and students to iteratively develop Storyfier, which leverages text generation models to enable learners to read a generated story that covers any target words, conduct a story cloze test, and use these words to write a new story with adaptive AI assistance. Our within-subjects study (N=28) shows that learners generally favor the generated stories for connecting target words and writing assistance for easing their learning workload. However, in the read-cloze-write learning sessions, participants using Storyfier perform worse in recalling and using target words than learning with a baseline tool without our AI features. We discuss insights into suppor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#27880;&#37322;&#26041;&#26696;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#20462;&#36766;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2308.03853</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#21307;&#23398;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#35814;&#32454;&#30340;&#32959;&#30244;&#30149;&#21490;&#21644;&#27835;&#30103;&#35745;&#21010;
&lt;/p&gt;
&lt;p&gt;
Extracting detailed oncologic history and treatment plan from medical oncology notes with large language models. (arXiv:2308.03853v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#27880;&#37322;&#26041;&#26696;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#25552;&#21462;&#21644;&#25512;&#29702;&#22797;&#26434;&#30340;&#20462;&#36766;&#65292;&#24182;&#24212;&#29992;&#20110;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25252;&#29702;&#21644;&#32959;&#30244;&#23398;&#35266;&#23519;&#30740;&#31350;&#37117;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#30142;&#30149;&#36827;&#23637;&#21644;&#27835;&#30103;&#21382;&#21490;&#65292;&#36825;&#20123;&#20449;&#24687;&#36890;&#24120;&#22312;&#20020;&#24202;&#35760;&#24405;&#20013;&#35814;&#32454;&#35760;&#24405;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32959;&#30244;&#23398;&#20013;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#20294;&#30446;&#21069;&#27809;&#26377;&#38024;&#23545;&#36825;&#20123;&#35760;&#24405;&#20013;&#35760;&#24405;&#30340;&#22810;&#26679;&#20449;&#24687;&#36827;&#34892;&#23436;&#25972;&#23553;&#35013;&#30340;&#32959;&#30244;&#23398;&#20449;&#24687;&#34920;&#31034;&#21644;&#27880;&#37322;&#26041;&#26696;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26368;&#36817;&#22312;&#21508;&#31181;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#30446;&#21069;&#32570;&#20047;&#20840;&#38754;&#27880;&#37322;&#30340;&#32959;&#30244;&#23398;&#25968;&#25454;&#38598;&#65292;&#23545;LLM&#22312;&#25552;&#21462;&#21644;&#25512;&#29702;&#32959;&#30244;&#23398;&#31508;&#35760;&#20013;&#30340;&#22797;&#26434;&#20462;&#36766;&#30340;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#19981;&#36275;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#26041;&#26696;&#65292;&#29992;&#20110;&#27880;&#37322;&#32959;&#30244;&#23398;&#25991;&#26412;&#20449;&#24687;&#65292;&#21253;&#25324;&#24739;&#32773;&#29305;&#24449;&#12289;&#32959;&#30244;&#29305;&#24449;&#12289;&#27979;&#35797;&#12289;&#27835;&#30103;&#21644;&#26102;&#38388;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#21152;&#21033;&#31119;&#23612;&#20122;&#22823;&#23398;&#26087;&#37329;&#23665;&#20998;&#26657;&#30340;10&#20010;&#21435;&#26631;&#35782;&#21270;&#20083;&#33146;&#30284;&#36827;&#23637;&#31508;&#35760;&#35821;&#26009;&#24211;&#65292;&#24212;&#29992;&#20102;&#36825;&#20010;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both medical care and observational studies in oncology require a thorough understanding of a patient's disease progression and treatment history, often elaborately documented in clinical notes. Despite their vital role, no current oncology information representation and annotation schema fully encapsulates the diversity of information recorded within these notes. Although large language models (LLMs) have recently exhibited impressive performance on various medical natural language processing tasks, due to the current lack of comprehensively annotated oncology datasets, an extensive evaluation of LLMs in extracting and reasoning with the complex rhetoric in oncology notes remains understudied. We developed a detailed schema for annotating textual oncology information, encompassing patient characteristics, tumor characteristics, tests, treatments, and temporality. Using a corpus of 10 de-identified breast cancer progress notes at University of California, San Francisco, we applied this
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#30740;&#31350;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;</title><link>http://arxiv.org/abs/2308.03800</link><description>&lt;p&gt;
&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#25991;&#26412;&#25968;&#25454;&#25366;&#25496;&#65306;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Textual Data Mining for Financial Fraud Detection: A Deep Learning Approach. (arXiv:2308.03800v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03800
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#24182;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#27604;&#36739;&#22810;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#35813;&#30740;&#31350;&#23545;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#36827;&#34892;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;&#20197;&#19979;&#31616;&#31216;NLP&#65289;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#20197;&#20998;&#26512;&#37329;&#34701;&#27450;&#35784;&#25991;&#26412;&#12290;&#39318;&#20808;&#65292;&#25105;&#25628;&#32034;&#20102;&#28207;&#20132;&#25152;&#26032;&#38395;&#30340;&#30417;&#31649;&#20844;&#21578;&#21644;&#25191;&#27861;&#20844;&#21578;&#65292;&#20197;&#23450;&#20041;&#27450;&#35784;&#20844;&#21496;&#24182;&#25552;&#21462;&#20854;MD&#65286;A&#25253;&#21578;&#65292;&#28982;&#21518;&#25972;&#29702;&#20102;&#25253;&#21578;&#20013;&#30340;&#21477;&#23376;&#65292;&#24182;&#26631;&#35760;&#20102;&#25253;&#21578;&#26102;&#38388;&#12290;&#25105;&#30340;&#26041;&#27861;&#21253;&#25324;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21253;&#25324;&#20855;&#26377;&#23884;&#20837;&#23618;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#65292;&#22522;&#26412;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#65292;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#21644;&#38376;&#38480;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20010;&#22810;&#26679;&#21270;&#30340;&#27169;&#22411;&#38598;&#21512;&#65292;&#25105;&#26088;&#22312;&#20840;&#38754;&#27604;&#36739;&#23427;&#20204;&#22312;&#26816;&#27979;&#37329;&#34701;&#27450;&#35784;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#30340;&#32467;&#26524;&#23545;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#22240;&#20026;&#36825;&#39033;&#24037;&#20316;&#20026;&#28145;&#24230;&#23398;&#20064;&#65292;NLP&#21644;&#37329;&#34701;&#20132;&#21449;&#30740;&#31350;&#30340;&#19981;&#26029;&#22686;&#38271;&#36129;&#29486;&#20102;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#25104;&#26524;
&lt;/p&gt;
&lt;p&gt;
In this report, I present a deep learning approach to conduct a natural language processing (hereafter NLP) binary classification task for analyzing financial-fraud texts. First, I searched for regulatory announcements and enforcement bulletins from HKEX news to define fraudulent companies and to extract their MD&amp;A reports before I organized the sentences from the reports with labels and reporting time. My methodology involved different kinds of neural network models, including Multilayer Perceptrons with Embedding layers, vanilla Recurrent Neural Network (RNN), Long-Short Term Memory (LSTM), and Gated Recurrent Unit (GRU) for the text classification task. By utilizing this diverse set of models, I aim to perform a comprehensive comparison of their accuracy in detecting financial fraud. My results bring significant implications for financial fraud detection as this work contributes to the growing body of research at the intersection of deep learning, NLP, and finance, providing valuabl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25214;&#20986;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#21644;&#20351;&#29992;&#25490;&#21517;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03795</link><description>&lt;p&gt;
&#24536;&#35760;&#28436;&#31034;&#65292;&#19987;&#27880;&#20110;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Forget Demonstrations, Focus on Learning from Textual Instructions. (arXiv:2308.03795v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#25214;&#20986;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#21644;&#20351;&#29992;&#25490;&#21517;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#38646;&#31034;&#33539;&#36328;&#20219;&#21153;&#27867;&#21270;&#30340;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26356;&#30495;&#23454;&#30340;&#24773;&#22659;&#36827;&#34892;&#30740;&#31350;&#65306;&#20174;&#25991;&#26412;&#25351;&#20196;&#20013;&#23398;&#20064;&#32780;&#26080;&#38656;&#31034;&#33539;&#65292;&#20551;&#35774;&#23384;&#22312;&#19968;&#31181;&#27573;&#33853;&#24335;&#20219;&#21153;&#23450;&#20041;&#20294;&#19981;&#23384;&#22312;&#31034;&#33539;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#20174;&#23450;&#20041;&#20013;&#23398;&#20064;&#20219;&#21153;&#30417;&#30563;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65306;&#39318;&#20808;&#65292;&#33258;&#21160;&#25214;&#20986;&#23450;&#20041;&#20013;&#30340;&#20851;&#38190;&#21477;&#23376;&#65307;&#20854;&#27425;&#65292;&#20351;&#29992;&#25490;&#21517;&#30446;&#26631;&#26469;&#24378;&#21046;&#27169;&#22411;&#22312;&#23450;&#20041;&#20013;&#31361;&#20986;&#26174;&#31034;&#36825;&#20123;&#20851;&#38190;&#37096;&#20998;&#26102;&#29983;&#25104;&#37329;&#26631;&#36755;&#20986;&#30340;&#27010;&#29575;&#26356;&#39640;&#12290;&#36825;&#20004;&#31181;&#31574;&#30053;&#30340;&#20849;&#21516;&#21162;&#21147;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#20135;&#29983;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#35770;&#25991;&#30340;&#26368;&#32456;&#29256;&#26412;&#20013;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies a challenging yet more realistic setting for zero-shot cross-task generalization: demonstration-free learning from textual instructions, presuming the existence of a paragraph-style task definition while no demonstrations exist. To better learn the task supervision from the definition, we propose two strategies: first, to automatically find out the critical sentences in the definition; second, a ranking objective to force the model to generate the gold outputs with higher probabilities when those critical parts are highlighted in the definition. The joint efforts of the two strategies yield state-of-the-art performance on the challenging benchmark. Our code will be released in the final version of the paper.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#20215;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT Base&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;11%&#30340;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.03782</link><description>&lt;p&gt;
Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#35770;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Bio+Clinical BERT, BERT Base, and CNN Performance Comparison for Predicting Drug-Review Satisfaction. (arXiv:2308.03782v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03782
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;Bio+Clinical BERT&#12289;BERT Base&#21644;CNN&#22312;&#39044;&#27979;&#33647;&#29289;&#35780;&#20215;&#28385;&#24847;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT Base&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;11%&#30340;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#33021;&#22815;&#20998;&#26512;&#24739;&#32773;&#33647;&#29289;&#35780;&#35770;&#24182;&#20934;&#30830;&#20998;&#31867;&#28385;&#24847;&#31243;&#24230;&#20026;&#31215;&#26497;&#12289;&#20013;&#24615;&#25110;&#28040;&#26497;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#27169;&#22411;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#23558;&#20943;&#36731;&#21307;&#30103;&#20445;&#20581;&#19987;&#19994;&#20154;&#21592;&#30340;&#24037;&#20316;&#36127;&#25285;&#65292;&#24182;&#25552;&#20379;&#26356;&#22810;&#20851;&#20110;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#30340;&#35265;&#35299;&#65292;&#36825;&#26159;&#27835;&#30103;&#25928;&#26524;&#30340;&#37325;&#35201;&#25351;&#26631;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#22810;&#20010;&#20998;&#31867;&#27169;&#22411;&#65292;&#21253;&#25324;BERT base&#27169;&#22411;&#12289;Bio+Clinical BERT&#20197;&#21450;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;CNN&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21307;&#23398;&#39046;&#22495;&#29305;&#23450;&#30340;Bio+Clinical BERT&#27169;&#22411;&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#26174;&#33879;&#20248;&#20110;&#36890;&#29992;&#39046;&#22495;&#30340;BERT base&#27169;&#22411;&#65292;&#22914;&#34920;2&#25152;&#31034;&#65292;&#23427;&#22312;Macro F1&#21644;&#21484;&#22238;&#29575;&#24471;&#20998;&#19978;&#25552;&#21319;&#20102;11%&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#21487;&#20197;&#25506;&#32034;&#22914;&#20309;&#20805;&#20998;&#21033;&#29992;&#27599;&#20010;&#27169;&#22411;&#30340;&#29305;&#23450;&#20248;&#21183;&#12290;Bio+Clinical BERT&#22312;&#25972;&#20307;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#21307;&#23398;&#34892;&#35805;&#26041;&#38754;&#65292;&#32780;&#26356;&#31616;&#21333;&#30340;CNN&#21017;&#23637;&#29616;&#20986;&#35782;&#21035;&#20851;&#38190;&#35789;&#21644;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of this study is to develop natural language processing (NLP) models that can analyze patients' drug reviews and accurately classify their satisfaction levels as positive, neutral, or negative. Such models would reduce the workload of healthcare professionals and provide greater insight into patients' quality of life, which is a critical indicator of treatment effectiveness. To achieve this, we implemented and evaluated several classification models, including a BERT base model, Bio+Clinical BERT, and a simpler CNN. Results indicate that the medical domain-specific Bio+Clinical BERT model significantly outperformed the general domain base BERT model, achieving macro f1 and recall score improvement of 11%, as shown in Table 2. Future research could explore how to capitalize on the specific strengths of each model. Bio+Clinical BERT excels in overall performance, particularly with medical jargon, while the simpler CNN demonstrates the ability to identify crucial words and a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#21477;&#23383;&#24149;&#25551;&#36848;3D&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#22330;&#26223;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#19979;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.03767</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#22686;&#24378;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhancing image captioning with depth information using a Transformer-based framework. (arXiv:2308.03767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#12290;&#36890;&#36807;&#20351;&#29992;&#22810;&#21477;&#23383;&#24149;&#25551;&#36848;3D&#22330;&#26223;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#22330;&#26223;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#19979;&#36827;&#34892;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#26159;&#36830;&#25509;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#12290;&#34429;&#28982;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#20248;&#31168;&#25551;&#36848;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35813;&#39046;&#22495;&#20027;&#35201;&#38598;&#20013;&#20110;&#20026;2D&#22270;&#20687;&#29983;&#25104;&#21333;&#20010;&#21477;&#23376;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#23558;&#28145;&#24230;&#20449;&#24687;&#19982;RGB&#22270;&#20687;&#38598;&#25104;&#65292;&#33021;&#21542;&#22686;&#24378;&#23383;&#24149;&#29983;&#25104;&#20219;&#21153;&#24182;&#29983;&#25104;&#26356;&#22909;&#30340;&#25551;&#36848;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Transformer&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;3D&#22330;&#26223;&#30340;&#22810;&#21477;&#23383;&#24149;&#25551;&#36848;&#12290;RGB&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#28145;&#24230;&#22270;&#20316;&#20026;&#25105;&#20204;&#26694;&#26550;&#30340;&#36755;&#20837;&#65292;&#23558;&#23427;&#20204;&#32467;&#21512;&#36215;&#26469;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#36755;&#20837;&#22330;&#26223;&#12290;&#28145;&#24230;&#22270;&#21487;&#20197;&#26159;&#30495;&#23454;&#20540;&#25110;&#20272;&#35745;&#20540;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#20219;&#20309;RGB&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#37117;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#26469;&#34701;&#21512;RGB&#21644;&#28145;&#24230;&#22270;&#20687;&#12290;&#23454;&#39564;&#22312;NYU-v2&#25968;&#25454;&#38598;&#21644;Stanford&#22270;&#20687;&#38598;&#19978;&#36827;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Captioning images is a challenging scene-understanding task that connects computer vision and natural language processing. While image captioning models have been successful in producing excellent descriptions, the field has primarily focused on generating a single sentence for 2D images. This paper investigates whether integrating depth information with RGB images can enhance the captioning task and generate better descriptions. For this purpose, we propose a Transformer-based encoder-decoder framework for generating a multi-sentence description of a 3D scene. The RGB image and its corresponding depth map are provided as inputs to our framework, which combines them to produce a better understanding of the input scene. Depth maps could be ground truth or estimated, which makes our framework widely applicable to any RGB captioning dataset. We explored different fusion approaches to fuse RGB and depth images. The experiments are performed on the NYU-v2 dataset and the Stanford image para
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#25512;&#29702;&#26041;&#38754;&#26080;&#33021;&#20026;&#21147;&#65292;&#23613;&#31649;&#26377;&#30528;&#20598;&#23572;&#26174;&#31034;&#30340;&#20998;&#26512;&#25165;&#26234;&#12290;</title><link>http://arxiv.org/abs/2308.03762</link><description>&lt;p&gt;
GPT-4&#26080;&#27861;&#36827;&#34892;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
GPT-4 Can't Reason. (arXiv:2308.03762v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03762
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#25512;&#29702;&#26041;&#38754;&#26080;&#33021;&#20026;&#21147;&#65292;&#23613;&#31649;&#26377;&#30528;&#20598;&#23572;&#26174;&#31034;&#30340;&#20998;&#26512;&#25165;&#26234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GPT-4&#20110;2023&#24180;3&#26376;&#21457;&#24067;&#65292;&#24191;&#21463;&#22909;&#35780;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;GPT-3.5&#65288;OpenAI&#20043;&#21069;&#26368;&#22909;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;ChatGPT&#30340;&#21021;&#27425;&#21457;&#24067;&#65289;&#65292;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25913;&#36827;&#65292;&#23545;&#20110;GPT-4&#30340;&#25512;&#29702;&#33021;&#21147;&#23384;&#22312;&#20805;&#20998;&#30340;&#24576;&#30097;&#26159;&#26377;&#36947;&#29702;&#30340;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#25512;&#29702;&#30340;&#26412;&#36136;&#65307;&#25209;&#35780;&#20102;&#24403;&#21069;NLP&#31038;&#21306;&#20013;&#25512;&#29702;&#38382;&#39064;&#30340;&#34920;&#36848;&#26041;&#24335;&#65292;&#20197;&#21450;&#30446;&#21069;LLM&#25512;&#29702;&#24615;&#33021;&#30340;&#35780;&#20272;&#26041;&#24335;&#65307;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#30001;21&#20010;&#22810;&#26679;&#21270;&#25512;&#29702;&#38382;&#39064;&#32452;&#25104;&#30340;&#38598;&#21512;&#65307;&#24182;&#23545;GPT-4&#22312;&#36825;&#20123;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#26412;&#25991;&#24471;&#20986;&#32467;&#35770;&#65292;&#23613;&#31649;&#20598;&#23572;&#26174;&#31034;&#20986;&#20998;&#26512;&#19978;&#30340;&#25165;&#26234;&#65292;&#20294;&#30446;&#21069;&#30340;GPT-4&#23436;&#20840;&#26080;&#27861;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4 was released in March 2023 to wide acclaim, marking a very substantial improvement across the board over GPT-3.5 (OpenAI's previously best model, which had powered the initial release of ChatGPT). However, despite the genuinely impressive improvement, there are good reasons to be highly skeptical of GPT-4's ability to reason. This position paper discusses the nature of reasoning; criticizes the current formulation of reasoning problems in the NLP community, as well as the way in which LLM reasoning performance is currently evaluated; introduces a small collection of 21 diverse reasoning problems; and performs a detailed qualitative evaluation of GPT-4's performance on those problems. Based on this analysis, the paper concludes that, despite its occasional flashes of analytical brilliance, GPT-4 at present is utterly incapable of reasoning.
&lt;/p&gt;</description></item><item><title>MedMine&#36890;&#36807;&#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.03629</link><description>&lt;p&gt;
MedMine: &#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
MedMine: Examining Pre-trained Language Models on Medication Mining. (arXiv:2308.03629v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03629
&lt;/p&gt;
&lt;p&gt;
MedMine&#36890;&#36807;&#26816;&#39564;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#33647;&#29289;&#25366;&#25496;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20174;&#20020;&#24202;&#21644;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#36827;&#34892;&#33647;&#29289;&#25366;&#25496;&#24050;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#35805;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#23545;&#21307;&#30103;&#24212;&#29992;&#30340;&#30495;&#23454;&#24433;&#21709;&#20197;&#21450;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#20840;&#33258;&#21160;&#25552;&#21462;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#38556;&#30861;&#65292;&#20197;&#20415;&#21487;&#20197;&#30452;&#25509;&#37096;&#32626;&#21040;&#20020;&#24202;&#23454;&#36341;&#20013;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#38556;&#30861;&#21253;&#25324;&#23427;&#20204;&#22312;&#19981;&#21516;&#23454;&#20307;&#31867;&#22411;&#21644;&#20020;&#24202;&#20107;&#20214;&#19978;&#30340;&#19981;&#24179;&#34913;&#34920;&#29616;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;&#65292;&#21253;&#25324;&#22522;&#20110;&#21333;&#35821;&#35328;&#27169;&#22411;Med7&#21644;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;XLM-RoBERTa&#30340;&#26041;&#24335;&#65292;&#26816;&#39564;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#25361;&#25112;&#36187;&#30340;&#21382;&#21490;&#33647;&#29289;&#25366;&#25496;&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23427;&#20204;&#30340;&#20248;&#21155;&#27604;&#36739;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#36825;&#20123;&#24494;&#35843;&#23454;&#39564;&#30340;&#32467;&#26524;&#65292;&#20197;&#20415;&#20419;&#36827;&#26410;&#26469;&#30740;&#31350;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#27604;&#22914;&#22914;&#20309;&#32467;&#21512;&#23427;&#20204;&#30340;&#36755;&#20986;&#65292;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25110;&#32773;&#25913;&#36827;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic medication mining from clinical and biomedical text has become a popular topic due to its real impact on healthcare applications and the recent development of powerful language models (LMs). However, fully-automatic extraction models still face obstacles to be overcome such that they can be deployed directly into clinical practice for better impacts. Such obstacles include their imbalanced performances on different entity types and clinical events. In this work, we examine current state-of-the-art pre-trained language models (PLMs) on such tasks, via fine-tuning including the monolingual model Med7 and multilingual large language model (LLM) XLM-RoBERTa. We compare their advantages and drawbacks using historical medication mining shared task data sets from n2c2-2018 challenges. We report the findings we get from these fine-tuning experiments such that they can facilitate future research on addressing them, for instance, how to combine their outputs, merge such models, or impr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#30340;&#25299;&#25169;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#21477;&#23376;&#21521;&#37327;&#30340;&#36317;&#31163;&#21644;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.03565</link><description>&lt;p&gt;
GPT-3&#30340;&#25299;&#25169;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Topological Interpretations of GPT-3. (arXiv:2308.03565v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#30340;&#25299;&#25169;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#35745;&#31639;&#21477;&#23376;&#21521;&#37327;&#30340;&#36317;&#31163;&#21644;&#30456;&#20851;&#24615;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#21477;&#23376;&#20043;&#38388;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#39033;&#23454;&#35777;&#30740;&#31350;&#65292;&#26088;&#22312;&#25506;&#32034;&#19968;&#31181;&#19968;&#33268;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25512;&#23548;&#21477;&#21521;&#37327;&#19982;&#21477;&#23376;&#35821;&#20041;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#19977;&#31181;&#26368;&#20808;&#36827;&#30340;&#35789;/&#21477;&#23376;&#23884;&#20837;&#26041;&#27861;&#65292;&#21253;&#25324;GPT-3&#65292;Word2Vec&#21644;Sentence-BERT&#65292;&#23558;&#32431;&#25991;&#26412;&#21477;&#23376;&#23383;&#31526;&#20018;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#23884;&#20837;&#31354;&#38388;&#20013;&#20219;&#24847;&#20004;&#20010;&#21477;&#21521;&#37327;&#30340;&#37197;&#23545;&#36317;&#31163;&#65292;&#24182;&#23558;&#20854;&#26144;&#23556;&#21040;&#19968;&#20010;&#30697;&#38453;&#20013;&#12290;&#26681;&#25454;&#27599;&#20010;&#36317;&#31163;&#30697;&#38453;&#65292;&#25105;&#20204;&#35745;&#31639;&#21477;&#21521;&#37327;&#30456;&#23545;&#20110;&#20854;&#20182;&#21477;&#21521;&#37327;&#30340;&#36317;&#31163;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#36317;&#31163;&#30697;&#38453;&#23545;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#30456;&#21516;&#21477;&#23376;&#22312;&#19981;&#21516;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#23884;&#20837;&#31354;&#38388;&#20013;&#19981;&#21516;&#21477;&#23376;&#30340;&#30456;&#20851;&#24615;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#19982;&#25105;&#20204;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#24182;&#24102;&#39046;&#25105;&#20204;&#36827;&#20837;&#19979;&#19968;&#20010;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is an experiential study of investigating a consistent method for deriving the correlation between sentence vector and semantic meaning of a sentence. We first used three state-of-the-art word/sentence embedding methods including GPT-3, Word2Vec, and Sentence-BERT, to embed plain text sentence strings into high dimensional spaces. Then we compute the pairwise distance between any possible combination of two sentence vectors in an embedding space and map them into a matrix. Based on each distance matrix, we compute the correlation of distances of a sentence vector with respect to the other sentence vectors in an embedding space. Then we compute the correlation of each pair of the distance matrices. We observed correlations of the same sentence in different embedding spaces and correlations of different sentences in the same embedding space. These observations are consistent with our hypothesis and take us to the next stage.
&lt;/p&gt;</description></item><item><title>RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.03421</link><description>&lt;p&gt;
RecycleGPT&#65306;&#19968;&#31181;&#20855;&#26377;&#21487;&#22238;&#25910;&#27169;&#22359;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
RecycleGPT: An Autoregressive Language Model with Recyclable Module. (arXiv:2308.03421v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03421
&lt;/p&gt;
&lt;p&gt;
RecycleGPT&#26159;&#19968;&#31181;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#22810;&#27425;&#36816;&#34892;&#25972;&#20010;&#27169;&#22411;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#20854;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#39640;&#36895;&#35299;&#30721;&#21644;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24517;&#39035;&#36816;&#34892;K&#27425;&#25165;&#33021;&#29983;&#25104;K&#20010;&#20196;&#29260;&#30340;&#24207;&#21015;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RecycleGPT&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#24555;&#36895;&#35299;&#30721;&#36895;&#24230;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22238;&#25910;&#39044;&#29983;&#25104;&#30340;&#27169;&#22411;&#29366;&#24577;&#32780;&#26080;&#38656;&#23558;&#25972;&#20010;&#27169;&#22411;&#36816;&#34892;&#22810;&#27425;&#27493;&#39588;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#65306;&#24207;&#21015;&#20013;&#30456;&#37051;&#30340;&#20196;&#29260;&#36890;&#24120;&#20855;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#21487;&#20197;&#26681;&#25454;&#21069;&#38754;&#30340;&#20196;&#29260;&#21512;&#29702;&#29468;&#27979;&#25110;&#25512;&#26029;&#20986;&#19979;&#19968;&#20010;&#20196;&#29260;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38477;&#20302;&#25512;&#29702;&#24310;&#36831;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;1.4&#20493;&#30340;&#21152;&#36895;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing large language models have to run K times to generate a sequence of K tokens. In this paper, we present RecycleGPT, a generative language model with fast decoding speed by recycling pre-generated model states without running the whole model in multiple steps. Our approach relies on the observation that adjacent tokens in a sequence usually have strong correlations and the next token in a sequence can be reasonably guessed or inferred based on the preceding ones. Experiments and analysis demonstrate the effectiveness of our approach in lowering inference latency, achieving up to 1.4x speedup while preserving high performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#26469;&#22686;&#24378;&#21305;&#37197;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#30340;3.9\%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.03131</link><description>&lt;p&gt;
&#36808;&#21521;&#22810;&#21442;&#32771;&#26102;&#20195; &#8212;&#8212; &#35299;&#20915;NLG&#35780;&#20272;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#21644;&#21442;&#32771;&#22810;&#26679;&#24615;&#26377;&#38480;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards Multiple References Era -- Addressing Data Leakage and Limited Reference Diversity in NLG Evaluation. (arXiv:2308.03131v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#22810;&#20010;&#21442;&#32771;&#26469;&#22686;&#24378;&#21305;&#37197;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#65292;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#30340;3.9\%&#30340;&#20934;&#30830;&#29575;&#25552;&#39640;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-gram&#21305;&#37197;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#22914;BLEU&#21644;chrF&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#20123;&#22522;&#20110;&#21305;&#37197;&#30340;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#36739;&#24369;&#30340;&#30456;&#20851;&#24615;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#26631;&#22914;BLEURT&#30456;&#27604;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#27979;&#21305;&#37197;&#25351;&#26631;&#24615;&#33021;&#29942;&#39048;&#30340;&#21407;&#22240;&#21487;&#33021;&#26159;&#21442;&#32771;&#36164;&#26009;&#22810;&#26679;&#24615;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;"&#22810;&#20010;&#21442;&#32771;"&#26469;&#22686;&#24378;&#36825;&#20123;&#25351;&#26631;&#19982;&#20154;&#31867;&#35780;&#20272;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#22312;WMT Metrics&#22522;&#20934;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22810;&#21442;&#32771;F200spBLEU&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#21333;&#21442;&#32771;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;7.2\%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#36824;&#36229;&#36807;&#20102;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;BERTscore&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;3.9\%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#21040;&#32531;&#35299;&#36890;&#36807;&#25105;&#20204;&#30340;&#22810;&#21442;&#32771;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-gram matching-based evaluation metrics, such as BLEU and chrF, are widely utilized across a range of natural language generation (NLG) tasks. However, recent studies have revealed a weak correlation between these matching-based metrics and human evaluations, especially when compared with neural-based metrics like BLEURT. In this paper, we conjecture that the performance bottleneck in matching-based metrics may be caused by the limited diversity of references. To address this issue, we propose to utilize \textit{multiple references} to enhance the consistency between these metrics and human evaluations. Within the WMT Metrics benchmarks, we observe that the multi-references F200spBLEU surpasses the conventional single-reference one by an accuracy improvement of 7.2\%. Remarkably, it also exceeds the neural-based BERTscore by an accuracy enhancement of 3.9\%. Moreover, we observe that the data leakage issue in large language models (LLMs) can be mitigated to a large extent by our multi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2308.02582</link><description>&lt;p&gt;
&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Adapt and Decompose: Efficient Generalization of Text-to-SQL via Domain Adapted Least-To-Most Prompting. (arXiv:2308.02582v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02582
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#21644;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#30340;&#26041;&#24335;&#23454;&#29616;&#25991;&#26412;&#21040;SQL&#30340;&#39640;&#25928;&#27867;&#21270;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#24182;&#21512;&#25104;&#19968;&#20010;&#36890;&#29992;&#25552;&#31034;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#21644;&#20998;&#35299;&#30340;&#26041;&#27861;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#39046;&#22495;&#21644;&#36328;&#32452;&#21512;&#24335;&#30340;&#25991;&#26412;&#21040;SQL&#35821;&#20041;&#35299;&#26512;&#30340;&#27867;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#38598;&#20013;&#25512;&#29702;&#20986;&#23569;&#37327;&#26679;&#26412;&#65292;&#20197;&#21512;&#25104;&#27599;&#20010;&#33258;&#28982;&#35821;&#35328;&#65288;NL&#65289;&#27979;&#35797;&#26597;&#35810;&#30340;&#36816;&#34892;&#26102;&#25552;&#31034;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#31163;&#32447;&#25277;&#26679;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#33719;&#21462;&#23569;&#37327;&#26679;&#26412;&#65292;&#23436;&#20840;&#35206;&#30422;SQL&#23376;&#21477;&#12289;&#36816;&#31639;&#31526;&#21644;&#20989;&#25968;&#65292;&#24182;&#22312;&#20801;&#35768;&#30340;&#20196;&#29260;&#38271;&#24230;&#33539;&#22260;&#20869;&#23454;&#29616;&#26368;&#22823;&#39046;&#22495;&#35206;&#30422;&#12290;&#36825;&#26679;&#21487;&#20197;&#21512;&#25104;&#19968;&#20010;&#22266;&#23450;&#30340;&#36890;&#29992;&#25552;&#31034;&#65288;GP&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;NL&#27979;&#35797;&#26597;&#35810;&#20043;&#38388;&#20849;&#29992;&#30340;&#22810;&#26679;&#21270;&#26679;&#26412;&#38598;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#27979;&#35797;&#26102;&#38388;&#26679;&#26412;&#26816;&#32034;&#12290;&#25105;&#20204;&#36824;&#23558;GP&#33258;&#36866;&#24212;&#21040;&#30446;&#26631;&#25968;&#25454;&#24211;&#39046;&#22495;&#65288;DA-GP&#65289;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#36328;&#39046;&#22495;&#27867;&#21270;&#65307;&#28982;&#21518;&#37319;&#29992;&#20998;&#35299;&#30340;&#26368;&#23569;&#21040;&#26368;&#22810;&#25552;&#31034;&#65288;LTMP-DA-GP&#65289;&#26469;&#22788;&#29702;&#36328;&#32452;&#21512;&#27867;&#21270;&#12290;LTMP-DA-GP&#30340;&#21512;&#25104;&#26159;&#31163;&#32447;&#20219;&#21153;&#65292;
&lt;/p&gt;
&lt;p&gt;
Cross-domain and cross-compositional generalization of Text-to-SQL semantic parsing is a challenging task. Existing Large Language Model (LLM) based solutions rely on inference-time retrieval of few-shot exemplars from the training set to synthesize a run-time prompt for each Natural Language (NL) test query. In contrast, we devise an algorithm which performs offline sampling of a minimal set-of few-shots from the training data, with complete coverage of SQL clauses, operators and functions, and maximal domain coverage within the allowed token length. This allows for synthesis of a fixed Generic Prompt (GP), with a diverse set-of exemplars common across NL test queries, avoiding expensive test time exemplar retrieval. We further auto-adapt the GP to the target database domain (DA-GP), to better handle cross-domain generalization; followed by a decomposed Least-To-Most-Prompting (LTMP-DA-GP) to handle cross-compositional generalization. The synthesis of LTMP-DA-GP is an offline task, to
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;</title><link>http://arxiv.org/abs/2308.02013</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02013
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#21512;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#38754;&#21521;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#32852;&#37030;&#34920;&#31034;&#23398;&#20064;&#65292;&#23558;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#38544;&#31169;&#25968;&#25454;&#29992;&#20110;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#27169;&#24335;&#65292;&#20801;&#35768;&#36793;&#32536;&#35774;&#22791;&#22312;&#19981;&#20849;&#20139;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#12290;&#20687;Alexa&#21644;Siri&#36825;&#26679;&#30340;&#36793;&#32536;&#35774;&#22791;&#26159;&#28508;&#22312;&#30340;&#38750;&#26631;&#35760;&#38899;&#39057;&#25968;&#25454;&#26469;&#28304;&#65292;&#21487;&#20197;&#29992;&#26469;&#23398;&#20064;&#20581;&#22766;&#30340;&#38899;&#39057;&#34920;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#21644;FL&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#36981;&#23432;&#25968;&#25454;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#65292;&#23398;&#20064;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#35821;&#38899;&#25968;&#25454;&#38598;Libri-Light&#20013;&#30340;&#35828;&#35805;&#32773;&#21644;&#31456;&#33410;&#20449;&#24687;&#65292;&#27169;&#25311;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#35828;&#35805;&#32773;&#38548;&#31163;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;FedSGD&#22312;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#26694;&#26550;&#19979;&#36827;&#34892;LSTM&#32534;&#30721;&#22120;&#30340;&#39044;&#35757;&#32451;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;FL&#20013;&#39044;&#35757;&#32451;&#30340;ASR&#32534;&#30721;&#22120;&#30340;&#24615;&#33021;&#19982;&#20013;&#24515;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#26377;12-15%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#32852;&#37030;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#65292;&#27861;&#35821;&#65292;&#24182;&#19988;&#30456;&#27604;&#27809;&#26377;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;20%&#65288;WER&#65289;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a privacy-preserving paradigm, allowing edge devices to learn collaboratively without sharing data. Edge devices like Alexa and Siri are prospective sources of unlabeled audio data that can be tapped to learn robust audio representations. In this work, we bring Self-supervised Learning (SSL) and FL together to learn representations for Automatic Speech Recognition respecting data privacy constraints. We use the speaker and chapter information in the unlabeled speech dataset, Libri-Light, to simulate non-IID speaker-siloed data distributions and pre-train an LSTM encoder with the Contrastive Predictive Coding framework with FedSGD. We show that the pre-trained ASR encoder in FL performs as well as a centrally pre-trained model and produces an improvement of 12-15% (WER) compared to no pre-training. We further adapt the federated pre-trained models to a new language, French, and show a 20% (WER) improvement over no pre-training.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.01681</link><description>&lt;p&gt;
NBIAS: &#29992;&#20110;&#25991;&#26412;&#20013;&#20559;&#35265;&#35782;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
NBIAS: A Natural Language Processing Framework for Bias Identification in Text. (arXiv:2308.01681v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;NBIAS&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26694;&#26550;&#65292;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#25910;&#38598;&#26469;&#33258;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#31561;&#39046;&#22495;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#26469;&#35782;&#21035;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#20013;&#23384;&#22312;&#20559;&#35265;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20351;&#29992;&#26102;&#20135;&#29983;&#20542;&#26012;&#30340;&#35299;&#37322;&#21644;&#32467;&#26524;&#12290;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#25345;&#32493;&#24378;&#21270;&#21051;&#26495;&#21360;&#35937;&#12289;&#27495;&#35270;&#25110;&#20854;&#20182;&#24418;&#24335;&#30340;&#19981;&#20844;&#24179;&#24453;&#36935;&#12290;&#22312;&#26377;&#20559;&#35265;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#31639;&#27861;&#26368;&#32456;&#20250;&#20570;&#20986;&#19981;&#24179;&#31561;&#24433;&#21709;&#26576;&#20010;&#32676;&#20307;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#21644;&#28040;&#38500;&#36825;&#20123;&#20559;&#35265;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#23545;&#25968;&#25454;&#30340;&#20844;&#24179;&#21644;&#36947;&#24503;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#32780;&#24378;&#22823;&#30340;&#26694;&#26550;"NBIAS"&#65292;&#23427;&#21253;&#25324;&#25968;&#25454;&#23618;&#12289;&#35821;&#26009;&#24211;&#26500;&#24314;&#12289;&#27169;&#22411;&#24320;&#21457;&#23618;&#21644;&#35780;&#20272;&#23618;&#12290;&#25968;&#25454;&#38598;&#30001;&#20174;&#21508;&#20010;&#39046;&#22495;&#25910;&#38598;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#26500;&#24314;&#65292;&#21253;&#25324;&#31038;&#20132;&#23186;&#20307;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#32844;&#20301;&#25307;&#32856;&#38376;&#25143;&#32593;&#31449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;Transformer&#30340;&#20196;&#29260;&#20998;&#31867;&#27169;&#22411;&#65292;&#36890;&#36807;&#19968;&#20010;&#21807;&#19968;&#30340;&#21629;&#21517;&#23454;&#20307;&#33021;&#22815;&#35782;&#21035;&#20986;&#20559;&#35265;&#35789;/&#30701;&#35821;&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#26041;&#27861;&#26469;&#35780;&#20272;&#25105;&#20204;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias in textual data can lead to skewed interpretations and outcomes when the data is used. These biases could perpetuate stereotypes, discrimination, or other forms of unfair treatment. An algorithm trained on biased data ends up making decisions that disproportionately impact a certain group of people. Therefore, it is crucial to detect and remove these biases to ensure the fair and ethical use of data. To this end, we develop a comprehensive and robust framework \textsc{Nbias} that consists of a data layer, corpus contruction, model development layer and an evaluation layer. The dataset is constructed by collecting diverse data from various fields, including social media, healthcare, and job hiring portals. As such, we applied a transformer-based token classification model that is able to identify bias words/ phrases through a unique named entity. In the assessment procedure, we incorporate a blend of quantitative and qualitative evaluations to gauge the effectiveness of our models.
&lt;/p&gt;</description></item><item><title>Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2307.15002</link><description>&lt;p&gt;
Gzip&#19982;KNN&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Gzip versus bag-of-words for text classification with KNN. (arXiv:2307.15002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15002
&lt;/p&gt;
&lt;p&gt;
Gzip&#19982;KNN&#30456;&#27604;&#36739;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#31616;&#21333;&#30340;&#35789;&#34955;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;KNN&#30340;&#25991;&#26412;&#20998;&#31867;&#20013;&#21387;&#32553;&#36317;&#31163;&#65288;gzip&#65289;&#30340;&#26377;&#25928;&#24615;&#24341;&#36215;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#21487;&#33021;&#19981;&#38656;&#35201;&#25991;&#26412;&#21387;&#32553;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#30340;&#8220;&#35789;&#34955;&#8221;&#21305;&#37197;&#21487;&#20197;&#36798;&#21040;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of compression distance in KNN-based text classification ('gzip') has recently garnered lots of attention. In this note, we show that similar or better effectiveness can be achieved with simpler means, and text compression may not be necessary. Indeed, we find that a simple 'bag-of-words' matching can achieve similar or better accuracy, and is more efficient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.11661</link><description>&lt;p&gt;
&#29992;GPT-4&#22686;&#24378;CLIP&#65306;&#21033;&#29992;&#35270;&#35273;&#25551;&#36848;&#20316;&#20026;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing CLIP with GPT-4: Harnessing Visual Descriptions as Prompts. (arXiv:2307.11661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22914;CLIP&#22312;&#19979;&#28216;&#25968;&#25454;&#38598;&#19978;&#25552;&#20379;&#20102;&#33391;&#22909;&#24615;&#33021;&#65292;&#20174;&#32780;&#38761;&#26032;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;VLMs&#36890;&#36807;&#35774;&#35745;&#19982;&#25968;&#25454;&#38598;&#30456;&#20851;&#30340;&#25552;&#31034;&#26469;0-shot&#36866;&#24212;&#19979;&#28216;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#25552;&#31034;&#24037;&#31243;&#21033;&#29992;&#20102;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#39564;&#35777;&#25968;&#25454;&#38598;&#12290;&#21516;&#26102;&#65292;&#20687;GPT-4&#36825;&#26679;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#21457;&#23637;&#24847;&#21619;&#30528;&#23427;&#20204;&#21487;&#20197;&#29992;&#20316;&#20808;&#36827;&#30340;&#20114;&#32852;&#32593;&#25628;&#32034;&#24037;&#20855;&#12290;&#23427;&#20204;&#36824;&#21487;&#20197;&#34987;&#25805;&#20316;&#20197;&#25552;&#20379;&#20219;&#20309;&#32467;&#26500;&#21270;&#30340;&#35270;&#35273;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;GPT-4&#29983;&#25104;&#20855;&#26377;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#36866;&#24212;CLIP&#21040;&#19979;&#28216;&#20219;&#21153;&#12290;&#19982;CLIP&#30340;&#40664;&#35748;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22312;&#19987;&#38376;&#32454;&#31890;&#24230;&#25968;&#25454;&#38598;&#65288;&#22914;EuroSAT&#65288;~7&#65285;&#65289;&#12289;DTD&#65288;~7&#65285;&#65289;&#12289;SUN397&#65288;~4.6&#65285;&#65289;&#21644;CUB&#65288;~3.3&#65285;&#65289;&#65289;&#19978;&#26174;&#31034;&#20986;&#20102;&#36739;&#22823;&#30340;0-shot&#36801;&#31227;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#23569;&#37327;&#26679;&#26412;&#36866;&#37197;&#22120;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#30340;s
&lt;/p&gt;
&lt;p&gt;
Contrastive pretrained large Vision-Language Models (VLMs) like CLIP have revolutionized visual representation learning by providing good performance on downstream datasets. VLMs are 0-shot adapted to a downstream dataset by designing prompts that are relevant to the dataset. Such prompt engineering makes use of domain expertise and a validation dataset. Meanwhile, recent developments in generative pretrained models like GPT-4 mean they can be used as advanced internet search tools. They can also be manipulated to provide visual information in any structure. In this work, we show that GPT-4 can be used to generate text that is visually descriptive and how this can be used to adapt CLIP to downstream tasks. We show considerable improvements in 0-shot transfer accuracy on specialized fine-grained datasets like EuroSAT (~7%), DTD (~7%), SUN397 (~4.6%), and CUB (~3.3%) when compared to CLIP's default prompt. We also design a simple few-shot adapter that learns to choose the best possible s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Masked Language Modeling (MLM)&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mask-tuning&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#39640;&#20102;PLMs&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.10457</link><description>&lt;p&gt;
&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Pre-trained Language Models' Generalization. (arXiv:2307.10457v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10457
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;Masked Language Modeling (MLM)&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Mask-tuning&#22312;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#25552;&#39640;&#20102;PLMs&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#36890;&#24120;&#21463;&#21040;&#20854;&#27867;&#21270;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#21363;&#24403;&#22312;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#19981;&#21516;&#30340;&#31034;&#20363;&#19978;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#20854;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#36825;&#31181;&#31034;&#20363;&#34987;&#31216;&#20026;&#8220;&#38750;&#20998;&#24067;/&#26410;&#35265;&#31034;&#20363;&#8221;&#12290;&#36825;&#19968;&#38480;&#21046;&#28304;&#20110;PLMs&#23545;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#20381;&#36182;&#65292;&#34394;&#20551;&#30456;&#20851;&#24615;&#23545;&#20110;&#24120;&#35265;&#31034;&#20363;&#31867;&#22411;&#25928;&#26524;&#33391;&#22909;&#65292;&#20294;&#23545;&#20110;&#19968;&#33324;&#31034;&#20363;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Mask-tuning&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#36974;&#34109;&#35821;&#35328;&#24314;&#27169;&#65288;MLM&#65289;&#35757;&#32451;&#30446;&#26631;&#25972;&#21512;&#21040;&#24494;&#35843;&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;PLMs&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20840;&#38754;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Mask-tuning&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#65292;&#24182;&#22686;&#24378;&#20102;PLMs&#23545;&#38750;&#20998;&#24067;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#20998;&#24067;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;Mask-tuning&#25552;&#39640;&#20102;PLMs&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#21487;&#37325;&#22797;&#20351;&#29992;&#24615;&#65292;&#20351;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#21152;&#23454;&#29992;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reusability of state-of-the-art Pre-trained Language Models (PLMs) is often limited by their generalization problem, where their performance drastically decreases when evaluated on examples that differ from the training dataset, known as Out-of-Distribution (OOD)/unseen examples. This limitation arises from PLMs' reliance on spurious correlations, which work well for frequent example types but not for general examples. To address this issue, we propose a training approach called Mask-tuning, which integrates Masked Language Modeling (MLM) training objectives into the fine-tuning process to enhance PLMs' generalization. Comprehensive experiments demonstrate that Mask-tuning surpasses current state-of-the-art techniques and enhances PLMs' generalization on OOD datasets while improving their performance on in-distribution datasets. The findings suggest that Mask-tuning improves the reusability of PLMs on unseen data, making them more practical and effective for real-world applications
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;&#65292;&#20998;&#26512;&#20102;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#31526;&#21495;&#21644;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#21508;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.09998</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;
&lt;/p&gt;
&lt;p&gt;
Generating Mathematical Derivations with Large Language Models. (arXiv:2307.09998v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09998
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#25968;&#23398;&#23548;&#20986;&#65292;&#20998;&#26512;&#20102;&#24494;&#35843;&#27169;&#22411;&#23545;&#26410;&#35265;&#31526;&#21495;&#21644;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#21508;&#20010;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#19987;&#19994;&#39046;&#22495;&#20013;&#29983;&#25104;&#25968;&#23398;&#32467;&#26524;&#30340;&#23548;&#20986;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21487;&#20197;&#24110;&#21161;&#35782;&#21035;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#26377;&#21487;&#33021;&#25903;&#25345;&#25968;&#23398;&#21457;&#29616;&#12290;&#26412;&#25991;&#21033;&#29992;&#31526;&#21495;&#24341;&#25806;&#22312;&#22823;&#35268;&#27169;&#19978;&#29983;&#25104;&#26041;&#31243;&#30340;&#23548;&#20986;&#65292;&#24182;&#30740;&#31350;&#20102;LLM&#22312;&#20174;&#21069;&#25552;&#20013;&#23548;&#20986;&#30446;&#26631;&#26041;&#31243;&#26102;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26469;&#23545;GPT&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23545;&#19968;&#31995;&#21015;T5&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#20197;&#27604;&#36739;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#19987;&#38376;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#32463;&#36807;&#24494;&#35843;&#30340;FLAN-T5-large&#65288;MathT5&#65289;&#22312;&#25152;&#26377;&#38745;&#24577;&#21644;&#36229;&#20986;&#20998;&#24067;&#30340;&#27979;&#35797;&#38598;&#19978;&#30340;&#32477;&#23545;&#24615;&#33021;&#20248;&#20110;GPT&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#24494;&#35843;&#27169;&#22411;&#23545;&#28041;&#21450;&#26410;&#35265;&#31526;&#21495;&#30340;&#25200;&#21160;&#65288;&#20197;&#21450;&#22312;&#36739;&#23567;&#31243;&#24230;&#19978;&#30340;&#26041;&#31243;&#32467;&#26500;&#26356;&#25913;&#65289;&#26356;&#20026;&#25935;&#24863;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;1.7K&#20010;&#26041;&#31243;&#21644;200&#22810;&#20010;&#23548;&#20986;&#20197;&#20984;&#26174;&#20986;LLM&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The derivation of mathematical results in specialised fields using Large Language Models (LLMs) is an emerging research direction that can help identify models' limitations, and potentially support mathematical discovery. In this paper, we leverage a symbolic engine to generate derivations of equations at scale, and investigate the capabilities of LLMs when deriving goal equations from premises. Specifically, we employ in-context learning for GPT and fine-tune a range of T5 models to compare the robustness and generalisation of pre-training strategies to specialised models. Empirical results show that fine-tuned FLAN-T5-large (MathT5) outperforms GPT models on all static and out-of-distribution test sets in terms of absolute performance. However, an in-depth analysis reveals that the fine-tuned models are more sensitive to perturbations involving unseen symbols and (to a lesser extent) changes to equation structure. In addition, we analyse 1.7K equations and over 200 derivations to hig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.06713</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26080;&#30417;&#30563;&#26657;&#20934;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#30340;&#20808;&#39564;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Calibration through Prior Adaptation for Text Classification using Large Language Models. (arXiv:2307.06713v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06713
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#26080;&#30417;&#30563;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26377;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#27491;&#22312;&#20351;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30740;&#31350;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#22823;&#37327;&#26080;&#30417;&#30563;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#24494;&#35843;&#12289;&#26657;&#20934;&#25110;&#19978;&#19979;&#25991;&#23398;&#20064;&#31561;&#26041;&#27861;&#36827;&#34892;&#36866;&#24212;&#20197;&#25191;&#34892;&#19979;&#28216;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20808;&#39564;&#31867;&#21035;&#20998;&#24067;&#65292;&#23454;&#29616;&#22312;&#27809;&#26377;&#26631;&#35760;&#26679;&#26412;&#21644;&#20165;&#23569;&#37327;&#39046;&#22495;&#20869;&#26679;&#26412;&#26597;&#35810;&#30340;&#24773;&#20917;&#19979;&#25191;&#34892;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#23558;LLM&#35270;&#20026;&#40657;&#30418;&#65292;&#22312;&#27169;&#22411;&#23631;&#38556;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#38454;&#27573;&#65292;&#29992;&#20110;&#26657;&#20934;&#27169;&#22411;&#21518;&#39564;&#20197;&#23436;&#25104;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#37327;&#30340;&#25552;&#31034;&#35757;&#32451;&#26679;&#26412;&#21644;&#26080;&#36866;&#24212;&#25968;&#25454;&#19979;&#30340;&#26657;&#20934;&#26041;&#27861;&#20013;&#20248;&#20110;&#26410;&#36866;&#24212;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A wide variety of natural language tasks are currently being addressed with large-scale language models (LLMs). These models are usually trained with a very large amount of unsupervised text data and adapted to perform a downstream natural language task using methods like fine-tuning, calibration or in-context learning. In this work, we propose an approach to adapt the prior class distribution to perform text classification tasks without the need for labelled samples and only few in-domain sample queries. The proposed approach treats the LLM as a black box, adding a stage where the model posteriors are calibrated to the task. Results show that these methods outperform the un-adapted model for different number of training shots in the prompt and a previous approach were calibration is performed without using any adaptation data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09841</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30495;&#30340;&#26159;&#33391;&#22909;&#30340;&#36923;&#36753;&#25512;&#29702;&#32773;&#21527;&#65311;&#22522;&#20110;&#28436;&#32462;&#12289;&#24402;&#32435;&#21644;&#38463;&#24067;&#36798;&#26031;&#35266;&#28857;&#30340;&#20840;&#38754;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are Large Language Models Really Good Logical Reasoners? A Comprehensive Evaluation From Deductive, Inductive and Abductive Views. (arXiv:2306.09841v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#65292;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#25968;&#25454;&#38598;&#65292;&#32771;&#34385;&#20102;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#25512;&#29702;&#24418;&#24335;&#65292;&#24182;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#36827;&#34892;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#35780;&#20272;&#12290;&#25552;&#20986;&#31934;&#32454;&#32423;&#21035;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23545;LLMs&#30340;&#20855;&#20307;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#35780;&#20272;&#65292;&#22914;&#22810;&#35821;&#35328;&#25512;&#29702;&#21644;&#25968;&#23398;&#25512;&#29702;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#20851;&#38190;&#25512;&#29702;&#35270;&#35282;&#20043;&#19968;&#65292;&#36923;&#36753;&#25512;&#29702;&#33021;&#21147;&#36824;&#27809;&#26377;&#24471;&#21040;&#24443;&#24213;&#35780;&#20272;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#24182;&#25552;&#20379;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;&#39318;&#20808;&#65292;&#20026;&#20102;&#36827;&#34892;&#31995;&#32479;&#21270;&#35780;&#20272;&#65292;&#26412;&#25991;&#36873;&#25321;&#20102;15&#20010;&#20856;&#22411;&#30340;&#36923;&#36753;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#25104;&#28436;&#32462;&#12289;&#24402;&#32435;&#12289;&#38463;&#24067;&#36798;&#26031;&#21644;&#28151;&#21512;&#24418;&#24335;&#30340;&#25512;&#29702;&#35774;&#32622;&#12290;&#32771;&#34385;&#35780;&#20272;&#30340;&#20840;&#38754;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;LLMs&#65288;text-davinci-003&#65292;ChatGPT&#21644;BARD&#65289;&#65292;&#24182;&#22312;&#38646;&#26679;&#26412;&#12289;&#19968;&#27425;&#21644;&#19977;&#27425;&#30340;&#35774;&#32622;&#19979;&#23545;&#25152;&#26377;&#36873;&#25321;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#20854;&#27425;&#65292;&#19982;&#20197;&#24448;&#20165;&#20381;&#36182;&#31616;&#21333;&#25351;&#26631;&#65288;&#22914;&#20934;&#30830;&#24615;&#65289;&#30340;&#35780;&#20272;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#30446;&#26631;&#25512;&#29702;&#35282;&#24230;&#36827;&#34892;&#30340;&#31934;&#32454;&#32423;&#21035;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved great success in various natural language tasks. It has aroused much interest in evaluating the specific reasoning capability of LLMs, such as multilingual reasoning and mathematical reasoning. However, as one of the key reasoning perspectives, logical reasoning capability has not yet been thoroughly evaluated. In this work, we aim to bridge those gaps and provide comprehensive evaluations. Firstly, to offer systematic evaluations, this paper selects fifteen typical logical reasoning datasets and organizes them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include three representative LLMs (i.e., text-davinci-003, ChatGPT and BARD) and evaluate them on all selected datasets under zero-shot, one-shot and three-shot settings. Secondly, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations from objective 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.07848</link><description>&lt;p&gt;
GEmo-CLAP: &#38754;&#21521;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#30340;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GEmo-CLAP: Gender-Attribute-Enhanced Contrastive Language-Audio Pretraining for Speech Emotion Recognition. (arXiv:2306.07848v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;GEmo-CLAP&#27169;&#22411;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65292;&#32467;&#21512;&#20102;&#24615;&#21035;&#23646;&#24615;&#20449;&#24687;&#65292;&#30456;&#27604;&#20110;&#20854;&#20182;&#20808;&#36827;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#22312;IEMOCAP&#19978;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#38899;-&#35821;&#35328;&#39044;&#35757;&#32451;&#65288;CLAP&#65289;&#26368;&#36817;&#22312;&#19981;&#21516;&#39046;&#22495;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GEmo-CLAP&#30340;&#39640;&#25928;&#24615;&#21035;&#23646;&#24615;&#22686;&#24378;CLAP&#27169;&#22411;&#65292;&#29992;&#20110;&#35821;&#38899;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#21508;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#26500;&#24314;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#24773;&#24863;CLAP&#27169;&#22411;&#65288;&#31216;&#20026;Emo-CLAP&#65289;&#65292;&#29992;&#20110;SER&#12290;&#28982;&#21518;&#65292;&#32771;&#34385;&#21040;&#22312;&#35821;&#38899;&#24773;&#24863;&#24314;&#27169;&#20013;&#24615;&#21035;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#65292;&#26469;&#25972;&#21512;&#35821;&#38899;&#20449;&#21495;&#30340;&#24773;&#24863;&#21644;&#24615;&#21035;&#20449;&#24687;&#65292;&#24418;&#25104;&#26356;&#21512;&#29702;&#30340;&#30446;&#26631;&#12290;&#22312;IEMOCAP&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#20004;&#31181;GEmo-CLAP&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#22522;&#32447;Emo-CLAP&#27169;&#22411;&#65288;&#20351;&#29992;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65289;&#65292;&#21516;&#26102;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#23454;&#29616;&#20102;&#26356;&#20248;&#36234;&#30340;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Audio Pretraining (CLAP) has recently exhibited impressive success in diverse fields. In this paper, we propose GEmo-CLAP, a kind of efficient gender-attribute-enhanced CLAP model for speech emotion recognition (SER). Specifically, we first build an effective emotion CLAP model termed Emo-CLAP for SER, utilizing various self-supervised learning based pre-trained models. Then, considering the importance of the gender attribute in speech emotion modeling, two GEmo-CLAP approaches are further proposed to integrate the emotion and gender information of speech signals, forming more reasonable objectives. Extensive experiments conducted on the IEMOCAP corpus demonstrate that our proposed two GEmo-CLAP approaches consistently outperform the baseline Emo-CLAP with different pre-trained models, while also achieving superior recognition performance compared with other state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#24037;&#20855;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;33K&#20010;&#26679;&#26412;&#21644;22.5M&#20010;&#26631;&#35760;&#30340;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#29702;&#35299;&#36825;&#31867;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#35821;&#35328;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.02864</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20844;&#20849;&#20107;&#21153;&#39046;&#22495;&#36827;&#34892;&#20027;&#39064;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Topic Classification in the Domain of Public Affairs. (arXiv:2306.02864v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02864
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#20998;&#31867;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#27491;&#21017;&#34920;&#36798;&#24335;&#24037;&#20855;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;33K&#20010;&#26679;&#26412;&#21644;22.5M&#20010;&#26631;&#35760;&#30340;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#25968;&#25454;&#24211;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#21644;&#29702;&#35299;&#36825;&#31867;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#35821;&#35328;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#23545;&#20844;&#27665;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#20419;&#36827;&#36879;&#26126;&#24230;&#12289;&#38382;&#36131;&#21046;&#21644;&#20915;&#31574;&#12290;&#23427;&#20351;&#20844;&#27665;&#33021;&#22815;&#20102;&#35299;&#25919;&#24220;&#25919;&#31574;&#65292;&#21442;&#19982;&#20844;&#20849;&#35805;&#35821;&#65292;&#24182;&#36861;&#31350;&#20195;&#34920;&#30340;&#36131;&#20219;&#12290;&#23545;&#20110;&#20381;&#38752;&#26576;&#20123;&#35268;&#23450;&#36816;&#33829;&#30340;&#20844;&#21496;&#26469;&#35828;&#65292;&#36825;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#26377;&#26102;&#29978;&#33267;&#20851;&#20046;&#29983;&#27515;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26377;&#28508;&#21147;&#36890;&#36807;&#26377;&#25928;&#22788;&#29702;&#21644;&#29702;&#35299;&#36825;&#31867;&#25991;&#20214;&#20013;&#20351;&#29992;&#30340;&#22797;&#26434;&#35821;&#35328;&#65292;&#22823;&#22823;&#22686;&#24378;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#30340;&#20998;&#26512;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#22312;&#20998;&#31867;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#20013;&#30340;&#24615;&#33021;&#12290;&#20316;&#20026;&#19968;&#39033;&#33258;&#28982;&#30340;&#22810;&#26631;&#31614;&#20219;&#21153;&#65292;&#36825;&#20123;&#25991;&#20214;&#30340;&#20998;&#31867;&#20855;&#26377;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22522;&#20110;&#27491;&#21017;&#34920;&#36798;&#24335;&#30340;&#24037;&#20855;&#26469;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;33K&#20010;&#26679;&#26412;&#21644;22.5M&#20010;&#26631;&#35760;&#30340;&#20844;&#20849;&#20107;&#21153;&#25991;&#20214;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#20102;4&#20010;&#19981;&#21516;&#30340;&#35199;&#29677;&#29273;&#35821;LLM&#22312;&#26368;&#22810;30&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#25991;&#20214;&#20998;&#31867;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The analysis of public affairs documents is crucial for citizens as it promotes transparency, accountability, and informed decision-making. It allows citizens to understand government policies, participate in public discourse, and hold representatives accountable. This is crucial, and sometimes a matter of life or death, for companies whose operation depend on certain regulations. Large Language Models (LLMs) have the potential to greatly enhance the analysis of public affairs documents by effectively processing and understanding the complex language used in such documents. In this work, we analyze the performance of LLMs in classifying public affairs documents. As a natural multi-label task, the classification of these documents presents important challenges. In this work, we use a regex-powered tool to collect a database of public affairs documents with more than 33K samples and 22.5M tokens. Our experiments assess the performance of 4 different Spanish LLMs to classify up to 30 diff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10652</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#35821;&#38899;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Speech Separation based on Contrastive Learning and Deep Modularization. (arXiv:2305.10652v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#21644;&#28145;&#24230;&#27169;&#22359;&#21270;&#30340;&#23436;&#20840;&#26080;&#30417;&#30563;&#35821;&#38899;&#20998;&#31163;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#23384;&#22312;&#30340;&#25490;&#21015;&#38382;&#39064;&#12289;&#35828;&#35805;&#20154;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#21644;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#20381;&#36182;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#35821;&#38899;&#20998;&#31163;&#30340;&#26368;&#20808;&#36827;&#24037;&#20855;&#20381;&#36182;&#20110;&#26377;&#30417;&#30563;&#23398;&#20064;&#12290;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#24517;&#39035;&#22788;&#29702;&#25490;&#21015;&#38382;&#39064;&#65292;&#23427;&#20204;&#21463;&#21040;&#35757;&#32451;&#21644;&#25512;&#26029;&#20013;&#20351;&#29992;&#30340;&#35828;&#35805;&#32773;&#25968;&#37327;&#19981;&#21305;&#37197;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#23384;&#22312;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#23436;&#20840;&#26080;&#30417;&#30563;&#30340;&#35821;&#38899;&#20998;&#31163;&#25216;&#26415;&#26377;&#25928;&#22320;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#24314;&#31435;&#24103;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#30340;&#28145;&#24230;&#27169;&#22359;&#21270;&#20219;&#21153;&#20013;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35821;&#38899;&#20998;&#31163;&#20013;&#65292;&#35828;&#35805;&#20154;&#30340;&#19981;&#21516;&#24103;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#32473;&#23450;&#37027;&#20010;&#35828;&#35805;&#20154;&#30340;&#38544;&#21547;&#26631;&#20934;&#24103;&#30340;&#22686;&#24378;&#29256;&#12290;&#35828;&#35805;&#20154;&#30340;&#24103;&#21253;&#21547;&#36275;&#22815;&#30340;&#38901;&#24459;&#20449;&#24687;&#37325;&#21472;&#65292;&#36825;&#26159;&#35821;&#38899;&#20998;&#31163;&#30340;&#20851;&#38190;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#23398;&#20064;&#32553;&#23567;&#24103;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current monaural state of the art tools for speech separation relies on supervised learning. This means that they must deal with permutation problem, they are impacted by the mismatch on the number of speakers used in training and inference. Moreover, their performance heavily relies on the presence of high-quality labelled data. These problems can be effectively addressed by employing a fully unsupervised technique for speech separation. In this paper, we use contrastive learning to establish the representations of frames then use the learned representations in the downstream deep modularization task. Concretely, we demonstrate experimentally that in speech separation, different frames of a speaker can be viewed as augmentations of a given hidden standard frame of that speaker. The frames of a speaker contain enough prosodic information overlap which is key in speech separation. Based on this, we implement a self-supervised learning to learn to minimize the distance between frames
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65306;&#33258;&#21160;&#35782;&#21035;&#21465;&#36848;&#20013;&#30340;&#26032;&#20107;&#20214;&#65292;&#20197;&#35782;&#21035;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#20182;&#20204;&#23558;&#20107;&#20214;&#23450;&#20041;&#20026;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#30340;&#19977;&#20803;&#32452;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#26032;&#20107;&#20214;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20854;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#26469;&#25512;&#23548;&#12290;</title><link>http://arxiv.org/abs/2302.07748</link><description>&lt;p&gt;
&#20160;&#20040;&#26159;&#26032;&#20107;&#20214;&#65311;&#22312;&#21465;&#20107;&#20013;&#35782;&#21035;&#26032;&#20107;&#20214;&#30340;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whats New? Identifying the Unfolding of New Events in Narratives. (arXiv:2302.07748v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65306;&#33258;&#21160;&#35782;&#21035;&#21465;&#36848;&#20013;&#30340;&#26032;&#20107;&#20214;&#65292;&#20197;&#35782;&#21035;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;&#20182;&#20204;&#23558;&#20107;&#20214;&#23450;&#20041;&#20026;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#30340;&#19977;&#20803;&#32452;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#26032;&#20107;&#20214;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#20854;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#26469;&#25512;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21465;&#20107;&#21253;&#21547;&#20102;&#19978;&#19979;&#25991;&#21644;&#26102;&#38388;&#30340;&#20016;&#23500;&#20107;&#20214;&#36164;&#28304;&#12290;&#23545;&#36825;&#20123;&#20107;&#20214;&#30340;&#33258;&#21160;&#29702;&#35299;&#25552;&#20379;&#20102;&#25688;&#35201;&#29702;&#35299;&#65292;&#20197;&#20379;&#36827;&#19968;&#27493;&#30340;&#35745;&#31639;(&#22914;&#25512;&#29702;)&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20107;&#20214;&#30340;&#20449;&#24687;&#29366;&#24577;(IS)&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;:&#33258;&#21160;&#35782;&#21035;&#21465;&#36848;&#20013;&#30340;&#26032;&#20107;&#20214;&#12290;&#25105;&#20204;&#23558;&#20107;&#20214;&#23450;&#20041;&#20026;&#20027;&#35821;&#12289;&#35859;&#35821;&#21644;&#23486;&#35821;&#30340;&#19977;&#20803;&#32452;&#12290;&#35813;&#20107;&#20214;&#30456;&#23545;&#20110;&#35805;&#35821;&#19978;&#19979;&#25991;&#34987;&#24402;&#31867;&#20026;&#26032;&#20107;&#20214;&#65292;&#24182;&#21462;&#20915;&#20110;&#26159;&#21542;&#21487;&#20197;&#36890;&#36807;&#24120;&#35782;&#25512;&#29702;&#26469;&#25512;&#23548;&#12290;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#26631;&#27880;&#32773;&#22312;&#20844;&#24320;&#30340;&#21465;&#36848;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#21477;&#23376;&#32423;&#21035;&#30340;&#26032;&#20107;&#20214;&#26631;&#27880;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26631;&#27880;&#21327;&#35758;&#65292;&#24182;&#30740;&#31350;&#20102;&#27880;&#37322;&#30340;&#36136;&#37327;&#21644;&#20219;&#21153;&#30340;&#38590;&#24230;&#12290;&#25105;&#20204;&#20844;&#24320;&#20102;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#12289;&#26631;&#27880;&#26448;&#26009;&#21644;&#29992;&#20110;&#21465;&#36848;&#29702;&#35299;&#20013;&#26032;&#20107;&#20214;&#25552;&#21462;&#30340;&#26426;&#22120;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Narratives include a rich source of events unfolding over time and context. Automatic understanding of these events provides a summarised comprehension of the narrative for further computation (such as reasoning). In this paper, we study the Information Status (IS) of the events and propose a novel challenging task: the automatic identification of \textit{new} events in a narrative. We define an event as a triplet of subject, predicate, and object. The event is categorized as new with respect to the discourse context and whether it can be inferred through commonsense reasoning. We annotated a publicly available corpus of narratives with the new events at sentence level using human annotators. We present the annotation protocol and study the quality of the annotation and the difficulty of the task. We publish the annotated dataset, annotation materials, and machine learning baseline models for the task of new event extraction for narrative understanding.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#23588;&#20854;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#20256;&#32479;&#30340;NER&#26041;&#27861;&#21644;&#36817;&#24180;&#26469;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#38463;&#25289;&#20271;NER&#30340;&#32972;&#26223;&#21644;&#29616;&#26377;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2302.03512</link><description>&lt;p&gt;
&#23545;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35843;&#26597;&#65306;&#36807;&#21435;&#12289;&#36817;&#26399;&#36827;&#23637;&#21644;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
A Survey on Arabic Named Entity Recognition: Past, Recent Advances, and Future Trends. (arXiv:2302.03512v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#38463;&#25289;&#20271;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#23588;&#20854;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#35813;&#35843;&#26597;&#24635;&#32467;&#20102;&#20256;&#32479;&#30340;NER&#26041;&#27861;&#21644;&#36817;&#24180;&#26469;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#38463;&#25289;&#20271;NER&#30340;&#32972;&#26223;&#21644;&#29616;&#26377;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#38463;&#25289;&#20271;&#25991;&#26412;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#65292;&#20174;&#36825;&#20123;&#38463;&#25289;&#20271;&#25991;&#26412;&#20013;&#25552;&#21462;&#37325;&#35201;&#20449;&#24687;&#21464;&#24471;&#23588;&#20026;&#26377;&#29992;&#12290;&#20316;&#20026;&#19968;&#39033;&#22522;&#26412;&#25216;&#26415;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#22312;&#20449;&#24687;&#25552;&#21462;&#25216;&#26415;&#20013;&#20805;&#24403;&#26680;&#24515;&#32452;&#20214;&#65292;&#21516;&#26102;&#22312;&#35768;&#22810;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22914;&#38382;&#31572;&#21644;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#38463;&#25289;&#20271;NER&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#38463;&#25289;&#20271;NER&#30340;&#32972;&#26223;&#65292;&#21253;&#25324;&#38463;&#25289;&#20271;&#35821;&#29305;&#28857;&#21644;&#29616;&#26377;&#30340;&#38463;&#25289;&#20271;NER&#36164;&#28304;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#22238;&#39038;&#20102;&#38463;&#25289;&#20271;NER&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#20256;&#32479;&#30340;&#38463;&#25289;&#20271;NER&#31995;&#32479;&#20391;&#37325;&#20110;&#29305;&#24449;&#24037;&#31243;&#21644;&#35774;&#35745;&#29305;&#23450;&#39046;&#22495;&#30340;&#35268;&#21017;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#36830;&#32493;&#21521;&#37327;&#34920;&#31034;&#25991;&#26412;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
As more and more Arabic texts emerged on the Internet, extracting important information from these Arabic texts is especially useful. As a fundamental technology, Named entity recognition (NER) serves as the core component in information extraction technology, while also playing a critical role in many other Natural Language Processing (NLP) systems, such as question answering and knowledge graph building. In this paper, we provide a comprehensive review of the development of Arabic NER, especially the recent advances in deep learning and pre-trained language model. Specifically, we first introduce the background of Arabic NER, including the characteristics of Arabic and existing resources for Arabic NER. Then, we systematically review the development of Arabic NER methods. Traditional Arabic NER systems focus on feature engineering and designing domain-specific rules. In recent years, deep learning methods achieve significant progress by representing texts via continuous vector repres
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#38656;&#35201;&#20445;&#35777;&#20854;&#21487;&#25511;&#24615;&#12290;</title><link>http://arxiv.org/abs/2201.05337</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Controllable Text Generation using Transformer-based Pre-trained Language Models. (arXiv:2201.05337v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.05337
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#22810;&#26679;&#21270;&#12289;&#27969;&#30021;&#30340;&#25991;&#26412;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#38656;&#35201;&#20445;&#35777;&#20854;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#39046;&#22495;&#20013;&#26032;&#20852;&#30340;&#26041;&#21521;&#65292;&#34987;&#35748;&#20026;&#23545;&#20110;&#24320;&#21457;&#26356;&#33258;&#28982;&#12289;&#26356;&#31526;&#21512;&#29305;&#23450;&#24212;&#29992;&#22330;&#26223;&#30340;&#20808;&#36827;&#25991;&#26412;&#29983;&#25104;&#25216;&#26415;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#23588;&#20854;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20110;Transformer&#30340;PLMs&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26032;&#33539;&#24335;&#65292;&#21487;&#20197;&#29983;&#25104;&#26356;&#22810;&#26679;&#21270;&#12289;&#26356;&#27969;&#30021;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#36739;&#20302;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#21487;&#25511;&#24615;&#38656;&#35201;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#22522;&#20110;Transformer&#30340;PLMs&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#24050;&#25104;&#20026;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26032;&#30740;&#31350;&#28909;&#28857;&#12290;&#26368;&#36817;3-4&#24180;&#20986;&#29616;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#38024;&#23545;&#21487;&#33021;&#38656;&#35201;&#19981;&#21516;&#31867;&#22411;&#25511;&#21046;&#32422;&#26463;&#30340;&#19981;&#21516;CTG&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#24403;&#21069;&#22522;&#20110;Transformer&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25511;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that are more natural and better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the lower level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks which may require different types of controlled constraints. In this paper, we present a systematic critical review on the com
&lt;/p&gt;</description></item></channel></rss>