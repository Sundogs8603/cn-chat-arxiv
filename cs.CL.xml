<rss version="2.0"><channel><title>Chat Arxiv cs.CL</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.CL</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570; PGIM &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992; ChatGPT &#20316;&#20026;&#38544;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312; MNER &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12212</link><description>&lt;p&gt;
Prompt ChatGPT &#22312; MNER &#20013;&#30340;&#24212;&#29992;&#65306;&#22522;&#20110; ChatGPT &#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#30340;&#25913;&#36827;&#24335;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT. (arXiv:2305.12212v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21483;&#20570; PGIM &#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992; ChatGPT &#20316;&#20026;&#38544;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312; MNER &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#27169;&#24577;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;MNER&#65289;&#26088;&#22312;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#22270;&#20687;&#30340;&#32447;&#32034;&#26469;&#22686;&#24378;&#25991;&#26412;&#23454;&#20307;&#39044;&#27979;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26368;&#22823;&#21270;&#22270;&#20687;&#30456;&#20851;&#20449;&#24687;&#30340;&#21033;&#29992;&#25110;&#23558;&#22806;&#37096;&#30693;&#35782;&#20174;&#26174;&#24335;&#30693;&#35782;&#24211;&#65288;KBs&#65289;&#20013;&#24341;&#20837;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#20040;&#24573;&#35270;&#20102;&#21521;&#27169;&#22411;&#25552;&#20379;&#30456;&#20851;&#22806;&#37096;&#30693;&#35782;&#30340;&#24517;&#35201;&#24615;&#65292;&#35201;&#20040;&#24341;&#20837;&#30340;&#22806;&#37096;&#30693;&#35782;&#23384;&#22312;&#37325;&#22797;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#24565;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#31216;&#20026; Prompt ChatGPT In MNER (PGIM)&#12290;&#25105;&#20204;&#21033;&#29992; ChatGPT &#20316;&#20026;&#38544;&#24335;&#30693;&#35782;&#24341;&#25806;&#26469;&#33719;&#21462;&#36741;&#21161;&#31934;&#28860;&#30693;&#35782;&#65292;&#20174;&#32780;&#22686;&#24378;&#27169;&#22411;&#22312; MNER &#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Named Entity Recognition (MNER) on social media aims to enhance textual entity prediction by incorporating image-based clues. Existing research in this domain has primarily focused on maximizing the utilization of potentially relevant information in images or incorporating external knowledge from explicit knowledge bases (KBs). However, these methods either neglect the necessity of providing the model with relevant external knowledge, or the retrieved external knowledge suffers from high redundancy. To address these problems, we propose a conceptually simple two-stage framework called Prompt ChatGPT In MNER (PGIM) in this paper. We leverage ChatGPT as an implicit knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks. Specifically, we first utilize a Multimodal Similar Example Awareness module to select suitable examples from a small number of manually annotated samples. These examples are then integrated into a form
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#21098;&#26525;&#21644;&#20803;&#23398;&#20064;&#33258;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#38271;&#23614;&#26679;&#26412;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12209</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#23398;&#20064;&#30340;&#33258;&#33976;&#39311;&#32593;&#32476;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Self-Distillation with Meta Learning for Knowledge Graph Completion. (arXiv:2305.12209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#21098;&#26525;&#21644;&#20803;&#23398;&#20064;&#33258;&#33976;&#39311;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20013;&#38271;&#23614;&#26679;&#26412;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21160;&#24577;&#21098;&#26525;&#30340;&#20803;&#23398;&#20064;&#33258;&#33976;&#39311;&#26694;&#26550;(MetaSD)&#65292;&#26088;&#22312;&#23398;&#20064;&#21387;&#32553;&#30340;&#22270;&#23884;&#20837;&#24182;&#35299;&#20915;&#38271;&#23614;&#26679;&#26412;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#21098;&#26525;&#25216;&#26415;&#65292;&#20174;&#19968;&#20010;&#22823;&#28304;&#27169;&#22411;&#20013;&#33719;&#21462;&#19968;&#20010;&#36739;&#23567;&#30340;&#21098;&#26525;&#27169;&#22411;&#65292;&#20854;&#20013;&#21098;&#26525;&#25513;&#27169;&#21487;&#20197;&#22312;&#27169;&#22411;&#26435;&#37325;&#26356;&#26032;&#21518;&#27599;&#20010;&#26102;&#26399;&#33258;&#36866;&#24212;&#22320;&#26356;&#26032;&#65292;&#21098;&#26525;&#27169;&#22411;&#24212;&#27604;&#28304;&#27169;&#22411;&#26356;&#25935;&#24863;&#20110;&#38590;&#20110;&#35760;&#24518;&#30340;&#26679;&#26412;&#65288;&#20363;&#22914;&#65292;&#38271;&#23614;&#26679;&#26412;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#27493;&#20803;&#33258;&#33976;&#39311;&#26041;&#27861;&#65292;&#20197;&#23558;&#26469;&#33258;&#28304;&#27169;&#22411;&#30340;&#20840;&#38754;&#30693;&#35782;&#33976;&#39311;&#21040;&#21098;&#26525;&#27169;&#22411;&#20013;&#65292;&#20854;&#20013;&#20004;&#20010;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#20197;&#21160;&#24577;&#26041;&#24335;&#20849;&#21516;&#36827;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19982;&#28304;&#27169;&#22411;&#21516;&#26102;&#22312;&#19968;&#20010;&#36845;&#20195;&#20013;&#35757;&#32451;&#30340;&#21098;&#26525;&#27169;&#22411;&#30340;&#24615;&#33021;&#26469;&#36890;&#36807;&#20803;&#23398;&#20064;&#25552;&#39640;&#28304;&#27169;&#22411;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#65292;&#20197;&#36827;&#34892;&#19979;&#19968;&#27425;&#36845;&#20195;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a selfdistillation framework with meta learning(MetaSD) for knowledge graph completion with dynamic pruning, which aims to learn compressed graph embeddings and tackle the longtail samples. Specifically, we first propose a dynamic pruning technique to obtain a small pruned model from a large source model, where the pruning mask of the pruned model could be updated adaptively per epoch after the model weights are updated. The pruned model is supposed to be more sensitive to difficult to memorize samples(e.g., longtail samples) than the source model. Then, we propose a onestep meta selfdistillation method for distilling comprehensive knowledge from the source model to the pruned model, where the two models coevolve in a dynamic manner during training. In particular, we exploit the performance of the pruned model, which is trained alongside the source model in one iteration, to improve the source models knowledge transfer ability for the next iteration via meta l
&lt;/p&gt;</description></item><item><title>GPT-4&#22312;&#19968;&#20010;&#26410;&#21457;&#34920;&#30340;&#27491;&#24335;&#31995;&#32479;&#19978;&#25104;&#21151;&#23436;&#25104;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#21457;&#26126;&#20102;&#26377;&#29992;&#30340;&#26032;&#35821;&#27861;&#21644;&#35821;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12196</link><description>&lt;p&gt;
&#23558;GPT-4&#24212;&#29992;&#20110;&#26410;&#21457;&#34920;&#30340;&#24418;&#24335;&#35821;&#35328;&#30340;&#23454;&#39564;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Experimental results from applying GPT-4 to an unpublished formal language. (arXiv:2305.12196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12196
&lt;/p&gt;
&lt;p&gt;
GPT-4&#22312;&#19968;&#20010;&#26410;&#21457;&#34920;&#30340;&#27491;&#24335;&#31995;&#32479;&#19978;&#25104;&#21151;&#23436;&#25104;&#20102;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#21457;&#26126;&#20102;&#26377;&#29992;&#30340;&#26032;&#35821;&#27861;&#21644;&#35821;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#20110;&#23436;&#25104;&#20256;&#32479;&#25163;&#21160;&#25110;&#20351;&#29992;&#23450;&#29702;&#35777;&#26126;&#22120;&#23436;&#25104;&#30340;&#25968;&#23398;&#20219;&#21153;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#20808;&#36827;&#30340;&#31995;&#32479;GPT-4&#20351;&#29992;&#31616;&#27905;&#30340;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#65292;&#38024;&#23545;&#20808;&#21069;&#26410;&#21457;&#34920;&#30340;&#27491;&#24335;&#31995;&#32479;&#65292;&#23436;&#25104;&#20102;&#19968;&#20123;&#20219;&#21153;&#65292;&#21253;&#25324;&#38472;&#36848;&#20989;&#25968;&#21644;&#31867;&#22411;&#23450;&#20041;&#12289;&#35777;&#26126;&#31616;&#21333;&#23450;&#29702;&#21644;&#39564;&#35777;&#29992;&#25143;&#25552;&#20379;&#30340;&#35777;&#26126;&#12290;&#35813;&#31995;&#32479;&#25104;&#21151;&#23436;&#25104;&#20102;&#25152;&#26377;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#21457;&#26126;&#20102;&#26377;&#29992;&#30340;&#26032;&#35821;&#27861;&#21644;&#35821;&#20041;&#65292;&#24182;&#23637;&#31034;&#20102;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#22240;&#27492;&#31572;&#26696;&#20284;&#20046;&#26159;&#65306;&#21487;&#20197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can large language models be used to complete mathematical tasks that are traditionally performed either manually or with the aid of theorem provers? To answer this question, a state-of-the-art system, GPT-4, was provided with a concise natural language specification for a previously unpublished formal system and asked to complete a number of tasks, from stating function and type definitions to proving simple theorems and verifying user-supplied proofs. The system completed all tasks successfully, showed extensive domain knowledge, invented helpful new syntax and semantics, and exhibited generalization and inference abilities. So the answer seems to be: yes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20114;&#20449;&#24687;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#26469;&#39044;&#27979;&#26356;&#21152;&#20445;&#30495;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2305.12191</link><description>&lt;p&gt;
&#22522;&#20110;&#28857;&#20114;&#20449;&#24687;&#24230;&#37327;&#21644;&#35299;&#30721;&#31574;&#30053;&#30340;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20445;&#30495;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Pointwise Mutual Information Based Metric and Decoding Strategy for Faithful Generation in Document Grounded Dialogs. (arXiv:2305.12191v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12191
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#20445;&#30495;&#24230;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28857;&#20114;&#20449;&#24687;&#30340;&#24230;&#37327;&#26041;&#27861;&#21644;&#35299;&#30721;&#31574;&#30053;&#26469;&#39044;&#27979;&#26356;&#21152;&#20445;&#30495;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25991;&#26723;&#23548;&#21521;&#23545;&#35805;&#29983;&#25104;&#20013;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#22238;&#31572;&#21487;&#33021;&#19982;&#24213;&#23618;&#25991;&#26723;&#19981;&#19968;&#33268;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#30340;&#22238;&#31572;&#26159;&#21542;&#19982;&#24213;&#23618;&#25991;&#26723;&#19968;&#33268;&#30340;&#33258;&#21160;&#21270;&#24230;&#37327;&#26041;&#27861;&#65292;&#24230;&#37327;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#25991;&#26723;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20284;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33258;&#21160;&#21270;&#24230;&#37327;&#26041;&#27861;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#27604;&#24046;&#24322;&#24456;&#22823;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#25552;&#39640;&#20445;&#30495;&#24230;&#30340;&#27979;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26465;&#20214;&#28857;&#20114;&#20449;&#24687;&#65288;PMI&#65289;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#35813;&#24230;&#37327;&#26041;&#27861;&#37327;&#21270;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#28304;&#25991;&#26723;&#20043;&#38388;&#30340;PMI&#65292;&#21463;&#23545;&#35805;&#26465;&#20214;&#30340;&#24433;&#21709;&#12290;PMI&#37327;&#21270;&#25991;&#26723;&#23545;&#29983;&#25104;&#30340;&#22238;&#31572;&#30340;&#24433;&#21709;&#31243;&#24230;&#65292;PMI&#36234;&#39640;&#21017;&#22238;&#31572;&#36234;&#20445;&#30495;&#12290;&#25105;&#20204;&#22522;&#20110;&#27492;&#24605;&#24819;&#26500;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#30721;&#25216;&#26415;&#65292;&#23558;PMI&#24182;&#20837;&#21040;&#29983;&#25104;&#27969;&#31243;&#20013;&#65292;&#20197;&#39044;&#27979;&#26356;&#21152;&#20445;&#30495;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
A major concern in using deep learning based generative models for document-grounded dialogs is the potential generation of responses that are not \textit{faithful} to the underlying document. Existing automated metrics used for evaluating the faithfulness of response with respect to the grounding document measure the degree of similarity between the generated response and the document's content. However, these automated metrics are far from being well aligned with human judgments. Therefore, to improve the measurement of faithfulness, we propose a new metric that utilizes (Conditional) Point-wise Mutual Information (PMI) between the generated response and the source document, conditioned on the dialogue. PMI quantifies the extent to which the document influences the generated response -- with a higher PMI indicating a more faithful response. We build upon this idea to create a new decoding technique that incorporates PMI into the response generation process to predict more faithful re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#23618;&#32423;&#30340;&#24341;&#29992;&#24314;&#35758;&#20219;&#21153;&#8212;&#8212;&#27573;&#33853;&#32423;&#24341;&#29992;&#24314;&#35758;&#65292;&#21363;&#20197;&#27573;&#33853;&#30340;&#20027;&#39064;&#21477;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#22312;&#27573;&#33853;&#20013;&#24341;&#29992;&#30340;&#24314;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#27492;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12190</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#21477;&#20316;&#20026;&#26597;&#35810;&#30340;&#27573;&#33853;&#32423;&#24341;&#29992;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Paragraph-level Citation Recommendation based on Topic Sentences as Queries. (arXiv:2305.12190v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20013;&#38388;&#23618;&#32423;&#30340;&#24341;&#29992;&#24314;&#35758;&#20219;&#21153;&#8212;&#8212;&#27573;&#33853;&#32423;&#24341;&#29992;&#24314;&#35758;&#65292;&#21363;&#20197;&#27573;&#33853;&#30340;&#20027;&#39064;&#21477;&#20026;&#36755;&#20837;&#65292;&#36755;&#20986;&#22312;&#27573;&#33853;&#20013;&#24341;&#29992;&#30340;&#24314;&#35758;&#65292;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#35299;&#20915;&#27492;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#29992;&#24314;&#35758;(CR)&#27169;&#22411;&#21487;&#24110;&#21161;&#20316;&#32773;&#22312;&#35770;&#25991;&#20889;&#20316;&#36807;&#31243;&#20013;&#30340;&#21508;&#20010;&#38454;&#27573;&#25214;&#21040;&#30456;&#20851;&#25991;&#31456;&#12290;&#22823;&#22810;&#25968;&#30740;&#31350;&#22788;&#29702;&#20840;&#23616;CR&#65292;&#35813;&#20840;&#23616;CR&#36866;&#29992;&#20110;&#21021;&#22987;&#20889;&#20316;&#38454;&#27573;&#30340;&#19968;&#33324;&#24314;&#35758;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#27573;&#33853;&#32423;CR&#20219;&#21153;&#65292;&#20316;&#20026;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#19968;&#31181;&#20013;&#38388;&#22320;&#24102;&#65292;&#20854;&#20013;&#27573;&#33853;&#30340;&#20027;&#39064;&#21477;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#27573;&#33853;&#20869;&#24341;&#29992;&#30340;&#24314;&#35758;&#20316;&#20026;&#36755;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#20219;&#21153;&#65292;&#24182;&#20351;&#29992;ACL&#35770;&#25991;&#25968;&#25454;&#38598;&#19978;&#30340;&#22235;&#20803;&#32452;&#25439;&#22833;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#32467;&#26524;&#26174;&#31034;&#30456;&#23545;&#20110;&#22522;&#32447;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Citation recommendation (CR) models may help authors find relevant articles at various stages of the paper writing process. Most research has dealt with either global CR, which produces general recommendations suitable for the initial writing stage, or local CR, which produces specific recommendations more fitting for the final writing stages. We propose the task of paragraph-level CR as a middle ground between the two approaches, where the paragraph's topic sentence is taken as input and recommendations for citing within the paragraph are produced at the output. We propose a model for this task, fine-tune it using the quadruplet loss on the dataset of ACL papers, and show improvements over the baselines.
&lt;/p&gt;</description></item><item><title>Glot500&#26159;&#19968;&#20010;&#27700;&#24179;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;511&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#30456;&#27604;&#20110;XLM-R&#22522;&#32447;&#65292;Glot500&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#36136;&#37327;&#30340;&#20915;&#23450;&#22240;&#32032;&#21253;&#25324;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#33050;&#26412;&#12289;&#30456;&#20851;&#35821;&#35328;&#30340;&#8220;&#24110;&#21161;&#8221;&#21644;&#27169;&#22411;&#30340;&#24635;&#23481;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.12182</link><description>&lt;p&gt;
Glot500&#65306;&#25193;&#23637;500&#31181;&#35821;&#35328;&#30340;&#22810;&#35821;&#26009;&#24211;&#21644;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Glot500: Scaling Multilingual Corpora and Language Models to 500 Languages. (arXiv:2305.12182v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12182
&lt;/p&gt;
&lt;p&gt;
Glot500&#26159;&#19968;&#20010;&#27700;&#24179;&#25193;&#23637;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#35206;&#30422;&#20102;511&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#30456;&#27604;&#20110;XLM-R&#22522;&#32447;&#65292;Glot500&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#34920;&#29616;&#12290;&#35813;&#27169;&#22411;&#36136;&#37327;&#30340;&#20915;&#23450;&#22240;&#32032;&#21253;&#25324;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#33050;&#26412;&#12289;&#30456;&#20851;&#35821;&#35328;&#30340;&#8220;&#24110;&#21161;&#8221;&#21644;&#27169;&#22411;&#30340;&#24635;&#23481;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#19968;&#30452;&#19987;&#27880;&#20110;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#32422;100&#31181;&#35821;&#35328;&#20013;&#26356;&#21152;&#20986;&#33394;&#12290;&#25105;&#20204;&#36890;&#36807;&#19981;&#26029;&#30340;&#39044;&#35757;&#32451;&#65292;&#27700;&#24179;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;Glot500-m&#65292;&#36825;&#26159;&#19968;&#20010;&#35206;&#30422;&#20102;511&#31181;&#35821;&#35328;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20854;&#20013;&#20960;&#20046;&#25152;&#26377;&#35821;&#35328;&#37117;&#26159;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#37325;&#35201;&#37096;&#20998;&#26159;&#25910;&#38598;&#21644;&#28165;&#29702;Glot500-c&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#36825;511&#31181;&#35821;&#35328;&#30340;&#35821;&#26009;&#24211;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#23545;Glot500-m&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35821;&#35328;&#19978;&#35780;&#20272;&#20102;Glot500-m&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#19982;XLM-R&#22522;&#32447;&#30456;&#27604;&#65292;Glot500-m&#22312;&#39640;&#36164;&#28304;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#34920;&#29616;&#37117;&#26377;&#20102;&#24456;&#22823;&#30340;&#25552;&#39640;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#21333;&#19968;&#22240;&#32032;&#21487;&#20197;&#35299;&#37322;&#22810;&#35821;&#35328;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#31034;&#30340;&#36136;&#37327;&#12290;&#30456;&#21453;&#65292;&#22810;&#20010;&#22240;&#32032;&#20915;&#23450;&#20102;&#36136;&#37327;&#65292;&#21253;&#25324;&#35821;&#26009;&#24211;&#22823;&#23567;&#12289;&#33050;&#26412;&#12289;&#30456;&#20851;&#35821;&#35328;&#30340;&#8220;&#24110;&#21161;&#8221;&#20197;&#21450;&#27169;&#22411;&#30340;&#24635;&#23481;&#37327;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35299;&#20915;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#30446;&#26631;&#65306;&#25105;&#20204;&#19981;&#24212;&#35813;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#23616;&#38480;&#20110;&#19990;&#30028;&#35821;&#35328;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#32780;&#26159;&#24212;&#35813;&#35753;&#23427;&#28085;&#30422;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
The NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 languages, almost all of them low-resource. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and lowresource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, "help" from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world's languages and instead 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMPSITION&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;CG&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12169</link><description>&lt;p&gt;
&#23398;&#20064;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#20197;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learn to Compose Syntactic and Semantic Representations Appropriately for Compositional Generalization. (arXiv:2305.12169v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;COMPSITION&#30340;&#26032;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#26469;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;CG&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24207;&#21015;&#21040;&#24207;&#21015;&#65288;Seq2Seq&#65289;&#27169;&#22411;&#22312;&#35299;&#20915;&#32452;&#21512;&#27867;&#21270;&#65288;CG&#65289;&#20219;&#21153;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#26080;&#27861;&#31995;&#32479;&#24615;&#22320;&#25512;&#24191;&#21040;&#30475;&#19981;&#35265;&#30340;&#24050;&#30693;&#32452;&#20214;&#32452;&#21512;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#38459;&#30861;CG&#30340;&#21407;&#22240;&#20043;&#19968;&#26159;&#32534;&#30721;&#22120;&#26368;&#19978;&#23618;&#30340;&#34920;&#31034;&#26159;&#32416;&#32544;&#30340;&#65292;&#20063;&#23601;&#26159;&#35828;&#65292;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#34987;&#19981;&#36866;&#24403;&#22320;&#25197;&#26354;&#20102;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20197;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#22312;&#26631;&#35760;&#32423;&#21035;&#19978;&#22686;&#24378;&#35821;&#20041;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#22320;&#32452;&#21512;&#24207;&#21015;&#30340;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#65292;&#23601;&#20687;&#20154;&#31867;&#25152;&#20570;&#30340;&#37027;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#20182;&#20204;&#21457;&#29616;&#30340;&#34920;&#31034;&#32416;&#32544;&#38382;&#39064;&#19981;&#20840;&#38754;&#65292;&#24182;&#36827;&#19968;&#27493;&#20551;&#35774;&#20256;&#36882;&#21040;&#19981;&#21516;&#35299;&#30721;&#22120;&#23618;&#30340;&#28304;&#38190;&#20540;&#34920;&#31034;&#20063;&#26159;&#32416;&#32544;&#22312;&#19968;&#36215;&#30340;&#12290;&#22522;&#20110;&#36825;&#20010;&#30452;&#35273;&#21644;&#21463;&#20154;&#31867;CG&#31574;&#30053;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COMPSITION&#65288;&#36866;&#24403;&#22320;&#32452;&#21512;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#20197;&#36827;&#34892;&#32452;&#21512;&#27867;&#21270;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#35299;&#20915;CG&#20219;&#21153;&#30340;&#26032;&#26694;&#26550;&#12290;COMPSITION&#36890;&#36807;&#20998;&#21035;&#24314;&#27169;&#21477;&#27861;&#21644;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#20960;&#20309;&#34920;&#31034;&#27169;&#22359;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#65292;&#26174;&#24335;&#22320;&#32452;&#21512;&#32534;&#30721;&#22120;&#30340;&#26368;&#19978;&#23618;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;COMPSITION&#22312;&#21512;&#25104;&#21644;&#33258;&#28982;&#35821;&#35328;CG&#20219;&#21153;&#19978;&#22343;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have shown that sequence-to-sequence (Seq2Seq) models are limited in solving the compositional generalization (CG) tasks, failing to systematically generalize to unseen compositions of seen components. There is mounting evidence that one of the reasons hindering CG is the representation of the encoder uppermost layer is entangled. In other words, the syntactic and semantic representations of sequences are twisted inappropriately. However, most previous studies mainly concentrate on enhancing semantic information at token-level, rather than composing the syntactic and semantic representations of sequences appropriately as humans do. In addition, we consider the representation entanglement problem they found is not comprehensive, and further hypothesize that source keys and values representations passing into different decoder layers are also entangled. Staring from this intuition and inspired by humans' strategies for CG, we propose COMPSITION (Compose Syntactic and Seman
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#20027;&#39064;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#30830;&#23450;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2305.12152</link><description>&lt;p&gt;
&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23457;&#35270;&#33258;&#21160;&#20027;&#39064;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Re-visiting Automated Topic Model Evaluation with Large Language Models. (arXiv:2305.12152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#20027;&#39064;&#27169;&#22411;&#36755;&#20986;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#30830;&#23450;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#27169;&#22411;&#29992;&#20110;&#23545;&#22823;&#22411;&#25991;&#26412;&#38598;&#21512;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;&#33258;&#21160;&#35780;&#20272;&#20027;&#39064;&#27169;&#22411;&#36755;&#20986;&#21644;&#30830;&#23450;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#19968;&#30452;&#26159;&#38271;&#26399;&#30340;&#25361;&#25112;&#65292;&#30446;&#21069;&#23578;&#26080;&#26377;&#25928;&#30340;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35780;&#20272;&#36825;&#31181;&#36755;&#20986;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30456;&#27604;&#29616;&#26377;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26356;&#33021;&#36866;&#24403;&#22320;&#35780;&#20272;&#20986;&#20135;&#29983;&#30340;&#20027;&#39064;&#65292;&#24182;&#19988;&#26356;&#33021;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#33021;&#21542;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#30830;&#23450;&#26368;&#20339;&#30340;&#20027;&#39064;&#25968;&#37327;&#12290;&#25105;&#20204;&#33258;&#21160;&#20026;&#25991;&#26723;&#20998;&#37197;&#26631;&#31614;&#65292;&#24182;&#36873;&#25321;&#20855;&#26377;&#26368;&#32431;&#20928;&#26631;&#31614;&#30340;&#37197;&#32622;&#65292;&#20197;&#36820;&#22238;&#21512;&#29702;&#30340;&#26368;&#20339;&#20027;&#39064;&#25968;&#37327;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topic models are used to make sense of large text collections. However, automatically evaluating topic model output and determining the optimal number of topics both have been longstanding challenges, with no effective automated solutions to date. This paper proposes using large language models to evaluate such output. We find that large language models appropriately assess the resulting topics, correlating more strongly with human judgments than existing automated metrics. We then investigate whether we can use large language models to automatically determine the optimal number of topics. We automatically assign labels to documents and choosing configurations with the most pure labels returns reasonable values for the optimal number of topics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12147</link><description>&lt;p&gt;
LogiCoT&#65306;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;4&#65288;GPT-4&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#33258;&#25105;&#25351;&#23548;&#35843;&#25972;&#30740;&#31350;&#65288;&#22914;Alpaca&#65289;&#20391;&#37325;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;&#36825;&#20123;&#25351;&#20196;&#20351;&#27169;&#22411;&#22312;&#19968;&#33324;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#21644;&#37322;&#20041;&#65289;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;GPT-3.5&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LogiCoT&#65292;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#25910;&#38598;&#25351;&#20196;&#20197;&#25552;&#31034;GPT-4&#29983;&#25104;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;LogiCoT&#20316;&#20026;&#25945;&#25480;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#24341;&#20986;&#20102;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#24615;&#25991;&#20214;&#32763;&#35793;&#20013;&#30340;&#25514;&#36766;&#22788;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#25919;&#27835;&#25991;&#26412;&#20013;&#30340;&#25514;&#36766;&#22312;&#33521;&#25991;&#20013;&#20986;&#29616;&#26356;&#39057;&#32321;&#65292;&#19988;&#32763;&#35793;&#26041;&#21521;&#24433;&#21709;&#25514;&#36766;&#20351;&#29992;&#39057;&#29575;&#21644;&#32763;&#35793;&#31574;&#30053;&#12290;&#21516;&#26102;&#36824;&#35266;&#23519;&#21040;&#20102;&#25514;&#36766;&#22312;&#21382;&#26102;&#26041;&#38754;&#30340;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2305.12146</link><description>&lt;p&gt;
&#23459;&#20256;&#24615;&#25991;&#20214;&#21452;&#21521;&#32763;&#35793;&#20013;&#30340;&#25514;&#36766;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hedges in Bidirectional Translations of Publicity-Oriented Documents. (arXiv:2305.12146v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12146
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23459;&#20256;&#24615;&#25991;&#20214;&#32763;&#35793;&#20013;&#30340;&#25514;&#36766;&#22788;&#29702;&#38382;&#39064;&#65292;&#21457;&#29616;&#25919;&#27835;&#25991;&#26412;&#20013;&#30340;&#25514;&#36766;&#22312;&#33521;&#25991;&#20013;&#20986;&#29616;&#26356;&#39057;&#32321;&#65292;&#19988;&#32763;&#35793;&#26041;&#21521;&#24433;&#21709;&#25514;&#36766;&#20351;&#29992;&#39057;&#29575;&#21644;&#32763;&#35793;&#31574;&#30053;&#12290;&#21516;&#26102;&#36824;&#35266;&#23519;&#21040;&#20102;&#25514;&#36766;&#22312;&#21382;&#26102;&#26041;&#38754;&#30340;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25514;&#36766;&#26159;&#36328;&#19987;&#19994;&#39046;&#22495;&#24191;&#27867;&#30740;&#31350;&#30340;&#35789;&#27719;&#65292;&#20294;&#25919;&#27835;&#25991;&#26412;&#20013;&#25514;&#36766;&#30340;&#32763;&#35793;&#30740;&#31350;&#26497;&#20026;&#26377;&#38480;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25514;&#36766;&#22312;&#30446;&#26631;&#25991;&#26412;&#20013;&#30340;&#35789;&#39057;&#21464;&#21270;&#26159;&#21542;&#20855;&#26377;&#21382;&#26102;&#24615;&#65292;&#32763;&#35793;&#20013;&#25514;&#36766;&#36890;&#36807;&#24180;&#20221;&#21464;&#21270;&#30340;&#31243;&#24230;&#22914;&#20309;&#24402;&#22240;&#20110;&#28304;&#25991;&#26412;&#65292;&#24182;&#37319;&#29992;&#20102;&#20309;&#31181;&#32763;&#35793;&#31574;&#30053;&#26469;&#22788;&#29702;&#23427;&#20204;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30740;&#31350;&#30446;&#30340;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#26469;&#33258;&#20013;&#22269;&#21644;&#32852;&#21512;&#22269;&#30340;&#20004;&#31181;&#20844;&#21153;&#25919;&#27835;&#25991;&#26412;&#21450;&#20854;&#32763;&#35793;&#65292;&#24418;&#25104;&#20102;&#19977;&#20010;&#23376;&#35821;&#26009;&#24211;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25514;&#36766;&#22312;&#33521;&#25991;&#25919;&#27835;&#25991;&#26412;&#20013;&#65288;&#26080;&#35770;&#26159;&#21407;&#22987;&#33521;&#25991;&#36824;&#26159;&#32763;&#35793;&#33521;&#25991;&#65289;&#20284;&#20046;&#26356;&#39057;&#32321;&#20986;&#29616;&#12290;&#27492;&#22806;&#65292;&#32763;&#35793;&#26041;&#21521;&#20284;&#20046;&#22312;&#24433;&#21709;&#25514;&#36766;&#20351;&#29992;&#30340;&#39057;&#29575;&#21644;&#32763;&#35793;&#31574;&#30053;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;&#65292;&#25514;&#36766;&#20351;&#29992;&#39057;&#29575;&#22312;&#25105;&#20204;&#30340;&#23376;&#35821;&#26009;&#24211;&#20013;&#20986;&#29616;&#20102;&#26174;&#33879;&#30340;&#21382;&#26102;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hedges are widely studied across registers and disciplines, yet research on the translation of hedges in political texts is extremely limited. This contrastive study is dedicated to investigating whether there is a diachronic change in the frequencies of hedging devices in the target texts, to what extent the changing frequencies of translated hedges through years are attributed to the source texts, and what translation strategies are adopted to deal with them. For the purposes of this research, two types of official political texts and their translations from China and the United Nations were collected to form three sub-corpora. Results show that hedges tend to appear more frequently in English political texts, be it original English or translated English. In addition, directionality seems to play an important role in influencing both the frequencies and translation strategies regarding the use of hedges. A noticeable diachronic increase of hedging devices is also observed in our corp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;MiniMoE&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#23398;&#29983;&#30340;&#23481;&#37327;&#32780;&#19981;&#26126;&#26174;&#22686;&#21152;&#25512;&#29702;&#35745;&#31639;&#35299;&#38500;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;&#65292;&#24182;&#22312;GLUE&#21644;CoNLL&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.12129</link><description>&lt;p&gt;
&#35299;&#38500;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
Lifting the Curse of Capacity Gap in Distilling Language Models. (arXiv:2305.12129v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12129
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65288;MiniMoE&#65289;&#65292;&#36890;&#36807;&#22686;&#21152;&#23398;&#29983;&#30340;&#23481;&#37327;&#32780;&#19981;&#26126;&#26174;&#22686;&#21152;&#25512;&#29702;&#35745;&#31639;&#35299;&#38500;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;&#65292;&#24182;&#22312;GLUE&#21644;CoNLL&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#25512;&#29702;&#35745;&#31639;&#12290;&#30693;&#35782;&#33976;&#39311;&#36890;&#36807;&#24072;&#29983;&#33539;&#24335;&#20026;&#23567;&#22411;&#27169;&#22411;&#21387;&#32553;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#24403;&#24072;&#29983;&#20043;&#38388;&#30340;&#23481;&#37327;&#24046;&#36317;&#24456;&#22823;&#26102;&#65292;&#23481;&#37327;&#24046;&#36317;&#35781;&#21650;&#20250;&#20986;&#29616;&#65292;&#23548;&#33268;&#33976;&#39311;&#35821;&#35328;&#27169;&#22411;&#19981;&#36275;&#12290;&#34429;&#28982;&#24050;&#26377;&#20960;&#39033;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#35781;&#21650;&#20173;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#35299;&#20915;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#22686;&#21152;&#23398;&#29983;&#30340;&#23481;&#37327;&#32780;&#19981;&#26126;&#26174;&#22686;&#21152;&#25512;&#29702;&#35745;&#31639;&#26469;&#35299;&#38500;&#23481;&#37327;&#24046;&#24322;&#35781;&#21650;&#12290;&#21463;&#28151;&#21512;&#19987;&#23478;(Sparse Activation Regime of Mixture of Experts (MoE))&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26368;&#23567;&#19987;&#23478;(MiniMoE)&#30340;&#28151;&#21512;&#29289;&#65292;&#36825;&#20026;&#23398;&#29983;&#24341;&#20837;&#20102;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#20294;&#20960;&#20046;&#27809;&#26377;&#24341;&#20837;&#20219;&#20309;&#39069;&#22806;&#30340;&#25512;&#29702;&#35745;&#31639;&#12290;&#22312;GLUE&#21644;CoNLL&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MiniMoE&#30340;&#39764;&#21147;&#28040;&#38500;&#20102;&#23481;&#37327;&#24046;&#36317;&#35781;&#21650;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Q-Diversity&#65292;&#19982;group DRO&#36739;&#22909;&#22320;&#37197;&#21512;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#35757;&#32451;&#27169;&#24335;&#30452;&#25509;&#21442;&#25968;&#21270;&#32452;&#30340;&#35782;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12123</link><description>&lt;p&gt;
Robust Optimization&#20013;&#26368;&#23567;&#26368;&#22823;&#21338;&#24328;&#20013;Q-Diversity&#30340;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling the Q-Diversity in a Min-max Play Game for Robust Optimization. (arXiv:2305.12123v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Q-Diversity&#65292;&#19982;group DRO&#36739;&#22909;&#22320;&#37197;&#21512;&#65292;&#36890;&#36807;&#20132;&#20114;&#24335;&#35757;&#32451;&#27169;&#24335;&#30452;&#25509;&#21442;&#25968;&#21270;&#32452;&#30340;&#35782;&#21035;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#35757;&#32451;&#30340;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#20381;&#36182;&#34920;&#38754;&#30456;&#20851;&#24615;&#65292;&#23548;&#33268;&#27867;&#21270;&#24615;&#24046;&#12290;&#20998;&#32452;&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#65288;group DRO&#65289;&#21487;&#20197;&#36890;&#36807;&#22312;&#39044;&#23450;&#20041;&#32452;&#19978;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#25439;&#22833;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#26114;&#36149;&#30340;&#27880;&#37322;&#21644;&#38544;&#31169;&#31561;&#22240;&#32032;&#37117;&#20250;&#38459;&#30861;&#32452;&#26631;&#31614;&#30340;&#21487;&#29992;&#24615;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#36229;&#20986;&#20998;&#24067;&#20043;&#22806;&#30340;&#27867;&#21270;&#30340;&#22833;&#36133;&#27169;&#24335;&#26102;&#65292;&#8220;group DRO&#8221;&#20013;&#30340;&#37325;&#26032;&#21152;&#26435;&#20856;&#22411;&#36807;&#31243;&#20250;&#22833;&#21435;&#25928;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#38480;&#21046;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;Q-Diversity&#37325;&#26032;&#26500;&#24314;&#20102;&#8220;group DRO&#8221;&#26694;&#26550;&#12290;Q-Diversity&#37319;&#29992;&#20132;&#20114;&#24335;&#35757;&#32451;&#27169;&#24335;&#65292;&#24182;&#23558;&#32452;&#30340;&#35782;&#21035;&#20174;&#27880;&#37322;&#20013;&#25918;&#23485;&#21040;&#30452;&#25509;&#21442;&#25968;&#21270;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#31574;&#30053;&#65292;&#20197;&#20998;&#25955;&#26410;&#34987;&#20805;&#20998;&#20195;&#34920;&#30340;&#32452;&#12290;&#20316;&#32773;&#22312;&#19968;&#31995;&#21015;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25991;&#26412;&#20998;&#31867;&#30340;&#23454;&#39564;&#20013;&#27979;&#35797;&#20102;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models trained with empirical risk minimization (ERM) are revealed to easily rely on spurious correlations, resulting in poor generalization. Group distributionally robust optimization (group DRO) can alleviate this problem by minimizing the worst-case loss over pre-defined groups. While promising, in practice factors like expensive annotations and privacy preclude the availability of group labels. More crucially, when taking a closer look at the failure modes of out-of-distribution generalization, the typical procedure of reweighting in group DRO loses efficiency. Hinged on the limitations, in this work, we reformulate the group DRO framework by proposing Q-Diversity. Characterized by an interactive training mode, Q-Diversity relaxes the group identification from annotation into direct parameterization. Furthermore, a novel mixing strategy across groups is presented to diversify the under-represented groups. In a series of experiments on both synthetic and real-world text classificati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#20449;&#24687;&#21644;&#24378;&#35843;&#39044;&#27979;&#22120;&#23454;&#29616;&#34920;&#29616;&#21147;&#24378;&#19988;&#33258;&#28982;&#30340;TTS&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.12107</link><description>&lt;p&gt;
&#29992;&#35821;&#35328;&#20449;&#24687;&#23454;&#29616;&#24378;&#28872;&#24863;&#24773;&#34920;&#29616;&#21644;&#35821;&#35843;&#30340;TTS&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
EE-TTS: Emphatic Expressive TTS with Linguistic Information. (arXiv:2305.12107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12107
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#20449;&#24687;&#21644;&#24378;&#35843;&#39044;&#27979;&#22120;&#23454;&#29616;&#34920;&#29616;&#21147;&#24378;&#19988;&#33258;&#28982;&#30340;TTS&#31995;&#32479;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#34920;&#29616;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;TTS&#31995;&#32479;&#33021;&#22815;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#65292;&#20294;&#35201;&#20135;&#29983;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#35821;&#38899;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#24378;&#35843;&#26159;&#20915;&#23450;&#35821;&#38899;&#34920;&#29616;&#21147;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#28155;&#21152;&#20013;&#38388;&#29305;&#24449;&#26469;&#22686;&#24378;&#24378;&#35843;&#65292;&#20294;&#19981;&#33021;&#20445;&#35777;&#35821;&#38899;&#30340;&#25972;&#20307;&#34920;&#29616;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EE-TTS&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;&#21477;&#27861;&#21644;&#35821;&#20041;&#19978;&#30340;&#22810;&#23618;&#35821;&#35328;&#20449;&#24687;&#12290;EE-TTS&#21253;&#21547;&#19968;&#20010;&#24378;&#35843;&#20301;&#32622;&#39044;&#27979;&#22120;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#20013;&#35782;&#21035;&#20986;&#36866;&#24403;&#30340;&#24378;&#35843;&#20301;&#32622;&#65292;&#24182;&#20855;&#26377;&#19968;&#20010;&#26465;&#20214;&#22768;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#21512;&#25104;&#24102;&#24378;&#35843;&#21644;&#35821;&#35328;&#20449;&#24687;&#30340;&#34920;&#29616;&#21147;&#35821;&#38899;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;EE-TTS&#22312;&#34920;&#29616;&#21147;&#21644;&#33258;&#28982;&#24230;&#26041;&#38754;&#27604;&#22522;&#32447;&#26377;0.49&#21644;0.67&#30340;MOS&#25552;&#21319;&#12290;EE-TTS&#36824;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26681;&#25454;AB&#27979;&#35797;&#32467;&#26524;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#37117;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Current TTS systems perform well in synthesizing high-quality speech, producing highly expressive speech remains a challenge. Emphasis, as a critical factor in determining the expressiveness of speech, has attracted more attention nowadays. Previous works usually enhance the emphasis by adding intermediate features, but they can not guarantee the overall expressiveness of the speech. To resolve this matter, we propose Emphatic Expressive TTS (EE-TTS), which leverages multi-level linguistic information from syntax and semantics. EE-TTS contains an emphasis predictor that can identify appropriate emphasis positions from text and a conditioned acoustic model to synthesize expressive speech with emphasis and linguistic information. Experimental results indicate that EE-TTS outperforms baseline with MOS improvements of 0.49 and 0.67 in expressiveness and naturalness. EE-TTS also shows strong generalization across different datasets according to AB test results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#33021;&#21542;&#27491;&#30830;&#22320;&#25512;&#29702;&#25171;&#30772;&#24120;&#35265;&#20551;&#35774;&#30340;&#24773;&#20917;&#30340;&#35821;&#22659;&#65292;&#24182;&#31995;&#32479;&#22320;&#21019;&#24314;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.12096</link><description>&lt;p&gt;
NLP&#27169;&#22411;&#33021;&#21542;&#27491;&#30830;&#22788;&#29702;&#25171;&#30772;&#24120;&#35265;&#20551;&#35774;&#30340;&#24773;&#22659;&#25512;&#29702;?
&lt;/p&gt;
&lt;p&gt;
Can NLP Models Correctly Reason Over Contexts that Break the Common Assumptions?. (arXiv:2305.12096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#33021;&#21542;&#27491;&#30830;&#22320;&#25512;&#29702;&#25171;&#30772;&#24120;&#35265;&#20551;&#35774;&#30340;&#24773;&#20917;&#30340;&#35821;&#22659;&#65292;&#24182;&#31995;&#32479;&#22320;&#21019;&#24314;&#20102;&#30456;&#24212;&#30340;&#35780;&#20272;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22522;&#30784;&#19978;&#65292;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#33719;&#21462;&#20016;&#23500;&#30340;&#20107;&#23454;&#21644;&#24120;&#35782;&#30693;&#35782;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#33719;&#24471;&#38750;&#20961;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#20986;&#29616;&#19981;&#31526;&#21512;&#36825;&#20123;&#35268;&#24459;&#30340;&#24773;&#20917;&#65292;&#21363;&#25171;&#30772;&#24120;&#35265;&#20551;&#35774;&#30340;&#22330;&#26223;&#12290;&#26368;&#20808;&#36827;&#30340;NLP&#27169;&#22411;&#33021;&#21542;&#27491;&#30830;&#22320;&#25512;&#29702;&#36825;&#20123;&#24773;&#20917;&#30340;&#35821;&#22659;&#21602;&#65311;&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#27491;&#30830;&#22788;&#29702;&#25171;&#30772;&#24120;&#35265;&#20551;&#35774;&#30340;&#24773;&#22659;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#31995;&#32479;&#22320;&#21019;&#24314;&#20102;&#35780;&#20272;&#25968;&#25454;&#65292;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#37117;&#21253;&#25324;&#19968;&#20010;&#24120;&#35265;&#20551;&#35774;&#12289;&#19968;&#20010;&#36981;&#24490;&#35813;&#20551;&#35774;&#30340;&#35821;&#22659;&#12289;&#19968;&#20010;&#25171;&#30772;&#35813;&#20551;&#35774;&#30340;&#35821;&#22659;&#21644;&#22522;&#20110;&#36825;&#20123;&#35821;&#22659;&#30340;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#22312;&#23454;&#39564;&#20013;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-training on large corpora of text enables the language models to acquire a vast amount of factual and commonsense knowledge which allows them to achieve remarkable performance on a variety of language understanding tasks. They typically acquire this knowledge by learning from the pre-training text and capturing certain patterns from it. However, real-world settings often present scenarios that do not abide by these patterns i.e. scenarios that break the common assumptions. Can state-of-the-art NLP models correctly reason over the contexts of such scenarios?  Addressing the above question, in this paper, we investigate the ability of models to correctly reason over contexts that break the common assumptions. To this end, we first systematically create evaluation data in which each data instance consists of (a) a common assumption, (b) a context that follows the assumption, (c) a context that breaks the assumption, and (d) questions based on the contexts. Then, through evaluations on
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ESCOXLM-R&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#22788;&#29702;&#32844;&#19994;&#24066;&#22330;&#39046;&#22495;&#30340;&#22810;&#39033;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.12092</link><description>&lt;p&gt;
ESCOXLM-R: &#22810;&#35821;&#35328;&#30340;&#22522;&#20110;&#20998;&#31867;&#27861;&#30340;&#32844;&#19994;&#22521;&#35757;
&lt;/p&gt;
&lt;p&gt;
ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain. (arXiv:2305.12092v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12092
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ESCOXLM-R&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#25216;&#26415;&#65292;&#33021;&#22815;&#25104;&#21151;&#22320;&#22788;&#29702;&#32844;&#19994;&#24066;&#22330;&#39046;&#22495;&#30340;&#22810;&#39033;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#22522;&#20934;&#25968;&#37327;&#22312;&#35745;&#31639;&#32844;&#19994;&#24066;&#22330;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#65292;&#24378;&#35843;&#38656;&#35201;&#33021;&#22815;&#22788;&#29702;&#19982;&#25216;&#33021;&#25552;&#21462;&#12289;&#25216;&#33021;&#20998;&#31867;&#12289;&#24037;&#20316;&#26631;&#39064;&#20998;&#31867;&#21644;&#21435;&#26631;&#35782;&#31526;&#31561;&#26041;&#38754;&#30456;&#20851;&#30340;&#32844;&#30456;&#20851;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#24050;&#32463;&#24320;&#21457;&#20102;&#19968;&#20123;&#29305;&#23450;&#20110;&#32844;&#19994;&#24066;&#22330;&#39046;&#22495;&#30340;&#26041;&#27861;&#65292;&#20294;&#32570;&#20047;&#36890;&#29992;&#30340;&#12289;&#22810;&#35821;&#35328;&#30340;&#27169;&#22411;&#21644;&#36825;&#20123;&#20219;&#21153;&#30340;&#22522;&#20934;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#35821;&#35328;&#27169;&#22411;&#65292;&#31216;&#20026;ESCOXLM-R&#65292;&#23427;&#26159;&#22522;&#20110;XLM-R&#30340;&#65292;&#20351;&#29992;&#27431;&#27954;&#25216;&#33021;&#12289;&#31454;&#20105;&#21147;&#12289;&#36164;&#26684;&#21644;&#32844;&#19994;ESCO&#20998;&#31867;&#27861;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#39044;&#35757;&#32451;&#65292;&#35206;&#30422;27&#31181;&#35821;&#35328;&#12290;ESCOXLM-R&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#21253;&#25324;&#21160;&#24577;&#36974;&#30422;&#35821;&#35328;&#24314;&#27169;&#21644;&#24341;&#20837;&#22810;&#35821;&#35328;&#20998;&#31867;ESCOS&#20851;&#31995;&#30340;&#26032;&#22411;&#38468;&#21152;&#30446;&#26631;&#12290;&#25105;&#20204;&#22312;4&#31181;&#35821;&#35328;&#19978;&#23545;ESCOXLM-R&#30340;6&#20010;&#24207;&#21015;&#26631;&#31614;&#20219;&#21153;&#21644;3&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#21462;&#24471;&#20102;&#26497;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing number of benchmarks for Natural Language Processing (NLP) tasks in the computational job market domain highlights the demand for methods that can handle job-related tasks such as skill extraction, skill classification, job title classification, and de-identification. While some approaches have been developed that are specific to the job market domain, there is a lack of generalized, multilingual models and benchmarks for these tasks. In this study, we introduce a language model called ESCOXLM-R, based on XLM-R, which uses domain-adaptive pre-training on the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy, covering 27 languages. The pre-training objectives for ESCOXLM-R include dynamic masked language modeling and a novel additional objective for inducing multilingual taxonomical ESCO relations. We comprehensively evaluate the performance of ESCOXLM-R on 6 sequence labeling and 3 classification tasks in 4 languages and find that it achieves s
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#21046;&#20316;&#20102;&#30456;&#24212;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;</title><link>http://arxiv.org/abs/2305.12091</link><description>&lt;p&gt;
&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
"What do others think?": Task-Oriented Conversational Modeling with Subjective Knowledge. (arXiv:2305.12091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12091
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#21046;&#20316;&#20102;&#30456;&#24212;&#25968;&#25454;&#38598;&#12290;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#26032;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#31995;&#32479;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#33021;&#22815;&#24110;&#21161;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#30446;&#26631;&#30340;&#23545;&#35805;&#31995;&#32479;&#65292;&#20363;&#22914;&#39044;&#23450;&#37202;&#24215;&#25110;&#39184;&#21381;&#12290;&#20256;&#32479;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#31995;&#32479;&#20381;&#36182;&#20110;&#29305;&#23450;&#39046;&#22495;&#30340;API/&#25968;&#25454;&#24211;&#25110;&#22806;&#37096;&#20107;&#23454;&#30693;&#35782;&#26469;&#29983;&#25104;&#21709;&#24212;&#65292;&#26080;&#27861;&#28385;&#36275;&#20027;&#35266;&#29992;&#25143;&#35831;&#27714;&#65288;&#20363;&#22914;&#8220;Wi-Fi&#21487;&#38752;&#21527;&#65311;&#8221;&#25110;&#8220;&#39184;&#21381;&#29615;&#22659;&#22909;&#21527;&#65311;&#8221;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20027;&#35266;&#30693;&#35782;&#39537;&#21160;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#24314;&#27169;&#65288;SK-TOD&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20027;&#35266;&#30693;&#35782;&#23547;&#27714;&#23545;&#35805;&#19978;&#19979;&#25991;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#22522;&#20110;&#20027;&#35266;&#30693;&#35782;&#26469;&#28304;&#30340;&#21709;&#24212;&#12290;&#22312;&#19982;&#29616;&#26377;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20010;&#20219;&#21153;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#22914;&#20309;&#27719;&#24635;&#22810;&#20010;&#30693;&#35782;&#29255;&#27573;&#20013;&#30340;&#19981;&#21516;&#24847;&#35265;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#33021;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#20219;&#21153;&#23548;&#21521;&#24335;&#23545;&#35805;&#21644;&#20027;&#35266;&#20869;&#23481;&#29702;&#35299;&#30340;&#30740;&#31350;&#12290;&#20195;&#30721;&#21644;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312; https://github.com/alex &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-oriented Dialogue (TOD) Systems aim to build dialogue systems that assist users in accomplishing specific goals, such as booking a hotel or a restaurant. Traditional TODs rely on domain-specific APIs/DBs or external factual knowledge to generate responses, which cannot accommodate subjective user requests (e.g., "Is the WIFI reliable?" or "Does the restaurant have a good atmosphere?"). To address this issue, we propose a novel task of subjective-knowledge-based TOD (SK-TOD). We also propose the first corresponding dataset, which contains subjective knowledge-seeking dialogue contexts and manually annotated responses grounded in subjective knowledge sources. When evaluated with existing TOD approaches, we find that this task poses new challenges such as aggregating diverse opinions from multiple knowledge snippets. We hope this task and dataset can promote further research on TOD and subjective content understanding. The code and the dataset are available at https://github.com/alex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2305.12090</link><description>&lt;p&gt;
UP5: &#38754;&#21521;&#20844;&#24179;&#24615;&#25512;&#33616;&#30340;&#26080;&#20559;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UP5: Unbiased Foundation Model for Fairness-aware Recommendation. (arXiv:2305.12090v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#30784;&#27169;&#22411;UP5&#65292;&#23427;&#37319;&#29992;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#26469;&#28040;&#38500;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20559;&#35265;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31561;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24050;&#23558;&#23427;&#20204;&#25512;&#21040;&#20102;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#30340;&#21069;&#27839;&#12290;&#27492;&#22806;&#65292;RS&#20013;&#30340;&#20844;&#24179;&#24615;&#24456;&#20851;&#38190;&#65292;&#22240;&#20026;&#35768;&#22810;&#29992;&#25143;&#23558;&#20854;&#29992;&#20110;&#20915;&#31574;&#21644;&#38656;&#27714;&#23653;&#34892;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#32570;&#20047;&#23545;&#25512;&#33616;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20844;&#24179;&#24615;&#27700;&#24179;&#21644;&#20844;&#24179;&#22788;&#29702;&#19981;&#21516;&#29992;&#25143;&#32676;&#32452;&#30340;&#36866;&#24403;&#26041;&#27861;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#29992;&#25143;&#26041;&#38754;&#30340;&#19981;&#20844;&#24179;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24443;&#24213;&#26816;&#26597;&#34920;&#26126;&#65292;LLMs&#20013;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#25512;&#33616;&#32467;&#26524;&#12290;&#20026;&#20102;&#28040;&#38500;LLM&#20013;&#30340;&#20559;&#24046;&#20197;&#23454;&#29616;&#38754;&#21521;&#20844;&#24179;&#24615;&#30340;&#25512;&#33616;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#21453;&#20107;&#23454;&#20844;&#24179;&#20419;&#36827;&#25216;&#26415;&#30340;&#26032;&#22411;&#26080;&#20559;P5&#65288;UP5&#65289;&#22522;&#30784;&#27169;&#22411;&#12290;CFP&#21253;&#25324;&#20004;&#20010;&#23376;&#27169;&#22359;&#65306;&#20010;&#24615;&#21270;&#21069;&#32512;&#25552;&#31034;&#21644;Prompt&#28151;&#21512;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#20010;&#20307;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that int
&lt;/p&gt;</description></item><item><title>&#21069;&#32512;&#20256;&#25773;&#26159;&#19968;&#31181;&#38024;&#23545;&#38271;&#24207;&#21015;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;50%&#20943;&#23569;&#21442;&#25968;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20219;&#21153;&#26102;&#20855;&#26377;&#26356;&#20248;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12086</link><description>&lt;p&gt;
&#21069;&#32512;&#20256;&#25773;: &#38024;&#23545;&#38271;&#24207;&#21015;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prefix Propagation: Parameter-Efficient Tuning for Long Sequences. (arXiv:2305.12086v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12086
&lt;/p&gt;
&lt;p&gt;
&#21069;&#32512;&#20256;&#25773;&#26159;&#19968;&#31181;&#38024;&#23545;&#38271;&#24207;&#21015;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#21487;&#23454;&#29616;50%&#20943;&#23569;&#21442;&#25968;&#19988;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20219;&#21153;&#26102;&#20855;&#26377;&#26356;&#20248;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#26088;&#22312;&#20943;&#36731;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#35843;&#25972;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#20869;&#23384;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#21069;&#32512;&#20256;&#25773;&#36825;&#19968;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#24357;&#34917;&#30446;&#21069;&#21069;&#32512;&#35843;&#25972;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#21069;&#32512;&#20256;&#25773;&#19982;&#21069;&#32512;&#35843;&#25972;&#30456;&#27604;&#22312;&#22788;&#29702;&#38271;&#25991;&#26723;&#20219;&#21153;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25152;&#38656;&#21442;&#25968;&#20063;&#21482;&#26377;&#21069;&#32773;&#30340;&#19968;&#21322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient tuning aims to mitigate the large memory requirements of adapting pretrained language models for downstream tasks. For example, one popular method, prefix-tuning, prepends trainable tokens to sequences while freezing the rest of the model's parameters. Although such models attain comparable performance with fine-tuning when applied to sequences with short to moderate lengths, we show their inferior performance when modelling long sequences. To bridge this gap, we propose prefix-propagation, a simple but effective approach that conditions prefixes on previous hidden states. We empirically demonstrate that prefix-propagation outperforms prefix-tuning across long-document tasks, while using 50% fewer parameters. To further investigate the proposed architecture, we also show its advantage in calibration, and perform additional study on its relationship with kernel attention. To the best of our knowledge, this work is the first to focus on parameter-efficient learning fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20272;&#20102;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#19979;&#33521;&#25991;&#25991;&#26412;&#30340;&#27010;&#29575;&#25552;&#20986;&#30340;&#29109;&#29575;&#24658;&#23450;&#21407;&#29702;&#65292;&#26410;&#33021;&#25214;&#21040;&#26126;&#26174;&#30340;&#25903;&#25345;&#29109;&#29575;&#24658;&#23450;&#30340;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.12084</link><description>&lt;p&gt;
&#37325;&#35775;&#25991;&#26412;&#29109;&#29575;&#24658;&#23450;
&lt;/p&gt;
&lt;p&gt;
Revisiting Entropy Rate Constancy in Text. (arXiv:2305.12084v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12084
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20272;&#20102;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#19979;&#33521;&#25991;&#25991;&#26412;&#30340;&#27010;&#29575;&#25552;&#20986;&#30340;&#29109;&#29575;&#24658;&#23450;&#21407;&#29702;&#65292;&#26410;&#33021;&#25214;&#21040;&#26126;&#26174;&#30340;&#25903;&#25345;&#29109;&#29575;&#24658;&#23450;&#30340;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#65288;UID&#65289;&#20551;&#35828;&#34920;&#26126;&#65292;&#20154;&#31867;&#20542;&#21521;&#20110;&#22312;&#35805;&#35821;&#25110;&#35805;&#35821;&#20013;&#22823;&#33268;&#22343;&#21248;&#20998;&#24067;&#20449;&#24687;&#12290;&#25903;&#25345;UID&#20551;&#35828;&#30340;&#26089;&#26399;&#35777;&#25454;&#26469;&#33258;Genzel&#65286;Charniak&#65288;2002&#65289;&#65292;&#20182;&#20204;&#22522;&#20110;n-gram&#35821;&#35328;&#27169;&#22411;&#19979;&#33521;&#25991;&#25991;&#26412;&#30340;&#27010;&#29575;&#25552;&#20986;&#20102;&#29109;&#29575;&#24658;&#23450;&#21407;&#29702;&#12290;&#26412;&#25991;&#20351;&#29992;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#35780;&#20272;Genzel&#65286;Charniak&#65288;2002&#65289;&#30340;&#35828;&#27861;&#65292;&#26410;&#33021;&#25214;&#21040;&#26126;&#26174;&#30340;&#25903;&#25345;&#29109;&#29575;&#24658;&#23450;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#22312;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#35821;&#35328;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#24182;&#35752;&#35770;&#20102;&#32479;&#19968;&#20449;&#24687;&#23494;&#24230;&#20551;&#35828;&#21644;&#26356;&#24191;&#27867;&#30340;&#26377;&#25928;&#20256;&#25773;&#35821;&#35328;&#29702;&#35770;&#30340;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
The uniform information density (UID) hypothesis states that humans tend to distribute information roughly evenly across an utterance or discourse. Early evidence in support of the UID hypothesis came from Genzel &amp; Charniak (2002), which proposed an entropy rate constancy principle based on the probability of English text under n-gram language models. We re-evaluate the claims of Genzel &amp; Charniak (2002) with neural language models, failing to find clear evidence in support of entropy rate constancy. We conduct a range of experiments across datasets, model sizes, and languages and discuss implications for the uniform information density hypothesis and linguistic theories of efficient communication more broadly.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#36827;&#34892;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39592;&#26550;&#29983;&#25104;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#26356;&#22909;&#22320;&#28040;&#32791;&#23545;&#35805;&#29366;&#24577;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;SkeletonNet&#36827;&#34892;&#39592;&#26550;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12077</link><description>&lt;p&gt;
&#21033;&#29992;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#36827;&#34892;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Dialogue Summarization via Skeleton-Assisted Prompt Transfer. (arXiv:2305.12077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#36827;&#34892;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39592;&#26550;&#29983;&#25104;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#26356;&#22909;&#22320;&#28040;&#32791;&#23545;&#35805;&#29366;&#24577;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;SkeletonNet&#36827;&#34892;&#39592;&#26550;&#29983;&#25104;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#23545;&#35805;&#25688;&#35201;&#30340;&#26631;&#35760;&#26679;&#26412;&#36890;&#24120;&#26159;&#26377;&#38480;&#30340;&#65288;&#21363;&#23569;&#26679;&#26412;&#65289;&#65292;&#22240;&#20026;&#20026;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#25688;&#35201;&#20184;&#20986;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#20174;&#23569;&#26679;&#26412;&#26679;&#26412;&#20013;&#23398;&#20064;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#21033;&#29992;&#20102;&#20854;&#20182;&#19979;&#28216;&#20219;&#21153;&#30340;&#28023;&#37327;&#27880;&#37322;&#25968;&#25454;&#65292;&#24182;&#22312;&#25552;&#31034;&#35843;&#25972;&#20013;&#25191;&#34892;&#25552;&#31034;&#20256;&#36882;&#65292;&#20197;&#23454;&#29616;&#36328;&#20219;&#21153;&#30693;&#35782;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36890;&#29992;&#25552;&#31034;&#20256;&#36882;&#25216;&#26415;&#32570;&#20047;&#23545;&#23545;&#35805;&#29305;&#23450;&#20449;&#24687;&#30340;&#32771;&#34385;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#25913;&#21892;&#20174;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#21040;&#23545;&#35805;&#25688;&#35201;&#30340;&#25552;&#31034;&#20256;&#36882;&#65292;&#24182;&#25552;&#20986;&#20102;&#39592;&#26550;&#36741;&#21161;&#30340;&#25552;&#31034;&#20256;&#36882;&#65288;SAPT&#65289;&#65292;&#23427;&#21033;&#29992;&#39592;&#26550;&#29983;&#25104;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#65292;&#20316;&#20026;&#36830;&#25509;&#19981;&#21516;&#28304;&#21644;&#30446;&#26631;&#20219;&#21153;&#30340;&#23186;&#20171;&#65292;&#20351;&#27169;&#22411;&#26356;&#22909;&#22320;&#28040;&#32791;&#23545;&#35805;&#29366;&#24577;&#20449;&#24687;&#12290;&#20026;&#20102;&#33258;&#21160;&#25552;&#21462;&#23545;&#35805;&#39592;&#26550;&#20316;&#20026;&#39592;&#26550;&#29983;&#25104;&#30340;&#21463;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;SkeletonNet&#30340;&#26032;&#22411;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#27169;&#22411;&#65292;&#20854;&#20013;&#28041;&#21450;&#19968;&#20010;&#19987;&#38376;&#35774;&#35745;&#30340;&#39592;&#26550;&#29983;&#25104;&#27169;&#22359;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;SAPT&#22312;&#20004;&#20010;&#23569;&#26679;&#26412;&#23545;&#35805;&#25688;&#35201;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world scenarios, labeled samples for dialogue summarization are usually limited (i.e., few-shot) due to high annotation costs for high-quality dialogue summaries. To efficiently learn from few-shot samples, previous works have utilized massive annotated data from other downstream tasks and then performed prompt transfer in prompt tuning so as to enable cross-task knowledge transfer. However, existing general-purpose prompt transfer techniques lack consideration for dialogue-specific information. In this paper, we focus on improving the prompt transfer from dialogue state tracking to dialogue summarization and propose Skeleton-Assisted Prompt Transfer (SAPT), which leverages skeleton generation as extra supervision that functions as a medium connecting the distinct source and target task and resulting in the model's better consumption of dialogue state information. To automatically extract dialogue skeletons as supervised training data for skeleton generation, we design a novel 
&lt;/p&gt;</description></item><item><title>DisCo&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24494;&#35843;&#30001;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;&#37319;&#29992;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DisCo&#30456;&#23545;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;</title><link>http://arxiv.org/abs/2305.12074</link><description>&lt;p&gt;
DisCo: &#20351;&#29992;&#33976;&#39311;&#32858;&#21512;&#21327;&#21516;&#35757;&#32451;&#21322;&#30417;&#30563;&#25991;&#26412;&#25366;&#25496;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining. (arXiv:2305.12074v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12074
&lt;/p&gt;
&lt;p&gt;
DisCo&#26159;&#19968;&#20010;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#24494;&#35843;&#30001;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#65292;&#37319;&#29992;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;DisCo&#30456;&#23545;&#20110;&#24050;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#25928;&#26524;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#25991;&#26412;&#25366;&#25496;&#27169;&#22411;&#26159;&#36890;&#36807;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#24494;&#35843;&#22823;&#22411;&#28145;&#24230;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26500;&#24314;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#25105;&#20204;&#20351;&#29992;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#30340;&#36731;&#37327;&#32423;&#27169;&#22411;&#26102;&#65292;&#20854;&#20013;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#20445;&#25345;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DisCo&#65292;&#36825;&#26159;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#24494;&#35843;&#30001;&#22823;&#22411;PLM&#29983;&#25104;&#30340;&#23567;&#22411;&#23398;&#29983;&#27169;&#22411;&#38431;&#21015;&#65292;&#35813;&#38431;&#21015;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#21147;&#26159;&#20849;&#20139;&#31934;&#21326;&#30693;&#35782;&#20197;&#20419;&#36827;&#20854;SSL&#26377;&#25928;&#24615;&#30340;&#33976;&#39311;&#23398;&#29983;&#38431;&#21015;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;DisCo&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#21327;&#21516;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#30340;&#33976;&#39311;&#31574;&#30053;&#21644;&#21508;&#31181;&#36755;&#20837;&#22686;&#24378;&#20135;&#29983;&#30340;&#27169;&#22411;&#35270;&#22270;&#21644;&#25968;&#25454;&#35270;&#22270;&#19979;&#20419;&#36827;&#23398;&#29983;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#26469;&#20248;&#21270;&#22810;&#20010;&#23567;&#23398;&#29983;&#27169;&#22411;&#12290;&#25105;&#20204;&#38024;&#23545;&#21322;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#21644;&#25552;&#21462;&#24335;&#24635;&#32467;&#20219;&#21153;&#23545;DisCo&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DisCo&#21487;&#20197;&#20135;&#29983;&#27604;&#21407;&#22987;&#27169;&#22411;&#23567;7.6&#20493;&#21644;&#27604;&#24050;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge is maintaining performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6 times smaller and 4.8 t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12057</link><description>&lt;p&gt;
&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#31934;&#20934;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Accurate Knowledge Distillation with n-best Reranking. (arXiv:2305.12057v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;n-best&#37325;&#25490;&#24207;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#27169;&#22411;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#35757;&#32451;&#20986;&#21442;&#25968;&#26356;&#23569;&#20294;&#31934;&#24230;&#30456;&#24403;&#30340;&#23398;&#29983;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;n-best&#37325;&#25490;&#24207;&#30340;&#24207;&#21015;&#32423;&#21035;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#25945;&#24072;&#27169;&#22411;&#30340;top-1&#20551;&#35774;&#20197;&#21450;top n-best&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#21253;&#25324;&#20844;&#24320;&#21487;&#29992;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20869;&#30340;&#22810;&#31181;&#27169;&#22411;&#65292;&#20026;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#22312;WMT21&#24503;&#33521;&#32763;&#35793;&#20219;&#21153;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#23398;&#29983;&#27169;&#22411;&#22312;&#20855;&#26377;&#20004;&#20010;&#25968;&#37327;&#32423;&#36739;&#23569;&#30340;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#19982;Tran&#31561;&#20154;&#65288;2021&#24180;&#65289;&#30340;&#21253;&#21547;47&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#32763;&#35793;&#27169;&#22411;&#30456;&#24403;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose extending the Sequence-level Knowledge Distillation (Kim and Rush, 2016) with n-best reranking to consider not only the top-1 hypotheses but also the top n-best hypotheses of teacher models. Our approach leverages a diverse set of models, including publicly-available large pretrained models, to provide more accurate pseudo-labels for training student models. We validate our proposal on the WMT21 German-English translation task and demonstrate that our student model achieves comparable accuracy to a large translation model with 4.7 billion parameters from (Tran et al., 2021) while having two orders of magnitude fewer parameters.
&lt;/p&gt;</description></item><item><title>&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.12031</link><description>&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#65306;&#19968;&#31181;&#20855;&#26377;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#19987;&#23478;&#32423;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Clinical Camel: An Open-Source Expert-Level Medical Language Model with Dialogue-Based Knowledge Encoding. (arXiv:2305.12031v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12031
&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#39558;&#39548;&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#30340;&#24320;&#28304;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21307;&#30103;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#25968;&#25454;&#38544;&#31169;&#12289;&#30417;&#31649;&#21512;&#35268;&#24615;&#21644;&#27169;&#22411;&#31283;&#23450;&#24615;&#31561;&#38382;&#39064;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#23545;&#35805;&#30340;&#30693;&#35782;&#32534;&#30721;&#65288;DBKE&#65289;&#12290;DBKE&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#38544;&#24335;&#30693;&#35782;&#24211;&#65292;&#20351;&#20854;&#20855;&#26377;&#26356;&#24378;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#20026;&#21518;&#32493;&#29992;&#20363;&#25552;&#20379;&#20102;&#36719;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Clinical Camel&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#12289;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#30340;&#20250;&#35805;&#27169;&#22411;&#65292;&#26469;&#23637;&#31034;DBKE&#30340;&#26377;&#25928;&#24615;&#12290;Clinical Camel&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#27700;&#24179;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#23427;&#36824;&#20026;&#21307;&#30103;&#24212;&#29992;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#20449;&#36182;&#30340;&#12289;&#24320;&#25918;&#28304;&#20195;&#30721;&#30340;&#26367;&#20195;&#21697;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) present immense potential in the medical field, yet concerns over data privacy, regulatory compliance, and model stability restrict their widespread adoption. Although the distillation of high-performing closed-source LLMs has proven effective for general tasks, their application in healthcare is limited due to reduced domain knowledge and remnants of alignment behavior hindering clinical tasks. To address these challenges, we propose Dialogue-Based Knowledge Encoding (DBKE). DBKE enhances models' implicit knowledge base and primes them for conversational recall, augmenting their conversational capabilities and enabling a soft alignment for subsequent use cases. By transforming dense academic source text into synthetic dialogue, DBKE broadens the model's knowledge base and enables a soft alignment that guides downstream behaviours. We present Clinical Camel, an open-source, healthcare-focused conversational model, to showcase the effectiveness of DBKE. Clin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.12029</link><description>&lt;p&gt;
MultiTurnCleanup&#65306;&#29992;&#20110;&#22810;&#36718;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#28165;&#29702;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MultiTurnCleanup: A Benchmark for Multi-Turn Spoken Conversational Transcript Cleanup. (arXiv:2305.12029v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12029
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MultiTurnCleanup&#20219;&#21153;&#65292;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#24182;&#25552;&#20379;&#20102;&#20004;&#20010;&#21487;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#35821;&#35843;&#19981;&#36830;&#32493;&#26816;&#27979;&#27169;&#22411;&#20391;&#37325;&#20110;&#21333;&#20010;&#35828;&#35805;&#32773;&#30340;&#27599;&#20010;&#35805;&#35821;&#12290;&#28982;&#32780;&#65292;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#35768;&#22810;&#19981;&#36830;&#32493;&#29616;&#35937;&#37117;&#21457;&#29983;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#65292;&#36825;&#24433;&#21709;&#20102;&#20154;&#31867;&#30340;&#21487;&#35835;&#24615;&#21644;&#19979;&#28216; NLP &#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20986;&#21019;&#26032;&#30340;&#8220;MultiTurnCleanup&#8221;&#20219;&#21153;&#65292;&#38024;&#23545;&#21475;&#35821;&#20250;&#35805;&#36716;&#24405;&#20013;&#30340;&#19981;&#36830;&#32493;&#29616;&#35937;&#36827;&#34892;&#25506;&#35752;&#65292;&#24182;&#25910;&#38598;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;MultiTurnCleanup1&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#27880;&#27169;&#24335;&#20197;&#25910;&#38598;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#25968;&#25454;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20004;&#31181;&#24314;&#27169;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;&#65292;&#20316;&#20026;&#26410;&#26469;&#30740;&#31350;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current disfluency detection models focus on individual utterances each from a single speaker. However, numerous discontinuity phenomena in spoken conversational transcripts occur across multiple turns, hampering human readability and the performance of downstream NLP tasks. This study addresses these phenomena by proposing an innovative Multi-Turn Cleanup task for spoken conversational transcripts and collecting a new dataset, MultiTurnCleanup1. We design a data labeling schema to collect the high-quality dataset and provide extensive data analysis. Furthermore, we leverage two modeling approaches for experimental evaluation as benchmarks for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2305.12027</link><description>&lt;p&gt;
&#26497;&#22320;&#40493;&#23376;&#30340;&#21457;&#29616;&#20043;&#26053;&#65306;&#20351;&#29992;&#40493;&#24335;&#36776;&#26512;&#21644;&#26497;&#22352;&#26631;&#30418;&#23884;&#20837;&#22686;&#24378;&#23454;&#20307;&#38142;&#25509;
&lt;/p&gt;
&lt;p&gt;
Polar Ducks and Where to Find Them: Enhancing Entity Linking with Duck Typing and Polar Box Embeddings. (arXiv:2305.12027v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#25226;&#30418;&#23884;&#20837;&#27010;&#24565;&#24341;&#20837;&#21040;&#26497;&#22352;&#26631;&#20013;&#65292;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#65292;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#65292;&#23454;&#29616;&#32858;&#31867;&#12290;&#22312;&#23454;&#20307;&#38142;&#25509;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#23588;&#20854;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#38598;&#26816;&#32034;&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#26159;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#39640;&#25928;&#19988;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#33021;&#19978;&#19981;&#22914;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#23545;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#25935;&#24863;&#12290;&#20026;&#35299;&#20915;&#27492;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102; DUCK &#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#23454;&#20307;&#31867;&#22411;&#30340;&#20808;&#21069;&#30693;&#35782;&#22312;&#23454;&#20307;&#34920;&#31034;&#31354;&#38388;&#20013;&#27880;&#20837;&#32467;&#26500;&#20449;&#24687;&#12290;&#21463;&#32534;&#31243;&#35821;&#35328;&#20013;&#40493;&#24335;&#36776;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#35758;&#26681;&#25454;&#23454;&#20307;&#22312;&#30693;&#35782;&#22270;&#20013;&#30340;&#20851;&#31995;&#23450;&#20041;&#23454;&#20307;&#30340;&#31867;&#22411;&#12290;&#28982;&#21518;&#65292;&#23558;&#30418;&#23884;&#20837;&#30340;&#27010;&#24565;&#31227;&#26893;&#21040;&#29699;&#24418;&#26497;&#22352;&#26631;&#20013;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#20851;&#31995;&#34920;&#31034;&#20026;&#36229;&#29699;&#38754;&#19978;&#30340;&#30418;&#23376;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20855;&#26377;&#30456;&#20284;&#31867;&#22411;&#30340;&#23454;&#20307;&#25918;&#32622;&#22312;&#23545;&#24212;&#20110;&#23427;&#20204;&#20851;&#31995;&#30340;&#30418;&#23376;&#20869;&#26469;&#20248;&#21270;&#27169;&#22411;&#20197;&#32858;&#31867;&#23454;&#20307;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#20934;&#23454;&#20307;&#28040;&#27495;&#22522;&#20934;&#27979;&#35797;&#19978;&#35774;&#32622;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#23427;&#25552;&#39640;&#20102;&#23494;&#38598;&#26816;&#32034;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#29305;&#21035;&#19981;&#36866;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#19981;&#21487;&#34892;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking methods based on dense retrieval are an efficient and widely used solution in large-scale applications, but they fall short of the performance of generative models, as they are sensitive to the structure of the embedding space. In order to address this issue, this paper introduces DUCK, an approach to infusing structural information in the space of entity representations, using prior knowledge of entity types. Inspired by duck typing in programming languages, we propose to define the type of an entity based on the relations that it has with other entities in a knowledge graph. Then, porting the concept of box embeddings to spherical polar coordinates, we propose to represent relations as boxes on the hypersphere. We optimize the model to cluster entities of similar type by placing them inside the boxes corresponding to their relations. Our experiments show that our method sets new state-of-the-art results on standard entity-disambiguation benchmarks, it improves the perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;BOLT&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35843;&#20559;&#32622;&#30452;&#25509;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;logits&#20197;&#23454;&#29616;&#24555;&#36895;&#33021;&#37327;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#24773;&#24863;&#21644;&#20027;&#39064;&#25511;&#21046;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27969;&#30021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.12018</link><description>&lt;p&gt;
BOLT&#65306;&#21487;&#35843;&#20559;&#32622;&#30340;&#24555;&#36895;&#33021;&#37327;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;.
&lt;/p&gt;
&lt;p&gt;
BOLT: Fast Energy-based Controlled Text Generation with Tunable Biases. (arXiv:2305.12018v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;BOLT&#26041;&#27861;&#65292;&#36890;&#36807;&#21487;&#35843;&#20559;&#32622;&#30452;&#25509;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;logits&#20197;&#23454;&#29616;&#24555;&#36895;&#33021;&#37327;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#24773;&#24863;&#21644;&#20027;&#39064;&#25511;&#21046;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26356;&#39640;&#30340;&#25928;&#29575;&#21644;&#27969;&#30021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#22240;&#20854;&#22312;&#24191;&#27867;&#32422;&#26463;&#33539;&#22260;&#20869;&#30340;&#39640;&#36866;&#29992;&#24615;&#32780;&#22312;&#25511;&#21046;&#29983;&#25104;&#25991;&#26412;&#26041;&#38754;&#21464;&#24471;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20174;EBMs&#20013;&#36827;&#34892;&#37319;&#26679;&#26159;&#38750;&#24179;&#20961;&#30340;&#65292;&#22240;&#20026;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#36845;&#20195;&#25165;&#33021;&#25910;&#25947;&#21040;&#21512;&#29702;&#30340;&#25991;&#26412;&#65292;&#36825;&#20250;&#20943;&#24930;&#35299;&#30721;&#36807;&#31243;&#24182;&#20351;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#19981;&#37027;&#20040;&#23454;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BOLT&#65292;&#23427;&#20381;&#36182;&#20110;&#21487;&#35843;&#20559;&#32622;&#20197;&#30452;&#25509;&#35843;&#25972;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;logits&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;BOLT&#20445;&#25345;&#29983;&#25104;&#22120;&#30340;&#33258;&#22238;&#24402;&#24615;&#36136;&#65292;&#20197;&#26029;&#35328;&#23545;&#26631;&#35760;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#21644;&#24635;&#20307;&#27969;&#30021;&#24615;&#30340;&#24378;&#25511;&#21046;&#65292;&#22240;&#27492;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;&#22312;&#20351;&#29992;&#36719;&#32422;&#26463;&#65288;&#20363;&#22914;&#24773;&#24863;&#25511;&#21046;&#65289;&#21644;&#30828;&#32422;&#26463;&#65288;&#20363;&#22914;&#20851;&#38190;&#23383;&#24341;&#23548;&#30340;&#20027;&#39064;&#25511;&#21046;&#65289;&#30340;&#21463;&#25511;&#29983;&#25104;&#20219;&#21153;&#19978;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#26102;&#65292;BOLT&#23637;&#31034;&#20102;&#26174;&#30528;&#30340;&#25928;&#29575;&#21644;&#27969;&#30021;&#24615;&#25913;&#36827;&#12290;&#22312;&#24773;&#24863;&#25511;&#21046;&#26041;&#38754;&#65292;BOLT&#27604;&#31454;&#20105;&#22522;&#32447;&#24555;7&#20493;&#65292;&#24182;&#19988;&#26356;&#27969;&#30021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-based models (EBMs) have gained popularity for controlled text generation due to their high applicability to a wide range of constraints. However, sampling from EBMs is non-trivial, as it often requires a large number of iterations to converge to plausible text, which slows down the decoding process and makes it less practical for real-world applications. In this work, we propose BOLT, which relies on tunable biases to directly adjust the language model's output logits. Unlike prior work, BOLT maintains the generator's autoregressive nature to assert a strong control on token-wise conditional dependencies and overall fluency, and thus converges faster. When compared with state-of-the-arts on controlled generation tasks using both soft constraints (e.g., sentiment control) and hard constraints (e.g., keyword-guided topic control), BOLT demonstrates significantly improved efficiency and fluency. On sentiment control, BOLT is 7x faster than competitive baselines, and more fluent in
&lt;/p&gt;</description></item><item><title>XuanYuan 2.0&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20013;&#25991;&#32842;&#22825;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;BLOOM-176B&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#24494;&#35843;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20013;&#25991;&#37329;&#34701;&#39046;&#22495;&#25552;&#20379;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;</title><link>http://arxiv.org/abs/2305.12002</link><description>&lt;p&gt;
XuanYuan 2.0&#65306;&#19968;&#20010;&#20855;&#26377;&#25968;&#30334;&#20159;&#21442;&#25968;&#30340;&#22823;&#22411;&#20013;&#25991;&#37329;&#34701;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters. (arXiv:2305.12002v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12002
&lt;/p&gt;
&lt;p&gt;
XuanYuan 2.0&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20013;&#25991;&#32842;&#22825;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;BLOOM-176B&#26550;&#26500;&#65292;&#24182;&#25552;&#20986;&#20102;&#28151;&#21512;&#24494;&#35843;&#35757;&#32451;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20013;&#25991;&#37329;&#34701;&#39046;&#22495;&#25552;&#20379;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#19987;&#38376;&#38024;&#23545;&#20013;&#25991;&#35821;&#35328;&#30340;&#24320;&#28304;&#32842;&#22825;&#27169;&#22411;&#22312;&#35268;&#27169;&#19978;&#20173;&#23384;&#22312;&#32570;&#20047;&#65292;&#23588;&#20854;&#26159;&#22312;&#20013;&#25991;&#37329;&#34701;&#39046;&#22495;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; XuanYuan 2.0&#65292;&#23427;&#26159;&#30446;&#21069;&#26368;&#22823;&#30340;&#20013;&#25991;&#32842;&#22825;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102; BLOOM-176B &#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#31216;&#20026;&#28151;&#21512;&#24494;&#35843;&#65292;&#20197;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#36890;&#36807;&#23558;&#36890;&#29992;&#39046;&#22495;&#19982;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#24182;&#38598;&#25104;&#39044;&#35757;&#32451;&#19982;&#24494;&#35843;&#38454;&#27573;&#65292;XuanYuan 2.0 &#33021;&#22815;&#22312;&#20013;&#25991;&#37329;&#34701;&#39046;&#22495;&#25552;&#20379;&#20934;&#30830;&#21644;&#19978;&#19979;&#25991;&#36866;&#24403;&#30340;&#22238;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, pre-trained language models have undergone rapid development with the emergence of large-scale models. However, there is a lack of open-sourced chat models specifically designed for the Chinese language, especially in the field of Chinese finance, at the scale of hundreds of billions. To address this gap, we introduce XuanYuan 2.0, the largest Chinese chat model to date, built upon the BLOOM-176B architecture. Additionally, we propose a novel training method called hybrid-tuning to mitigate catastrophic forgetting. By combining general-domain with domain-specific knowledge and integrating the stages of pre-training and fine-tuning, XuanYuan 2.0 is capable of providing accurate and contextually appropriate responses in the Chinese financial domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35777;&#26126;&#35299;&#37322;&#22312;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#24615;&#33021;&#24433;&#21709;&#19981;&#26174;&#33879;&#65292;&#20294;&#22312;&#25552;&#31034;&#27169;&#22411;&#26102;&#20351;&#29992;&#35299;&#37322;&#21487;&#25552;&#39640;&#27169;&#22411;&#22312;&#26576;&#20123;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12001</link><description>&lt;p&gt;
OPT-R: &#25506;&#31350;&#35299;&#37322;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#19982;&#25552;&#31034;&#25512;&#29702;&#25216;&#33021;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models. (arXiv:2305.12001v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12001
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#35777;&#26126;&#35299;&#37322;&#22312;&#27169;&#22411;&#24494;&#35843;&#36807;&#31243;&#20013;&#23545;&#24615;&#33021;&#24433;&#21709;&#19981;&#26174;&#33879;&#65292;&#20294;&#22312;&#25552;&#31034;&#27169;&#22411;&#26102;&#20351;&#29992;&#35299;&#37322;&#21487;&#25552;&#39640;&#27169;&#22411;&#22312;&#26576;&#20123;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29305;&#21035;&#20851;&#27880;&#20195;&#34920;&#36825;&#31181;&#27169;&#22411;&#30340;Open Pretrained Transformers&#65288;OPT&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#31934;&#24515;&#31574;&#21010;&#30340;&#25512;&#29702;&#35821;&#26009;&#24211;&#19978;&#24494;&#35843;&#20102;&#19977;&#31181;&#19981;&#21516;&#22823;&#23567;&#30340;OPT&#65292;&#24471;&#21040;&#20102;&#20004;&#32452;&#24494;&#35843;&#27169;&#22411;&#65306;&#27809;&#26377;&#35299;&#37322;&#30340;OPT-R&#21644;&#24102;&#26377;&#35299;&#37322;&#30340;OPT-RE&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19977;&#31181;&#25552;&#31034;&#25216;&#26415;&#23545;&#25152;&#26377;&#27169;&#22411;&#22312;&#26469;&#33258;SUPER-NATURAL INSTRUCTIONS&#22522;&#20934;&#27979;&#35797;&#30340;57&#20010;&#22495;&#22806;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#28085;&#30422;26&#20010;&#19981;&#21516;&#30340;&#25512;&#29702;&#25216;&#33021;&#12290;&#36890;&#36807;&#19968;&#20010;&#20840;&#38754;&#30340;27&#20010;&#37197;&#32622;&#21644;6,156&#20010;&#27979;&#35797;&#35780;&#20272;&#30697;&#38453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24494;&#35843;&#12289;&#25552;&#31034;&#21644;&#35268;&#27169;&#30340;&#32500;&#24230;&#65292;&#20197;&#20102;&#35299;&#22312;&#19981;&#21516;&#25512;&#29702;&#25216;&#33021;&#26041;&#38754;&#35299;&#37322;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#27169;&#22411;&#24494;&#35843;&#26102;&#65292;fewshot&#31034;&#20363;&#20013;&#26377;&#27809;&#26377;&#35299;&#37322;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#27809;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#32780;&#22312;&#25552;&#31034;&#27169;&#22411;&#26102;&#20351;&#29992;&#35299;&#37322;&#21487;&#25552;&#39640;&#27169;&#22411;&#22312;&#26576;&#20123;&#25512;&#29702;&#25216;&#33021;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the SUPER-NATURALINSTRUCTIONS benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model's performance when the model is finetuned, while p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20316;&#20026;&#35789;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#35821;&#20041;&#21464;&#21270;&#20998;&#26512;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#30452;&#35266;&#35299;&#37322;&#35789;&#20041;&#30340;&#21382;&#26102;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#23450;&#20041;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#20063;&#20248;&#20110;&#20196;&#29260;&#25110;&#20351;&#29992;&#21477;&#23884;&#20837;&#12290;</title><link>http://arxiv.org/abs/2305.11993</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#20041;&#29983;&#25104;&#23454;&#29616;&#21487;&#35299;&#37322;&#30340;&#35789;&#20041;&#34920;&#31034;&#65306;&#20197;&#35821;&#20041;&#21464;&#21270;&#20998;&#26512;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Interpretable Word Sense Representations via Definition Generation: The Case of Semantic Change Analysis. (arXiv:2305.11993v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11993
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#20316;&#20026;&#35789;&#20041;&#34920;&#31034;&#65292;&#21487;&#20197;&#20351;&#35821;&#20041;&#21464;&#21270;&#20998;&#26512;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#30452;&#35266;&#35299;&#37322;&#35789;&#20041;&#30340;&#21382;&#26102;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#23450;&#20041;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#20063;&#20248;&#20110;&#20196;&#29260;&#25110;&#20351;&#29992;&#21477;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#26469;&#34920;&#31034;&#21487;&#35299;&#37322;&#30340;&#35789;&#21644;&#35789;&#20041;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#35789;&#30340;&#20351;&#29992;&#31034;&#20363;&#38598;&#21512;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#39537;&#21160;&#20351;&#29992;&#32858;&#31867;&#65288;&#21363;&#35789;&#20041;&#65289;&#65292;&#20351;&#29992;&#19987;&#38376;&#30340;Flan-T5&#35821;&#35328;&#27169;&#22411;&#20026;&#27599;&#20010;&#29992;&#27861;&#29983;&#25104;&#23450;&#20041;&#65292;&#24182;&#36873;&#25321;&#20351;&#29992;&#32858;&#31867;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#23450;&#20041;&#20316;&#20026;&#35813;&#35789;&#20041;&#26631;&#31614;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#30340;&#35789;&#20041;&#26631;&#31614;&#20351;&#29616;&#26377;&#30340;&#35821;&#20041;&#21464;&#21270;&#20998;&#26512;&#26041;&#27861;&#26356;&#20855;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#21450;&#22914;&#20309;&#20801;&#35768;&#29992;&#25143; - &#21382;&#21490;&#35821;&#35328;&#23398;&#23478;&#12289;&#35789;&#20856;&#32534;&#32386;&#32773;&#25110;&#31038;&#20250;&#31185;&#23398;&#23478; - &#25506;&#32034;&#24182;&#30452;&#35266;&#22320;&#35299;&#37322;&#35789;&#20041;&#30340;&#21382;&#26102;&#36712;&#36857;&#12290;&#35821;&#20041;&#21464;&#21270;&#20998;&#26512;&#20165;&#26159;&#8220;&#23450;&#20041;&#20316;&#20026;&#34920;&#31034;&#8221;&#30340;&#27169;&#24335;&#30340;&#20247;&#22810;&#21487;&#33021;&#24212;&#29992;&#20043;&#19968;&#12290;&#38500;&#20102;&#20154;&#31867;&#21487;&#35835;&#22806;&#65292;&#19978;&#19979;&#25991;&#21270;&#30340;&#23450;&#20041;&#22312;&#19978;&#19979;&#25991;&#20013;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#19978;&#20063;&#20248;&#20110;&#20196;&#29260;&#25110;&#20351;&#29992;&#21477;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose using automatically generated natural language definitions of contextualised word usages as interpretable word and word sense representations. Given a collection of usage examples for a target word, and the corresponding data-driven usage clusters (i.e., word senses), a definition is generated for each usage with a specialised Flan-T5 language model, and the most prototypical definition in a usage cluster is chosen as the sense label.  We demonstrate how the resulting sense labels can make existing approaches to semantic change analysis more interpretable, and how they can allow users -historical linguists, lexicographers, or social scientists -- to explore and intuitively explain diachronic trajectories of word meaning. Semantic change analysis is only one of many possible applications of the `definitions as representations' paradigm. Beyond being human-readable, contextualised definitions also outperform token or usage sentence embeddings in word-in-context semantic simi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#23567;&#20026;&#20013;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#26816;&#32034;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#38382;&#31572;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#27604;&#21333;&#32431;&#20381;&#36182;&#21442;&#25968;&#25968;&#37327;&#26356;&#37325;&#35201;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;46.4%&#30340;&#27491;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.11991</link><description>&lt;p&gt;
&#22312;&#38646;-shot&#23553;&#38381;&#29983;&#25104;&#24335;&#38382;&#31572;&#20013;&#35780;&#20272;&#22823;&#23567;&#20026;&#20013;&#22411;-&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluation of medium-large Language Models at zero-shot closed book generative question answering. (arXiv:2305.11991v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#23567;&#20026;&#20013;&#22411;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#27809;&#26377;&#22806;&#37096;&#26816;&#32034;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#38382;&#31572;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#27604;&#21333;&#32431;&#20381;&#36182;&#21442;&#25968;&#25968;&#37327;&#26356;&#37325;&#35201;&#65292;&#26368;&#22909;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;46.4%&#30340;&#27491;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24341;&#36215;&#20102;&#37325;&#35201;&#20851;&#27880;&#65292;&#20294;&#8220;&#22823;&#8221;&#36825;&#20010;&#23450;&#20041;&#32570;&#20047;&#28165;&#26224;&#24230;&#12290;&#26412;&#25991;&#20851;&#27880;&#20013;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#65292;&#36825;&#34987;&#23450;&#20041;&#20026;&#20855;&#26377;&#33267;&#23569;60&#20159;&#21442;&#25968;&#20294;&#23569;&#20110;1000&#20159;&#30340;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;MLMs&#22312;&#38646;-shot&#29983;&#25104;&#24335;&#38382;&#31572;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#36825;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#35814;&#32454;&#30340;&#31572;&#26696;&#32780;&#26080;&#38656;&#22806;&#37096;&#25991;&#26723;&#26816;&#32034;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#32473;&#20986;&#20102;&#20154;&#31867;&#35780;&#20272;&#30340;&#32467;&#26524;&#65292;&#32467;&#26524;&#26174;&#31034;&#23558;&#19981;&#21516;MLMs&#30340;&#26368;&#20339;&#31572;&#26696;&#32452;&#21512;&#21487;&#20197;&#23454;&#29616;82.7%&#30340;&#25972;&#20307;&#27491;&#30830;&#29575;&#65292;&#20248;&#20110;ChatGPT&#30340;60.9%&#12290;&#34920;&#29616;&#26368;&#22909;&#30340;MLM&#23454;&#29616;&#20102;46.4%&#65292;&#20854;&#20855;&#26377;70&#20159;&#21442;&#25968;&#65292;&#24378;&#35843;&#20102;&#20351;&#29992;&#36866;&#24403;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#20165;&#20165;&#20381;&#36182;&#20110;&#21442;&#25968;&#25968;&#37327;&#12290;&#26356;&#32454;&#31890;&#24230;&#30340;&#21453;&#39304;&#24212;&#35813;&#34987;&#29992;&#20110;&#36827;&#19968;&#27493;&#25552;&#39640;&#31572;&#26696;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have garnered significant attention, but the definition of "large" lacks clarity. This paper focuses on medium-sized lan-guage models (MLMs), defined as having at least six billion parameters but less than 100 billion. The study evaluates MLMs regarding zero-shot genera-tive question answering, which requires models to provide elaborate answers without external document retrieval. The paper introduces an own test da-taset and presents results from human evaluation. Results show that combin-ing the best answers from different MLMs yielded an overall correct answer rate of 82.7% which is better than the 60.9% of ChatGPT. The best MLM achieved 46.4% and has 7B parameters, which highlights the importance of using appropriate training data for fine-tuning rather than solely relying on the number of parameters. More fine-grained feedback should be used to further improve the quality of answers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31649;&#36947;&#26041;&#27861;&#30340;&#22024;&#26434;&#25968;&#25454;&#38598;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#36866;&#24212;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#22312;&#26410;&#32463;&#24494;&#35843;&#26102;&#33021;&#32988;&#36807;&#20043;&#21069;&#30340;&#29366;&#24577;&#33402;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.11979</link><description>&lt;p&gt;
&#24369;&#30417;&#30563;&#26041;&#27861;&#29992;&#20110;&#23569;&#26679;&#26412;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Weak Supervision Approach for Few-Shot Aspect Based Sentiment. (arXiv:2305.11979v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11979
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#25552;&#39640;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31649;&#36947;&#26041;&#27861;&#30340;&#22024;&#26434;&#25968;&#25454;&#38598;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20197;&#36866;&#24212;&#23569;&#26679;&#26412;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#65292;&#22312;&#26410;&#32463;&#24494;&#35843;&#26102;&#33021;&#32988;&#36807;&#20043;&#21069;&#30340;&#29366;&#24577;&#33402;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#26469;&#25552;&#39640;&#23569;&#26679;&#26412;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31649;&#36947;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#22024;&#26434;&#30340;ABSA&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20197;&#36866;&#24212;ABSA&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;ABSA&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#32467;&#26524;&#27169;&#22411;&#65292;&#22312;&#24494;&#35843;&#20043;&#21069;&#21644;&#20043;&#21518;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#23436;&#25972;&#24494;&#35843;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#22312;&#36739;&#38590;&#30340;&#20219;&#21153;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22330;&#26223;&#20013;&#26174;&#31034;&#20986;&#26174;&#30528;&#30340;&#25913;&#36827;&#65288;15.84&#65285;&#32477;&#23545;F1&#65289;&#12290;&#22312;&#38646;&#26679;&#26412;&#65288;&#21363;&#26080;&#38656;&#24494;&#35843;&#65289;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20197;&#21069;&#30340;&#29366;&#24577;&#33402;&#26415;&#22312;&#26041;&#38754;&#25552;&#21462;&#24773;&#24863;&#20998;&#31867;&#65288;AESC&#65289;&#20219;&#21153;&#19978;&#65292;&#27492;&#22806;&#36824;&#33021;&#22815;&#25191;&#34892;&#26356;&#38590;&#30340;&#26041;&#38754;&#24773;&#24863;&#19977;&#20803;&#32452;&#25552;&#21462;&#65288;ASTE&#65289;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore how weak supervision on abundant unlabeled data can be leveraged to improve few-shot performance in aspect-based sentiment analysis (ABSA) tasks. We propose a pipeline approach to construct a noisy ABSA dataset, and we use it to adapt a pre-trained sequence-to-sequence model to the ABSA tasks. We test the resulting model on three widely used ABSA datasets, before and after fine-tuning. Our proposed method preserves the full fine-tuning performance while showing significant improvements (15.84% absolute F1) in the few-shot learning scenario for the harder tasks. In zero-shot (i.e., without fine-tuning), our method outperforms the previous state of the art on the aspect extraction sentiment classification (AESC) task and is, additionally, capable of performing the harder aspect sentiment triplet extraction (ASTE) task.
&lt;/p&gt;</description></item><item><title>Self-QA&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#37327;&#30340;&#26080;&#30417;&#30563;&#30693;&#35782;&#26469;&#20195;&#26367;&#20256;&#32479;&#30340;&#20154;&#24037;&#25776;&#20889;&#30340;&#25351;&#20196;&#31181;&#23376;&#65292;&#20197;&#29983;&#25104;&#26356;&#22810;&#27491;&#30830;&#19988;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.11952</link><description>&lt;p&gt;
&#33258;&#25105;&#38382;&#31572;&#65306;&#26080;&#30417;&#30563;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Self-QA: Unsupervised Knowledge Guided Language Model Alignment. (arXiv:2305.11952v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11952
&lt;/p&gt;
&lt;p&gt;
Self-QA&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30693;&#35782;&#24341;&#23548;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#37327;&#30340;&#26080;&#30417;&#30563;&#30693;&#35782;&#26469;&#20195;&#26367;&#20256;&#32479;&#30340;&#20154;&#24037;&#25776;&#20889;&#30340;&#25351;&#20196;&#31181;&#23376;&#65292;&#20197;&#29983;&#25104;&#26356;&#22810;&#27491;&#30830;&#19988;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT &#21644; GPT-4 &#31561;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22240;&#20854;&#20986;&#33394;&#30340;&#23545;&#35805;&#21644;&#29983;&#25104;&#33021;&#21147;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20026;&#25351;&#23548;&#27169;&#22411;&#35843;&#25972;&#32780;&#21019;&#24314;&#30417;&#30563;&#24335;&#37197;&#23545;&#30340;&#38382;&#31572;&#25968;&#25454;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#25361;&#25112;&#12290;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#21147;&#29992;&#20110;&#25968;&#25454;&#27880;&#37322;&#24182;&#28041;&#21450;&#25968;&#25454;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20934;&#30830;&#24615;&#21644;&#20854;&#20182;&#30456;&#20851;&#22240;&#32032;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026; Self-QA&#65292;&#23427;&#29992;&#22823;&#37327;&#30340;&#26080;&#30417;&#30563;&#30693;&#35782;&#20195;&#26367;&#20256;&#32479;&#30340;&#20154;&#24037;&#25776;&#20889;&#30340;&#25351;&#20196;&#31181;&#23376;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#26356;&#22810;&#27491;&#30830;&#19988;&#29305;&#23450;&#20110;&#39046;&#22495;&#30340;&#25351;&#20196;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#19981;&#21516;&#39046;&#22495;&#30340;&#26080;&#30417;&#30563;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#23454;&#39564;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models like ChatGPT and GPT-4 have gained attention for their impressive conversational and generative capabilities. However, the creation of supervised paired question-answering data for instruction tuning presents formidable challenges. This endeavor necessitates substantial human effort for data annotation and wrestles with issues concerning data quality, diversity, accuracy, and other related factors. To overcome these obstacles, we introduce an innovative framework named Self-QA, which replaces the traditional practice of human-written instruction seeds with a vast amount of unsupervised knowledge, enabling the model to generate a larger quantity of correct and domain-specific instruction data. The effectiveness of our proposed method is demonstrated through experiments conducted on unsupervised corpora from various domains.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Eye-SpatialNet&#27169;&#24335;&#65292;&#29992;&#20110;&#34920;&#31034;&#30524;&#31185;&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;BERT&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#30524;&#31185;&#23454;&#20307;&#31354;&#38388;&#21644;&#35821;&#22659;&#20449;&#24687;&#30340;&#33258;&#21160;&#21270;&#31934;&#20934;&#26631;&#27880;&#21644;&#25552;&#21462;&#12290;</title><link>http://arxiv.org/abs/2305.11948</link><description>&lt;p&gt;
&#30524;&#31185;&#31508;&#35760;&#20013;&#30340;&#31354;&#38388;&#20449;&#24687;&#25552;&#21462;&#65306;Eye-SpatialNet
&lt;/p&gt;
&lt;p&gt;
Eye-SpatialNet: Spatial Information Extraction from Ophthalmology Notes. (arXiv:2305.11948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Eye-SpatialNet&#27169;&#24335;&#65292;&#29992;&#20110;&#34920;&#31034;&#30524;&#31185;&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#35821;&#35328;&#65292;&#24182;&#20351;&#29992;BERT&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#30524;&#31185;&#23454;&#20307;&#31354;&#38388;&#21644;&#35821;&#22659;&#20449;&#24687;&#30340;&#33258;&#21160;&#21270;&#31934;&#20934;&#26631;&#27880;&#21644;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24102;&#26377;&#35814;&#32454;&#30524;&#31185;&#23454;&#20307;&#31354;&#38388;&#21644;&#35821;&#22659;&#20449;&#24687;&#26631;&#27880;&#30340;600&#20010;&#30524;&#31185;&#31508;&#35760;&#30340;&#35821;&#26009;&#24211;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#20808;&#21069;&#25552;&#20986;&#30340;&#22522;&#20110;&#26694;&#26550;&#35821;&#20041;&#30340;&#31354;&#38388;&#34920;&#31034;&#27169;&#24335;Rad-SpatialNet&#65292;&#20197;&#34920;&#31034;&#30524;&#31185;&#25991;&#26412;&#20013;&#30340;&#31354;&#38388;&#35821;&#35328;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;Eye-SpatialNet&#27169;&#24335;&#12290;&#31354;&#38388;&#23450;&#20301;&#30340;&#23454;&#20307;&#26159;&#21457;&#29616;&#12289;&#31243;&#24207;&#21644;&#33647;&#29289;&#12290;&#20026;&#20102;&#20934;&#30830;&#25429;&#33719;&#25152;&#26377;&#30340;&#31354;&#38388;&#32454;&#33410;&#65292;&#25105;&#20204;&#22312;Eye-SpatialNet&#20013;&#28155;&#21152;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30340;&#20803;&#32032;&#12290;&#26631;&#27880;&#30340;&#35821;&#26009;&#24211;&#21253;&#21547;1715&#20010;&#31354;&#38388;&#35302;&#21457;&#22120;&#12289;7308&#20010;&#21457;&#29616;&#12289;2424&#20010;&#35299;&#21078;&#23398;&#21644;9914&#20010;&#25551;&#36848;&#31526;&#12290;&#20026;&#20102;&#33258;&#21160;&#25552;&#21462;&#31354;&#38388;&#20449;&#24687;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#22522;&#20110; transformer &#35821;&#35328;&#27169;&#22411; BERT &#30340;&#20004;&#36718;&#38382;&#31572;&#26041;&#27861;&#12290;&#32467;&#26524;&#24456;&#26377;&#21069;&#36884;&#65292;F1 &#24471;&#20998;&#20998;&#21035;&#20026; 89.31&#12289;74.86 &#21644; 88.47&#65292;&#29992;&#20110;&#31354;&#38388;&#35302;&#21457;&#22120;&#12289;&#22270;&#24418;&#21644;&#22320;&#38754;&#26694;&#26550;&#20803;&#32032;&#12290;&#36825;&#26159;&#31532;&#19968;&#31687;&#22312;&#30524;&#31185;&#39046;&#22495;&#20013;&#34920;&#31034;&#21644;&#25552;&#21462;&#21508;&#31181;&#20020;&#24202;&#20449;&#24687;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an annotated corpus of 600 ophthalmology notes labeled with detailed spatial and contextual information of ophthalmic entities. We extend our previously proposed frame semantics-based spatial representation schema, Rad-SpatialNet, to represent spatial language in ophthalmology text, resulting in the Eye-SpatialNet schema. The spatially-grounded entities are findings, procedures, and drugs. To accurately capture all spatial details, we add some domain-specific elements in Eye-SpatialNet. The annotated corpus contains 1715 spatial triggers, 7308 findings, 2424 anatomies, and 9914 descriptors. To automatically extract the spatial information, we employ a two-turn question answering approach based on the transformer language model BERT. The results are promising, with F1 scores of 89.31, 74.86, and 88.47 for spatial triggers, Figure, and Ground frame elements, respectively. This is the first work to represent and extract a wide variety of clinical information in ophthalmology.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25506;&#32034;&#29992;&#20110;&#39044;&#27979;&#23545;&#25991;&#26723;&#30340;&#26597;&#35810;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25968;&#25454;&#20013;&#30456;&#20851;&#24615;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.11944</link><description>&lt;p&gt;
&#25506;&#32034;&#29992;&#20110;&#30456;&#20851;&#24615;&#39044;&#27979;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring the Viability of Synthetic Query Generation for Relevance Prediction. (arXiv:2305.11944v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#20013;&#65292;&#21033;&#29992;&#24378;&#22823;&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#25506;&#32034;&#29992;&#20110;&#39044;&#27979;&#23545;&#25991;&#26723;&#30340;&#26597;&#35810;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#26041;&#27861;&#65292;&#24182;&#23581;&#35797;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25968;&#25454;&#20013;&#30456;&#20851;&#24615;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;-&#25991;&#26723;&#30456;&#20851;&#24615;&#39044;&#27979;&#26159;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#65288;&#39044;&#20808;&#35757;&#32451;&#30340;&#65289;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;&#65292;&#36825;&#20123;&#27169;&#22411;&#20351;&#29992;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#31561;&#19987;&#19994;&#39046;&#22495;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#21463;&#21040;&#39046;&#22495;&#20869;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#21294;&#20047;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#21033;&#29992;&#36825;&#20123;&#24378;&#22823;&#30340;&#27169;&#22411;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#23450;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#25506;&#32034;&#20102;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#25110;&#29992;&#20110;&#38382;&#31572;&#21644;&#20108;&#20803;&#65288;&#26159;/&#21542;&#65289;&#30456;&#20851;&#24615;&#39044;&#27979;&#30340;&#26597;&#35810;&#29983;&#25104;&#65288;QGen&#65289;, &#20854;&#20013;&#20363;&#22914;&#65292;QGen&#27169;&#22411;&#32473;&#20986;&#19968;&#20010;&#25991;&#26723;&#65292;&#24182;&#35757;&#32451;&#29983;&#25104;&#19968;&#20010;&#19982;&#35813;&#25991;&#26723;&#30456;&#20851;&#30340;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#24615;&#26377;&#19968;&#20010;&#26356;&#32454;&#31890;&#24230;&#30340;&#27010;&#24565;&#65292;&#32780;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#26159;/&#21542;&#26631;&#31614;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;QGen&#26041;&#27861;&#23454;&#29616;&#32454;&#24494;&#30340;&#30456;&#20851;&#24615;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21512;&#25104;&#26597;&#35810;&#26469;&#39044;&#27979;&#23545;&#25991;&#26723;&#30340;&#26597;&#35810;&#20998;&#32423;&#30456;&#20851;&#24615;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25506;&#32034;&#20351;&#29992;&#26080;&#30417;&#30563;&#32858;&#31867;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#23545;&#25968;&#25454;&#20013;&#30456;&#20851;&#24615;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Query-document relevance prediction is a critical problem in Information Retrieval systems. This problem has increasingly been tackled using (pretrained) transformer-based models which are finetuned using large collections of labeled data. However, in specialized domains such as e-commerce and healthcare, the viability of this approach is limited by the dearth of large in-domain data. To address this paucity, recent methods leverage these powerful models to generate high-quality task and domain-specific synthetic data. Prior work has largely explored synthetic data generation or query generation (QGen) for Question-Answering (QA) and binary (yes/no) relevance prediction, where for instance, the QGen models are given a document, and trained to generate a query relevant to that document. However in many problems, we have a more fine-grained notion of relevance than a simple yes/no label. Thus, in this work, we conduct a detailed study into how QGen approaches can be leveraged for nuanced
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; XTREME-UP&#65292;&#19968;&#20010;&#20197;&#23569;&#37327;&#25968;&#25454;&#35780;&#20272;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340; NLP &#31995;&#32479;&#24615;&#33021;&#30340;&#29992;&#25143;&#20013;&#24515;&#31232;&#32570;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#19987;&#27880;&#20110;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65292;&#32858;&#28966;&#20110;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#35206;&#30422; 88 &#31181;&#35821;&#35328;&#65292;&#24182;&#20171;&#32461;&#20102;&#26032;&#30340; OCR&#12289;&#33258;&#21160;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#38899;&#35793;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#25512;&#36827;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.11938</link><description>&lt;p&gt;
XTREME-UP&#65306;&#38024;&#23545;&#23569;&#26679;&#26412;&#25968;&#25454;&#30340;&#29992;&#25143;&#20013;&#24515;&#31232;&#32570;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#65292;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#19978;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
XTREME-UP: A User-Centric Scarce-Data Benchmark for Under-Represented Languages. (arXiv:2305.11938v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11938
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102; XTREME-UP&#65292;&#19968;&#20010;&#20197;&#23569;&#37327;&#25968;&#25454;&#35780;&#20272;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340; NLP &#31995;&#32479;&#24615;&#33021;&#30340;&#29992;&#25143;&#20013;&#24515;&#31232;&#32570;&#25968;&#25454;&#22522;&#20934;&#27979;&#35797;&#12290;&#23427;&#19987;&#27880;&#20110;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65292;&#32858;&#28966;&#20110;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#35206;&#30422; 88 &#31181;&#35821;&#35328;&#65292;&#24182;&#20171;&#32461;&#20102;&#26032;&#30340; OCR&#12289;&#33258;&#21160;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#38899;&#35793;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#24110;&#21161;&#25512;&#36827;&#20195;&#34920;&#24615;&#19981;&#36275;&#35821;&#35328;&#30340;&#39640;&#24230;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#32570;&#26159;&#39640;&#24230;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#21457;&#23637;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#28982;&#32780;&#23545;&#20110;&#35768;&#22810;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65288;UL&#65289;&#8212;&#8212; NLP &#30740;&#31350;&#22312;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#26041;&#38754;&#29305;&#21035;&#28382;&#21518;&#30340;&#35821;&#35328;&#8212;&#8212;&#27880;&#37322;&#23569;&#37327;&#25968;&#25454;&#26159;&#21487;&#34892;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; XTREME-UP&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#37325;&#28857;&#26159;&#31232;&#32570;&#25968;&#25454;&#26041;&#26696;&#32780;&#19981;&#26159;&#38646;&#26679;&#26412;&#65307;&#20854;&#32858;&#28966;&#20110;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65288;&#21363;&#35768;&#22810;&#39640;&#36164;&#28304;&#35821;&#35328;&#20351;&#29992;&#32773;&#24191;&#27867;&#37319;&#29992;&#30340;&#20219;&#21153;&#65289;&#65307;&#20197;&#21450;&#20854;&#32858;&#28966;&#20110;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#65292;&#22312;&#36825;&#20123;&#35821;&#35328;&#20013;&#65292;&#31232;&#32570;&#25968;&#25454;&#30340;&#24773;&#20917;&#24448;&#24448;&#26368;&#20026;&#29616;&#23454;&#12290;XTREME-UP &#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#22312; 88 &#31181;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#19978;&#65292;&#36328;&#36234;&#20102; 9 &#39033;&#20027;&#35201;&#30340;&#29992;&#25143;&#20013;&#24515;&#25216;&#26415;&#65292;&#21253;&#25324; ASR&#12289;OCR&#12289;MT &#21644;&#20449;&#24687;&#35775;&#38382;&#20219;&#21153;&#12290;&#25105;&#20204;&#20026; OCR&#12289;&#33258;&#21160;&#23436;&#25104;&#12289;&#35821;&#20041;&#20998;&#26512;&#21644;&#38899;&#35793;&#21019;&#24314;&#20102;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20854;&#20182;&#20219;&#21153;&#19978;&#26500;&#24314;&#24182;&#23436;&#21892;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#12290;XTREME-UP &#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272; NLP &#31995;&#32479;&#22312;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#19978;&#31232;&#32570;&#25968;&#25454;&#26041;&#26696;&#30340;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#23427;&#19987;&#27880;&#20110;&#23454;&#38469;&#20215;&#20540;&#23545;&#20110;&#39640;&#36164;&#28304;&#35821;&#35328;&#20351;&#29992;&#32773;&#30340;&#29992;&#25143;&#20013;&#24515;&#20219;&#21153;&#65292;&#24182;&#28085;&#30422;&#20102; 88 &#31181;&#35821;&#35328;&#12290;&#20854;&#30446;&#26631;&#26159;&#24110;&#21161;&#25512;&#36827;&#22810;&#35821;&#35328; NLP &#31995;&#32479;&#23545;&#20110;&#31232;&#32570;&#25968;&#25454;&#36164;&#28304;&#30340;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#35821;&#35328;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data scarcity is a crucial issue for the development of highly multilingual NLP systems. Yet for many under-represented languages (ULs) -- languages for which NLP re-search is particularly far behind in meeting user needs -- it is feasible to annotate small amounts of data. Motivated by this, we propose XTREME-UP, a benchmark defined by: its focus on the scarce-data scenario rather than zero-shot; its focus on user-centric tasks -- tasks with broad adoption by speakers of high-resource languages; and its focus on under-represented languages where this scarce-data scenario tends to be most realistic. XTREME-UP evaluates the capabilities of language models across 88 under-represented languages over 9 key user-centric technologies including ASR, OCR, MT, and information access tasks that are of general utility. We create new datasets for OCR, autocomplete, semantic parsing, and transliteration, and build on and refine existing datasets for other tasks. XTREME-UP provides methodology for e
&lt;/p&gt;</description></item><item><title>MParrotTTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#20026;&#22522;&#30784;&#65307;&#23427;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#30417;&#30563;&#25968;&#25454;&#23601;&#36866;&#24212;&#20110;&#26032;&#35821;&#35328;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24179;&#34892;&#25110;&#21452;&#35821;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#20256;&#36882;&#35828;&#35805;&#20154;&#29305;&#23450;&#30340;&#35821;&#38899;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.11926</link><description>&lt;p&gt;
MParrotTTS&#65306;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#30340;&#22810;&#35821;&#35328;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
MParrotTTS: Multilingual Multi-speaker Text to Speech Synthesis in Low Resource Setting. (arXiv:2305.11926v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11926
&lt;/p&gt;
&lt;p&gt;
MParrotTTS&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#65292;&#20197;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#20026;&#22522;&#30784;&#65307;&#23427;&#21487;&#20197;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#20165;&#20351;&#29992;&#23569;&#37327;&#26377;&#30417;&#30563;&#25968;&#25454;&#23601;&#36866;&#24212;&#20110;&#26032;&#35821;&#35328;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#24179;&#34892;&#25110;&#21452;&#35821;&#35821;&#26009;&#30340;&#24773;&#20917;&#19979;&#20256;&#36882;&#35828;&#35805;&#20154;&#29305;&#23450;&#30340;&#35821;&#38899;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;MParrotTTS&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#35828;&#35805;&#20154;&#25991;&#26412;&#36716;&#35821;&#38899;(TTS)&#21512;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35821;&#38899;&#12290;MParrotTTS&#21463;&#30410;&#20110;&#27169;&#22359;&#21270;&#22521;&#35757;&#33539;&#24335;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#31034;&#65292;&#20197;&#26368;&#23567;&#30340;&#30417;&#30563;&#25968;&#25454;&#36866;&#24212;&#20110;&#26032;&#35821;&#35328;&#65292;&#24182;&#22312;&#35757;&#32451;&#33258;&#30417;&#30563;&#21518;&#39592;&#24178;&#20013;&#23545;&#26410;&#35265;&#36807;&#30340;&#35821;&#35328;&#36827;&#34892;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;MParrotTTS&#19981;&#38656;&#35201;&#20219;&#20309;&#21452;&#35821;&#25110;&#24179;&#34892;&#31034;&#20363;&#30340;&#35757;&#32451;&#65292;&#21487;&#20197;&#22312;&#35821;&#38899;&#20013;&#20256;&#36882;&#35821;&#38899;&#65292;&#21516;&#26102;&#20445;&#30041;&#35828;&#35805;&#20154;&#30340;&#29305;&#23450;&#29305;&#24449;&#65292;&#20363;&#22914;&#20351;&#29992;&#27861;&#35821;&#28436;&#35762;&#32773;&#30340;&#22768;&#38899;&#21644;&#21475;&#38899;&#21512;&#25104;&#27969;&#21033;&#30340;&#21360;&#22320;&#35821;&#35821;&#38899;&#12290;&#25105;&#20204;&#22312;&#20845;&#31181;&#35821;&#35328;&#19978;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#32467;&#26524;&#65292;&#21253;&#25324;&#24182;&#34892;&#21644;&#36328;&#35821;&#35328;&#32508;&#21512;&#30340;&#35821;&#38899;&#33258;&#28982;&#24230;&#21644;&#35828;&#35805;&#20154;&#30456;&#20284;&#24230;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#21482;&#20351;&#29992;&#23569;&#37327;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22810;&#35821;&#35328;TTS&#27169;&#22411;&#21644;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#22312;https://paper2438.github.io/tts&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present MParrotTTS, a unified multilingual, multi-speaker text-to-speech (TTS) synthesis model that can produce high-quality speech. Benefiting from a modularized training paradigm exploiting self-supervised speech representations, MParrotTTS adapts to a new language with minimal supervised data and generalizes to languages not seen while training the self-supervised backbone. Moreover, without training on any bilingual or parallel examples, MParrotTTS can transfer voices across languages while preserving the speaker-specific characteristics, e.g., synthesizing fluent Hindi speech using a French speaker's voice and accent. We present extensive results on six languages in terms of speech naturalness and speaker similarity in parallel and cross-lingual synthesis. The proposed model outperforms the state-of-the-art multilingual TTS models and baselines, using only a small fraction of supervised training data. Speech samples from our model can be found at https://paper2438.github.io/tts
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25226;&#30740;&#31350;&#20013;&#37096;&#20998;&#20869;&#23481;&#20132;&#30001;&#29983;&#25104;&#24335;AI&#23436;&#25104;&#65292;&#20250;&#23548;&#33268;&#20154;&#20204;&#19981;&#20449;&#20219;&#21644;&#36140;&#20302;&#30740;&#31350;&#20154;&#21592;&#21644;&#31185;&#23398;&#36755;&#20986;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#21040;&#29983;&#25104;&#24335;AI&#20351;&#29992;&#30340;&#25253;&#21578;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.11873</link><description>&lt;p&gt;
&#30001;&#29983;&#25104;&#24335;AI&#21512;&#20316;&#21019;&#20316;&#30340;&#30740;&#31350;&#30340;&#35780;&#20215;&#65306;&#23454;&#39564;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Judgments of research co-created by generative AI: experimental evidence. (arXiv:2305.11873v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25226;&#30740;&#31350;&#20013;&#37096;&#20998;&#20869;&#23481;&#20132;&#30001;&#29983;&#25104;&#24335;AI&#23436;&#25104;&#65292;&#20250;&#23548;&#33268;&#20154;&#20204;&#19981;&#20449;&#20219;&#21644;&#36140;&#20302;&#30740;&#31350;&#20154;&#21592;&#21644;&#31185;&#23398;&#36755;&#20986;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#21040;&#29983;&#25104;&#24335;AI&#20351;&#29992;&#30340;&#25253;&#21578;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30340;&#24341;&#20837;&#24341;&#21457;&#20102;&#20851;&#20110;&#29983;&#25104;&#24335;AI&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65307;LLMs&#65289;&#30340;&#20351;&#29992;&#30340;&#20844;&#20849;&#20105;&#35770;&#65292;&#21253;&#25324;&#30740;&#31350;&#20154;&#21592;&#30340;&#20351;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27979;&#35797;&#20102;&#23558;&#30740;&#31350;&#36807;&#31243;&#30340;&#26576;&#20123;&#37096;&#20998;&#22996;&#25176;&#32473;LLMs&#26159;&#21542;&#20250;&#23548;&#33268;&#20154;&#20204;&#19981;&#20449;&#20219;&#21644;&#36140;&#20302;&#30740;&#31350;&#20154;&#21592;&#21644;&#31185;&#23398;&#36755;&#20986;&#12290;&#21442;&#19982;&#32773;&#65288;N=402&#65289;&#32771;&#34385;&#19968;&#20010;&#23558;&#30740;&#31350;&#36807;&#31243;&#30340;&#20803;&#32032;&#22996;&#25176;&#32473;&#21338;&#22763;&#29983;&#25110;LLM&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#24182;&#23545;&#20197;&#19979;&#36827;&#34892;&#35780;&#20998;&#65306;&#65288;1&#65289;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#65292;&#65288;2&#65289;&#30456;&#20449;&#31185;&#23398;&#23478;&#30417;&#30563;&#26410;&#26469;&#39033;&#30446;&#65292;&#20197;&#21450;&#65288;3&#65289;&#36755;&#20986;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#12290;&#20154;&#20204;&#35748;&#20026;&#23558;&#20219;&#21153;&#22996;&#25176;&#32473;LLMs&#27604;&#22996;&#25176;&#32473;&#20154;&#31867;&#19981;&#22826;&#21487;&#25509;&#21463;&#65288;d=-0.78&#65289;&#12290;&#22996;&#25176;&#32473;LLMs&#20063;&#20250;&#38477;&#20302;&#20154;&#20204;&#23545;&#20110;&#30417;&#30563;&#26410;&#26469;&#30740;&#31350;&#39033;&#30446;&#30340;&#20449;&#20219;&#65288;d=-0.80&#65289;&#65292;&#32780;&#19988;&#20154;&#20204;&#35748;&#20026;&#32467;&#26524;&#30340;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#20250;&#26356;&#20302;&#65288;d=-0.85&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#36140;&#20540;&#22914;&#20309;&#36716;&#21270;&#20026;LLMs&#20351;&#29992;&#30340;&#20302;&#20272;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The introduction of ChatGPT has fuelled a public debate on the use of generative AI (large language models; LLMs), including its use by researchers. In the current work, we test whether delegating parts of the research process to LLMs leads people to distrust and devalue researchers and scientific output. Participants (N=402) considered a researcher who delegates elements of the research process to a PhD student or LLM, and rated (1) moral acceptability, (2) trust in the scientist to oversee future projects, and (3) the accuracy and quality of the output. People judged delegating to an LLM as less acceptable than delegating to a human (d = -0.78). Delegation to an LLM also decreased trust to oversee future research projects (d = -0.80), and people thought the results would be less accurate and of lower quality (d = -0.85). We discuss how this devaluation might transfer into the underreporting of generative AI use.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;</title><link>http://arxiv.org/abs/2305.11828</link><description>&lt;p&gt;
LLM&#22312;&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#20013;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews. (arXiv:2305.11828v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;LLM&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#65292;&#25351;&#20986;LLM&#26377;&#21487;&#33021;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#65292;&#20294;&#30001;&#20110;&#21487;&#33021;&#20986;&#29616;&#34394;&#26500;&#25110;&#36951;&#28431;&#20449;&#24687;&#30340;&#24773;&#20917;&#65292;LLM&#30340;&#20351;&#29992;&#38656;&#35201;&#35880;&#24910;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#31995;&#32479;&#32508;&#36848;&#23545;&#20110;&#21046;&#23450;&#20020;&#24202;&#20915;&#31574;&#21644;&#21307;&#30103;&#25919;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20294;&#26159;&#21046;&#20316;&#36825;&#26679;&#30340;&#32508;&#36848;&#24456;&#36153;&#21147;&#19988;&#32791;&#26102;&#12290;&#22240;&#27492;&#65292;&#24456;&#22810;&#38382;&#39064;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#35777;&#25454;&#32508;&#36848;&#65292;&#21363;&#20351;&#36825;&#20123;&#32508;&#36848;&#21487;&#29992;&#65292;&#22312;&#23457;&#26597;&#36807;&#31243;&#20013;&#21487;&#33021;&#24050;&#32463;&#36807;&#26102;&#12290;&#29616;&#22312;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24050;&#32463;&#33021;&#22815;&#29983;&#25104;&#38271;&#31687;&#25991;&#26412;&#65292;&#36825;&#24847;&#21619;&#30528;&#33258;&#21160;&#29983;&#25104;&#25991;&#29486;&#32508;&#36848;&#30340;&#35825;&#20154;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34394;&#26500;&#25110;&#36951;&#28431;&#37325;&#35201;&#20449;&#24687;&#65292;LLM&#26377;&#26102;&#20250;&#20135;&#29983;&#19981;&#20934;&#30830;&#65288;&#29978;&#33267;&#21487;&#33021;&#20855;&#26377;&#35823;&#23548;&#24615;&#65289;&#30340;&#25991;&#26412;&#12290;&#22312;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#65292;&#36825;&#21487;&#33021;&#20351;LLM&#22312;&#26368;&#22909;&#24773;&#20917;&#19979;&#26080;&#27861;&#20351;&#29992;&#65292;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#20250;&#24102;&#26469;&#21361;&#38505;&#12290;&#23545;&#20110;LLM&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;&#30340;&#22823;&#22810;&#25968;&#35752;&#35770;&#19982;&#20855;&#20307;&#24212;&#29992;&#33073;&#31163;&#20102;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#23450;&#24615;&#25551;&#36848;LLM&#22312;&#21327;&#21161;&#21046;&#20316;&#21307;&#23398;&#35777;&#25454;&#32508;&#36848;&#26041;&#38754;&#30340;&#28508;&#22312;&#29992;&#36884;&#21644;&#39118;&#38505;&#12290;&#25105;&#20204;&#23545;16&#20301;&#22269;&#38469;&#19987;&#23478;&#36827;&#34892;&#20102;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical systematic reviews are crucial for informing clinical decision making and healthcare policy. But producing such reviews is onerous and time-consuming. Thus, high-quality evidence synopses are not available for many questions and may be outdated even when they are available. Large language models (LLMs) are now capable of generating long-form texts, suggesting the tantalizing possibility of automatically generating literature reviews on demand. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucinating or omitting important information. In the healthcare context, this may render LLMs unusable at best and dangerous at worst. Most discussion surrounding the benefits and risks of LLMs have been divorced from specific applications. In this work, we seek to qualitatively characterize the potential utility and risks of LLMs for assisting in production of medical evidence reviews. We conducted 16 semi-structured interviews with international experts
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;</title><link>http://arxiv.org/abs/2305.11790</link><description>&lt;p&gt;
&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Prompting with Pseudo-Code Instructions. (arXiv:2305.11790v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#33021;&#21542;&#25552;&#39640;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#35777;&#26126;&#20351;&#29992;&#20266;&#20195;&#30721;&#25552;&#31034;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25552;&#39640;7-16&#20998;&#65292;&#24182;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25552;&#31034;&#24050;&#25104;&#20026;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#37492;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#65292;&#22240;&#27492;&#32771;&#34385;&#20351;&#29992;&#26356;&#23569;&#27495;&#20041;&#30340;&#25552;&#31034;&#26679;&#24335;&#65292;&#22914;&#20266;&#20195;&#30721;&#25552;&#31034;&#65292;&#21487;&#33021;&#20855;&#26377;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25163;&#21160;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26469;&#33258;Super-NaturalInstructions&#25968;&#25454;&#38598;&#30340;132&#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#20266;&#20195;&#30721;&#25552;&#31034;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20998;&#31867;&#12289;QA&#21644;&#29983;&#25104;&#35821;&#35328;&#20219;&#21153;&#12290;&#20351;&#29992;&#36825;&#20123;&#20266;&#20195;&#30721;&#25552;&#31034;&#20197;&#21450;&#23427;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#23545;&#24212;&#29289;&#65292;&#22312;&#20004;&#20010;LLM&#23478;&#26063;-BLOOM&#21644;CodeGen&#19978;&#30740;&#31350;&#23427;&#20204;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#20266;&#20195;&#30721;&#25351;&#20196;&#25552;&#31034;&#20250;&#24102;&#26469;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;F1&#20998;&#25968;&#24179;&#22343;&#22686;&#21152;&#65288;&#32477;&#23545;&#20540;&#65289;7-16&#20998;&#65292;&#30456;&#23545;&#25913;&#21892;12-38%&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompting with natural language instructions has recently emerged as a popular method of harnessing the capabilities of large language models. Given the inherent ambiguity present in natural language, it is intuitive to consider the possible advantages of prompting with less ambiguous prompt styles, such as the use of pseudo-code.  In this paper we explore if prompting via pseudo-code instructions helps improve the performance of pre-trained language models. We manually create a dataset of pseudo-code prompts for 132 different tasks spanning classification, QA and generative language tasks, sourced from the Super-NaturalInstructions dataset. Using these prompts along with their counterparts in natural language, we study their performance on two LLM families - BLOOM and CodeGen. Our experiments show that using pseudo-code instructions leads to better results, with an average increase (absolute) of 7-16 points in F1 scores for classification tasks and an improvement (relative) of 12-38% 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65288;HELMA&#65289;&#65292;&#20854;&#20026;&#26631;&#20934;&#21270;&#21644;&#21487;&#38752;&#30340;&#20272;&#31639;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#34920;&#26126;&#20854;&#23384;&#22312;&#24187;&#35273;&#30340;&#39118;&#38505;&#24182;&#20026;&#37492;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11747</link><description>&lt;p&gt;
HELMA&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. (arXiv:2305.11747v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24187;&#35273;&#35780;&#20272;&#22522;&#20934;&#65288;HELMA&#65289;&#65292;&#20854;&#20026;&#26631;&#20934;&#21270;&#21644;&#21487;&#38752;&#30340;&#20272;&#31639;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;ChatGPT&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#20197;&#34920;&#26126;&#20854;&#23384;&#22312;&#24187;&#35273;&#30340;&#39118;&#38505;&#24182;&#20026;&#37492;&#21035;&#21644;&#20943;&#36731;&#27169;&#22411;&#24187;&#35273;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;ChatGPT&#65292;&#23481;&#26131;&#29983;&#25104;&#24187;&#35273;&#65292;&#21363;&#19982;&#28304;&#20869;&#23481;&#20914;&#31361;&#25110;&#26080;&#27861;&#36890;&#36807;&#20107;&#23454;&#30693;&#35782;&#36827;&#34892;&#39564;&#35777;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#20102;&#35299;LLMs&#20250;&#20135;&#29983;&#21738;&#31181;&#31867;&#22411;&#30340;&#20869;&#23481;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#20135;&#29983;&#24187;&#35273;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hallucination Evaluation for Large Language Models&#65288;HELMA&#65289;&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#22823;&#37327;&#29983;&#25104;&#30340;&#21644;&#20154;&#24037;&#27880;&#37322;&#30340;&#24187;&#35273;&#26679;&#26412;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#22312;&#35782;&#21035;&#21644;&#20943;&#36731;&#24187;&#35273;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#29983;&#25104;&#36825;&#20123;&#26679;&#26412;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;ChatGPT&#30340;&#20004;&#27493;&#26694;&#26550;&#65292;&#21363;&#37319;&#26679;-&#36807;&#28388;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#20004;&#31181;&#19981;&#21516;&#30340;&#37319;&#26679;&#26041;&#27861;&#22522;&#20110;&#25351;&#20196;&#29983;&#25104;&#24187;&#35273;&#26679;&#26412;&#65292;&#28982;&#21518;&#20351;&#29992;&#19968;&#20010;&#31034;&#20363;&#22686;&#24378;&#36807;&#28388;&#26041;&#27861;&#36873;&#25321;&#26368;&#22909;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#32856;&#35831;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#21592;&#26469;&#27880;&#37322;ChatGPT&#21709;&#24212;&#20013;&#30340;&#24187;&#35273;&#12290;&#32463;&#39564;&#35777;&#23454;&#65292;ChatGPT&#26377;&#19968;&#23450;&#30340;&#27010;&#29575;&#20135;&#29983;&#24187;&#35273;&#65292;&#24182;&#23384;&#22312;&#20256;&#25773;&#38169;&#35823;&#20449;&#24687;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;HELMA&#22522;&#20934;&#21487;&#20316;&#20026;&#35782;&#21035;&#21644;&#20943;&#36731;LLMs&#24187;&#35273;&#38382;&#39064;&#30340;&#26631;&#20934;&#21270;&#21487;&#38752;&#35780;&#20272;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, \ie content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation for Large Language Models (HELMA) benchmark, a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing and alleviating hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, \ie sampling-then-filtering. Specifically, we first adopt two different sampling methods to generate hallucinated samples based on instructions, and then use an example-enhanced filtering method to select the best one. Furthermore, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT has some probabilities to generate hallucinations and exist
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.11595</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#30740;&#31350;&#65306;&#36890;&#36807;&#36777;&#35770;&#36827;&#34892;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate. (arXiv:2305.11595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#36777;&#35770;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#23454;&#39564;&#35777;&#26126;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;NLP&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#38646;&#26679;&#26412;&#25110;&#23569;&#37327;&#26679;&#26412;&#36890;&#35782;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#23427;&#20204;&#25317;&#26377;&#24378;&#22823;&#30340;&#24120;&#35782;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#21508;&#31181;&#19981;&#19968;&#33268;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#25506;&#32034;&#20004;&#20010;&#25110;&#22810;&#20010;LLMs&#20043;&#38388;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#36825;&#23545;&#20110;&#19981;&#21516;&#21644;&#31934;&#30830;&#30340;&#20915;&#31574;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20005;&#26684;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#22312;7&#20010;&#24120;&#35782;&#25512;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;LLMs&#19981;&#20165;&#36890;&#36807;&#22949;&#21327;&#21644;&#21453;&#39539;&#21464;&#24471;&#26356;&#20855;&#20869;&#37096;&#19968;&#33268;&#24615;&#65292;&#32780;&#19988;&#36824;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#24120;&#35782;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated impressive zero-shot or few-shot commonsense reasoning performance on various natural language processing (NLP) tasks. However, despite their strong commonsense reasoning abilities, LLMs still exhibit various kinds of inconsistency problems. While previous researches mainly focused on the self-consistency within a single LLM, we propose to explore the inter-consistency issue between two or more LLMs, which is critical for diverse and precise decision-making processes. Since the LLMs possess human-like intelligence after instruction tuning and reinforcement learning with human feedback (RLHF), we design a formal debate framework to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate. Through extensive experiments on 7 commonsense reasoning datasets, LLMs not only become more inter-consistent by compromising and refuting but also achieve higher performance and str
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cross-modality Data Augmentation&#65288;XmDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;XmDA&#22312;&#35813;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.11096</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Cross-modality Data Augmentation for End-to-End Sign Language Translation. (arXiv:2305.11096v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Cross-modality Data Augmentation&#65288;XmDA&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;XmDA&#22312;&#35813;&#39046;&#22495;&#20013;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#26088;&#22312;&#30452;&#25509;&#23558;&#25163;&#35821;&#35270;&#39057;&#36716;&#25442;&#20026;&#21475;&#35821;&#25991;&#26412;&#65292;&#26080;&#38656;&#20013;&#38388;&#34920;&#31034;&#12290;&#21463;&#25163;&#35821;&#35270;&#39057;&#21644;&#25991;&#26412;&#20043;&#38388;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#25361;&#25112;&#65292;&#36825;&#19968;&#20219;&#21153;&#19968;&#30452;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#36328;&#27169;&#24577;&#25968;&#25454;&#22686;&#24378;&#65288;XmDA&#65289;&#8221;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#32763;&#35793;&#27169;&#22411;&#30340;&#20266;&#25163;&#35821;&#21333;&#35789;-&#25991;&#26412;&#23545;&#65292;&#23558;&#24378;&#22823;&#30340;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#32763;&#35793;&#33021;&#21147;&#36716;&#31227;&#21040;&#20102;&#31471;&#21040;&#31471;&#25163;&#35821;&#32763;&#35793;&#65288;&#21363;&#35270;&#39057;&#21040;&#25991;&#26412;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;XmDA&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#21363;&#36328;&#27169;&#24577;&#28151;&#21512;&#21644;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#12290;&#21069;&#32773;&#26126;&#30830;&#22320;&#20419;&#36827;&#25163;&#35821;&#35270;&#39057;&#29305;&#24449;&#21644;&#25163;&#35821;&#21333;&#35789;&#23884;&#20837;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#20197;&#24357;&#21512;&#27169;&#24577;&#24046;&#36317;&#12290;&#21518;&#32773;&#21033;&#29992;&#26469;&#33258;&#25163;&#35821;&#21333;&#35789;&#21040;&#25991;&#26412;&#30340;&#25945;&#24072;&#27169;&#22411;&#30340;&#29983;&#25104;&#30693;&#35782;&#26469;&#25351;&#23548;&#21475;&#35821;&#25991;&#26412;&#29983;&#25104;&#12290;&#22312;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25163;&#35821;&#32763;&#35793;&#25968;&#25454;&#38598;LIBRISIGN&#21644;WLASL&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;XmDA&#22312;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#31867;&#35780;&#20272;&#26041;&#38754;&#22343;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end sign language translation (SLT) aims to convert sign language videos into spoken language texts directly without intermediate representations. It has been a challenging task due to the modality gap between sign videos and texts and the data scarcity of labeled data. To tackle these challenges, we propose a novel Cross-modality Data Augmentation (XmDA) framework to transfer the powerful gloss-to-text translation capabilities to end-to-end sign language translation (i.e. video-to-text) by exploiting pseudo gloss-text pairs from the sign gloss translation model. Specifically, XmDA consists of two key components, namely, cross-modality mix-up and cross-modality knowledge distillation. The former explicitly encourages the alignment between sign video features and gloss embeddings to bridge the modality gap. The latter utilizes the generation knowledge from gloss-to-text teacher models to guide the spoken language text generation. Experimental results on two widely used SLT datase
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#23545;&#35805;&#20013;&#22522;&#20110;&#38544;&#21947;&#30340;&#33457;&#21644;&#26893;&#29289;&#21517;&#31216;&#65292;&#37492;&#21035;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#65292;&#26368;&#22909;&#30340;&#34920;&#29616;&#22120;&#22312;&#20219;&#21153;&#20013;&#25253;&#21578;&#20102;92.2349&#65285;&#30340;F1&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.10833</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#25552;&#21462;&#33457;&#21644;&#26893;&#29289;&#30340;&#38544;&#21947;&#24615;&#21517;&#31216;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Methods for Extracting Metaphorical Names of Flowers and Plants. (arXiv:2305.10833v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35782;&#21035;&#23545;&#35805;&#20013;&#22522;&#20110;&#38544;&#21947;&#30340;&#33457;&#21644;&#26893;&#29289;&#21517;&#31216;&#65292;&#37492;&#21035;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;GPT-3.5&#65292;&#26368;&#22909;&#30340;&#34920;&#29616;&#22120;&#22312;&#20219;&#21153;&#20013;&#25253;&#21578;&#20102;92.2349&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26893;&#29289;&#23398;&#39046;&#22495;&#20805;&#28385;&#20102;&#38544;&#21947;&#24615;&#26415;&#35821;&#65292;&#36825;&#20123;&#26415;&#35821;&#22312;&#25551;&#36848;&#21644;&#35782;&#21035;&#33457;&#21644;&#26893;&#29289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22312;&#23545;&#35805;&#20013;&#35782;&#21035;&#36825;&#20123;&#26415;&#35821;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#22312;&#32763;&#35793;&#36807;&#31243;&#21644;&#35789;&#20856;&#32534;&#32386;&#20219;&#21153;&#20013;&#65292;&#36825;&#24448;&#24448;&#23548;&#33268;&#38169;&#35823;&#30340;&#21457;&#29983;&#12290;&#24403;&#28041;&#21450;&#21040;&#21333;&#35789;&#21644;&#30701;&#35821;&#26102;&#65292;&#22312;&#26426;&#22120;&#32763;&#35793;&#26041;&#38754;&#36825;&#20010;&#36807;&#31243;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#21644;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#25216;&#26415;&#30340;&#26368;&#26032;&#20851;&#27880;&#28857;&#20043;&#19968;&#26159;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#33258;&#21160;&#35782;&#21035;&#23545;&#35805;&#20013;&#22522;&#20110;&#38544;&#21947;&#30340;&#21333;&#35789;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#21313;&#19977;&#31181;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#20197;&#21450;ChatGPT&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#24182;&#19988;&#36890;&#36807;F1&#24471;&#20998;&#35777;&#26126;&#20102;&#37492;&#21035;&#27169;&#22411;&#20248;&#20110;GPT-3.5&#27169;&#22411;&#65292;&#25105;&#20204;&#26368;&#22909;&#30340;&#34920;&#29616;&#22120;&#22312;&#38544;&#21947;&#33457;&#21321;&#21644;&#26893;&#29289;&#21517;&#31216;&#35782;&#21035;&#20219;&#21153;&#20013;&#25253;&#21578;&#20102;92.2349&#65285;&#30340;F1&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
The domain of Botany is rich with metaphorical terms. Those terms play an important role in the description and identification of flowers and plants. However, the identification of such terms in discourse is an arduous task. This leads in some cases to committing errors during translation processes and lexicographic tasks. The process is even more challenging when it comes to machine translation, both in the cases of single-word terms and multi-word terms. One of the recent concerns of Natural Language Processing (NLP) applications and Machine Translation (MT) technologies is the automatic identification of metaphor-based words in discourse through Deep Learning (DL). In this study, we seek to fill this gap through the use of thirteen popular transformer based models, as well as ChatGPT, and we show that discriminative models perform better than GPT-3.5 model with our best performer reporting 92.2349% F1 score in metaphoric flower and plant names identification task.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10435</link><description>&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65306;&#21551;&#29992;&#25216;&#26415;&#12289;&#28508;&#22312;&#24212;&#29992;&#12289;&#26032;&#20852;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions. (arXiv:2305.10435v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10435
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#19988;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20294;&#20173;&#38754;&#20020;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#20195;&#34920;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#19968;&#39033;&#37325;&#22823;&#31361;&#30772;&#65292;&#23558;&#25105;&#20204;&#25512;&#21521;&#24320;&#21457;&#33021;&#22815;&#20687;&#20154;&#31867;&#19968;&#26679;&#29702;&#35299;&#21644;&#20351;&#29992;&#35821;&#35328;&#36827;&#34892;&#20132;&#27969;&#30340;&#26426;&#22120;&#12290;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22522;&#20110;&#21464;&#24418;&#22120;&#26550;&#26500;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#19988;&#33021;&#22815;&#26377;&#25928;&#22320;&#36827;&#34892;&#23545;&#35805;&#65292;&#29983;&#25104;&#30340;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#27169;&#22411;&#22312;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#30028;&#31038;&#21306;&#20013;&#33719;&#24471;&#20102;&#26174;&#33879;&#30340;&#30693;&#21517;&#24230;&#65292;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21450;&#30456;&#20851;&#39046;&#22495;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#21644;&#26377;&#25928;&#30340;&#27169;&#22411;&#20043;&#19968;&#65292;&#36825;&#20419;&#20351;&#36827;&#34892;&#20102;&#26412;&#32508;&#36848;&#12290;&#26412;&#32508;&#36848;&#35814;&#32454;&#20171;&#32461;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#24418;&#22120;&#65292;&#21253;&#25324;&#20854;&#26550;&#26500;&#12289;&#24037;&#20316;&#36807;&#31243;&#12289;&#35757;&#32451;&#36807;&#31243;&#12289;&#21551;&#29992;&#25216;&#26415;&#20197;&#21450;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#28508;&#22312;&#24212;&#29992;&#12290;&#21516;&#26102;&#65292;&#26412;&#32508;&#36848;&#36824;&#35752;&#35770;&#20102;&#35813;&#27169;&#22411;&#38754;&#20020;&#30340;&#26032;&#20852;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#21487;&#33021;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Generative Pre-trained Transformer models represent a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. Generative Pre-trained Transformer models are based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, Generative Pre-trained Transformer models have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the Generative Pre-trained Transformer, including its architecture, working process, training procedures, enabling technologies, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.10400</link><description>&lt;p&gt;
&#20320;&#30475;&#21040;&#30340;&#23601;&#26159;&#20320;&#35835;&#21040;&#30340;? &#25913;&#36827;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;SeeTRUE&#35780;&#20272;&#38598;&#21644;&#20004;&#31181;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#21508;&#31181;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#21462;&#24471;&#20102;&#26174;&#30528;&#25913;&#36827;&#65292;&#22312;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30830;&#23450;&#25991;&#26412;&#21644;&#30456;&#24212;&#30340;&#22270;&#20687;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#26159;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#39033;&#37325;&#35201;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#29983;&#25104;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33258;&#21160;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;SeeTRUE&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#38598;&#65292;&#28085;&#30422;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#20154;&#31867;&#30340;&#21028;&#26029;&#65292;&#21028;&#26029;&#32473;&#23450;&#30340;&#25991;&#26412;-&#22270;&#20687;&#23545;&#26159;&#21542;&#35821;&#20041;&#19978;&#23545;&#40784;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20004;&#31181;&#33258;&#21160;&#30830;&#23450;&#23545;&#40784;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#38382;&#39064;&#29983;&#25104;&#21644;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#27169;&#22411;&#30340;&#31649;&#36947;&#65292;&#31532;&#20108;&#31181;&#26159;&#36890;&#36807;&#24494;&#35843;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#22312;&#21508;&#31181;&#25991;&#26412;-&#22270;&#20687;&#23545;&#40784;&#20219;&#21153;&#20013;&#22343;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#22312;&#28041;&#21450;&#22797;&#26434;&#32452;&#21512;&#25110;&#38750;&#33258;&#28982;&#22270;&#20687;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#26377;&#26174;&#30528;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21363;&#20351;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#19978;&#36824;&#26377;&#24456;&#22823;&#30340;&#25913;&#36827;&#31354;&#38388;&#65292;&#36825;&#28608;&#21169;&#20102;&#26410;&#26469;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate 
&lt;/p&gt;</description></item><item><title>UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10306</link><description>&lt;p&gt;
UniEX&#65306;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25552;&#21462;&#30340;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#30340;&#26377;&#25928;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10306
&lt;/p&gt;
&lt;p&gt;
UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#33539;&#24335;&#65292;&#23427;&#19982;&#20219;&#20309;&#27169;&#24335;&#26684;&#24335;&#20860;&#23481;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026; token-pair &#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#25552;&#21462;&#26694;&#26550; UniEX&#65292;&#23558;&#25152;&#26377;&#25552;&#21462;&#30446;&#26631;&#37117;&#32479;&#19968;&#20998;&#35299;&#20026;&#32852;&#21512;&#36328;&#24230;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#20851;&#32852;&#38382;&#39064;&#12290;UniEX &#21487;&#20197;&#21516;&#26102;&#32534;&#30721;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#23398;&#20064;&#39044;&#23450;&#20041;&#20449;&#24687;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102; traffine &#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#21253;&#25324;&#20219;&#21153;&#12289;&#26631;&#31614;&#21644;&#20869;&#37096; token &#22312;&#20869;&#30340;&#24322;&#26500;&#22240;&#32032;&#38598;&#25104;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#30697;&#38453;&#33719;&#24471;&#25552;&#21462;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniEX &#22312; $14$&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#37117;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#22269;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;M3KE&#65292;&#25910;&#38598;&#20102;20,477&#20010;&#38382;&#39064;&#20197;&#35206;&#30422;&#20013;&#22269;&#25945;&#32946;&#20307;&#31995;&#30340;&#25152;&#26377;&#20027;&#35201;&#23618;&#27425;&#21644;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#27979;&#35797;&#27861;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;GPT-2&#65292;RoBERTa&#65292;ERNIE&#21644;ELECTRA&#23545;&#22810;&#28304;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10263</link><description>&lt;p&gt;
M3KE:&#19968;&#31181;&#38754;&#21521;&#20013;&#22269;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
M3KE: A Massive Multi-Level Multi-Subject Knowledge Evaluation Benchmark for Chinese Large Language Models. (arXiv:2305.10263v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10263
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20013;&#22269;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;M3KE&#65292;&#25910;&#38598;&#20102;20,477&#20010;&#38382;&#39064;&#20197;&#35206;&#30422;&#20013;&#22269;&#25945;&#32946;&#20307;&#31995;&#30340;&#25152;&#26377;&#20027;&#35201;&#23618;&#27425;&#21644;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#27979;&#35797;&#27861;&#26377;&#25928;&#22320;&#35780;&#20272;&#20102;&#22235;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;GPT-2&#65292;RoBERTa&#65292;ERNIE&#21644;ELECTRA&#23545;&#22810;&#28304;&#30693;&#35782;&#30340;&#25972;&#21512;&#21644;&#21033;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#20010;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#65292;&#20363;&#22914;&#36328;&#20219;&#21153;&#36890;&#29992;&#24615;&#65292;&#25351;&#20196;&#36981;&#24490;&#31561;&#12290;&#20840;&#38754;&#35780;&#20272;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;M3KE&#65292;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#32423;&#22810;&#20027;&#39064;&#30693;&#35782;&#35780;&#20272;&#22522;&#20934;&#65292;&#26088;&#22312;&#36890;&#36807;&#27979;&#35797;&#38646;&#21644;&#20960;&#20010;&#31034;&#20363;&#35774;&#32622;&#19979;&#30340;&#22810;&#20219;&#21153;&#20934;&#30830;&#24615;&#26469;&#34913;&#37327;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;71&#20010;&#20219;&#21153;&#30340;20,477&#20010;&#38382;&#39064;&#12290;&#36873;&#25321;&#28085;&#30422;&#20102;&#20013;&#22269;&#25945;&#32946;&#20307;&#31995;&#30340;&#25152;&#26377;&#20027;&#35201;&#23618;&#27425;&#65292;&#20174;&#23567;&#23398;&#21040;&#22823;&#23398;&#65292;&#20197;&#21450;&#24191;&#27867;&#30340;&#23398;&#31185;&#65292;&#21253;&#25324;&#20154;&#25991;&#65292;&#21382;&#21490;&#65292;&#25919;&#27835;&#65292;&#27861;&#24459;&#65292;&#25945;&#32946;&#65292;&#24515;&#29702;&#65292;&#31185;&#23398;&#65292;&#25216;&#26415;&#65292;&#33402;&#26415;&#21644;&#23447;&#25945;&#12290;&#25152;&#26377;&#38382;&#39064;&#37117;&#26159;&#22235;&#20010;&#36873;&#39033;&#30340;&#22810;&#36873;&#39064;&#65292;&#22240;&#27492;&#20445;&#35777;&#20102;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#27969;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20102;&#19968;&#20123;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#20013;&#25991;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324;GPT-2&#65292;RoBERTa&#65292;ERNIE&#21644;ELECTRA&#65292;&#24182;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#32467;&#26524;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;M3KE&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20840;&#38754;&#20102;&#35299;&#23427;&#20204;&#25972;&#21512;&#21644;&#21033;&#29992;&#22810;&#20010;&#30693;&#35782;&#26469;&#28304;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have recently made tremendous progress in a variety of aspects, e.g., cross-task generalization, instruction following. Comprehensively evaluating the capability of large language models in multiple tasks is of great importance. In this paper, we propose M3KE, a Massive Multi-Level Multi-Subject Knowledge Evaluation benchmark, which is developed to measure knowledge acquired by Chinese large language models by testing their multitask accuracy in zero- and few-shot settings. We have collected 20,477 questions from 71 tasks. Our selection covers all major levels of Chinese education system, ranging from the primary school to college, as well as a wide variety of subjects, including humanities, history, politics, law, education, psychology, science, technology, art and religion. All questions are multiple-choice questions with four options, hence guaranteeing a standardized and unified assessment process. We've assessed a number of state-of-the-art open-source Chines
&lt;/p&gt;</description></item><item><title>MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10250</link><description>&lt;p&gt;
MemoryBank: &#29992;&#38271;&#26399;&#35760;&#24518;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10250
&lt;/p&gt;
&lt;p&gt;
MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#19981;&#36275;&#20043;&#22788;&#26159;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#36825;&#22312;&#38656;&#35201;&#25345;&#32493;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20363;&#22914;&#20010;&#20154;&#20276;&#20387;&#31995;&#32479;&#21644;&#24515;&#29702;&#21672;&#35810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoryBank&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;LLM&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#12290;MemoryBank&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#24182;&#26377;&#36873;&#25321;&#22320;&#20445;&#23384;&#35760;&#24518;&#65292;MemoryBank&#37319;&#29992;&#20102;&#21463;Ebbinghaus&#36951;&#24536;&#26354;&#32447;&#29702;&#35770;&#21551;&#21457;&#30340;&#35760;&#24518;&#26356;&#26032;&#26426;&#21046;&#65292;&#36825;&#26679;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#21644;&#35760;&#24518;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#36951;&#24536;&#21644;&#21152;&#24378;&#35760;&#24518;&#65292;&#20174;&#32780;&#20026;LLM&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;SpeechBSD&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#12290;&#20316;&#32773;&#25351;&#20986;&#19978;&#19979;&#25991;&#26159;&#35813;&#20219;&#21153;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#21452;&#35821;&#19978;&#19979;&#25991;&#27604;&#21333;&#35821;&#19978;&#19979;&#25991;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.09210</link><description>&lt;p&gt;
&#21521;&#19981;&#21516;&#35821;&#35328;&#21457;&#35328;&#20154;&#26017;&#26059;&#30340;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Towards Speech Dialogue Translation Mediating Speakers of Different Languages. (arXiv:2305.09210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#21363;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;&#12290;&#20316;&#32773;&#26500;&#24314;&#20102;SpeechBSD&#25968;&#25454;&#38598;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#12290;&#20316;&#32773;&#25351;&#20986;&#19978;&#19979;&#25991;&#26159;&#35813;&#20219;&#21153;&#20013;&#38656;&#35201;&#32771;&#34385;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#26368;&#32456;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35813;&#20219;&#21153;&#20013;&#21452;&#35821;&#19978;&#19979;&#25991;&#27604;&#21333;&#35821;&#19978;&#19979;&#25991;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#39033;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#26159;&#20026;&#19981;&#21516;&#35821;&#35328;&#30340;&#21457;&#35328;&#20154;&#26500;&#24314;&#35821;&#38899;&#23545;&#35805;&#32763;&#35793;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;SpeechBSD&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20102;&#22522;&#20934;&#23454;&#39564;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#19978;&#19979;&#25991;&#26159;&#36825;&#39033;&#20219;&#21153;&#20013;&#38656;&#35201;&#35299;&#20915;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#65292;&#21363;&#21333;&#35821;&#19978;&#19979;&#25991;&#21644;&#21452;&#35821;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#20351;&#29992;Whisper&#21644;mBART&#36827;&#34892;&#32423;&#32852;&#24335;&#35821;&#38899;&#32763;&#35793;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#21452;&#35821;&#19978;&#19979;&#25991;&#22312;&#25105;&#20204;&#30340;&#35774;&#32622;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new task, speech dialogue translation mediating speakers of different languages. We construct the SpeechBSD dataset for the task and conduct baseline experiments. Furthermore, we consider context to be an important aspect that needs to be addressed in this task and propose two ways of utilizing context, namely monolingual context and bilingual context. We conduct cascaded speech translation experiments using Whisper and mBART, and show that bilingual context performs better in our settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Clue And Reasoning Prompting (CARP)&#31639;&#27861;&#65292;&#37319;&#29992;&#36880;&#27493;&#25512;&#29702;&#31574;&#30053;&#20248;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#30340;&#33021;&#21147;&#65307;&#24182;&#36890;&#36807;&#22312;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;$k$NN&#28436;&#31034;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#26377;&#38480;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08377</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text Classification via Large Language Models. (arXiv:2305.08377v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08377
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Clue And Reasoning Prompting (CARP)&#31639;&#27861;&#65292;&#37319;&#29992;&#36880;&#27493;&#25512;&#29702;&#31574;&#30053;&#20248;&#21270;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#30340;&#33021;&#21147;&#65307;&#24182;&#36890;&#36807;&#22312;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;$k$NN&#28436;&#31034;&#25628;&#32034;&#65292;&#35299;&#20915;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#26377;&#38480;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#20173;&#28982;&#26174;&#33879;&#19981;&#21450;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#26159;&#30001;&#20110;(1)&#32570;&#20047;&#22788;&#29702;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#65288;&#20363;&#22914;&#24378;&#35843;&#12289;&#23545;&#27604;&#12289;&#21453;&#35773;&#31561;&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65307; (2)&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#21482;&#20801;&#35768;&#26377;&#38480;&#25968;&#37327;&#30340;&#26631;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Clue And Reasoning Prompting (CARP)&#65292;CARP&#37319;&#29992;&#19968;&#31181;&#36880;&#27493;&#25512;&#29702;&#31574;&#30053;&#65292;&#26088;&#22312;&#24212;&#23545;&#28041;&#21450;&#25991;&#26412;&#20998;&#31867;&#30340;&#22797;&#26434;&#35821;&#35328;&#29616;&#35937;&#65306;CARP&#39318;&#20808;&#25552;&#31034;LLMs&#25214;&#21040;&#34920;&#38754;&#32447;&#32034;&#65288;&#20363;&#22914;&#20851;&#38190;&#35789;&#12289;&#35821;&#27668;&#12289;&#35821;&#20041;&#20851;&#31995;&#12289;&#21442;&#32771;&#31561;&#65289;&#65292;&#28982;&#21518;&#35825;&#23548;&#35786;&#26029;&#24615;&#25512;&#29702;&#36807;&#31243;&#20316;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#35299;&#20915;&#26377;&#38480;&#26631;&#35760;&#30340;&#38382;&#39064;&#65292;CARP&#22312;&#30417;&#30563;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;$k$NN&#28436;&#31034;&#25628;&#32034;&#65292;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#20805;&#20998;&#21033;&#29992;&#20102;LLM&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable success of large-scale Language Models (LLMs) such as GPT-3, their performances still significantly underperform fine-tuned models in the task of text classification. This is due to (1) the lack of reasoning ability in addressing complex linguistic phenomena (e.g., intensification, contrast, irony etc); (2) limited number of tokens allowed in in-context learning.  In this paper, we introduce Clue And Reasoning Prompting (CARP). CARP adopts a progressive reasoning strategy tailored to addressing the complex linguistic phenomena involved in text classification: CARP first prompts LLMs to find superficial clues (e.g., keywords, tones, semantic relations, references, etc), based on which a diagnostic reasoning process is induced for final decisions. To further address the limited-token issue, CARP uses a fine-tuned model on the supervised dataset for $k$NN demonstration search in the in-context learning, allowing the model to take the advantage of both LLM's generali
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.08135</link><description>&lt;p&gt;
&#21306;&#20998;&#20808;&#20110;&#22238;&#31572;&#65306;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#20316;&#20026;&#24120;&#35782;&#38382;&#31572;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering. (arXiv:2305.08135v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08135
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;CPACE&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#24120;&#35782;&#38382;&#31572;&#20013;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#30693;&#35782;&#22686;&#24378;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#30693;&#35782;&#24211;&#33719;&#21462;&#19981;&#21516;&#30340;&#30693;&#35782;&#65292;&#22312;&#26576;&#20123;&#38382;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#21463;&#21040;&#26816;&#32034;&#30693;&#35782;&#30340;&#29305;&#24615;&#38480;&#21046;&#65292;&#23427;&#20204;&#20173;&#28982;&#38590;&#20197;&#21516;&#26102;&#20174;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#21306;&#20998;&#24615;&#26041;&#38754;&#21463;&#30410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPACE&#65292;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#20013;&#24515;&#25552;&#31034;&#30340;&#23545;&#27604;&#35299;&#37322;&#29983;&#25104;&#27169;&#22411;&#65292;&#26088;&#22312;&#23558;&#33719;&#24471;&#30340;&#31526;&#21495;&#30693;&#35782;&#36716;&#21270;&#20026;&#23545;&#27604;&#35299;&#37322;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#21306;&#20998;&#32473;&#23450;&#20505;&#36873;&#32773;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledg
&lt;/p&gt;</description></item><item><title>CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.07922</link><description>&lt;p&gt;
CodeT5+: &#29992;&#20110;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#24320;&#25918;&#20195;&#30721;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CodeT5+: Open Code Large Language Models for Code Understanding and Generation. (arXiv:2305.07922v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07922
&lt;/p&gt;
&lt;p&gt;
CodeT5+&#26159;&#19968;&#32452;&#28789;&#27963;&#32452;&#21512;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#28151;&#21512;&#20102;&#22810;&#31181;&#19981;&#21516;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#65292;&#21487;&#20197;&#36866;&#24212;&#22810;&#31181;&#19981;&#21516;&#30340;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#27604;&#29616;&#26377;&#20195;&#30721;-specific LLMs&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22312;&#22823;&#37327;&#28304;&#20195;&#30721;&#19978;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#20195;&#30721;&#26234;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20195;&#30721;LLM&#22312;&#26550;&#26500;&#21644;&#39044;&#35757;&#32451;&#20219;&#21153;&#26041;&#38754;&#26377;&#20004;&#20010;&#20027;&#35201;&#38480;&#21046;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#29305;&#23450;&#30340;&#26550;&#26500;(&#20165;&#32534;&#30721;&#22120;&#25110;&#20165;&#35299;&#30721;&#22120;)&#25110;&#20381;&#36182;&#20110;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#32479;&#19968;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#32593;&#32476;&#12290;&#21069;&#19968;&#31181;&#33539;&#24335;&#21463;&#21040;&#24212;&#29992;&#28789;&#27963;&#24615;&#30340;&#38480;&#21046;&#65292;&#32780;&#22312;&#21518;&#19968;&#31181;&#33539;&#24335;&#20013;&#65292;&#27169;&#22411;&#34987;&#35270;&#20026;&#25152;&#26377;&#20219;&#21153;&#30340;&#21333;&#19968;&#31995;&#32479;&#65292;&#23548;&#33268;&#22312;&#26576;&#20123;&#20219;&#21153;&#30340;&#23376;&#38598;&#19978;&#24615;&#33021;&#19981;&#20248;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#36890;&#24120;&#37319;&#29992;&#26377;&#38480;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#21487;&#33021;&#19982;&#26576;&#20123;&#19979;&#28216;&#20219;&#21153;&#19981;&#30456;&#20851;&#65292;&#22240;&#27492;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;CodeT5+&#8221;&#65292;&#36825;&#26159;&#19968;&#32452;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;LLM&#26063;&#65292;&#29992;&#20110;&#20195;&#30721;&#65292;&#20854;&#20013;&#32452;&#20214;&#27169;&#22359;&#21487;&#20197;&#28789;&#27963;&#32452;&#21512;&#20197;&#36866;&#24212;&#21508;&#31181;&#19979;&#28216;&#20195;&#30721;&#20219;&#21153;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#26159;&#36890;&#36807;&#25105;&#20204;&#25552;&#20986;&#30340;&#28151;&#21512;&#39044;&#35757;&#32451;&#30446;&#26631;&#23454;&#29616;&#30340;&#65292;&#21253;&#25324;&#20195;&#30721;&#29983;&#25104;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#31243;&#24207;&#21512;&#25104;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#65292;&#35777;&#26126;CodeT5+&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#20195;&#30721;&#29305;&#23450;LLM&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pretrained on vast source code have achieved prominent progress in code intelligence. However, existing code LLMs have two main limitations in terms of architecture and pretraining tasks. First, they often adopt a specific architecture (encoder-only or decoder-only) or rely on a unified encoder-decoder network for different downstream tasks. The former paradigm is limited by inflexibility in applications while in the latter, the model is treated as a single system for all tasks, leading to suboptimal performance on a subset of tasks. Secondly, they often employ a limited set of pretraining objectives which might not be relevant to some downstream tasks and hence result in substantial performance degrade. To address these limitations, we propose ``CodeT5+'', a family of encoder-decoder LLMs for code in which component modules can be flexibly combined to suit a wide range of downstream code tasks. Such flexibility is enabled by our proposed mixture of pretrai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07402</link><description>&lt;p&gt;
&#25628;&#32034;&#24341;&#25806;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#20248;&#21270;&#30693;&#35782;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Knowledge Refinement via Interaction Between Search Engines and Large Language Models. (arXiv:2305.07402v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;InteR&#65292;&#36890;&#36807;&#25628;&#32034;&#24341;&#25806;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26816;&#32034;&#22312;&#20174;&#22823;&#37327;&#25968;&#25454;&#20013;&#23450;&#20301;&#30456;&#20851;&#36164;&#28304;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#20316;&#29992;&#65292;&#20854;&#24212;&#29992;&#24050;&#20174;&#20256;&#32479;&#30693;&#35782;&#24211;&#21457;&#23637;&#33267;&#29616;&#20195;&#25628;&#32034;&#24341;&#25806;&#65288;SEs&#65289;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#19982;&#25628;&#32034;&#31995;&#32479;&#20132;&#20114;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#35813;&#39046;&#22495;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;LLMs&#21644;SEs&#30340;&#20248;&#32570;&#28857;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#29702;&#35299;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#26368;&#26032;&#20449;&#24687;&#26041;&#38754;&#30340;&#21508;&#33258;&#20248;&#21183;&#12290;&#20026;&#20102;&#21033;&#29992;&#20004;&#31181;&#33539;&#20363;&#30340;&#20248;&#21183;&#24182;&#36991;&#20813;&#20854;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InteR&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;SEs&#21644;LLMs&#20043;&#38388;&#30340;&#20132;&#20114;&#20419;&#36827;&#30693;&#35782;&#31934;&#28860;&#30340;&#26032;&#26694;&#26550;&#12290; InteR&#20351;SEs&#33021;&#22815;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#25688;&#35201;&#26469;&#35843;&#25972;&#26597;&#35810;&#65292;&#21516;&#26102;&#20351;LLMs&#33021;&#22815;&#20351;&#29992;SE&#26816;&#32034;&#21040;&#30340;&#25991;&#26723;&#26469;&#22686;&#24378;&#25552;&#31034;&#12290;&#36825;&#31181;&#36845;&#20195;&#30340;&#31934;&#28860;&#36807;&#31243;&#22686;&#24378;&#20102;SEs&#21644;LLMs&#30340;&#36755;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#26356;&#20934;&#30830;&#30340;&#26816;&#32034;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information retrieval (IR) plays a crucial role in locating relevant resources from vast amounts of data, and its applications have evolved from traditional knowledge bases to modern search engines (SEs). The emergence of large language models (LLMs) has further revolutionized the field by enabling users to interact with search systems in natural language. In this paper, we explore the advantages and disadvantages of LLMs and SEs, highlighting their respective strengths in understanding user-issued queries and retrieving up-to-date information. To leverage the benefits of both paradigms while circumventing their limitations, we propose InteR, a novel framework that facilitates knowledge refinement through interaction between SEs and LLMs. InteR allows SEs to refine knowledge in query using LLM-generated summaries and enables LLMs to enhance prompts using SE-retrieved documents. This iterative refinement process augments the inputs of SEs and LLMs, leading to more accurate retrieval. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2305.05352</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Designing Foundation Model based Systems. (arXiv:2305.05352v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05352
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#20307;&#31995;&#65292;&#20998;&#31867;&#21644;&#27604;&#36739;&#20102;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#12290;&#23427;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25512;&#20986;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#36825;&#24341;&#36215;&#20102;&#20154;&#20204;&#23545;&#22522;&#30784;&#27169;&#22411;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#34987;&#24191;&#27867;&#35748;&#20026;&#23558;&#25104;&#20026;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#22522;&#30707;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#65292;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#35774;&#35745;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#22320;&#25506;&#32034;&#12290;&#20154;&#20204;&#23545;&#22312;&#36719;&#20214;&#26550;&#26500;&#20013;&#24341;&#20837;&#22522;&#30784;&#27169;&#22411;&#30340;&#24433;&#21709;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#20998;&#31867;&#27861;&#65292;&#23545;&#22522;&#30784;&#27169;&#22411;&#21644;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#30340;&#29305;&#28857;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#27861;&#21253;&#25324;&#19977;&#20010;&#31867;&#21035;&#65306;&#22522;&#30784;&#27169;&#22411;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12289;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26550;&#26500;&#35774;&#35745;&#21644;&#36127;&#36131;&#20219;&#30340;AI&#35774;&#35745;&#12290;&#36825;&#20010;&#20998;&#31867;&#27861;&#20026;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#31995;&#32479;&#26102;&#20570;&#20986;&#20027;&#35201;&#30340;&#35774;&#35745;&#20915;&#31574;&#25552;&#20379;&#20102;&#20855;&#20307;&#30340;&#25351;&#23548;&#65292;&#24182;&#31361;&#20986;&#20102;&#30456;&#20851;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent release of large language model (LLM) based chatbots, such as ChatGPT, has attracted significant attention on foundations models. It is widely believed that foundation models will serve as the fundamental building blocks for future AI systems. As foundation models are in their early stages, the design of foundation model based systems has not yet been systematically explored. There is little understanding about the impact of introducing foundation models in software architecture. Therefore, in this paper, we propose a taxonomy of foundation model based systems, which classifies and compares the characteristics of foundation models and foundation model based systems. Our taxonomy comprises three categories: foundation model pretraining and fine-tuning, architecture design of foundation model based systems, and responsible-AI-by-design. This taxonomy provides concrete guidance for making major design decisions when designing foundation model based systems and highlights trade-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04160</link><description>&lt;p&gt;
X-LLM: &#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#20026;&#22806;&#35821;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21551;&#21160;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#22522;&#20110;&#39640;&#32423;LLM&#30340;GPT-4&#34920;&#29616;&#20986;&#36229;&#24120;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#24402;&#21151;&#20110;&#19982;&#20197;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;LLM&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;GPT-4&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-LLM&#65292;&#36890;&#36807;&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;&#12289;&#35821;&#38899;&#12289;&#35270;&#39057;&#65289;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGLM&#65289;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;X-LLM&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#20010;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;LLM&#23545;&#40784;&#65292;&#20854;&#20013;&#8220;X&#8221;&#34920;&#31034;&#22810;&#27169;&#24577;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#65292;&#8220;L&#8221;&#34920;&#31034;&#35821;&#35328;&#12290;X-LLM&#30340;&#35757;&#32451;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36716;&#25442;&#22810;&#27169;&#24577;&#20449;&#24687;&#65306;&#31532;&#19968;&#38454;&#27573;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;X2L&#25509;&#21475;&#19982;&#20854;&#21508;&#33258;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#36755;&#20837;&#21040;ChatGLM&#20013;&#12290;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#21010;&#21644;&#35299;&#20915;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#21046;&#23450;&#35745;&#21010;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25353;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#65307;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;-CoT&#12290;</title><link>http://arxiv.org/abs/2305.04091</link><description>&lt;p&gt;
&#35745;&#21010;&#21644;&#35299;&#20915;&#25552;&#31034;&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models. (arXiv:2305.04091v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#21010;&#21644;&#35299;&#20915;&#30340;&#25552;&#31034;&#26041;&#27861;&#26469;&#25913;&#21892;&#38646;&#26679;&#26412;&#24605;&#32771;&#38142;&#25512;&#29702;&#65292;&#35813;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#21046;&#23450;&#35745;&#21010;&#23558;&#20219;&#21153;&#21010;&#20998;&#20026;&#23376;&#20219;&#21153;&#65292;&#24182;&#25353;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#65307;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;-CoT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#22810;&#27493;&#39588;&#25512;&#29702;&#20219;&#21153;&#65292;&#23569;&#26679;&#26412;&#30340;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25552;&#31034;&#21253;&#25324;&#19968;&#20123;&#25163;&#24037;&#21046;&#20316;&#30340;&#36880;&#27493;&#25512;&#29702;&#28436;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#26126;&#30830;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#24182;&#25552;&#39640;&#20854;&#25512;&#29702;&#20219;&#21153;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#28040;&#38500;&#25163;&#21160;&#21171;&#21160;&#65292;&#38646;&#26679;&#26412;&#24605;&#32500;&#38142;&#23558;&#30446;&#26631;&#38382;&#39064;&#38472;&#36848;&#19982;&#8220;&#35753;&#25105;&#20204;&#36880;&#27493;&#24605;&#32771;&#8221;&#36830;&#25509;&#36215;&#26469;&#20316;&#20026;&#36755;&#20837;&#25552;&#31034;LLMs&#12290;&#23613;&#31649;&#38646;&#26679;&#26412;-CoT&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#20173;&#23384;&#22312;&#35745;&#31639;&#38169;&#35823;&#12289;&#32570;&#22833;&#27493;&#39588;&#38169;&#35823;&#21644;&#35821;&#20041;&#35823;&#35299;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#32570;&#22833;&#27493;&#39588;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35745;&#21010;&#21644;&#35299;&#20915;&#65288;PS&#65289;&#25552;&#31034;&#12290;&#23427;&#21253;&#21547;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#65306;&#39318;&#20808;&#65292;&#21046;&#23450;&#35745;&#21010;&#23558;&#25972;&#20010;&#20219;&#21153;&#21010;&#20998;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#28982;&#21518;&#25353;&#29031;&#35745;&#21010;&#25191;&#34892;&#23376;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#35745;&#31639;&#38169;&#35823;&#24182;&#25552;&#39640;&#29983;&#25104;&#25512;&#29702;&#27493;&#39588;&#30340;&#36136;&#37327;&#65292;&#25105;&#20204;&#23558;&#36755;&#20837;&#25552;&#31034;&#25193;&#23637;&#21040;&#21253;&#25324;&#31616;&#21333;&#31639;&#26415;&#35745;&#31639;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;PS&#25552;&#31034;&#22312;&#24605;&#32500;&#38142;&#25512;&#29702;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#32988;&#36807;&#20102;&#38646;&#26679;&#26412;CoT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently been shown to deliver impressive performance in various NLP tasks. To tackle multi-step reasoning tasks, few-shot chain-of-thought (CoT) prompting includes a few manually crafted step-by-step reasoning demonstrations which enable LLMs to explicitly generate reasoning steps and improve their reasoning task accuracy. To eliminate the manual effort, Zero-shot-CoT concatenates the target problem statement with "Let's think step by step" as an input prompt to LLMs. Despite the success of Zero-shot-CoT, it still suffers from three pitfalls: calculation errors, missing-step errors, and semantic misunderstanding errors. To address the missing-step errors, we propose Plan-and-Solve (PS) Prompting. It consists of two components: first, devising a plan to divide the entire task into smaller subtasks, and then carrying out the subtasks according to the plan. To address the calculation errors and improve the quality of generated reasoning steps, we extend 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.03169</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20013;&#30340;&#25935;&#24863;&#25968;&#25454;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sensitive Data Detection with High-Throughput Machine Learning Models in Electrical Health Records. (arXiv:2305.03169v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20197;&#20415;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#35299;&#20915;&#19981;&#21516;&#25968;&#25454;&#38598;PHI&#23383;&#27573;&#24322;&#36136;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#12289;&#31038;&#21306;&#21644;&#30740;&#31350;&#20154;&#21592;&#38656;&#35201;&#20998;&#20139;&#25968;&#25454;&#24182;&#21512;&#20316;&#25913;&#21892;&#20581;&#24247;&#32467;&#26524;&#12289;&#33719;&#21462;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#21644;&#25512;&#36827;&#30740;&#31350;&#12290;1996&#24180;&#12298;&#20581;&#24247;&#20445;&#38505;&#27969;&#36890;&#19982;&#36131;&#20219;&#27861;&#26696;&#12299;(HIPAA)&#26159;&#19968;&#39033;&#32852;&#37030;&#27861;&#24459;&#65292;&#26088;&#22312;&#36890;&#36807;&#21046;&#23450;&#20445;&#25252;&#20581;&#24247;&#20449;&#24687;&#30340;&#35268;&#23450;&#26469;&#20445;&#25252;&#25935;&#24863;&#20581;&#24247;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#20849;&#20139;&#20043;&#21069;&#65292;HIPAA&#27809;&#26377;&#25552;&#20379;&#26377;&#25928;&#30340;&#26816;&#27979;&#25110;&#21024;&#38500;PHI&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#35782;&#21035;&#32467;&#26500;&#21270;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#21464;&#37327;&#65292;&#20174;&#32780;&#20415;&#20110;&#21435;&#36523;&#20221;&#21270;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, there is an increasing need for healthcare providers, communities, and researchers to share data and collaborate to improve health outcomes, generate valuable insights, and advance research. The Health Insurance Portability and Accountability Act of 1996 (HIPAA) is a federal law designed to protect sensitive health information by defining regulations for protected health information (PHI). However, it does not provide efficient tools for detecting or removing PHI before data sharing. One of the challenges in this area of research is the heterogeneous nature of PHI fields in data across different parties. This variability makes rule-based sensitive variable identification systems that work on one database fail on another. To address this issue, our paper explores the use of machine learning algorithms to identify sensitive variables in structured data, thus facilitating the de-identification process. We made a key observation that the distributions of metadata of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#21518;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.02468</link><description>&lt;p&gt;
&#38754;&#21521;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#31995;&#32479;&#20013;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. (arXiv:2305.02468v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#21518;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#30340;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#26469;&#29420;&#31435;&#22320;&#23398;&#20064;&#27599;&#20010;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#26088;&#22312;&#36890;&#36807;&#36319;&#36394;&#23545;&#35805;&#29366;&#24577;&#21644;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#26469;&#25191;&#34892;&#29305;&#23450;&#20219;&#21153;&#65292;&#24110;&#21161;&#29992;&#25143;&#23454;&#29616;&#23450;&#20041;&#30340;&#30446;&#26631;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#39044;&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#23545;&#35805;&#27169;&#22411;&#22312;&#23545;&#35805;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20849;&#20139;&#30456;&#21516;&#30340;&#21442;&#25968;&#20197;&#35757;&#32451;&#23545;&#35805;&#31995;&#32479;&#30340;&#20219;&#21153;(NLU&#65292;DST&#65292;NLG)&#65292;&#22240;&#27492;&#27599;&#20010;&#20219;&#21153;&#30340;&#35843;&#35797;&#37117;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#30456;&#36739;&#20110;PLM&#65292;&#23558;&#22823;&#37327;&#21442;&#25968;&#24494;&#35843;&#26469;&#21019;&#24314;&#38754;&#21521;&#20219;&#21153;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#65292;&#36825;&#20351;&#24471;&#38750;&#19987;&#23478;&#38590;&#20197;&#22788;&#29702;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25171;&#31639;&#35757;&#32451;&#30456;&#23545;&#36731;&#37327;&#32423;&#21644;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20219;&#21153;&#20248;&#21270;&#36866;&#37197;&#22120;&#30340;&#31471;&#21040;&#31471;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#31995;&#32479;&#65292;&#27599;&#20010;&#20219;&#21153;&#29420;&#31435;&#23398;&#20064;&#65292;&#22312;&#39044;&#35757;&#32451;&#32593;&#32476;&#30340;&#22266;&#23450;&#23618;&#20043;&#21518;&#20165;&#28155;&#21152;&#23569;&#37327;&#21442;&#25968;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;&#20102;DST&#21644;NLG&#27169;&#22359;&#30340;&#24615;&#33021;&#65292;&#20811;&#26381;&#20102;&#23398;&#20064;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task-Oriented Dialogue (TOD) systems are designed to carry out specific tasks by tracking dialogue states and generating appropriate responses to help users achieve defined goals. Recently, end-to-end dialogue models pre-trained based on large datasets have shown promising performance in the conversational system. However, they share the same parameters to train tasks of the dialogue system (NLU, DST, NLG), so debugging each task is challenging. Also, they require a lot of effort to fine-tune large parameters to create a task-oriented chatbot, making it difficult for non-experts to handle. Therefore, we intend to train relatively lightweight and fast models compared to PLM. In this paper, we propose an End-to-end TOD system with Task-Optimized Adapters which learn independently per task, adding only small number of parameters after fixed layers of pre-trained network. We also enhance the performance of the DST and NLG modules through reinforcement learning, overcoming the learning curv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2305.01918</link><description>&lt;p&gt;
&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Improving Contrastive Learning of Sentence Embeddings from AI Feedback. (arXiv:2305.01918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21453;&#39304;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#65292;&#24182;&#32467;&#21512;&#20154;&#31867;&#21453;&#39304;&#26469;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21477;&#23376;&#23884;&#20837;&#23398;&#20064;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#33258;&#28982;&#35821;&#35328;&#30340;&#31163;&#25955;&#24615;&#20351;&#24471;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#29983;&#25104;&#30340;&#27491;&#36127;&#26679;&#26412;&#23545;&#30340;&#36136;&#37327;&#38590;&#20197;&#20445;&#35777;&#12290;&#34429;&#28982;&#26377;&#30417;&#30563;&#30340;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#26631;&#31614;&#29983;&#25104;&#26356;&#20934;&#30830;&#30340;&#26679;&#26412;&#23545;&#65292;&#20294;&#20173;&#32570;&#20047;&#32454;&#31890;&#24230;&#30340;&#35757;&#32451;&#20449;&#21495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#21453;&#39304;&#26469;&#25913;&#36827;&#21477;&#23376;&#23884;&#20837;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65288;CLAIF&#65289;&#65292;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;AI&#21453;&#39304;&#26500;&#24314;&#24102;&#26377;&#32454;&#31890;&#24230;&#26679;&#26412;&#30456;&#20284;&#24230;&#20998;&#25968;&#30340;&#26679;&#26412;&#23545;&#65292;&#20197;&#25913;&#36827;&#23545;&#27604;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32467;&#21512;&#20154;&#24037;&#21453;&#39304;&#21644;AI&#21453;&#39304;&#20026;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#21477;&#23376;&#23884;&#20837;&#25552;&#20379;&#26356;&#22909;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \textbf{C}ontrastive \textbf{L}earning of sentence embeddings from \textbf{AI} \textbf{F}eedback \textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves stat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.01879</link><description>&lt;p&gt;
SCOTT: &#33258;&#25105;&#19968;&#33268;&#24615;&#24605;&#36335;&#20018;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01879
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#20986;&#19968;&#23450;&#35268;&#27169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#34920;&#29616;&#20986;&#36890;&#36807;&#19968;&#31995;&#21015;&#36830;&#32493;&#30340;&#24605;&#32771;&#36807;&#31243;&#33719;&#24471;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#30340;&#31361;&#20986;&#33021;&#21147;&#12290;&#34429;&#28982;&#24605;&#36335;&#20018;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20165;&#22312;&#36275;&#22815;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25165;&#33021;&#35266;&#23519;&#21040;&#36825;&#31181;&#25910;&#30410;&#12290;&#26356;&#20196;&#20154;&#25285;&#24551;&#30340;&#26159;&#65292;&#29983;&#25104;&#30340;&#29702;&#30001;&#24456;&#23569;&#20445;&#35777;&#19982;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#20445;&#25345;&#19968;&#33268;&#25110;&#32773;&#24544;&#23454;&#22320;&#35777;&#26126;&#20915;&#31574;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24544;&#23454;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#27604;&#25945;&#24072;&#27169;&#22411;&#22823;&#25968;&#20493;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#19968;&#20010;&#23567;&#30340;&#12289;&#33258;&#25105;&#19968;&#33268;&#30340;&#24605;&#36335;&#20018;&#27169;&#22411;&#12290;&#20026;&#20102;&#24418;&#25104;&#26356;&#22909;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#35299;&#30721;&#24341;&#23548;&#25945;&#24072;&#27169;&#22411;&#20135;&#29983;&#25903;&#25345;&#27491;&#30830;&#31572;&#26696;&#30340;&#29702;&#30001;&#65292;&#36825;&#40723;&#21169;&#25945;&#24072;&#27169;&#22411;&#29983;&#25104;&#30340;token&#21482;&#22312;&#32771;&#34385;&#21040;&#31572;&#26696;&#26102;&#25165;&#26356;&#21152;&#21487;&#20449;&#12290;&#20026;&#20102;&#20445;&#35777;&#24544;&#23454;&#30340;&#33976;&#39311;&#65292;&#25105;&#20204;&#20351;&#29992;&#25945;&#24072;&#29983;&#25104;&#30340;&#29702;&#30001;&#26469;&#23398;&#20064;&#19968;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21453;&#20107;&#23454;&#25512;&#29702;&#30446;&#26631;&#65292;&#21363;&#26681;&#25454;&#20855;&#26377;&#33258;&#25105;&#19968;&#33268;&#24615;&#19988;&#24544;&#23454;&#20110;&#25945;&#24072;&#39044;&#27979;&#30340;&#24605;&#36335;&#20018;&#29702;&#30001;&#39044;&#27979;&#20915;&#31574;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#21644;&#25277;&#35937;&#25688;&#35201;&#22522;&#20934;&#27979;&#35797;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23398;&#29983;&#27169;&#22411;&#20013;&#30340;&#33258;&#25105;&#19968;&#33268;&#24615;&#26377;&#21161;&#20110;&#35777;&#26126;&#20915;&#31574;&#24182;&#25552;&#39640;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LMs) beyond a certain scale, demonstrate the emergent capability of generating free-text rationales for their predictions via chain-of-thought (CoT) prompting. While CoT can yield dramatically improved performance, such gains are only observed for sufficiently large LMs. Even more concerning, there is little guarantee that the generated rationales are consistent with LM's predictions or faithfully justify the decisions. In this work, we propose a faithful knowledge distillation method to learn a small, self-consistent CoT model from a teacher model that is orders of magnitude larger. To form better supervision, we elicit rationales supporting the gold answers from a large LM (teacher) by contrastive decoding, which encourages the teacher to generate tokens that become more plausible only when the answer is considered. To ensure faithful distillation, we use the teacher-generated rationales to learn a student LM with a counterfactual reasoning objective, which pre
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.14732</link><description>&lt;p&gt;
&#22522;&#20110;SearChain&#30340;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#31934;&#30830;&#12289;&#21487;&#20449;&#21644;&#21487;&#36861;&#28335;&#20869;&#23481;&#29983;&#25104;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Search-in-the-Chain: Towards the Accurate, Credible and Traceable Content Generation for Complex Knowledge-intensive Tasks. (arXiv:2304.14732v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SearChain&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;SearChain&#36890;&#36807;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#23454;&#29616;&#65292;&#20854;&#24605;&#36335;&#26159;&#36890;&#36807;&#26500;&#36896;&#26597;&#35810;&#38142;&#65292;&#23558;&#22810;&#36339;&#38382;&#39064;&#36827;&#34892;&#20998;&#35299;&#65292;&#26368;&#32456;&#25351;&#23548;LLM&#29983;&#25104;&#27491;&#30830;&#30340;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#22914;&#20309;&#20351;LLM&#29983;&#25104;&#30340;&#20869;&#23481;&#20934;&#30830;&#21487;&#20449;&#22312;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Search-in-the-Chain&#65288;SearChain&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#25913;&#36827;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#31561;&#20856;&#22411;&#22797;&#26434;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;LLM&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#12289;&#21487;&#20449;&#24230;&#21644;&#21487;&#36861;&#28335;&#24615;&#12290;SearChain&#26159;&#19968;&#20010;&#28145;&#24230;&#38598;&#25104;LLM&#21644;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#30340;&#26694;&#26550;&#12290;&#22312;SearChain&#20013;&#65292;LLM&#26500;&#24314;&#26597;&#35810;&#38142;&#65292;&#20316;&#20026;&#22810;&#36339;&#38382;&#39064;&#30340;&#20998;&#35299;&#12290;&#38142;&#30340;&#27599;&#20010;&#33410;&#28857;&#37117;&#26159;&#30001;IR&#23548;&#21521;&#30340;&#26597;&#35810;-&#31572;&#26696;&#23545;&#65292;&#20197;&#21450;&#30001;LLM&#29983;&#25104;&#30340;&#35813;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;IR&#39564;&#35777;&#12289;&#23436;&#21892;&#21644;&#36319;&#36394;&#38142;&#20013;&#27599;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#65292;&#20197;&#25351;&#23548;LLM&#26500;&#24314;&#27491;&#30830;&#30340;&#26597;&#35810;&#38142;&#65292;&#24182;&#26368;&#32456;&#22238;&#31572;&#22810;&#36339;&#38382;&#39064;&#12290;SearChain&#20351;LLM&#20174;&#19968;&#27425;&#24615;&#31572;&#26696;&#36716;&#21464;&#20026;&#22810;&#27493;&#31572;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#20449;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SearChain&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide application of Large Language Models (LLMs) such as ChatGPT, how to make the contents generated by LLM accurate and credible becomes very important, especially in complex knowledge-intensive tasks. In this paper, we propose a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering, which is a typical complex knowledge-intensive task. SearChain is a framework that deeply integrates LLM and information retrieval (IR). In SearChain, LLM constructs a chain-of-query, which is the decomposition of the multi-hop question. Each node of the chain is a query-answer pair consisting of an IR-oriented query and the answer generated by LLM for this query. IR verifies, completes, and traces the information of each node of the chain, so as to guide LLM to construct the correct chain-of-query, and finally answer the multi-hop question. SearChain makes LLM change from trying to gi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;PMC-LLaMA&#65292;&#20854;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26377;&#26395;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.14454</link><description>&lt;p&gt;
PMC-LLaMA: &#22312;&#21307;&#23398;&#35770;&#25991;&#20013;&#36827;&#34892;LLaMA&#30340;&#36827;&#19968;&#27493;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
PMC-LLaMA: Further Finetuning LLaMA on Medical Papers. (arXiv:2304.14454v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#39046;&#22495;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;PMC-LLaMA&#65292;&#20854;&#36890;&#36807;&#22686;&#21152;&#21307;&#23398;&#30693;&#35782;&#25552;&#39640;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#26377;&#26395;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#39046;&#22495;&#26377;&#26356;&#22909;&#30340;&#24212;&#29992;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26041;&#38754;&#20855;&#26377;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#26085;&#24120;&#23545;&#35805;&#25110;&#38382;&#31572;&#22330;&#26223;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#28982;&#32780;&#65292;&#22312;&#27880;&#37325;&#31934;&#24230;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#21307;&#30103;&#24212;&#29992;&#20013;&#65292;&#23427;&#20204;&#24448;&#24448;&#34920;&#29616;&#20986;&#19981;&#23613;&#20154;&#24847;&#30340;&#24615;&#33021;&#65292;&#21407;&#22240;&#26159;&#32570;&#20047;&#29305;&#23450;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PMC-LLaMA&#65292;&#36825;&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#24635;&#20849;480&#19975;&#31687;&#29983;&#29289;&#21307;&#23398;&#35770;&#25991;&#19978;&#24494;&#35843;&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#65292;&#20197;&#36827;&#19968;&#27493;&#27880;&#20837;&#21307;&#23398;&#30693;&#35782;&#65292;&#22686;&#24378;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#21021;&#27493;&#35780;&#20272;&#65292;&#21253;&#25324;PubMedQA&#12289;MedMCQA&#21644;USMLE&#31561;&#19977;&#20010;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32463;&#36807;&#24494;&#35843;&#21518;&#65292;&#21363;PMC-LLaMA&#65292;&#23545;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29305;&#23450;&#27010;&#24565;&#26377;&#26356;&#22909;&#30340;&#29702;&#35299;&#65292;&#22240;&#27492;&#22312;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#21644;&#20195;&#30721;&#20197;&#21450;&#22312;&#32447;&#28436;&#31034;&#22343;&#21487;&#22312;https://github.com/cstorm125/pmc-llama&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have showcased remarkable capabilities in natural language understanding in various domains. These models can usually behave well on daily dialog, or question answering scenarios, however, in areas that value precision, for example, in medical applications, they often exhibit unsatisfactory performance due to a lack of domain-specific knowledge. In this report, we introduce PMC-LLaMA, an open-source language model that is acquired by fine-tuning an open-source language model on a total of 4.8 million biomedical academic papers for further injecting medical knowledge, enhancing its capability in medical domain. Our preliminary evaluations are conducted on three biomedical QA datasets, including PubMedQA, MedMCQA, and USMLE, showing that the our model after finetuning, i.e., PMC-LLaMA, demonstrates better understanding of biomedical domain-specific concepts, thus achieving high performance on QA benchmarks. The model and codes, along with an online demo, are 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LaMP&#65288;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#30340;&#20010;&#24615;&#21270;&#22522;&#20934;&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19971;&#39033;&#20010;&#24615;&#21270;&#20219;&#21153;&#20197;&#21450;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21033;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#29983;&#25104;&#32467;&#26524;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11406</link><description>&lt;p&gt;
LaMP&#65306;&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#20010;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
LaMP: When Large Language Models Meet Personalization. (arXiv:2304.11406v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24378;&#35843;&#20102;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;LaMP&#65288;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#30340;&#20010;&#24615;&#21270;&#22522;&#20934;&#65289;&#65292;&#24182;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#20219;&#21153;&#65292;&#35774;&#35745;&#20102;&#19971;&#39033;&#20010;&#24615;&#21270;&#20219;&#21153;&#20197;&#21450;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21033;&#29992;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#20854;&#29983;&#25104;&#32467;&#26524;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24378;&#35843;&#22312;&#24403;&#21069;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#39046;&#22495;&#30340;&#20010;&#24615;&#21270;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;LaMP&#22522;&#20934;&#8212;&#8212;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#29983;&#25104;&#20010;&#24615;&#21270;&#36755;&#20986;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20856;&#33539;&#12290;LaMP&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20855;&#26377;&#22810;&#26679;&#21270;&#30340;&#35821;&#35328;&#20219;&#21153;&#21644;&#27599;&#20010;&#29992;&#25143;&#30340;&#22810;&#20010;&#26465;&#30446;&#65292;&#21253;&#25324;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#21644;&#22235;&#20010;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#30340;&#19971;&#20010;&#20010;&#24615;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20174;&#29992;&#25143;&#37197;&#32622;&#25991;&#20214;&#20013;&#26816;&#32034;&#20010;&#24615;&#21270;&#39033;&#30446;&#65292;&#26500;&#24314;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;&#21270;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#22522;&#32447;&#38646;-shot&#21644;&#24494;&#35843;&#27169;&#22411;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#20010;&#20154;&#36164;&#26009;&#25193;&#23637;&#30340;LM&#20248;&#20110;&#19981;&#32771;&#34385;&#20010;&#20154;&#36164;&#26009;&#20449;&#24687;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper highlights the importance of personalization in the current state of natural language understanding and generation and introduces the LaMP benchmark -- a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three classification and four text generation tasks. We also propose a retrieval augmentation approach that retrieves personalized items from user profiles to construct personalized prompts for large language models. Our baseline zero-shot and fine-tuned model results indicate that LMs utilizing profile augmentation outperform their counterparts that do not factor in profile information.
&lt;/p&gt;</description></item><item><title>RRHF&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.05302</link><description>&lt;p&gt;
RRHF: &#26080;&#38656;&#28902;&#24700;&#22320;&#20351;&#29992;&#25490;&#21517;&#21709;&#24212;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
RRHF: Rank Responses to Align Language Models with Human Feedback without tears. (arXiv:2304.05302v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05302
&lt;/p&gt;
&lt;p&gt;
RRHF&#26159;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#21487;&#20197;&#24110;&#21161;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20154;&#31867;&#19982;&#36825;&#20123;&#27169;&#22411;&#38388;&#30340;&#20132;&#20114;&#36136;&#37327;&#12290;&#19982;PPO&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;RRHF&#65292;&#23427;&#36890;&#36807;&#25490;&#24207;&#25439;&#22833;&#23545;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#35780;&#20998;&#65292;&#24182;&#23398;&#20064;&#23558;&#23427;&#20204;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;RRHF&#21487;&#20197;&#39640;&#25928;&#22320;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#27010;&#29575;&#19982;&#20154;&#31867;&#20559;&#22909;&#65292;&#20854;&#25928;&#26524;&#21644;Fine-Tuning&#19968;&#26679;&#31283;&#20581;&#65292;&#32780;&#22312;&#35843;&#25972;&#36807;&#31243;&#20013;&#21482;&#38656;1&#21040;2&#20010;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;RRHF&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;SFT&#21644;&#22870;&#21169;&#27169;&#22411;&#30340;&#25193;&#23637;&#65292;&#19982;PPO&#30456;&#27604;&#22312;&#32534;&#30721;&#21644;&#27169;&#22411;&#25968;&#37327;&#26041;&#38754;&#26356;&#20026;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) facilitates the alignment of large language models with human preferences, significantly enhancing the quality of interactions between humans and these models. InstructGPT implements RLHF through several stages, including Supervised Fine-Tuning (SFT), reward model training, and Proximal Policy Optimization (PPO). PPO, however, is sensitive to hyperparameters and requires a minimum of four models in its standard implementation, which makes it hard to train. In contrast, we propose a novel learning paradigm called RRHF, which scores responses generated by different sampling policies and learns to align them with human preferences through ranking loss. RRHF can efficiently align language model output probabilities with human preferences as robust as fine-tuning and it only needs 1 to 2 models during tuning. In addition, RRHF can be considered an extension of SFT and reward models while being simpler than PPO in terms of coding, model count
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2304.03843</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#35201;&#36880;&#27493;&#24605;&#32771;&#65311;&#25512;&#29702;&#28304;&#20110;&#32463;&#39564;&#30340;&#23616;&#37096;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why think step-by-step? Reasoning emerges from the locality of experience. (arXiv:2304.03843v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03843
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26377;&#30528;&#24378;&#22823;&#32780;&#31070;&#31192;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#32431;&#31929;&#30340;&#24605;&#32500;&#27493;&#39588;&#65292;&#25105;&#20204;&#21487;&#20197;&#25512;&#29702;&#20986;&#25105;&#20204;&#26080;&#27861;&#30452;&#25509;&#24471;&#20986;&#30340;&#25512;&#35770; - &#23613;&#31649;&#25105;&#20204;&#20174;&#19990;&#30028;&#19978;&#27809;&#26377;&#24471;&#21040;&#20219;&#20309;&#39069;&#22806;&#25968;&#25454;&#12290;&#21516;&#26679;&#22320;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#19968;&#27493;&#27493;&#30340;&#25512;&#29702;&#65292;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#29983;&#25104;&#20013;&#38388;&#27493;&#39588;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#23436;&#25104;&#22797;&#26434;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#25512;&#29702;&#26159;&#26377;&#24110;&#21161;&#30340;&#65292;&#27979;&#35797;&#25512;&#29702;&#22312;&#35757;&#32451;&#25968;&#25454;&#30001;&#30456;&#20114;&#24433;&#21709;&#24378;&#28872;&#30340;&#23616;&#37096;&#21464;&#37327;&#38598;&#32676;&#32452;&#25104;&#26102;&#26159;&#21542;&#26377;&#25928;&#12290;&#36825;&#20123;&#35757;&#32451;&#26465;&#20214;&#33021;&#22815;&#23558;&#20934;&#30830;&#30340;&#23616;&#37096;&#25512;&#29702;&#38142;&#25509;&#22312;&#19968;&#36215;&#65292;&#20197;&#20272;&#31639;&#22312;&#35757;&#32451;&#20013;&#27809;&#26377;&#21516;&#26102;&#35266;&#23519;&#21040;&#30340;&#21464;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#23450;&#20041;&#30340;&#32852;&#21512;&#20998;&#24067;&#30340;&#26679;&#21697;&#23545;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#27599;&#20010;&#26679;&#21697;&#21482;&#21253;&#25324;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21464;&#37327;&#12290;&#25105;&#20204;&#27604;&#36739;&#20351;&#29992;&#25512;&#29702;&#29983;&#25104;&#30340;&#21464;&#37327;&#23376;&#38598;&#19982;&#20351;&#29992;&#23436;&#25972;&#38598;&#21512;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans have a powerful and mysterious capacity to reason. By working through a series of purely mental steps, we can make inferences we would not be capable of making directly -- despite that fact that we get no additional data from the world. Similarly, large language models can perform better at complex tasks through chain-of-thought reasoning, where they generate intermediate steps before answering a question. We use language models to investigate the questions of when and why reasoning is helpful, testing the hypothesis that reasoning is effective when training data consisting of local clusters of variables that influence each other strongly. These training conditions enable the chaining of accurate local inferences in order to estimate relationships between variables that were not seen together in training. We train an autoregressive transformer on samples from joint distributions defined by Bayes nets, but only include a subset of all the variables in each sample. We compare lang
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.12528</link><description>&lt;p&gt;
MEGA: &#22810;&#35821;&#35328;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
MEGA: Multilingual Evaluation of Generative AI. (arXiv:2303.12528v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12528
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545; 33 &#31181;&#35821;&#35328;&#20013; 8 &#20010;&#19981;&#21516;&#20219;&#21153;&#30340;&#29983;&#25104; AI &#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#27604;&#36739;&#20102;&#29983;&#25104; LLMs &#21644;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#34920;&#29616;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#35821;&#35328;&#29983;&#25104;&#65289;&#19978;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#24403;&#20170;AI&#31038;&#21306;&#26368;&#37325;&#35201;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;&#65292;&#35780;&#20272;&#29983;&#25104;AI&#26174;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#29983;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#37117;&#38480;&#20110;&#33521;&#35821;&#65292;&#19981;&#28165;&#26970;&#36825;&#20123;&#27169;&#22411;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#20854;&#20182;&#35821;&#35328;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#20840;&#38754;&#35780;&#20272; 8 &#39033;&#19981;&#21516;&#20219;&#21153;&#21644; 33 &#31181;&#35821;&#35328;&#30340;&#29983;&#25104;LLMs MEGA &#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#36824;&#23558;&#29983;&#25104;LLMs&#30340;&#24615;&#33021;&#19982;&#36825;&#20123;&#20219;&#21153;&#19978;&#26368;&#20808;&#36827;&#30340;&#38750;&#33258;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#30830;&#23450;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#22914;&#20309;&#19982;&#19978;&#19968;&#20195;LLMs&#30456;&#27604;&#12290;&#25105;&#20204;&#23545;&#27169;&#22411;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative AI models have impressive performance on many Natural Language Processing tasks such as language understanding, reasoning and language generation. One of the most important questions that is being asked by the AI community today is about the capabilities and limits of these models, and it is clear that evaluating generative AI is very challenging. Most studies on generative Large Language Models (LLMs) are restricted to English and it is unclear how capable these models are at understanding and generating other languages. We present the first comprehensive benchmarking of generative LLMs MEGA, which evaluates models on standard NLP benchmarks, covering 8 diverse tasks and 33 typologically diverse languages. We also compare the performance of generative LLMs to State of the Art (SOTA) non-autoregressive models on these tasks to determine how well generative models perform compared to the previous generation of LLMs. We present a thorough analysis of the performance of model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.12314</link><description>&lt;p&gt;
&#20855;&#26377;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#30340;&#33258;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization. (arXiv:2303.12314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12314
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;SUPMER&#65292;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65292;&#36890;&#36807;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#21644;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#33391;&#22909;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#21644;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#26159;&#19968;&#31181;&#21442;&#25968;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#36719;&#25552;&#31034;&#24182;&#20351;&#20923;&#32467;&#30340;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#25552;&#31034;&#35843;&#25972;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#19968;&#26041;&#38754;&#20005;&#37325;&#20381;&#36182;&#20110;&#33391;&#22909;&#30340;&#36719;&#25552;&#31034;&#21021;&#22987;&#21270;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23427;&#24456;&#23481;&#26131;&#23548;&#33268;&#36807;&#24230;&#25311;&#21512;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#21033;&#29992;&#39044;&#35757;&#32451;&#25110;&#30417;&#30563;&#20803;&#23398;&#20064;&#26469;&#21021;&#22987;&#21270;&#36719;&#25552;&#31034;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#23545;&#26410;&#35265;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#25968;&#25454;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#20197;&#19978;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#30417;&#30563;&#20803;&#25552;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#27867;&#21270;&#65288;SUPMER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#32452;&#33258;&#30417;&#30563;&#38170;&#23450;&#30340;&#20803;&#35757;&#32451;&#20219;&#21153;&#65292;&#20855;&#26377;&#19981;&#21516;&#30340;&#20219;&#21153;&#26684;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#35838;&#31243;&#30340;&#20219;&#21153;&#22686;&#24378;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#20219;&#21153;&#20998;&#24067;&#12290;&#28982;&#21518;&#23558;&#19968;&#31181;&#26032;&#30340;&#20803;&#26799;&#24230;&#27491;&#21017;&#21270;&#26041;&#27861;&#38598;&#25104;&#21040;&#20803;&#25552;&#31034;&#23398;&#20064;&#20013;&#12290;&#23427;&#20803;&#23398;&#20064;&#22312;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#22914;&#20309;&#36716;&#25442;&#21407;&#22987;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily result in overfitting. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they cannot data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with meta-gradient Regularization for few-shot generalization (SUPMER). We first design a set of self-supervised anchor meta-training tasks with different task formats and further enrich the task distribution with curriculum-based task augmentation. Then a novel meta-gradient regularization method is integrated into meta-prompt learning. It meta-learns to transform the raw gradients during few
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.11366</link><description>&lt;p&gt;
Reflexion&#65306;&#20855;&#26377;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Reflexion: an autonomous agent with dynamic memory and self-reflection. (arXiv:2303.11366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986; Reflexion &#26041;&#27861;&#65292;&#32473;&#26234;&#33021;&#20307;&#36171;&#20104;&#20102;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#65292;&#20197;&#22686;&#24378;&#20854;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#30340;&#21457;&#23637;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20869;&#37096;&#27169;&#22411;&#24494;&#35843;&#12289;&#22806;&#37096;&#27169;&#22411;&#24494;&#35843;&#25110;&#22312;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#19978;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#30001;&#20110;&#39640;&#36136;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#25110;&#32570;&#20047;&#33391;&#22909;&#23450;&#20041;&#30340;&#29366;&#24577;&#31354;&#38388;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20195;&#29702;&#27809;&#26377;&#20154;&#31867;&#20915;&#31574;&#36807;&#31243;&#22266;&#26377;&#30340;&#26576;&#20123;&#21697;&#36136;&#65292;&#29305;&#21035;&#26159;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#21453;&#24605;&#65292;&#20154;&#31867;&#21487;&#20197;&#36890;&#36807;&#35797;&#38169;&#36807;&#31243;&#39640;&#25928;&#22320;&#35299;&#20915;&#26032;&#30340;&#38382;&#39064;&#12290;&#22312;&#26368;&#36817;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986; Reflexion&#65292;&#19968;&#31181;&#23558;&#21160;&#24577;&#35760;&#24518;&#21644;&#33258;&#25105;&#21453;&#24605;&#33021;&#21147;&#36171;&#20104;&#26234;&#33021;&#20307;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20854;&#29616;&#26377;&#30340;&#25512;&#29702;&#36712;&#36857;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#34892;&#21160;&#36873;&#25321;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in decision-making large language model (LLM) agents have demonstrated impressive performance across various benchmarks. However, these state-of-the-art approaches typically necessitate internal model fine-tuning, external model fine-tuning, or policy optimization over a defined state space. Implementing these methods can prove challenging due to the scarcity of high-quality training data or the lack of well-defined state space. Moreover, these agents do not possess certain qualities inherent to human decision-making processes, specifically the ability to learn from mistakes. Self-reflection allows humans to efficiently solve novel problems through a process of trial and error. Building on recent research, we propose Reflexion, an approach that endows an agent with dynamic memory and self-reflection capabilities to enhance its existing reasoning trace and task-specific action choice abilities. To achieve full automation, we introduce a straightforward yet effective 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#21644;reranker&#27169;&#22411;&#65292;&#33976;&#39311;&#20026;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2303.00807</link><description>&lt;p&gt;
UDAPDR: &#22522;&#20110;LLM&#25552;&#31034;&#19982;reranker&#33976;&#39311;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. (arXiv:2303.00807v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.00807
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#21644;reranker&#27169;&#22411;&#65292;&#33976;&#39311;&#20026;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#36866;&#29992;&#20110;&#38271;&#23614;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24456;&#22810;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#38656;&#35201;&#22823;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#36890;&#24120;&#19981;&#21487;&#29992;&#65292;&#19988;&#22312;&#24212;&#29992;&#20110;&#30495;&#23454;&#22330;&#26223;&#20013;&#26102;&#21487;&#33021;&#20250;&#22240;&#20026;&#39046;&#22495;&#28418;&#31227;&#32780;&#36805;&#36895;&#22833;&#21435;&#25928;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24265;&#20215;&#29983;&#25104;&#22823;&#37327;&#21512;&#25104;&#26597;&#35810;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#21033;&#29992;&#26114;&#36149;&#30340;LLM&#29983;&#25104;&#23569;&#37327;&#21512;&#25104;&#26597;&#35810;&#65292;&#28982;&#21518;&#20877;&#21033;&#29992;&#25104;&#26412;&#36739;&#20302;&#30340;LLM&#29983;&#25104;&#22823;&#37327;&#30340;&#21512;&#25104;&#26597;&#35810;&#20197;&#24494;&#35843;&#19968;&#32452;reranker&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;reranker&#20250;&#34987;&#33976; distill &#25104;&#19968;&#20010;&#39640;&#25928;&#30340;&#26816;&#32034;&#22120;&#65292;&#29992;&#20110;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26816;&#32034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;&#38271;&#23614;&#39046;&#22495;&#20013;&#30340;&#38646;&#26679;&#26412;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;2K&#20010;&#21512;&#25104;&#26597;&#35810;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#30340;reranking&#26041;&#27861;&#20855;&#26377;&#26356;&#20302;&#30340;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20379;&#23436;&#25972;&#30340;&#31471;&#21040;&#31471;&#26041;&#26696;&#65292;&#21253;&#25324;&#21512;&#25104;&#25968;&#25454;&#38598;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many information retrieval tasks require large labeled datasets for fine-tuning. However, such datasets are often unavailable, and their utility for real-world applications can diminish quickly due to domain shifts. To address this challenge, we develop and motivate a method for using large language models (LLMs) to generate large numbers of synthetic queries cheaply. The method begins by generating a small number of synthetic queries using an expensive LLM. After that, a much less expensive one is used to create large numbers of synthetic queries, which are used to fine-tune a family of reranker models. These rerankers are then distilled into a single efficient retriever for use in the target domain. We show that this technique boosts zero-shot accuracy in long-tail domains, even where only 2K synthetic queries are used for fine-tuning, and that it achieves substantially lower latency than standard reranking methods. We make our end-to-end approach, including our synthetic datasets an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#23545;&#40784;&#12289;&#26080;&#23545;&#40784;&#21644;&#28151;&#21512;&#26041;&#27861;&#22788;&#29702;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#30446;&#26631;&#25805;&#20316;&#28857;&#19978;&#26080;&#23545;&#40784;&#31995;&#32479;&#27604;&#22522;&#20110;&#23545;&#40784;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#32780;&#20351;&#29992;&#23569;&#37327;&#23545;&#40784;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#23545;&#40784;&#25968;&#25454;&#30340;&#28151;&#21512;&#26041;&#27861;&#22312;&#28385;&#36275;&#21021;&#22987;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20986;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.08950</link><description>&lt;p&gt;
&#22788;&#29702;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#23545;&#40784;&#65306;&#22522;&#20110;&#23545;&#40784;&#12289;&#26080;&#23545;&#40784;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Handling the Alignment for Wake Word Detection: A Comparison Between Alignment-Based, Alignment-Free and Hybrid Approaches. (arXiv:2302.08950v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;&#23545;&#40784;&#12289;&#26080;&#23545;&#40784;&#21644;&#28151;&#21512;&#26041;&#27861;&#22788;&#29702;&#21796;&#37266;&#35789;&#26816;&#27979;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#30446;&#26631;&#25805;&#20316;&#28857;&#19978;&#26080;&#23545;&#40784;&#31995;&#32479;&#27604;&#22522;&#20110;&#23545;&#40784;&#30340;&#26041;&#27861;&#26356;&#22909;&#65292;&#32780;&#20351;&#29992;&#23569;&#37327;&#23545;&#40784;&#25968;&#25454;&#21644;&#22823;&#37327;&#26410;&#23545;&#40784;&#25968;&#25454;&#30340;&#28151;&#21512;&#26041;&#27861;&#22312;&#28385;&#36275;&#21021;&#22987;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20986;&#20102;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21796;&#37266;&#35789;&#26816;&#27979;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26234;&#33021;&#23478;&#23621;&#21644;&#20415;&#25658;&#35774;&#22791;&#20013;&#12290;&#23427;&#20026;&#36825;&#20123;&#35774;&#22791;&#25552;&#20379;&#20102;&#22312;&#21484;&#21796;&#26102;&#8220;&#21796;&#37266;&#8221;&#30340;&#33021;&#21147;&#65292;&#24182;&#33410;&#30465;&#20102;&#21151;&#29575;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#23545;&#40784;&#22312;&#24320;&#21457;&#22238;&#31572;&#36890;&#29992;&#30701;&#35821;&#30340;&#21796;&#37266;&#35789;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#31181;&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26159;&#22522;&#20110;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#20351;&#29992;&#36880;&#24103;&#20132;&#21449;&#29109;&#36827;&#34892;&#35757;&#32451;&#12290;&#31532;&#20108;&#31181;&#26159;&#26080;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#27169;&#22411;&#20351;&#29992;CTC&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#31532;&#19977;&#31181;&#26159;&#28151;&#21512;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#27492;&#26041;&#26696;&#20013;&#65292;&#27169;&#22411;&#20351;&#29992;&#23569;&#37327;&#23545;&#40784;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;&#22823;&#37327;&#26410;&#23545;&#40784;&#30340;&#25968;&#25454;&#36827;&#34892;&#35843;&#20248;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#19981;&#21516;&#23545;&#40784;&#21040;&#26410;&#23545;&#40784;&#27604;&#29575;&#23545;&#28151;&#21512;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#30446;&#26631;&#25805;&#20316;&#28857;&#19978;&#65292;&#26080;&#23545;&#40784;&#31995;&#32479;&#27604;&#22522;&#20110;&#23545;&#40784;&#26356;&#22909;&#65292;&#24182;&#19988;&#22312;&#20165;&#20351;&#29992;20&#65285;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35757;&#32451;&#19968;&#20010;&#31526;&#21512;&#25105;&#20204;&#30340;&#21021;&#22987;&#32422;&#26463;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wake word detection exists in most intelligent homes and portable devices. It offers these devices the ability to "wake up" when summoned at a low cost of power and computing. This paper focuses on understanding alignment's role in developing a wake-word system that answers a generic phrase. We discuss three approaches. The first is alignment-based, where the model is trained with frame-wise cross-entropy. The second is alignment-free, where the model is trained with CTC. The third, proposed by us, is a hybrid solution in which the model is trained with a small set of aligned data and then tuned with a sizeable unaligned dataset. We compare the three approaches and evaluate the impact of the different aligned-to-unaligned ratios for hybrid training. Our results show that the alignment-free system performs better than the alignment-based for the target operating point, and with a small fraction of the data (20%), we can train a model that complies with our initial constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2302.08143</link><description>&lt;p&gt;
&#23398;&#20064;&#21021;&#22987;&#21270;&#65306;&#20803;&#23398;&#20064;&#33021;&#21542;&#25552;&#39640;Prompt Tuning&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#65311;
&lt;/p&gt;
&lt;p&gt;
Learning to Initialize: Can Meta Learning Improve Cross-task Generalization in Prompt Tuning?. (arXiv:2302.08143v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning&#65288;MPT&#65289;&#22914;&#20309;&#24110;&#21161;&#25913;&#21892;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#20351;&#29992;&#20803;&#23398;&#20064;&#21487;&#20197;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#20013;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#39564;&#20102;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning (PT)&#26159;&#19968;&#31181;&#21482;&#35843;&#25972;&#27599;&#20010;&#20219;&#21153;&#30340;&#19968;&#20010;&#39069;&#22806;&#26631;&#35760;&#24207;&#21015;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#35757;&#32451;&#23436;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#19981;&#21464;&#65292;&#24050;&#32463;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;PT&#24050;&#32463;&#34987;&#35777;&#26126;&#26497;&#22823;&#22320;&#20381;&#36182;&#20110;&#24456;&#22909;&#30340;Prompt&#23884;&#20837;&#30340;&#21021;&#22987;&#21270;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20803;Prompt Tuning (MPT) &#26469;&#31995;&#32479;&#22320;&#25506;&#32034;&#20803;&#23398;&#20064;&#22914;&#20309;&#24110;&#21161;&#36890;&#36807;&#20174;&#20854;&#20182;&#30456;&#20851;&#20219;&#21153;&#23398;&#20064;&#21021;&#22987;&#21270;Prompt&#23884;&#20837;&#26469;&#25913;&#21892;&#65288;&#22914;&#26524;&#21487;&#20197;&#65289;PT&#20013;&#30340;&#36328;&#20219;&#21153;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#30340;&#23569;&#26679;&#26412;&#20219;&#21153;&#19978;&#20351;&#29992;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#26469;&#32463;&#39564;&#20998;&#26512;&#20102;&#19968;&#31995;&#21015;&#20195;&#34920;&#24615;&#30340;&#20803;&#23398;&#20064;&#31639;&#27861;&#65292;&#20998;&#26512;&#19981;&#21516;&#28304;/&#30446;&#26631;&#20219;&#21153;&#37197;&#32622;&#19979;&#30340;&#21508;&#31181;&#35843;&#25972;&#35774;&#32622;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;MPT&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#29305;&#21035;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#30340;&#25552;&#21319;&#26159;&#26174;&#33879;&#30340;&#12290;&#23545;&#20110;&#20854;&#20182;&#31867;&#22411;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#38382;&#39064;&#22238;&#31572;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;MPT&#21487;&#20197;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20248;&#20110;PT&#65292;
&lt;/p&gt;
&lt;p&gt;
Prompt tuning (PT) which only tunes the embeddings of an additional sequence of tokens per task, keeping the pre-trained language model (PLM) frozen, has shown remarkable performance in few-shot learning. Despite this, PT has been shown to rely heavily on good initialization of the prompt embeddings. In this work, we study meta prompt tuning (MPT) to systematically explore how meta-learning can help improve (if it can) cross-task generalization in PT through learning to initialize the prompt embeddings from other relevant tasks. We empirically analyze a representative set of meta learning algorithms in a wide range of adaptation settings with different source/target task configurations on a large set of few-shot tasks. With extensive experiments and analysis, we demonstrate the effectiveness of MPT. We find the improvement to be significant particularly on classification tasks. For other kinds of tasks such as question answering, we observe that while MPT can outperform PT in most case
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102; UniAdapter&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#39640;&#25928;&#36328;&#27169;&#24577;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29992;&#36739;&#23569;&#30340;&#35843;&#21442;&#25104;&#26412;&#25552;&#21319;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.06605</link><description>&lt;p&gt;
UniAdapter: &#29992;&#20110;&#36328;&#27169;&#24577;&#24314;&#27169;&#30340;&#32479;&#19968;&#21442;&#25968;&#39640;&#25928;&#20256;&#36882;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling. (arXiv:2302.06605v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06605
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102; UniAdapter&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#27169;&#24577;&#36866;&#37197;&#22120;&#65292;&#21487;&#20197;&#39640;&#25928;&#36328;&#27169;&#24577;&#36866;&#24212;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29992;&#36739;&#23569;&#30340;&#35843;&#21442;&#25104;&#26412;&#25552;&#21319;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#38543;&#30528;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#19979;&#28216;&#20219;&#21153;&#30340;&#25968;&#37327;&#22686;&#21152;&#65292;&#30001;&#20110;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#30340;&#22686;&#21152;&#65292;&#26631;&#20934;&#30340;&#23436;&#20840;&#24494;&#35843;&#33539;&#24335;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; UniAdapter&#65292;&#23427;&#23558;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36866;&#37197;&#22120;&#32479;&#19968;&#36215;&#26469;&#65292;&#23545;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#27169;&#24577;&#36866;&#24212;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36866;&#37197;&#22120;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#27169;&#24577;&#21450;&#20854;&#20132;&#20114;&#20013;&#65292;&#24182;&#36890;&#36807;&#37096;&#20998;&#26435;&#37325;&#20849;&#20139;&#26469;&#20943;&#23569;&#21487;&#35843;&#21442;&#25968;&#30340;&#24635;&#25968;&#12290;&#32479;&#19968;&#21644;&#30693;&#35782;&#20849;&#20139;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#36328;&#27169;&#24577;&#34920;&#31034;&#65292;&#21487;&#20197;&#24110;&#21161;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;1.0&#65285;-2.0&#65285;&#21487;&#35843;&#21442;&#25968;&#12290;&#22312;&#21253;&#25324;&#35270;&#39057;-&#25991;&#26412;&#26816;&#32034;&#12289;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#12289;VideoQA&#21644;VQA&#22312;&#20869;&#30340;6&#20010;&#36328;&#27169;&#24577;&#19979;&#28216;&#22522;&#20934;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;UniAdapter&#37117;&#33021;&#24102;&#26469;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-trained models have shown promising transferability to various downstream tasks. As the size of these foundation models and the number of downstream tasks grow, the standard full fine-tuning paradigm becomes unsustainable due to heavy computational and storage costs. This paper proposes UniAdapter, which unifies unimodal and multimodal adapters for parameter-efficient cross-modal adaptation on pre-trained vision-language models. Specifically, adapters are distributed to different modalities and their interactions, with the total number of tunable parameters reduced by partial weight sharing. The unified and knowledge-sharing design enables powerful cross-modal representations that can benefit various downstream tasks, requiring only 1.0%-2.0% tunable parameters of the pre-trained model. Extensive experiments on 6 cross-modal downstream benchmarks (including video-text retrieval, image-text retrieval, VideoQA, and VQA) show that in most cases, UniAdapter 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35789;&#21521;&#37327;&#27169;&#25311;&#65292;&#30740;&#31350;&#21457;&#29616;&#26576;&#20123;&#22788;&#29702;&#25928;&#24212;&#21487;&#19981;&#38656;&#35201;&#24773;&#26223;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2301.08731</link><description>&lt;p&gt;
&#33457;&#29983;&#33021;&#19982;&#20998;&#24067;&#35821;&#20041;&#26377;&#24651;&#29233;&#20043;&#24773;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Peanuts Fall in Love with Distributional Semantics?. (arXiv:2301.08731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08731
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#21644;&#21333;&#35789;&#21521;&#37327;&#27169;&#25311;&#65292;&#30740;&#31350;&#21457;&#29616;&#26576;&#20123;&#22788;&#29702;&#25928;&#24212;&#21487;&#19981;&#38656;&#35201;&#24773;&#26223;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#23545;&#39044;&#26399;&#35789;&#26377;&#24433;&#21709;&#65292;&#36890;&#36807;&#28041;&#21450;&#26893;&#29289;&#20154;&#30340;&#25925;&#20107;&#65292;&#29702;&#35299;&#32773;&#39044;&#26399;&#21477;&#23376;&#8220;&#33457;&#29983;&#24651;&#29233;&#20102;&#8221;&#65292;&#32780;&#19981;&#26159;&#8220;&#33457;&#29983;&#34987;&#30416;&#33100;&#20102;&#8221;&#65292;&#22914;Nieuwland&#21644;van Berkum&#65288;2006&#65289;&#25152;&#31034;&#12290;&#36825;&#31181;&#39044;&#26399;&#30340;&#26356;&#26032;&#26159;&#36890;&#36807;&#24773;&#26223;&#27169;&#22411;-&#25152;&#25551;&#36848;&#20107;&#20214;&#30340;&#24515;&#29702;&#34920;&#31034;&#26469;&#35299;&#37322;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#20165;&#36890;&#36807;&#20998;&#24067;&#20449;&#24687;&#23601;&#21487;&#20197;&#39044;&#27979;N400&#30340;&#25391;&#24133;&#65292;&#36825;&#23601;&#24341;&#21457;&#20102;&#65306;&#36825;&#20123;&#24773;&#22659;&#25928;&#24212;&#26159;&#21542;&#38656;&#35201;&#24773;&#26223;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20845;&#20010;&#35745;&#31639;&#35821;&#35328;&#27169;&#22411;&#21644;&#19977;&#22871;&#21333;&#35789;&#21521;&#37327;&#26469;&#27169;&#25311;Nieuwland&#21644;van Berkum&#65288;2006&#65289;&#30340;&#32467;&#26524;&#65292;&#20854;&#20013;&#27809;&#26377;&#26174;&#24335;&#30340;&#24773;&#26223;&#27169;&#22411;&#25110;&#35821;&#20041;&#22522;&#30784;&#12290;&#25105;&#20204;&#21457;&#29616;&#20854;&#20013;&#30340;&#19968;&#37096;&#20998;&#21487;&#20197;&#23436;&#20840;&#27169;&#25311;Nieuwland&#21644;van Berkum&#65288;2006&#65289;&#25152;&#21457;&#29616;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#33267;&#23569;&#26377;&#20123;&#36890;&#36807;&#24773;&#26223;&#27169;&#22411;&#35299;&#37322;&#30340;&#22788;&#29702;&#25928;&#24212;&#23454;&#38469;&#19978;&#19981;&#38656;&#35201;&#26174;&#24335;&#30340;&#24773;&#26223;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Context changes expectations about upcoming words - following a story involving an anthropomorphic peanut, comprehenders expect the sentence the peanut was in love more than the peanut was salted, as indexed by N400 amplitude (Nieuwland &amp; van Berkum, 2006). This updating of expectations has been explained using Situation Models - mental representations of a described event. However, recent work showing that N400 amplitude is predictable from distributional information alone raises the question whether situation models are necessary for these contextual effects. We model the results of Nieuwland and van Berkum (2006) using six computational language models and three sets of word vectors, none of which have explicit situation models or semantic grounding. We find that a subset of these can fully model the effect found by Nieuwland and van Berkum (2006). Thus, at least some processing effects normally explained through situation models may not in fact require explicit situation models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#21452;&#38454;&#27573;&#19978;&#19979;&#25991;&#36807;&#28388;&#26041;&#27861;&#65292;&#23558;&#27969;&#24335;&#36755;&#20986;&#21644;&#39044;&#23450;&#20041;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#31471;&#23545;&#31471;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#24182;&#21152;&#24555;&#20102;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.06735</link><description>&lt;p&gt;
&#32479;&#19968;&#27969;&#24335;&#21644;&#38750;&#27969;&#24335;&#21464;&#25442;&#22120;&#20013;&#30340;&#19978;&#19979;&#25991;&#20559;&#24046;&#21452;&#38454;&#27573;&#19978;&#19979;&#25991;&#36807;&#28388;
&lt;/p&gt;
&lt;p&gt;
Two Stage Contextual Word Filtering for Context bias in Unified Streaming and Non-streaming Transducer. (arXiv:2301.06735v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06735
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;&#19978;&#19979;&#25991;&#20559;&#24046;&#30340;&#21452;&#38454;&#27573;&#19978;&#19979;&#25991;&#36807;&#28388;&#26041;&#27861;&#65292;&#23558;&#27969;&#24335;&#36755;&#20986;&#21644;&#39044;&#23450;&#20041;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#31471;&#23545;&#31471;&#27169;&#22411;&#30340;&#35782;&#21035;&#20934;&#30830;&#29575;&#65292;&#24182;&#21152;&#24555;&#20102;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#19968;&#20010;&#31471;&#23545;&#31471;&#30340;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#31995;&#32479;&#26469;&#35828;&#65292;&#24456;&#38590;&#35782;&#21035;&#20687;&#23454;&#20307;&#36825;&#26679;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#20986;&#29616;&#19981;&#39057;&#32321;&#30340;&#21333;&#35789;&#12290;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#24120;&#29992;&#26041;&#27861;&#26159;&#23558;&#19978;&#19979;&#25991;&#20449;&#24687;&#36755;&#20837;&#21040;&#22768;&#23398;&#27169;&#22411;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#19968;&#20010;&#32039;&#20945;&#32780;&#20934;&#30830;&#30340;&#19978;&#19979;&#25991;&#21015;&#34920;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20026;&#32479;&#19968;&#30340;&#27969;&#24335;/&#38750;&#27969;&#24335;&#30340;&#31471;&#23545;&#31471;&#27169;&#22411;&#24471;&#21040;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#19978;&#19979;&#25991;&#21015;&#34920;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#30005;&#35805;&#32423;&#21035;&#30340;&#27969;&#24335;&#36755;&#20986;&#26469;&#39318;&#20808;&#36807;&#28388;&#39044;&#23450;&#20041;&#30340;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#65292;&#28982;&#21518;&#23558;&#20854;&#34701;&#21512;&#21040;&#38750;&#22240;&#26524;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#29983;&#25104;&#26368;&#32456;&#30340;&#35782;&#21035;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#19978;&#19979;&#25991;ASR&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#24182;&#21152;&#24555;&#20102;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#22522;&#32447;&#31995;&#32479;&#65292;CERR&#25552;&#39640;&#20102;20%&#20197;&#19978;&#12290;&#21516;&#26102;&#65292;&#24403;&#19978;&#19979;&#25991;&#21333;&#35789;&#21015;&#34920;&#30340;&#22823;&#23567;&#36229;&#36807;6000&#26102;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#30340;RTF&#21487;&#20197;&#31283;&#23450;&#22312;0.15&#24038;&#21491;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is difficult for an E2E ASR system to recognize words such as entities appearing infrequently in the training data. A widely used method to mitigate this issue is feeding contextual information into the acoustic model. Previous works have proven that a compact and accurate contextual list can boost the performance significantly. In this paper, we propose an efficient approach to obtain a high quality contextual list for a unified streaming/non-streaming based E2E model. Specifically, we make use of the phone-level streaming output to first filter the predefined contextual word list then fuse it into non-casual encoder and decoder to generate the final recognition results. Our approach improve the accuracy of the contextual ASR system and speed up the inference process. Experiments on two datasets demonstrates over 20% CERR comparing to the baseline system. Meanwile, the RTF of our system can be stabilized within 0.15 when the size of the contextual word list grows over 6000.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.14815</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#31350;&#40657;&#21283;&#23376;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Black-box language model explanation by context length probing. (arXiv:2212.14815v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14815
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#26032;&#39062;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#36890;&#36807;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#19982;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20851;&#31995;&#26469;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#26377;&#21033;&#20110;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24191;&#27867;&#37319;&#29992;&#24378;&#35843;&#20102;&#25913;&#21892;&#20854;&#21487;&#35299;&#37322;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#25216;&#26415;&#65306;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#65292;&#23427;&#22522;&#20110;&#36319;&#36394;&#27169;&#22411;&#39044;&#27979;&#20316;&#20026;&#21487;&#29992;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#20989;&#25968;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#21516;&#19978;&#19979;&#25991;&#20998;&#37197;&#19981;&#21516;&#30340;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290;&#35813;&#25216;&#26415;&#26159;&#27169;&#22411;&#19981;&#21487;&#30693;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#38500;&#35745;&#31639;token&#32423;&#27010;&#29575;&#20043;&#22806;&#30340;&#27169;&#22411;&#20869;&#37096;&#35775;&#38382;&#12290;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#38271;&#24230;&#25506;&#27979;&#24212;&#29992;&#20110;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20123;&#21021;&#22987;&#30340;&#20998;&#26512;&#21644;&#35265;&#35299;&#65292;&#21253;&#25324;&#30740;&#31350;&#36828;&#36317;&#31163;&#20381;&#36182;&#24615;&#30340;&#28508;&#21147;&#12290;&#26041;&#27861;&#30340;&#28304;&#20195;&#30721;&#21644;&#20132;&#20114;&#24335;&#28436;&#31034;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts. The technique is model-agnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The source code and an interactive demo of the method are available.
&lt;/p&gt;</description></item><item><title>DialGuide&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#25110;&#25351;&#21335;&#25511;&#21046;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#19968;&#33268;&#30340;&#21709;&#24212;&#65292;&#25552;&#21319;&#29992;&#25143;&#20449;&#20219;&#12290;&#20316;&#32773;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#22238;&#22797;&#29983;&#25104;&#30340;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;DialGuide&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.10557</link><description>&lt;p&gt;
DialGuide&#65306;&#23558;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#19982;&#24320;&#21457;&#32773;&#25351;&#21335;&#23545;&#20934;
&lt;/p&gt;
&lt;p&gt;
DialGuide: Aligning Dialogue Model Behavior with Developer Guidelines. (arXiv:2212.10557v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10557
&lt;/p&gt;
&lt;p&gt;
DialGuide&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#25110;&#25351;&#21335;&#25511;&#21046;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#30340;&#26032;&#26694;&#26550;&#65292;&#33021;&#22815;&#24110;&#21161;&#27169;&#22411;&#29983;&#25104;&#26356;&#21152;&#19968;&#33268;&#30340;&#21709;&#24212;&#65292;&#25552;&#21319;&#29992;&#25143;&#20449;&#20219;&#12290;&#20316;&#32773;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#22238;&#22797;&#29983;&#25104;&#30340;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;DialGuide&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#36830;&#36143;&#27969;&#30021;&#30340;&#22238;&#22797;&#65292;&#20294;&#26159;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21560;&#24341;&#20154;&#12289;&#19981;&#23433;&#20840;&#30340;&#32467;&#26524;&#65292;&#36825;&#31181;&#19981;&#21487;&#39044;&#27979;&#24615;&#20250;&#24433;&#21709;&#29992;&#25143;&#20449;&#20219;&#24182;&#38480;&#21046;&#27169;&#22411;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DialGuide&#65292;&#36825;&#26159;&#19968;&#20010;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#35268;&#21017;&#25110;&#25351;&#21335;&#25511;&#21046;&#23545;&#35805;&#27169;&#22411;&#34892;&#20026;&#30340;&#26032;&#26694;&#26550;&#12290;&#36825;&#20123;&#25351;&#21335;&#25552;&#20379;&#20102;&#20851;&#20110;&#23427;&#20204;&#36866;&#29992;&#30340;&#19978;&#19979;&#25991;&#21644;&#21709;&#24212;&#20869;&#23481;&#30340;&#20449;&#24687;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#19982;&#24320;&#21457;&#32773;&#26399;&#26395;&#21644;&#24847;&#22270;&#26356;&#21152;&#19968;&#33268;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#22312;&#24320;&#25918;&#22495;&#23545;&#35805;&#22238;&#22797;&#29983;&#25104;&#30340;&#19977;&#20010;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;DialGuide&#65306;&#25351;&#21335;&#36873;&#25321;&#65292;&#21709;&#24212;&#29983;&#25104;&#21644;&#21709;&#24212;&#21253;&#21547;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#20010;&#39046;&#22495;&#65288;&#38386;&#32842;&#21644;&#23433;&#20840;&#65289;&#30340;10,737&#20010;&#27491;&#21521;&#21644;15,467&#20010;&#36127;&#21521;&#23545;&#35805;&#19978;&#19979;&#25991;-&#21709;&#24212;-&#25351;&#21335;&#19977;&#20803;&#32452;&#65292;&#20026;&#36825;&#20123;&#20219;&#21153;&#25552;&#20379;&#20102;&#22522;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue models are able to generate coherent and fluent responses, but they can still be challenging to control and may produce non-engaging, unsafe results. This unpredictability diminishes user trust and can hinder the use of the models in the real world. To address this, we introduce DialGuide, a novel framework for controlling dialogue model behavior using natural language rules, or guidelines. These guidelines provide information about the context they are applicable to and what should be included in the response, allowing the models to generate responses that are more closely aligned with the developer's expectations and intent. We evaluate DialGuide on three tasks in open-domain dialogue response generation: guideline selection, response generation, and response entailment verification. Our dataset contains 10,737 positive and 15,467 negative dialogue context-response-guideline triplets across two domains - chit-chat and safety. We provide baseline models for the tasks and benc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#19981;&#21516;&#30340;&#25925;&#20107;&#35268;&#21010;&#12290;&#32467;&#26524;&#34920;&#26126;&#23558;&#25925;&#20107;&#20998;&#20026;&#19977;&#24149;&#21487;&#20197;&#24102;&#26469;&#26356;&#19968;&#33268;&#21644;&#26377;&#36259;&#30340;&#21465;&#36848;&#65292;&#21516;&#26102;&#20801;&#35768;&#26126;&#30830;&#25511;&#21046;&#20854;&#20869;&#23481;&#21644;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2212.10471</link><description>&lt;p&gt;
&#23567;&#32418;&#24125;&#29615;&#29699;&#26053;&#34892;&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#35268;&#21010;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Little Red Riding Hood Goes Around the Globe:Crosslingual Story Planning and Generation with Large Language Models. (arXiv:2212.10471v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#19981;&#21516;&#30340;&#25925;&#20107;&#35268;&#21010;&#12290;&#32467;&#26524;&#34920;&#26126;&#23558;&#25925;&#20107;&#20998;&#20026;&#19977;&#24149;&#21487;&#20197;&#24102;&#26469;&#26356;&#19968;&#33268;&#21644;&#26377;&#36259;&#30340;&#21465;&#36848;&#65292;&#21516;&#26102;&#20801;&#35768;&#26126;&#30830;&#25511;&#21046;&#20854;&#20869;&#23481;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#22312;&#21333;&#35821;&#29615;&#22659;&#19979;&#35268;&#21010;&#25925;&#20107;&#29983;&#25104;&#30340;&#26377;&#25928;&#24615;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#33521;&#35821;&#19978;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#36328;&#35821;&#35328;&#30340;&#33258;&#21160;&#25925;&#20107;&#29983;&#25104;&#20013;&#65292;&#35268;&#21010;&#26159;&#21542;&#24102;&#26469;&#20102;&#20248;&#21183;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36328;&#35821;&#35328;&#25925;&#20107;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20026;&#35813;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21019;&#36896;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#30340;&#25925;&#20107;&#35268;&#21010;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#22810;&#31181;&#35821;&#35328;&#20013;&#29983;&#25104;&#20102;&#25925;&#20107;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#25925;&#20107;&#32467;&#26500;&#21270;&#20026;&#19977;&#24149;&#30340;&#35268;&#21010;&#21487;&#20197;&#24102;&#26469;&#26356;&#19968;&#33268;&#21644;&#26377;&#36259;&#30340;&#21465;&#36848;&#65292;&#21516;&#26102;&#20801;&#35768;&#26126;&#30830;&#25511;&#21046;&#20854;&#20869;&#23481;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous work has demonstrated the effectiveness of planning for story generation exclusively in a monolingual setting focusing primarily on English. We consider whether planning brings advantages to automatic story generation across languages. We propose a new task of cross-lingual story generation with planning and present a new dataset for this task. We conduct a comprehensive study of different plans and generate stories in several languages, by leveraging the creative and reasoning capabilities of large pre-trained language models. Our results demonstrate that plans which structure stories into three acts lead to more coherent and interesting narratives, while allowing to explicitly control their content and structure.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.10325</link><description>&lt;p&gt;
SeqDiffuSeq: &#19968;&#31181;&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SeqDiffuSeq: Text Diffusion Model with Encoder-Decoder Transformers for Sequence-to-Sequence Generation. (arXiv:2212.10325v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SeqDiffuSeq&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#37319;&#29992;&#20102;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#21644;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#65292;&#26088;&#22312;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24314;&#27169;&#33539;&#24335;&#65292;&#22312;&#22270;&#20687;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#25991;&#26412;&#30340;&#31163;&#25955;&#20998;&#31867;&#24615;&#36136;&#65292;&#23558;&#36830;&#32493;&#25193;&#25955;&#27169;&#22411;&#25193;&#23637;&#21040;&#33258;&#28982;&#35821;&#35328;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#32780;&#19988;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#30740;&#31350;&#36739;&#23569;&#12290;&#24207;&#21015;&#29983;&#25104;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#35805;&#39064;&#20043;&#19968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#65292;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;&#30340;&#20248;&#36234;&#29983;&#25104;&#24615;&#33021;&#33021;&#21542;&#36716;&#31227;&#21040;&#33258;&#28982;&#35821;&#35328;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;SeqDiffuSeq&#65292;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#29983;&#25104;&#30340;&#25991;&#26412;&#25193;&#25955;&#27169;&#22411;&#12290;SeqDiffuSeq&#20351;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;Transformer&#26550;&#26500;&#26469;&#24314;&#27169;&#21435;&#22122;&#20989;&#25968;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#36136;&#37327;&#65292;SeqDiffuSeq&#32467;&#21512;&#20102;&#33258;&#25105;&#35843;&#33410;&#25216;&#26415;&#21644;&#19968;&#20010;&#26032;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#25216;&#26415;&#12290;&#33258;&#36866;&#24212;&#22122;&#22768;&#35843;&#24230;&#20855;&#26377;&#22343;&#21248;&#21435;&#22122;&#30340;&#22256;&#38590;
&lt;/p&gt;
&lt;p&gt;
Diffusion model, a new generative modelling paradigm, has achieved great success in image, audio, and video generation. However, considering the discrete categorical nature of text, it is not trivial to extend continuous diffusion models to natural language, and text diffusion models are less studied. Sequence-to-sequence text generation is one of the essential natural language processing topics. In this work, we apply diffusion models to approach sequence-to-sequence text generation, and explore whether the superiority generation performance of diffusion model can transfer to natural language domain. We propose SeqDiffuSeq, a text diffusion model for sequence-to-sequence generation. SeqDiffuSeq uses an encoder-decoder Transformers architecture to model denoising function. In order to improve generation quality, SeqDiffuSeq combines the self-conditioning technique and a newly proposed adaptive noise schedule technique. The adaptive noise schedule has the difficulty of denoising evenly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;KNIFE&#65292;&#21487;&#20197;&#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#36827;&#32780;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09721</link><description>&lt;p&gt;
KNIFE: &#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales. (arXiv:2212.09721v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09721
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;KNIFE&#65292;&#21487;&#20197;&#20174;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#20013;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#36827;&#32780;&#22312;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#39640;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#35821;&#35328;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#23427;&#20204;&#30340;&#24847;&#22806;&#38169;&#35823;&#24341;&#36215;&#20102;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#24576;&#30097;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24494;&#35843;/&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#21253;&#21547;&#20219;&#21153;&#23454;&#20363;&#21644;&#20854;&#20851;&#32852;&#30340;&#33258;&#30001;&#25991;&#26412;&#29702;&#30001;&#65288;FTR&#65289;&#65292;&#36825;&#20123;&#29702;&#30001;&#35299;&#37322;&#20102;&#39044;&#27979;&#27491;&#30830;&#20219;&#21153;&#36755;&#20986;&#30340;&#27491;&#30830;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24494;&#35843;&#26041;&#27861;&#26080;&#27861;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#25552;&#31034;&#38656;&#35201;&#36807;&#22823;&#65288;&#21363;&gt;50B&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#25165;&#33021;&#33391;&#22909;&#24037;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;KNIFE&#65292;&#35777;&#26126;&#20174;FTR&#20013;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#25512;&#29702;&#30693;&#35782;&#65292;&#23558;&#20854;&#28748;&#36755;&#21040;&#23567;&#22411;&#65288;&#21363;&lt;1B&#65289;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;KNIFE&#23545;&#19968;&#20010;&#24072;&#29983;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#32473;&#23450;&#20219;&#21153;&#36755;&#20837;&#21644;FTR&#65289;&#65292;&#20197;&#39044;&#27979;&#20219;&#21153;&#36755;&#20986;&#65292;&#23558;&#25512;&#29702;&#30693;&#35782;&#20174;FTR&#36716;&#31227;&#33267;&#24072;&#29983;&#38544;&#34255;&#29366;&#24577;&#12290;&#20854;&#27425;&#65292;KNIFE&#23545;&#19968;&#20010;&#23398;&#29983;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65288;&#20165;&#32473;&#23450;&#20219;&#21153;&#36755;&#20837;&#65289;&#65292;&#20197;&#20351;&#20854;&#38544;&#34255;&#29366;&#24577;&#31867;&#20284;&#20110;&#24072;&#29983;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be "right for the right reasons"). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e., &gt;50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e., &lt;1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;SegAugment&#65292;&#21033;&#29992;&#38899;&#39057;&#20998;&#27573;&#31995;&#32479;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26367;&#20195;&#29256;&#26412;&#65292;&#33021;&#22815;&#25552;&#39640;&#35821;&#38899;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;2.5&#20998;&#65292;&#29978;&#33267;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;5&#20998;&#12290;</title><link>http://arxiv.org/abs/2212.09699</link><description>&lt;p&gt;
SegAugment: &#22522;&#20110;&#20998;&#27573;&#22686;&#24378;&#30340;&#35821;&#38899;&#32763;&#35793;&#25968;&#25454;&#21033;&#29992;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
SegAugment: Maximizing the Utility of Speech Translation Data with Segmentation-based Augmentations. (arXiv:2212.09699v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09699
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;SegAugment&#65292;&#21033;&#29992;&#38899;&#39057;&#20998;&#27573;&#31995;&#32479;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26367;&#20195;&#29256;&#26412;&#65292;&#33021;&#22815;&#25552;&#39640;&#35821;&#38899;&#32763;&#35793;&#30340;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#24179;&#22343;BLEU&#20998;&#25968;&#25552;&#39640;&#20102;2.5&#20998;&#65292;&#29978;&#33267;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;5&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31471;&#21040;&#31471;&#30340;&#35821;&#38899;&#32763;&#35793;&#30001;&#20110;&#32570;&#20047;&#21487;&#29992;&#30340;&#25968;&#25454;&#36164;&#28304;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36164;&#28304;&#26159;&#22522;&#20110;&#25991;&#26723;&#30340;&#65292;&#20294;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#21477;&#23376;&#32423;&#21035;&#30340;&#29256;&#26412;&#65292;&#20294;&#26159;&#23427;&#21482;&#26377;&#21333;&#20010;&#24182;&#19988;&#26159;&#22266;&#23450;&#30340;&#65292;&#21487;&#33021;&#20250;&#22952;&#30861;&#25968;&#25454;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;SegAugment&#65292;&#36890;&#36807;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#22810;&#20010;&#21477;&#23376;&#32423;&#21035;&#30340;&#26367;&#20195;&#29256;&#26412;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#20998;&#27573;&#31995;&#32479;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#38271;&#24230;&#32422;&#26463;&#37325;&#26032;&#20998;&#27573;&#27599;&#20010;&#25991;&#26723;&#30340;&#35821;&#38899;&#65292;&#28982;&#21518;&#36890;&#36807;&#23545;&#40784;&#26041;&#27861;&#33719;&#21462;&#30446;&#26631;&#25991;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;MuST-C&#30340;&#20843;&#31181;&#35821;&#35328;&#23545;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#33268;&#30340;&#22686;&#30410;&#65292;&#24179;&#22343;&#22686;&#21152;&#20102;2.5 BLEU&#20998;&#25968;&#65292;&#24182;&#22312;mTEDx&#30340;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#33719;&#24471;&#20102;&#22810;&#36798;5 BLEU&#20998;&#25968;&#30340;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#24403;&#19982;&#24378;&#22823;&#30340;&#31995;&#32479;&#32467;&#21512;&#26102;&#65292;SegAugment&#22312;MuST-C&#20013;&#26641;&#31435;&#20102;&#26032;&#30340;&#29366;&#24577;&#35760;&#24405;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#22686;&#24378;&#21477;&#23376;&#32423;&#21035;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20351;&#24471;&#31471;&#21040;&#31471;&#27169;&#22411;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#24471;&#21040;&#20102;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-end Speech Translation is hindered by a lack of available data resources. While most of them are based on documents, a sentence-level version is available, which is however single and static, potentially impeding the usefulness of the data. We propose a new data augmentation strategy, SegAugment, to address this issue by generating multiple alternative sentence-level versions of a dataset. Our method utilizes an Audio Segmentation system, which re-segments the speech of each document with different length constraints, after which we obtain the target text via alignment methods. Experiments demonstrate consistent gains across eight language pairs in MuST-C, with an average increase of 2.5 BLEU points, and up to 5 BLEU for low-resource scenarios in mTEDx. Furthermore, when combined with a strong system, SegAugment establishes new state-of-the-art results in MuST-C. Finally, we show that the proposed method can also successfully augment sentence-level datasets, and that it enables 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36339;&#23383;&#27169;&#22411;&#21644;&#36127;&#37319;&#26679;&#26041;&#27861;&#20013;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#24179;&#26041;&#33539;&#25968;&#32534;&#30721;&#20102;&#35789;&#25152;&#20256;&#36798;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#36890;&#36807;&#19982;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#23450;&#20041;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#35789;&#24615;&#21306;&#20998;&#21644;&#19978;&#20301;&#35789;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.09663</link><description>&lt;p&gt;
&#35789;&#21521;&#37327;&#30340;&#33539;&#25968;&#32534;&#30721;&#20449;&#24687;&#22686;&#30410;
&lt;/p&gt;
&lt;p&gt;
Norm of Word Embedding Encodes Information Gain. (arXiv:2212.09663v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#36339;&#23383;&#27169;&#22411;&#21644;&#36127;&#37319;&#26679;&#26041;&#27861;&#20013;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#24179;&#26041;&#33539;&#25968;&#32534;&#30721;&#20102;&#35789;&#25152;&#20256;&#36798;&#30340;&#20449;&#24687;&#22686;&#30410;&#65292;&#36890;&#36807;&#19982;&#35821;&#26009;&#24211;&#20013;&#21333;&#35789;&#30340;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#23450;&#20041;&#65292;&#21487;&#29992;&#20110;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#35789;&#24615;&#21306;&#20998;&#21644;&#19978;&#20301;&#35789;&#20998;&#31867;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#32534;&#30721;&#20102;&#35789;&#27719;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#26159;&#32534;&#30721;&#20102;&#21738;&#20123;&#31867;&#22411;&#30340;&#20449;&#24687;&#65311;&#20197;&#21450;&#22914;&#20309;&#32534;&#30721;&#65311;&#26412;&#25991;&#38024;&#23545;&#36339;&#23383;&#27169;&#22411;&#21644;&#36127;&#37319;&#26679;&#26041;&#27861;&#65292;&#21457;&#29616;&#38745;&#24577;&#35789;&#21521;&#37327;&#30340;&#24179;&#26041;&#33539;&#25968;&#32534;&#30721;&#20102;&#35789;&#25152;&#20256;&#36798;&#30340;&#20449;&#24687;&#22686;&#30410;&#65307;&#32780;&#20449;&#24687;&#22686;&#30410;&#26159;&#36890;&#36807;&#35789;&#22312;&#20849;&#29616;&#20998;&#24067;&#21644;&#35821;&#26009;&#24211;&#30340;&#21333;&#35789;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#23450;&#20041;&#30340;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#36890;&#36807;&#25351;&#25968;&#26063;&#27010;&#29575;&#20998;&#24067;&#30340;&#29702;&#35770;&#26694;&#26550;&#35828;&#26126;&#30340;&#65292;&#24182;&#36890;&#36807;&#28040;&#38500;&#35789;&#39057;&#24341;&#36215;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#31934;&#23494;&#23454;&#39564;&#36827;&#34892;&#20102;&#30830;&#35748;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#26080;&#35770;&#26159;KL&#25955;&#24230;&#36824;&#26159;&#35789;&#23884;&#20837;&#30340;&#24179;&#26041;&#33539;&#25968;&#65292;&#22312;&#20851;&#38190;&#35789;&#25552;&#21462;&#12289;&#35789;&#24615;&#21306;&#20998;&#21644;&#19978;&#20301;&#35789;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#37117;&#25552;&#20379;&#20102;&#26377;&#29992;&#30340;&#35789;&#20449;&#24687;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed representations of words encode lexical semantic information, but what type of information is encoded, and how? Focusing on the skip-gram with negative-sampling method, we found that the squared norm of static word embedding encodes the information gain conveyed by the word; the information gain is defined by the Kullback-Leibler divergence of the co-occurrence distribution of the word to the unigram distribution of the corpus. Our findings are explained by the theoretical framework of the exponential family of probability distributions and confirmed through precise experiments that remove spurious correlations arising from word frequency. We demonstrate that both the KL divergence and the squared norm of embedding provide a useful metric of a word's informativeness in tasks such as keyword extraction, part-of-speech discrimination, and hypernym classification.
&lt;/p&gt;</description></item><item><title>RISE&#26159;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#25688;&#35201;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#30340;&#29983;&#25104;&#25688;&#35201;&#12290;RISE&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.08775</link><description>&lt;p&gt;
RISE: &#21033;&#29992;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#25688;&#35201;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
RISE: Leveraging Retrieval Techniques for Summarization Evaluation. (arXiv:2212.08775v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08775
&lt;/p&gt;
&lt;p&gt;
RISE&#26159;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#25688;&#35201;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#30340;&#29983;&#25104;&#25688;&#35201;&#12290;RISE&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#25688;&#35201;&#36827;&#34892;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#23613;&#31649;&#20986;&#29616;&#20102;&#35768;&#22810;&#26377;&#36259;&#30340;&#26041;&#27861;&#65292;&#20294;&#20173;&#28982;&#26080;&#27861;&#36798;&#21040;&#20154;&#31867;&#35780;&#20272;&#30340;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;RISE&#65292;&#19968;&#31181;&#21033;&#29992;&#20449;&#24687;&#26816;&#32034;&#25216;&#26415;&#36827;&#34892;&#25688;&#35201;&#35780;&#20272;&#30340;&#26032;&#26041;&#27861;&#12290;RISE&#39318;&#20808;&#20316;&#20026;&#19968;&#20010;&#26816;&#32034;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#65292;&#20351;&#29992;&#21452;&#32534;&#30721;&#22120;&#26816;&#32034;&#35774;&#32622;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#30340;&#24773;&#20917;&#19979;&#35780;&#20272;&#32473;&#23450;&#36755;&#20837;&#25991;&#26723;&#30340;&#29983;&#25104;&#25688;&#35201;&#12290;&#24403;&#35780;&#20272;&#26032;&#25968;&#25454;&#38598;&#26102;&#65292;RISE&#29305;&#21035;&#36866;&#29992;&#65292;&#22240;&#20026;&#21487;&#33021;&#27809;&#26377;&#21442;&#32771;&#25688;&#35201;&#21487;&#29992;&#20110;&#35780;&#20272;&#12290;&#25105;&#20204;&#22312;SummEval&#22522;&#20934;&#27979;&#35797;&#65288;Fabbri et al.&#65292;2021&#65289;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;RISE&#19982;&#35768;&#22810;&#36807;&#21435;&#30340;&#25688;&#35201;&#35780;&#20272;&#26041;&#27861;&#30456;&#27604;&#65292;&#19982;&#20154;&#31867;&#35780;&#20272;&#20855;&#26377;&#26356;&#39640;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;RISE&#36824;&#23637;&#31034;&#20102;&#36328;&#35821;&#35328;&#30340;&#25968;&#25454;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating automatically-generated text summaries is a challenging task. While there have been many interesting approaches, they still fall short of human evaluations. We present RISE, a new approach for evaluating summaries by leveraging techniques from information retrieval. RISE is first trained as a retrieval task using a dual-encoder retrieval setup, and can then be subsequently utilized for evaluating a generated summary given an input document, without gold reference summaries. RISE is especially well suited when working on new datasets where one may not have reference summaries available for evaluation. We conduct comprehensive experiments on the SummEval benchmark (Fabbri et al., 2021) and the results show that RISE has higher correlation with human evaluations compared to many past approaches to summarization evaluation. Furthermore, RISE also demonstrates data-efficiency and generalizability across languages.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;15&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20154;&#26684;&#20998;&#31867;&#27861;&#65292;&#27979;&#37327;&#36136;&#37327;&#65292;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#35780;&#20272;&#65292;&#24314;&#27169;&#36873;&#25321;&#20197;&#21450;&#36947;&#24503;&#21644;&#20844;&#24179;&#24615;&#65292;&#26088;&#22312;&#28608;&#21457;&#26356;&#22810;&#30340;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;TPC&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2212.06711</link><description>&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#65306;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
On Text-based Personality Computing: Challenges and Future Directions. (arXiv:2212.06711v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;15&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#20154;&#26684;&#20998;&#31867;&#27861;&#65292;&#27979;&#37327;&#36136;&#37327;&#65292;&#25968;&#25454;&#38598;&#65292;&#24615;&#33021;&#35780;&#20272;&#65292;&#24314;&#27169;&#36873;&#25321;&#20197;&#21450;&#36947;&#24503;&#21644;&#20844;&#24179;&#24615;&#65292;&#26088;&#22312;&#28608;&#21457;&#26356;&#22810;&#30340;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;TPC&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25991;&#26412;&#30340;&#20154;&#26684;&#35745;&#31639;&#65288;TPC&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#24341;&#36215;&#20102;&#35768;&#22810;&#30740;&#31350;&#20852;&#36259;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;15&#20010;&#25361;&#25112;&#65292;&#20540;&#24471;&#30740;&#31350;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#25361;&#25112;&#25353;&#20197;&#19979;&#20027;&#39064;&#32452;&#32455;&#65306;&#20154;&#26684;&#20998;&#31867;&#27861;&#12289;&#27979;&#37327;&#36136;&#37327;&#12289;&#25968;&#25454;&#38598;&#12289;&#24615;&#33021;&#35780;&#20272;&#12289;&#24314;&#27169;&#36873;&#25321;&#20197;&#21450;&#36947;&#24503;&#21644;&#20844;&#24179;&#24615;&#12290;&#22312;&#24212;&#23545;&#27599;&#20010;&#25361;&#25112;&#26102;&#65292;&#25105;&#20204;&#19981;&#20165;&#32467;&#21512;&#20102;NLP&#21644;&#31038;&#20250;&#31185;&#23398;&#30340;&#35270;&#35282;&#65292;&#36824;&#25552;&#20379;&#20855;&#20307;&#30340;&#24314;&#35758;&#12290;&#25105;&#20204;&#24076;&#26395;&#28608;&#21457;&#26356;&#22810;&#26377;&#25928;&#21644;&#21487;&#38752;&#30340;TPC&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based personality computing (TPC) has gained many research interests in NLP. In this paper, we describe 15 challenges that we consider deserving the attention of the research community. These challenges are organized by the following topics: personality taxonomies, measurement quality, datasets, performance evaluation, modelling choices, as well as ethics and fairness. When addressing each challenge, not only do we combine perspectives from both NLP and social sciences, but also offer concrete suggestions. We hope to inspire more valid and reliable TPC research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#29992;&#20110;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#30340;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#30340;&#25928;&#26524;&#21644;&#24212;&#29992;&#26041;&#21521;&#65292;&#25552;&#20986;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#27979;&#35797;&#35774;&#32622;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2212.04068</link><description>&lt;p&gt;
&#25506;&#31350;&#29992;&#20110;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#30340;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#65306;&#26377;&#25928;&#24615;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next. (arXiv:2212.04068v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#29992;&#20110;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#30340;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#30340;&#25928;&#26524;&#21644;&#24212;&#29992;&#26041;&#21521;&#65292;&#25552;&#20986;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#27979;&#35797;&#35774;&#32622;&#65292;&#24182;&#20844;&#24320;&#20102;&#25152;&#26377;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#39044;&#35757;&#32451;&#30340;&#20013;&#25991;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#27721;&#35821;&#28431;&#23383;&#26816;&#26597;&#65288;CSC&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#23383;&#24418;&#21644;&#38899;&#26631;&#31561;&#20449;&#24687;&#26469;&#25913;&#21892;&#21306;&#20998;&#38169;&#25340;&#23383;&#31526;&#30340;&#33021;&#21147;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#24182;&#19981;&#22909;&#29702;&#35299;&#65306;&#19981;&#28165;&#26970;&#23427;&#20204;&#26159;&#21542;&#21253;&#21547;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#65292;&#20197;&#21450;&#22914;&#26524;&#21253;&#21547;&#36825;&#20123;&#20449;&#24687;&#65292;&#26159;&#21542;&#20805;&#20998;&#21033;&#29992;&#20102;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#23383;&#24418;&#38899;&#26631;&#20449;&#24687;&#22312; CSC &#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;&#30340;&#26041;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#12289;&#26356;&#20855;&#25361;&#25112;&#24615;&#21644;&#23454;&#29992;&#30340; CSC &#27169;&#22411;&#27867;&#21270;&#24615;&#27979;&#35797;&#35774;&#32622;&#12290;&#25152;&#26377;&#20195;&#30721;&#37117;&#24050;&#20844;&#24320;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability to distinguish misspelled characters, with good results. However, the generalization ability of these models is not well understood: it is unclear whether they incorporate glyph-phonetic information and, if so, whether this information is fully utilized. In this paper, we aim to better understand the role of glyph-phonetic information in the CSC task and suggest directions for improvement. Additionally, we propose a new, more challenging, and practical setting for testing the generalizability of CSC models. All code is made publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#65288;EHR&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21040;&#36880;&#20986;&#29366;&#24577;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;KIRESH&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.02762</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#35760;&#24405;&#20013;&#35782;&#21035;&#36880;&#20986;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
Automated Identification of Eviction Status from Electronic Health Record Notes. (arXiv:2212.02762v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02762
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#65288;EHR&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21040;&#36880;&#20986;&#29366;&#24577;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#27169;&#22411;KIRESH&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#36880;&#20986;&#26159;&#20581;&#24247;&#30340;&#37325;&#35201;&#31038;&#20250;&#21644;&#34892;&#20026;&#20915;&#23450;&#22240;&#32032;&#12290;&#36880;&#20986;&#19982;&#19968;&#31995;&#21015;&#36127;&#38754;&#20107;&#20214;&#30456;&#20851;&#65292;&#21487;&#33021;&#23548;&#33268;&#22833;&#19994;&#12289;&#20303;&#25151;&#19981;&#23433;&#20840;/&#26080;&#23478;&#21487;&#24402;&#12289;&#38271;&#26399;&#36139;&#22256;&#21644;&#31934;&#31070;&#20581;&#24247;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#21487;&#20197;&#33258;&#21160;&#20174;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;&#65288;EHR&#65289;&#35760;&#24405;&#20013;&#26816;&#27979;&#21040;&#36880;&#20986;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Evictions are important social and behavioral determinants of health. Evictions are associated with a cascade of negative events that can lead to unemployment, housing insecurity/homelessness, long-term poverty, and mental health problems. In this study, we developed a natural language processing system to automatically detect eviction status from electronic health record (EHR) notes.  Materials and Methods: We first defined eviction status (eviction presence and eviction period) and then annotated eviction status in 5000 EHR notes from the Veterans Health Administration (VHA). We developed a novel model, KIRESH, that has shown to substantially outperform other state-of-the-art models such as fine-tuning pre-trained language models like BioBERT and BioClinicalBERT. Moreover, we designed a novel prompt to further improve the model performance by using the intrinsic connection between the two sub-tasks of eviction presence and period prediction. Finally, we used the Temperatur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;ESPnet&#26080;&#30417;&#30563;ASR&#24320;&#28304;&#24037;&#20855;&#21253;&#65288;EURO&#65289;&#30340;&#32454;&#33410;&#21450;&#20854;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#24449;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#35813;&#24037;&#20855;&#21253;&#38598;&#25104;&#20102;27&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;UASR&#30340;&#25928;&#29575;&#21644;&#21487;&#37325;&#29616;&#24615;&#65307;&#22312;TIMIT&#21644;LibriSpeech&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.17196</link><description>&lt;p&gt;
EURO: ESPnet&#26080;&#30417;&#30563;ASR&#24320;&#28304;&#24037;&#20855;&#21253;&#65288;arXiv:2211.17196v3 [cs.CL] UPDATED&#65289;
&lt;/p&gt;
&lt;p&gt;
EURO: ESPnet Unsupervised ASR Open-source Toolkit. (arXiv:2211.17196v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.17196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ESPnet&#26080;&#30417;&#30563;ASR&#24320;&#28304;&#24037;&#20855;&#21253;&#65288;EURO&#65289;&#30340;&#32454;&#33410;&#21450;&#20854;&#22312;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#24449;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#26041;&#38754;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#35813;&#24037;&#20855;&#21253;&#38598;&#25104;&#20102;27&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#21644;&#22522;&#20110;&#22270;&#24418;&#30340;&#35299;&#30721;&#31574;&#30053;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;UASR&#30340;&#25928;&#29575;&#21644;&#21487;&#37325;&#29616;&#24615;&#65307;&#22312;TIMIT&#21644;LibriSpeech&#25968;&#25454;&#19978;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ESPnet&#26080;&#30417;&#30563;ASR&#24320;&#28304;&#24037;&#20855;&#21253;&#65288;EURO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26080;&#30417;&#30563;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;UASR&#65289;&#24320;&#28304;&#24037;&#20855;&#21253;&#12290;EURO&#37319;&#29992;&#20102;&#30001;Wav2vec-U&#24341;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;UASR&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26368;&#21021;&#30001;FAIRSEQ&#23454;&#29616;&#65292;&#21033;&#29992;&#33258;&#30417;&#30563;&#35821;&#38899;&#34920;&#24449;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#38500;&#20102;wav2vec2&#20043;&#22806;&#65292;EURO&#36890;&#36807;&#38598;&#25104;S3PRL&#21644;k2&#25193;&#23637;&#20102;UASR&#20219;&#21153;&#30340;&#21151;&#33021;&#21644;&#20419;&#36827;&#20102;&#21487;&#37325;&#29616;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;27&#20010;&#33258;&#30417;&#30563;&#27169;&#22411;&#30340;&#28789;&#27963;&#21069;&#31471;&#21644;&#21508;&#31181;&#22522;&#20110;&#22270;&#24418;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290;EURO&#22312;ESPnet&#20013;&#23454;&#29616;&#24182;&#36981;&#24490;&#20854;&#32479;&#19968;&#27969;&#31243;&#65292;&#25552;&#20379;&#20102;&#20855;&#26377;&#23436;&#25972;&#35774;&#32622;&#30340;UASR&#37197;&#26041;&#12290;&#36825;&#25552;&#39640;&#20102;&#27969;&#31243;&#30340;&#25928;&#29575;&#65292;&#24182;&#20801;&#35768;EURO&#36731;&#26494;&#24212;&#29992;&#20110;ESPnet&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#12290;&#23545;&#19977;&#20010;&#20027;&#27969;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#35813;&#24037;&#20855;&#21253;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;TIMIT&#21644;LibriSpeech&#25968;&#25454;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;UASR&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes the ESPnet Unsupervised ASR Open-source Toolkit (EURO), an end-to-end open-source toolkit for unsupervised automatic speech recognition (UASR). EURO adopts the state-of-the-art UASR learning method introduced by the Wav2vec-U, originally implemented at FAIRSEQ, which leverages self-supervised speech representations and adversarial training. In addition to wav2vec2, EURO extends the functionality and promotes reproducibility for UASR tasks by integrating S3PRL and k2, resulting in flexible frontends from 27 self-supervised models and various graph-based decoding strategies. EURO is implemented in ESPnet and follows its unified pipeline to provide UASR recipes with a complete setup. This improves the pipeline's efficiency and allows EURO to be easily applied to existing datasets in ESPnet. Extensive experiments on three mainstream self-supervised models demonstrate the toolkit's effectiveness and achieve state-of-the-art UASR performance on TIMIT and LibriSpeech data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35821;&#27861;&#35268;&#21017;&#30340;&#35825;&#23548;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35821;&#27861;&#21487;&#26367;&#20195;&#24615;&#65292;&#33021;&#22815;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2211.16031</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#20381;&#23384;&#35821;&#27861;&#30340;&#35821;&#27861;&#21487;&#26367;&#20195;&#24615;
&lt;/p&gt;
&lt;p&gt;
Syntactic Substitutability as Unsupervised Dependency Syntax. (arXiv:2211.16031v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16031
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35821;&#27861;&#35268;&#21017;&#30340;&#35825;&#23548;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35821;&#27861;&#21487;&#26367;&#20195;&#24615;&#65292;&#33021;&#22815;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#27861;&#26159;&#26500;&#25104;&#20154;&#31867;&#35821;&#35328;&#40092;&#26126;&#21644;&#32452;&#21512;&#24615;&#30340;&#28508;&#22312;&#20998;&#23618;&#32467;&#26500;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#25506;&#31350;&#20102;&#20381;&#36182;&#20110;&#35821;&#35328;&#27169;&#22411;&#27880;&#24847;&#21147;&#20998;&#24067;&#30340;&#21477;&#27861;&#20381;&#23384;&#20851;&#31995;&#34920;&#31034;&#30340;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#38656;&#35821;&#27861;&#35268;&#21017;&#30340;&#35825;&#23548;&#26041;&#27861;&#12290;&#25105;&#20204;&#26088;&#22312;&#27169;&#25311;&#21477;&#27861;&#26367;&#20195;&#24615;&#36825;&#20010;&#26356;&#20026;&#26222;&#36941;&#30340;&#20851;&#31995;&#65292;&#32780;&#19981;&#26159;&#20005;&#26684;&#25353;&#29031;&#26631;&#27880;&#26550;&#26500;&#24314;&#27169;&#20381;&#23384;&#20851;&#31995;&#12290;&#36825;&#31181;&#20851;&#31995;&#20307;&#29616;&#20102;&#20107;&#23454;&#65292;&#21363;&#21477;&#27861;&#20381;&#23384;&#20851;&#31995;&#20004;&#31471;&#30340;&#21333;&#35789;&#21487;&#20197;&#34987;&#21516;&#19968;&#21477;&#27861;&#33539;&#30068;&#30340;&#21333;&#35789;&#25152;&#26367;&#25442;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#19968;&#32452;&#35821;&#27861;&#19981;&#21464;&#30340;&#21477;&#23376;&#12290;&#36825;&#20123;&#21477;&#23376;&#30340;&#34920;&#31034;&#34987;&#29992;&#20316;&#35299;&#26512;&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#26041;&#38754;&#37117;&#26377;&#25152;&#25552;&#21319;&#65292;&#20363;&#22914;&#22312;&#38271;&#36317;&#31163;&#20027;&#35859;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;78.3&#65285;&#30340;&#21484;&#22238;&#29575;&#65292;&#32780;&#20043;&#21069;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#21482;&#26377;8.5&#65285;&#30340;&#21484;&#22238;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Syntax is a latent hierarchical structure which underpins the robust and compositional nature of human language. In this work, we further explore the hypothesis that syntactic dependencies can be represented in the attention distributions of language models trained on text and propose a new method to induce these structures theory-agnostically. Instead of modeling syntactic relations as defined by annotation schemata, we model a more general property implicit in the definition of dependency relations, syntactic substitutability. This property captures the fact that the words at either end of a syntactic dependency can be substituted with words from the same syntactic category, defining a set of syntactically-invariant sentences whose representations are then used as the basis for parsing. We demonstrate that our method results in both qualitative and quantitative gains, for example achieving 78.3% recall on a long-distance subject-verb agreement task vs. 8.5% with a previous unsupervis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.11093</link><description>&lt;p&gt;
[RE]VER&#65306;&#23398;&#20064;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#20197;&#38416;&#36848;&#23454;&#20307;&#21644;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
[RE]VER: Learning Natural Language Representations for Verbalizing Entities and Relations. (arXiv:2211.11093v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#33021;&#22815;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#65292;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#21450;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#20154;&#20204;&#36890;&#36807;&#29702;&#35299;&#23454;&#20307;&#21644;&#20851;&#31995;&#26469;&#20102;&#35299;&#19990;&#30028;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;[RE]VER&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#26469;&#23398;&#20064;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#24182;&#29983;&#25104;&#19968;&#20010;&#33021;&#22815;&#34920;&#31034;&#23454;&#20307;&#19982;&#20854;&#20182;&#23454;&#20307;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20854;&#30456;&#27604;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entities and relationships between entities are vital in the real world. Essentially, we understand the world by understanding entities and relations. For instance, to understand a field, e.g., computer science, we need to understand the relevant concepts, e.g., machine learning, and the relationships between concepts, e.g., machine learning and artificial intelligence. To understand a person, we should first know who he/she is and how he/she is related to others. To understand entities and relations, humans may refer to natural language descriptions. For instance, when learning a new scientific term, people usually start by reading its definition in dictionaries or encyclopedias. To know the relationship between two entities, humans tend to create a sentence to connect them. In this paper, we propose [RE]VER: A Unified Model for Verbalizing Entities and Relations. Specifically, we attempt to build a system that takes any entity or entity set as input and generates a sentence to repres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#30446;&#21069;&#35821;&#29992;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#24314;&#35758;&#24182;&#20998;&#26512;&#20102;&#35821;&#35328;&#21547;&#20041;&#30340;&#20016;&#23500;&#24615;&#12290;&#26410;&#26469;&#30340;&#20219;&#21153;&#35774;&#35745;&#38656;&#35201;&#24341;&#20986;&#35821;&#29992;&#29616;&#35937;&#65292;&#24182;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#20132;&#27969;&#19978;&#19979;&#25991;&#21644;&#25928;&#30410;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2211.08371</link><description>&lt;p&gt;
&#35821;&#35328;&#22522;&#30784;&#20013;&#30340;&#35821;&#29992;&#23398;&#65306;&#29616;&#35937;&#12289;&#20219;&#21153;&#21644;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pragmatics in Language Grounding: Phenomena, Tasks, and Modeling Approaches. (arXiv:2211.08371v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#30446;&#21069;&#35821;&#29992;&#23398;&#27169;&#22411;&#30340;&#30740;&#31350;&#29616;&#29366;&#65292;&#25552;&#20986;&#20102;&#24314;&#35758;&#24182;&#20998;&#26512;&#20102;&#35821;&#35328;&#21547;&#20041;&#30340;&#20016;&#23500;&#24615;&#12290;&#26410;&#26469;&#30340;&#20219;&#21153;&#35774;&#35745;&#38656;&#35201;&#24341;&#20986;&#35821;&#29992;&#29616;&#35937;&#65292;&#24182;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#20132;&#27969;&#19978;&#19979;&#25991;&#21644;&#25928;&#30410;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#22312;&#20132;&#27969;&#20013;&#32463;&#24120;&#20381;&#36182;&#19978;&#19979;&#25991;&#26469;&#20016;&#23500;&#35328;&#22806;&#20043;&#24847;&#65292;&#20174;&#32780;&#23454;&#29616;&#31616;&#26126;&#32780;&#26377;&#25928;&#30340;&#27807;&#36890;&#12290;&#20026;&#20102;&#33021;&#22815;&#19982;&#20154;&#31867;&#25104;&#21151;&#22320;&#33258;&#28982;&#20132;&#20114;&#65292;&#38754;&#21521;&#29992;&#25143;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23558;&#38656;&#35201;&#31867;&#20284;&#30340;&#35821;&#29992;&#23398;&#25216;&#33021;&#65306;&#20381;&#38752;&#21508;&#31181;&#19978;&#19979;&#25991;&#20449;&#24687;&#8212;&#8212;&#20174;&#20849;&#20139;&#30340;&#35821;&#35328;&#30446;&#26631;&#21644;&#32422;&#23450;&#21040;&#35270;&#35273;&#21644;&#20855;&#36523;&#19990;&#30028;&#65292;&#26377;&#25928;&#22320;&#20351;&#29992;&#35821;&#35328;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#29616;&#26377;&#30340;&#35821;&#22659;&#35774;&#32622;&#21644;&#35821;&#29992;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#20998;&#26512;&#20102;&#27599;&#20010;&#24037;&#20316;&#20013;&#20219;&#21153;&#30446;&#26631;&#12289;&#29615;&#22659;&#19978;&#19979;&#25991;&#21644;&#20132;&#38469;&#25928;&#30410;&#22914;&#20309;&#20016;&#23500;&#35821;&#35328;&#21547;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#22522;&#30784;&#20219;&#21153;&#35774;&#35745;&#30340;&#24314;&#35758;&#65292;&#20197;&#33258;&#28982;&#22320;&#24341;&#20986;&#35821;&#29992;&#23398;&#29616;&#35937;&#65292;&#24182;&#24314;&#35758;&#20851;&#27880;&#26356;&#24191;&#27867;&#30340;&#20132;&#27969;&#19978;&#19979;&#25991;&#21644;&#25928;&#30410;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
People rely heavily on context to enrich meaning beyond what is literally said, enabling concise but effective communication. To interact successfully and naturally with people, user-facing artificial intelligence systems will require similar skills in pragmatics: relying on various types of context -from shared linguistic goals and conventions, to the visual and embodied world -- to use language effectively. We survey existing grounded settings and pragmatic modeling approaches and analyze how the task goals, environmental contexts, and communicative affordances in each work enrich linguistic meaning. We present recommendations for future grounded task design to naturally elicit pragmatic phenomena, and suggest directions that focus on a broader range of communicative contexts and affordances.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MEAL&#26041;&#27861;&#65292;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#31283;&#23450;&#21644;&#27963;&#36291;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#36129;&#29486;&#65292;&#19968;&#20010;&#26159;&#25552;&#20986;&#26032;&#39062;&#30340;&#20943;&#23569;&#36816;&#34892;&#21464;&#24322;&#24615;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#26159;&#24341;&#20837;AL&#20934;&#21017;&#29992;&#20110;&#25968;&#25454;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2211.08358</link><description>&lt;p&gt;
MEAL&#65306;&#23569;&#26679;&#26412;&#25552;&#31034;&#30340;&#31283;&#23450;&#21644;&#27963;&#36291;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MEAL: Stable and Active Learning for Few-Shot Prompting. (arXiv:2211.08358v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MEAL&#26041;&#27861;&#65292;&#26159;&#19968;&#20010;&#21487;&#20197;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#20998;&#31867;&#65292;&#31283;&#23450;&#21644;&#27963;&#36291;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#26041;&#27861;&#12290;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#36129;&#29486;&#65292;&#19968;&#20010;&#26159;&#25552;&#20986;&#26032;&#39062;&#30340;&#20943;&#23569;&#36816;&#34892;&#21464;&#24322;&#24615;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#21478;&#19968;&#20010;&#26159;&#24341;&#20837;AL&#20934;&#21017;&#29992;&#20110;&#25968;&#25454;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21551;&#21160;&#21644;&#25552;&#31034;&#65292;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#65292;&#22312;&#23569;&#26679;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#23569;&#26679;&#26412;&#38598;&#21512;&#65288;&#25968;&#25454;&#36873;&#25321;&#65289;&#21644;&#19981;&#21516;&#30340;&#24494;&#35843;&#36816;&#34892;&#65288;&#36816;&#34892;&#21464;&#24322;&#24615;&#65289;&#20043;&#38388;&#20855;&#26377;&#39640;&#21464;&#21270;&#29575;&#65292;&#36825;&#19981;&#20165;&#38459;&#30861;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#32780;&#19988;&#20351;&#24471;&#23569;&#26679;&#26412;&#23398;&#20064;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#36807;&#20110;&#19981;&#21487;&#38752;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#36129;&#29486;&#65292;&#29992;&#20110;&#26356;&#31283;&#23450;&#21644;&#26377;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#36816;&#34892;&#21464;&#24322;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#20027;&#21160;&#23398;&#20064;&#65288;AL&#65289;&#20934;&#21017;&#29992;&#20110;&#25968;&#25454;&#36873;&#25321;&#65292;&#24182;&#21576;&#29616;&#20102;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;AL&#26041;&#27861;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#32852;&#21512;&#26041;&#27861;MEAL&#65288;&#22810;&#25552;&#31034;&#24494;&#35843;&#19982;&#39044;&#27979;&#38598;&#25104;&#19982;&#20027;&#21160;&#23398;&#20064;&#65289;&#21487;&#20197;&#31283;&#23450;&#22320;&#22312;&#23569;&#37327;&#26679;&#26412;&#19979;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot classification has made great strides due to foundation models that, through priming and prompting, are highly effective few-shot learners. However, this approach has high variance both across different sets of few shots (data selection) and across different finetuning runs (run variability). This is problematic not only because it impedes the fair comparison of different approaches, but especially because it makes few-shot learning too unreliable for many real-world applications. To alleviate these issues, we make two contributions for more stable and effective few-shot learning: First, we propose novel ensembling methods and show that they substantially reduce run variability. Second, we introduce a new active learning (AL) criterion for data selection and present the first AL-based approach specifically tailored towards prompt-based learning. In our experiments, we show that our combined method, MEAL (Multiprompt finetuning and prediction Ensembling with Active Learning), i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2211.08073</link><description>&lt;p&gt;
GLUE-X: &#20174;ODD&#26222;&#36866;&#24615;&#35282;&#24230;&#35780;&#20272;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective. (arXiv:2211.08073v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08073
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#36890;&#36807;&#21033;&#29992;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;NLP&#20219;&#21153;&#20013;&#30340;ODD&#26222;&#36866;&#24615;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#36825;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#21019;&#24314;&#21517;&#20026;&#26041;&#27861;&#30340;&#32479;&#19968;&#22522;&#20934;&#30340;&#23581;&#35797;&#65292;&#29992;&#20110;&#35780;&#20272;NLP&#27169;&#22411;&#20013;&#30340;OOD&#40065;&#26834;&#24615;&#65292;&#24378;&#35843;OOD&#40065;&#26834;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20379;&#22914;&#20309;&#34913;&#37327;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#20197;&#21450;&#22914;&#20309;&#25913;&#21892;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;13&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;OOD&#27979;&#35797;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;21&#20010;&#24120;&#29992;&#30340;PLMs&#65288;&#21253;&#25324;GPT-3&#21644;GPT-3.5&#65289;&#19978;&#23545;8&#20010;&#32463;&#20856;NLP&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#30830;&#35748;&#20102;&#22312;&#25152;&#26377;&#35774;&#32622;&#19979;&#65292;&#19982;ID&#20934;&#30830;&#24230;&#30456;&#27604;&#65292;&#23384;&#22312;&#26174;&#30528;&#30340;&#24615;&#33021;&#19979;&#38477;&#65292;&#38656;&#35201;&#25913;&#21892;NLP&#20219;&#21153;&#20013;&#30340;OOD&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named \method for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#20110;&#35821;&#35328;&#32467;&#26500;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#24207;&#21015;&#26631;&#27880;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102; PLMs &#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#25512;&#24191;&#24615;&#65292;&#36825;&#33021;&#22815;&#24110;&#21161;&#26816;&#32034;&#20219;&#24847;&#26631;&#31614;&#30340;&#35821;&#35328;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.07830</link><description>&lt;p&gt;
&#25361;&#25112;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#32467;&#26500;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Prompting Language Models for Linguistic Structure. (arXiv:2211.07830v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.07830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#23545;&#20110;&#35821;&#35328;&#32467;&#26500;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#25552;&#31034;&#26041;&#27861;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#24207;&#21015;&#26631;&#27880;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102; PLMs &#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#35821;&#35328;&#30693;&#35782;&#30340;&#25512;&#24191;&#24615;&#65292;&#36825;&#33021;&#22815;&#24110;&#21161;&#26816;&#32034;&#20219;&#24847;&#26631;&#31614;&#30340;&#35821;&#35328;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21487;&#20197;&#23436;&#25104;&#21508;&#31181;&#21508;&#26679;&#30340;&#35821;&#35328;&#20219;&#21153;&#65292;&#20294;&#22522;&#20110;&#21487;&#25512;&#24191;&#24615;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19982;&#22522;&#20110;&#34920;&#38754;&#32423;&#21035;&#30340;&#35789;&#27719;&#27169;&#24335;&#26377;&#22810;&#23569;&#20851;&#31995;&#20173;&#28982;&#26159;&#20010;&#26410;&#35299;&#20043;&#35868;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#35328;&#32467;&#26500;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#26041;&#27861;&#65292;&#20801;&#35768;&#25105;&#20204;&#22312;&#33258;&#22238;&#24402;PLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#36827;&#34892;&#24207;&#21015;&#26631;&#27880;&#12290;&#25105;&#20204;&#23558;&#20854;&#29992;&#20110;&#35789;&#24615;&#26631;&#27880;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#21477;&#23376;&#20998;&#22359;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#30340;&#24378;&#22823;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#30001;&#20110;&#20219;&#21153;&#27844;&#38706;&#21040;&#39044;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#65292;PLMs&#21253;&#21547;&#20102;&#37325;&#35201;&#30340;&#20219;&#21153;&#26631;&#31614;&#20808;&#39564;&#30693;&#35782;&#65292;&#20294;&#32467;&#26500;&#21270;&#25552;&#31034;&#20063;&#21487;&#20197;&#26816;&#32034;&#20219;&#24847;&#26631;&#31614;&#30340;&#35821;&#35328;&#32467;&#26500;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;PLMs&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21644;&#35821;&#35328;&#30693;&#35782;&#20855;&#26377;&#25512;&#24191;&#24615;&#65292;&#19981;&#20165;&#23616;&#38480;&#20110;&#20854;&#35757;&#32451;&#25968;&#25454;&#30340;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although pretrained language models (PLMs) can be prompted to perform a wide range of language tasks, it remains an open question how much this ability comes from generalizable linguistic understanding versus surface-level lexical patterns. To test this, we present a structured prompting approach for linguistic structured prediction tasks, allowing us to perform zero- and few-shot sequence tagging with autoregressive PLMs. We evaluate this approach on part-of-speech tagging, named entity recognition, and sentence chunking, demonstrating strong few-shot performance in all cases. We also find that while PLMs contain significant prior knowledge of task labels due to task leakage into the pretraining corpus, structured prompting can also retrieve linguistic structure with arbitrary labels. These findings indicate that the in-context learning ability and linguistic knowledge of PLMs generalizes beyond memorization of their training data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;DiaASQ&#65292;&#23427;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#30446;&#26631;-&#26041;&#38754;-&#35266;&#28857;-&#24773;&#24863;&#22235;&#20803;&#32452;&#12290;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#39640;&#36136;&#37327;DiaASQ&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#27169;&#22411;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27169;&#22411;&#22312;&#36328;&#35805;&#35821;&#22235;&#20803;&#32452;&#25552;&#21462;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2211.05705</link><description>&lt;p&gt;
DiaASQ&#65306;&#19968;&#31181;&#22522;&#20110;&#23545;&#35805;&#30340;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#32452;&#20998;&#26512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
DiaASQ : A Benchmark of Conversational Aspect-based Sentiment Quadruple Analysis. (arXiv:2211.05705v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.05705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;DiaASQ&#65292;&#23427;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#30446;&#26631;-&#26041;&#38754;-&#35266;&#28857;-&#24773;&#24863;&#22235;&#20803;&#32452;&#12290;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#39640;&#36136;&#37327;DiaASQ&#25968;&#25454;&#38598;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#27169;&#22411;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27169;&#22411;&#22312;&#36328;&#35805;&#35821;&#22235;&#20803;&#32452;&#25552;&#21462;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20960;&#21313;&#24180;&#26469;&#65292;&#22522;&#20110;&#26041;&#38754;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#23637;&#29616;&#20986;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;ABSA&#24037;&#20316;&#22823;&#22810;&#38480;&#20110;&#21333;&#20010;&#25991;&#26412;&#22330;&#26223;&#65292;&#32570;&#20047;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#30740;&#31350;&#12290;&#20026;&#20102;&#24357;&#21512;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#21644;&#23545;&#35805;&#20013;&#35266;&#28857;&#25366;&#25496;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#22522;&#20110;&#23545;&#35805;&#30340;&#26041;&#38754;&#24773;&#24863;&#22235;&#20803;&#32452;&#20998;&#26512;&#65288;DiaASQ&#65289;&#65292;&#26088;&#22312;&#26816;&#27979;&#23545;&#35805;&#20013;&#30340;&#30446;&#26631;-&#26041;&#38754;-&#35266;&#28857;-&#24773;&#24863;&#22235;&#20803;&#32452;&#12290;&#25105;&#20204;&#25163;&#21160;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#39640;&#36136;&#37327;DiaASQ&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20013;&#25991;&#21644;&#33521;&#25991;&#20004;&#31181;&#35821;&#35328;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31070;&#32463;&#27169;&#22411;&#26469;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27169;&#22411;&#22312;&#26377;&#25928;&#22320;&#25191;&#34892;&#31471;&#21040;&#31471;&#22235;&#20803;&#32452;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#24182;&#35774;&#27861;&#32467;&#21512;&#20016;&#23500;&#30340;&#23545;&#35805;&#29305;&#23450;&#21644;&#35805;&#35821;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#35805;&#35821;&#22235;&#20803;&#32452;&#25552;&#21462;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#19968;&#26032;&#30340;&#22522;&#20934;&#23558;&#28608;&#21457;&#26356;&#22810;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid development of aspect-based sentiment analysis (ABSA) within recent decades shows great potential for real-world society. The current ABSA works, however, are mostly limited to the scenario of a single text piece, leaving the study in dialogue contexts unexplored. To bridge the gap between fine-grained sentiment analysis and conversational opinion mining, in this work, we introduce a novel task of conversational aspect-based sentiment quadruple analysis, namely DiaASQ, aiming to detect the quadruple of target-aspect-opinion-sentiment in a dialogue. We manually construct a large-scale high-quality DiaASQ dataset in both Chinese and English languages. We deliberately develop a neural model to benchmark the task, which advances in effectively performing end-to-end quadruple prediction, and manages to incorporate rich dialogue-specific and discourse feature representations for better cross-utterance quadruple extraction. We hope the new benchmark will spur more advancements in th
&lt;/p&gt;</description></item><item><title>&#20026;&#35299;&#20915;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#20013;&#8220;&#26080;&#28165;&#26224;&#35821;&#38899;&#8221;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#22122;&#22768;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.15368</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#22122;&#22768;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#29992;&#20110;&#26080;&#28165;&#26224;&#35821;&#38899;&#30340;&#35821;&#38899;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech. (arXiv:2210.15368v3 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15368
&lt;/p&gt;
&lt;p&gt;
&#20026;&#35299;&#20915;&#35821;&#38899;&#22686;&#24378;&#39046;&#22495;&#20013;&#8220;&#26080;&#28165;&#26224;&#35821;&#38899;&#8221;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#21363;&#20351;&#22312;&#22495;&#20869;&#21644;&#22495;&#22806;&#22122;&#22768;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#20173;&#33021;&#26377;&#25928;&#65292;&#23454;&#39564;&#32467;&#26524;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#28165;&#26224;&#35821;&#38899;&#26159;&#21457;&#23637;&#35821;&#38899;&#22686;&#24378;&#31995;&#32479;&#30340;&#23454;&#38469;&#25361;&#25112;&#65292;&#36825;&#24847;&#21619;&#30528;&#23427;&#20204;&#30340;&#35757;&#32451;&#20934;&#21017;&#21644;&#35780;&#20272;&#25351;&#26631;&#20043;&#38388;&#23384;&#22312;&#19981;&#21487;&#36991;&#20813;&#30340;&#19981;&#21305;&#37197;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#19981;&#21033;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35757;&#32451;&#21644;&#25512;&#29702;&#31574;&#30053;&#65292;&#20854;&#20013;&#20351;&#29992;&#22686;&#24378;&#35821;&#38899;&#20316;&#20026;&#30446;&#26631;&#26469;&#25913;&#36827;&#20808;&#21069;&#25552;&#20986;&#30340;&#22122;&#22768;&#30446;&#26631;&#35757;&#32451;&#65288;NyTT&#65289;&#12290;&#30001;&#20110;&#22495;&#20869;&#22122;&#22768;&#19982;&#22806;&#37096;&#22122;&#22768;&#30340;&#21516;&#36136;&#24615;&#26159;NyTT&#26377;&#25928;&#24615;&#30340;&#20851;&#38190;&#65292;&#25105;&#20204;&#36890;&#36807;&#28151;&#38899;&#35757;&#32451;&#22810;&#20010;&#23398;&#29983;&#27169;&#22411;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#20272;&#35745;&#30340;&#35821;&#38899;&#21644;&#22122;&#22768;&#36827;&#34892;&#22686;&#24378;&#30446;&#26631;&#35757;&#32451;&#65292;&#25110;&#32773;2&#65289;&#20351;&#29992;&#21407;&#22987;&#30340;&#22122;&#22768;&#35821;&#38899;&#21644;&#25945;&#24072;&#27169;&#22411;&#20272;&#35745;&#30340;&#22122;&#22768;&#36827;&#34892;&#22122;&#22768;&#30446;&#26631;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20960;&#31181;&#22522;&#32447;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#25945;&#24072;/&#23398;&#29983;&#25512;&#29702;&#26041;&#38754;&#65292;&#20854;&#20013;&#39044;&#27979;&#30340;&#28165;&#26224;&#35821;&#38899;&#26159;&#36890;&#36807;&#25945;&#24072;&#21644;&#26368;&#32456;&#23398;&#29983;&#27169;&#22411;&#25104;&#21151;&#22320;&#25512;&#23548;&#20986;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#32467;&#21512;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#65292;&#30456;&#27604;&#32423;&#32852;&#31995;&#32479;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#32763;&#35793;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#20250;&#26377;&#25152;&#19979;&#38477;&#12290;</title><link>http://arxiv.org/abs/2210.11987</link><description>&lt;p&gt;
&#32467;&#21512;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#35821;&#38899;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Joint Speech Translation and Named Entity Recognition. (arXiv:2210.11987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#32467;&#21512;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#65292;&#30456;&#27604;&#32423;&#32852;&#31995;&#32479;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#32763;&#35793;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#19981;&#20250;&#26377;&#25152;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#33258;&#21160;&#32763;&#35793;&#31995;&#32479;&#26088;&#22312;&#23558;&#20154;&#31867;&#25918;&#22312;&#20013;&#24515;&#20301;&#32622;&#65292;&#36890;&#36807;&#25552;&#20379;&#19978;&#19979;&#25991;&#25903;&#25345;&#21644;&#30693;&#35782;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26159;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#23454;&#20307;&#38142;&#25509;&#31995;&#32479;&#26469;&#20016;&#23500;&#36755;&#20986;&#30340;&#26377;&#20851;&#25552;&#21040;&#30340;&#23454;&#20307;&#30340;&#20449;&#24687;&#65292;&#32780;&#36825;&#30446;&#21069;&#26159;&#22312;&#29983;&#25104;&#30340;&#32763;&#35793;&#19978;&#36827;&#34892;&#30340;&#12290;&#37492;&#20110;&#30452;&#25509;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#27169;&#22411;&#30340;&#26368;&#36817;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#21644;&#32423;&#32852;&#31995;&#32479;&#30340;&#24050;&#30693;&#32570;&#38519;&#65288;&#35823;&#24046;&#20256;&#25773;&#21644;&#39069;&#22806;&#24310;&#36831;&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#20849;&#21516;&#25191;&#34892;ST&#21644;NER&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#32423;&#32852;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;NER&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#32423;&#32852;&#27169;&#22411;&#65288;0.4-1.0 F1&#65289;&#65292;&#32763;&#35793;&#36136;&#37327;&#27809;&#26377;&#38477;&#20302;&#65292;&#24182;&#19988;&#19982;&#32431;ST&#27169;&#22411;&#30456;&#21516;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern automatic translation systems aim at place the human at the center by providing contextual support and knowledge. In this context, a critical task is enriching the output with information regarding the mentioned entities, which is currently achieved processing the generated translation with named entity recognition (NER) and entity linking systems. In light of the recent promising results shown by direct speech translation (ST) models and the known weaknesses of cascades (error propagation and additional latency), in this paper we propose multitask models that jointly perform ST and NER, and compare them with a cascade baseline. The experimental results show that our models significantly outperform the cascade on the NER task (by 0.4-1.0 F1), without degradation in terms of translation quality, and with the same computational efficiency of a plain direct ST model.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#65292;&#24182;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#65292;&#36824;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2210.10260</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-to-End Entity Detection with Proposer and Regressor. (arXiv:2210.10260v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#65292;&#24182;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#65292;&#36824;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#20256;&#32479;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#23884;&#22871;&#22330;&#26223;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#23884;&#22871;&#23454;&#20307;&#35782;&#21035;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23558;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#38598;&#21512;&#39044;&#27979;&#34987;&#36716;&#31227;&#24212;&#29992;&#20110;&#24212;&#23545;&#23454;&#20307;&#23884;&#22871;&#65292;&#20294;&#26159;&#36825;&#20123;&#26041;&#27861;&#30340;&#38382;&#39064;&#22312;&#20110;&#38656;&#35201;&#25163;&#21160;&#21019;&#24314;&#26597;&#35810;&#21521;&#37327;&#65292;&#26080;&#27861;&#36866;&#24212;&#19978;&#19979;&#25991;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#35758;&#22120;&#21644;&#22238;&#24402;&#22120;&#30340;&#31471;&#21040;&#31471;&#23454;&#20307;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#25552;&#35758;&#22120;&#21033;&#29992;&#29305;&#24449;&#37329;&#23383;&#22612;&#32593;&#32476;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23454;&#20307;&#25552;&#35758;&#12290;&#28982;&#21518;&#65292;&#22238;&#24402;&#22120;&#23545;&#25552;&#35758;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#20197;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#20102;&#20165;&#32534;&#30721;&#22120;&#26550;&#26500;&#65292;&#22240;&#27492;&#20855;&#26377;&#26597;&#35810;&#35821;&#20041;&#20016;&#23500;&#12289;&#23454;&#20307;&#23450;&#20301;&#31934;&#24230;&#39640;&#12289;&#27169;&#22411;&#35757;&#32451;&#23481;&#26131;&#31561;&#20248;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31354;&#38388;&#35843;&#21046;&#21464;&#21387;&#22120;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;&#19981;&#21516;&#23454;&#20307;&#20043;&#38388;&#20869;&#37096;&#20851;&#31995;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Named entity recognition is a traditional task in natural language processing. In particular, nested entity recognition receives extensive attention for the widespread existence of the nesting scenario. The latest research migrates the well-established paradigm of set prediction in object detection to cope with entity nesting. However, the manual creation of query vectors, which fail to adapt to the rich semantic information in the context, limits these approaches. An end-to-end entity detection approach with proposer and regressor is presented in this paper to tackle the issues. First, the proposer utilizes the feature pyramid network to generate high-quality entity proposals. Then, the regressor refines the proposals for generating the final prediction. The model adopts encoder-only architecture and thus obtains the advantages of the richness of query semantics, high precision of entity localization, and easiness of model training. Moreover, we introduce the novel spatially modulated
&lt;/p&gt;</description></item><item><title>DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2210.06996</link><description>&lt;p&gt;
DICTDIS&#65306;&#22522;&#20110;&#35789;&#20856;&#32422;&#26463;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#28040;&#27495;&#26041;&#27861;&#23545; NMT &#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
DICTDIS: Dictionary Constrained Disambiguation for Improved NMT. (arXiv:2210.06996v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06996
&lt;/p&gt;
&lt;p&gt;
DICTDIS&#26159;&#19968;&#31181;&#26032;&#39062;&#26377;&#35789;&#20856;&#32422;&#26463;&#30340;NMT&#31995;&#32479;&#65292;&#20854;&#21033;&#29992;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#20174;&#22810;&#20041;&#35789;&#20013;&#28040;&#38500;&#32763;&#35793;&#27495;&#20041;&#30340;&#30446;&#30340;&#65292;&#25552;&#39640;&#20102;&#32763;&#35793;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#29305;&#23450;&#30340;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65288;&#20363;&#22914;&#25945;&#32946;&#24212;&#29992;&#31243;&#24207;&#65289;&#22312;&#22810;&#35821;&#35328;&#31038;&#20250;&#20013;&#24110;&#21161;&#20351;&#20449;&#24687;&#23545;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#29992;&#25143;&#21487;&#35775;&#38382;&#26159;&#20855;&#26377;&#31038;&#20250;&#24847;&#20041;&#30340;&#12290;&#36825;&#31181; NMT &#31995;&#32479;&#24212;&#35813;&#20855;&#26377;&#35789;&#27719;&#32422;&#26463;&#24182;&#20174;&#39046;&#22495;&#29305;&#23450;&#30340;&#35789;&#20856;&#20013;&#27762;&#21462;&#12290;&#30001;&#20110;&#21333;&#35789;&#30340;&#22810;&#20041;&#24615;&#65292;&#35789;&#20856;&#20013;&#21487;&#33021;&#20250;&#20026;&#28304;&#21333;&#35789;&#25110;&#30701;&#35821;&#21576;&#29616;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#12290;&#36825;&#26102;&#65292;NMT &#27169;&#22411;&#38656;&#35201;&#36873;&#25321;&#19982;&#35821;&#22659;&#26368;&#30456;&#20851;&#30340;&#20505;&#36873;&#32763;&#35793;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#24573;&#30053;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#32780;&#20391;&#37325;&#20110;&#21333;&#20010;&#20505;&#36873;&#32422;&#26463;&#35774;&#32622;&#65292;&#20854;&#20013;&#30446;&#26631;&#35789;&#25110;&#30701;&#35821;&#34987;&#21333;&#20010;&#32422;&#26463;&#26367;&#25442;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DICTDIS&#30340;&#35789;&#20856;&#32422;&#26463; NMT &#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#28040;&#38500;&#20102;&#20174;&#23383;&#20856;&#20013;&#24471;&#20986;&#30340;&#22810;&#20010;&#20505;&#36873;&#32763;&#35793;&#30340;&#27495;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#35757;&#32451;&#25968;&#25454;&#19982;&#22810;&#20010;&#23383;&#20856;&#20505;&#36873;&#39033;&#36827;&#34892;&#22686;&#37327;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20174;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#31215;&#26497;&#40723;&#21169;&#28040;&#38500;&#27495;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-specific neural machine translation (NMT) systems (\eg, in educational applications) are socially significant with the potential to help make information accessible to a diverse set of users in multilingual societies. It is desirable that such NMT systems be lexically constrained and draw from domain-specific dictionaries. Dictionaries could present multiple candidate translations for a source word/phrase due to the polysemous nature of words. The onus is then on the NMT model to choose the contextually most appropriate candidate. Prior work has largely ignored this problem and focused on the single candidate constraint setting wherein the target word or phrase is replaced by a single constraint. In this work we present \dictdis, a lexically constrained NMT system that disambiguates between multiple candidate translations derived from dictionaries. We achieve this by augmenting training data with multiple dictionary candidates to actively encourage disambiguation during training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;&#25163;&#21160;&#21019;&#24314;&#30340;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20182;&#20204;&#30340;MKV&#26041;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#22833;&#34913;&#38382;&#39064;&#65292;&#20135;&#29983;&#20102;&#26356;&#26377;&#25928;&#65292;&#26356;&#40065;&#26834;&#30340;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#22120;&#65292;&#20026;&#31616;&#21382;&#25552;&#21462;&#30340;&#23450;&#21046;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2209.09450</link><description>&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Few-shot Approach to Resume Information Extraction via Prompts. (arXiv:2209.09450v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#23569;&#26679;&#26412;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#26041;&#27861;&#65292;&#20351;&#29992;&#25163;&#21160;&#21019;&#24314;&#30340;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#65292;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20182;&#20204;&#30340;MKV&#26041;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#22833;&#34913;&#38382;&#39064;&#65292;&#20135;&#29983;&#20102;&#26356;&#26377;&#25928;&#65292;&#26356;&#40065;&#26834;&#30340;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#22120;&#65292;&#20026;&#31616;&#21382;&#25552;&#21462;&#30340;&#23450;&#21046;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#25552;&#20379;&#20102;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#25552;&#31034;&#23398;&#20064;&#30340;&#24494;&#35843;&#24615;&#33021;&#24341;&#36215;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#23558;&#20854;&#24212;&#29992;&#20110;&#31616;&#21382;&#20449;&#24687;&#25552;&#21462;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#36866;&#29992;&#20110;&#31616;&#21382;&#25991;&#26412;&#30340;&#25163;&#21160;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#65292;&#24182;&#27604;&#36739;&#20102;&#25513;&#34109;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#21644;&#24207;&#21015;&#21040;&#24207;&#21015;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#65288;Seq2Seq PLMs&#65289;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#30693;&#35782;&#24615;&#25552;&#31034;&#24494;&#35843;&#65288;Knowledgeable Prompt-tuning&#65289;&#30340;&#35821;&#35328;&#34920;&#36848;&#35774;&#35745;&#65292;&#20026;&#36328;NLP&#20219;&#21153;&#30340;&#25552;&#31034;&#27169;&#26495;&#35774;&#35745;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25163;&#21160;&#30693;&#35782;&#35821;&#35328;&#34920;&#36848;&#22120;&#8221;&#65288;MKV&#65289;&#65292;&#29992;&#20110;&#26500;&#24314;&#29305;&#23450;&#24212;&#29992;&#31243;&#24207;&#30340;&#35821;&#35328;&#34920;&#36848;&#22120;&#30340;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;MKV&#35268;&#21017;&#20135;&#29983;&#30340;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#22120;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#26377;&#25928;&#21644;&#40065;&#26834;&#12290;&#25105;&#20204;&#30340;MKV&#26041;&#27861;&#35299;&#20915;&#20102;&#26679;&#26412;&#22833;&#34913;&#30340;&#38382;&#39064;&#65292;&#36229;&#36234;&#20102;&#24403;&#21069;&#33258;&#21160;&#25552;&#31034;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#24378;&#35843;&#20102;&#20026;&#31616;&#21382;&#25552;&#21462;&#37327;&#36523;&#23450;&#21046;&#30340;&#25552;&#31034;&#23398;&#20064;&#30340;&#20215;&#20540;&#65292;&#24182;&#24378;&#35843;&#20102;&#23450;&#21046;&#27169;&#26495;&#21644;&#35821;&#35328;&#34920;&#36848;&#22120;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt learning's fine-tune performance on text classification tasks has attracted the NLP community. This paper applies it to resume information extraction, improving existing methods for this task. We created manual templates and verbalizers tailored to resume texts and compared the performance of Masked Language Model (MLM) and Seq2Seq PLMs. Also, we enhanced the verbalizer design for Knowledgeable Prompt-tuning, contributing to prompt template design across NLP tasks. We present the Manual Knowledgeable Verbalizer (MKV), a rule for constructing verbalizers for specific applications. Our tests show that MKV rules yield more effective, robust templates and verbalizers than existing methods. Our MKV approach resolved sample imbalance, surpassing current automatic prompt methods. This study underscores the value of tailored prompt learning for resume extraction, stressing the importance of custom-designed templates and verbalizers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#24515;&#29702;&#23398;&#24341;&#23548;&#24605;&#32500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#38544;&#21947;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#26469;&#36873;&#25321;&#27491;&#30830;&#30340;&#37322;&#20041;&#12290;</title><link>http://arxiv.org/abs/2209.08141</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#24515;&#29702;&#23398;&#21551;&#21457;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#35782;&#21035;&#38544;&#21547;&#21464;&#37327;&#21644;&#25512;&#29702;&#20851;&#31995;&#36827;&#34892;&#38544;&#21947;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models. (arXiv:2209.08141v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#20351;&#29992;&#24515;&#29702;&#23398;&#24341;&#23548;&#24605;&#32500;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#38544;&#21947;&#29702;&#35299;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#20102;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#26469;&#36873;&#25321;&#27491;&#30830;&#30340;&#37322;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#29702;&#35299;&#30340;&#27010;&#29575;&#27169;&#22411;&#26159;&#30740;&#31350;&#20154;&#20204;&#35821;&#35328;&#20351;&#29992;&#30340;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#38656;&#35201;&#25163;&#21160;&#35774;&#35745;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#29992;&#36328;&#39046;&#22495;&#30340;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#27010;&#29575;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#37319;&#29992;&#24605;&#32500;&#38142;&#35302;&#21457;&#26041;&#24335;&#26469;&#23558;&#27010;&#29575;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24341;&#20837;LLMs&#20013;&#65292;&#20197;&#38544;&#21947;&#29702;&#35299;&#20026;&#20363;&#26469;&#25506;&#31350;&#36825;&#19968;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#32500;&#38142;&#35302;&#21457;&#26041;&#24335;&#23548;&#33268;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#38544;&#21547;&#21464;&#37327;&#65292;&#24182;&#24605;&#32771;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#36873;&#25321;&#36866;&#24403;&#30340;&#38544;&#21947;&#37322;&#20041;&#12290;&#25152;&#36873;&#25321;&#30340;&#38544;&#21547;&#21464;&#37327;&#21644;&#20851;&#31995;&#37117;&#22522;&#20110;&#35748;&#30693;&#24515;&#29702;&#23398;&#20013;&#30340;&#38544;&#21947;&#29702;&#35299;&#29702;&#35770;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25552;&#31034;&#24212;&#29992;&#20110;GPT-3&#30340;&#20004;&#20010;&#26368;&#22823;&#29256;&#26412;&#65292;&#24182;&#26174;&#31034;&#23427;&#20204;&#21487;&#20197;&#25552;&#39640;&#37322;&#20041;&#36873;&#25321;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#25286;&#20998;&#20026;&#37096;&#20998;&#24182;&#27880;&#20837;&#25512;&#29702;&#25968;&#25454;&#38598;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;16&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#36798;&#21040;17.83&#65285;&#30340;&#32477;&#23545;&#22686;&#30410;&#12290;</title><link>http://arxiv.org/abs/2205.12495</link><description>&lt;p&gt;
ToKen&#65306;&#23569;&#26679;&#26412;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#30340;&#20219;&#21153;&#25286;&#35299;&#21644;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
ToKen: Task Decomposition and Knowledge Infusion for Few-Shot Hate Speech Detection. (arXiv:2205.12495v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#25286;&#20998;&#20026;&#37096;&#20998;&#24182;&#27880;&#20837;&#25512;&#29702;&#25968;&#25454;&#38598;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#22312;16&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#21487;&#36798;&#21040;17.83&#65285;&#30340;&#32477;&#23545;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#23427;&#20381;&#36182;&#20110;&#24120;&#35782;&#25512;&#29702;&#12289;&#23545;&#21051;&#26495;&#21360;&#35937;&#30340;&#20102;&#35299;&#20197;&#21450;&#23545;&#19981;&#21516;&#25991;&#21270;&#32972;&#26223;&#19979;&#31038;&#20132;&#32454;&#24494;&#24046;&#21035;&#30340;&#29702;&#35299;&#12290;&#32780;&#19988;&#24456;&#38590;&#25910;&#38598;&#22823;&#35268;&#27169;&#30340;&#26631;&#27880;&#22909;&#30340;&#20167;&#24680;&#35328;&#35770;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#20854;&#8220;&#26500;&#25104;&#8221;&#37096;&#20998;&#65292;&#21487;&#20197;&#21462;&#24471;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20174;&#25512;&#29702;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;Atomic2020&#65289;&#20013;&#27880;&#20837;&#30693;&#35782;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#27867;&#21270;&#21040;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;&#38598;&#65292;&#26174;&#31034;&#20102;&#20219;&#21153;&#25286;&#35299;&#21644;&#30693;&#35782;&#27880;&#20837;&#30456;&#23545;&#20110;&#20808;&#21069;&#20351;&#29992;&#30340;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;16&#20010;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22522;&#32447;&#19978;&#34920;&#29616;&#20986;17.83&#65285;&#30340;&#32477;&#23545;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hate speech detection is complex; it relies on commonsense reasoning, knowledge of stereotypes, and an understanding of social nuance that differs from one culture to the next. It is also difficult to collect a large-scale hate speech annotated dataset. In this work, we frame this problem as a few-shot learning task, and show significant gains with decomposing the task into its "constituent" parts. In addition, we see that infusing knowledge from reasoning datasets (e.g. Atomic2020) improves the performance even further. Moreover, we observe that the trained models generalize to out-of-distribution datasets, showing the superiority of task decomposition and knowledge infusion compared to previously used methods. Concretely, our method outperforms the baseline by 17.83% absolute gain in the 16-shot case.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; CombLM &#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#35843;&#25972;&#22823;&#22411;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#19988;&#19981;&#38656;&#35201;&#35775;&#38382;&#23427;&#20204;&#30340;&#26435;&#37325;&#25110;&#20013;&#38388;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2205.12213</link><description>&lt;p&gt;
CombLM: &#36890;&#36807;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#35843;&#25972;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models. (arXiv:2205.12213v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181; CombLM &#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#24494;&#35843;&#27169;&#22411;&#35843;&#25972;&#22823;&#22411;&#40657;&#30418;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#19988;&#19981;&#38656;&#35201;&#35775;&#38382;&#23427;&#20204;&#30340;&#26435;&#37325;&#25110;&#20013;&#38388;&#28608;&#27963;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#22810;&#20010;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#24615;&#33021;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#23558;&#35821;&#35328;&#27169;&#22411;&#36866;&#24212;&#26032;&#20219;&#21153;&#21644;&#22495;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#23545;&#27169;&#22411;&#26377;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#20462;&#25913;&#20854;&#21442;&#25968;&#36827;&#34892;&#25805;&#20316;&#12290;&#20294;&#36825;&#19982;&#35813;&#39046;&#22495;&#30340;&#26368;&#39640;&#36136;&#37327;&#27169;&#22411;&#20165;&#36890;&#36807;&#25512;&#29702;API&#20316;&#20026;&#40657;&#30418;&#21487;&#29992;&#30340;&#26368;&#36817;&#36235;&#21183;&#19981;&#20860;&#23481;&#12290;&#21363;&#20351;&#21487;&#29992;&#27169;&#22411;&#26435;&#37325;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#35745;&#31639;&#25104;&#26412;&#20063;&#21487;&#33021;&#23545;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#36866;&#24212;&#26032;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#20551;&#35774;&#27809;&#26377;&#35775;&#38382;&#23427;&#20204;&#30340;&#26435;&#37325;&#25110;&#20013;&#38388;&#28608;&#27963;&#30340;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#23567;&#39564;&#35777;&#38598;&#19978;&#23398;&#20064;&#30340;&#23567;&#22411;&#32593;&#32476;&#65292;&#22312;&#27010;&#29575;&#32423;&#21035;&#19978;&#24494;&#35843;&#23567;&#22411;&#30333;&#30418;LM&#65292;&#24182;&#23558;&#20854;&#19982;&#22823;&#22411;&#40657;&#30418;LM&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#22823;&#22411;LM&#65288;OPT-30B&#65289;&#36866;&#24212;&#22810;&#20010;&#39046;&#22495;&#21644;&#19979;&#28216;&#20219;&#21153;&#65288;&#26426;&#22120;&#32763;&#35793;&#65289;&#65292;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#35266;&#23519;&#21040;&#24615;&#33021;&#30340;&#25552;&#39640;&#65292;&#26368;&#39640;&#21487;&#36798;9\%&#65292;&#21516;&#26102;&#20351;&#29992;&#39046;&#22495;&#19987;&#23478;23&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9\%, while using a domain expert 23x 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#35270;&#22270;&#30340;&#25439;&#22833;&#20989;&#25968;CORAL&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#23545;&#35805;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;</title><link>http://arxiv.org/abs/2205.10558</link><description>&lt;p&gt;
CORAL&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#29983;&#25104;&#23545;&#35805;&#27169;&#22411;&#30340;&#21709;&#24212;&#21487;&#26816;&#32034;&#24615;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
CORAL: Contextual Response Retrievability Loss Function for Training Dialog Generation Models. (arXiv:2205.10558v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#35270;&#22270;&#30340;&#25439;&#22833;&#20989;&#25968;CORAL&#65292;&#29992;&#20110;&#35780;&#20272;&#29983;&#25104;&#23545;&#35805;&#27169;&#22411;&#30340;&#21709;&#24212;&#36136;&#37327;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#21516;&#26102;&#32771;&#34385;&#21040;&#20102;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;&#20132;&#21449;&#29109;&#65288;CE&#65289;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35768;&#22810;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#23545;&#35805;&#30340;&#20219;&#21153;&#23545;CE&#25439;&#22833;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36825;&#26159;&#22240;&#20026;CE&#25439;&#22833;&#20551;&#23450;&#23545;&#20110;&#20219;&#20309;&#32473;&#23450;&#30340;&#36755;&#20837;&#65292;&#21807;&#19968;&#21487;&#33021;&#30340;&#36755;&#20986;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20316;&#20026;&#22522;&#26412;&#30495;&#23454;&#30340;&#36755;&#20986;&#12290;&#20294;&#26159;&#65292;&#22312;&#29983;&#25104;&#23545;&#35805;&#20013;&#65292;&#21487;&#20197;&#26377;&#22810;&#20010;&#26377;&#25928;&#30340;&#21709;&#24212;&#65288;&#23545;&#20110;&#32473;&#23450;&#30340;&#19978;&#19979;&#25991;&#65289;&#65292;&#23427;&#20204;&#19981;&#20165;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#38754;&#24418;&#24335;&#65292;&#32780;&#19988;&#21487;&#20197;&#26159;&#35821;&#20041;&#19978;&#19981;&#21516;&#30340;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#23545;&#35805;&#29983;&#25104;&#20219;&#21153;&#30340;CE&#25439;&#22833;&#35745;&#31639;&#19981;&#32771;&#34385;&#36755;&#20837;&#19978;&#19979;&#25991;&#65292;&#24182;&#19988;&#22522;&#20110;&#21709;&#24212;&#23545;&#20854;&#36827;&#34892;&#35780;&#20998;&#65292;&#32780;&#19981;&#32771;&#34385;&#19978;&#19979;&#25991;&#12290;&#35201;&#23545;&#29983;&#25104;&#30340;&#21709;&#24212;&#36827;&#34892;&#31867;&#20284;&#20851;&#32852;&#24615;&#12289;&#21560;&#24341;&#21147;&#31561;&#21697;&#36136;&#30340;&#35780;&#20998;&#65292;&#25439;&#22833;&#20989;&#25968;&#24212;&#35813;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#21644;&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#35270;&#22270;&#30340;CORAL&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Natural Language Processing, there are many tasks that can be tackled effectively using the cross-entropy (CE) loss function. However, the task of dialog generation poses unique challenges for CE loss. This is because CE loss assumes that, for any given input, the only possible output is the one available as the ground truth in the training dataset. But, in dialog generation, there can be multiple valid responses (for a given context) that not only have different surface forms but can also be semantically different. Furthermore, CE loss computation for the dialog generation task does not take the input context into consideration and, hence, it grades the response irrespective of the context. To grade the generated response for qualities like relevance, engagingness, etc., the loss function should depend on both the context and the generated response. To address these limitations, this paper proposes CORAL, a novel loss function based on a reinforcement learning (RL) vie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#36328;&#35821;&#35328;&#22810;&#35828;&#35805;&#20154;TTS&#21644;&#36328;&#35821;&#35328;&#35821;&#38899;&#36716;&#25442;&#36827;&#34892;ASR&#25968;&#25454;&#22686;&#24191;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#20010;&#35828;&#35805;&#20154;&#25913;&#21892;ASR&#31995;&#32479;&#65292;&#24182;&#19988;&#33021;&#22815;&#32553;&#23567;ASR&#27169;&#22411;&#38388;&#20351;&#29992;&#20154;&#24037;&#21644;&#21512;&#25104;&#35821;&#38899;&#35757;&#32451;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#20351;&#29992;&#21333;&#19968;&#30495;&#23454;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#21516;&#26679;&#33021;&#22815;&#33719;&#24471;&#26377;&#24076;&#26395;&#30340;ASR&#35757;&#32451;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.00618</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#36328;&#35821;&#35328;&#22810;&#35828;&#35805;&#20154;TTS&#21644;&#36328;&#35821;&#35328;&#35821;&#38899;&#36716;&#25442;&#36827;&#34892;ASR&#25968;&#25454;&#22686;&#24191;
&lt;/p&gt;
&lt;p&gt;
ASR data augmentation in low-resource settings using cross-lingual multi-speaker TTS and cross-lingual voice conversion. (arXiv:2204.00618v5 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.00618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#22312;&#20302;&#36164;&#28304;&#24773;&#20917;&#19979;&#20351;&#29992;&#36328;&#35821;&#35328;&#22810;&#35828;&#35805;&#20154;TTS&#21644;&#36328;&#35821;&#35328;&#35821;&#38899;&#36716;&#25442;&#36827;&#34892;ASR&#25968;&#25454;&#22686;&#24191;&#30340;&#26041;&#27861;&#65292;&#20854;&#21487;&#20197;&#36890;&#36807;&#21482;&#20351;&#29992;&#19968;&#20010;&#35828;&#35805;&#20154;&#25913;&#21892;ASR&#31995;&#32479;&#65292;&#24182;&#19988;&#33021;&#22815;&#32553;&#23567;ASR&#27169;&#22411;&#38388;&#20351;&#29992;&#20154;&#24037;&#21644;&#21512;&#25104;&#35821;&#38899;&#35757;&#32451;&#30340;&#24046;&#36317;&#65292;&#21516;&#26102;&#20351;&#29992;&#21333;&#19968;&#30495;&#23454;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#21516;&#26679;&#33021;&#22815;&#33719;&#24471;&#26377;&#24076;&#26395;&#30340;ASR&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#22810;&#35828;&#35805;&#20154;&#35821;&#38899;&#21512;&#25104;&#21644;&#36328;&#35821;&#35328;&#35821;&#38899;&#36716;&#25442;&#22312;&#20302;/&#20013;&#36164;&#28304;&#24773;&#22659;&#19979;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#25968;&#25454;&#22686;&#24191;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#20351;&#29992;&#19968;&#20010;&#30446;&#26631;&#35821;&#35328;&#35828;&#35805;&#20154;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24212;&#29992;&#35821;&#38899;&#21512;&#25104;&#21644;&#35821;&#38899;&#36716;&#25442;&#21487;&#20197;&#25913;&#21892;ASR&#31995;&#32479;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#30456;&#27604;&#20351;&#29992;&#22810;&#35828;&#35805;&#20154;&#30340;&#20154;&#24037;&#21644;&#21512;&#25104;&#35821;&#38899;&#35757;&#32451;ASR&#27169;&#22411;&#38388;&#30340;&#24046;&#36317;&#24471;&#20197;&#32553;&#23567;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21482;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#21333;&#19968;&#30495;&#23454;&#35828;&#35805;&#20154;&#30340;&#25968;&#25454;&#22686;&#24191;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26377;&#24076;&#26395;&#30340;ASR&#35757;&#32451;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore cross-lingual multi-speaker speech synthesis and cross-lingual voice conversion applied to data augmentation for automatic speech recognition (ASR) systems in low/medium-resource scenarios. Through extensive experiments, we show that our approach permits the application of speech synthesis and voice conversion to improve ASR systems using only one target-language speaker during model training. We also managed to close the gap between ASR models trained with synthesized versus human speech compared to other works that use many speakers. Finally, we show that it is possible to obtain promising ASR training results with our data augmentation method using only a single real speaker in a target language.
&lt;/p&gt;</description></item><item><title>PoNet&#26159;&#19968;&#31181;&#27744;&#21270;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;&#30340;token&#28151;&#21512;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#27604;Transformer&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.02442</link><description>&lt;p&gt;
PoNet&#65306;&#38271;&#24207;&#21015;&#20013;&#39640;&#25928;Token&#28151;&#21512;&#30340;&#27744;&#21270;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PoNet: Pooling Network for Efficient Token Mixing in Long Sequences. (arXiv:2110.02442v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.02442
&lt;/p&gt;
&lt;p&gt;
PoNet&#26159;&#19968;&#31181;&#27744;&#21270;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;&#30340;token&#28151;&#21512;&#65292;&#20854;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#20197;&#27604;Transformer&#26356;&#22909;&#22320;&#22788;&#29702;&#38271;&#24207;&#21015;&#12290;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;NLP&#12289;&#35270;&#35273;&#21644;&#35821;&#38899;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;Transformer&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#21644;&#24207;&#21015;&#38271;&#24230;&#30340;&#24179;&#26041;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#38459;&#30861;&#20102;Transformer-based&#27169;&#22411;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#26377;&#35768;&#22810;&#26041;&#27861;&#34987;&#25552;&#20986;&#65292;&#20363;&#22914;&#31232;&#30095;&#27880;&#24847;&#26426;&#21046;&#12289;&#20302;&#31209;&#30697;&#38453;&#36817;&#20284;&#21644;&#21487;&#25193;&#23637;&#30340;&#26680;&#20989;&#25968;&#65292;&#20197;&#21450;&#26367;&#20195;&#33258;&#27880;&#24847;&#30340;token&#28151;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38271;&#24207;&#21015;&#20013;token&#28151;&#21512;&#30340;&#26032;&#22411;&#27744;&#21270;&#32593;&#32476;(PoNet)&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;&#32447;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#22810;&#31890;&#24230;&#27744;&#21270;&#21644;&#27744;&#21270;&#34701;&#21512;&#26469;&#25429;&#25417;&#19981;&#21516;&#32423;&#21035;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#23558;&#20854;&#19982;token&#30340;&#20132;&#20114;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#38271;&#24207;&#21015;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;PoNet&#26174;&#33879;&#20248;&#20110;Transformer&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#19988;&#22312;&#25152;&#26377;&#22312;GPU&#19978;&#27979;&#37327;&#30340;&#24207;&#21015;&#38271;&#24230;&#19978;&#65292;&#23427;&#20165;&#27604;&#26368;&#24555;&#30340;&#27169;&#22411;FNet&#31245;&#24930;&#12290;&#25105;&#20204;&#36824;&#33021;&#21487;&#35270;&#21270;PoNet&#30340;&#27880;&#24847;&#21147;&#22270;&#65292;&#20197;&#23637;&#31034;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#28151;&#21512;&#20855;&#26377;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;token&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20026;&#22788;&#29702;&#38271;&#24207;&#21015;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#26377;&#25928;&#30340;&#33258;&#27880;&#24847;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#65292;&#20026;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2101.07140</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;RL&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees. (arXiv:2101.07140v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2101.07140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#24494;&#20998;&#20915;&#31574;&#26641;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#20204;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#65292;&#20026;&#26426;&#22120;&#20154;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#35268;&#33539;&#36807;&#31243;&#65292;&#21487;&#20197;&#20351;&#20154;&#31867;&#19982;&#26426;&#22120;&#20154;&#20849;&#21516;&#21551;&#21160;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#35813;&#36807;&#31243;&#21253;&#21547;&#20004;&#20010;&#27493;&#39588;&#65306;&#25919;&#31574;&#35268;&#33539;&#19982;&#25919;&#31574;&#20248;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#21327;&#20316;&#26694;&#26550;&#65292;&#20801;&#35768;&#20154;&#36890;&#36807;&#38750;&#32467;&#26500;&#21270;&#33258;&#28982;&#35821;&#35328;&#25351;&#23450;&#21021;&#22987;&#34892;&#20026;&#27169;&#22411;&#65292;&#24182;&#23558;&#20854;&#36716;&#25442;&#20026;&#35789;&#27719;&#20915;&#31574;&#26641;&#26469;&#21551;&#21160;&#21644;&#35299;&#37322;&#19968;&#20010;&#33258;&#20027;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. the robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this paper, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning an
&lt;/p&gt;</description></item></channel></rss>